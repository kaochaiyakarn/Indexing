fl kluwer academic publishers boston 
manufactured netherlands 
bayesian network classifiers nir friedman nir cs berkeley edu computer science division soda hall university california berkeley ca dan geiger dang cs technion ac il computer science department technion haifa israel moises goldszmidt moises erg sri com sri international ravenswood ave menlo park ca editor provan langley smyth 
supervised learning shown surprisingly simple bayesian classifier strong assumptions independence features called naive bayes competitive state art classifiers 
fact raises question classifier restrictive assumptions perform better 
evaluate approaches inducing classifiers data theory learning bayesian networks 
networks factored representations probability distributions generalize naive bayesian classifier explicitly represent statements independence 
approaches single method call tree augmented naive bayes tan outperforms naive bayes time maintains computational simplicity search involved robustness characterize naive bayes 
experimentally tested approaches problems university california irvine repository compared naive bayes wrapper methods feature selection 

classification basic task data analysis pattern recognition requires construction classifier function assigns class label instances described set attributes 
induction classifiers data sets preclassified instances central problem machine learning 
numerous approaches problem various functional representations decision trees decision lists neural networks decision graphs rules 
effective classifiers sense predictive performance competitive state art classifiers called naive bayesian classifier described example duda hart langley 

classifier learns training data conditional probability attribute class label classification done applying bayes rule compute probability particular instance predicting class highest posterior probability 
computation rendered feasible making strong independence assumption attributes conditionally independent value class independence extended version geiger friedman goldszmidt 
friedman geiger goldszmidt 
structure naive bayes network 
mean probabilistic independence independent pr ajb pr ajc possible values pr 
performance naive bayes somewhat surprising assumption clearly unrealistic 
consider example classifier assessing risk loan applications counterintuitive ignore correlations age education level income 
example raises question improve performance naive bayesian classifiers avoiding unwarranted data assumptions independence 
order tackle problem effectively need appropriate language efficient machinery represent manipulate independence assertions 
provided bayesian networks pearl 
networks directed acyclic graphs allow efficient effective representation joint probability distribution set random variables 
vertex graph represents random variable edges represent direct correlations variables 
precisely network encodes conditional independence statements variable independent graph state parents 
independencies exploited reduce number parameters needed characterize probability distribution efficiently compute posterior probabilities evidence 
probabilistic parameters encoded set tables variable form local conditional distributions variable parents 
independence statements encoded network joint distribution uniquely determined local conditional distributions 
represented bayesian network naive bayesian classifier simple structure depicted 
network captures main assumption naive bayesian classifier attribute leaf network independent rest attributes state class variable root network 
means represent manipulate independence assertions obvious question follows induce better classifiers learning unrestricted bayesian networks 
learning bayesian networks data rapidly growing field research seen great deal activity years including buntine cooper herskovits friedman goldszmidt lam bayesian network classifiers bacchus heckerman heckerman geiger chickering 
form unsupervised learning sense learner distinguish class variable attribute variables data 
objective induce network set networks best describes probability distribution training data 
optimization process implemented practice heuristic search techniques find best candidate space possible networks 
search process relies scoring function assesses merits candidate network 
start examining straightforward application current bayesian networks techniques 
learn networks score minimum description length mdl principle lam bacchus suzuki classification 
results analyzed section mixed learned networks perform significantly better naive bayes data sets perform worse 
trace reasons results definition mdl scoring function 
roughly speaking problem mdl score measures error learned bayesian network variables domain 
minimizing error necessarily minimize local error predicting class variable attributes 
argue similar problems occur scoring functions literature 
accordingly limit attention class network structures structure naive bayes requiring class variable parent attribute 
ensures learned network probability pr cja main term determining classification take attribute account 
naive bayesian classifier classifier allows additional edges attributes capture correlations 
extension incurs additional computational costs 
induction naive bayesian classifier requires simple bookkeeping induction bayesian networks requires searching space possible networks space possible combinations edges 
address problem examine restricted form correlation edges 
resulting method call tree augmented naive bayes tan approximates interactions attributes tree structure imposed naive bayesian structure 
show approximation optimal precise sense learn tan classifiers polynomial time 
result extends known result chow liu see pearl learning tree structured bayesian networks 
examine generalization models idea correlations attributes may vary specific instance class variable 
tan model collection networks classifier 
interestingly chow liu investigated classifiers type recognition handwritten characters 
describing methods report results empirical evaluation comparing state art machine learning methods 
experiments show tan maintains robustness computational complexity naive bayes time displays better accuracy 
compared tan friedman geiger goldszmidt naive bayes selective naive bayes wrapper approach feature subset selection method combined naive bayes set problems university california irvine uci repository see section 
experiments show tan significant improvement approaches 
obtained similar results modified version chow liu original method eliminates errors due variance parameters 
interesting method originally proposed decades ago competitive state art methods developed machine learning community 
organized follows 
section review bayesian networks learn 
section examine straightforward application learning bayesian networks classification show approach yield classifiers exhibit poor performance 
section examine address problem extensions naive bayesian classifier 
section describe detail experimental setup results 
section discuss related alternative solutions problems point previous sections 
conclude section 
appendix reviews concepts information theory relevant contents 

learning bayesian networks consider finite set fx xng discrete random variables variable may take values finite set denoted val 
capital letters variable names lower case letters denote specific values taken variables 
sets variables denoted boldface capital letters assignments values variables sets denoted boldface lowercase letters val obvious way 
joint probability distribution variables subsets say conditionally independent val val val 
bayesian network annotated directed acyclic graph encodes joint probability distribution set random variables formally bayesian network pair hg thetai 
component directed acyclic graph vertices correspond random variables xn edges represent direct dependencies variables 
graph encodes independence assumptions variable independent parents second component pair theta represents set parameters quantifies network 
contains parameter pi pb pi possible value pi pi pi denotes set parents bayesian network defines unique joint probability distribution pb xn pb pi pi bayesian network classifiers note may associate notion minimality definition bayesian network done pearl association irrelevant material 
example fa cg variables attributes class variable 
consider graph structure class variable root pi attribute class variable unique parent pi fcg structure depicted 
type graph structure equation yields pr pr delta pr jc 
definition conditional probability get pr cja ff delta pr delta pr jc ff normalization constant 
fact definition naive bayes commonly literature langley 
problem learning bayesian network informally stated training set fu un instances find network best matches common approach problem introduce scoring function evaluates network respect training data search best network function 
general optimization problem intractable 
certain restricted classes networks efficient algorithms requiring polynomial time number variables network 
take advantage efficient algorithms section propose particular extension naive bayes 
main scoring functions commonly learn bayesian networks bayesian scoring function cooper herskovits heckerman function principle minimal description length mdl lam bacchus suzuki see friedman goldszmidt account scoring function 
scoring functions asymptotically equivalent sample size increases furthermore asymptotically correct probability equal learned distribution converges underlying distribution number samples increases heckerman bouckaert geiger 
depth discussion pros cons scoring function scope 
henceforth concentrate mdl scoring function 
mdl principle rissanen casts learning terms data compression 
roughly speaking goal learner find model facilitates shortest description original data 
length description takes account description model description data model 
context learning bayesian networks model network 
network describes probability distribution pb instances appearing data 
distribution build encoding scheme assigns shorter code words probable instances 
mdl principle choose network combined length network description encoded data respect pb minimized 
hg thetai bayesian network fu un training set assigns value variables mdl scoring friedman geiger goldszmidt function network training data set written mdl bjd mdl bjd log jbj gamma ll bjd jbj number parameters network 
term represents length describing network counts bits needed encode specific network delta log bits parameter theta 
second term negation log likelihood ll bjd log measures bits needed describe probability distribution pb see appendix 
log likelihood statistical interpretation higher log likelihood closer modeling probability distribution data pd delta empirical distribution defined frequencies events pd event val applying equation log likelihood changing order summation yields known decomposition log likelihood structure ll bjd val pi val pi pd pi log pi easy show expression maximized pi pd pi consequently bayesian networks hg thetai hg theta share structure theta satisfies equation ll bjd ll jd 
network structure closed form solution parameters maximize log likelihood score equation 
term equation depend choice parameters solution minimizes mdl score 
crucial observation relieves searching space bayesian networks lets search smaller space network structures fill parameters computing appropriate frequencies data 
henceforth state assume choice parameters satisfies equation 
log likelihood score suitable learning structure network tends favor complete graph structures variable connected variable 
highly undesirable networks provide useful representation independence assertions bayesian network classifiers learned distributions 
networks require exponential number parameters extremely high variance lead poor predictions 
learned parameters maximal network perfectly match training data poor performance test data 
problem called overfitting avoided mdl score 
term mdl score equation regulates complexity networks penalizing contain parameters 
mdl score larger network worse larger smaller network match data better 
practice mdl score regulates number parameters learned helps avoid overfitting training data 
stress mdl score asymptotically correct 
sufficient number independent samples best mdl scoring networks arbitrarily close sampled distribution 
regarding search process rely greedy strategy obvious computational reasons 
procedure starts empty network successively applies local operations maximally improve score local minima 
operations applied search procedure include arc addition arc deletion arc reversal 

bayesian networks classifiers method just described induce bayesian network encodes distribution pb training set 
resulting model set attributes classifier returns label maximizes posterior probability pb cja 
note inducing classifiers manner addressing main concern expressed remove bias introduced independence assumptions embedded naive bayesian classifier 
approach justified asymptotic correctness bayesian learning procedure 
large data set learned network close approximation probability distribution governing domain assuming instances sampled independently fixed distribution 
argument provides sound theoretical basis practice may encounter cases learning process returns network relatively mdl score performs poorly classifier 
understand possible discrepancy predictive accuracy mdl score re examine mdl score 
recall log likelihood term equation measures quality learned model fu un denotes training set 
classification task tuple form ha assigns values attributes class variable rewrite log likelihood function equation friedman geiger goldszmidt ll bjd log pb ja log pb term equation measures estimates probability class attributes 
second term measures estimates joint distribution attributes 
classification determined pb cja term related score network classifier predictive accuracy 
unfortunately term dominated second term attributes grows larger probability particular assignment smaller number possible assignments grows exponentially expect terms form pb yield values closer zero consequently gamma log pb grow larger 
time conditional probability class remain 
implies relatively large error conditional term equation may reflected mdl score 
mdl scoring functions learning unrestricted bayesian networks may result poor classifier cases attributes 
phrase unrestricted networks sense structure graph constrained case naive bayesian classifier 
confirm hypothesis conducted experiment comparing classification accuracy bayesian networks learned mdl score classifiers unrestricted networks naive bayesian classifier 
ran experiment data sets uci repository murphy aha 
section describes detail experimental setup evaluation methods results 
results show classifier unrestricted networks performed significantly better naive bayes data sets performed significantly worse data sets 
quick examination data sets reveals data sets unrestricted networks performed poorly contain attributes 
closer inspection networks induced data sets unrestricted networks performed substantially worse reveals networks number relevant attributes influencing classification small 
data sets soybean large satimage contain attributes respectively classifiers induced relied attributes class prediction 
base definition relevant attributes notion markov blanket variable consists parents children parents children network structure pearl 
set property conditioned markov blanket independent variables network 
particular assignment attributes markov blanket class variable class variable independent rest attributes 
prediction classifier bayesian network examines values attributes markov blanket 
note naive bayesian classifier markov blanket includes attributes bayesian network classifiers percentage classification error data set bayesian network naive bayes naive bayes error bayesian network error 
error curves top scatter plot bottom comparing networks solid line axis naive bayes dashed line axis 
error curves horizontal axis lists data sets sorted curves cross vertical axis measures percentage test instances misclassified prediction errors 
smaller value better accuracy 
data point annotated confidence interval 
scatter plot point represents data set coordinate point percentage misclassifications unsupervised bayesian networks coordinate percentage misclassifications naive bayes 
points diagonal line correspond data sets unrestricted bayesian networks perform better points diagonal line correspond data sets naive bayes performs better 
attributes children graph 
learning structure network learning algorithm chooses attributes relevant friedman geiger goldszmidt predicting class 
words learning procedure performs feature selection 
selection useful discards truly irrelevant attributes 
examples show procedure discard attributes crucial classification 
choices learning procedure reflect bias mdl score penalizes addition crucial attributes class variable markov blanket 
analysis suggests root problem scoring function network better score necessarily better classifier 
straightforward approach problem specialize scoring function mdl case classification task 
restricting log likelihood term equation 
formally conditional log likelihood bayesian network data set cll bjd log pb ja 
problem associated application conditional scoring function practice computational nature 
function decompose structure network analogue equation 
consequence setting parameters pi pd pi longer maximizes score fixed network structure 
need implement addition procedure maximize new function space parameters 
discuss issue section 
alternative approaches discussed section 

extensions naive bayesian classifier section examine approaches maintain basic structure naive bayes classifier ensure attributes part class variable markov blanket 
approaches remove strong assumptions independence naive bayes finding correlations attributes warranted training data 

augmented naive bayesian networks classifiers argued performance bayesian network classifier may improve learning procedure takes account special status class variable 
easy way ensure bias structure network naive bayesian classifier edge class variable attribute 
ensures learned network probability cja take attributes account 
order improve performance classifier bias propose augment naive bayes structure edges attributes needed dispensing strong assumptions independence 
call structures augmented naive bayesian networks edges augmenting edges 
augmented structure edge implies influence assessment class variable depends value bayesian network classifiers pregnant insulin age dpf glucose mass 
tan model learned data set pima 
dashed lines edges required naive bayesian classifier 
solid lines correlation edges attributes 
example influence attribute glucose class depends value insulin naive bayesian classifier influence attribute class variable independent attributes 
edges affect classification process value glucose typically surprising low may unsurprising value correlated attribute insulin high 
situation naive bayesian classifier probability class variable considering observations augmented network 
adding best set augmenting edges intractable problem equivalent learning best bayesian network root 
improve performance naive bayes classifier way computational effort required may worthwhile 
imposing acceptable restrictions form allowed interactions learn optimal set augmenting edges polynomial time 
proposal learn tree augmented naive bayesian tan network class variable parents attribute parents class variable attribute 
attribute augmenting edge pointing 
network fact tan model 
show take advantage restriction learn tan model efficiently 
procedure learning edges known method reported chow liu cl learning tree bayesian networks see pearl pp 

start reviewing cl result 
directed acyclic graph fx xng tree pi contains exactly parent variable parents variable referred root 
tree network described identifying parent variable 
function ng 
ng said define tree xn exactly root tree sequence friedman geiger goldszmidt cycles 
function defines tree network pi fx pi 
chow liu describe procedure constructing tree bayesian network data 
procedure reduces problem constructing maximum likelihood tree finding maximal weighted spanning tree graph 
problem finding tree select subset arcs graph selected arcs constitute tree sum weights attached selected arcs maximized 
known algorithms solving problem time complexity log number vertices graph cormen 
construct tree procedure cl consists steps 
compute pd pair variables log mutual information function 
roughly speaking function measures information provides see appendix detailed description function 

build complete undirected graph vertices variables annotate weight edge connecting pd 

build maximum weighted spanning tree 

transform resulting undirected tree directed choosing root variable setting direction edges outward 
cl prove procedure finds tree maximizes likelihood data theorem chow liu collection instances xn construct tree procedure constructs tree maximizes ll jd time complexity delta 
result adapted learn maximum likelihood tan structure 
set attribute variables class variable 
say tan model pi function defines tree pi fc pi fcg 
optimization problem consists finding tree defining function log likelihood maximized 
prove procedure call construct tan solves optimization problem 
procedure follows general outline cl procedure mutual information attributes uses conditional bayesian network classifiers mutual information attributes class variable 
function defined log xjz roughly speaking function measures information provides value known 
appendix gives detailed description function 
construct tan procedure consists main steps 
compute pd pair attributes 
build complete undirected graph vertices attributes annotate weight edge connecting pd 

build maximum weighted spanning tree 

transform resulting undirected tree directed choosing root variable setting direction edges outward 

construct tan model adding vertex labeled adding arc theorem collection instances procedure construct tan builds tan maximizes ll bt jd time complexity delta 
proof start reformulation log likelihood ll bt jd delta pd pi constant term derive appendix maximizing log likelihood equivalent maximizing term pd pi specialize term tan models 
tan defined delta 
parents pd pi 
parents defined set pd pi pd pd pi pd 
need maximize term pd pd friedman geiger goldszmidt simplify term identity known chain law mutual information cover thomas ip rewrite expression pd pd jc note term affected choice 
suffices maximize second term 
note tan model construct tan guaranteed maximize term maximizes log likelihood 
step construct tan complexity delta third step complexity log 
usually log get stated time complexity 
initial experiments showed tan model works yields classifiers compared naive bayes shown tables 
performance improved additional smoothing operation 
recall learn parameters network estimate conditional frequencies form pd xj pi 
partitioning training data possible values pi computing frequency partition 
partitions contain instances estimate conditional probability unreliable 
problem acute case naive bayesian classifier partitions data class variable usually values class variables adequately represented training data 
tan networks attribute assess conditional probability class variable attribute 
means number partitions twice large 
surprising encounter unreliable estimates especially small data sets 
deal problem introduce smoothing operation parameters learned tan models motivated bayesian considerations 
bayesian learning multinomial distribution start prior probability measure possible settings parameters theta kg compute posterior probability pr theta 
predicted probability new instance weighted average predictions possible setting theta weighted posterior probability 
pr jd pr theta theta theta 
particular family priors called dirichlet priors known closed form solution integral 
dirichlet prior specified hyperparameters theta initial estimate theta number summarizes confidence initial estimate 
think number samples seen lifetime prior making estimate theta hyperparameters theta data set length prediction form jd pd bayesian network classifiers percentage classification error data set tan naive bayes naive bayes error tan error 
error curves scatter plot comparing smoothed tan solid axis naive bayes dashed axis 
error curves smaller value better accuracy 
scatter plot points diagonal line correspond data sets smoothed tan performs better points diagonal line correspond data sets naive bayes performs better 
see detailed description 
friedman geiger goldszmidt percentage classification error data set tan selective naive bayes selective naive bayes error tan error 
error curves scatter plot comparing smoothed tan solid axis selective naive bayes dashed axis 
error curves smaller value better accuracy 
scatter plot points diagonal line correspond data sets smoothed tan performs better points diagonal line correspond data sets selective naive bayes performs better 
see detailed description 
bayesian network classifiers percentage classification error data set tan error tan error 
error curves scatter plot comparing smoothed tan solid axis dashed axis 
error curves smaller value better accuracy 
scatter plot points diagonal line correspond data sets smoothed tan performs better points diagonal line correspond data set performs better 
see detailed description 
friedman geiger goldszmidt refer interested reader degroot 
easy see prediction biases learned parameters manner depends confidence prior number new instances data instances training data bias applied 
number instances large relative bias essentially disappears 
hand number instances small prior dominates 
context learning bayesian networks different dirichlet prior distribution particular value parents heckerman 
results choosing parameters xj pi delta pd pi delta pd pi xj pi delta pd xj pi xj pi delta pd pi xj pi xj pi xj pi prior estimate xj pi xj pi confidence associated prior 
note application dirichlet priors biases estimation parameters depending number instances data particular values parents 
mainly affects estimation parts conditional probability table rarely seen training data 
method choose prior parameters 
reasonable choice prior uniform distribution small reasonable choice uses marginal probability data prior probability 
choice assumption conditional probabilities close observed marginal 
set pi pd 
initial trials choose value experiments 
precisely tried values data sets slightly better 
note smoothing performed determining structure tan model 
smoothed model qualitative structure original model different numerical parameters 
form smoothing standard practice bayesian statistics 
experiments comparing prediction error smoothed tan unsmoothed tan observed smoothed tan performs tan occasionally outperforms tan significantly see results soybean large segment lymphography table 
henceforth assume version tan uses smoothing operator noted 
compares prediction error tan classifier naive bayes 
seen tan classifier dominates naive bayes 
result supports hypothesis relaxing strong independence assumptions naive bayes learn better classifiers 
tried smoothed version naive bayes 
lead significant improvement unsmoothed naive bayes 
data set noticeable improvement lymphography smoothed version accuracy compared smoothing 
note particular data set bayesian network classifiers smoothed version tan accuracy compared smoothing 
complete results smoothed version naive bayes reported table 
tan performs better naive bayes naive bayes comparable quinlan state art decision tree learner may infer tan perform comparison 
confirm prediction performed experiments comparing tan selective naive bayesian classifier langley sage john kohavi 
approach searches subset attributes naive bayes best performance 
results displayed figures table show tan competitive approaches lead significant improvements cases 

bayesian multinets classifiers tan approach forces relations attributes different instances class variable immediate generalization different augmenting edges tree structures case tan class collection networks classifier 
implement idea partition training data set classes 
class val construct bayesian network attribute variables fa resulting probability distribution pb approximates joint distribution attributes specific class pd 
bayesian network called local network set local networks combined prior called bayesian multinet heckerman geiger heckerman 
formally multinet tuple hpc pc distribution bayesian network multinet defines joint distribution pm pc delta pb learning multinet set pc frequency class variable training data pd learn networks manner just described 
classify choosing class maximizes posterior probability pm cja 
partitioning data class variable methodology ensures interactions class variable attributes taken account 
multinet proposal strictly generalization augmented naive bayes sense augmented naive bayesian network easily simulated multinet local networks structure 
note computational complexity finding unrestricted augmenting edges attributes aggravated need learn different network value class variable 
search learning bayesian network structure carried times time different data set 
friedman geiger goldszmidt case augmented naive bayes address problem constraining class local networks learn 
construction set trees minimizes log likelihood score original method chow liu build classifiers recognizing handwritten characters 
reported experiments error rate method half naive bayes 
algorithm theorem separately attributes correspond value class variable 
results multinet network tree 
corollary chow liu collection instances procedure time complexity delta constructs multinet consisting trees maximizes log likelihood 
proof procedure follows 
split partitions contains instances 
set pc pd 

apply procedure construct tree theorem construct steps take linear time 
theorem states step time complexity jd jd conclude procedure time complexity 
tan models apply smoothing avoid unreliable estimation parameters 
note partition data run higher risk missing accurate weight edge contrast tan 
hand tan forces model show augmenting edges classes 
expected experiments see show chow liu cl multinets perform tan approach clearly dominates 

tree networks previous sections concentrated attention tree augmented naive bayesian networks bayesian multinets respectively 
restriction motivated mainly computational considerations networks induced provably effective manner 
raises question achieve better performance cost computational efficiency 
straightforward approach question search space augmented naive bayesian networks larger space bayesian multinets select minimizes mdl score 
approach presents problems 
examine possible network structures resort heuristic search 
bayesian network classifiers percentage classification error data set tan cl multinet cl multinet error tan error 
error curves scatter plot comparing smoothed tan solid line axis smoothed cl multinet classifier dashed line axis 
error curves smaller value better accuracy 
scatter plot points diagonal line corresponds data sets smoothed tan performs better points diagonal line corresponds data sets smoothed cl multinets classifier performs better 
see detailed description 
friedman geiger goldszmidt examined greedy search procedure 
procedure usually finds approximation minimal mdl scoring network 
occasionally poor local minimum 
illustrate point ran procedure data set generated parity function 
concept captured augmenting naive bayes structure complete subgraph 
greedy procedure returned naive bayes structure resulted poor classification rate 
greedy procedure learns network attributes independent class 
consequence addition single edge improve score greedy procedure terminated adding edges 
second problem involves mdl score 
recall mdl score penalizes larger networks 
relative size penalty grows larger smaller data sets score heavily biased simple networks 
result procedure just described learn augmenting edges 
problem especially acute classes 
case naive bayesian structure requires parameters addition augmenting edge involves adding parameters number classes 
contrast note tan cl multinet classifier learn spanning tree attributes 
shown experimental results see table unrestricted augmented naive bayesian networks unrestricted multinets lead improved performance unrestricted bayesian networks section 
data sets better accuracy tan cl multinets 

experimental methodology results ran experiments data sets listed table 
data sets come uci repository murphy aha exception corral 
artificial data sets designed john kohavi evaluate methods feature subset selection 
accuracy classifier percentage successful predictions test sets data set 
mlc system kohavi estimate prediction accuracy classifier variance accuracy 
accuracy measured holdout method larger data sets learning procedures subset instances evaluated remaining instances fold cross validation methods described kohavi smaller ones 
deal missing data removed instances missing values data sets 
currently handle continuous attributes 
applied pre discretization step manner described dougherty 

pre discretization variant fayyad irani discretization method 
preprocessing stages carried mlc system 
runs various learning procedures carried training sets bayesian network classifiers table 
description data sets experiments 
dataset attributes classes instances train test australian cv breast cv chess cleve cv corral cv crx cv diabetes cv flare cv german cv glass cv glass cv heart cv hepatitis cv iris cv letter lymphography cv pima cv satimage segment shuttle small soybean large cv vehicle cv vote cv waveform evaluated test sets 
particular cross validation folds experiments data set 
table displays accuracies main classification approaches discussed abbreviations nb naive bayesian classifier bn unrestricted bayesian networks learned mdl score tan tan networks learned theorem smoothed parameters cl cl multinet classifier bayesian multinets learned theorem smoothed parameters decision tree induction method developed quinlan selective naive bayesian classifier wrapper feature selection applied naive bayes implementation john kohavi previous sections discussed results detail 
summarize highlights 
results displayed table show unrestricted bayesian networks lead significant improvement naive bayesian classifier result poor classifiers presence multiple attributes 
results show tan cl multinet classifier friedman geiger goldszmidt roughly equivalent terms accuracy dominate naive bayesian classifier compare favorably selective naive bayesian classifier 
table displays accuracies naive bayesian classifier tan classifier cl multinet classifier smoothing 
columns labeled nb tan cl accuracies smoothing columns labeled nb tan cl describe accuracies smoothing 
results show smoothing significantly improve accuracy tan cl multinet classifier significantly degrade accuracy results data sets 
improvement noticed mainly small data sets data sets large numbers classes 
hand smoothing significantly improve accuracy naive bayesian classifier 
table summarize accuracies learning unrestricted augmented naive bayes networks anb multinets mn mdl score 
table contains corresponding tree classifiers comparison 
results show learning unrestricted networks improve accuracy data sets contain strong interactions attributes large mdl score add edges 
data sets mdl score reluctant add edges giving structures similar naive bayesian classifier 
consequently data sets predictive accuracy poor compared tan cl multinet classifier 

discussion section review related expand issue conditional log likelihood scoring function 
additionally discuss extend methods deal complicating factors numeric attributes missing values 

related naive bayes interest explaining surprisingly performance naive bayesian classifier domingos pazzani friedman 
analysis provided friedman particularly illustrative focuses characterizing bias variance components estimation error combine influence classification performance 
naive bayesian classifier shows certain conditions low variance associated classifier dramatically mitigate effect high bias results strong independence assumptions 
goal described improve performance naive bayesian classifier relaxing independence assumptions 
empirical results indicate accurate modeling dependencies features leads improved classification 
previous extensions naive bayesian classifier identified strong independence assumptions source bayesian network classifiers table 
experimental results primary approaches discussed 
data set nb bn tan cl australian breast chess cleve corral crx diabetes flare german glass glass heart hepatitis iris letter lymphography pima satimage segment shuttle small soybean large vehicle vote waveform friedman geiger goldszmidt table 
experimental results describing effect smoothing parameters 
data set nb nb tan tan cl cl australian breast chess cleve corral crx diabetes flare german glass glass heart hepatitis iris letter lymphography pima satimage segment shuttle small soybean large vehicle vote waveform bayesian network classifiers table 
experimental results comparing tree networks unrestricted augmented naive bayes multinets 
data set tan anb cl mn australian breast chess cleve corral crx diabetes flare german glass glass heart hepatitis iris letter lymphography pima satimage segment shuttle small soybean large vehicle vote waveform friedman geiger goldszmidt classification errors differ address problem 
works fall categories 
category langley sage john kohavi attempted improve prediction accuracy rendering attributes irrelevant 
rationale follows 
explained section attributes say correlated naive bayesian classifier may weight evidence attributes class 
proposed solution category simply ignore attributes 
removing attributes useful attributes irrelevant introduce noise classification problem 
straightforward application feature subset selection 
usual approach problem search subset attributes estimation scheme cross validation repeatedly evaluate predictive accuracy naive bayesian classifier various subsets 
resulting classifier called selective naive bayesian classifier langley sage 
clear attributes perfectly correlated removal improve performance naive bayesian classifier 
problems arise attributes partially correlated 
cases removal attribute may lead loss useful information selective naive bayesian classifier may retain attributes 
addition wrapper approach general computationally expensive 
experimental results see show methods examine usually accurate selective naive bayesian classifier john kohavi 
second category kononenko pazzani closer spirit proposal attempt improve predictive accuracy removing independence assumptions 
semi naive bayesian classifier kononenko model form delta jc delta delta delta jc pairwise disjoint groups attributes 
model assumes conditionally independent different groups 
assumption independence attributes group 
kononenko method uses statistical tests independence partition attributes groups 
procedure tends select large groups lead overfitting problems 
number parameters needed estimate jc delta gamma grows exponentially number attributes group 
parameters assessed jc may quickly unreliable contains attributes 
pazzani suggests problem solved cross validation scheme evaluate accuracy classifier 
procedure starts singleton groups fa fang combines greedy manner pairs groups 
examines procedure performs feature subset selection stage joining attributes 
procedure general select bayesian network classifiers large groups lead poor prediction accuracy cross validation test 
pazzani procedure learns classifiers partition attributes small groups 
group attributes considered independent rest class classifiers capture small number correlations attributes 
kononenko pazzani essentially assume attributes group arbitrarily correlated 
understand implications assumption bayesian network representation 
fa chain rule get jc jc delta ja delta delta delta ja gamma applying decomposition terms equation get product form bayesian network built 
augmented naive bayes network complete subgraph add arcs introducing cycle variables group contrast tan network tree spans attributes models retain conditional independencies correlated attributes 
example consider data set attributes correlated attribute independent correlations captured semi naive bayesian classifier attributes group 
contrast tan classifier place edge edges capture correlations attributes 
attributes class variable boolean tan model require parameters semi naive bayesian classifier require parameters 
representational tools bayesian networks relax independence assumptions attributes gradual flexible manner study characterize tradeoffs possibility selecting right compromise application hand 
describe method correlations attributes different manner 
computes pairwise mutual information attributes sorts descending order 
method adds edges attributes going computed order reaches predefined threshold approach presents problem 
consider attributes correlated example correlated correlated probabilistically independent method pairwise mutual information combinations appear high algorithm propose edge pair attributes 
edge superfluous relation mediated problem aggravated consider fourth attribute strongly correlated superfluous edges added threshold reached genuine edge ignored favor superfluous 
tan approach relies pairwise computation mutual information avoids problem restricting types interactions form tree 
reiterate restriction friedman geiger goldszmidt tan approach finds optimal tree see theorem 
example shows learning structures trees attributes parent requires examine higher order interactions mutual information related effort categories mentioned reported singh provan 
combine feature subset selection strategies unsupervised bayesian network learning routine 
procedure computationally intensive strategies singh provan involve repeated calls bayesian network learning routine 

conditional log likelihood log likelihood warranted asymptotic argument seen may limited number samples 
section suggested approach decomposition equation evaluates predictive error model restricting log likelihood term equation 
approach example node monitor terminology spiegelhalter dawid lauritzen cowell 
conditional log likelihood bayesian network data set cll bjd log pb ja 
maximizing term amounts maximizing ability correctly predict assignment manipulations analogous described appendix easy show maximizing conditional log likelihood equivalent minimizing conditional cross entropy pd cja jjp cja pd pd cja jjp cja equation shows maximizing conditional log likelihood learning model best approximates conditional probability attribute values 
consequently model maximizes scoring function yield best classifier 
easily derive conditional mdl score conditional log likelihood 
variant learning task stated attempt efficiently describe class values fixed collection attribute records 
term describing length encoding bayesian network model remains second term equal delta cll bjd 
unfortunately effective way maximize term cll bjd computation network minimizes score infeasible 
recall discussed section structure network fixed mdl score minimized simply substituting frequencies data parameters network see equation 
change score bayesian network classifiers cll bjd true restricted class structures 
leaf network outgoing arcs easy prove setting parameters equation maximizes cll bjd fixed network structure 
outgoing arcs describe pb cja product parameters theta pb cja involves normalization constant requires sum values consequence cll bjd decompose maximize choice conditional probability table independently 
closed form equation choosing optimal parameters conditional log likelihood score 
implies maximize choice parameters fixed network structure resort search methods gradient descent space parameters techniques binder 
learning network structure search repeated structure candidate rendering method computationally expensive 
find heuristic approaches allow effective learning conditional log likelihood remains open question 
difference procedures maximize log likelihood ones maximize conditional log likelihood similar standard distinction statistics literature 
dawid describes paradigms estimating 
paradigms differ decompose 
sampling paradigm assume deltap jc assess terms 
diagnostic paradigm assume delta cja assess second term relevant classification process 
general approaches dominates ripley 
naive bayesian classifier extensions evaluated belong sampling paradigm 
unrestricted bayesian networks described section strictly belong paradigm closer spirit sampling paradigm 

numerical attributes missing values assumptions attributes finite numbers values training data complete instance assigns values variables interest 
briefly discuss move restrictions 
approach dealing numerical attributes discretize prior learning model 
done discretization procedure suggested fayyad irani partition range numerical attribute 
invoke learning method treating variables having discrete values 
shown dougherty 
approach quite effective practice 
alternative discretize numerical attributes learning process lets procedure adjust discretization variable contains just partitions capture interactions adjacent friedman geiger goldszmidt variables network 
friedman goldszmidt propose principled method performing discretization 
conceptual difficulty representing hybrid bayesian networks contain discrete continuous variables 
approach involves choosing appropriate representation conditional density numerical variable parents example heckerman geiger examine learning networks gaussian distributions 
straightforward combine representations classes bayesian networks described 
example gaussian variant naive bayesian classifier appears duda hart variant kernel estimators appears john langley 
suspect exist analogues theorem hybrid networks leave issue 
regarding problem missing values theory probabilistic methods provide principled solution 
assume values missing random rubin marginal likelihood probability assigned parts instance observed basis scoring models 
values missing random careful modeling exercised order include mechanism responsible missing data 
source difficulty learning incomplete data marginal likelihood decompose 
score written sum local terms equation 
evaluate optimal choice parameters candidate network structure perform nonlinear optimization em lauritzen gradient descent binder 
problem selecting best structure usually intractable presence missing values 
efforts geiger chickering heckerman examined approximations marginal score evaluated efficiently 
additionally friedman proposed variant em selecting graph structure efficiently search candidates 
computational cost associated methods directly related problem inference learned networks 
fortunately inference tan models performed efficiently 
example friedman method efficiently applied learning tan models presence missing values 
plan examine effectiveness methods dealing missing values 

analyzed direct application mdl method learning unrestricted bayesian networks classification tasks 
showed mdl method presents strong asymptotic guarantees necessarily optimize classification accuracy learned networks 
analysis suggests class scoring functions may better suited task 
bayesian network classifiers scoring functions appear computationally intractable plan explore effective approaches approximations scoring functions 
main contribution experimental evaluation naive bayesian classifiers tan chow liu multinet classifier 
clear situations useful model correlations attributes captured tree structure collections tree structures 
models preferable training instances robustly estimate higher order conditional probabilities 
tan cl multinets embody tradeoff quality approximation correlations attributes computational complexity learning stage 
learning procedures guaranteed find optimal tree structure experimental results show approaches perform practice state art classification methods machine learning 
propose worthwhile tools machine learning community 
acknowledgments authors grateful denise draper ken fertig joe halpern david heckerman ron kohavi pat langley judea pearl lewis stiller comments previous versions useful discussions relating 
ron kohavi technical help mlc library 
done nir friedman moises goldszmidt rockwell science center palo alto california 
nir friedman acknowledges support ibm graduate fellowship nsf iri 
appendix information theoretic interpretation log likelihood review information theoretic notions represent log likelihood score 
concentrate essentials refer interested reader cover thomas comprehensive treatment notions 
joint probability distribution entropy defined hp gamma log 
function hp optimal number bits needed store value roughly measures amount information carried precisely suppose xm sequence independent samples represent xm delta hp bits assuming known 
interpretation mind easy understand properties entropy 
entropy nonnegative encoding length negative 
second entropy zero perfectly predictable value probability 
case reconstruct xm looking friedman geiger goldszmidt encoding 
entropy maximal totally uninformative assigns uniform probability suppose bayesian network learned theta satisfies equation 
entropy associated simply 
applying equation log pb moving product logarithm changing order summation derive equation gamma pi pd pi log pi immediately follows gamma ll bjd equation 
equality consequences 
implies bjd optimal number bits needed describe assuming pb distribution sampled 
observation justifies term bjd measuring representation mdl encoding scheme 
second equality implies maximizing log likelihood equivalent searching model minimizes entropy shown lewis 
reading suggests maximizing log likelihood minimizing description way viewing optimization process cross entropy known kullback leibler divergence kullback leibler 
cross entropy measure distance probability distributions 
formally jjq val log information theoretic interpretation cross entropy average redundancy incurred encoding wrong probability measure 
roughly speaking incur overhead jjq instance encoding samples assume distributed encoding xm hp jjq bits long md jjq optimal 
interpretation cross entropy surprising minimizing pd jjp equivalent minimizing equivalent maximizing ll bjd 
turn attention structure log likelihood term 
measure related entropy conditional entropy measures entropy know value hp xjy gamma xjy log xjy 
terms encoding hp xjy measures optimal number bits needed encode value value 
intuitively knowing value useful encoding compactly 
hp xjy hp 
difference values called mutual information measures information bears formally mutual information defined hp gamma hp xjy log bayesian network classifiers applying definitions equation immediately derive equation ll bjd gamman pd pi pd pi gamma pd observations order 
notice pd independent choice maximize ll gjd maximize term 
representation provides appealing intuition amounts maximizing correlation parents 
second representation lets easily prove complete networks maximize log likelihood superset arcs pi pi immediately derive ll bjd ll jd 
notes 
tan structures called bayesian conditional trees geiger 

alternative notion cestnik context learning naive bayesian classifiers 

choice fold cross validation recommendations kohavi 
binder koller russell kanazawa 
adaptive probabilistic networks hidden variables 
machine learning issue 
bouckaert 

properties bayesian network learning algorithms 
opez de poole eds proceedings tenth conference uncertainty artificial intelligence pp 

san francisco ca morgan kaufmann 
buntine 

theory refinement bayesian networks 
ambrosio smets bonissone eds proceedings seventh annual conference uncertainty artificial intelligence pp 

san francisco ca morgan kaufmann 
buntine 

guide literature learning probabilistic networks data 
ieee trans 
knowledge data engineering 
cestnik 

estimating probabilities crucial task machine learning 
aiello ed proceedings th european conference artificial intelligence pp 

london pitman 
chickering heckerman 
efficient approximations marginal likelihood incomplete data bayesian network 
jensen eds proceedings twelfth conference uncertainty artificial intelligence pp 

san francisco ca morgan kaufmann 
chow liu 
approximating discrete probability distributions dependence trees 
ieee trans 
info 
theory 
cooper herskovits 
bayesian method induction probabilistic networks data 
machine learning 
cormen leiserson rivest 
algorithms 
cambridge ma mit press 
cover thomas 
elements information theory 
new york john wiley sons 
friedman geiger goldszmidt dawid 

properties diagnostic data distributions 
biometrics 
degroot 

optimal statistical decisions 
new york mcgraw hill 
domingos pazzani 
independence conditions optimality simple bayesian classifier 
saitta ed proceedings thirteenth international conference machine learning pp 

san francisco ca morgan kaufmann 
dougherty kohavi sahami 
supervised unsupervised discretization continuous features 
prieditis russell eds proceedings twelfth international conference machine learning pp 

san francisco ca morgan kaufmann 
duda hart 
pattern classification scene analysis 
new york john wiley sons 

fraud debt detection bayesian network learning system rare binary outcome mixed data structures 
besnard hanks eds proceedings eleventh conference uncertainty artificial intelligence pp 

san francisco ca morgan kaufmann 
fayyad irani 
multi interval discretization continuous valued attributes classification learning 
proceedings thirteenth international joint conference artificial intelligence pp 

san francisco ca morgan kaufmann 
friedman 

bias variance loss curse dimensionality 
data mining knowledge discovery 
friedman 

learning belief networks presence missing values hidden variables 
fisher ed proceedings fourteenth international conference machine learning pp 

san francisco ca morgan kaufmann 
friedman goldszmidt 
building classifiers bayesian networks 
proceedings national conference artificial intelligence pp 

menlo park ca aaai press 
friedman goldszmidt 
discretization continuous attributes learning bayesian networks 
saitta ed proceedings thirteenth international conference machine learning pp 

san francisco ca morgan kaufmann 
friedman goldszmidt 
learning bayesian networks local structure 
jensen eds proceedings twelfth conference uncertainty artificial intelligence pp 

san francisco ca morgan kaufmann 
geiger 

entropy learning algorithm bayesian conditional trees 
dubois wellman ambrosio smets eds proceedings eighth annual conference uncertainty artificial intelligence pp 

san francisco ca morgan kaufmann 
geiger heckerman 
knowledge representation inference similarity networks bayesian multinets 
artificial intelligence 
geiger heckerman meek 
asymptotic model selection directed graphs hidden variables 
jensen eds proceedings twelfth conference uncertainty artificial intelligence pp 

san francisco ca morgan kaufmann 
heckerman 

probabilistic similarity networks 
cambridge ma mit press 
heckerman 

tutorial learning bayesian networks 
technical report msr tr microsoft research 
heckerman geiger 
learning bayesian networks unification discrete gaussian domains 
besnard hanks eds proceedings eleventh conference uncertainty artificial intelligence pp 

san francisco ca morgan kaufmann 
heckerman geiger chickering 
learning bayesian networks combination knowledge statistical data 
machine learning 
john kohavi 
wrappers feature subset selection 
artificial intelligence 
accepted publication 
preliminary version appears proceedings eleventh international conference machine learning pp 
title irrelevant features subset selection problem 
john langley 
estimating continuous distributions bayesian classifiers 
besnard hanks eds proceedings eleventh conference uncertainty artificial intelligence pp 

san francisco ca morgan kaufmann 
bayesian network classifiers kohavi 

study cross validation bootstrap accuracy estimation model selection 
proceedings fourteenth international joint conference artificial intelligence pp 

san francisco ca morgan kaufmann 
kohavi john long manley pfleger 
mlc machine learning library 
proc 
sixth international conference tools artificial intelligence pp 

ieee computer society press 
kononenko 

semi classifier 
kodratoff ed proc 
sixth european working session learning pp 

berlin springer verlag 
kullback leibler 
information sufficiency 
annals mathematical statistics 
lam bacchus 
learning bayesian belief networks 
approach mdl principle 
computational intelligence 
langley iba thompson 
analysis bayesian classifiers 
proceedings tenth national conference artificial intelligence pp 

menlo park ca aaai press 
langley sage 
induction selective bayesian classifiers 
opez de poole eds proceedings tenth conference uncertainty artificial intelligence pp 

san francisco ca morgan kaufmann 
lauritzen 

em algorithm graphical association models missing data 
computational statistics data analysis 
lewis 

approximating probability distributions reduce storage requirements 
information control 
murphy aha 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
pazzani 

searching dependencies bayesian classifiers 
fisher lenz eds proceedings fifth international workshop artificial intelligence statistics ft lauderdale fl 
pearl 

probabilistic reasoning intelligent systems 
san francisco ca morgan kaufmann 
quinlan 

programs machine learning 
san francisco ca morgan kaufmann 
ripley 

pattern recognition neural networks 
cambridge cambridge university press 
rissanen 

modeling shortest data description 
automatica 
rubin 

inference missing data 

singh provan 
comparison induction algorithms selective bayesian classifiers 
prieditis russell eds proceedings twelfth international conference machine learning pp 

san francisco ca morgan kaufmann 
singh provan 
efficient learning selective bayesian network classifiers 
saitta ed proceedings thirteenth international conference machine learning pp 

san francisco ca morgan kaufmann 
spiegelhalter dawid lauritzen cowell 
bayesian analysis expert systems 
statistical science 
suzuki 

construction bayesian networks databases mdl scheme 
heckerman mamdani eds proceedings ninth conference uncertainty artificial intelligence pp 

san francisco ca morgan kaufmann 
