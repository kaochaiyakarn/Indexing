note decomposition methods support vector regression peng liao tien lin chih jen lin department computer science information engineering national taiwan university taipei taiwan cjlin csie ntu edu tw dual formulation support vector regression involves closely related sets variables 
decomposition method existing approaches pairs indices sets working set 
basically select base set rst expand indices pairs 
implementation di erent support vector classi cation 
addition larger optimization sub problem solved iteration 
di erent aspects demonstrate needs 
particular show directly base set working set leads similar convergence number iterations 
program simpler smaller working set similar number iterations ecient 
set data points input target output 
major form solving support vector regression svr optimization problem vapnik min upper bound ij lagrange multipliers associated ith data error users tolerate 
note training vectors mapped higher dimensional space function 
important property optimal solution due density currently decomposition method major method solve smola sch olkopf keerthi :10.1.1.46.8538:10.1.1.146.375
iterative process iteration index set variables separated sets working set 
iteration variables corresponding xed sub problem variables corresponding minimized 
approaches support vector classi cation methods selecting working set 
existing approaches regression methods applied nd base set expand elements pairs 
example chosen rst include working set 
sub problem variables solved min ii ij ji jj note variables corresponding xed 
reason doing maintain iterations 
number nonzero variables iterative process kept small 
shown lin theorem existing 
keerthi expand base set pairs property holds :10.1.1.46.8538:10.1.1.146.375
section elaborate detail 
implementation pairs indices 
example libsvm chang lin svmtorch bengio 
question immediately raised performance approaches 
hand think expanded working set leads larger sub problem total number iterations may 
note additional elements pairs obtained free 
hand larger sub problem takes time cost iteration higher 
discuss issue section 
consider approaches smallest working set size approaches analytic solution sub problem available 
sequential minimal optimization smo platt 
mathematical explanation show solving variable sub problems cases variables obtained rst stage working set selection updated 
number iterations variable variable approaches nearly 
need expand working set pairs indices 
discussions larger working sets included 
important clarify properties 
implementation simpler easier 
section conduct experiments demonstrate validity analysis 
discuss expanding working set implementation regression code nearly classi cation 
decomposition approaches support vector regression example flake lawrence 
dealt di erent situations discussed 
working set selection consider working set selection joachims keerthi originally designed classi cation cases :10.1.1.46.8538:10.1.1.146.375
remember dual formulations support vector classi cation min vector ones iteration current iterate problem solved min rf gj number rf gradient note gj means number components zero 
components non zero included working set problem originally proposed joachims 
detailed discussion working set selection lin sections 
special case keerthi :10.1.1.46.8538:10.1.1.146.375
regression min rf gj objective function vector 
solved earlier approaches keerthi expand set elements pairs :10.1.1.46.8538:10.1.1.146.375
directly elements obtained theorem proved lin theorem theorem initial solution zero working set selection problem optimal objective value equal zero 
optimal objective value strictly negative obtained min rf min rf min min frf cg min frf min min frf min frf cg addition kkt condition optimal solution min rf min rf practically replacing small positive number stopping criterion 
stopping criterion discussions 
addition procedure nding solution elements 
number iterations consider case 
easy see chosen time 
example selected respectively rf rf 
objective value 
optimum objective value zero contradiction 
hand selected respectively 
theorem implies 
similarly implies causes contradiction 
chosen 
similar arguments show selected zero 
goal see chosen di erence solving variable problem variable problem 
loss generality consider case chosen 
denote corresponding variables current iteration earlier discussions know 
variable problem solved assume new values 
kkt condition easy see optimal solution variable problem 
di erence happens jump plane plane variables objective value decreased 
illustrate 
gure square represents plane nonzero variables 
linear constraint dashed parallel lines show possible solution plane changes 
example zero optimal solution may reduce objective value entering plane 
check conditions optimal solution 
adjusted line consider objective value constant possible situation plane changes function single variable ii ij ji jj ii ij jj rf rf constant 
increased know rf rf optimal solution plane de ne new function similar 
increased rf rf rf rf rf rf rf ii ij rf ji jj rf rf ii ij jj positive semide nite ii jj ij implies ii ij jj 
know rf rf ii ij jj impossible move plane 
optimal solution 
cases 
results 
note rf rf value obtained 
number checking stopping criterion 
theorem theorem iterations violation stopping criterion optimal solution variable sub problem optimal solution corresponding variable sub problem 
small iterations stopping tolerance smaller 
addition decomposition iterations spent nal stage due slow convergence theorem shown conclusive result matter variable approaches di erence number iterations 
general cases may able get results elegant theorem 
exactly know relation changes 
change variable di erent 
clues kkt condition 
suppose selected 
kkt condition sub problem rf rf rf global convergent sequence unique solution lim certain amount iterations away zero sure 
nal iterations situation happen kkt condition optimal solution shows rf general rf middle 
close greater equal zero 
means satis es kkt condition larger problem including solving smaller problem generated solution 
experiments table problem abalone parameters iter var 
iter var 
sv bsv candidates jumps table problem add parameters iter var 
iter var 
sv bsv candidates jumps section experiment practical problems con rm analysis 
consider regression problems abalone data add data blake merz friedman respectively 
rbf kernel ij kx number attributes data 
problems respectively 
tables results di erent problems 
model selections purpose solution quality 
consider number support vectors approached number training data 
hand largest number support vectors close zero 
parameter set number iterations variable variable approaches number support vectors bounded support vectors number iterations violated conditions theorem possible candidates jumps number real jumps variable approach 
analytic solution variable approach obtained follows variable problem obtained solved 
variable goes zero variable problem solved 
indicated variable problems needed 
experiments versions code directly modi ed libsvm version 
clearly seen approaches take nearly number iterations 
addition number jumps variable approach small especially larger 
furthermore number jump candidates larger number real jumps 
means ii ij jj large usually satis ed 
experiment larger working sets 
simple implementation written matlab small problems tested 
consider rst data points abalone add 
results tables show number iterations approaches 
parameters 
comparing tables di erence number iterations approaches larger 
similarly pairs may lead fewer iterations 
addition pairs indices working set union sets complement check solving sub problem components changed 
total number changes shown jumps columns tables 
clearly seen comparing totally iter components size iteration may contains pairs seldom happens number components changed small 
table problem abalone rst data parameters iter base iter pairs jumps iter base iter pairs jumps note jumps means number changes totally iter components 
analysis experimental results discuss possibility table problem add rst data parameters iter base iter pairs jumps iter base iter pairs jumps classi cation code regression 
note write problem min vector de ned 
clearly structure classi cation problem 
possible nearly program solving 
example libsvm strategy 
directly applied regression properties possible experience shows performance may little worse software specially designed regression 
example properties follows implementing need consider part 
number oating point comparisons smaller 
acknowledgments supported part national science council taiwan nsc 
blake merz 
uci repository machine learning databases 
available www ics uci edu mlearn mlrepository html 
chang 

lin 
libsvm solving di erent support vector formulations 
software available www csie ntu edu tw cjlin libsvm 
bengio 
svmtorch support vector machine large scale regression classi cation problems 
appear machine learning research 
available www idiap ch learning svmtorch html 
flake lawrence 
ecient svm regression training smo 
technical report nec research institute 
friedman 

multivariate adaptive regression splines 
technical report laboratory computational statistics department statistics stanford university 
joachims 

making large scale svm learning practical 
sch olkopf burges smola eds advances kernel methods support vector learning cambridge ma 
mit press 
keerthi bhattacharyya murthy 
improvements platt smo algorithm svm classi er design 
technical report department mechanical production engineering national university singapore 
appear neural computation 
keerthi bhattacharyya murthy 
improvements smo algorithm svm regression 
technical report cd department mechanical production engineering national university singapore 
appear ieee transactions neural networks 


improved decomposition algorithm regression support vector machines 
workshop support vector machines nips 
lin 

convergence decomposition method support vector machines 
technical report department computer science information engineering national taiwan university taipei taiwan 
submitted ieee transactions neural networks 
platt 

fast training support vector machines sequential minimal optimization 
sch olkopf burges smola eds advances kernel methods support vector learning cambridge ma 
mit press 


support vector machines 
software available www ai cs uni dortmund de software 
smola sch olkopf 
tutorial support vector regression 
neuro colt technical report tr royal holloway college 
vapnik 

statistical learning theory 
new york ny john wiley 

