simple fast effective rule learner william cohen yoram singer labs research shannon laboratory park avenue florham park nj usa research att com describe slipper new rule learner generates rulesets repeatedly boosting simple greedy rule builder 
rulesets built rule learners ensemble rules created slipper compact comprehensible 
possible imposing appropriate constraints rule builder proposed generalization adaboost called confidence rated boosting 
spite relative simplicity slipper highly scalable effective learner 
experimentally slipper scales worse log number examples set benchmark problems slipper achieves lower error rates ripper times lower error rates rules times 
boosting schapire freund freund schapire usually create ensemble classifiers :10.1.1.32.8918
popular simple easy implement understood formally effective improving accuracy 
disadvantage boosting improvements accuracy obtained expense comprehensibility 
comprehensibility important appropriate learner produces compact understandable hypothesis instance rule learning system cn clark niblett ripper cohen rules quinlan :10.1.1.50.8204
rule learning systems perform best experimentally disadvantage complex hard implement understood formally 
describe new rule learning algorithm called slipper simple learner iterative pruning produce error reduction 
slipper generates rulesets repeatedly boosting simple greedy 
slipper rule builder inner loops ripper cohen irep furnkranz widmer :10.1.1.50.8204:10.1.1.50.8204
slipper employ set covering process conventional copyright fl american association artificial intelligence 
rights reserved 
rule learners removing examples covered new rule slipper uses boosting reduce weight examples 
rulesets constructed ripper rule learners slipper rulesets desirable property label assigned instance depends rules fire instance 
property shared earlier applications boosting rule learning see instance freund schapire behavior entire ensemble rules affect instance classification 
property classifications rulesets easier understand possible imposing appropriate constraints base learner proposed generalization adaboost schapire singer :10.1.1.156.2440
slipper simpler better understood formally state art rule learners 
spite slipper scales large datasets extremely effective learner 
experimentally slipper run time large real world datasets scales worse log number examples 
set benchmark problems slipper achieves lower error rates ripper times lower error rates rules times 
rulesets produced slipper comparable size produced rules 
slipper algorithm slipper uses boosting create ensemble rules 
weak learner boosted finds single rule essentially process inner loops irep furnkranz widmer ripper cohen :10.1.1.50.8204
specifically weak learner splits training data grows single rule subset data prunes rule subset 
slipper ad hoc metrics guide growing pruning rules replaced metrics formal analysis boosting algorithms 
specific boosting algorithm generalization freund schapire adaboost freund schapire employs confidence rated predictions schapire singer :10.1.1.156.2440:10.1.1.32.8918
generalization allows rules generated weak learner abstain vote confidence zero examples covered rule vote appropriate nonzero confidence covered examples 
current implementation slipper handles class classification problems 
output slipper weighted ruleset rule associated confidence cr classify instance computes sum confidences rules cover predicts sign sum sum greater zero predicts positive class 
order ruleset comprehensible constrain slipper generate rules associated positive confidence rating rules predict membership positive class 
rule negative confidence rating predicts membership negative class single default rule 
representation generalization propositional dnf similar rule learners rule learners classifier set rules associated numerical confidence measure sort voting scheme resolving possible conflicts predictions 
describe slipper algorithm detail 
boosting confidence rated rules boosting algorithms schapire freund developed theoretical reasons answer certain fundamental questions pac learnability kearns valiant 
mathematically beautiful algorithms impractical 
freund schapire developed adaboost algorithm proved practically useful meta learning algorithm 
adaboost works making repeated calls weak learner call weak learner generates single weak hypothesis examples re weighted 
weak hypotheses combined ensemble called strong hypothesis schapire singer studied generalization adaboost weak hypothesis assign real valued confidence prediction 
weak hypothesis assign different confidences different instances particular abstain instances making prediction zero confidence 
ability abstain important purposes 
give brief overview extended boosting framework describe constructing weighted rulesets 
far implemented class version slipper focus class case theory extends nicely multiple classes 
assume set examples ym instance belongs domain label gamma 
assume access weak learning algorithm accepts input training examples distribution instances initially uniform 
generalized boosting setting weak learner computes weak hypothesis form ym gamma initialize ffl train weak learner distribution ffl get weak hypothesis ffl choose ff ffl update exp gammaff output final hypothesis sign ff generalized version adaboost real valued predictions schapire singer :10.1.1.156.2440
sign interpreted predicted label magnitude jh confidence prediction large numbers jh indicate high confidence prediction numbers close zero indicate low confidence 
weak hypothesis abstain predicting label instance setting 
pseudo code describing generalized boosting algorithm normalization constant ensures distribution sums ff depends weak learner 
weak hypotheses rules slipper rules conjunctions primitive conditions 
boosting algorithm rule hypothesis partitions set instances subsets set instances satisfy covered rule satisfied rule 
satisfies write order strong hypothesis similar conventional ruleset force weak hypothesis rule abstain instances unsatisfied setting prediction 
force rules predict confidence cr words th rule generated weak learner require ff cr classify instance strong hypothesis simply adds confidence cr rule satisfied predicts sign sum 
final constraint require rule forms default rule cr positive 
non default rule associated single real valued confidence cr interpreted follows satisfied predict class positive confidence cr abstain 
real value normalize distribution exp gammaff 
depends ff schapire singer showed minimize training error weak learning algorithm pick round boosting weak hypothesis weight ff lead smallest value assume rule generated weak learner 
show confidence value cr rule set minimize omitting dependency rewritten case exp gammay cr cr ffh 
gamma gamma 
simplify equ 
rewrite exp gammac gamma exp cr schapire singer find cr need solve equation dz dcr implies minimized setting cr ln gamma rule may cover examples gamma equal leading extreme confidence values prevent practice smooth confidence adding gamma cr ln gamma smoothed confidence value rule bounded ln 
analysis singer schapire suggests objective function weak learner constructs rules 
plugging value cr equ 
get gamma gamma gamma gamma gamma gamma gamma gamma rule minimizes iff maximizes gamma gamma note rule minimizes maximizing gamma gamma may negatively correlated positive class confidence value cr negative 
described earlier slipper restrict positively correlated rules objective function attempt maximize searching rule gamma gamma summary boosting corresponds roughly outer set covering loop rule learners pagallo haussler quinlan brunk pazzani furnkranz widmer cohen :10.1.1.50.8204
major difference examples covered rule immediately removed training set 
covered examples lower weights degree example weight reduced depends accuracy new rule 
formal analysis boosting schapire singer suggests new quality metric rules ym gamma initialize 
train weak learner current distribution split data 
starting empty rule greedily add conditions maximize equ 

starting output delete final sequence conditions minimize equ 
cr computed equ 

return output default rule whichever minimizes equ 


construct cr equ 
evaluated entire dataset 
ae cr 
update set exp delta cr 
set output final hypothesis sign cr slipper algorithm notice encompassed natural trade accuracy proportion positive examples satisfied rule total number examples rule satisfies coverage fraction examples satisfy rule 
discuss construct rules objective function equ 

rule growing pruning describe weak learner generates individual rules 
procedure similar heuristic rule building procedure ripper cohen irep furnkranz widmer :10.1.1.50.8204:10.1.1.50.8204
rule builder begins randomly splitting dataset disjoint subsets 
split constrained total weight examples 
rule builder invokes routine 
begins empty conjunction conditions considers adding conjunction condition forms nominal attribute legal value continuous variable value occurs training data 
adds condition attains maximal value 
process repeated rule covers negative examples refinement improves rule specific overfits training data resulting rule immediately pruned routine 
considers deleting final sequence conditions rule 
sequence deletions defines new rule goodness evaluated 
candidate rule partitions subsets depending satisfied 
similar definition gamma respectively gamma total weight examples covered labeled respectively gamma 
denote cr smoothed prediction confidence obtained evaluating equ 
gamma associated 
minimizes formula gamma gamma gamma exp gamma cr gamma exp cr interpreted loss defined singer schapire rule associated confidence cr estimated examples 
subject limitations greedy incomplete search procedure rule low score 
guaranteed positively correlated positive class 
allow default rule rule satisfied examples hypothesis rule impossible strong hypothesis classify instances negative 
rule builder return booster output default rule whichever rule lowest value determined equ 

behavior different typically add single default rule rules learned 
note value equ 
confidence value cr calculated weak learner search rule booster assign confidence equ 
entire dataset 
pseudo code slipper 
details possible weak learner generate rule times instance default rule generated times boosting 
round boosting final strong hypothesis compressed removing duplicate rules 
specifically strong hypothesis contains set identical rules 
replaced single rule confidence cr cr step reduces size strong hypothesis reducing classification time improving comprehensibility 
note step alter actual predictions learned ruleset 
approaches perform lossy compaction strong hypothesis instance deleting rules associated low confidence values described slipper free parameter number rounds boosting theoretical analyses number rounds needed boosting freund schapire schapire tend give practically useful bounds :10.1.1.32.8918
internal fold cross validation training set fix training holdout divisions data created usual way algorithm run times tmax rounds training sets tmax upper bound set user 
number rounds produces lowest average error holdout data determined breaking ties favor smaller values algorithm run rounds entire dataset 
experiments value tmax 
experiments evaluate slipper sets benchmark problems containing class classification problems 
set development set debugging slipper evaluating certain variations 
second set prospective set secondary evaluation slipper algorithm development complete 
stage procedure intended guard possibility overfitting benchmark problems experimental results qualitatively similar development prospective sets focus results benchmark problems discussion 
results summarized table detail table 
benchmark problems summarized table 
problems development set discussed cohen :10.1.1.50.8204
problems prospective set taken modification uc irvine repository blake keogh merz exceptions hypothyroid splice junction problems artificially problems case goal separate frequent class remaining classes adult element subsample designated training set market market realworld customer modeling problems provided measure generalization error designated test set available single random partition training set larger problems stratified fold cross validation indicated 
compared slipper performance ripper cohen optimization step decision tree learner quinlan pruning rules rule learner lead better generalization error see instance dietterich scope :10.1.1.50.8204
percent error test data problem name source train test feat ripper opt trees rules rules slipper prospective adult uci att market att market att splice junction uci hypothyroid uci breast wisc uci cv bands uci cv crx uci cv echocardiogram uci cv german uci cv hepatitis uci cv heart hungarian uci cv ionosphere uci cv liver uci cv horse colic uci cv development mushroom uci vote uci move att network att network att market att weather att coding uci ocr att labor uci cv bridges uci cv promoters uci cv sonar uci cv ticket att cv ticket att cv ticket att cv average prospective set average development set average problems average rank problems lowest error rates problems table summary datasets error rates slipper alternative rule learners ripper optimization rules rules decision tree learner 
forth rules rule learner henceforth rules proprietary unpublished descendent rules 
ripper optimization included relatively simple separate conquer variant algorithm evaluated names irep cohen furnkranz :10.1.1.50.8204
results shown detail table 
slipper obtains average lowest error rate sets run option 
benchmarks rule learners slipper ripper rules rules slipper obtains lowest error rate times rules times ripper times rules times 
rule learners average rank slipper compared ripper rules rules 
summaries experimental results table 
scatterplot corresponding figures learning algorithms compared table 
error rate slipper vs ripper opt vs ripper opt vs rules vs rules summary experimental results 
points lines correspond datasets slipper performs better second learner 
point compares slipper second learning system single dataset axis position point error rate slipper axis position error rate points lines correspond datasets slipper performs better second learner 
visual inspection confirms slipper substantially outperforms rule learners performance close best rule learners 
table lr learner corresponding row table lc correspond column 
upper triangle entries average benchmarks quantity error error instance entries fourth column indicate slipper error rate average lower rule learners 
lower triangle entries won loss tied record learner lr versus lc win indicating lr achieved lower error rate 
record underlined statistically significant level bold faced statistically significant level 
instance entry fourth row indicates slipper achieves lower error rate ripper times higher error rate times error rate times 
slipper records versus rules rules similar 
lines table give slipper won records development set prospective set indicating results generally comparable test sets 
exception slipper performance versus rules appears superior development set comparable prospective set 
measured size rulesets produced sole exception network slipper performs noticeably worse methods 
reject null hypothesis probability win tie ripper rules rules slipper ripper rules rules slipper slipper 
devel 
table summary experimental results 
lr lc learners corresponding row column respectively upper triangle entries average error lc error lr 
lower triangle entries won loss tied record learner lr versus lc win indicating lr achieved lower error rate 
different algorithms 
compact rulesets produced ripper average size ripper rulesets rules optimization ripper virtually produces smallest ruleset 
remaining learners produce similar sized rulesets slipper tending produce somewhat smaller rulesets 
average size rulesets rules rules slipper rules rules rules respectively respective average ranks 
largest ruleset produced slipper rules coding 
evaluated scalability rule learners large datasets 
adult addition irrelevant noise variables market examples available 
rules run known scalability problems cohen :10.1.1.50.8204
results shown log log plots 
fastest rule learner datasets usually rules followed ripper variants 
slipper current implementation slower rules ripper scales increasing amounts data 
absolute terms slipper performance quite reasonable slipper needs hours process examples market datasets minutes process training examples adult dataset 
summarize slipper obtains lowest error rates average 
scales large datasets somewhat efficient rules ripper 
slipper rulesets comparable size rules rules somewhat larger ripper 
tailed binomial test 
cv experiments looked size ruleset generated running data average cross validation runs 
argued ripper prunes sort smaller problems predominate uc irvine repository frank witten 
timing results cpu seconds mips irix mhz processors 
examples adult dataset slipper ripper opt ripper opt rules log examples dataset slipper ripper opt ripper opt rules log time examples market dataset slipper ripper opt ripper opt rules log run time performance slipper ripper rules large datasets 
concluding remarks described slipper new rule learning algorithm uses confidence rated boosting learn ensemble rules 
slipper algorithm relatively simple slipper performs set benchmark problems relative ripper slipper achieves lower error rates times error rate times relative rules slipper achieves lower error rates times rate times relative rules slipper achieves lower error rates times rate times 
tailed sign test differences ripper rules rules significant levels respectively 
slipper performs best systems measures aggregate performance average rank 
slipper rulesets moderate size comparable produced rules rules algorithm scales large datasets 
noted slipper lines research 
line research scalable separate conquer rule learning algorithms pagallo haussler quinlan reduced error pruning rep rules brunk pazzani irep furnkranz widmer ripper cohen :10.1.1.50.8204
second line research boosting schapire freund particular adaboost algorithm freund schapire successor developed schapire singer :10.1.1.32.8918
slipper similar earlier application boosting rule learning freund schapire adaboost boost rule builder called 
contrast slipper freund schapire heuristic information gain criterion formal guarantees 
slipper places greater emphasis generating comprehensible rulesets particular slipper generates relatively compact rulesets slipper boosting allows construct rules abstain instances covered rule label assigned instance depends rules fire instance 
freund schapire rule boosting algorithm contrast label instance depends rules ensemble 
algorithm generates ruleset fixed size experiments rules 
slipper boosting departure separate conquer approach earlier rule learners 
alternative rise algorithm domingos combines rule learning nearest neighbour classification bottomup separating control structure 
ruleset constructed rise somewhat difficult interpret label assigned instance depends rules cover rule nearest 
hsu etzioni soderland described experimental rule learner called dairy extends set covering approach traditional rule learners recycling examples reducing weight examples covered previous rules removing examples 
dairy recycling method shown experimentally improve performance number text classification problems 
slipper combination boosting rule building similar recycling viewed formally justified variant 
note important practical advantages learning methods formally understood 
instance existing formal analysis schapire singer generalizes boosting method multi class learning problems setting misclassification costs unequal :10.1.1.156.2440
plan implement multiclass version slipper extension slipper minimizing arbitrary cost matrix maps pair predicted label correct label associated cost 
plan evaluate slipper text classification benchmarks current implementation slipper code ripper inherits ripper ability handle text efficiently 
acknowledgments rob schapire helpful discussions haym hirsh comments draft 
blake keogh merz 
uci repository machine learning databases www ics uci edu mlearn mlrepository html 
irvine ca university california department information computer science 
brunk pazzani 
noise tolerant relational concept learning algorithms 
proceedings eighth international workshop machine learning 
ithaca new york morgan kaufmann 
clark niblett 
cn induction algorithm 
machine learning 
cohen 
fast effective rule induction 
machine learning proceedings twelfth international conference 
lake tahoe california morgan kaufmann 
domingos 
unifying instance rulebased induction 
machine learning 
frank witten 
generating accurate rule sets global optimization 
machine learning proceedings fifteenth international conference 
freund schapire 
experiments new boosting algorithm 
machine learning proceedings thirteenth international conference 
freund schapire 
decisiontheoretic generalization line learning application boosting 
journal computer system sciences 
freund 
boosting weak learning algorithm majority 
information computation 
furnkranz widmer 
incremental reduced error pruning 
machine learning proceedings eleventh annual conference 
new brunswick new jersey morgan kaufmann 
furnkranz 
integrative windowing 
journal artificial intelligence research 
hsu etzioni soderland 
redundant covering algorithm applied text classification 
aaai workshop learning text categorization 
kearns valiant 
cryptographic limitations learning boolean formulae finite automata 
journal association computing machinery 
dietterich 
pruning adaptive boosting 
machine learning proceedings fourteenth international conference 
pagallo haussler 
boolean feature discovery empirical learning 
machine learning 
quinlan 
learning logical definitions relations 
machine learning 
quinlan 
programs machine learning 
morgan kaufmann 
schapire singer 
improved boosting algorithms confidence rated predictions 
proceedings eleventh annual conference computational learning theory 
schapire freund bartlett lee 
boosting margin new explanation effectiveness voting methods 
machine learning proceedings fourteenth international conference 
schapire 
strength weak learnability 
machine learning 
published american association artificial intelligence www aaai org 
document copyrighted american association artificial intelligence available solely benefit authors preparing papers aaai press publications 
document may permission writing copyright holder 
