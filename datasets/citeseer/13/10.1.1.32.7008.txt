mcp jensen cohen cs umass edu keywords running head multiple comparison procedure multiple comparisons induction algorithms david jensen paul cohen experimental knowledge systems laboratory department computer science box university massachusetts amherst ma single mechanism responsible pathologies induction algorithms attribute selection errors overfitting oversearching 
pathology induction algorithms compare multiple items scores evaluation function select item maximum score 
call 
analyze statistical properties show failure adjust properties leads pathologies 
discuss approaches control pathological behavior including bonferroni adjustment randomization testing cross validation 
inductive learning overfitting oversearching attribute selection hypothesis testing parameter estimation multiple comparisons machine learning submitted example mcp mcp mcp mcp multiple comparison procedures best defines analyzes 
ubiquitous induction algorithms ai algorithms 
important statistical properties failure adjust properties produces pathologies induction algorithms attribute selection errors overfitting oversearching 
contribution identify single statistical mechanism underlying pathologies 
induction algorithms implicitly explicitly statistical inferences nearly incorrectly 
understanding inferences incorrect explains pathologies identifies potential solutions explains previously proposed solutions succeeded failed 
discussing induction algorithms analogy suppose deciding hire investment advisor 
person job predict stock market close day 
hope avoid hiring predictions better chance 
evaluate candidate devise test candidate predictions days predictions correct conclude candidate 
threshold chosen probability predicting correctly day probability predict correctly days 
reason candidate passes eleven test probably chances making mistake hiring 
applied single candidate logic 
gather candidates record predictions days select candidate largest number correct predictions apply test candidate 
test just candidate chance producing error probability error depends number candidates 
probability passing test general probability selecting greater probability greater 
adjusting number candidates underestimate roughly order magnitude probability alternatively pass eleven test 
sufficiently large pool practically guarantee achieve performance level doesn mean candidate question performing better chance 
jsj machine learning submitted multiple comparison procedure mcp candidate max max max max max max max max max max max max multiple comparison procedures statistical inferences generate items calculate score item select item maximum score sampling distribution induction algorithms inferences directly analogous deciding hire investment advisor 
discuss instances inferences section understand analogy analyze investment advisor example detail 
decision hire investment advisor divided parts selecting top scoring candidate inferring candidate performing better chance 
selecting top scoring candidate uses multiple comparison procedure mcp 
find candidates 

evaluation function data sample calculate score candidate number correct predictions past fourteen days stock market activity 


select candidate largest number correct predictions 
score inherently statistical particular data sample different samples produce different scores 
statistical terms specific value random variable defined evaluation function item evaluated size sample population data samples drawn 
item values possible samples size population define similarly specific value random variable defined items examined just single item 
sampling distribution depends number items examined 
difference critical making types inferences score example illustrates type infer top scoring candidate 
inference compare sampling distribution generated assumption single candidate performing chance level compare sampling distribution drawn sampling distribution conclude advisor probably 
indicated example sampling distribution generally underestimate probability selecting 
correct sampling distribution distribution depends population machine learning mcp max max max max max max submitted induction algorithms pathologies second type inference illustrated supposing friend selecting investment advisors 
evaluate performance candidates friend evaluates candidates 
compare score best candidate score friend best candidate 
suppose candidates advisor better 
probability top scoring candidate predict correctly days 
case probability greater friend case probability twice 
merely examining candidates friend find score past days candidates perform chance level 
general number candidates evaluate differs number candidates friend evaluates performance top scoring candidates respectively directly comparable drawn different sampling distributions 
problem particularly acute estimate true long run score candidate 
long run score called score generally poor estimate 
suppose quite friend top scoring candidate passed test predicted correctly days 
sample performance infer population predict correctly quarters time 
mistaken friend top scoring candidate just actual probability correct prediction 
types inferences inherently statistical 
problem statistical hypothesis testing 
wish answer question candidate candidate predictions better chance sample score 
second problem parameter estimation 
wish estimate value population long run score sample score accurately compare candidates proportion time candidate predict correctly 
cases scores calculated data sample inherently statistical regardless statistical techniques explicitly 
cases score introduces special problems statistical inference 
example investment advisor directly relevant induction algorithms 
algorithms implicit explicit statistical inferences score examining advisors stock predictions week period induction algorithms examine models predictions training set 
nearly cases induction algorithms adjust number items making max max mcp mcp machine learning unnecessary overfitting errors hypothesis tests attribute selection error overfitting oversearching overfitting submitted problem means limited induction algorithms 
algorithm uses consider making statistical inferences term overfitting ways literature induction algorithms 
refers producing models components reduce population accuracy uses constraining requiring added components harm accuracy 
inferences 
example induction algorithms decide variables model component variable node decision tree decide add component existing model add term linear regression equation select different models 
contexts empirical studies revealed associated pathology respectively 
pathology occurs incorrect statistical inferences score case overfitting inferences viewed statistical hypothesis tests 
cases attribute selection errors oversearching inferences viewed parameter estimates 
formally describe pathologies highlight essential similarities overfitting attribute selection errors oversearching 
formal proofs effects described section provided section appendices 
errors adding components model usually called probably best known pathology induction algorithms quinlan quinlan rivest mingers weiss kulikowski white liu oates jensen 
empirical studies induction algorithms add spurious components models 
components improve accuracy reduce models tested new data samples 
overfitting harmful reasons 
overfitted models incorrect indicate variables related 
applications induced models support additional reasoning brodley rissland correctness central issue 
second overfitted models require space store computational resources models contain unnecessary components 
third presence irrelevant components overfitted model requires collection unnecessary data increasing cost complexity making predictions 
fourth overfitted models difficult understand 
unnecessary components complicate attempts integrate induced models existing knowledge derived sources 
overfitting avoidance justified solely grounds producing comprehensible models quinlan 
overfitted models lower accuracy new data models cmax machine learning max max max max max max max max incorrectly attribute selection error attribute selection errors errors parameter estimates submitted algorithms delay decisions appear final model pruning phase implicit explicit hypothesis tests time 
incorrect inferences occur statistical hypotheses tested correctly 
probability errors arbitrarily small 
term attribute pathology name derived tree building algorithms variables called attributes 
overfitted 
effect demonstrated variety domains systems quinlan jensen 
overfitting occurs multiple comparison procedure applied model components 
algorithm generates set components calculates score component selects component maximum score algorithms decide adding existing model improve model predictive accuracy 
induction algorithms vary widely generate evaluate components algorithms decide add model implicit explicit statistical hypothesis tests 
common form test asks null hypothesis component improve predictive power model probability score large probability small algorithms reject null hypothesis infer adding improve predictive power form test usually applied component associated score test incorrect adjust number components examined 
test ask null hypothesis components improve predictive power model probability maximum score large overfitting occurs wrong form test 
algorithm incorrect inference adds improve predictive power induction algorithms suffer pathology systematic unwarranted preference certain types variables 
example decision tree algorithms far construct models discrete variables values home town discrete variables relatively values gender 
behavior occurs models variables consistently higher scores tested new data samples 
pathology called attribute selection errors particularly tree building systems reported decade quinlan mingers fayyad irani liu white 
errors harmful resulting models eye color blue green brown height gender male eye color green brown class variable value green brown blue node branch left left right variable value green brown blue truth value true true false true false machine learning eye color eye color submitted max max max max max max max max max green brown blue green brown mcp mcp settings map variable values component output consistently lower accuracy new data models considered rejected algorithm 
attribute selection errors result induction algorithms construct model components 
examples model components include nodes decision trees clauses rules nodes connectionist networks terms regression equations 
general component consists variable setting variable drawn directly data sample constructed combination variables 
setting defines mapping values component output 
decision trees setting maps variable values particular branches subtree 
example shows node decision tree 
setting node maps values variable left right branches node 
similarly setting rule maps variable values clause truth value 
shows clause rule 
setting clause bold maps values algorithms select setting component find best setting variable sample 
simplicity examine variable case generalize variables 
variables data sample algorithm generates settings variable settings second variable 
variable algorithm calculates score setting selects setting maximum score produces settings scores respectively 
ideally maximum scores estimates respective population scores denote population score item selected oversearching machine learning max max max max max max max max oversearching errors parameter estimates max mcp submitted early treatments attribute selection error quinlan identify secondary cause pathology evaluation function inherently biased attributes larger numbers possible values 
source error long corrected induction algorithms pathology remains quinlan 
implies incorrect interpretation 
population score item maximum sample score necessarily maximum population score 
estimates population scores determine variables produces best component 
terms classical statistical inference wish produce accurate estimates parameters population scores settings selected unfortunately obvious estimates biased directly comparable 
place scores equal footing score adjusted respective number settings 
scores resulting variables large incorrectly favored scores resulting variables small effect generalizes variables general directly analogous second part investment advisor example 
recall examined performance advisors friend examined performance advisors 
advisors perform chance level friend far find high scoring advisor merely examined advisors 
similarly induction algorithm construct high scoring component number settings large 
induction algorithms directly compare making mistake directly compared advisor friend top scorer 
third pathology revealed studies murthy salzberg quinlan cameron jones examining behavior induction algorithms efficiently search extremely large spaces models 
paradoxically algorithms produce models accurate new data models produced algorithms search fraction space 
pathology termed harmful resulting models lower accuracy constructing models uses computational resources 
algorithms suffer oversearching examine progressively larger spaces models 
initially algorithm examines small space models selects model maximum score 
expands search larger space models selects model maximum score 
expansion continues fixed resource bound reached predefined class models searched exhaustively 
critical value machine learning submitted individual maximum scores max max max max max max max max max max sampling distribution maximum mcp mcp mcp mcp mcp ff ff mcp max max searching progressively larger spaces models involves applications multiple comparison procedure 
attribute selection errors relevant inference produces item best population score sample scores scores directly comparable 
score adjusted number models examined scores resulting large incorrectly favored scores resulting small validity types statistical inferences induction algorithms hypothesis tests parameter estimates depend correct sampling distribution 
investment advisor example sketched sampling distribution depends number items examined section provide general proofs effect sampling distribution distribution compares sampling distribution individual score statistical hypothesis tests sampling distributions directly 
comparing score sampling distribution derived null hypothesis algorithm estimate 
alternatively algorithm sampling distribution derive probability incorrectly rejecting null hypothesis 
induction algorithms explicitly test statistical hypotheses implicitly 
nearly algorithms require component score exceed threshold algorithm include component final model 
threshold serves function critical value just critical value threshold set sampling distribution 
probabilistic interpretation exceeding threshold unknown 
sampling distribution alternatively correct threshold value depends number items examined simplicity concreteness assume scores specific values drawn independent uniform distributions integers 
distribution shown table 
entry table represents joint event resulting maximum score example result 
independent uniform joint event probability probability maximum score generally higher example 
machine learning submitted max max max max max max max max table joint distribution maximum scores takes integer values 
independent identically distributed scores easy specify relationship cumulative probabilities individual scores cumulative probabilities maximum scores example table identical 
useful look upper tail distribution maximum expressions distribution table clear distribution individual score scores underestimates distribution 
underestimates values distributions continuous 
said differently distribution heavier upper tail distribution disparity increases number scores 
consider scores distributed way table 
underestimates half value 
effect demonstrated empirically 
draw data samples instances population single binary classification variable binary attribute variables 
variables independent uniformly distributed 
attribute calculate score indicating predicts classification chi square statistic evaluation function 
machine learning proposition proof submitted maximum score biased estimators max max max max max max max max max max max max max max max max mcp max xp produces values scores distributed chi square 
samples find maximum score scores 
distributions maximum scores approximate sampling distributions 
shows distribution single score compares distributions maximum scores 
sampling distribution diverges sampling distribution 
degree divergence increases practice induction algorithms regularly 
number items considered mcp strongly affects sampling distribution hypothesis tests inaccurate compare sample scores sampling distribution poor parameter estimates responsible pathologies attribute selection error oversearching 
induction algorithms sample score estimate population score item maximum sample score 
way examine estimates compare expected value statistical terms estimator population parameter said unbiased establish discrete continuous random variables 
relationship show biased estimator discrete random variables expected value discrete random variable usually defined sum possible values value multiplied probability scores possible value derived samples sample produces single value samples may produce value mapping samples values expected value discrete random variable equivalently defined possible samples max frequency max machine learning submitted distributions machine learning proposition proof submitted max max max max max max max max max max max max max max max max max max max dx dx dx value sample function selects values score succinctly population summed samples samples identical probability distributions 
samples proven continuous random variables continuous random variables non negative values integrating sides known theorem probability states ross 
samples effect demonstrated empirically 
distributions shown calculate expected value set scores 
table shows expected value maximum score varies know expected value prove biased estimator machine learning submitted effects bias proposition proof proposition max max max max max max max max max max max max max max max max max max mcp table expected value chi square population score item maximum sample score biased estimator population score unbiased estimator population score previously proven 
samples positively biased estimator including population score item maximum sample score words biased estimator shown biased estimator descriptions attribute selection errors oversearching section additional claim degree bias increases making scores 
proofs different cases provided appendix summarize entire section sampling distribution differs 
addition biased estimator population score item maximum sample score 
degree bias increases number items examined independence machine learning submitted influences maximum score jsj max max max max max max max max max max max mcp max max factors influence degree sampling distribution diverges sampling distribution convenience define 
informally indicates probability error assumes distributions equal 
increasing increases probability error 
shown things equal increases section examine factors 
increases approach independence sample size decreases approach equality 
random variables independent knowing value variable tells distribution ross 
discrete random variables independent 
continuous random variables independent 
practice examine items scores independent 
example decision tree algorithms examine multiple partitions continuous variable partitions 
partitions certain dependent scores define related partitions 
addition model components dependent scores variables intrinsically dependent height weight 
prove form dependence positive correlation scores decreases understand effect informally consider effect positive correlation shown 
shows possible joint distributions point graph represents joint event 
score marked variable axis 
points shaded region indicate events independent 
location 
indicated points shaded region making 
shows effect strong positive correlation 
slightly larger nearer zero 
positive correlation scores perfect 
distribution identical distribution 
appendix contains formal proof continuous random variables values positively correlated 
standard error sample size machine learning submitted max max max max max max oe oe oe oe oe max max oe oe oe oe positive correlation affects size sample determinant decreasing sample size increases standard deviation increasing probability values far increasing increasing sampling distribution score standard deviation known score denoted size approaches size entire population approaches zero 
practice induction algorithms calculate scores small samples 
example tree building algorithms systematically decrease sample size repeatedly splitting original data sample 
starting sample size tree branching factor produces leaves fewer instances levels 
lower levels decision trees larger higher levels 
show increasing increases 
restriction holds true nearly situations interest nearly interested cases small probability near 
consider graphical example 
standard errors largest 
standard errors decrease values tend zero 
appendix gives formal proof identically independently distributed 
solutions machine learning ae submitted max max max max max max max difference expected value ffi ffi max max mcp standard error affects previous sections assumed expected values individual scores equal assumption incorrect 
example constructing model components domain medical diagnosis expected values equal diagnostic tests symptoms equally useful predicting disease 
reality utility diagnostic signs varies greatly similar situation prevails induction problems scores different models components settings rarely identical expected values 
convenience define difference expected values scores prove varies inversely shows effect graphically 
shaded portion making 
making 
appendix formally prove identically independently distributed 
methods compensate effects allow valid statistical inferences score covered new data sample derive scores item maximum sample score cross validation derive scores constructing distribution machine learning submitted max max max new max new new mcp mcp mcp mcp new data sample cross validation expected value affects randomization modifying results standard distribution bonferroni adjustment 
methods calculate score treated individual score maximum score methods create sampling distribution appropriate obvious method adjust effects evaluate items new data sample disjoint original sample suppose selects component data sample valid statistical inferences adjust inferences new data sample need consider selected long shares instances case investment advisor analogy test best candidate additional days new sample 
candidate passes eleven test new sample probability incorrectly rejecting hypothesis greater 
induction algorithms quinlan jensen new data compensate effects partition training sample samples sample hypothesis tests parameter estimates resulting items 
cross validation sophisticated method obtaining scores disjoint data samples kohavi cohen weiss kulikowski 
ae machine learning randomization submitted max max max max max max max mcp mcp mcp mcp cross validation divides sample instances disjoint sets contains instances 
selects maximum scoring items sample items evaluated sample produces different estimates accuracy combined produce single estimate averaging 
cross validation compensates effects partially avoids highly variable results obtained single partition data 
method computationally intensive typically results highly variable kohavi 
randomization cohen jensen construct empirical sampling distribution 
iteration randomization creates sample consistent null hypothesis 
obtain actual score repeated producing value sampling distribution null hypothesis 
large number iterations produces approximation complete sampling distribution example consider problem finding binary variables predictive binary variable predictive variable highly correlated sample call correlation hypothesis test requires sampling distribution null hypothesis uncorrelated variables 
randomization produce approximate sampling distribution generating randomized samples finding correlation predictive variable 
randomized sample reproduces values randomly values respect values variables enforcing null hypothesis 
exceeds significant fraction correlations randomized samples infer predictive randomization tests desirable features 
produce distributions appropriate require individual scores examined independent identically distributed requirements technique bonferroni adjustment discussed 
randomization tests create distribution evaluation function just distributions analytically derived 
unfortunately randomization tests computationally expensive requiring evaluation randomized samples 
values typically greater resolution randomization test depends certainly impossible distinctions probability values differ necessary fine distinctions reliably 
previous machine learning submitted max max max max bonferroni adjustment mcp bonferroni adjustment converts probability values single score probability values basic form bonferroni adjustment equation 
scores set equal actual maximum score calculated particular sample determine sampling distribution single score equation determine null hypothesis 
consider algorithm generates models evaluates selects model maximum score 
evaluation function statistic maximum value chi square distribution degree freedom 
algorithm bonferroni adjustment compensate evaluating models conclude 
bonferroni adjustment imposes additional computational burden adjust effects equation holds scores mutually independent identically distributed 
illustrates dependence scores affects bonferroni adjustment randomization cross validation 
experiment similar produced 
create random data samples binary classification variable attribute variables varying levels dependence attributes measured median pairwise correlation 
conduct trials level dependence attributes 
trial uses methods infer correlation classification best attribute significant level significance test distribution single score cross validation randomization bonferroni adjusted test 
axis indicates percentage trials method inferred significant relationship 
ideally empirical probability values median pairwise correlation 
distribution single score clearly fails attributes exhibit complete dependence 
cross validation randomization accurately adjust number comparisons entire range attribute dependence 
bonferroni adjusted estimate correct low values attribute dependence high values 
cross validation randomization tests accurate levels attribute dependence 
previous theories empirical findings machine learning statistics implicate statistical properties multiple comparison procedures cause pathologies induction algorithms 
provides explicit proof median pairwise correlation distribution bonferroni cross validation randomization mcp mcp machine learning tba induce irt submitted multiple comparisons different methods compensate dependence scores prior qualitative explanations 
example overfitting oversearching attribute selection errors attributed fluke relationships 
statistical properties explain frequency indicate effective solutions 
cases previous lends support notion important influence credibility induced models 
example vapnik chervonenkis dimension minimum description length principle point number comparisons important factor overfitting 
explanation mechanism overfitting oversearching attribute selection errors enhanced looking related concepts overfitting avoidance bias bias variance tradeoff 
points elaborated 
large statistical literature examines effects multiple comparisons miller 
literature concerned experimental design design induction algorithms 
machine learning gascuel salzberg pursues course correctly noting effect multiple comparisons empirical evaluation learning algorithms 
induction algorithms explicitly compensate multiple comparisons 
kass jensen schmill bonferroni adjustment compensate multiple comparisons tree construction 
gaines uses bonferroni adjustment compensate comparing multiple rules 
jensen uses randomization tests aid aid cart machine learning submitted model complexity credibility compensate comparing multiple classification rules 
effects multiple comparisons led researchers reject statistical hypothesis tests entirely 
example early tree building algorithms completely dispense significance tests 
program authors morgan andrews baker morgan multiple comparisons render statistical significance tests useless 
similarly quinlan quinlan rejects conventional significance tests empirical grounds favor error pruning current approach despite infrequent statistical tests lack attention multiple comparisons qualitative explanations pathologies induction algorithms statistical 
explanations overfitting mingers frequently cite problem fitting models noise random variation 
noted explanations oversearching murthy salzberg quinlan cameron jones cite fluke models discovered extensive search 
explanations attribute selection errors increased likelihood finding spuriously high scores components variables possible discrete values mingers 
explanations qualitative fewer include theoretical proofs 
attempts provide theoretical basis avoiding pathologies particular overfitting focuses tradeoffs complexity accuracy model 
example algorithms explicitly consider complexity accuracy evaluating model components iba langley 
cost complexity pruning technique employed algorithm breiman attempts find near optimal complexity tree cross validation 
formal treatments consider model complexity way avoid overfitting 
treatment minimum description length mdl principle formally balances accuracy complexity quinlan rivest 
mdl characterizes datasets models number bits required encode 
total information dataset described sum information necessary encode model encode exceptions model remaining best model results smallest total description length data smallest sum model description description remaining data 
mdl applied avoid overfitting quinlan rivest attribute selection errors quinlan decision trees 
vapnik chervonenkis vc dimension links complexity overfitting 
characterizes relationship hypothesis space instance space blumer 
member distinguish possible dichotomy said shattered priori machine learning submitted overfitting avoidance bias vc dimension equal largest number instances shattered induction algorithm select member final model training sample smaller vc dimension possible achieve perfect classification relationship binary classification variable variables 
theory vc dimension compensates multiple comparisons provides little guidance construct realistic learning algorithms 
despite substantial body research complexity exists little theory complexity overfitting related 
notable exception pearl complexity credibility inferred models pearl 
pearl explains complexity related accuracy complexity final model related number intermediate models components compared construction 
comparing models turn overfitting 
pearl analysis shows complexity merely surrogate multiple comparisons 
pearl probable researchers understand complexity mere surrogate multiple comparisons easy confuse 
complexity poor indicator number comparisons 
algorithms search different proportions space possible components 
algorithms search exhaustively employ strong search biases 
construct models complexity vast differences number comparisons 
oversearching demonstrates precisely effect 
cases extensive search produces models accurate equally complex models produced extensive search 
second relationship complexity number comparisons depends number variables dataset contains variables algorithm evaluate thousands components order construct relatively simple final model 
contains variables algorithm evaluate far fewer components construct final model complexity 
final models constructed cases complexity resulted radically different numbers comparisons 
vc dimension mdl usually cast defining model complexity closely related number comparisons induction algorithm 
pearl insights vc dimension mdl principle point multiple comparisons important factor overfitting 
schaffer schaffer characterizes overfitting avoidance learning bias method preferring model appropriateness domain specific 
view extended extreme forms referred mcp mcp mcp machine learning submitted bias variance analysis bias errors variance errors law generalization performance free lunch nfl theorem schaffer wolpert 
holds gain accuracy obtained avoiding overfitting bias domain necessarily offset reduced accuracy domains 
course induction problems overfitting avoidance technique produce net gain accuracy 
theories highly controversial rest unrealistic assumptions estimates true accuracy exclude instances sample possible assignments class labels equally effectively making generalization impossible rao gordon spears 
regardless larger claims generalization accuracy overfitting avoidance bias particularly schaffer reveals important fact avoiding overfitting universally improve accuracy 
attempts avoid overfitting decrease accuracy new data situations 
schaffer little identify conditions lead situations 
contrast understanding statistical properties identifies overfitting attribute selection errors oversearching serious complementing schaffer 
analyses induction algorithms geman bienenstock doursat kohavi wolpert characterization prediction errors appeared originally statistics literature 
context linear regression total error defined sum intrinsic measurement error errors due factors bias variance 
stem systematic errors model 
regression typically arise incorrectly specified models models contain incorrect components contain components incorrect functional form 
stem random errors model 
regression typically arise errors parameter estimation incorrect estimates coefficients variables regression equation 
produce bias variance errors 
bias errors increase attribute selection errors oversearching 
example components decision tree systematically favored attributes node large number discrete values suboptimal components added model 
models suboptimal components incorrectly specified introducing bias errors 
variance errors increase overfitting 
example decision trees overly complex reduce number instances available leaf estimate correct label 
increase variance parameter estimates introducing variance errors 
bias variance analysis complements analysis characterizing errors introduced attribute selection errors overfitting oversearching 
machine learning submitted implications acknowledgments statistical properties multiple comparison procedures depend strongly number items compared 
statistical properties affect inferences induction algorithm generates tests models model components 
adjust algorithms add useless components models systematically prefer suboptimal models model components 
effects multiple comparisons statistical experiments known effects induction algorithms explored 
tried address gap theoretical proofs empirical demonstrations relate multiple comparisons common procedures inductive learning 
surveyed approaches adjusting multiple comparisons bonferroni adjustment cross validation randomization testing 
addition practical implications properties multiple comparisons provide single causal explanation phenomena widely observed induction algorithms overfitting attribute selection errors oversearching 
prior research documents situations pathologies occur provide quantitative causal explanation occur 
authors wish tim oates paul utgoff gunnar comments early draft 
research supported darpa rome laboratory contract 

government authorized reproduce distribute reprints governmental purposes notwithstanding copyright notation 
views contained authors interpreted necessarily representing official policies endorsements expressed implied defense advanced research projects agency rome laboratory government 
machine learning effects bias submitted proposition proof case case max max max max max max max max max max max max max max max max max max max max max max max max max max max considers subset items considered simplest case scores summed samples samples consider disjoint sets items 
consider disjoint sets random variables third set variables domains probability distributions 
know equation machine learning max max max max max submitted sample machine learning proposition proof submitted max max max max max max max influence independence maximum score max max continuous random variables values positively correlated 
positively correlated identically distributed independent identically distributed simple axioms probability inequality machine learning proposition proof max max max submitted influence standard error maximum score max max oe oe oe oe oe oe identically independently distributed see 
distributions know 
conditions proven appendix adding sides converting probabilities machine learning submitted max max max max max max max max max max pr pr pr pr pr pr pr pr pr pr pr pr pr pr pr pr pr pr pr pr pr pr pr pr pr pr adding negative sides converting probabilities similarly know 
conditions proven appendix prove 
special case max max max max max machine learning proposition proof submitted pr pr pr pr pr pr pr influence difference expected value maximum score max max identically independently distributed 
distributions adding sides converting probabilities subtracting sides converting probabilities machine learning submitted max max max max max max pr pr pr pr pr pr pr pr pr pr maximum expected value measure respect respect machine learning submitted proposition proof proposition proof probability relations prior proofs xy xy xy xy xy xy xy xy probabilities adding sides proposition proven values greater 
probabilities adding sides machine learning submitted journal association computing machinery classification regression trees working notes aaai spring symposium training issues incremental learning empirical methods artificial intelligence randomization tests public opinion quarterly proceedings tenth national conference artificial intelligence preliminary papers fifth international artificial intelligence statistics proceedings sixth international workshop machine learning ecai proceedings th european conference artificial intelligence neural computation proceedings fifth international conference machine learning proceedings third international conference knowledge discovery data mining proceedings knowledge discovery databases workshop blumer ehrenfeucht haussler warmuth 
learnability vapnik chervonenkis dimension 

breiman friedman olshen stone 
wadsworth international 
brodley rissland 
measuring concept change 

cohen 
cambridge mit press 

marcel dekker 

behavioral sciences 

fayyad irani 
attribute selection problem decision tree generation 


method learns data 
methodological issues analysis comparative studies 
gaines 
knowledge worth ton data quantitative studies trade expertise data statistically founded empirical induction 

gascuel 
statistical significance inductive learning 
neumann ed 
chichester wiley 
geman bienenstock doursat 
neural networks bias variance dilemma 

iba langley 
trading simplicity coverage incremental concept learning 

jensen schmill 
adjusting multiple comparisons decision tree pruning 
jensen 
knowledge discovery induction randomization testing 

machine learning submitted induction randomization testing decision oriented analysis large data sets applied statistics proceedings thirteenth international conference machine learning icml proceedings fourteenth joint conference artificial intelligence machine learning simultaneous statistical inferences machine learning machine learning public opinion quarterly proceedings fourteenth international joint conference artificial intelligence computer intensive methods testing hypotheses proceedings fourteenth international conference machine learning international journal general systems proceedings fourteenth international joint conference artificial intelligence information computation machine learning international journal man machine studies jensen 
ph dissertation washington university 
kass 
exploratory technique investigating large quantities categorical data 

kohavi wolpert 
bias plus variance decomposition zero loss functions 
kohavi 
study cross validation bootstrap accuracy estimation model selection 
liu white 
importance attribute selection measures decision tree induction 

miller 
new york springerverlag 
mingers 
empirical comparison pruning methods decision tree induction 

mingers 
empirical comparison selection measures induction 

morgan andrews 
comment behavior sciences 

murthy salzberg 
lookahead pathology decision tree induction 


new york ny john wiley sons oates jensen 
effects training set size decision tree complexity 

pearl 
connection complexity credibility inferred models 

quinlan cameron jones 
oversearching layered search empirical learning 

quinlan rivest 
inferring decision trees minimum description length principle 

quinlan 
induction decision trees 

quinlan 
simplifying decision trees 

machine learning submitted machine intelligence journal artificial intelligence research proceedings th international conference machine learning course probability data mining knowledge discovery machine learning proceedings eleventh international conference machine learning searching structure alias aid iii approach analysis substantial bodies micro data documentation computer program successor automatic interaction detector program computer systems learn artificial intelligence review complex systems quinlan 
decision trees multi valued attributes 

quinlan 
improved continuous attributes 

rao gordon spears 
generalization action really equal opposite reaction 
analysis conservation law generalization performance 

ross 
macmillan 
salzberg 
comparing classifiers pitfalls avoid recommended approach 
appear 
schaffer 
overfitting avoidance bias 

schaffer 
conservation law generalization performance 

baker morgan 
survey research center institute social research university michigan ann arbor michigan 
weiss kulikowski 
san mateo ca morgan kaufmann publishers white liu 
learning induction 

wolpert 
connection sample testing generalization error 

wolpert 
training set error priori distinctions learning algorithms 
technical report santa fe institute santa fe nm 
