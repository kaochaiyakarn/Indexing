machine learning proceedings fourteenth international conference 
boosting margin new explanation effectiveness voting methods robert schapire yoav freund labs mountain avenue murray hill nj usa research att com peter bartlett dept systems engineering aust 
national university canberra act australia peter bartlett anu edu au wee sun lee electrical engineering department university college unsw australian defence force academy canberra act australia lee ee oz au 
surprising recurring phenomena observed experiments boosting test error generated hypothesis usually increase size large observed decrease training error reaches zero 
show phenomenon related distribution margins training examples respect generated voting classification rule margin example simply difference number correct votes maximum number votes received incorrect label 
show techniques analysis vapnik support vector classifiers neural networks small weights applied voting methods relate margin distribution test error 
show theoretically experimentally boosting especially effective increasing margins training examples 
compare explanation bias variance decomposition 
years growing interest learning algorithms achieve high accuracy voting predictions classifiers 
example researchers reported significant improvements performance decision tree learning algorithms cart voting methods :10.1.1.133.1040
refer hypotheses combined vote base hypothesis final voted hypothesis combined hypothesis 
examples effectiveness methods consider results experiments letter dataset 
experiment breiman bagging method top 
reran times random bootstrap subsamples combined computed trees simple voting 
left labs planning move murray hill 
new address park avenue florham park nj 
non synthetic datasets research part statlog database retrieved electronically uci repository www ics uci edu mlearn mlrepository html 
shown training test error curves lower upper curves respectively combined hypothesis function number trees combined 
test error dataset run just 
test error bagging trees significant improvement 
error rates indicated horizontal grid lines 
second experiment freund schapire adaboost algorithm top dataset :10.1.1.32.8918
algorithm similar bagging subsamples chosen manner concentrates hardest examples 
results experiment shown 
note boosting drives test error just 
error curves reveal remarkable phenomenon observed drucker cortes quinlan breiman 
ordinarily hypotheses complex expect generalization error eventually degrade 
curves reveal test error increase method trees combined point combined hypothesis involves decision tree nodes 
complex hypotheses low error rates 
especially surprising boosting new decision tree trained specialized subsample training set 
apparent paradox revealed error curve adaboost 
just trees combined training error combined hypothesis dropped zero test error continues drop round round 
surely combination trees simpler combination trees perform equally training set perfectly fact 
larger complex combined hypothesis performs better test set 
training error combined hypothesis reaches zero adaboost continues obtain new base hypotheses training base learning algorithm different subsamples data 
combined hypothesis continues evolve training error reaches zero 
see section detail 
bagging boosting bagging boosting error curves margin distribution graphs bagging boosting letter dataset 
results experiments contradict occam razor fundamental principles theory machine learning 
principle states order achieve test error hypothesis simple possible difference training error test error increase number parameters needed describe hypothesis increases 
analysis boosting applied bagging carried freund schapire methods baum haussler 
analysis predicts test error eventually increase number base hypotheses combined increases 
prediction clearly incorrect case experiments described pointed quinlan breiman 
apparent contradiction especially stark boosting experiment test error continues decrease training error reached zero 
breiman argued voting methods primarily reducing variance learning algorithm 
explanation useful bagging bagging tends effective variance large 
boosting explanation best incomplete 
seen section large variance base hypotheses requirement boosting effective 
cases boosting increases variance reducing generalization error 
alternative theoretical analysis voting methods applicable instance bagging boosting arcing 
approach similar result bartlett different context :10.1.1.57.2302
prove rigorous upper bounds generalization error voting methods terms measure performance combined hypothesis training set 
bounds depend number training examples complexity base hypotheses depend explicitly number base hypotheses 
explaining shape observed learning curves analysis may helpful understanding algorithms fail succeed may lead design effective voting methods 
key idea analysis 
order analyze generalization error consider just training error number incorrect classifications training set 
take account confidence classifications 
measure classification confidence possible prove improvement measure confidence training set guarantees improvement upper bound generalization error 
consider combined hypothesis prediction result vote weighted vote set base hypotheses 
suppose weights assigned different base hypotheses normalized sum 
fixing attention particular example refer sum weights base hypotheses predict particular label weight label 
define classification margin example difference weight assigned correct label maximal weight assigned single incorrect label 
easy see margin number range gamma example classified correctly margin positive 
large positive margin interpreted confident correct classification 
consider distribution margin set training examples 
visualize distribution plot fraction examples margin function gamma 
refer graphs margin distribution graphs 
right side show margin distribution graphs correspond experiments described 
graphs show margin distributions bagging boosting iterations indicated short dashed long dashed hidden solid curves respectively 
main observation boosting bagging tend increase margins associated examples converge margin distribution examples large margins 
boosting especially aggressive effect examples initial margin small 
training error remains unchanged zero round margin distribution graph changes quite significantly iterations examples margin larger 
comparison round examples margin 
see experiments detailed type reduction fraction training examples small margin predictor improvements test error 
idea maximizing margin improve generalization error classifier previously suggested studied vapnik led cortes support vector classifiers boser guyon optimal margin classifiers 
section discuss relation vapnik greater detail 
shawe taylor gave bounds generalization error classifiers terms margins bartlett related techniques give similar bound neural networks small weights :10.1.1.57.2302
voting classifiers special case neural networks immediate consequence bartlett result bound generalization error voting classifier terms fraction training examples small margin 
section similar simpler approach give slightly better bound 
give main intuition proof 
idea brings back occam razor indirect way 
recall example classified correctly margin positive 
example classified large margin positive negative small changes weights majority vote change label 
examples large margin classification error original majority vote perturbed majority vote similar 
suppose small set weighted majority rules fixed ahead time called approximating set 
way perturbing weights hypothesis majority vote find nearby rule approximating set 
approximating set small guarantee error approximating rule training set similar generalization error error similar original rule generalization error original rule small 
back occam razor argument arguing classification rule simple argue rule close simple rule 
boosting particularly finding hypotheses large margins concentrates examples margins small negative forces base learning algorithm generate classifications examples 
process continues training error reached zero explains continuing drop test error 
section show powerful effect boosting margin merely empirical observation fact result provable property algorithm 
specifically able prove upper bounds number training examples particular margin terms training errors hypotheses 
certain reasonable conditions bounds imply number training examples small margin drops exponentially fast number base hypotheses 
section give examples margin distribution graphs datasets base learning algorithms combination methods 
section discuss relation decompositions section compare vapnik optimal margin classifiers 
generalization error function margin distributions section prove achieving large margin training set results improved bound generalization error 
bound depend number hypotheses combined vote 
approach take similar shawe taylor bartlett proof simpler direct :10.1.1.57.2302
slightly weaker version theorem special case bartlett main result 
give proof special case just possible labels gamma 
proof generalized larger finite sets labels 
denote space base hypotheses chosen example cart space decision trees 
base hypothesis mapping instance space gamma 
assume examples generated independently random fixed unknown distribution theta gamma 
training set list pairs xm ym chosen denote probability event example chosen denote probability respect choosing example uniformly random training set 
clear context abbreviate pd 
ed denote expected value similar manner 
define convex hull set mappings generated weighted average hypotheses 
fi fi fi fi fi understood finitely may nonzero 
majority vote rule associated gives wrong prediction example yf 
margin example case simply yf 
theorem main result section states high probability generalization error majority vote hypothesis bounded terms number training examples margin threshold plus additional term depends number training examples complexity measure threshold preventing choosing close zero 
space constraints prove theorem case base hypothesis space finite set decision trees size set discrete valued features 
case bound depends finite support requirement proof sufficient application majority votes finite number base hypotheses 
log jhj roughly description length hypothesis means tolerate large hypothesis classes 
infinite class decision trees continuous features state bound proof terms vc dimension note theorem applies majority vote hypothesis regardless computed 
theorem applies voting method including boosting bagging theorem sample examples chosen independently random assume base hypothesis space finite ffi 
probability gamma ffi random choice training set weighted average function satisfies bound pd theta yf ps theta yf log log jhj log ffi generally finite infinite vc dimension bound holds pd theta yf ps theta yf log log ffi proof prove case finite 
sake proof define cn set unweighted averages elements cn 
jh allow appear multiple times sum 
set play role approximating set proof 
majority vote hypothesis associated distribution defined coefficients choosing elements independently random distribution generate element cn construction map distribution cn goal upper bound generalization error cn separate probability terms pd theta yf pd theta yg pd theta yg yf inequality holds cn take expected value right hand side respect distribution get pd theta yf pd gq theta yg pd gq theta yg yf gq theta pd theta yg ed theta gq theta yg yf bound terms separately starting second term 
consider fixed example take probability inside expectation respect random choice clear gq theta probability inside expectation equal probability average random draws distribution gamma different expected value 
chernoff bound yields gq theta yg yf gamman upper bound term union bound 
probability choice exists cn pd theta yg ps theta yg ffl je gamma exponential term comes chernoff bound holds single choice 
term jc upper bound number choices fact form functions cn need consider values form note jc jhj set ffl ln jhj ffi take expectation respect get probability gamma ffi pd gq theta yg gq theta yg ffl choice distribution finish argument relate fraction training set yg fraction yf quantity measure 
notice gq theta yg theta yf es theta gq theta yg yf bound expression inside expectation chernoff bound equation get gq theta yg yf gamman ffi ffi probability failure ffi ffi combining equations get probability gamma ffi pd theta yf theta yf gamman ln jhj ffi statement part theorem follows setting sigma ln ln jhj upsilon extend proof general case may infinite vc dimension log jhj 
sketch alternative general approach applied class real valued functions 
approximating class cn proof theorem central approach 
refer approximating class sloppy cover 
formally class real valued functions training set length positive real numbers ffl say function class ffl sloppy cover respect exists xs gamma ffl 
ffl denote maximum training sets size size smallest ffl sloppy cover respect standard techniques allow prove probability random choice exists pd theta yf theta yf ffl ffl exp gammaffl 
proof essentially identical theorem bartlett :10.1.1.57.2302
second part theorem proved constructing sloppy cover probabilistic argument proof choosing element cn randomly sampling functions addition result leads slight improvement log factors main result bartlett gives bounds generalization error neural networks real outputs terms size network weights margin distribution :10.1.1.57.2302
effect boosting margin distributions give theoretical evidence freund schapire adaboost algorithm especially suited task maximizing number training examples large margin 
space limitations permit terse review algorithm 
adopt notation previous section restrict attention binary case 
boosting works sequentially rerunning base learning algorithm time different distribution training examples 
round distribution computed training examples formally set indices mg 
goal base learning algorithm find hypothesis small error ffl id theta distribution adaboost initially uniform updated multiplicatively round exp gammay ff ff ln gamma ffl ffl normalization constant chosen sums 
verified case ffl gamma ffl 
final combined hypothesis weighted majority vote base hypotheses sign ff ff note round adaboost places weight examples gamma ff smallest 
quantity exactly margin combined hypothesis computed point 
freund schapire prove training error rates base hypotheses bounded ffl gamma fl fl training error combined hypothesis decreases exponentially fast number base hypotheses combined 
training error equal fraction examples yf 
simple matter extend proof show large fraction training examples yf decreases zero exponentially fast number base hypotheses boosting iterations 
theorem suppose base learning algorithm called adaboost generates hypotheses weighted training errors ffl ffl theta yf ffl gamma gamma ffl proof sketch note yf ff ff exp gammay ff ff theta yf exp gammay ff ff exp ff exp gammay ff exp ff dt equality follows definition plugging values ff gives theorem 
understand significance result assume ffl gamma fl fl 
simplify upper bound equation gamma fl gamma fl stumps boosting bagging ecoc boosting bagging ecoc error curves margin distribution graphs learning methods letter top satimage middle vehicle bottom datasets 
fl shown expression inside parentheses smaller probability yf decreases exponentially fast theorem applies binary classification problems freund schapire give extensive treatment multi class case 
results extended prove analogous theorems margin distributions general case 
margin distribution graphs shows series error curves margin distribution graphs variety datasets learning methods 
vertically split sections corresponding statlog datasets tested letter show fl known exponential decrease probability achieved slightly different boosting algorithm fl 
don know achieve improvement nontrivial lower bound gamma ffl known priori 
age vehicle 
came test sets 
vehicle dataset randomly selected half data held test set 
note curves represent single run algorithm 
addition bagging boosting variant dietterich bakiri method error correcting output codes viewed voting method 
carefully constructing error correcting codes simply random output codes highly similar properties 
base learning algorithm simple algorithm finding best binary split decision tree decision stump 
algorithm weak pseudoloss versions boosting bagging 
see freund schapire details :10.1.1.133.1040
explained learning curve figures shows training error bottom test error top curves 
indicated horizontal grid lines error rate base hypothesis run just error rate combined hypothesis iterations 
note log scale figures 
margin distribution graphs shown iterations indicated short dashed long dashed barely visible solid curves respectively 
interesting datasets learning algorithms tend produce margin distribution graphs roughly character 
noted boosting especially aggressive increasing margins examples willing suffer significant reductions margins examples large margins 
seen observe maximal margin final hypothesis bounded away 
contrast final graph bagging half examples margin 
graphs ecoc resemble shape boosting bagging tend lower margins 
note dataset boosting bagging eventually achieve perfect nearly perfect accuracy training sets generalization error boosting better 
explanation evident margin distribution graphs see boosting far fewer training examples margin close zero 
stumps boosting unable achieve large margins consistent theorem base hypotheses higher training errors 
presumably low margins adversely affect generalization error complexity decision stumps smaller full decision trees 
relation bias variance theory main explanations improvements achieved voting classifiers separating expected error classifier bias term variance term 
details definitions differ author author attempts capture quantities bias term measures persistent error learning algorithm words error remain infinite number independently trained hypotheses :10.1.1.57.5909:10.1.1.48.4661
variance term measures error due fluctuations part generating single hypothesis 
idea averaging hypotheses reduce variance term way reduce expected error 
section discuss strengths weakness bias variance theory explanation performance voting methods especially boosting 
bias variance decomposition classification 
origins bias variance analysis quadratic regression 
averaging independently trained regression functions increase expected error 
encouraging fact nicely reflected bias variance separation expected quadratic error 
bias variance nonnegative averaging decreases variance term changing bias term 
naturally hope beautiful analysis carry quadratic regression classification 
unfortunately observed majority vote classification rules result increase expected classification error 
simple observation suggests may inherently difficult impossible find decomposition classification natural satisfying quadratic regression case 
difficulty reflected myriad definitions proposed bias variance :10.1.1.57.5909:10.1.1.48.4661
discussing separately remainder section noted follow definitions kong dietterich referred definition breiman :10.1.1.57.5909
bagging variance reduction 
notion variance certainly helpful understanding bagging empirically bagging appears effective learning algorithms large variance 
fact idealized conditions variance definition amount decrease error effected bagging large number base hypotheses 
ideal situation bootstrap samples bagging faithfully approximate truly independent samples 
assumption fail hold practice case bagging may perform expected variance dominates error base learning algorithm 
happen data distribution simple 
example consider data generated distribution 
label gamma chosen uniformly random 
instance gamma chosen picking bits equal probability gammay probability 
coordinate independent noisy version base learner learning algorithm generates hypothesis equal single coordinate best predictor respect training set 
clear coordinate probability chosen hypothesis random training set aggregate predictor independently trained samples unweighted majority vote coordinates bayes optimal predictor case 
bias learning algorithm exactly zero 
prediction error majority rule roughly variance strongly dominates expected error rate 
favorable case hope bagging get close error bayes optimal predictor 
training set examples generalization error achieved bagging iterations 
results averaged runs 
reason poor performance random sample coordinates slightly correlated bagging tends pick coordinates 
case kong dietterich definitions breiman definitions stumps stumps error pseudo loss error error pseudo loss error name boost bag boost bag boost bag boost bag boost bag boost bag waveform bias var error twonorm bias var error bias var error ringnorm bias var gamma gamma error kong bias dietterich var error table bias variance experiments boosting bagging synthetic data :10.1.1.57.5909
columns labeled dash indicate base learning algorithm run just 
behavior bagging different expected behavior truly independent training sets 
boosting data achieved test error 
boosting variance reduction 
breiman argued boosting primarily variance reducing procedure 
evidence comes observed effectiveness boosting cart algorithms known empirically high variance 
error algorithms due variance surprising reduction error primarily due reduction variance 
experiments show boosting highly effective learning algorithms error tends dominated bias variance 
ran boosting bagging artificial datasets described breiman artificial problem studied kong dietterich :10.1.1.57.5909
previous authors training sets size problem 
base learning algorithm tested 
simple base learning algorithm essentially finds single node decision tree decision stump minimizes training error pseudoloss see freund schapire details :10.1.1.133.1040
estimated bias variance average error algorithms rerunning times 
experiments bias variance definitions kong dietterich proposed breiman :10.1.1.57.5909
multi class problems freund schapire tested error pseudoloss versions bagging boosting :10.1.1.133.1040
class problems error versions 
results summarized table 
clearly boosting doing reducing variance 
instance ringnorm boosting decreases error fact original goal boosting reduce error called weak learning algorithms tend large bias 
stump algorithm increases variance gamma kong dietterich definitions breiman definitions 
breiman tested boosting low variance base learning algorithm linear discriminant analysis lda attributed ineffectiveness boosting case stability low variance lda 
experiments fairly stable stump algorithm suggest stability may sufficient predict boosting failure 
theory specific rigorous pinpointing reasons boosting fail 
taken theorem theorem state boosting perform poorly insufficient training data relative complexity base hypotheses training errors base hypotheses ffl theorem large quickly 
relation vapnik maximal margin classifiers margins real valued hypotheses predict generalization error previously studied vapnik boser guyon cortes optimal margin classifiers 
method boosting aims find linear combination base hypotheses maximizes margins training data 
methods differ assumptions norms base hypotheses weights 
optimal margin method sum squared outputs base hypotheses sum squared weights squared norms assumed bounded 
boosting maximum absolute value base hypotheses norm sum absolute values weights norm assumed bounded 
assumptions mean boosting particularly suited case base hypotheses similar output range gamma 
hand optimal margin method suitable case magnitudes base hypotheses decay number increases dual spaces vapnik 
related optimal margin method uses quadratic programming optimization boosting algorithm seen method approximate linear programming 
vapnik showed constraint set points set normalized linear combinations points margin fl points vc dimension decreases fl result implies bounds generalization error terms expected margin test points typically known 
shawe taylor techniques theory learning real valued functions give bounds generalization error terms margins training examples linear combinations bound coefficients 
shawe taylor gave related results arbitrary real classes bartlett gave bounds type neural networks bounded weights :10.1.1.33.8995:10.1.1.57.2302
boosting neural network learning algorithms attempt find functions large margins 
vapnik gave alternative analysis optimal margin classifiers number support vectors number examples define final hypothesis 
analysis preferable analysis depends size margin training examples support vectors 
previous suggested boosting method selecting small number informative examples training set 
investigating relevance type bound applying boosting real world problems interesting open research direction 
open problems methods allow upper bound generalization error classifier simple statistics measured training data 
believe bounds give correct qualitative predictions regarding behavior generalization error 
unfortunately current form upper bounds pessimistic actual numerical estimates error quite useful practice 
important open problem derive careful precise bounds purpose 
paying closer attention constant factors analysis involve measurement sophisticated statistics 
acknowledgments leo breiman email exchange challenged think harder problems 
contributed datasets 
peter bartlett 
valid generalization size weights important size network 
advances neural information processing systems 
eric baum david haussler 
size net gives valid generalization 
neural information processing systems pages 
morgan kaufmann 
bernhard boser isabelle guyon vladimir vapnik 
training algorithm optimal margin classifiers 
proceedings fifth annual acm workshop computational learning theory pages 
leo breiman 
bagging predictors 
machine learning 
leo breiman 
bias variance arcing classifiers 
technical report statistics department university california berkeley 
corinna cortes vladimir vapnik 
support vector networks 
machine learning september 
thomas dietterich bakiri 
solving multiclass learning problems error correcting output codes 
journal artificial intelligence research january 
harris drucker corinna cortes 
boosting decision trees 
advances neural information processing systems pages 
yoav freund 
boosting weak learning algorithm majority 
information computation 
yoav freund robert schapire 
experiments new boosting algorithm 
machine learning proceedings thirteenth international conference pages 
yoav freund robert schapire 
game theory line prediction boosting 
proceedings ninth annual conferenceon computational pages 
yoav freund robert schapire 
decision theoretic generalization line learning application boosting 
journal computer system sciences appear 
extended appeared eurocolt 
ron kohavi david wolpert 
bias plus variance decomposition zero loss functions 
machine learning proceedings thirteenth international conference pages 
eun bae kong thomas dietterich :10.1.1.57.5909
error correcting output coding corrects bias variance 
proceedings twelfth international conference machine learning pages 
quinlan 
bagging boosting 
proceedings thirteenth national conference artificial intelligence pages 
john shawe taylor peter bartlett robert williamson martin anthony 
framework structural risk minimisation 
proceedings ninth annual conference computational learning theory pages 
john shawe taylor peter bartlett robert williamson martin anthony :10.1.1.33.8995
structural risk minimization hierarchies 
technical report nc tr neurocolt 
robert tibshirani 
bias variance prediction error classification rules 
technical report university toronto november 
vapnik 
estimation empirical data 
springer verlag 

