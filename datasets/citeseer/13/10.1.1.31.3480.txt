empirical model network traffic empirical model network traffic bruce mah cs berkeley edu tenet group computer science division university california berkeley berkeley ca tel fax workload global internet dominated hypertext transfer protocol application protocol world wide web clients servers 
simulation studies environment require model traffic patterns world wide web order investigate performance aspects increasingly popular application 
developed empirical model network traffic produced 
relying server client logs approach gathering packet traces network conversations 
traffic analysis determined statistics distributions higher level quantities size items retrieved number items web page think time user browsing behavior 
quantities form model simulations mimic world wide web network applications wide area ip internetworks 
keywords world wide web traffic model traffic measurements workload internet 
simulations long tool evaluation computer networks 
order simulations yield useful performance data require accurate models system study expected workload placed system 
particular workloads need capture various characteristics network applications 
number synthetic workloads constructed traditional types network traffic telnet remote logins ftp file transfers design deployment new applications requires development new traffic models 
new applications world wide web popular approach retrieving information global internet 
web hypertext transfer protocol associated protocol come strongly influence current state internet network traffic 
simulate various aspects network performance 
just nsfnet backbone transitioned new architecture april leading source network traffic backbone network measured number bytes number packets transferred nsfnet 
submitted infocom 
empirical model network traffic contemporary internet traffic workloads necessary able describe network usage rapidly growing application 
developed empirically derived model network traffic designed provide synthetic workload simulation wide area ip internetwork 
captures variety aspects world wide web network activity 
lowest level describes sizes individual web files set heuristics files classified multi file documents separated user think time 
highest level model describes browsing behavior users single web server different web servers 
model network packet traces uses analysis heuristics derive information files higher layer units information 
section provides brief background world wide web 
section describe measurement methodologies prior studies www internet applications discuss applicability describe actual approach measurement gathering section 
section describes various components model 
experimental results apply model section 
background world wide web frequently shortened www web collection documents services available global internet 
servers furnish documents request clients known browsers 
document referred page may consist number files 
example multipart document may consist text represented hypertext markup language html berners lee number images displayed inline text 
hypertext transfer protocol berners lee request response protocol designed transfer files making parts web documents 
transfer consists client requesting file server server replying requested file error notification 
request reply contain identification control information headers 
uses services tcp postel reliable transport unreliable global internet 
current versions tcp connection retrieval 
empirical model network traffic versions described fielding incorporate padmanabhan mogul propose reuse tcp connections multiple retrievals client server 
occasionally take liberties terminology 
strictly speaking web documents transferred means 
particular file transfer protocol ftp postel portions web example document archives deployed administrative reasons ftp servers exist 
terms web server server strictly synonymous frequently interchangeably 
usage terms web browser client document similar 
contexts differentiation required easily apparent 
prior section summarize approaches taken attempting characterize internet applications 
methods server logs client logs prior investigations world wide web 
approach traffic traces past studies number internet applications file transfers remote logins 
server logs web servers keep logs files served reasons ranging operational monitoring collecting demographic information users 
workload model created processing logs running web server 
sense approach easiest take machinery collecting model data exists fact data collected anyway 
studies mogul arlitt appropriate characterization stream requests arriving web server 
principal drawbacks approach 
important disadvantage server logs easily capture user access patterns multiple web servers 
particular may difficult determination locality user session 
shortcoming current server logs capture aspects overheads protocol headers 
empirical model network traffic client logs crovella cunha catledge relied data gathered instrumenting ncsa mosaic web browser mosaic log retrievals web user sessions :10.1.1.146.2304
instrumented systems public computing laboratories academic environments 
studies primarily concerned investigating various characteristics web accesses 
reasonable expect construct corresponding model suitable generating synthetic workload 
server logs approach captures user accesses multiple web servers quite 
addition allows characterization effects client side caching documents parts documents 
technique requires browsers able log requests availability source code web browser logging added 
source newer web browsers including popular netscape navigator netscape generally available 
addition supporting variety browsers may difficult modifications logging need 
packet traces method gathering workload data consists analyzing packet traces taken subnet carrying traffic typically ethernet broadcast style lan 
packet traces knowledge higher layer protocols traffic analysis yield model behavior original application 
approach number traffic studies paxson web 
stevens analyzes packets arriving server presents interesting statistics observations 
danzig describes library traffic models common circa internet applications designed inclusion network simulators requiring synthetic workloads 
paxson additionally describes analytic models derived traffic traces compact representation purely empirical models parameterized accurately reflect particular networks 

possible methodology point point link acting transit network opportunities common 
empirical model network traffic approach eliminates principal disadvantages previous methods mentioned 
introduces drawbacks 
models application level logs easily record higher level information specific files requested message types document types 
information principle gleaned packet trace involve considerable effort reconstructing contents tcp connection 
addition effects client caching documents different ascertain cache misses generate network traffic 
methodology chose packet trace approach model principally allowed capture behavior individual users able methodology popular currently deployed client netscape navigator 
approach lose higher level information actual files accessed felt characterization essential network workload model 
freely available tcpdump packet capture utility jacobson running dec alpha record packet headers shared mbps ethernet computer science division university california berkeley periods late 
procedure saved tcp ip headers packet small number payload bytes captured 
data saved disk line processing 
subnet examined stub network transit traffic dozen computer science division 
approximately hosts subnet majority desktop unix workstations principally single user 
user community consists primarily computer science graduate students 
statistics available relative popularity different web clients environment operational experience suggests prevalent netscape navigator netscape 
www servers subnet associated various research groups 
empirical model network traffic servers bind known tcp port 
looking tcp packets known port captured believe vast majority traffic 
table summarizes traffic traces gathered study 
traces collected part effort examine various types network traffic just traffic packet counts traces include packets attributable 
traffic trace collected packets 
streams packets extracted comprising connections originating clients local network 
complete packet loss figures traces record packet loss approximately packets november trace filtering isolate packets 
figures yield packet loss ratio 
similar packet capture experiments hardware network produced loss figures consistent trace 
model model traffic captures logically physically meaningful parameters web client behavior 
traffic traces described preceding section provide empirical probability distributions describing various components behavior 
distributions determine characteristics synthetic workload 
section various components model summarized table 
lowest level model deals individual transactions 
transfer consists single request reply pair messages 
common case client application sends request data 
study characteristics html documents indexed inktomi web crawler approximately documents surveyed accessed standard port woodruff 
start time time number packets tue sep thu sep wed oct thu oct wed nov thu nov mon nov sun nov table 
summary traffic traces 
empirical model network traffic server turn replies supplying data 
data transmitted single tcp connection 
quantities model request length reply length transfers 
glance may appropriate model network traffic concern number size interarrival times tcp segments 
note particular packet interarrival times governed tcp flow control congestion control algorithms 
algorithms depend part latency effective bandwidth path client server 
information known priori conclude accurate packet level network simulation depend simulation actual tcp algorithms 
fact approach taken types tcp bulk transfers traffic model described danzig 
web documents employ model document consist multiple files 
server client may need employ multiple transactions requires distinct tcp connection transfer single document 
example document consist html text berners lee turn specify images displayed inline body document 
document require tcp connections serving request reply 
higher level behavior individual files naturally web document characterized terms number files needed represent document 

section show appropriate model request reply lengths transfer web page separately remaining retrievals page 
simplicity postponed discussion distinction 
quantity units description request length bytes request length reply length bytes reply length document size files number files document think time seconds interval retrieval successive documents consecutive document retrievals pages number consecutive documents retrieved server server selection server relative popularity web server select succeeding server accessed table 
quantities modeled 
empirical model network traffic web page retrievals user generally considering action 
readily admit difficulty reliably characterizing user behavior due dependency various human factors scope study 
construct distribution user think time empirical observations 
assuming users tend access documents server consecutive page retrievals useful characterize locality different web pages 
define consecutive document retrievals distribution number consecutive pages user retrieve single web server moving new server result hyperlinks existing document selecting completely unrelated document 
server selection distribution defines relative popularity web server terms particular server accessed set consecutive document retrievals 
experimental results traffic traces subsequent analysis derived various probability distributions different components model 
distributions model consistent existing web measurement studies 
summarized interesting facets measurements table 
implicit component model additional assumption components web document tend come server 
request sizes show bimodal distribution 
reply sizes heavy tailed distribution tend larger request sizes 
simple heuristic timing group individual files documents 
number files document tends small documents required file transfers 
requests retrieve file multi file web document tend longer subsequent requests 
files retrieved file multi file web document tend larger subsequent files 
number consecutive documents retrieved server tends small 
visits server document space resulted fewer documents retrieved 
table 
selected measurement results 
empirical model network traffic anomalies cases noticed odd trends data indicated large number nearly identical web documents transferred periodic intervals 
example october trace showed number web page retrievals interarrival times minutes 
instances document transferred accounting approximately estimated transferred trace 
investigation determined documents came web server displayed real time images san francisco ca skyline 
page extension html caused client automatically reload document regular intervals updating picture minutes netscape 
periodic retrievals skewing data removed traces prior analysis 
request length requests sent client server 
typically specify file retrieve may provide information computation performed server 
contained request identifying information user client software request 
user bytes sent client server contained part request measure request sizes simply counting number bytes appropriate direction tcp connection 
statistics summarizing requests traces shown table 
may argued retrievals contribute traffic model occur real life nature model accurately capture correlations successive document retrievals web client 
model attempting characterize periodic web traffic explicitly account behavior 
sep oct nov nov number minimum size maximum size mean size median size table 
summary request lengths bytes 
empirical model network traffic cumulative distribution functions cdfs request size distributions shown 
reply sizes traces exhibited bimodal distribution large peak occurring bytes smaller kb 
believe requests correspond simple file retrievals may contain complex requests generated html forms 
insufficient information existing traces prove disprove hypothesis 
investigating require packet traces containing payload bytes packet 
reply length reply consists bytes sent server client 
typically reply contains html text multimedia data image audio clip display web client 
case error nonexistent file access denial reply contains error message 
requests identifying information included 
table summary replies recorded traces 
cdfs reply size distributions shown 
note maximum file sizes identical 
investigation replies generated downloads single large data archive file single web server operated local research group 

cumulative distribution functions request lengths 
cdf request length bytes september october november november empirical model network traffic traces minimum reply length short tens bytes 
replies represent errors modified responses modified conditional document retrieval requests 
actual length files may range tens bytes addition headers reply messages somewhat longer 
note maximum reply sizes large mb traces 
means kb larger median reply sizes kb 
characteristics consistent distributions reply sizes heavy tailed large amount probability mass tail distribution 
fact demonstrated www file sizes heavy tailed crovella :10.1.1.146.2304
sep oct nov nov number minimum size maximum size mean size median table 
summary reply lengths bytes 

cumulative distribution functions reply lengths 
cdf reply length bytes september october november november empirical model network traffic assumption retrievals generally result transfer www file particular assumption large replies contain www files natural expect replies share characteristic 
repeated analysis crovella data distributions reply sizes kb reasonably modeled pareto distributions estimates ranging details table :10.1.1.146.2304
comparison crovella arrived estimate page length determining number files page straightforward :10.1.1.146.2304
way determine exactly tcp connections transferred parts single document 
client merely issues requests files making document succession 
simple heuristics determine connections belong document 
connections originate ip address retrievals different client machines possibly belong document 
note possible connections ip address associated unrelated documents happen case different users fetching document time 
evaluate possibility remote environment hosts workstations exclusively single user 
second connections separated time interval determined parameter call formally connections 
arrival time starting packet 
pareto distribution heavy tailed probability distribution cdf minimum value sep oct nov nov table 
estimates parameter tail reply size distributions 
coefficient determination takes values range values near indicate fit regression simple linear regression estimate account nearly variation 
thresh empirical model network traffic connection arrival time packet connection assuming judge belong document connections overlap judge respective files belong document 
condition occur browsers multiple overlapping tcp connections improve interactive performance netscape navigator 
illustrates role determining relation connections 
heuristic requires course definition suitable value short may smaller time necessary client initiate retrievals files belonging page 
case connections really belong web document falsely classified belonging different documents 
conversely large may longer time user react displayed document select new document view 
files different pages appear part document 
thresh thresh thresh thresh thresh 
heuristic determining relation connections 
timelines run right tcp connections represented thick arrows 
top timeline starts time judge belong document 
center timeline gap greater belong different documents 
bottom document starts finishes case judged belong document 
thresh thresh time time time thresh thresh thresh empirical model network traffic analysis crovella required similar classification order analyze distribution idle times connections :10.1.1.146.2304
analysis classified files separated second idle time belonging document due limitations users reaction time 
idle times greater seconds deemed separate independent documents items take longer processed displayed 
idle times intermediate range assumed belong transition region 
reasoning reasonable values range picked study 
primary influence choice value users generally take longer second react display new page order new document retrieval 
clients perform multiple overlapping file transfers time process display significant influence choice various components multipart document downloaded processed displayed parallel 
choice idle threshold characterize number files document shown table 
note survey html documents bray slightly half pages contained zero inlined image corresponding connections document 
considering documents single file single connection downloads tend skew distribution downward feel observations consistent statistic shown cumulative distribution function 
note distribution number files document vary changes distribution similar values exact choice critical analysis 
illustrates fact graphically set connections recorded september trace 
sep oct nov nov mean median table 
mean median number files document thresh sec thresh sec thresh sec thresh thresh sec thresh thresh sec thresh empirical model network traffic primary secondary retrievals classifying files belonging different pages partition retrievals classes 
term primary retrievals consists file document 
typically reply primary retrieval consist html text reply consist image data file downloaded error message 
class retrievals called secondary retrievals consists transactions needed transfer remaining files document document primary retrieval 
inlined images referenced primary file consisting html text currently known example data transferred secondary retrievals 
find sizes requests replies slightly different primary secondary retrievals 
table summarizes sizes primary secondary requests traces seen average maximum sizes primary requests larger secondary requests 
tendency illustrated example shows cdf primary secondary request lengths september trace 

cumulative distribution functions document length files september 
curves correspond varying values seconds 
thresh cdf files document thresh thresh thresh thresh thresh thresh thresh thresh thresh thresh sec empirical model network traffic table statistics primary secondary reply sizes table shows results fitting reply sizes pareto distributions 
find tails primary reply sizes secondary reply sizes 
estimates parameter pareto distribution lower primary replies secondary replies datasets 
distinguish differences primarily secondary request sizes computed confidence intervals estimates determined datasets corresponding parameters primary sep oct nov nov primary number minimum size maximum size mean size median size secondary number minimum size maximum size mean size median size table 
primary secondary request sizes bytes 

cumulative distribution functions primary secondary request lengths september 
cdf request length bytes primary secondary thresh sec empirical model network traffic secondary reply sizes significantly different confidence level 
believe differences sizes primary secondary retrievals due dissimilar types data transferred 
particular note arbitrary files downloaded web servers transferred primary transfers html text files 
secondary transfers consist exclusively inlined images 
analysis conclude appropriate model types retrievals differently 
sep oct nov nov primary number minimum size maximum size mean size median size secondary number minimum size maximum size mean size median size table 
primary secondary reply sizes bytes 

cumulative distribution functions primary secondary reply lengths september 
cdf reply length bytes primary secondary empirical model network traffic user think time selection empirical distribution user think times pages characterized set interconnection idle times table summarize user think times extracted web traces 
november trace longer mean think time 
believe fact due timing particular trace covered american holiday late november 
university california observes holiday day weekend conceivably account long idle times 
cdfs user think time distributions distributions 
sep oct nov nov primary estimate confidence interval secondary estimate confidence interval table 
estimates tail primary secondary reply size distributions 
thresh thresh 
cumulative distribution functions user think times 
cdf user think time seconds september october november november empirical model network traffic consecutive document retrievals current design web document archives users frequently access documents server succession 
fact may important example network systems rely locality allocating virtual circuits network resources mah 
table summarizes number consecutive document retrievals servers network traces 
contrast catledge noted users accessed average consecutive pages server considerably average document retrievals observed 
believe difference attributable interaction user browsing strategies client caching web browsers 
users tend browsing strategy described spoke hub involves frequent backtracking visited pages 
browsers implement client side caching revisited pages generate network traffic appear network trace counted client side event trace 
expect consecutive document retrieval count somewhat lower corresponding client access trace half observed 
show cdf consecutive document retrievals distribution traces 
seen users tend switch servers fairly frequently median number consecutive documents retrieved sep oct nov nov number maximum time mean time median time table 
user think times seconds 
sep oct nov nov number maximum documents mean documents median documents table 
consecutive document retrievals server access 
empirical model network traffic approximately 
noted cases users continued access single web server tens consecutive documents 
server selection server selection distribution characterizes relative popularity web servers strings documents retrieved 
computed number times web server set consecutive document retrievals 
table summarize popular servers consecutive document retrievals september trace total servers accessed start consecutive document retrievals 
popular server trace traces local departmental web server 
items interest contains homepages vast majority users machines attached network traced 
top servers metric located site 
characteristics particular fact servers accessed local tracing site believe insufficient information properly characterize aspect model existing data 
chosen approximate server selection distribution zipf law distribution 
zipf law discrete heavy tailed distribution states probability selecting popular item 
cumulative distribution functions consecutive document retrievals 
cdf consecutive documents retrieved september october november november ith empirical model network traffic set proportional originally describe frequency words texts phenomena zipf 
distribution applied frequency www documents accessed crovella arlitt 
reasonable apply zipf law heavy tailed distribution access patterns servers confirmation assertion requires larger data sample available 
note difficulties attempting gauge popularity web servers ip layer packet traces 
problem ip layer packet traces reveal exact hostname originally access documents ip address server 
hostnames obtained performing queries nameservers return canonical names hosts aliases 
may difficult impossible determine example machine canonical name kohler cs berkeley edu frequently accessed cs berkeley edu 
related problem case hostname maps multiple ip addresses may difficult associate accesses various ip addresses single name 
particular situation may arise case replicated servers rely randomization domain name system spread accesses single web server multiple machines described katz 
rank frequency type local local remote remote local remote remote local remote remote table 
top servers observed september 
frequency column shows number times server accessed start stream consecutive document retrievals 
type server reflects located locally site 
ian empirical model network traffic model representation choosing representation various probability distributions making traffic model basic approaches 
attempt fit observed data probability distributions easily described analytically 
simple analytic representation advantage compact easier analysis 
approach discussed paxson fact performed rudimentary curve fitting analyzing tails data samples discussed section section 
circumstances data set described known distribution bimodal request size distributions discussed section technique easily 
alternative represent probability distributions cdfs inverse transformation method example described jain applied danzig 
requiring storage slower generating random values approach virtue able represent arbitrary probability distributions 
due flexibility representation chose maintain cdf representation distributions zipf law substitute server selection distribution calculated analytically 
distributions traffic gathered september trace 
network simulator model need simulate activity clients servers network 
behavior simple web browser illustrated pseudo code algorithm simulating single threaded web server shown 
earlier version model implemented incorporated network simulator discrete event simulator investigating performance ip atm designs mah 
simulation complex applications web browsers perform multiple concurrent retrievals multi threaded web servers analogous 
emphasize meaningful internet simulation actual request reply data regulated tcp congestion control flow control mechanisms included part model 
empirical model network traffic select server number documents retrieve server server retrieve documents succession primary retrieval document send reply receive secondary retrievals document send reply receive wait user think wait 
pseudo code simple client 
get request client request receive length reply request type request primary send 
pseudo code simple server 
empirical model network traffic cient network applications simply transmit data network approach accurately model timing packets transmitted 
constructed empirical model network traffic produced hypertext transfer protocol world wide web applications 
model consists number probability distributions determined analysis actual conversations 
packet traces built higher layers communication patterns individual retrievals web pages groups pages 
approach gives sufficient level detail serve component workload generator packet level simulation ip internetwork carry web traffic 
characterization www generated network traffic shown requests exhibit bimodal distribution revealed prior studies sizes replies heavy tailed distribution 
shown simple heuristic separate transfers different web pages differences subsequent transfers multi file web document statistically significant 
characterized aspects user web page selection terms locality consecutive documents referenced 
possible compared results measurements analysis web measurement studies consistent prior results 
course areas model refined list topics possible 
feel zipf law substitute server selection distribution replaced empirical distribution adequately long trace network data 
desirable investigate correlations different components model example may correlation popularity server number consecutive documents fetched 
protocol developments persistent connection may require new measurement analysis methodologies 
conversion empirical dis empirical model network traffic tributions closed form analytic expressions aid making models adaptable data workload different types user communities document archives 
availability subset empirical probability distributions gathered study classes easily generate values distributions available cs berkeley edu software acknowledgments author gratefully acknowledges comments suggestions kimberly keeton venkata padmanabhan computer science division university california berkeley 
funded bell labs national research initiatives doe de fg er digital equipment hitachi international computer science institute mitsubishi electric research laboratories nsf arpa ncr pacific bell 
arlitt martin arlitt carey williamson 
web server workload characterization search invariants 
proceedings acm sigmetrics conference measurement modeling computer systems pages philadelphia pa may 
berners lee tim berners lee daniel connolly 
hypertext markup language 
internet request comments november 
berners lee tim berners lee roy fielding henrik frystyk nielsen 
hypertext transfer protocol 
internet draft draft ietf spec february 
draft progress valid maximum months publication date 
bray tim bray 
measuring web 
proceedings fifth international world wide web conference paris france may 
peter danzig sugih jamin danny mitzel 
characteristics wide area tcp ip conversations 
proceedings acm sigcomm zurich switzerland september 
empirical model network traffic catledge lara catledge james pitkow 
characterizing browsing strategies worldwide web 
proceedings third international world wide web conference darmstadt germany april 
crovella mark crovella azer bestavros :10.1.1.146.2304
self similarity world wide web traffic evidence possible causes 
proceedings acm sigmetrics conference measurement modeling computer systems pages philadelphia pa may 
cunha carlos cunha azer bestavros mark crovella 
characteristics www client traces 
technical report bu cs computer science department boston university july 
danzig peter danzig sugih jamin 
tcplib library tcp internetwork traffic characteristics 
technical report usc cs computer science department university southern california los angeles ca 
fielding roy fielding jim gettys jeffrey mogul henrik frystyk nielsen tim 
hypertext transfer protocol 
internet draft draft ietf spec june 
draft progress valid maximum months publication date 
jacobson van jacobson craig leres steven mccanne 
tcpdump software version 
software available ftp ftp ee lbl gov tcpdump tar jain raj jain 
art computer systems performance analysis 
john wiley sons new york ny 
katz eric dean katz michelle butler robert mcgrath 
scalable server ncsa prototype 
proceedings international www conference geneva switzerland may 
mah bruce mah 
quality service ip atm 
technical report csd university california berkeley september 
mah bruce mah 
users manual 
computer science division university california berkeley may 
mogul jeffrey mogul 
case persistent connection 
proceedings acm sigcomm pages cambridge ma august 
mosaic ncsa mosaic software version 
software available www ncsa uiuc edu sdg software 
netscape netscape navigator software version 
software available home netscape com 
nsfnet nsfnet backbone traffic distribution service april 
document available ftp nic merit edu nsfnet statistics nsf ports gz 
padmanabhan venkata padmanabhan jeffrey mogul 
improving latency 
proceedings second international world wide web conference chicago il october 
empirical model network traffic paxson vern paxson 
measurements wide area tcp conversations 
masters report university california berkeley may 
paxson vern paxson 
derived analytic models wide area tcp connections 
ieee acm transactions networking august 
postel jon postel 
transmission control protocol 
internet request comments september 
postel jon postel joyce reynolds 
file transfer protocol ftp 
internet request comments october 
stevens richard stevens 
tcp ip illustrated volume 
addison wesley publishing reading ma 
woodruff allison woodruff paul aoki eric brewer paul gauthier lawrence rowe 
investigation documents world wide web 
proceedings fifth international world wide web conference paris france may 
zipf george zipf 
human behavior principle effort 
hafner publishing new york ny 
