proceedings th international joint conference arti cial intelligence edited john mylopoulos ray reiter morgan kaufmann san mateo ca 
holographic reduced representations convolution algebra compositional distributed representations solution problem representing compositional structure distributed representations described 
method uses circular convolution associate items represented vectors 
arbitrary variable bindings short sequences various lengths frames reduced representations compressed xed width vector 
representations items right constructing compositional structures 
noisy reconstructions convolution memories cleaned separate associative memory reconstructive properties 
distributed representations hinton attractive reasons 
er concepts continuous space degrade gracefully noise processed parallel network simple processing elements 
problem representing compositional structure distributed representations time prominent concern followers critics connectionist faith fodor pylyshyn hinton connectionist networks back propagation nets hop eld nets boltzmann machines willshaw nets easy represent associations xed number items 
di culty representing compositional structure networks items associations represented di erent spaces 
hinton discusses problem proposes framework reduced descriptions 
framework requires compressed reduced single vector size original vectors 
vector acts reduced description set vectors member set vectors 
reduction reversible move directions part hierarchy 
compositional structure represented 
hinton suggest concrete way performing reduction mapping 
researchers built models designed frame tony plate department computer science university toronto ontario canada tap ai utoronto ca works compositional structure distributed representations 
examples see papers touretzky pollack smolensky aij 
propose new method representing compositional structure distributed representations 
circular convolution construct associations vectors 
representation association vector dimensionality vectors associated 
allows construction representations objects compositional structure 
call holographic reduced representations hrrs convolution correlation memories closely related holographic storage provide implementation hinton reduced descriptions 
describe hrrs error correcting associative item memories build distributed connectionist systems manipulate complex structures 
item memories necessary clean noisy items extracted convolution representations 
associative memories associative memories store associations items represented distributed fashion vectors 
nearly associative memory concerned storing items pairs items 
convolution correlation memories referred holographic matrix memories regarded alternate methods implementing associative memory willshaw murdock pike 
matrix memories received interest probably due relative simplicity higher capacity terms dimensionality vectors associated 
properties matrix memories understood 
best known matrix memories willshaw networks willshaw hop eld networks hop eld 
matrix memories construct auto associative content addressable memories pattern correction completion 
represent associations vectors 
associated cue retrieve 
operations associative memories encoding decoding trace composition 
encoding operation takes item vectors produces memory trace vector matrix 
decoding operation takes memory trace single item cue produces item originally associated cue noisy version thereof 
memory traces composed addition superposition 
decoding operation sum individual traces retrieved items may noisier 
models encoding decoding linear murdock decoding non linear hop eld operations nonlinear willshaw 
illustrate ibe space vectors representing items space vectors matrices representing memory traces 

encoding operation 
decoding operation 
trace composition operation 
item vectors memory traces 
association items represented trace recover decoding operation cue gives noisy version 
noisy versions cues 
depending properties particular scheme retrieved vector similar trace represent anumber associations item pair cue recover item pair gives noisy version recovered vector increases number associations stored single memory trace 
number associations represented usefully single trace usually referred capacity memory model 
matrix memories encoding operation outer product convolution memories encoding operation convolution 
addition superposition trace composition operation matrix convolution memories 
convolution correlation memories nearly convolution memory models aperiodic convolution operation form associations 
aperiodic convolution vectors elements results vector elements 
result convolved vector recursive convolution vector elements result elements 
resulting vectors grow recursive convolution 
growing property exhibited dramatic form matrix memories smolensky tensor product representations 
researchers solutions problem growth recursive associations limit depth composition smolensky discard elements usually distributional constraints elements vectors elements drawn independent distributions 
exception non linear willshaw rst published 
outside central ones metcalfe nite vectors murdock 
growth problem avoided entirely circular convolution operation known signal processing 
result circular convolution vectors elements just elements 
circular convolution growth property recursively connectionist systems xed width vectors 
close relationship matrix convolution memories 
convolution vectors circular aperiodic regarded compression outer product vectors 
compression achieved summing outer product 
circular convolution denoted illustrated 
small circles represents product pair elements summed indicated diagonals 
circular convolution operation straightforward remarkable circular correlation illustrated approximate inverse 
pair vectors convolved give memory trace member pair cue correlated trace produce member pair 
suppose cue vector correlation allows reconstruction noisy version correlation operation aperiodic circular versions approximate inverses respective convolution operations 
circular convolution de ned nr rn tj pn subscripts interpreted modulo gives operation circular nature 
tj ton subscripts modulo circular convolution represented compressed outer product 
circular correlation de ned pn kt convolution computed log time fast fourier transforms ffts 
method simple known fa fb discrete fourier transform range vector complex numbers inverse componentwise multiplication vectors 
certain conditions distribution elements vectors 
yj ton subscripts modulo circular correlation represented compressed outer product 
distributional constraints correlation decode convolution elements vector independently distributed mean zero variance euclidean length vector mean 
examples suitable distributions normal distribution discrete distribution values analysis signal strength capacity depends elements vectors independently distributed 
tension constraints need vectors meaningful features discussed plate 
information stored convolution trace numbers may strange pairs vectors stored vectors numbers 
reason vectors stored poor delity successfully store vector need store information discriminate vectors 
vectors represent di erent equiprobable items log bits information needed represent pairs items 
size vectors enter calculation matters 
addition memories simplest ways store set vectors add 
storage allow recall reconstruction stored items allow recognition determining particular item stored 
principle addition memory stated adding high dimensional vectors gives similar similar 
principle underlies convolution matrix memories sort analysis applied linear versions 
addition memories discussed greater length plate slightly log bits required pairs unordered 
applies degree elements vectors randomly independently distributed 
need reconstructive item memories system convolution representations sort recall opposed recognition additional error correcting associative item memory 
needed clean noisy vectors retrieved convolution traces 
reconstructive memory store items system produce 
input noisy version items output closest item indicate input close ofthe stored items 
example suppose system store pairs letters suppose letters represented random vectors item memory store vectors able output closest item input vector clean operation 
system shown 
trace sum convolved pairs system item input cue task output item cue associated trace 
output scalar value strength high input cue member pair low input cue member pair 
cue produce high strength 
cue give strength 
item outputs unimportant strength low 
trace input cue clean output item strength hetero associator machine 
convolution trace stores associations items item memory stores items 
item memory acts auto associator clean noisy items retrieved convolution trace 
exact method implementation item memory unimportant 
hop eld networks probably candidate low capacity 
kanerva networks kanerva su cient capacity store binary vectors 
experiments nearest neighbor matching memory 
representing complex structure pairs items easy represent associative memory convolution memory suited representation complex structure 
sequences sequences represented convolution encoding 
entire sequence repre assumes items represented real vectors convolution memories binary vectors willshaw 
sented memory trace providing soft capacity limits exceeded chunking represent sequence memory traces 
murdock proposes chaining method representing sequences single memory trace models large number psychological phenomena 
technique stores item pair information memory trace example sequence vectors stored abc trace suitable weighting constants generated underlying parameters 
retrieval sequence begins retrieving strongest component trace retrieval chaining correlating trace current item retrieve item 
sequence detected correlation trace current item similar item item memory 
way represent sequences entire previous sequence context just previous item murdock 
possible store sequences repeated items 
trace type sequence retrieved similar way previous retrieval cue built convolutions 
retrieval items representations improved subtracting pre components items sequence retrieved 
way represent sequences xed cue position sequence store abc trace retrieval storage cues arbitrary generated manner single vector chunking sequences methods soft limits length sequences stored 
sequences get longer noise retrieved items increases items impossible identify 
limit overcome chunking creating new non terminal items representing subsequences murdock 
second sequence representation method suitable chunking 
suppose want represent sequence abcdefgh 
create new items representing subsequences abc de new items added item memory marked way non terminals 
representation sequence abc abc de abc de cues normalized case pi length increases exponentially decoding chunked sequence slightly requiring stack decisions item non terminal decoded 
machine decode representations described section 
variable binding simple binding convolution convolve variable representation value representation 
desired representation avariable binding somewhat similar variable value vectors added 
gives representation variable binding 
type variable binding implemented types associative memory touretzky hinton outer product roles llers touretzky geva 
systems variable value objects di erent dimension binding object 
possible binding item association binding similar variable value 
frame slot representations frames represented convolution encoding manner analogous cross products roles llers hinton frames touretzky geva 
frame consists frame label set roles represented 
instantiated frame sum frame label roles slots convolved respective llers 
example suppose simpli ed frame seeing 
vector frame label see vectors roles agent object 
frame instantiated llers jane spot represent jane saw spot seeing see agent jane object spot fillers roles retrieved instantiated frame correlating role ller 
vectors representing roles frame speci agent see di erent agent run just similar 
uninstantiated frames stored sums vectors representing components seeing see agent object 
section describes way manipulating uninstantiated frames selecting appropriate roles ll 
frame representation similar vector adding vector 
example add jane instantiated frame representation jane doing similarity representation jane 
reduced representations types representations described section trivial step building reduced representations represent complex hierarchical structure xed width vector 
instantiated slack article discovered writing suggests distributed memory representation trees frame ller spot frame built previous section 
example spot ran running run agent spot seeing frame dick saw spot run seeing see agent dick object running see agent dick object run agent spot representation manipulated chunking 
chunking extract agent object correlating object agent 
chunking extract object correlating object clean extract agent giving noisy vector chunking 
implements hinton idea system able focus attention constituents able meaning 
suggests possibility sacri cing accuracy speed cleaned retrievals accurate 
simple machines hrrs section simple machines operate complex convolution representations described 
machines successfully simulated convolution calculator vectors elements 
role ller selector manipulate frames roles llers able select appropriate roles llers convolving 
describe way extracting appropriate role uninstantiated frame 
appropriate role particular ller rst role frame role combines best ller 
selection criteria combined single mechanism 
uninstantiated frame stored sum roles frame label 
role ller stored separately item memory 
uninstantiated frame 
task select role combines best ller 
suppose item item memory quite similar presence similar binding item memory de nes best tting role roles frame selected best approximately equal selected rst greater 
selection role done convolving uninstantiated frame potential ller 
cleaned item memory give correlated involving convolution products similarity representation suggested 
normalization lengths vectors issue consider lack 
form generalization similarity extensively 
give written noise magnitude noise depend result added uninstantiated frame give noise 
strongest role selected cleaning item memory 
strongest depend relative strengths turn depends similarity machine accomplishes operation shown 
uninstantiated frame filler clean clean role selection mechanism best role chunked sequence readout machine reads chunked sequences described section built bu ers stack classi er correlator clean memory gating paths 
classi er tells item prominent trace terminal non terminal chunk 
iteration machine executes action sequences depending output classi er 
stack implemented ways including way suggested plate fast weights 
machine shown 
control loop chunked sequence readout machine loop stack gives signal clean trace recover prominent item clean 
classify terminal non terminal case pop appropriate action appropriate action sequences 
terminal item output 
gates path replace trace follower 
non terminal signal tells stack push follower non terminal push 
signal gates path replace trace non terminal pop signal gates path replace trace top stack top 
signal tells stack discard top stack pop 
stack gives signal empty 
chunked sequence readout machine example system meaning focus attention constituents 
trace stack push key control value pop control value path gate clean current output item item classi er terminal nonterminal pop output signal vector data path chunked sequence readout machine 
mathematical properties mathematical properties circular convolution correlation discussed plate including algebraic properties reason convolution approximate inverse correlation existence usefulness exact inverses convolutions variances dot products various convolution products 
discussion circular convolution bilinear operation consequence linearity storage ciency 
storage ciency high usable scales linearly 
convolution endowed positive features virtue linear properties 
computed quickly ffts 
analysis capacity scaling generalization properties straightforward 
possibility system hrrs retain ambiguity processing ambiguous input 
convolution xed mapping connectionist network replace usual weight matrix vector mappings 
activations propagated forward quickly ffts gradients propagated backward quickly ffts 
learn take advantage convolution mapping learn distributed representations inputs 
memory models circular convolution provide way representing compositional structure distributed representations 
operations involved linear properties scheme relatively easy analyze 
learning involved scheme works wide range vectors 
systems employing representation need error correcting auto associative memory 
conversations jordan pollack janet metcalfe geo hinton essential development ideas expressed 
research supported part canadian natural sciences engineering research council 
aij special issue connectionist symbol processing 
arti cial intelligence 
fodor pylyshyn fodor pylyshyn 
connectionism cognitive architecture critical analysis 
cognition 
hinton hinton 
implementing semantic networks parallel hardware 
hinton anderson editors parallel models associative memory 
hillsdale nj erlbaum 
hinton hinton 
distributed representations 
technical report cmu cs carnegie mellon university pittsburgh pa 
hinton hinton 
mapping part connectionist networks 
arti cial intelligence 
hop eld hop eld 
neural networks physical systems emergent collective computational abilities 
proceedings national academy sciences 
kanerva kanerva 
sparse distributed memory 
mit press cambridge ma 
metcalfe janet metcalfe 
composite holographic associative recall model 
psychological review 
murdock bennet murdock 
theory storage retrieval item associative information 
psychological review 
murdock murdock 
distributed memory model serial order information 
psychological review 
murdock bennet murdock 
serial order ects distributed memory model 
david robert ho man editors memory learning ebbinghaus conference pages 
lawrence erlbaum associates 
pike ray pike 
comparison convolution matrix distributed memory systems associative recall recognition 
psychological review 
plate tony plate 
holographic reduced representations 
technical report crg tr department computer science university 

algebraic relations involutions convolutions correlations applications holographic memories 
biological cybernetics 
slack slack 
parsing architecture distributed memory machines 
proceedings coling pages stanford calif 
association computational linguistics 
slack slack 
role distributed memory natural language processing 
shea editor advances arti cial intelligence proceedings sixth european conference arti cial intelligence ecai 
elsevier science publishers 
slack slack 
parsing architecture distributed memory machines 
proceedings coling pages 
association computational linguistics 
smolensky smolensky 
tensor product variable binding representation symbolic structures connectionist systems 
arti cial intelligence 
touretzky geva touretzky geva 
distributed connectionist representation concept structures 
proceedings ninth annual cognitive science society conference hillsdale nj 
erlbaum 
touretzky hinton touretzky hinton 
symbols neurons details connectionist inference architecture 
ijcai pages 
willshaw willshaw 
associative memory inductive generalization 
hinton anderson editors parallel models associative memory 
erlbaum hillsdale nj 

