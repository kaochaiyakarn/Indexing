improving memory system performance sparse matrix vector multiplication toledo xerox palo alto research center coyote hill road palo alto ca november sparse matrix vector multiplication important kernel runs inefficiently superscalar risc processors 
describe techniques increase instruction level parallelism improve performance 
techniques include reordering reduce cache misses originally due das blocking reduce load instructions prefetching prevent multiple load store units stalling 
techniques improve mflops ordered matrix mflops mflops machine 
techniques applicable superscalar risc processors improved performance sun ultrasparc workstation example 
sparse matrix vector multiplication important computational kernel iterative linear solvers see example 
unfortunately computers kernel runs slowly relative numerical codes dense matrix computations 
proposes new techniques improving performance sparse matrix vector multiplication superscalar risc processors 
experimentally analyzes techniques techniques proposed show improve performance factor matrices 
main factors contribute poor performance sparse matrix vector multiplication modern superscalar risc processors 
lack data locality causes large number cache misses 
typically accesses data structures represent sparse matrix temporal locality whatsoever spatial locality data reuse accesses stride loop 
accesses dense vector multiplied reuse data access pattern depends sparsity structure technique reduce number cache misses reorder matrix reduce number cache misses technique proposed das analyzed certain cases jalby investigated burgess giles 
study technique show effectiveness new techniques propose depends 
part done author postdoctoral fellow ibm watson research center yorktown heights new york 
second factor limits performance tendency multiple load store functional units cache line 
superscalar risc processors load store units 
processors unit stalled due cache unit continue load data cache 
unfortunately stride loops units soon try load data cache line caused consequently units stalled cache line 
compulsory accesses temporal locality unit spend time waiting serviced 
misses generated units compulsory show prevent prefetching 
sparse matrix vector multiplication codes typically perform large number load instruction relative number floating point operations perform 
phenomenon caused poor data locality difficult reuse data registers code load row column indices addition floating point data 
large number load instruction places heavy load load store units interface register files cache integer alu compute addresses loaded 
alu part load store units part integer execution units 
current processors units bottleneck sparse matrix vector multiplication 
floating point units underutilized 
techniques address issue including blocking reduce number load instructions 
blocking sparse multiplication somewhat different forms 
techniques propose applied separately effective combined 
particular reordering matrix enhance degrade effect blocking 
reduction number cache misses reordering yields prefetching technique ineffective large matrices 
implemented techniques evaluated ibm superscalar risc workstation suite matrices matrix collections 
matrices structurally symmetric code general exploit symmetry 
matrix vector code symmetric matrices load fewer coefficients run efficiently 
techniques applicable superscalar risc processors 
describe techniques section comment appropriate applicability current superscalar processors 
techniques prefetching difficult implement irregular loops comprise sparse matrix vector multiplication code 
section explains implemented technique 
section presents experimental results conclude section 
algorithmic techniques consider typical sparse matrix vector multiplication code shown 
code assumes matrix stored compressed row format considerations apply storage formats support general sparsity patterns 
inner loop code loads jp jp loads jp may element performs multiply add operation 
loaded stride access pattern jp may element potential performance problems code 
accesses generate lot cache misses cache line stride access ensures entire line evicted cache 
depending number nonzeros details iterative algorithm matrix vector multiplication sum sum sum jp jp sum sparse matrix vector multiplication code matrices stored compressed row format 
nonzeros gamman matrix compressed single vector rowwise ordering column indices nonzeros compressed integer vector 
vector stores index row vectors element contains 
cache misses occur level cache cache away processor 
accesses poor spatial temporal locality generate cache misses 
ratio words loaded registers floating point operation means code performance limited performance processor load store units 
conversion jp integer index byte offset required processors indirect addressing requires integer alu perform additional instruction iteration 
see cope problems 
reducing cache misses bandwidth reduction das proposed reorder sparse matrices bandwidth reducing technique order reduce number cache misses accesses generate 
jalby analyzed number cache misses function bandwidth certain cache configurations 
technique investigated burgess giles extended unstructured grid computations 
burgess giles studied experimentally reordering strategies including reverse cuthill mckee greedy blocking method 
reordering improved performance relative random ordering find particular sensitivity particular ordering method 
performed additional experiments larger set matrices 
experiments described detail section essentially validates results burgess giles 
compared random ordering bandwidth reduction nested dissection orderings reduce cache misses improve performance 
performance improve factor large matrices 
bandwidth reduction technique blocking prefetching particular ordering method matter performance matrix vector multiplication 
consequently reordering methods selected reordering speed effect ordering sensitive preconditioners incomplete lu 
bandwidth reduction technique combined blocking prefetching different ordering methods yield different performance results 
situation cuthill mckee ordering usually best choice 
fast algorithm benefits preconditioning explained section 
sparse matrix vector multiplication uses large data structures poor data locality reducing cache misses important goal code processors order execution cover cache latency codes 
reducing load instructions blocking reduce number load instructions code performs splitting general sparse matrix sum matrices block matrices 
reasonable expect matrix contain dense blocks application areas give rise matrices example equations defined grids variable grid point 
multiplying block sparse matrix vector reduce number loads unblocked multiplication different ways 
blocking enables code load fewer row column indices registers index block required nonzero 
second elements loaded times matrix blocked blocks form register blocking 
third power processor load quad instruction loads consecutive floating point registers blocks allow processor load nonzeros elements load instructions 
scan matrix preprocessing phase extract blocks find blocks find 
extraction done greedy fashion extracts blocks find rowwise scanning matrix 
blocks greedy algorithms optimal sense may find fewer blocks possible 
greedy algorithm optimal blocks 
preserve data locality accesses multiply blocks blocks remaining unblocked nonzeros 
process row row row perform multiplication unblocked nonzeros row multiplication blocked nonzeros rows 
processors quad load instruction practically current risc processors benefit blocks realized replacing blocks 
require floating point loads element loaded twice 
balance capabilities load store units floating point units superscalar processors worse balance power processor 
power processor perform flops issuing multiply add instructions load bit words issuing quad load instructions cycle 
digital alpha sun ultrasparc floating point units issue add multiply instruction cycle 
alpha issue load instructions load words giving balance power ultrasparc issue load instruction cycle giving worse balance 
conclude reducing number loads important processors power 
approach blocking different previous algorithms mainly algorithm attempts find small completely dense blocks 
researchers proposed algorithms attempt find larger fewer dense blocks blocks completely dense 
agarwal describe algorithm designed vector processors 
algorithm tried find large relatively dense blocks 
divide rows matrix blocks attempt find fairly dense rectangular block block rows 
nonzeros block rows remain unblocked may coalesced dense diagonals 
scheme works vector processors vector startup cost significant cost 
realized risc processors small blocks suffice obtain performance important objective block nonzeros possible blocks small 
addition small vector startup cost risc machines implies probably beneficial dense blocks contain zeros 
extra time takes load structural zeros cache registers mask performance benefit larger dense blocks afford 
simple analysis shows allowing zero block improve performance slightly power processor 
petsc toolkit scientific computations uses blocked sparse matrix vector multiplication subroutine find blocks rows nonzero structure 
approach similar petsc approaches require blocks completely dense important difference approaches 
algorithm block nonzeros matrix rows identical petsc 
penalty algorithm pays extra flexibility quite small blocked algorithm loads column index floating point words loaded 
addition petsc allows different numbers rows different blocks cause runtime overhead fixed size blocks 
prefetching irregular loops traditionally prefetching considered technique hiding latency sense prefetching prevent memory access latency degrading performance long memory bandwidth sufficient keep processing units busy see example 
codes example dense matrix multiplication ratio floating point load instructions high 
high ratio allows algorithm hide latency cache misses prefetching cache lines early 
fact load store unit stalled cycles negligible load instruction relative floating point instructions 
matrix vector multiplication especially sparse matrices ratio floating point load instructions low 
memory bandwidth required keep processing units high 
computers sufficient memory bandwidth loading data memory bottleneck determines performance computation 
important load store unit stalls early prefetching late needed time 
prefetching hide latency improve memory bandwidth 
far know prefetching novel 
particular prefetching prevent multiple load store units stalling cache line 
cache misses generated accesses vectors accessed stride loop multiple load store units cache line 
unit misses word line stalls 
second unit tries load word vector stalls 
happen vectors accessed second unit loads word cache second vector misses 
units stall line effective memory cache cache register bandwidths reduced 
possible avoid bandwidth reduction prefetching 
strategy prefetch elements 
prefetching instruction misses stalls load store units 
unit continues stalling long cache misses rare matrix reordered 
details quite complicated explain section 
prefetching order prevent multiple load store units stall cache line optimization beneficial effect risc processors multiple load store units 
eliminating integer pointer conversions expression jp compiled instruction loads jp register instruction load jp 
jp loaded instruction accessed stride loop instruction loads increments pointer jp size integer 
loading jp requires instruction instruction loads word offset requires byte offset word offset 
jp multiplied size word bytes yield byte offset 
multiplication done arithmetic shift instruction executes cycle integer alu 
processors integer alu compute addresses load instructions power processor digital alpha extra integer integer instruction places additional overhead integer alu overloaded load instructions 
perform preprocessing phase replace integer indices pointers elements show optimization improve performance 
optimization amounts moving transformation column indices vector loop multiplies vector matrix times 
optimization principle performed compiler capable interprocedural analysis 
compiler optimization may possible iterative algorithm matrix vector multiplication kernel compiled time 
requires compiler recognize column indices overwritten pointers allocate large temporary array store pointers 
processors dedicated virtual address adder sun ultrasparc optimization effect load shift instructions compete functional units 
experiments ultrasparc workstation described section verify optimization effect performance code processor 
prefetching irregular loop implement prefetching strategy need perform prefetch instruction certain number iterations inner loop blocked counterparts 
cache lines target machine contain integers 
ideally prefetch cache line iterations cache line iterations 
simpler scheme prefetch vectors iterations 
prefetch iteration jp jp cache lines ahead prefetch iteration jp jp jp cache lines ahead 
constants depend cache configuration numbers illustration 
difficult implement precise strategy simpler introducing extra overhead inner loop 
problem number iterations inner loop performs depends length row may different different rows 
sorting rows length viable option conflicts ordering rows columns data locality 
padding rows zeros length divisible attractive computing padding zeros constitute significant overhead sparse matrices 
approximation 
main idea unroll inner loop say times place prefetching instruction unrolled instance instances 
rows long implement simple prefetching strategy 
approximation implement strategy prefetching iterations rows short 
row ends shortly prefetch prefetching row usually stalls second load store unit cache line 
rows shorter elements prefetched 
experiments shown section demonstrate technique effective 
preserve semantics original loop compare jp unrolled instance skip rest instances jp greater 
ensures step sparse row due unrolling 
approximate prefetching difficult implement high level language fortran 
difficulty revolves need compare jp conditionally branch slowing iterations 
solution compile algorithm written assembly language modify assembly language compile object code 
perform unrolling insertion prefetching instructions conditional branching assembly language 
working assembly language allows exploit counter instruction unroll irregular loop overhead 
able perform modifications language day ibm power workstation sun ultrasparc workstation 
hardware assisted prefetching propose simple hardware assisted prefetching mechanism handle prefetching regular irregular loops 
propose new instructions called prefetch start prefetch 
instructions serve hints processors just regular prefetch instructions take argument name integer register 
prefetch start instruction indicates processor register serves pointer stride loop 
hardware attempt prefetch cache lines ahead current value prefetch instruction indicates register longer pointer loop prefetching content 
prefetching iteration structures complex stride loops supported variants prefetch start instruction 
possible implementation generate signal addition generates carry bit bit cache lines bytes wide 
signal prefetching cache line ahead current value scheme ensures prefetching signal generated small increment cache line causes point new cache line 
form hardware assisted prefetching advantages regular prefetching instructions inserted compiler hand 
hardware assisted prefetching works equally regular irregular loops eliminates need prefetch fixed number inner iterations 
hardware assisted prefetching enables precise prefetching single binary binary executed machines cache configurations 
example machines ibm power processors come primary cache configurations requiring different prefetching rates different cache line sizes 
experimental results section results extensive experiments carried order assess effectiveness strategy 
experiments carried superscalar ibm rs workstation 
repeated experiments sun ultrasparc workstation order determine portable techniques 
believe findings apply risc processors 
experimental platform experiments carried rs workstation mhz power processor kbytes way set associative data cache bit wide memory bus mbytes main memory 
power processor architected physical floating point registers floating point units integer units serve load store units branch unit 
floating point units capable executing multiply add instruction cycle peak performance floating point operations second mflops 
data cache integer units capable loading storing cycle integer register floating point register floating point registers called quad load quad store instruction 
cache capable servicing hits integer unit continue load store data cache waiting cache completed 
separate branch unit enables processor perform branch instructions stalling execution units 
called zero cycle branches include virtually unconditional branches branch counter kind branch instruction innermost loop set nested fortran loops 
put results perspective note machine dense matrix multiplication subroutine dgemm ibm engineering scientific subroutine library achieve performance mflops applied square matrices fit cache 
performance dense matrix vector multiplication subroutine machine mflops applied large matrices 
performance algorithms assessed measurements running time cache misses 
time measured machines real time clock resolution cycle 
number cache misses measured power performance monitor 
performance monitor hardware subsystem processor capable counting cache misses processor events 
real time clock performance monitor oblivious time sharing 
minimize risk measurements influenced processes ran experiments users machine connected network 
verified measurements valid comparing real time clock measurements user time reported system call experiment experiment basis 
measurements reported ordering blocking times average executions 
coded algorithms compiled ibm compiler version 
compiler options pwr cause compiler optimize code utilize instructions specific power processor importantly quad loads 
matrix dimension nonzeros source model bcsstk boeing model automobile chassis msc boeing test matrix msc msc boeing test matrix msc ct boeing ct engine block boeing fem crystal free vibration boeing fem crystal free vibration bcsstk boeing automobile seat frame body attachment bcsstk boeing automobile shock assembly bcsstk boeing track ball cooling water jacket bmw engine large model high speed civil transport unstructured grid rock large model rock table characteristics test suite matrices 
boeing matrices stiffness structural engineering matrices roger grimes boeing matrices partitioning benchmarks nasa numerical aerodynamics simulations department 
optimization options compiler inner loop usually contains multiply adds 
implemented prefetching modifying compilers assembly language output 
specifically unrolled inner loops hand factor placed prefetching instruction unrolled loop element middle loop multiply adds element pointer vector 
methodology test matrices algorithms tested suite sparse symmetric stiffness matrices 
matrices structural engineering matrices boeing donated roger grimes boeing matrix collection tim davis matrix collection 
matrices partitioning benchmarks nasa numerical aerodynamics simulations department 
characteristics test matrices described table 
matrices sources 
matrices contributed roger grimes boeing represent dimensional degrees freedom variables grid point typically 
ordered band profile algorithm applied original grid degrees freedom associated single grid point remain contiguous 
matrices collection represent dimensional models degree freedom grid point 
consequence sparser contiguous dense blocks 
ordered placed matrix collection 
matrix measured performance combinations different multiplication codes different orderings 
codes described table 
orderings random ordering nested dissection type ordering denoted reverse cuthill mckee rcm cuthill mckee cm original ordering matrix 
mnemonic language indices blocking prefetching integers pointers assembly pointers assembly pointers pointers assembly pointers assembly pointers pointers assembly pointers assembly pointers table characteristics matrix vector multiplication codes 
stored matrix collection 
ordering code written gupta 
experiment measured running time matrix vector multiplication code number load instructions various kinds performed number cache tlb misses 
measurement average multiplications 
multiplications eliminate influences cold starts usually significant iterative algorithms 
matrices large remain cache multiplications 
cache hold experiments vector fit cache remain multiplications 
assess cost reordering blocking matrices relative potential benefits recorded running time ordering blocking algorithm preprocess matrices 
measurements single runs 
effect blocking prefetching shows performance codes 
matrices ordered original ordering matrix collections proved best close best ordering see details effect ordering 
striking feature emerges behavior boeing matrices different behavior matrices 
difference due differences sparseness matrices explored detail 
algorithmic improvements introduced boosts performance boeing matrices 
replacing integer indices pointers increases performance mflops mflops 
blocking extra unrolling assembly language code improves performance additional mflops 
prefetching improves performance mflops 
blocking essentially difference assembly language versions 
blocking prefetching improves performance mflops prefetching combined blocking boosts performance mflops 
blocking prefetching gives similar performance 
blocking combined prefetching yields performance mflops 
matrices pointers unrolling prefetching help blocking improve performance 
fact cases blocking degrades performance 
matrices best performing code yields performance mflops differences different codes smaller differences boeing matrices 
matrices sparser boeing matrices blocking multiplication code bcsstk msc msc ct bcsstk bcsstk bcsstk rock mflops blocking blocking blocking performance millions floating point operations second mflops sparse matrix vector multiplication codes 
performance test matrix represented bars code 
bars organized groups blocking blocking blocking blocking 
group represented different shade gray 
leftmost bar blocking represents performance code integer indices 
bars bars groups represent performance code pointer indices assembly language code pointer indices assembly language code pointer indices prefetching 
original orderings matrices matrix collections 
average nonzeros row speedup scatter plot percentage speedup running time due blocking prefetching versus average density matrix 
mark represents matrix boeing matrices represented matrices theta 
speedup computed gamma running time blocking prefetching running time blocking prefetching 
original ordering matrices 
introduces overhead delivering significant benefit 
shows correlation sparseness matrices benefit blocking prefetching yields 
reason matrices benefit blocking represent models degree freedom grid point 
underlying graphs cliques number dense blocks small matter matrices ordered 
effect ordering shows effect reordering matrices matrix vector multiplication codes 
cases performance ordered matrices better randomly permuted matrices 
differences especially large large matrices bcsstk ct rock 
shows correlation order matrix improvement performance due ordering 
plot indicates boeing matrices benefit ordering matrices conclude benefit ordering depends order matrix density 
blocking various ordering methods yield roughly performance 
level performance similar level achieved original ordering test matrices 
blocking performance matrices degrades slightly performance boeing matrices improves 
blocking different ordering methods yield different performance levels 
random ordering worst followed rcm ordering ordering cm ordering original ordering 
explains variations performance blocked codes various ordering methods 
analyze denser boeing matrices performance impact blocking sparser matrices marginal 
shows random reverse cuthill mckee orderings result matrices blocks 
blocked code randomly permuted matrix matrix rcm ordered improve performance 
ordering creates blocks 
boeing matrices nonzeros blocks 
usually smaller blocks 
cuthill mckee orderings results nonzeros blocks 
fraction blocks better ordering half 
original ordering nonzeros blocks vast majority blocks 
differences fractions nonzero blocked different orderings traced factors 
original ordering yields better blocking orderings probably applied grid points individual degrees freedom variables 
consequently dense blocks matrix produced grid generator remained dense 
applied orderings matrices dense blocks disappeared 
differences cuthill mckee reverse cuthill mckee may due fact blocking algorithm greedy scans matrix top left right rows 
cost reordering blocking shows time takes reorder randomly permuted test matrices 
time shown relative basic matrix vector multiplication time randomly permuted matrix 
cuthill mckee reverse cuthill mckee orderings take factor matrix vector multiplication 
worth cost 
ordering costs random rcm cm original bcsstk msc msc ct bcsstk bcsstk bcsstk rock mflops prefetching blocking bcsstk msc msc ct bcsstk bcsstk bcsstk rock mflops prefetching blocking bcsstk msc msc ct bcsstk bcsstk bcsstk rock mflops prefetching blocking bcsstk msc msc ct bcsstk bcsstk bcsstk rock mflops prefetching blocking performance mflops assembly language matrix vector multiplication codes 
top depict performance blocking bottom blocking 
graphs left prefetching graphs right 
graph shows performance matrix different orderings represented different shades gray 
order scatter plot relative reduction running time due ordering versus order matrix 
mark represents matrix boeing matrices represented matrices theta 
reduction running time computed gamma running time blocking prefetching random ordering running time blocking prefetching original ordering 
bcsstk msc msc ct bcsstk bcsstk bcsstk rock blocked random rcm cm original fraction matrix nonzeros blocked blocks 
graph shows fraction nonzeros blocks different orderings matrix usually result low levels blocking 
striped portion bar represents fraction nonzeros blocks solid portion fraction remaining nonzeros blocked blocks 
levels blocking blocks slightly higher combined levels shown 
bcsstk msc msc ct bcsstk bcsstk bcsstk rock reordering time basic matrix vector time rcm cm bcsstk msc msc ct bcsstk bcsstk bcsstk rock blocking time basic matrix vector time random rcm cm original cost reordering matrices top blocking bottom 
cost shown terms time relative basic matrix vector multiplication time randomly permuted ordering matrix 
longer blocking times caused excessive paging 
factor matrix vector vector multiplication 
ordering code designed fill reducing mechanism direct factorizations expensive single matrix vector multiplications 
appropriate application 
shows time takes block reordered test matrices blocks 
time shown relative basic matrix vector multiplication time randomly permuted matrix 
blocking usually costs times cost basic matrix vector multiplications 
graph shows experiments blocking took significantly time caused paging 
fact phenomenon occurs blocking follows ordering step additional memory allocated shows problem paging memory management 
estimate matrix vector multiplications performed blocking step pay cost blocking 
assume blocking costs times cost basic matrix vector multiplication times cost best unblocked code prefetching 
assume blocking reduces matrix vector multiplication time conservative estimate boeing matrices 
assumptions follows matrix multiplications blocking reduce running time 
comparison direct solver put results broader perspective compare performance sparse direct solver 
solver written gupta provided timings 
directly solving linear system ax test matrix bcsstk right hand side took seconds machine rest experiments 
seconds ordering matrix took seconds symbolic factorization took seconds numerical factorization took seconds triangular solve took seconds 
number floating point operations factorization giving computational rate mflops numerical factorization mflops entire solution 
comparison best matrix vector multiplication code took seconds matrix 
cost numerical factorization equivalent matrix vector multiplications 
total cost direct solution seconds equivalent seconds reordering blocking plus matrix vector multiplications 
portability experiments repeated experiments reported superscalar risc computer sun ultrasparc workstation 
machine mhz ultrasparc processor kbytes direct mapped primary data cache secondary chip kbytes cache bit wide memory bus mbytes memory 
ultrasparc processor floating point registers issue floating point multiply floating point add cycle 
gnu compiler loops optimization options preliminary testing showed gnu compiler produced faster code sun compiler 
language subroutines compiled ran problem 
took day produce assembly language versions routines prefetching 
performance results performance msc msc matrices summarized follows 
basic version randomly permuted matrices ran mflops 
version original ordering matrices ran mflops 
relative improvement improvement due reordering larger power machine matrices smaller size primary cache ultrasparc replacing integer indices pointers improve performance prefetching 
optimizations slow algorithm 
blocking matrices blocks improved performance mflops 
integer pointer conversion help probably ultrasparc dedicated address adder address calculations integer pointer conversions shift instructions complete functional units 
reason prefetching improve performance ultrasparc load store unit prefetching techniques targeted prevent multiple units stalling cache line 
performance improvements experiments show verify techniques possible exception prefetching portable improve performance superscalar risc machines 
note level experiments may low compared peak performance processor fact quite similar performance similar codes ultrasparc 
example double precision dot product subroutine fortran blas subroutine sun performance library run mflops machine 
main reason performance level small primary cache coupled short cache lines bytes 
techniques accelerating sparse matrix vector multiplication superscalar risc processors 
techniques precomputing addresses indirect addressing trivial important 
technique reordering matrix reduce bandwidth reduce cache misses proposed das investigated papers 
explored cuthill mckee yields excellent results variety matrices 
techniques representing nonzeros small dense blocks prefetching allow cache hits processing novel proposed represent nonzeros larger blocks 
improve performance significantly matrices 
ibm rs workstations combined effect techniques improve performance mflops mflops depending size sparseness matrix 
techniques exception prefetching portable 
ordering blocking techniques improve performance risc processors shown experiments experiments burgess giles 
replacing integers pointers improve performance risc machines functional units address calculations shifts 
code implements techniques portable 
prefetching technique improve performance superscalar risc processors load store unit 
implementation method technique duplicated machines technique considered portable 
basic matrix vector multiplication codes perform better matrices extremely sparse 
nonzeros row highly optimized codes sensitive sparseness matrices 
reordering sparse matrices cuthill mckee ordering benefit sparse iterative solvers 
conjugate gradient solver uses incomplete cholesky preconditioner ordering matrix effects convergence rate 
duff compared convergence rate incomplete cholesky preconditioned conjugate gradient different orderings model problems 
tests cuthill mckee reverse cuthill mckee resulted convergence rates best close best 
possible different orderings preconditioning matrix vector multiplication doing requires permuting vector twice iteration 
extra step renders iteration expensive 
cuthill mckee similar ordering steps eliminates need permute vectors iteration leads cache misses matrix vector multiplication step preconditioning step enables blocking accelerates convergence 
proposed hardware assisted prefetching eliminate somewhat complicated coding technique implement prefetching irregular loop sparse rows matrix 
mowry compared compiler sofware directed prefetching prefetching hardware software intervention proposed lee porterfield baer chen 
proposal lies extremes 
proposal enjoys little overhead prefetching instructions outside inside inner loop 
proposal suffer excessive memory contention overheads prefetching enables disabled software 
proposal enables prefetching irregular loops mowry consider 
main disadvantages proposal need augment instruction set hardware cost 
detailed study required order assess effectiveness proposed hardware assisted prefetching scheme 
acknowledgments fred bowen alpern dave burgess read commented early versions 
comments helped improve considerably 
gupta details regarding performance sparse factorization code assistance matrix reordering package 
ramesh agarwal stimulating discussions 
agarwal 
high performance algorithm preprocessing sparse matrix vector multiplication 
proceedings supercomputing pages november 
agarwal 
improving performance linear algebra algorithms dense matrices algorithmic prefetch 
ibm journal research development 

baer 
chen 
effective chip preloading scheme reduce data access penalty 
proceedings supercomputing 
satish william gropp lois barry smith 
petsc users manual 
technical report anl revision argonne national laboratory 
berry chan demmel donato dongarra van der vorst 
templates solution linear systems building blocks iterative methods 
siam pa 
burgess giles 
renumbering unstructured grids improve performance codes hierarchical memory machines 
technical report numerical analysis group oxford university computing laboratory may 
das saltz gupta 
design implementation parallel unstructured euler solver software primitives 
aiaa journal 
iain duff erard 
effect ordering preconditioned conjugate gradient 
bit 
john paul rubinfeld ronald preston rajagopalan 
superscalar instruction execution alpha microprocessor 
ieee micro pages april 
gupta 
fast effective algorithms graph partitioning sparse matrix ordering 
technical report rc ibm watson research center yorktown heights ny july 
gupta 
watson graph partitioning sparse matrix ordering package 
technical report rc ibm watson research center yorktown heights ny may 
lawson hanson kincaid krogh 
basic linear algebra subprogram fortran usage 
acm transactions mathematical software 
lee 
effectiveness caches data prefetch buffers large scale shared memory multiprocessors 
phd thesis university illinois urbana champaign may 
todd mowry 
tolerating latency software controlled data prefetching 
phd thesis stanford university march 
porterfield 
software methods improvement cache performance supercomputer applications 
phd thesis rice university may 
jalby 
characterizing behavior sparse algorithms caches 
proceedings supercomputing november 
marc tremblay michael connor 
ultrasparc issue processor supporting multimedia 
ieee micro pages april 
chan hicks 
power performance monitor 
ibm journal research development 
white 
power generation risc system family 
ibm journal research development 

