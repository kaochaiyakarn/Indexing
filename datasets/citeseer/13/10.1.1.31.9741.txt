independent component analysis mixed sub gaussian super gaussian sources te won lee technische universitat berlin computational neurobiology lab salk institute pines road la jolla california usa salk edu terrence sejnowski howard hughes medical institute computational neurobiology lab salk institute pines road la jolla california usa terry salk edu extension infomax algorithm bell sejnowski able separate mixed sub super gaussian source distributions 
learning rule derived girolami fyfe negentropy perspective projection pursuit 
laplacian prior propose learning rule especially convenient realize hardware 
natural gradient extension different perspectives preprocessing steps proposed speed convergence 
simulation results show algorithm able separate source variety source distributions 
real data jung 
mckeown 
demonstrate successful extended ica algorithm analyze eeg fmri recordings 
blind source separation independent component analysis ica received attention potential applications signal processing speech recognition systems telecommunications medical signal processing 
goal ica recover independent sources sensor outputs sources linearly mixed 
contrast correlation solutions principal component analysis pca ica signals nd order statistics reduces higher order statistical dependencies attempting signals independent possible 
blind source separation problem studied researchers field neural networks statistical signal processing 
bell sejnowski developed unsupervised learning algorithm entropy maximization feedforward neural network 
algorithm uses sigmoidal activation function especially suited separate sources higher kurtosis gaussian distribution super gaussian 
related informationtheoretic algorithm preserves simple architecture bell sejnowski allows extension separation mixtures super gaussian sub gaussian sources 
girolami fyfe derived learning rule negentropy viewpoint extended exploratory pursuit 
show algorithm successfully separate mixtures sources sound tracks obtained pearlmutter speech sound signals bell sejnowski uniformly distributed sub gaussian noise signals noise source gaussian distribution 
jung successfully applied extended ica algorithm remove artifacts electroencephalographic eeg recordings 
raw data blindly decomposed independent components line noise eye movements muscle movements 
eliminating artifactual components corrected eeg data free artifacts 
furthermore mckeown show extended ica algorithm find transient signals fmri data 
algorithm bell sejnowski proposed simple neural network algorithm blindly separates mixtures independent sources infomax 
show maximizing joint entropy output neural processor minimizes mutual information output components invertible bounded nonlinearity wx 
pearlmutter parra derive learning rule maximum likelihood ml density estimation approach kullback leibler distance measure log dx gamma log probability density function pdf observation parametric estimate distribution independent sources 
cardoso shows infomax ml equivalent relation kl distance ml differs constant entropy dependent gamma log dx gamma px girolami fyfe start negentropy point view kurtosis measure projection pursuit pu pg gamma pu pg entropy gaussian distribution entropy estimated sources 
negentropy pu ml perspective measure kl distance transformed vector normality 
observation close gaussian distribution linear mixing independent variables due central limit theorem difference maximizing distance observation gaussian distribution matter practice 
approaches output entropy neural processor maximized implies approximating output density sense minimum kl distance uniform density 
algorithm shapes signal derivative activation function independent possible 
independence achieved nonlinear squashing function example sigmoid function provides combination higher order statistics taylor series expansion 
relate determinant jacobian py det evaluating expected value logarithmic representation eq gives output entropy maximized respect equivalent maximizing volume jacobian transfer function 
gammat efficient way maximize joint entropy follow natural gradient deltaw reported amari optimal rescaling entropy gradient simplifying learning rule speeding convergence considerably 
theoretically form nonlinearity plays essential role success algorithm 
ideal form cumulative density function cdf distributions independent sources 
practice choose sigmoid function learning rule reduces proposed 
algorithm limited separating sources super gaussian distributions 
purpose extended ica algorithm provide learning rule separate variety distributions 
general computationally burdensome solution contextual ica pdf modeled parametric form account temporal information 
pearlmutter parra choose weighted sum logistic density functions variable means scales means linear functions history source ju gamma ik oe ik gamma ik oe ik ik weighting parameters oe ik scaling parameters 
component means ik linear functions time samples source 
way generalizing learning rule sources sub super gaussian distributions approximate estimated pdf edgeworth expansion gram charlier expansion 
th order edgeworth expansion estimated sources sum pdf gaussian approximations 
girolami fyfe th order edgeworth expansion approximations cases 
sub gaussians approximation possible tanh gamma super gaussians approximation follows gamma tanh gamma sign flip substituted normalized kurtosis computed adaptively estimated sources cumulant gamma gamma learning rule extracted eq eq eq deltaw theta gamma sign tanh gamma uu intuitively super gaussians term gamma tanh corresponds anti hebbian rule tends minimize variance sub gaussians corresponding term hebbian rule tends maximize variance choose sigmoidal activation function exp gammau additional term learning rule changes deltaw theta gamma sign gamma gamma uu eq eq differ performance tanh sigmoidal activation function proportional 
assume source distribution laplacian speech activation function modeled gamma exp dv nonlinearity reduces sign function 
deltaw theta gamma sign sign gamma uu nonlinearity eq realized hardware implementation simple bit quantizer 
note case separating natural signals super gaussian learning rule simply deltaw theta gamma sign speeding convergence significant improvement convergence natural gradient 
amari derives optimized learning rule information geometry approach riemannian manifold fisher information matrix provides optimal rescaling simple gradient 
ij ef log defined loss function ij fisher information matrix 
applied source separation problem fisher information matrix reduces identity independent components orthogonal 
extension provides rescaling gradient non orthogonal metric natural gradient gamma euclidean normal gradient gamma transformation matrix 
cardoso calls metric relative gradient uses equivariant learning rule 
mackay derives natural gradient rule ml perspective finds metric curvature objective function second derivative ml function 
resulting learning rule covariant means steepest ascent optimal respect curvature objective function 
preprocessing methods speed convergence 
common statistical tool applying learning rule eq remove nd order correlation observation vector preprocessing method speeds convergence 
separation matrix consists second order sphering matrix ws unmixing matrix infomax algorithm delta ws whitening matrix ws computed ws gamma furthermore preprocessing method extended th order correlation cancelation simply adding step cancels th order correlation 
xs xs gamma delta delta ws improve performance infomax repetitively forcing nd order th order correlations zero speeds convergence improves separating sources fewer data points 
learning process observe momentum helps stabilize convergence algorithm 
deltaw gamma ff deltaw ff takes account history increases increasing number iterations 
simulation experimental results obvious question different learning rules eq eq eq extent nonlinear activation function approximate cdf separate sources practice 
answer question perform simulation experiments 

separation sound sources obtained pearlmutter comparison purpose cica 

separation sources sound tracks obtained pearlmutter speech sound signals bell sejnowski uniformly distributed sub gaussian noise signals noise source gaussian distribution 
simulation experiment eq eq preprocessing steps eq 
mixed sources separated pass data 
data points blocksize pass equivalent iterations 
learning rate fixed 
measure measure proposed amari related snr measure 
jp ij max jp ik gamma jp ij max jp kj gamma delta close identity matrix permutation scaling sources separated 
shows performance index learning process iterations eq eq 
learning rules converge correct solution 
eq converges faster eq pdf sound sources closer derivative sigmoidal activation function laplacian prior 
gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma matrix shows performance matrix iterations close identity matrix rescaling reordering 
compared cica original infomax algorithm shows performance having learn nonlinear transfer function 
second experiment performed preprocessing steps eq 
blocksize passes data iterations convergence 
shows performance rows manually reordered normalized unity 
listening test shows clear separation sources mixture 
case eq noise source gaussian distribution separated completely mixtures 
laplacian prior suitable separate source approximately gaussian distribution 
words sigmoidal activation function provide compromise gaussian spiky laplacian distribution provides approximation wide range source distributions 
experimental results eeg fmri data applied learning rules eq eq preprocessing steps analyze eeg recordings fmri data 

iteration performance index separation sound sources 
line performance eq continuous line performance eq results separation sources 
shows separation performance matrix normalizing reordering 
perfect separation identity matrix 
jung showed proposed algorithm remove artifacts eeg recordings 
recorded eeg signals contaminated artifacts filtered 
extended ica algorithm eq separate eye movement artifacts periodic muscle spiking line noise cardiac contamination noise 
sources sub super gaussian distributions 
eliminating artifactual components corrected eeg data free artifacts 
mckeown show algorithm find time courses voxels fmri data correspond time course experiments 
extended ica algorithm promising generalization ica mixed super gaussian sources 
algorithm robust efficient successfully large data sets derived electrical blood flow measurements functional activity brain 
acknowledgments lee supported german academic exchange program 
grateful 
jung mckeown discussions comments 
amari cichocki yang 
new learning algorithm blind signal separation 
advances neural information processing systems 
bell sejnowski 
information maximization approach blind separation blind deconvolution 
neural computation july 

cardoso laheld 
equivariant adaptive source separation 
ieee trans 
signal processing dec 

cardoso 
infomax maximum likelihood blind source separation 
appear ieee signal processing letters 
cichocki unbehauen 
robust learning algorithm blind separation signals 
electronics letters comon 
independent component analysis new concept 
signal processing 
girolami fyfe 
extraction independent signal sources exploratory projection pursuit network lateral inhibition 
submitted proceedings vision image signal processing journal 
jung humphries lee makeig mckeown sejnowski 
extended ica removes artifacts electroencephalographic recordings 
submitted advances neural information processing systems may 
karhunen oja wang 
class neural networks independent component analysis 
ieee trans 
neural networks vol may 
koehler lee 
improving performance infomax statistical signal processing 
submitted icann lausanne 
lambert 
multichannel blind deconvolution fir matrix algebra separation multipath mixtures 
thesis university southern california department electrical engineering may 

lee bell 
blind source separation real world signals 
proc 
icnn houston usa 
mackay 
maximum likelihood covariant algorithms independent component analysis 
submitted journal dec 
mckeown jung makeig brown kindermann lee sejnowski 
transiently time locked fmri activations revealed independent component analysis submitted proceedings national academy sciences may pearlmutter parra 
context sensitive generalization ica 
iconip press 
roth baram 
multidimensional density shaping sigmoids 
ieee trans 
neural networks 
