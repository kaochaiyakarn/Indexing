appear weigend gershenfeld eds predicting understanding past 
redwood city ca addison wesley publishing 
neural net architectures temporal sequence processing michael mozer department computer science institute cognitive science university colorado boulder february general taxonomy neural net architectures processing time varying patterns 
taxonomy subsumes existing architectures literature points promising architectures examined 
architecture processes timevarying patterns requires conceptually distinct components short term memory holds relevant past events associator uses short term memory classify predict 
taxonomy characterization short term memory models dimensions form content adaptability 
experiments predicting values financial time series dollar swiss franc exchange rates alternative memory models 
results experiments serve baseline sophisticated architectures compared 
neural networks proven promising alternative traditional techniques nonlinear temporal prediction tasks lapedes farber weigend huberman rumelhart 
temporal prediction particularly challenging problem conventional neural net architectures algorithms suited patterns vary time 
prototypical neural nets structural pattern recognition 
task collection features visual semantic network network categorize input feature pattern belonging classes 
example network trained classify animal species set attributes describing living creatures tail lives water network trained recognize visual patterns dimensional pixel array letter fa 
zg 
tasks network relevant information simultaneously 
contrast temporal pattern recognition involves processing patterns evolve time 
appropriate response particular point time depends current input potentially previous inputs 
illustrated shows basic framework temporal prediction problem 
assume time quantized discrete steps sensible assumption time series interest intrinsically discrete continuous series sampled fixed interval 
input time denoted 
univariate series input mozer predictor generic short term memory formulation temporal prediction task 
denotes vectorial representation input time denotes prediction time input sequence 

short term memory retains aspects input sequence relevant prediction task 
scalar allow multivariate series considered vector valued 
output predictor time input sequence 
represents value input possibly function value 
vector valued 
prediction time step selected times 
prediction involves conceptually distinct components 
construct short term memory retains aspects input sequence relevant making predictions 
second prediction short term memory 
neural net framework predictor feedforward component net short term memory internal recurrent connections 
zhang hutchinson volume de vries principe distinction components 
designing neural net model consider issues 
ffl architecture internal structure short term memory predictor networks answering question involves specifying number layers units pattern connectivity units activation dynamics units 
ffl training set training examples internal parameters model connection weights adjusted 
training example consists input series fx associated series predictions fp number steps example target prediction time training neural net model involves setting weights predictions come close possible target predictions usually squares sense 
ffl representation time varying input sequence represented shortterm memory 
nature quantity information go memory domain dependent 
chapter focus representation issue 
issues architecture network dynamics training procedure representation related fact viewed different perspectives underlying problems 
choice representation may demand certain neural net architecture particular type learning algorithm compute representation 
conversely architecture learning algorithm may restrict class representations accommodated 
address representation issue characterizing space neural network short term memory models 
sections dimensions memory mozer predictor neural net 
tapped delay line memory model 
delta operator introduces time step delay 
models vary memory form content adaptability 
framework points interesting memory models explored neural net literature 
forms short term memory tapped delay line memory simplest form memory buffer containing inputs 
memory called tapped delay line model buffer formed series delay lines 
called delay space embedding forms basis traditional autoregressive ar models 
tapped delay line memories common neural net models elman mcclelland elman zipser lapedes farber plaut nowlan hinton waibel hinton shikano lang wan volume zhang hutchinson volume 
memories amount selecting certain elements sequence 
say total omega elements forming state representation 
omega gamma 
minor extension formulation permit nonuniform sampling past values involves specifying varying delays gamma integer delay associated component formalism characterizing delay line memory broad encompass forms memories discussed 
treat mozer time relative input strength time relative input strength time relative input strength time relative input strength kernel functions delay line memory trace memory gamma memory gaussian memory 
convolution input sequence kernel function gamma delay line memories 
shows kernel function 
substituting different kernel function obtains forms memories discussed sections 
exponential trace memory exponential trace memory formed kernel function gamma lies interval gamma 
form memory studied jordan mozer stornetta hogg huberman 
delay line memory exponential trace memory sharply drop fixed point time mozer strength input decays exponentially 
means inputs greater strength distant inputs 
noise exponential memory preserve information sequence 
instance consider sequence binary digits memory 
memory bit string kernel function assigns input time gamma bit position gamma string 
memory finite resolution noisy significant bits distant inputs lost 
information loss difficult neural network extract information contained memory 
important property exponential trace memories computed incrementally gamma gamma boundary condition 
delay line memory exponential trace memory consists omega state vectors 
omega 
view exponential trace memories maintaining moving averages exponentially weighted past inputs 
allow representation averages spanning various intervals time 
exponential trace memory special case traditional statistical moving average models ma model 
general case ma readily represented trace input sequence dealt 
gamma memory dimensions depth resolution de vries principe characterize delay line exponential trace memories 
roughly depth refers far back past memory stores information relative memory size quantified ratio moment kernel function number memory state variables 
low depth memory holds information high depth memory holds information distant past 
resolution refers degree information concerning individual elements input sequence preserved 
high resolution memory reconstruct actual elements input sequence low resolution memory holds coarser information sequence 
terms dimensions delay line memory low depth high resolution exponential trace memory high depth low resolution 
de vries principe proposed memory model generalizes delay lines exponential traces allowing continuum memory forms ranging high resolution low depth low resolution high depth 
continuous time kernel memory gamma density function name gamma memory 
de vries principe deal primarily continuous time dynamics discrete equivalent gamma density function negative binomial serve corresponding kernel discrete time gamma memories gamma gamma 

integer delay lies interval 
shows representative gamma kernel 
gamma kernel reduces exponential trace kernel 
limit reduces delay line kernel 
exponential trace kernel convolution gamma kernel input sequence computed incrementally de vries principe compute mozer denote 
necessary compute 
gamma 
recursive update equation gamma gamma gamma gamma boundary conditions gamma constructing gamma memory designer specify largest denoted omega gamma total number state vectors omega 
omega omega gamma tradeoff resolution depth achieved varying large yields high depth low resolution memory small yields low depth high resolution memory 
set omega state vectors forms gamma family 
gamma memory consist families family characterized omega course leads large potentially unwieldy state representation 
assumptions domain trim state representation selecting certain vectors family input neural net predictor subset omega view direct extension exponential trace memory form omega vectors maintained part recursive update algorithm need passed neural net predictor 
forms memory memories just described possibilities 
kernel function results distinct memory form 
instance gaussian kernel obtain symmetric memory point time 
gamma memory special cases particularly useful computed incremental update procedure forms gaussian require evaluating convolution kernel input sequence time step 
convolutions occasionally waibel tank hopfield hopfield tank terribly practical computational storage requirements 
radford neal personal communication suggested class kernels polynomial functions fixed interval time fixed point past omega ij gamma 
proposed incremental update procedure memories kernel 
number memory state vectors omega gamma order polynomial update time independent interval width gamma gamma appears promising unexplored alternative gamma memory 
kernel function expressed weighted sum gamma kernels omega mozer weighting coefficients resulting memory expressed terms gamma memory state vectors omega de vries principe show gamma kernels form basis set meaning arbitrary kernel function decays exponentially exists value omega set coefficients fq equation holds 
result easy case delay kernels see simple case required computation amounts essentially convolving complex kernel input sequence time step consequently great practical utility 
contents short term memory having described form memory turn content 
memory hold information pertaining input sequence memory raw input sequence assumed previously 
memory encoding process include additional step input sequence 
transformed new representation 
transformed representation encoded memory 
defined terms case considered previous sections identity transformation memories utilizing transformation called input memories 
models neural net literature classes transformation 
transformation nonlinear vector function transformation results call transformed input ti memory 
generally standard neural net activation function computes weighted sum input elements passes sigmoidal nonlinearity gammaw deltav second nonlinear transformation performed current input current internal memory state 
omega leads transformed input state tis memory 
memories implemented recurrent neural net architecture correspond activities layer hidden units 
note architecture quite similar fairly common recurrent neural net sequence processing architecture elman mozer ll refer standard architecture :10.1.1.117.1928
third tasks target output step prediction input mozer input hidden hidden hidden output input hidden hidden output tis memory neural net architecture 
rectangle represents layer processing units arrow represents complete connectivity layer 
input layer corresponds current element time series hidden layer tis representation second hidden layer memory state 
standard recurrent neural net architecture tis architecture hidden layers collapsed omega 
consider alternative content memory 
holding actual sequence value preceding prediction gamma transformation called output memory 
course tos memories constructed analogy ti tis memories suggesting characterization memory content transformation applied transformation applied input previous output 
great deal sense ignore input sequence information available include category output memories combine input output information difference input output gamma input information available training output information 
memory adaptability final dimension characterizes short term memories adaptability memory 
memory various parameters omega equations specified 
mozer parameters fixed advance call memory static memory state predetermined function input sequence 

task neural net best predictions possible fixed representation input history 
contrast neural net adjust memory parameters memory representation adaptive 
essentially adjusting memory parameters neural net selects limits capacity memory aspects input sequence available making predictions 
addition learning predictions neural net learn characteristics memory specified parameters best facilitate prediction task 
memory model example static memory 
model sketched intrinsically adaptive memory training hidden unit responses network preserve sufficient relevant information 
interesting cases adaptive memory models neural net literature include learning delays waibel hopfield tank learning exponential decay rates bachrach frasconi gori soda mozer learning gamma memory parameters de vries principe 
sequence processing recurrent neural net architecture trained back propagation time bptt rumelhart hinton williams real time recurrent learning rtrl robinson fallside schmidhuber williams zipser adaptive memory model training adjusts equation 
elman srn algorithm elman lies adaptive static memory training procedure amounts back propagation step time powerful full blown back propagation time rtrl :10.1.1.117.1928
static memory models reasonable approach adequate domain knowledge constrain type information preserved memory 
example tank hopfield argue memory high resolution events decreasing resolution distant events 
argument statistical model temporal distortions input 
local temporal uncertainty occurrence input event uncertainty relative time increases expected temporal lag event 
decreasing resolution increasing depth memory suggested argument achieved gamma family 
omega increases state vector 
represents increasing depth decreasing resolution trace input 
memory taxonomy described dimensions neural net memory models vary memory form delay line exponential trace gamma trace content ti tis tos adaptability static adaptive 
cartesian product dimensions yields distinct classes 
table presents combinations memory form content existing models literature class 
delay memory simplest class corresponds feedforward network delay space embedding input sequence 
ti delay memories basis tdnn waibel fir neural net wan volume 
models hidden unit nonlinear transformation input maintains history values values available layer 
ti delay models studied extensively physically oriented neural net community overview see kuhn van hemmen 
herz proposes tis delay memory network dynamics governed lyapunov functional tos varieties omitted received little study 
mozer table taxonomy neural net architectures temporal processing memory memory form contents delay exponential gamma elman zipser lapedes farber zhang hutchinson volume de vries principe ti waibel wan volume sompolinsky bachrach frasconi gori soda mozer tis herz mozer connor atlas martin jordan certain symmetry conditions time delayed weights 
connor atlas martin study nonlinear neural network arma model ma component constructed delay memory retains difference predicted actual outputs identical input 
general nonlinear ma models delay memories 
moving second column table bachrach frasconi gori soda mozer proposed ti exponential memory model described computationally efficient algorithm adapting parameters 
mozer multiscale integration model tis exponential memory 
motivation underlying design recurrent hidden units different time constants integration slow integrators forming coarse global sequence memory fast integrators forming fine grain local memory 
memory adaptive hidden unit response functions learned static fixed 
jordan incorporated exponential memory sequence production network 
training network target output values fed back output memory testing actual outputs 
third column table suggests gamma memory studied form 
de vries principe initial simulations confined gamma memory acknowledge possibilities 
simple memory classes hybrid schemes considered 
tdnn example delay memory attached series ti delay memory modules 
proposed architectures combining delay memory tis exponential memory mozer delay memory ti exponential memory mozer 
possibilities course diverse certain class memory best depends domain task 
taxonomy represents reasonable cut analyzing space memory models comprehensive address issues design memory model 
models literature certain cell taxonomy critical properties ignored taxonomy 
example schmidhuber schmidhuber mozer proposed type delay memory memory size delay parameters dynamically determined input sequence mozer processed 
idea underlying approach discard redundant input elements sequence 
see myers ring variants idea 
part issue network dynamics assuming typical back propagation style activation dynamics 
interesting literature temporal associative networks seek energy minima herz sompolinsky networks operate continuous time dynamics pearlmutter 
directly addresses issue network architecture representation indicated outset focused representational issues 
inadequacy standard recurrent neural net architectures standard recurrent neural net architecture sequence processing tasks amounts trivial case tis exponential memory equivalently tis delay memory omega 
refer limiting case tis memory 
invested years exploring tis architectures growing consensus architecture inadequate difficult temporal processing prediction tasks 
tis architecture sufficiently powerful principle handle arbitrary tasks 
tempted argue hidden units training architecture capable forming memory retains necessary task relevant information 
achieved adjusting parameters equations gradient descent procedure 
practice gradient descent sufficiently powerful discover sort relationships exist temporal sequences especially span long temporal intervals involve extremely high order statistics 
bengio frasconi simard theoretical arguments inherent limitations learning recurrent networks 
mozer illustrates examples relatively simple tasks solved tis memories 
tis memories inadequate tasks ti exponential tis exponential memories yielded significantly improved performance 
cases exponential trace components allowed networks bridge larger intervals time 
evidence consider active neural net speech recognition literature 
know serious attempts speech recognition tis memories 
tdnn architecture ti delay memory viewed quite successful 
interesting explorations ti exponential memories bengio de mori watrous shastri 
tis architectures gradient descent find meaningful values strengths 
gradients computed back propagation bptt rtrl tend quite small 
intuition consider network unfolded time necessary computing gradients bptt 
unfolded network copy original network units connections time step temporal sequence 
turns recurrent network deeply layered feedforward network 
consider consequence changing weight embedded deep unfolded network say weight connecting hidden unit time hidden unit time 
consequence time effect small gradient 
impact connection weight appropriate masked weights values inappropriate 
situation true feedforward nets recurrent nets problem exacerbated careful selection weights prevent gradients shrinking time hochreiter imposes serious restrictions sorts memories network form 
mozer deeply layered structure unfolded net 
result error surface fraught local optima 
situation may hold tis memories may case recurrent architectures specialized connectivity ti exponential memories 
tis memories conjunction types memory may help bootstrap learning promising possibility 
worth investigating classes recurrent network memory taxonomy explored 
best knowledge classes include tis delay ti gamma tis gamma nonlinear ma models delay variety ma models gamma exponential tos varieties 
experiments financial data series remainder chapter experiments examining financial data series prediction task memory types delay tis hybrid approach combining delay tis 
experiments explore sophisticated memory classes described earlier provide baseline experiments compared 
data series studied competition data set bids exchange rate swiss dollars recorded currency trading group august april 
means samples come irregular intervals time demanded traders 
sample indexed time day 
prediction tasks competition focused predicting value series fifteen minutes 
data set appears challenging average sampling rate high relative rate trends data 
nature data obviously noisy smoothing inappropriate may information high frequency changes 
consequently high depth memory necessary 
data set ideal tis architecture principle fixed memory depth 
training data consisted contiguous spans time unspecified gaps total days data 
treated day data independent sequence assumed useful information carried day 
day data transform samples fixed interval samples minute apart operating assumption value series remained constant time sample time sample 
sequence subsampled interval delta minutes 
fifteen minute prediction tasks delta respectively 
average trading day spanned minutes resulting daily series lengths 
days training data complete days chosen random set aside validation testing 
basis validation data parameter values delta selected 
input neural net predictor consisted values day week time day change series value sample delta gamma delta gamma index input sequence denotes sample minute sample day 
target output neural net prediction change series value delta minutes delta delta gamma delta 
input output quantities represented variable encoding ballard means input output activities monotonic represented quantity 
obvious encoding mention simply connectionist approaches representation mozer ballard smolensky offer range alternatives fruitfully explored done 
activity input unit normalized mean variance training set 
leads better conditioned search space learning le cun solla widrow stearns 
target output activity normalized mean variance training set 
despite large variance observed variance unit closer strong predictions due uncertainty data 
general architecture studied shown 
architecture includes memories types tis delay feed inputs directly layer hidden units map output prediction 
symmetric activation function activation range hidden units including hidden units tis memory 
recall tis memory implemented recurrent network internal hidden units see 
output unit linear activation function 
actual architectures tested special cases general architecture 
case eliminated tis memory step delay memory 
number hidden units varied 
number hidden units architecture reduces linear autoregressive model 
second case eliminated delay memory favor tis memory 
tis memory consisted internal hidden units plus hidden units penultimate layer 
third case hybrid architecture included tis delay memories parameters 
choice number hidden units number delays validation testing described 
various networks trained stochastic gradient descent weights updated presentation entire day data 
back propagation time rumelhart hinton williams train recurrent connections tis memory 
sufficiently small learning rates chosen training error decreased reliably time generally set learning rate initially decreased gradually passes training data 
networks initialized weights chosen randomly normal distribution mean zero normalized sum absolute values weights feeding unit 
pass training data errors training validation sets computed 
training network training set error converged commonly done training stopped validation error began rise method suggested weigend rumelhart huberman geman bienenstock doursat 
actuality involved training network convergence observing point validation error began rise restoring saved network weights critical point 
point final passes training data annealing learning rate 
purpose step fine tune weights removing bit jitter 
explored variations architecture training procedure including varying number hidden units varying number delays delay memory trying different values delta removing day week time day inputs generating larger training set transforming day data delta different streams offset minute 
generalization performance estimated variation validation set 
parameters choices previously described yielded performance 
validation set determine training bias introduced validation set selecting architecture 
retrospect distinct validation sets 
mozer day week time day delay memory hidden tis memory general architecture time series experiments 
depicts inputs feeding types memory map layer hidden units single output prediction unit 
arrows represent connections units sets units 
architecture includes direct connections day week time day output depicted 
mozer architecture studied replications performed different random initial weights 
models yielded best validation performance selected predictions competition data averaged form final predictions 
purpose procedure obtain predictions sensitive initial conditions networks 
similar procedures suggested lincoln rosenblatt 
competition test data submit official entry time series competition 
shortly close competition deadline began experimenting data set knowledge series available competition entrants 
tis version architecture obtain minute predictions competition data 
performance series measured units normalized mean squared error nmse 
normalization involved divide mean squared error error obtained assuming value series observed value 
normalization term squares prediction assuming random walk model series 
nmse values indicate structure extracted series 
submitting predictions andreas weigend reported back done bit better entrants competition minute prediction yielded nmse minute prediction nmse 
time series workshop zhang quite validly argued competition data set simply contain sufficient test points ascertain reliability predictions 
agreed perform additional experiments larger test sets 
extended competition test set set experiments utilized competition training set models developed competition test data came gaps training set 
recall training set consisted contiguous time spans gaps 
gaps covered days sufficient extended test set 
data gaps testing exception hour day order provide initialization period recurrent tis architecture 
results minute predictions shown table 
delay architecture tested different numbers hidden units tis model configuration described earlier 
delay architecture zero hidden units equivalent linear autoregressive model ar 
order architectures performed slightly better random walk model better zhang hutchinson volume 
attempted determine reliability differences get far 
straight test squared errors individual predictions yielded significant differences 
noting data distribution strongly violated assumption normality applied log transform squared errors achieve normal distribution came bit closer 
log transform reliable differences obtained pairs conditions differed roughly 
matter performance poor conditions 
zimmermann report performance comparisons broad spectrum tasks variety different training techniques 
explorations included variations techniques relatively simple technique stopping training mozer table nmse extended competition test set architecture minute prediction minute prediction minute prediction data points data points data points delay hidden delay hidden delay hidden delay hidden delay hidden tis table nmse test data architecture minute prediction data points delay hidden delay hidden delay hidden delay hidden tis hybrid tis delay validation set training weight decay appeared tasks 
experimented method obtained results essentially reported table 
larger validation set 
concerned small validation set responsible performance ran experiments validation corpus increased training set manipulation little effect performance 
corpus extended competition test set drawn time period training data 
zhang suggested additional data set testing corpus came different time period testing corpus 
proposed training data exchange rate series spanning time period testing data spanning time period 
encompassed days training data comparable amount testing data 
selected days training data roughly validation testing 
zhang hutchinson explored minute prediction task 
hour day data excluded testing reasons indicated earlier 
results summarized table variety architectures 
delay architecture hidden units equivalent linear ar model performed significantly worse architectures reliable difference architectures 
zhang hutchinson report nmse data set better obtained 
important emphasize performance scores comparable 
zhang hutchinson threw cases training test sets series values available sigma seconds time prediction 
predictions conditional assumptions time occurrence mozer ticks 
point prediction possibly know assumptions valid 
approach predictive model secondary component predicts prediction criteria satisfied 
alternatively retrain retest network data just data satisfying criterion 
making conditional prediction simpler task making unconditional prediction ensures homogeneity cases 
effect throws large number change predictions test set fraction change values versus zhang hutchinson set 
interesting measure performance network correctly predict change direction series change 
explore measure necessary quantize network outputs specify criterion call predictions greater classified second criterion gamma predictions gamma classified 
predictions gamma classified change 
principled approach selecting gamma involves selecting values yield best classification performance training set involves exhaustive search possible values 
simple technique determine chosen fraction training set predictions greater equaled fraction training cases target prediction likewise gamma values gamma test set hidden delay network yielded performance correct predictions 
sounds pretty reality quite poor cases change net obtained performance level simply responding change 
network greater ability predict direction change conditional change having occurred correctly predicts direction test cases predicting frequent category training set obtains performance 
seriously interested predicting direction change sensible train network output units mutually exclusive prediction normalized exponential output function bridle rumelhart press obtain probability distribution alternative responses 
begun exploration alternative architectures temporal sequence processing 
difficult time series prediction problem studied nonlinear extension autoregressive models basic delay architecture fared worse complex architecture hybrid delay tis 
impossible determine prediction limitations due having explored right architecture right preprocessing encoding input sequence right training methodology simply due structure data extracted series aid financial indicators 
studies zhang hutchinson volume data series delay memory different input output encoding training methodology yielded comparable results supporting hypothesis encoding methodology responsible poor predictability 
results serve baseline researchers compare sophisticated architectures representations training methodologies 
look forward reports showing improvements 
defense zhang hutchinson decision believed competition set implied value near prediction time advantage information 
mozer aim presenting taxonomy architectures suggest possibilities space explored 
don claim taxonomy encompassing necessarily best way dividing space possibilities 
reasonable attempt characterizing set models temporal processing commonly field 
andreas weigend neil gershenfeld organizing santa fe workshop particular andreas abundant feedback stages suggestions extensions 
jim hutchinson radford neal jurgen schmidhuber fu sheng anonymous reviewer provided helpful comments earlier draft chapter 
research supported nsf presidential young investigator award iri james mcdonnell foundation dec external research 
bachrach 

learning represent state 
unpublished master thesis university massachusetts amherst 
ballard 

cortical connections parallel processing structure function 
behavioral brain sciences 
bengio de mori 

speaker independent speech recognition neural networks speech knowledge 
touretzky ed advances neural network information processing systems ii pp 

san mateo ca morgan kaufmann 
bengio frasconi simard 

problem learning long term dependencies recurrent networks 
proceedings ieee international conference neural networks 
appear 
waibel 

tempo algorithm adjusting time delays supervised learning 
lippmann moody touretzky eds advances neural information processing systems pp 

san mateo ca morgan kaufmann 
bridle 

training stochastic model recognition algorithms networks lead maximum mutual information estimation parameters 
touretzky ed advances neural information processing systems pp 

san mateo ca morgan kaufmann 


adaptive control processes predictive neural networks technical report 
boulder university colorado joint center energy management 
de vries principe 

theory neural networks time delays 
lippmann moody touretzky eds advances neural information processing systems pp 

san mateo ca morgan kaufmann 
mozer de vries principe 

gamma model new neural net model temporal processing 
neural networks 
elman 

finding structure time 
cognitive science 
elman zipser 

learning hidden structure speech 
journal acoustical society america 
zimmermann 

improving model selection methods 
unpublished manuscript 
frasconi gori soda 

local feedback multilayered networks 
neural computation 
geman bienenstock doursat 

neural networks bias variance dilemma 
neural computation 
herz 

global analysis parallel analog networks retarded feedback 
physical review 
hochreiter 

diploma thesis 
institute fuer informatik technische universitaet muenchen 
jordan 

attractor dynamics parallelism connectionist sequential machine 
proceedings eighth annual conference cognitive science society pp 

hillsdale nj erlbaum 


sequential state generation model neural networks 
proceedings national academy sciences 
kuhn van hemmen 

self organizing maps adaptive filters 
domany van hemmen schulten eds models neural networks pp 

new york springer verlag 
lapedes farber 

nonlinear signal processing neural networks report 
la ur 
los alamos nm los alamos national laboratory 
le cun solla 

second order properties error surfaces learning time generalization 
lippmann moody touretzky eds advances neural information processing systems pp 

san mateo ca morgan kaufmann 
lincoln 

synergy clustering multiple back propagation networks 
touretzky ed advances neural information processing systems pp 

san mateo ca morgan kaufmann 
mozer mcclelland elman 

interactive processes speech perception trace model 
mcclelland rumelhart eds parallel distributed processing explorations microstructure cognition 
volume ii psychological biological models pp 

cambridge ma mit press 
mozer 

focused back propagation algorithm temporal pattern recognition 
complex systems 
mozer 

induction multiscale temporal structure 
moody hanson lippman eds advances neural information processing systems iv pp 

san mateo ca morgan kaufmann 
mozer 

concert connectionist composer tunes 
lippmann moody touretzky eds advances neural information processing systems pp 

san mateo ca morgan kaufmann 
myers 

learning delayed reinforcement attention driven buffering technical report 
london neural systems engineering group department electrical engineering imperial college science technology medicine 
pearlmutter 

learning state space trajectories recurrent neural networks 
neural computation 
plaut nowlan hinton 

experiments learning back propagation technical report cmu cs 
pittsburgh pa carnegie mellon university department computer science 
ring 

incremental development complex behaviors automatic construction sensory motor hierarchies 
birnbaum collins eds machine learning proceedings eighth international workshop pp 

san mateo ca morgan kaufmann publishers 
robinson fallside 

utility driven dynamic error propagation tech report cued infeng tr 
cambridge cambridge university department engineering 
rosenblatt 

principles neurodynamics 
new york spartan 
rumelhart hinton williams 

learning internal representations error propagation 
rumelhart mcclelland eds parallel distributed processing explorations microstructure cognition 
volume foundations pp 

cambridge ma mit press bradford books 
rumelhart 
press 
connectionist processing learning statistical inference 
chauvin rumelhart eds backpropagation theory architectures applications 
hillsdale nj erlbaum 
mozer schmidhuber 

fixed size storage time complexity learning algorithm fully recurrent continually running networks 
neural computation 
schmidhuber 

learning unambiguous reduced sequence descriptions 
moody hanson lippman eds advances neural information processing systems iv pp 

san mateo ca morgan kaufmann 
schmidhuber mozer 

continuous history compression 
manuscript preparation 
smolensky 

tensor product variable binding representation symbolic structures connectionist networks 
artificial intelligence 
sompolinsky 

temporal association asymmetric neural networks 
physical review letters 
stornetta hogg huberman 

dynamical approach temporal pattern processing 
neural information processing systems pp 

new york american institute physics 
tank hopfield 

neural computation concentrating information time 
proceedings national academy sciences 
hopfield tank 

connected digit speaker dependent speech recognition neural network time delayed connections 
ieee transactions signal processing 
waibel hinton shikano lang 

phoneme recognition time delay neural networks technical report tr 
japan atr interpreting telephony research labs 
wan 

finite impulse response neural networks autoregressive time series prediction 
volume 
watrous shastri 

learning acoustic features speech data connectionist networks 
proceedings ninth annual conference cognitive science society pp 

hillsdale nj erlbaum 
weigend huberman rumelhart 

predicting connectionist approach 
international journal neural systems 
weigend huberman rumelhart 

predicting sunspots exchange rates connectionist networks 
casdagli eubank eds nonlinear modeling forecasting proceedings workshop nonlinear modeling forecasting pp 

reading ma addison wesley publishing 
mozer widrow stearns 

adaptive signal processing 
englewood cliffs nj prenticehall 
williams zipser 

learning algorithm continually running fully recurrent neural networks 
neural computation 
zhang hutchinson 

practical issues nonlinear time series prediction 
volume 
