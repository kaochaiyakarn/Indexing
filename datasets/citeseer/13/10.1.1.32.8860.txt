machine learning proceedings fourteenth international conference 
output codes boost multiclass learning problems robert schapire labs mountain avenue room murray hill nj schapire research att com 
describes new technique solving multiclass learning problems combining freund schapire boosting algorithm main ideas dietterich bakiri method error correcting output codes ecoc 
boosting general method improving accuracy base weak learning algorithm 
ecoc robust method solving multiclass learning problems reducing sequence class problems 
show new hybrid method advantages ecoc method requires base learning algorithm binary labeled data 
boosting prove method comes strong theoretical guarantees training generalization error final combined hypothesis assuming base learning algorithm perform slightly better random guessing 
known problems new method may significantly faster require programming effort creating base learning algorithm 
compare new algorithm experimentally voting methods 
boosting general method improving accuracy learning algorithm 
definition boosting algorithm provably convert base weak learning algorithm accuracy just slightly better random guessing arbitrarily high accuracy 
boosting algorithms repeatedly reweighting examples training set rerunning weak learning algorithm reweighted examples 
boosting effectively forces weak learning algorithm concentrate hardest examples 
typically final combined hypothesis weighted vote weak hypotheses 
boosting algorithms discovered schapire freund 
freund schapire boosting algorithm called adaboost shown effective experiments conducted drucker cortes jackson craven freund schapire quinlan breiman :10.1.1.49.2457:10.1.1.133.1040:10.1.1.32.8918
labs planning move murray hill 
new address park avenue florham park nj 
simplest form adaboost requires accuracy weak hypothesis classification rule produced weak learner exceed 
binary classification problems example labeled value requirement minimal hoped random guessing achieve accuracy 
multiclass problems labels possible accuracy may harder achieve random guessing accuracy rate fairly powerful weak learners decision tree algorithms problem 
experimentally cart capable producing hypotheses accuracy difficult distributions examples produced boosting 
accuracy requirement difficulty powerful weak learners simple attribute value tests studied holte jackson craven freund schapire boosting experiments :10.1.1.133.1040
error rate better powerful weak learners expressive weak learners advantage final combined hypothesis usually complicated computation time may reasonable especially large datasets 
freund schapire provide solution problem modifying form weak hypotheses refining goal weak learner :10.1.1.32.8918
approach predicting single class example weak learner chooses set plausible labels example 
instance character recognition task weak hypothesis may predict particular example choosing just single label 
weak hypothesis evaluated pseudoloss measure example penalizes weak hypothesis failing include correct label predicted plausible label set incorrect label included plausible set 
final combined hypothesis example chooses single label occurs frequently plausible label sets chosen weak hypotheses possibly giving weight weak hypotheses 
xm ym ffl compute distribution mg 
ffl compute coloring 
ffl train weak learner examples xm ym weighted ffl get weak hypothesis 
compute coefficients ff ff output final hypothesis final arg max ff generic algorithm combining boosting ecoc 
exact form pseudoloss control boosting algorithm weak learning algorithm designed handle changes form loss measure 
design gives boosting algorithm freedom focus weak learner hard predict examples labels hardest distinguish correct label 
approach works experimentally suffers certain drawbacks :10.1.1.133.1040
requires design weak learner responsive pseudoloss defined boosting algorithm hypotheses generate predictions form plausibility sets 
offthe shelf learning algorithms error may demand extra effort creativity part programmer may completely impossible source code weak learning algorithm unavailable 
second drawback pseudoloss approach fairly slow 
typically running time weak learner times slower algorithm class problem 
describe alternative method boosting multiclass learning algorithms 
method combines boosting dietterich bakiri approach error correcting output codes ecoc designed handle multiclass problems binary learning algorithm 
briefly approach works follows boosting weak learning algorithm need designed class problems rerun repeatedly 
boosting examples reweighted 
round labels assigned example modified create new binary labeling data induced simple mapping set labels 
sequence bit assignments labels viewed code word 
test example classified choosing label associated code word closest hamming distance sequence predictions generated weak hypotheses 
coding theoretic interpretation led dietterich xm ym initialize gamma uniform incorrect labels ffl train weak learner pseudoloss defined ffl get weak hypothesis ffl ffl delta gamma delta ffl ff ln gamma ffl ffl ffl update delta exp gamma ff gamma delta delta normalization factor chosen sum 
output final hypothesis final arg max ff pseudoloss boosting algorithm adaboost 

bakiri beautiful idea choosing code words strong error correcting properties 
algorithm hybrid boosting ecoc approaches 
boosting round rerunning weak learner examples reweighted manner focusing hardest examples 
ecoc labels modified create binary classification problem 
result algorithm combines benefits approaches ecoc weak learning algorithm need able handle binary problems respect ordinary error complicated time consuming pseudoloss 
boosting strong theoretical guarantee weak learner consistently generate weak hypotheses slightly better random guessing respect distribution binary example labeling trained error final combined hypothesis arbitrarily small 
main theoretical result 
rest describe new algorithm detail prove strong theoretical bound error final hypothesis 
describe results experiments comparing new algorithm number voting methods including ecoc boosting 
xm ym initialize 
ffl compute coloring 
ffl 
ffl ffl train weak learner examples xm ym weighted ffl get weak hypothesis 
ffl ffl ffl ff 
ffl compute 
output final hypothesis final arg max ff algorithm adaboost oc combining boosting output coding 
new algorithm boosting ecoc iteratively rounds rerunning weak learning algorithm times 
boosting weak learner trained round new distribution weighting training examples 
ecoc weak learner trained new partition class labels induces new binary labeling data 
key idea algorithm proposed combine approaches round reweight relabel data 
generically algorithm looks 
notation define proposition holds 
algorithm training examples form chosen space associated class label chosen set finite cardinality round algorithm computes distribution training examples function refer coloring partitions label set parts 
data relabeled weak learner trained relabeled data weighted resulting weak hypothesis denoted goal weak learning algorithm minimize training error respect relabeled reweighted data minimize ffl pr id theta examples attributes missing name train test classes disc 
cont 
values soybean small iris glass audiology theta soybean large theta vehicle vowel segmentation splice satimage letter table benchmark machine learning problems experiments 
combined hypothesis final computed 
hypothesis viewed kind weighted vote weak hypotheses 
example interpret binary classification weak hypothesis vote labels labels color selected vote weighted real number ff label receiving weighted votes chosen final classification 
ties broken arbitrarily analysis counted errors 
complete description algorithm need derive reasonable choice coloring coefficients ff reduce pseudoloss method freund schapire development adaboost multiclass versions boosting algorithm :10.1.1.32.8918
reduction lead analysis resulting algorithm 
review adaboost review freund schapire pseudoloss method boosting algorithm adaboost :10.1.1.32.8918
shown 
round boosting algorithm computes distribution mg theta words viewed distribution pairs examples incorrect labels 
idea enable boosting algorithm concentrate weak learner hard examples incorrect labels hardest distinguish correct label 
distribution weak learner computes soft hypothesis power set explained interpret set plausible labels example intuitively easier weak learner identify set labels may plausibly correct selecting single label 
freund schapire allow soft hypotheses take general form functions mapping theta :10.1.1.32.9399:10.1.1.32.8918
soft consider equivalent restricting theirs range 
simplifying restriction special case theirs problem applying results 




soybean small 



iris 



glass 



audiology 



soybean large 



vehicle 



vowel 



segmentation 



splice 



satimage 



letter adaboost oc adaboost ecoc bagging arc comparison learning methods weak learner 
goal weak learner minimize pseudoloss ffl delta gamma delta loss measure penalizes weak hypothesis failing include correct label plausible set associated example penalizes incorrect label included plausible set 
recall correct labels contribute sum 
note pseudoloss pseudoloss obtained trivially setting freund schapire adaboost algorithm works increasing round weight placed examples incorrect labels contribute pseudoloss :10.1.1.32.9399:10.1.1.32.8918
combined hypothesis chooses single label occurs largest number plausible label sets chosen weak hypotheses votes weak hypotheses count 
ffl gamma fl freund schapire theorem show training error combined hypothesis final adaboost bounded gamma gamma fl gamma exp gamma fl fl bounded away training error goes zero exponentially fast 
note weak hypotheses evaluated respect pseudoloss final hypothesis final analyzed respect usual error measure 
freund schapire give method bounding generalization error combined hypothesis schapire come better analysis voting methods adaboost 
analysis yields bounds generalization error terms fl number training examples measure complexity weak hypothesis space independent number rounds boosting 
output coding pseudoloss ready describe new hybrid algorithm 
returning suppose weak hypothesis computed respect coloring 
mentioned earlier binary classification example viewed vote labels said differently labels identified plausible reducing pseudoloss setting natural identify soft hypothesis defined gamma choice soft hypothesis update distribution defined adaboost 
see turn defined sensibly terms 



soybean small 



iris 



glass 



audiology 



soybean large 



vehicle 



vowel 



segmentation 



splice 



satimage 



letter adaboost oc adaboost ecoc bagging arc comparison learning methods weak learner 
important relate pseudoloss error pseudoloss computed calculation gamma delta gamma delta gamma indicates coloring differs correct label indicates incorrect ith relabeled example 
convenient choice turns recall ffl training error eq 
pseudoloss ffl gamma gamma ffl definition ffl definition pseudoloss ffl expressed simply terms error ffl setting ffl gamma fl eq 
ffl gamma fl error ffl slightly better random guessing error rate pseudoloss slightly better provided 
resulting algorithm called adaboost oc shown 
method derivation algorithm fact special case adaboost weak soft hypothesis particular form 
immediately apply results freund schapire obtain theorem main theoretical result theorem sequence colorings ht sequence weak hypotheses returned weak learner :10.1.1.32.8918
ffl gamma fl error respect relabeled reweighted data trained eq 


training error final hypothesis final algorithm adaboost oc bounded gamma gamma fl gamma exp gamma fl 



soybean small 



iris 



glass 



audiology 



soybean large 



vehicle 



vowel 



segmentation 



splice 



satimage 



letter adaboost oc adaboost ecoc bagging arc comparison learning methods weak learner 
proof theorem follows directly eq 
argument relationship error weak hypotheses pseudoloss associated weak soft hypotheses 
show methods choosing coloring gives value possibly expectation 
plugging bound theorem gives bound gamma gamma fl gamma exp gamma fl training error 
note bound approaches zero exponentially fast fl bounded away zero 
weak learner perform just slightly better random guessing binary problems trained fl lower bounded fl training error final hypothesis quickly arbitrarily small 
simplicity focused training error generalization error bounded methods schapire 
leads bound generalization error combined hypothesis form gamma gamma fl gamma fl log log log ffi holds probability gamma ffi vc dimension weak hypothesis space weak learner 
second term small sample size sufficiently large relative vc dimension 
training error small values term drops zero exponentially fast fl bounded away zero 
details omitted lack space 
choosing coloring remains show choose coloring theorem clear want choose maximize note value depends weak hypothesis 
means attempt find maximizing prior calling weak learner 
propose number options choosing simplest option choose value uniformly independently random label probability exactly 
expected value exactly 
slightly refined method choose random ensuring near split labels 
choose uniformly random colorings soybean small iris glass 
audiology 

soybean large vehicle 
vowel 


segmentation 


splice 




satimage 





letter adaboost oc adaboost comparison computation time versus error rate achieved adaboost oc adaboost weak learner 
exactly bk labels mapped 
shown probability gamma expected value value slightly better 
method experiments section 
method attempt combinatorial optimization methods maximize eq 
rewritten 
written form straightforward show maximizing special case max cut problem known np complete various sophisticated known 
attempt methods plausible improve performance 
addition various greedy hill climbing methods guarantee 
experiments methods attempted give significant improvement reported section details reported 
experiments tested method experimentally collection eleven multiclass benchmark problems available repository university california irvine 
characteristics benchmarks summarized table 
test set provided experiments run times results averaged learning algorithms randomized 
test set provided fold cross validation rerun times total runs algorithm 
weak learner algorithms varying degrees expressiveness 
simplest called outputs hypothesis prediction result single test comparing attributes possible values 
discrete attributes equality tested continuous attributes threshold value compared 
best hypothesis form minimizes error pseudoloss direct efficient search method 
weak learners similar spirit studied holte 
second weak learner called outputs hypothesis tests conjunction attribute value comparisons 
rule built entropic potential pruned back held data 
method loosely rule formation part cohen ripper algorithm furnkranz widmer irep algorithm 
algorithms described detail freund schapire :10.1.1.133.1040
note url www ics uci edu mlearn mlrepository html soybean small iris 

glass 

audiology 

soybean large 

vehicle 


vowel 


segmentation 



splice 


satimage 



letter adaboost oc adaboost comparison computation time versus error rate achieved adaboost oc adaboost weak learner 
algorithms find single rule entire problem opposed learning rule class 
weak learner tested quinlan algorithm default options pruning turned 
handle weighted examples round boosting reran unweighted examples randomly resampled compared adaboost oc boosting ecoc adaboost error multiclass version adaboost called adaboost 
purpose derive algorithm effective boosting requires error pseudoloss weak learner compared algorithm various methods combine weak hypotheses 
ffl dietterich bakiri ecoc method 
searching error correcting code chose output code random selecting random nearly split described section 
random code highly error correcting properties certainly plausible carefully designed code perform better results reported 
ffl breiman bagging algorithm weak learner randomly chosen bootstrap samples 
ffl breiman arc algorithm ad adaptively data different rule computing distribution examples manner require weak hypotheses error 
algorithm shown breiman mimic performance ad cart theoretical properties unknown 
algorithm run rounds 
results shown figures 
axis shows number rounds axis shows test error algorithm percent 
note log scale figures 
advantage adaboost oc adaboost cases audiology performance worse adaboost oc 
expect time savings case comparing adaboost error 
expressive weak learners see datasets adaboost oc adaboost quickly match performance methods usually clearly superior performance 
round round adaboost oc unable keep adaboost 
savings computation time significant error algorithms usually faster pseudoloss algorithms 
figures show plot computation time seconds versus test error rate algorithms 
time figures include training time evaluation test data 
plots show computation time limited performance adaboost oc surpass adaboost 
described new method training combinations classifiers comes theoretical guarantees boosting algorithm output coding algorithm requires weak learner capable handling binary classification problems 
highly expressive weak learners experiments show method compromise test error performance keep pseudoloss boosting 
hand time savings relative ease programming weak learning algorithm significant 
acknowledgments contributed datasets 
leo breiman 
bagging predictors 
machine learning 
leo breiman 
bias variance arcing classifiers 
technical report statistics department university california berkeley 
william cohen 
fast effective rule induction 
proceedings twelfth international conference machine learning pages 
thomas dietterich bakiri 
solving multiclass learning problems error correcting output codes 
journal artificial intelligence research january 
harris drucker corinna cortes 
trees 
advances neural information processing systems pages 
yoav freund 
boosting weak learning algorithm majority 
information computation 
yoav freund robert schapire :10.1.1.133.1040
experiments new boosting algorithm 
machine learning proceedings thirteenth international conference pages 
yoav freund robert schapire :10.1.1.32.8918
decisiontheoretic generalization line learning application boosting 
journal computer system sciences appear 
extended appeared eurocolt 
johannes furnkranz gerhard widmer 
incremental reduced error pruning 
machine learning proceedings eleventh international conference pages 
michel goemans david williamson 
improved approximation algorithms maximum cut satisfiability problems semidefinite programming 
journal association computing machinery november 
oded goldreich shafi goldwasser dana ron 
property testing connection learning approximation 
th annual symposium foundations computer science pages 
robert holte 
simple classification rules perform commonly datasets 
machine learning 
jeffrey jackson mark craven 
learning sparse perceptrons 
advances neural information processing systems pages 
karp 
reducibility combinatorial problems 
miller thatcher editors complexity computer computations pages 
plenum press 
quinlan :10.1.1.49.2457
bagging boosting 
proceedings thirteenth national conference artificial intelligence pages 
ross quinlan 
programs machine learning 
morgan kaufmann 
robert schapire 
strength weak learnability 
machine learning 
robert schapire yoav freund peter bartlett wee sun lee 
boosting margin new explanation effectiveness voting methods 
machine learning proceedings fourteenth international conference 

