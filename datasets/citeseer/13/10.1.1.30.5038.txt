unified bias variance decomposition applications pedro domingos cs washington edu department computer science engineering university washington seattle wa presents unified bias variance decomposition applicable squared loss zero loss variable misclassification costs loss functions 
unified decomposition sheds light number significant issues relation previously proposed decompositions zero loss original squared loss relation bias variance schapire notion margin nature trade bias variance classification 
behavior zero loss variable misclassification costs quite di erent squared loss di erence derives directly di erent definitions loss 
applied proposed decomposition decision tree learning instancebased learning boosting large suite benchmark data sets significant observations 

bias variance decomposition key tool understanding machine learning algorithms years empirical studies grown rapidly 
notions bias variance help explain simple learners outperform sophisticated ones model ensembles outperform single models 
bias variance decomposition originally derived squared loss see example geman 

authors proposed corresponding decompositions zero loss 
decompositions significant shortcomings 
kong dietterich decomposition allows variance negative ignores noise component misclassification error 
breiman decomposition undefined example defined instance space allows variance zero undefined learner predictions fluctuate response training set 
tibshirani defines bias variance decomposes loss bias aggregation ect quantity unrelated definition variance 
james hastie extend approach defining bias variance decomposing loss terms quantities call systematic ect variance ect 
kohavi wolpert decomposition allows bias bayes optimal classifier nonzero 
friedman decomposition relates zero loss squared loss bias variance class probability estimates leaving bias variance zero loss undefined 
cases decomposition zero loss stated terms zero bias variance developed independently original squared loss clear relationship 
propose single definition bias variance applicable loss function show resulting decomposition zero loss su er shortcomings previous decompositions 
show notions order correctness breiman margin schapire previously proposed explain model ensembles reduce error reduced bias variance defined :10.1.1.31.2869
provide knowledge bias variance decomposition variable misclassification costs 
carry large scale empirical study measuring bias variance machine learning algorithms variety conditions extracting significant patterns 

unified decomposition training set 
learner produces model test example model produces prediction 
sake simplicity fact function remain implicit 
true value predicted variable test example loss function measures cost predicting true value commonly loss functions squared loss absolute loss zero loss 
goal learning stated producing model smallest possible loss model minimizes average examples example weighted probability 
general nondeterministic function sampled repeatedly different values seen 
optimal prediction example prediction minimizes subscript denotes expectation taken respect possible values weighted probabilities optimal model model general model non zero loss 
case zero loss optimal model called bayes classifier loss called bayes rate 
learner general produce di erent models di erent training sets function training set 
dependency removed averaging training sets 
particular training set size important parameter learning problem want average training sets size 
set training sets 
quantity interest expected loss ed expectation taken respect training sets respect predictions produced example applying learner training set 
bias variance decompositions decompose expected loss terms bias variance noise 
standard decomposition exists squared loss number di erent ones proposed zero loss 
order define bias variance arbitrary loss function need define notion main prediction 
definition main prediction loss function set training sets argmin ed 
danger ambiguity represent simply ym expectation taken respect training sets respect predictions produced learning training sets multiset predictions 
specific prediction appear produced training set 
words main prediction value average loss relative predictions minimum prediction di ers predictions 
main prediction squared loss mean predictions absolute loss median zero loss mode frequent prediction 
example training sets learn classifier classifiers predict class predict main prediction zero loss class 
main prediction necessarily member example main prediction squared loss 
define bias variance follows 
definition bias learner example ym 
words bias loss incurred main prediction relative optimal prediction 
definition variance learner example ed 
words variance average loss incurred predictions relative main prediction 
bias variance may averaged examples case refer average bias average variance 
convenient define noise follows 
definition noise example 
words noise unavoidable component loss incurred independently learning algorithm 
definitions intuitive properties associated bias variance measures 
ym measure central tendency learner 
central means depends loss function 
measures systematic loss incurred learner measures loss incurred fluctuations central tendency response di erent training sets 
loss function nonnegative bias variance nonnegative 
bias independent training set zero learner optimal prediction 
variance independent true value predicted variable zero learner prediction regardless training set 
property definitions require loss function expected value computable 
necessarily case expected loss ed loss function decomposed bias variance defined 
approach propose decomposition show applies di erent loss functions 
apply may worthwhile investigate expected loss expressed function 
consider example true prediction learner predicts training set certain loss functions decomposition ed holds ed ym ed multiplicative factors take di erent values di erent loss functions 
easily seen decomposition reduces standard squared loss considering squared loss ym ed geman ed ed ed show decomposition applies broad class loss functions class problems including zero loss 
extend multiclass problems zero loss 
pd probability training sets learner predicts optimal class theorem class problems equation valid real valued loss function pd pd ym ym ym 
proof 
showing 
equation trivially true 
true reduces 
true reduces 
class problem true 
true completing proof equation 
show similar manner ym ym ym ym ym 
ym equation trivially true 
ym ym ym ym ym true reduces ym ym 
ym ym ym ym true reduces ym ym 
ym problem ym true 
ym true ym ym ym ym completing proof equation 
equation considering depend depend ed ed ed ed ed substituting equation considering ed pd pd results equation 
particular loss function symmetric reduce pd ym 
specifically applies zero loss yielding decomposition similar kong dietterich 
main di erences kong dietterich ignored noise component defined variance simply di erence loss bias apparently unaware absolute value difference average loss incurred relative frequent prediction 
side ect kong dietterich incorporate definition variance negative 
kohavi wolpert criticized fact variance squared loss positive 
decomposition shows subtractive effect variance follows self consistent definition bias variance zero squared loss variance remains positive 
fact variance additive unbiased examples subtractive biased ones significant consequences 
learner biased example increasing variance decreases loss 
behavior markedly di erent squared loss obtained definitions bias variance purely result di erent properties zero loss 
helps explain highly unstable learners decision tree rule induction algorithms produce excellent results practice limited quantities data 
ect zero loss evaluation criterion higher tolerance variance bias variance decomposition purely additive increase average loss caused variance unbiased examples partly set set decrease biased ones 
average loss examples sum noise average bias termed net variance ed averaging equation test examples positive unbiased examples negative biased ones 
factor see theorem points key difference zero squared loss 
squared loss increasing noise increases error 
loss training sets test examples increasing noise decreases error high noise level principle beneficial performance 
general case theorem important 
practical applications machine learning loss highly asymmetric example classifying patient healthy costly reverse 
cases theorem essentially shows loss reducing ect variance biased examples greater smaller depending asymmetric costs direction greater 
equation apply case decomposition contains additional term corresponding cost correct predictions 
applies general multiclass case open problem 
applies general multiclass problem zero loss described theorem 
theorem equation valid zero loss multiclass problems pd pd ym pd ym 
omit proof interests space see domingos 
theorem means multiclass problems variance biased examples contributes reducing loss training sets ym loss reduced 

properties unified decomposition main concepts breiman explain bagging ensemble method reduces loss order correct learner 
learner order correct example pd pd 
breiman showed bagging transforms order correct learner nearly optimal 
bias closely related learner order correct example zero loss 
proof immediate definitions considering ym zero loss frequent prediction 
schapire 
proposed explanation boosting ensemble method works terms notion margin 
algorithms bagging boosting generate multiple hypotheses applying learner multiple training sets definition margin stated follows 
definition schapire class problems margin learner example pd pd :10.1.1.31.2869
positive margin indicates correct classification ensemble negative error 
intuitively large margin corresponds high confidence prediction 
set training sets learner applied 
example rounds boosting carried 
algorithms boosting di erent training sets corresponding predictions di erent weights sum pd 
computed weights 
definitions apply unchanged situation 
ect generalized notions bias variance apply training set selection scheme simply traditional possible training sets size equal weights 
schapire 
showed possible bound ensemble generalization error loss test examples terms distribution margins training examples vc dimension base learner 
particular smaller probability low margin lower bound generalization error 
theorem shows margin closely related bias variance defined 
theorem margin learner example expressed terms zero bias variance positive sign negative sign 
proof 
pd pd pd 
ym pd ym 


demonstration similar pd pd 
conversely possible express bias variance terms margin sign positive sign negative sign 
relationship margins bias variance expressed theorem implies schapire theorems stated terms bias variance training examples 
bias variance decompositions relate learner loss example bias variance example 
knowledge time generalization error related bias variance training examples 
theorem sheds light breiman schapire 
success ensemble methods bagging boosting best explained 
breiman argued bias variance explanation schapire argued margin explanation 
theorem shows faces coin helps explain bias variance explanation fail applied boosting 
maximizing margins combination reducing number biased examples decreasing variance unbiased examples increasing biased ones examples reverse 
di erentiating ects hard understand boosting ects bias variance 

experiments applied bias variance decomposition loss proposed series experiments classification algorithms 
knowledge extensive study date terms number data sets number algorithms parameter settings studied 
section summarizes results 
data sets uci repository blake merz annealing audiology breast cancer ljubljana chess king rook vs king pawn credit australian diabetes echocardiogram glass heart disease cleveland hepatitis horse colic hypothyroid iris labor led lenses liver disorders lung cancer lymphography mushroom post operative primary tumor promoters solar flare sonar soybean small splice junctions voting records wine zoology 
noise level di cult estimate followed previous authors kohavi wolpert assuming 
detrimental significance results mainly interested variation bias variance factors absolute values 
estimated bias variance zero loss method 
randomly divided dataset training data thirds examples test data third 
dataset generated di erent training sets bootstrap method efron tibshirani training data consists examples create bootstrap replicate samples replacement example having probability selected turn 
result examples appear training set 
training sets obtained taken sample set set training sets size model learned training set 
predictions models test examples estimate average zero loss average bias net variance defined section 
measured total contribution average variance unbiased examples contribution biased examples cb test example number test examples class problems pd ym multiclass problems see theorem estimated test set 
net variance di erence carried experiments decision tree induction boosting nearest neighbor results reported turn 
space limitations preclude presentation complete results see domingos 
summarize main observations representative examples 
decision tree induction decision tree learner release quinlan 
measured zero loss bias variance varying pruning parameter confidence level cf maximum pruning minimum steps 
default setting 
surprisingly data sets cf minor ect bias variance loss 
cf extreme tree pruned way root major impact high bias loss disappears cf 
results suggest may room improvement pruning method cf 
oates jensen 
order obtain clearer picture bias variance trade decision tree induction replaced native pruning scheme limit number levels allowed tree 
maximum level set path tree length greater pruned back length 
dominant ect observed rapid decrease bias levels typically stabilizes 
data sets occurs bias fact increases point slightly markedly 
data sets bias increases number levels echocardiogram post operative sonar increases markedly 
variance increases number levels data sets increase generally slower initial decrease bias 
regular patterns occur remaining data sets 
tend initially similar increases slowly decreases 
level typically sets large fraction making variance smaller contributor loss case ect positive 
leads hypothesis higher variance algorithms settings may better suited classification zero loss regression squared loss 
coincidentally research classification tended explore higher variance algorithms research regression 
representative examples patterns observed shown highest level shown highest produced runs limits 
expected pattern tradeo bias variance leading minimum loss intermediate level observed data sets decision stump best unlimited number levels best 
boosting experimented applying adaboost freund schapire 
allowed maximum rounds boosting 
data sets loss components stabilized th round part graphed 
boosting decreases loss data sets increases ect remainder 
decreases bias data sets increases decreasing net variance data sets increasing 
bulk bias reduction typically occurs rounds 
variance reduction tends gradual 
average data sets variance reduction larger contributor loss reduction bias reduction vs 
data sets variance reduction significant level sign wilcoxon tests bias reduction 
variance reduc loss level vu vb loss level vu vb 
ect varying number levels trees glass top primary tumor bottom 
tion clearly dominant ect boosting applied consistent notion strong learner 
boosting tends reduce reduces strongly vs 
ideal behavior reduce increase may possible design variant boosting achieves result reduces loss 
examples boosting behaviors observed shown 
nearest neighbor studied bias variance nearest neighbor algorithm cover hart function number neighbors predict test example class 
euclidean distance numeric attributes overlap symbolic ones 
varied increments typically small values extended range allows clearer observation ect 
pattern loss round vu vb loss round vu vb 
ect boosting audiology top splice junctions bottom 
increase bias decrease variance producing minimal loss intermediate value seldom observed effects dominates 
cases bias variance vary direction data sets lowest loss obtained maximum average data sets bias increases markedly variance decreases slightly resulting increased loss 
contradicts friedman hypothesis approximate analysis artificial data large values beneficial 
may attributable fact increases friedman calls boundary bias changes negative positive majority examples benefits low variance 
interestingly increasing nn ideal ect reducing average increasing 
shows examples di erent types behavior observed 
loss vu vb loss vu vb 
ect varying nearest neighbor audiology top chess bottom 

main limitation definitions bias variance proposed loss functions decomposed equation absolute loss 
meaningful definitions exist simple decomposition possible central direction determining general properties loss functions necessary su cient equation apply 
may possible usefully relate loss bias variance defined 
example domingos show long loss function metric bounded linear functions bias variance noise 
major direction applying decomposition wider variety learners order gain insight behavior respect variations method respect comparisons methods 
study experimentally ect di erent domain characteristics sparseness data bias variance di erent learning algorithms 
resulting improved understanding allow design learners easily adapted wide range domains 

proposed unified definitions bias variance applicable loss function 
resulting decomposition specializes conventional squared loss avoids di culties previous ones zero loss applicable variable misclassification costs 
decomposition purely additive believe insight gained approach formulating consistent definitions investigating follows crafting definitions case case decomposition purely additive 
example uncovering di erent role variance biased unbiased examples zero loss leads improved understanding classification algorithms di er regression ones 
illustrated extensive empirical study bias variance decision tree induction boosting nearest neighbor 
bias variance decomposition proposed available code www cs washington 
edu homes acknowledgments research partly supported praxis xxi 
author grateful provided data sets experiments 
blake merz 

uci repository machine learning databases 
department information computer science university california irvine irvine ca 
www ics uci edu mlearn mlrepository html 
breiman 

bagging predictors 
machine learning 
breiman 

bias variance arcing classifiers technical report 
statistics department university california berkeley berkeley ca 
breiman 

arcing edge technical report 
statistics department university california berkeley berkeley ca 
cover hart 

nearest neighbor pattern classification 
ieee transactions information theory 
domingos 

unified bias variance decomposition technical report 
department computer science engineering university washington seattle wa 
efron tibshirani 

bootstrap 
new york ny chapman hall 
freund schapire 

experiments new boosting algorithm 
proceedings thirteenth international conference machine learning pp 

bari italy morgan kaufmann 
friedman 

bias variance loss curse dimensionality 
data mining knowledge discovery 
geman bienenstock doursat 

neural networks bias variance dilemma 
neural computation 
james hastie 

generalizations bias variance decomposition prediction error technical report 
department statistics stanford university stanford ca 
kohavi wolpert 

bias plus variance decomposition zero loss functions 
proceedings thirteenth international conference machine learning pp 

bari italy morgan kaufmann 
kong dietterich 

errorcorrecting output coding corrects bias variance 
proceedings twelfth international conference machine learning pp 

tahoe city ca morgan kaufmann 
oates jensen 

ects training set size decision tree complexity 
proceedings fourteenth international conference machine learning pp 

madison wi morgan kaufmann 
quinlan 

programs machine learning 
san mateo ca morgan kaufmann 
schapire freund bartlett lee 

boosting margin new explanation ectiveness voting methods 
proceedings fourteenth international conference machine learning pp 

nashville tn morgan kaufmann 
tibshirani 

bias variance prediction error classification rules technical report 
department statistics university toronto toronto canada 
