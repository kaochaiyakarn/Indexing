hierarchical memory reinforcement learning hernandez arti cial intelligence lab massachusetts institute technology cambridge ma ai mit edu sridhar mahadevan department computer science michigan state university east lansing mi cse msu edu key challenge reinforcement learning scaling large partially observable domains 
show hierarchy behaviors create select variable length short term memories appropriate task 
higher levels hierarchy agent abstracts lower level details looks back variable number high level decisions time 
formalize idea framework called hierarchical sux memory hsm 
hsm uses memory smdp learning method rapidly propagate delayed reward long decision sequences 
describe detailed experimental study comparing memory vs hierarchy hsm framework realistic corridor navigation task 
reinforcement learning encompasses class machine learning problems agent learns experience interacts environment 
fundamental challenge faced reinforcement learning agents real world problems state space large consequently may long delay reward received 
previous addressed issue breaking large task hierarchy subtasks behaviors :10.1.1.9.313
dicult issue problem perceptual aliasing di erent real world states generate observations 
strategy deal perceptual aliasing add memory past percepts 
short term memory consisting linear tree sequence primitive actions observations shown useful strategy 
considering short term memory uniform resolution primitive actions scale poorly tasks long decision sequences 
just spatio temporal abstraction state space improves scaling completely observable environments large partially observable environments similar bene may result consider space past experience variable resolution 
task want hierarchical strategy rapidly bringing bear past experience appropriate grain size decisions considered 
junction corner dead abstraction level navigation abstraction level traversal abstraction level primitive gure illustrates memory decision making levels hierarchy navigation task 
level decision point shown star examines past experience nd states similar history shown shadows 
navigation level observations decisions occur intersections 
lower corridor traversal level observations decisions occur corridor 
show considering past experience variable resolution speed learning greatly improve performance perceptual aliasing 
resulting approach call hierarchical sux memory hsm general technique solving large perceptually aliased tasks 
hierarchical sux memory employing short term memory decisions involves hierarchy behaviors apply memory informative level abstraction 
important side ect agent look decision point steps back time ignoring exact sequence low level observations actions 
illustrates hsm framework 
problem learning perceptual aliasing viewed discovering informative sequence past actions observations history sux world state enables agent act optimally world 
think situation agent choose action choice point labeled pair refers abstraction level refers history sux 
completely observable case length decisions current observation 
partially observable case additionally consider past history making decisions 
case sux sequence past observations actions learned 
idea representing memory variable length sux derives learning approximations probabilistic sux automata 
general hsm procedure including model free model updates 
abstraction level choice point potential decision examine history level nd set past choice points executed incoming sux history closely matches current point 
call set instances voting set decision 
choose decision highest average discounted sum reward voting set 
occasionally choose exploration strategy 
event counter current choice point level 
execute decision record resulting observation reward received duration action measured number primitive environment transitions executed action 
note environment transition state state reward discount accumulate reward update discount factor 
update value current decision point instance voting set decision reward duration values recorded instance 
model free smdp learning update rule learning rate max model state transition model sweep value iteration executed state corresponding decision point time represented sux nd estimated immediate reward executing decision choice point estimated probability agent arrives executed utility situation average duration transition action hsm requires technique short term memory 
implemented nearest sequence memory nsm utile sux memory usm algorithms proposed mccallum 
nsm records raw experiences linear chain 
choose action agent evaluates outcomes nearest neighbors experience chain 
nsm evaluates closeness states match length sux chain preceding states 
chain grown inde nitely old experiences replaced chain reaches maximum length 
nsm model free learning method hsm uses smdp learning rule described 
usm records experience linear time chain 
attempting choose actions greedy history match usm tries explicitly determine memory useful predicting reward 
agent builds tree structure state representation online selectively adding depth tree additional history distinction helps predict reward 
usm learns model hsm updates values doing sweep value iteration leaves tree states 
implement hierarchy behaviors principle hierarchical reinforcement learning method may 
implementation hierarchy machines ham framework proposed parr russell 
executed machine executes partial policy returns control caller termination 
ham architecture uses learning rule modi ed smdps 
context state represented history sux 
instance state instance incoming history matches sux representing state 
case voting set exactly set instances state current choice point corridor environment nomad robot simulator 
goal way junction 
robot shown middle junction 
robot equipped short range infrared long range sonar sensors 
gures environment obstacles robot maneuver 
navigation task test hsm framework devised navigation task simulated corridor environment see 
task robot nd way start center junction goal way junction 
robot receives reward goal intersection small negative reward primitive step taken 
primary testbed simulated agent nomad robot simulator 
simulated robot equipped bumper sonar infrared sensors arranged radially 
dynamics simulator grid world dynamics nomad simulator represents continuous noisy sensor input occasional unreliability actuators 
environment presents signi cant perceptual ambiguity 
additionally sensor readings noisy agent goal intersection see 
note size robot relative environment 
task dicult activities executed concurrently 
conceptually levels navigation problem 
top level root task navigating goal 
lower level task physically traversing corridors avoiding obstacles maintaining alignment walls implementation learning agents experiments compared learning agents basic ham agent agents hsm di erent short term memory technique nsm agent 
build set behaviors hallway navigation level hierarchy 
top level basically choice state choosing hallway navigation direction see 
nominal directions front back left right agent observations wall open unknown 
agent learn choose machines reach follow wall obstacle avoid obs obstacle intersection start intersection choose maintain heading avoid obs follow wall intersection choose intersection go forward go left go right go backward goal go forward go forward hierarchical structure behaviors hallway navigation 
shows level responsible navigating environment 
figures show implementations hall traversal machines 
machine reactive machine choice point 
intersection 
top level machine control initially control intersections 
second level hierarchy contains machines traversing hallway 
traversal behavior shown 
machines level executes reactive strategy traversing corridor 
third level hierarchy implements follow wall avoid obstacle strategies primitive actions 
avoid obstacle follow wall strategies trained previously learning exploit power reuse hierarchical framework 
ham agent uses level behavior hierarchy described 
single choice state top level agent learns coordinate choices keeping table values 
value table indexed current percepts chosen action machines 
ham agent uses discount learning rate 
exploration done simple epsilon greedy strategy 
rst pair hsm agents behavior hierarchy ham agent 
short term memory level learn strategy navigating corridor 
rst agents uses nsm top level history length discount learning rate 
second agent uses usm top level discount 
performance top level memory agents studied control complex multi level memory agents described 
pair hsm agents short term memory navigation level intermediate level 
behavior decomposition navigation level previous agents traversal behavior turn composed machines decision short term memory 
machines traversal level uses short term memory learn coordinate strategy behaviors traversing corridor 
memorybased version traversal machine shown 
rst agents uses nsm short term memory technique levels hierarchy 
uses history length discount learning rate 
second agent uses usm short term memory technique top level discount 
intermediate level uses nsm learning parameters preceding agent 
exploration done simple epsilon greedy strategy cases 
study behavior nsm agent 
agent keep track perceptual data rst needs perceptual information top level ham identify goal second needs additional perceptual data aligning walls avoiding obstacles bumped angle wall binned groups 
agent chooses primitive actions go forward veer left veer right back 
learn goal simultaneously learn align walls avoid obstacles 
nsm agent uses history length discount learning rate 
exploration done simple epsilon greedy strategy 
experimental results see learning performance agent navigation task 
graphs show performance advantage multi level hsm agents agents 
particular nd memory agent considerably worse expected 
agent carry perceptual data perform high low level behaviors 
point view navigation results long strings uninformative corridor states informative intersection states 
takes agent longer discover patterns experience quite learns navigate successfully goal 
multi level memory hierarchical agents outperform ham agent 
ham agent better navigation agent abstracts away perceptually aliased corridor states 
unable distinguish intersections 
ability tell lead goal dead ham agent perform 
multi level hsm agents outperform single level ones 
multi level agents tune traversing strategy characteristics cluttered hallway short term memory intermediate level 
initially worse multi level hsm agent usm soon outperforms multi level hsm agent nsm 
usm algorithm forces agent learn state representation uses incoming history needed predict reward 
tries learn right history sux situation approximating sux simply matching greedily incoming history 
learning representation takes time learned produces better performance 
described framework solving large perceptually aliased tasks called hierarchical sux memory hsm 
approach uses hierarchical behavioral structure index past memory multiple levels resolution 
organizing past experience hierarchically scales better problems long decision sequences 
experiment comparing di erent learning methods showing hierarchical short term memory produces best performance number primitive steps multi level memory usm ham multi level memory nsm ham memory ham flat memory nsm number primitive steps multi level memory usm ham multi level memory nsm ham top level memory usm ham top level memory nsm ham learning performance navigation task 
curve averaged trials agent 
perceptually aliased corridor navigation task 
key limitation current hsm framework abstraction level examines history level 
allowing interaction memory streams level hierarchy bene cial 
consider navigation task decision intersection depends observation seen traversing corridor 
case level ability zoom inspect particular low level experience greater detail 
expect pursuit general frameworks hsm manage past experience variable granularity lead strategies control able gracefully scale large partially observable problems 
research carried rst author department computer science engineering michigan state university 
research supported part national science foundation ecs 
thomas dietterich 
maxq method hierarchical reinforcement learning 
autonomous robots journal special issue learning autonomous robots 
andrew mccallum 
reinforcement learning selective perception hidden state 
phd thesis university rochester 
ron parr 
hierarchical control learning markov decision processes 
phd thesis university california berkeley 
dana ron yoram singer naftali tishby 
power amnesia learning probabilistic automata variable mem ory length 
machine learning 
sutton precup singh 
intra option learning temporally actions 
proceedings th international conference machine learning pages 
