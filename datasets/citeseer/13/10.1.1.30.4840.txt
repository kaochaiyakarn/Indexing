maximum likelihood blind separation deconvolution noisy signals mixture models eric moulines jean francois cardoso elisabeth ecole nationale sup erieure des el enst epartement signal rue paris cedex france fax email moulines sig enst fr approximate maximum likelihood method blind source separation deconvolution noisy signal proposed 
technique relies data augmentation scheme unobserved input viewed missing data 
technique described contribution input signal distribution modeled mixture gaussian distributions enabling explicit formula computing posterior density conditional expectation avoiding monte carlo integrations 
technique able capture salient features input signal distribution performs generally better third order fourth order cumulant techniques 

contribution devoted blind source separation blind deconvolution 
models observed data fx theta vector assumed gamma fs theta unobserved input fv unobserved additive noise gammak unknown theta fir transfer function 
assumed sequel fv additive zero mean gaussian noise positive definite theta matrix fs sequence theta random vectors independent components density function fl fl fl fl fl delta delta delta fl fs fv independent 
model arises signal processing applications sonar array processing multichannel data communications 
variety methods criteria proposed solve problem 
criteria contrast functions higherorder statistics see 
concentrate maximum likelihood approach 
denote delta delta delta fl 
inference delta delta delta standard parametric problem maximum likelihood estimator appropriate regularity conditions may shown asymptotically efficient 
computation estimate difficult task distribution observed data typically depend intricate manner unknown parameters 
em algorithm ideally suited problems 
em algorithm iterative method finding mode observed incomplete data likelihood extremely useful common models hard maximize directly easy extended complete observation model obtained adding observed data appropriately chosen missing data 
blind identification context missing data obviously input data available identification simple task 
em algorithm formalizes relatively old idea handling missing data starting guess parameters replace missing values expectations guessed parameters ii estimate parameters assuming missing data estimated values iii reestimate missing values assuming new parameters correct iv reestimate parameters forth iterating convergence 
fact em algorithm efficient steps suggest missing data estimated separately functions missing data needed estimate model parameters estimated jointly 
name em comes alternating steps finding expectation needed functions sufficient statistics missing values maximizing estimate parameters functions missing data observed 
standard models missing values current estimate parameters estimating parameters current estimate missing values straightforward 
unfortunately case blind identification case step straightforward especially observation noise gaussian see input signal distributions analytically posterior distribution input signal expressed closed form 
solution problem resort monte carlo markov chain estimation technique simulate missing values posterior distribution stochastic version em algorithm basically expectation step replaced monte carlo integration see 
purpose contribution show modeling distribution input data mixture gaussian distributions ii convolutive mixture case splitting likelihood function step implemented exactly finite reasonably small number operations avoiding monte carlo integration 
implementation mle efficient statistically numerically 
appear proc 
icassp apr munich germany fl ieee 
noisy source separation identification concentrate instantaneous mixture unknown theta matrix 
give hints extend results convolutive mixtures section 
identifiability model discussed length 
basically mixing matrix estimated permutation columns numbering sources immaterial ii global scale factor column 
avoid set diagonal elements mixing matrix 
denote 
problem find value parameters fl delta delta delta 
posterior density xjs oe oe ffl denotes multivariate gaussian density function mean covariance matrix log oe gamma gamma gamma gamma gamma log rj ffl denotes determinant 
denote delta delta delta 
complete data likelihood log log oe log fl complete data maximum likelihood estimates obtained maximizing previous expression linear constraints delta ann 
quadratic minimization problem linear constraints solution problem explicitly shown brevity 
solution depends complete data sufficient statistics rss input data fs missing data em algorithm iteration em procedure consists step estimating conditional expectations current fit parameter step sp sjx ds ss sjx ds obtaining expected values complete data sufficient statistics xs ss ii step maximizing expected complete data log likelihood step lines simply substituting sufficient statistics rss conditional expectations xs ss striking similarity identification 
difficulty evaluation conditional expectations 
input takes finite number values happens case digital communication context input signal chosen finite alphabet conditional integration reduces finite summation possible solution try approximate integrals eq 
direct numerical integration monte carlo stochastic integration technique 
bayes rule implies sjx oe fl oe fl ds despite components independent posterior density sjx product dimensional marginal 
result absence special structure evaluation posterior density sjx second moments formidable task soon 
surprisingly simulating random variable posterior density difficult resort monte carlo markov chain technique powerful numerically involved procedure 
purposes contribution show modeling input mixture gaussian distributions gives attractive solution problem 
gaussian mixtures consider case pdf drop time index simplicity modeled finite mixture gaussian distributions fl ij oe ij oe ij fl delta delta delta iq oe delta delta delta iq oe iq dealing mixtures convenient consider exists hidden random variable values finite set delta delta delta probability ij conditional pdf jz oe ij oe ij 
joint distribution input data label delta delta delta zn nested structure xjs oe oe gamma delta delta delta nzn gamma diag oe oe delta delta delta oe nzn easily seen posterior distribution gaussian mean ff xz covariance delta respectively ff xz gamma rz gamma az delta gamma gamma gamma rz gamma rz gamma gamma similarly decomposition eq 
posterior distribution zx zjx zx sjx ds oe oe gamma ds gamma jja gamma gamma gamma basic property conditional probability zjx sjx zjx posterior distribution sjx finite mixture gaussian distributions sjx zjx oe ff xz delta note number components mixture equal product number components computation conditional expectations required eq 
done closed form avoiding monte carlo integration 
complete data likelihood decomposed log log oe log oe ij oe ij jg log ij jg em reestimation functional splitted terms computed closedform applying eq 
eq 
eq 
zjx log oe theta ff delta ds log oe ij oe ij jjx log ij jjx maximization simple closedform 
reestimation formula parameter oe brevity 

convolutive mixtures extend method previous section convolutive mixtures 
maximum likelihood estimation model involved problem complete data log likelihood decompose sum marginal components 
attention focused case input signal discrete random vector finite number values situation observed signal hidden markov process hmm finite hidden chain standard estimation procedure may applied 
authors derived approximate estimation procedure general input signal pdf monte carlo markov chain methods see example 
develop contribution alternative strategy concept split data likelihood introduced finite dimensional hmm estimation 
roughly speaking procedure amounts segment full observation blocks size proceed samples different blocks mutually independent 
keep notations simple assume set 
extensions vector case theory trivial 
integer denote pm delta delta delta joint pdf pm oe gammam oe gammak theta thetap gammal fl delta delta delta sm fl ds gammal delta delta delta dsm oe noise variance 
estimate parameter shall contrast function gamma log pm delta gammae gamma log pm delta kullback leibler information distributions course procedure equivalent maximum likelihood check appropriate contrast function 
known result kullback leibler distance equality pm pm word contrast function iff parameters identifiable dimensional marginal 
shown theorem assume exists js 
identifiability condition eq 
satisfied 
result proved extended version 
relies identifiability conditions obtained swami 
shall consider contrast process log pm sm sm gamma define maximizer theta set admissible parameter values 
idea contrast proposed hmm finite state hidden markov chain 
standard regularity conditions estimator may shown consistent asymptotically normal result obtained straightforward adaptation proofs consistency asymptotic normality proofs maximum likelihood estimate case 
slightly general contrast considered 
may beneficial overlap successive blocks certain amount say form log pm sr sr gamma estimators obtained maximizing contrasts may shown consistent asymptotically normal 
asymptotic variance estimators depends overlap may improve variance 
know focus implementation issues 
em paradigm model input signal distribution finite mixture gaussian distributions 
denote delta delta delta delta delta delta label random variable associated 
complete data likelihood decomposed way similar eq 
xm sm zm oe gammam theta gamma exp gamma oe gamma gamma gamma theta gammal exp gamma oe gamma delta delta delta oe delta delta delta oe respectively mean variance individual components mixture delta delta delta proportions xm gamma sm gamma gamma zm gamma gamma 
need compute quantities sm zm zm conditional distribution sm xm zm gaussian mean covariance distribution obtained efficiently efficient kalman state smoother kalman filter uses past observations state smoother yields estimators state vector full set observations sample 
classical fixed interval smoother efficient state smoother developed 
similarly conditional distribution zm xm may seen proportional zm xm sm zm dsm integral explicitely computed decomposition eq 

significant reduction computational burden achieved resorting kind forward backward algorithm coupling steps 
algorithm described full length version 
remaining steps closely follow previous section repeated 
method computationally intensive order magnitude faster monte carlo solutions problem 

simulation illustrative example suppose input signal sequence distributed mixture gaussian distribution fl oe oe gamma oe oe set gamma oe oe 
easily seen fl fl 
skewness null kurtosis gamma 
signal fed fir filter order coefficients 
additive noise zero mean gaussian signal noise ratio varied db db 
record size 
splitting procedure described 
reduce realization dependency simulations averaged monte carlo simulations 
initial estimates filter coefficients noise level obtained applying swami fourth order identification technique 
results summarized table 
seen procedure produces reliable estimates fir coefficients 
method clearly outperforms higher order methods expense significant increase computational burden 
true ma coef 
db mean std 
dev 
db mean std 
dev 
db mean std 
dev 
fact proposed method extends semiparametric context input signal distribution known necessary input data distributed gaussian mixtures 
crucial point mixture captures important features distribution input 
distributions approximated arbitrary precision gaussian mixtures approach offers route semi parametric estimation 
practical theoretical results direction conference 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
roy 
stat 
soc 



consistent asymptotically normal parameter estimates hidden markov models 
annals stat 

doucet 
fully bayesian analysis conditionally linear gaussian state space model 
proc 
icassp atlanta volume pages 
cardoso 
maximum likelihood source separation discrete sources 
proc 
eusipco pages edinburgh september 
tanner tools statistical inference springer series statistics springer verlag comon independent component analysis new concept signal processing volume pp 
weinstein system identification higher order statistics submitted ieee trans 
signal proc de jong 
smoothing interpolation state space model am 
statist 
assoc volume 
koopman disturbance smoother state space models biometrika volume 
blind channel estimation deconvolution colored noise higherorder cumulants proc 
spie volume swami estimating noncausal phase arma models non gaussian processes ieee trans 
acoust 
speech sig 
proc volume yellin weinstein multichannel signal separation methods analysis ieee trans 
acoust 
speech sig 
proc volume 
anderson moore optimal filtering englewood cliffs new jersey prentice hall 
