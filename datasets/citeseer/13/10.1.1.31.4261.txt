journal artificial intelligence research submitted published iterative optimization simplification hierarchical clusterings doug fisher vanderbilt edu department computer science box station vanderbilt university nashville tn usa clustering discovering structure data 
clustering systems differ objective function evaluate clustering quality control strategy search space clusterings 
ideally search strategy consistently construct clusterings high quality computationally inexpensive 
general ways partition search system inexpensively constructs tentative clustering initial examination followed iterative optimization continues search background improved clusterings 
motivation evaluate inexpensive strategy creating initial clusterings coupled control strategies iterative optimization repeatedly modifies initial clustering search better 
methods appears novel iterative optimization strategy clustering contexts 
clustering constructed judged analysts task specific criteria 
authors abstracted criteria posited generic performance task akin pattern completion error rate completed patterns externally judge clustering utility 
performance task adapt resampling pruning strategies supervised learning systems task simplifying hierarchical clusterings promising ease post clustering analysis 
propose number objective functions attribute selection measures decision tree induction perform error rate simplicity dimensions 

clustering discovering structure data 
clustering systems differ objective function evaluate clustering quality control strategy search space clusterings 
ideally search strategy consistently construct clusterings high quality computationally inexpensive 
combinatorial complexity general clustering problem search strategy computationally inexpensive give guarantee quality discovered clusterings diverse set domains objective functions 
partition search initial clustering inexpensively constructed followed iterative optimization procedures continue search background improved clusterings 
allows analyst get early indication possible presence form structure data search continue long worthwhile 
primary motivation design systems autoclass cheeseman kelly self stutz taylor freeman wallace dowe 
describes evaluates strategies iterative optimization inspired iterative seed selection strategy cluster michalski stepp fl ai access foundation morgan kaufmann publishers 
rights reserved 
fisher common form optimization iteratively single observations third method appears novel clustering literature 
strategy inspired part macro learning strategies iba collections observations en masse appears mitigate problems associated local maxima measured objective function 
evaluation purposes couple strategies simple inexpensive procedure cobweb fisher system anderson constructs initial hierarchical clustering 
iterative optimization strategies paired methods constructing initial clusterings 
clustering constructed judged analysts task specific criteria 
authors fisher cheeseman anderson abstracted criteria generic performance task akin pattern completion error rate completed patterns externally judge utility clustering 
systems objective function selected performance task mind 
performance task adapt resampling pruning strategies supervised learning systems task simplifying hierarchical clusterings post clustering analysis 
experimental evidence suggests hierarchical clusterings greatly simplified increase pattern completion error rate 
experiments clustering simplification suggest external criteria simplicity classification cost addition pattern completion error rate judging relative merits differing objective functions clustering 
suggest objective functions adaptations selection measures supervised decision tree induction may dimensions simplicity error rate 

generating hierarchical clusterings clustering form unsupervised learning partitions observations classes clusters collectively called clustering 
objective function quality measure guides search ideally clustering optimal measured objective function 
hierarchical clustering system creates tree structured clustering sibling clusters partition observations covered common parent 
section briefly summarizes simple strategy called hierarchical sorting creating hierarchical clusterings 
objective function assume observation vector nominal values ij distinct variables measure category utility gluck gluck cu ij jc gamma ij variants extensively system known cobweb fisher related systems gennari langley fisher mckusick thompson iba gennari mckusick langley reich biswas weinberg li de da veiga 
measure rewards clusters increase predictability variable values optimization hierarchical clusterings ij jc relative predictability population ij 
favoring clusters increase predictability ij jc ij necessarily favor clusters increase variable value ja ij 
clusters variable values predictable cohesive 
increases predictability stem shared variable values observations cluster 
cluster separated decoupled clusters variable values predictive cluster 
high stems differences variable values shared members cluster shared observations cluster 
general principle clustering increase similarity observations clusters cohesion decrease similarity observations clusters coupling 
category utility similar form gini index supervised systems construct decision trees mingers weiss kulikowski 
gini index typically intended address issue values variable predict priori known class labels supervised context 
summation gini indices reflected cu addresses extent cluster predicts values variables 
cu rewards clusters reduce collective impurity variables 
fisher cobweb system cu measure quality partition data pu fc cn cu average category utility clusters partition 
sections note measure partition quality suggest alternatives 
measure commonly take opportunity note problems techniques describe tied measure 
structure clusters cobweb autoclass cheeseman systems anderson assume clusters described probabilistically variable value associated conditional probability ij jc reflects proportion observations exhibit value ij variable fact variable value associated number observations cluster having value probabilities computed demand purposes evaluation 
probabilistically described clusters arranged tree form hierarchical clustering known probabilistic categorization tree 
set sibling clusters partitions observations covered common parent 
single root cluster identical structure clusters covering observations containing frequency information necessary compute ij required category utility 
gives example probablistic categorization tree hierarchical clustering node cluster observations summarized probabilistically 
observations leaves described variables size color shape 
hierarchical sorting strategy initial clustering sorting term adapted psychological task requires subjects perform roughly procedure describe ahn medin 
observation current partition sorting evaluates fisher jc jc root color shape size sma blu med sph gre lar red gre red lar sph med sma blu blu sma red sph med lar sph red sma gre jc jc probabilistic categorization tree 
quality new clusterings result placing observation existing clusters quality clustering results creating new cluster covers new observation option yields highest quality score pu selected 
clustering grows incrementally new observations added 
procedure easily incorporated recursive loop builds tree structured clusterings existing hierarchical clustering observation sorted relative top level partition children root existing child root chosen include observation observation sorted relative children node serves root recursive call 
leaf reached tree extended downward 
maximum height tree bounded limiting downward growth fixed depth 
shows tree new observations added observation extends left subtree downward second new leaf deepest existing level right subtree 
sorting strategy identical anderson 
children cluster partition observations covered parent measure pu guide sorting differs anderson 
observations stored singleton clusters leaves tree 
hierarchical sort strategies augment basic procedure manner described section fisher yun 
optimization hierarchical clusterings sma sqr gre sma gre new object new object root color shape size red lar sph med sma blu sma blu gre jc sma med lar sph blu gre red sma gre med sph red med red lar sph red jc jc jc jc updated probabilistic categorization tree 

iterative optimization hierarchical sorting quickly constructs tree structured clustering typically nonoptimal 
particular control strategy suffers ordering effects different orderings observations may yield different clusterings fisher xu 
initial clustering phase possibly offline process iterative optimization seeks uncover better clusterings 
seed selection reordering reclustering michalski stepp cluster seeks optimal partitioning data 
step selects random seed observations data 
seeds attractors clusters grown remaining data 
seed selection greatly impact clustering quality cluster selects new seeds centroids initial clusters 
clustering repeated new seeds 
process iterates improvement quality generated clusterings 
fisher function order root root leaf return observations covered root order children root covering observations covering 
child root order order merge fl objects constructed order return table procedure creating dissimilarity ordering data 
ordering effects sorting related effects arise due differing fixed seed selections initial observations ordering establish initial clusters attract remaining observations 
general sorting performs better initial observations diverse areas observation description space facilitates establishment initial clusters reflect different areas 
fisher xu showed ordering data consecutive observations dissimilar euclidean distance led clusterings 
biswas 
adapted technique iterate system similar results 
cases sorting pu score described previously 
procedure presumes observations appear dissimilar euclidean distance tend placed different clusters objective function 
lead cluster measure independent idea sorts random data ordering extracts biased dissimilarity ordering hierarchical clustering sorts 
function table outlines reordering procedure 
recursively extracts list observations probable largest cluster probable merges interleaves lists exiting recursive call step element probable cluster placed followed element second probable forth 
measure guides clustering observations differing clusters judged dissimilar measure 
measure independent procedure returns measure dependent dissimilarity ordering placing observations different clusters back back 
initial sorting extract dissimilarity ordering iterate improvement clustering quality 
iterative redistribution single observations common long known form iterative optimization moves single observations cluster cluster search better clustering duda hart 
basic strategy form numerous sort algorithms fisher 
idea iterative redistribution biswas weinberg yang koller simple observations single level clustering removed original cluster resorted relative clustering 
cluster contains observation cluster removed single observation resorted 
process continues consecutive iterations yield clustering 
optimization hierarchical clusterings omega omega omega omega omega gamma gamma omega omega omega omega omega gamma gamma 



hierarchical redistribution left subfigure indicates cluster just removed descendent producing resorted relative children root 
rightmost shows placed new child fisher 
reproduced permission proceedings international conference knowledge discovery data mining copyright fl american association artificial intelligence 
isodata algorithm duda hart determines target cluster observation change clustering targets observations determined point observations moved targets altering clustering 
limit sequential version described duda hart moves observation target identified sorting 
strategy conceptually simple limited ability overcome local maxima reclassification particular observation may true direction better clustering may perceived objective function applied clustering results resorting single observation 
iterative hierarchical redistribution iterative optimization strategy appears novel clustering literature iterative hierarchical redistribution 
strategy relative single observation iterative redistribution moving set observations cluster may lead better clustering movement single observation may initially reduce clustering quality preventing eventual discovery better clustering 
response hierarchical redistribution considers movement observation sets represented existing clusters hierarchical clustering 
existing hierarchical clustering recursive loop examines sibling clusters hierarchy depth fashion 
set siblings inner iterative loop examines sibling removes current place hierarchy subtree resorts cluster relative entire hierarchy 
removal requires various fisher counts ancestor clusters decremented 
sorting removed cluster done cluster probabilistic description requires minor generalization procedure sorting individual observations incrementing certain variable value counts cluster reflect addition new observation host cluster variable value counts incremented corresponding counts cluster classified 
cluster may return original place hierarchy illustrates may sorted entirely different location 
inner loop sibling set repeats consecutive iterations lead set siblings 
recursive loop turns attention children remaining siblings 
eventually individual observations represented leaves resorted relative entire hierarchy changes iteration 
recursive loop may applied hierarchy times defining outermost iterative loop terminates changes occur pass 
modification basic strategy implemented reasons cost change subtree pass outermost loop hierarchy subsequent passes attempt redistribute clusters subtree cluster location hierarchy placed subtree changing subtree structure 
addition cases pu scores obtained placing cluster typically singleton cluster hosts 
cases algorithm prefers placement original host candidates high pu score 
policy avoids infinite loops stemming ties pu score 
sum hierarchical redistribution takes large steps search better clustering 
similar macro operator learners iba problem solving contexts moving observation set cluster bridges distant points clustering space desirable change viewed desirable redistribution limited movement individual observations 
redistribution increasingly smaller granular clusters terminating individual observations serves increasingly refine clustering 
large extent hierarchical redistribution inspired fisher cobweb system fundamentally hierarchical sort strategy 
cobweb augmented operators merging splitting promotion 
merging combines sibling clusters hierarchical clustering increases quality partition clusters members splitting remove cluster promote children higher partition distinct promotion operator promote individual cluster higher level 
fact regarded iterative optimization operators keeping cobweb cognitive modeling motivations cost applying amortized time observations sorted cluster may migrate part hierarchical clustering collective repeated application merging splitting promotion 
similar view expressed mckusick langley system differs cobweb part way exploits promotion operator 
unfortunately cobweb lesser extent merging splitting promotion applied locally migration hierarchy limited practice 
contrast hierarchical redistribution resorts cluster regardless optimization hierarchical clusterings initial location tree root entire tree vigorously pursuing migration globally evaluating merits moves 
idea hierarchical redistribution closely related strategies reich systems 
particular identifies misplaced clusters hierarchical clustering criterion specified part domain expert hierarchical redistribution simply uses objective function 
misplaced cluster removed subtree cluster subtree resorted single unit observations covered cluster resorted individually 
approach captures part idea hierarchical redistribution resorting individual observations may escape local optima extent hierarchical redistribution 
existing hierarchical clustering new observation conducts branch bound search clustering looking cluster best matches observation 
best host clusters vicinity best host branch bound respect entire hierarchy 
clusters need singletons reclassification spawn termination condition reached 
unclear procedure scales large data sets number experimental trials size test data sets considerably describe shortly 
importance bridging distant regions clustering space observation sets en masse explicit 
cobweb incremental changes hierarchy triggered path classifies new observation changes may move observations simultaneously amortizing cost optimization time 
contrast hierarchical redistribution motivated philosophy sorting method produce tentative clustering data quickly followed iterative optimization procedures background revise clustering intermittently 
hierarchical redistribution reflects ideas implemented cobweb related systems appears novel iterative optimization strategy decoupled particular initial clustering strategy 
comparisons iterative optimization strategies section compares iterative optimization strategies experimental conditions 
condition random ordering observations generated hierarchically sorted 
optimization strategies applied independently resultant hierarchical clustering 
experiments assume primary goal clustering discover single level partitioning data optimal quality 
objective function score level partition taken important dependent variable 
independent variable height initially constructed clustering effect granularity clusters hierarchical redistribution 
hierarchical clus 
considering global changes motivated redistribution individual observations iterate 
notes commentary experimental comparisons iterate cobweb fisher global movement single observations typically perform local movement sets observations simultaneously implemented cobweb merging splitting operators 
fisher random similarity sort soybean small reorder resort obs vars iter 

hier 

sort soybean large reorder resort obs vars iter 

hier 

sort house reorder resort obs vars iter 

hier 

sort mushroom reorder resort obs vars iter 

hier 

table iterative optimization strategies initial clusterings generated sorting random similarity ordered observations 
tree height 
averages standard deviations pu scores trials 
tering height corresponds single level partition data depth root depth leaves corresponding individual observations depth 
addition experiments clusterings derived sorting random initial orderings redistribution strategy tested exceptionally poor initial clusterings generated nonrandom orderings 
just dissimilarity orderings lead clusterings similarity orderings lead poor clusterings fisher 
intuitively similarity ordering samples observations region data description space sampling observations differing regions 
reordering procedure section easily modified produce similarity orderings ranking set siblings hierarchical clustering probable appending interleaving observation lists differing clusters algorithm pops recursive levels 
similarity ordering produced applying procedure initial clustering produced earlier sort random ordering 
clustering produced sorting similarity ordered data iterative optimization strategies applied independently 
advocate build clusterings similarity orderings practice experiments orderings better test robustness various optimization strategies 
table shows results experiments random similarity orderings data databases uci repository 
results assume initial clustering height top level partition observations leaves 
cell represents average 
reduced mushroom data set obtained randomly selecting observations original data set 
optimization hierarchical clusterings standard deviation trials 
cell labeled sort domain mean pu scores initially obtained sorting 
subsequent rows domain reflect mean scores obtained reordering resorting procedure section iterative redistribution single observations described section hierarchical redistribution described section 
main findings reflected table 
initial hierarchical sorting random input reasonably pu scores case closer scores optimized trees poorest scores obtained sorting similarity orderings 
weakly suggests initial sorting random input takes substantial step space clusterings discovery final structure 

hierarchical redistribution achieves highest mean pu score random similarity case domains 
small soybean domain exception 

house domain random similarity case mushroom domain random case standard deviation pu scores clusterings optimized hierarchical redistribution indicating constructed level partitions pu score trials 

reordering reclustering comes closest hierarchical redistribution performance cases small soybean domain 

single observation redistribution modestly improves initial sort substantially worse optimization methods 
note initial hierarchical clusterings height difference iterative hierarchical redistribution redistribution single observations hierarchical redistribution considers merging clusters partition respect prior redistributing single observations pass hierarchy 
section suggested expected benefits hierarchical redistribution greater deeper initial trees granular clusters 
table shows results domains initial orderings tree height hierarchical redistribution reader convenience repeat results table hierarchical redistribution tree height 
moving height modest improvement small soybean domain particularly similarity orderings slight improvement large soybean domain mushroom domain similarity orderings 
improvements modest moving height trees leads near identical performance random similarity ordering conditions 
suggests hierarchical redistribution able effectively overcome disadvantage initially poor clusterings 
experiments reorder resort iterative distribution single observations varied respect tree height height 
methods 
standard deviation indicates standard deviation non observable nd decimal place rounding 
fisher random similarity height height height height soybean small soybean large house mushroom table hierarchical redistribution initial clusterings generated sorting random similarity ordered observations 
results shown tree heights copied table 
averages standard deviations pu scores trials 
deepest set clusters initial hierarchy leaves taken initial partition 
reordering resorting scores remained roughly height condition clusterings produced single observation redistribution pu scores considerably worse table 
recorded execution time method 
table shows time required method seconds 
particular domain table lists mean time initial sorting mean additional time optimization method 
ironically experiments demonstrate hierarchical redistribution bottoms single observation form redistribution consistently faster trees height cluster simultaneously moves set observations repeatedly evaluated redistribution individually increased time stabilization 
table assumes tree constructed initial sorting bounded height 
table gives time requirements hierarchical sorting hierarchical redistribution initial tree bounded height 
tree gets deeper cost hierarchical redistribution grows substantially comparison performance height trees table suggests drastically diminishing returns terms partition quality 
importantly limited experiments trees height indicate cost hierarchical redistribution comparable cost reorder resort greater tree heights significantly expensive single observation redistribution 
difficult give cost analysis hierarchical redistribution methods matter bounds loop iterations probably depend nature objective function 
suffice say number nodes subject hierarchical redistribution tree covering observations bounded gamma may leaves gamma internal nodes internal node children 
iterative optimization occur background real time response important cluster quality paramount probably worth applying hierarchical 
routines implemented sun common lisp compiled run sun 

similar timing results occur computational contexts 
consider relation insertion sort shell sort 
shell sort final pass table insertion sort limited moving table elements consecutive table locations time 
large efficiency advantage shell sort stems fact previous passes table moved elements large distances final pass table nearly sorted 
optimization hierarchical clusterings random similarity sort soybean small reorder resort obs vars iter 

hier 

sort soybean large reorder resort obs vars iter 

hier 

sort house reorder resort obs vars iter 

hier 

sort mushroom reorder resort obs vars iter 

hier 

table time requirements seconds hierarchical sorting iterative optimization initial clusterings generated sorting random similarity ordered observations 
tree height 
averages standard deviations trials 
tribution deeper trees consistent philosophy systems autoclass 
domains examined cost effective optimize trees height greater 
adopt tree construction strategy builds hierarchical clustering levels time hierarchical redistribution experiments section 
discussion iterative optimization methods experiments demonstrate relative abilities iterative optimization strategies coupled pu objective function hierarchical sorting generate initial clusterings 
reorder resort optimization strategy section sense sorting primary clustering strategy optimization techniques strongly tied particular initial clustering strategy 
example hierarchical redistribution applied hierarchical clusterings generated agglomerative strategy duda hart everitt fisher uses bottom procedure construct hierarchical clusterings repeatedly merging observations resulting clusters inclusive root cluster generated 
agglomerative methods suffer ordering effects greedy algorithms susceptible limitations local decision making generally benefit iterative optimization 
fisher random similarity height height height height small sort hier 

large sort hier 

house sort hier 

mushroom sort hier 

table time requirements seconds hierarchical sorting hierarchical redistribution initial clusterings generated sorting random similarity ordered observations 
results shown tree heights copied table 
averages standard deviations trials 
addition optimization strategies applied regardless objective function 
relative benefits methods undoubtedly varies objective function 
example pu function undesirable characteristic may particular circumstances view partitions close form separated cliff fisher fisher 
consider partition observations involving roughly equal sized clusters pu score form pu fc cu 
create partition clusters removing single observation say creating new singleton cluster pu fc cu 
relatively large cu small score due term see section 
average cu score clusters difference pu fc pu fc may quite large differ placement observation 
limiting experiments pu function may exaggerate general advantage hierarchical redistribution relative optimization methods 
statement simultaneously positive statement robustness hierarchical redistribution face objective function cliffs negative statement pu defining discontinuities 
pu variants adopted systems fall cobweb family gennari mckusick thompson reich iba gennari mckusick langley biswas 
section suggests alternative objective functions 
pu findings taken best strategies engineered particular clustering system 
introduce forms randomization systematic variation strategies 
example michalski stepp seed selection methodology inspires reordering resorting michalski stepp approach selects border observations selection centroids fails improve clustering quality iteration optimization hierarchical clusterings example kind systematic variations introduce pursuit better clusterings 
contrast autoclass may take large heuristically guided jumps away current clustering 
approach fact somewhat systematic equally successful variation macro operator theme inspired hierarchical redistribution similar approach 
wallace dowe employs variety search operators including operators similar cobweb merge split restrictions local application random restart clustering process new seed observations redistribution observations 
fact user program search strategy differing primitive search operators 
case systems cluster autoclass simply give fail improve clustering quality iteration 
illustrates strategies combined advantage 
additional example biswas 
adapt fisher xu dissimilarity ordering strategy preorder observations prior clustering 
sorting pu iterate system applies iterative redistribution single observations category match measure fisher langley 
combination iterative redistribution appears yield results iterate 
results reorder resort suggest primarily responsible quality benefits simple sort relative contribution iterate redistribution operator certain differs respects redistribution technique described 
different measures distance pu category match clustering may unnecessary adds undesirable coupling design clustering algorithm 
example wants experiment merits differing objective functions undesirable worry compatibility function measures 
contrast reordering resorting generalizes fisher ordering strategy generalization iterative redistribution strategy describe assume auxiliary measures objective function 
fact fisher evaluation iterate clusterings measures variable value predictability ij jc ja ij product 
clear system need exploit related albeit different measures generation evaluation clusterings undoubtedly single carefully selected objective function exclusively clustering 
reordering resorting iterative redistribution single observations combined manner similar iterate exploitation certain specializations procedures 
results suggest reordering resorting put clustering iterative redistribution subsequently modest refinements 
combined strategies sense conducted inverse ablation study evaluating individual strategies isolation 
limited number domains explored section appears difficult better hierarchical redistribution 
experiments applied various optimization techniques data sorted 
may desirable apply optimization procedures intermittent points sorting 
may improve quality final clusterings reordering resorting 
importantly autoclass assumes probabilistic assignment observations clusters 

iterate uses measure redistribution fisher langley probably smoothes cliffs uses isodata non sequential version redistribution 
fisher redistribution single observations reduce cost constructing final optimized clusterings methods including hierarchical redistribution appears quite quality dimension 
fact viewed performing akin restricted form hierarchical redistribution observation 
probably extreme iterative optimization performed resultant cost outweigh savings gleaned maintaining relatively optimized trees sorting process 
utgoff similar suggestion intermittent restructuring decision trees incremental supervised induction 

simplifying hierarchical clusterings hierarchical clustering grown arbitrary height 
structure data ideally top layers clustering reflect structure substructure descends hierarchy 
lower levels clustering may reflect meaningful structure 
result overfitting finds supervised induction 
inspired certain forms retrospective post tree construction pruning induction resampling identify frontiers hierarchical clustering candidates pruning 
initial hierarchy construction iterative optimization simplification process final phase search space hierarchical clusterings intended ease burden data analyst 
identifying variable frontiers resampling authors fisher cheeseman anderson motivate clustering means improving performance task akin pattern completion error rate completed patterns externally judge utility clustering 
probablistic categorization tree type assumed new observation unknown value variable classified hierarchy small variation hierarchical sorting procedure described earlier 
classification terminated selected node cluster classification path variable value highest probability cluster predicted unknown variable value new observation 
naively classification terminate leaf observation leaf value specified variable predicted variable value new observation 
simple resampling strategy known holdout weiss kulikowski motivated fact variable better predicted internal node classification path 
identification ideal prediction frontiers variable suggests pruning strategy hierarchical clusterings 
hierarchical clustering validation set observations validation set identify appropriate frontier clusters prediction variable 
illustrates preferred frontiers variables may differ clusters frontier may different depths 
variable objects validation set classified hierarchical clustering value variable masked purposes classification cluster encountered classification 
classification identical sorting observation added clustering statistics node encountered sorting permanently updated reflect new observation 
optimization hierarchical clusterings 
frontier frontiers variables hypothetical clustering 
fisher 
reproduced permission proceedings international conference knowledge discovery data mining copyright fl american association artificial intelligence 
observation value compared probable value cluster observation value correctly predicted cluster 
count correct predictions variable cluster maintained 
classification variables observations validation set preferred frontier variable identified maximizes number correct counts variable 
simple bottom procedure insures number correct counts node variable frontier greater equal sum correct counts variable set mutually exclusive collectively exhaustive descendents node 
variable specific frontiers enable number pruning strategies 
example node lies frontier variable offers apparent advantage terms error rate node probably reflects meaningful structure descendents may pruned 
analyst focusing attention subset variables frontiers flexibly exploited pruning 
experiments validation test validation procedure promise simplifying hierarchical clusterings data sets optimization experiments section randomly divided subsets training validation test 
hierarchical clustering constructed sorting training set randomized order 
hierarchy optimized iterative hierarchical redistribution 
cost considerations hierarchy constructed levels time 
hierarchy initially constructed height deepest level set training observations 
hierarchy optimized hierarchical redistribution 
clusters bottommost level removed children level clusters subset training observations fisher validated soybean small leaves accuracy ave frontier size soybean large leaves accuracy ave frontier size house leaves accuracy ave frontier size mushroom leaves accuracy ave frontier size table characteristics optimized clusterings validation 
average standard deviations trials 
covered cluster level hierarchically sorted height tree optimized 
roots subordinate clusterings substituted cluster depth original tree 
process repeated clusters level subordinate trees subsequent trees decomposition possible 
final hierarchy constant bounded height decomposes entire training set singleton clusters containing single training observation 
validation set identify variable frontiers entire hierarchy 
testing validated clustering variable test observation masked turn classification reaches cluster frontier masked variable probable value predicted value observation proportion correct predictions variable test set recorded 
comparative purposes test set evaluate predictions stemming tree variable predictions leaves singleton clusters tree 
table shows results experimental trials optimized validated clusterings generated just described random orderings 
row domain lists average number leaves experimental trials validated trees 
clusterings decompose training data single observation leaves number leaves equals number training observations 
validated clustering assume clusters pruned lie frontiers variables 
leaf validated clustering cluster original clustering frontier variable descendent clusters original clustering frontier variable 
example assume optimization hierarchical clusterings tree covers data described terms variables number leaves validated clustering 
prediction accuracies second row domain entry mean proportion correct predictions variables trials 
predictions generated leaves singleton clusters hierarchical clusterings appropriate variable frontiers validated clusterings 
cases validation pruning substantially reduces clustering size diminish accuracy 
number leaves validated case described assumes coarse pruning strategy necessarily discriminate clustering uniformly deep frontiers single deep frontiers 
suggested flexible pruning attention strategies possible analyst focusing variables 
specify strategies statistic row domain entry suggests clusterings rendered considerably simpler forms analyst attention selective 
row average number frontier clusters variable 
average variables experimental trials 
validated tree average frontier size 
intuitively frontier cluster variable leaf far prediction variable concerned 
frontier size clusterings simply number leaves variable predictions case 
results suggest attention selective partial clustering captures structure involving selected variables analyst simplified form 
discussion validation resampling validation method inspired earlier fisher identified variable frontiers strict incremental sorting context separate validation set reserved training set identifying variable frontiers 
particular training observation hierarchically sorted cobweb observation variable values predicted correct counts node updated correctly anticipated variables 
fisher variable values masked sorting knowledge variable value sorting helping guide classification validation 
addition hierarchy changed sorting validation 
incremental strategy led desirable results terms pattern completion error rate variable frontiers identified incremental method desirable frontiers identified holdout strictly segregate training validation sets observations 
addition fisher variable frontiers traced back ideas lebowitz kolodner directly fisher fisher schlimmer reich different method identify similar spirit frontiers defined 
method validation pruning inspired retrospective pruning strategies decision tree induction reduced error pruning quinlan mingers 
bayesian clustering system autoclass cheeseman 
standard deviations row mean standard deviations frontier sizes individual variables 
fisher minimum message length mml approach adopted wallace dowe expansion hierarchical clustering mediated tradeoff prior belief existence structure evidence data structure 
detail fundamental tradeoff suffice say expansion hierarchical clustering cease path evidence structure data insufficient face prior bias 
undoubtedly bayesian mml approaches adapted identify variable specific frontiers kind flexible pruning focusing strategies implied 
fact similar intent implemented autoclass hanson stutz cheeseman way reducing cost clustering system variables may blocked sense treated 
version autoclass searches space hierarchical clusterings blocks variables assigned particular clusters hierarchy 
interpretation assignments cluster inherits variable value distributions variable blocks assigned cluster ancestors 
inversely basic idea need proceed cluster determine value distributions variables assigned cluster 
experimental results suggest utility resampling validation identification variable frontiers pruning 
procedure described method se clustering available data requires validation set held initial hierarchy construction 
options worthy experimental evaluation adapting validation strategy tool simplification hierarchical clusterings 
strategy hold validation set cluster training set identify variable frontiers validation set sort validation set relative clustering 
single holdout methodology problems reasons similar identified single holdout supervised settings weiss kulikowski 
better strategy akin fold cross validation hierarchical clustering constructed available data observation removed validation respect variable observation reinstated original location original variable value statistics clusters path location 

general discussion evaluation various strategies discussed reflect paradigms validating clusterings 
internal validation concerned evaluating merits control strategy searches space clusterings evaluating extent search strategy uncovers clusterings high quality measured objective function 
internal validation focus section 
external validation concerned determining utility discovered clustering relative performance task 
noted 
purposes evaluating merits validation strategy terms error rate held separate test set 
having demonstrated point require separate test set held resampling validation strategy 

observation physically removed variable value statistics clusters lie path root observation decremented 
optimization hierarchical clusterings unoptimized optimized validated validated soybean small leaves accuracy ave frontier size soybean large leaves accuracy ave frontier size house leaves accuracy ave frontier size mushroom leaves accuracy ave frontier size table characteristics unoptimized optimized clusterings validation 
average standard deviations trials 
authors point minimization error rate pattern completion generic performance task motivates choice objective function 
external validation focus section 
section explores validation issues closely identifies error rate simplicity cost necessary external criteria discriminating clustering utility suggests number alternative objective functions usefully compared criteria speculates external validation criteria taken collectively reflect reasonable criteria data analysts may judge utility clusterings 
closer look external validation criteria ideally clustering quality measured objective function correlated clustering utility determined performance task higher quality clustering judged objective function greater performance improvement reduction error rate lower quality performance improves 
authors fisher devaney ram pointed pu scores correlated error rates 
precisely hierarchical clusterings constructed hierarchical sorting top level partition low pu score lead roughly error rates hierarchies top level partition high pu score variable value predictions leaves singleton clusters 
apparently poor partitions level measured pu test fisher observations classified similar observations leaves hierarchical clustering 
pattern completion error rate circumstances insufficient discriminate consider poor clusterings 
simplification section suggests addition error rate choose judge competing hierarchical clusterings simplicity criterion 
error rate simplicity judge classifiers supervised contexts 
seen holdout substantially simplify hierarchical clustering 
question ask hierarchical clusterings optimized relative pu simplified substantially unoptimized clusterings degradation pattern completion error rate 
answer question repeated validation experiments section second experimental condition hierarchical clusterings constructed similarity orderings observations hierarchical sorting 
saw section similarity orderings tend result clusterings judged poor pu function 
optimize hierarchies hierarchical optimization 
table shows accuracies number leaves average frontier sizes unoptimized hierarchies constructed similarity orderings case subjected holdout validation case 
results heading unoptimized 
convenience copy results table heading optimized 
optimized case identifying exploiting variable frontiers unoptimized clusterings appears simplify clustering substantially degradation error rate 
interest optimized clusterings simplified substantially greater extent unoptimized clusterings degradation error rate 
far focused criteria error rate simplicity applications real interest simplicity stems broader interest minimizing expected cost exploiting clustering classification expect simpler clusterings lower expected classification costs 
view various distinctions validated unoptimized optimized clusterings terms expected classification cost 
table shows additional data obtained experiments validation 
particular table shows leaves mean number leaves trials validation assuming coarse pruning strategy described section optimized unoptimized cases copied table 
mean total path length trials 
total path length tree leaf corresponds single observation sum depths leaves tree 
case validated tree leaf may cover multiple observations contribution leaf total path length depth leaf times number observations leaf 
depth average depth leaf tree computed breadth average branching factor tree 
log optimization hierarchical clusterings unoptimized optimized validated validated soybean small leaves depth breadth cost soybean large leaves depth breadth cost house leaves depth breadth cost mushroom leaves depth breadth cost table cost characteristics unoptimized optimized clustering validation 
average standard deviations trials 
characteristics ed computed mean values leaves 
cost expected cost classifying observation root leaf terms number nodes clusters examined classification 
level examine cluster select best 
cost product number levels number clusters level 
theta table illustrates expected cost classification optimized clusterings unoptimized clusterings validated cases 
results taken grain salt simply estimated values 
particular expressed cost terms expected number nodes need examined classification 
implicit assumption cost examination constant nodes 
fact cost examination roughly constant domain nodes implementation node variables examined 
consider measure cost cost fisher clustering splits observations thirds node forming balanced ternary tree regardless form structure data 
course tree reasonably capture structure data expect reflected error rate post validation simplicity 
probably better measures cost available 
particular gennari observed classifying observation evaluating objective function proper subset variables sufficient categorize observation relative cluster selected evaluation occurred variables 
ideal circumstances clusters partition separated decoupled testing critical variables may sufficient advance classification 
gennari implemented focusing algorithm sequentially evaluated objective function variables additional variable time critical categorization respect clusters unambiguously 
gennari procedure examination cost constant nodes 
fisher fisher xu reich chen biswas weinberg adapted gennari procedure effect diagnosis task intent minimize number probes necessary diagnose fault 
gennari offers principled focusing strategy conjunction objective function general idea focusing selected features classification traced back lebowitz kolodner 
results table illustrate form expected classification cost analysis measured cost time directly test set 
fact comparisons time requirements sorting random similarity ordering conditions tables suggest cost differences poor clusterings terms time 
regardless form analysis desirable express branching factor cost terms number variables need tested assuming focusing strategy gennari 
tend better distinctions clusterings 
evaluating objective functions getting bang buck results section suggest pu function useful identifying structure data clusterings optimized relative function simpler accurate clusterings optimized relative function 
pu leads reasonable error rate simplicity dimensions objective functions better job dimensions 
earlier discussion limitations pu notably averaging cu clusters partition introduced cliffs space partitions better objective functions 
example consider bayesian variants autoclass cheeseman anderson system closely related mml approach wallace dowe 
evaluate alternative measures suggest number candidates 

fact cost constant observations classified exactly path number variables need test depends observation values previously examined variables 
optimization hierarchical clusterings section noted cu function viewed summation gini indices measured collective impurity variables conditioned cluster membership 
intuition may helped information theoretic analog cu gluck ij jc log ij jc gamma ij log ij information theoretic analog understood summation information gain values information gain selection criterion decision tree induction quinlan clustering analog rewards clusters maximize sum information gains individual variables gini information gain measures bases selection measures decision tree induction 
measure expected decrease impurity uncertainty class label conditioned knowledge variable value 
clustering context interested decrease impurity variable value conditioned knowledge cluster membership summation suitable gini indices alternatively information gain scores 
known context decision tree induction measures biased select variables legal values 
various normalizations measures different measures altogether devised 
clustering adaptation measures normalization necessary cu information theoretic analog favor clustering greatest cardinality data partitioned singleton clusters observation 
pu normalizes sum gini indices averaging 
general observation selection measures decision tree induction adapted objective functions clustering 
number selection measures suggest candidates clustering normalization principled averaging 
candidates quinlan gain ratio lopez de mantaras normalized information gain 
ij ja ij log ja ij gammap log gamma ij log ij quinlan ij ja ij log ja ij gammap log gamma ij log ij lopez de mantaras derive objective functions clustering ij jc log ij jc gammap ij log ij gamma log ij jc log ij jc gammap ij log ij gamma ij log ij 
jan hajek independently pointed relationship cu measure gini index suggestions select normalizations 
fisher clustering variations defined fisher 
nonsystematic experimentation lopez de mantaras normalized information gain variant suggests mitigates problems associated pu merits await experimentation 
general wealth promising objective functions decision tree selection measures consider 
described fayyad ort function 
relationship supervised unsupervised measures pointed context bayesian systems duda hart 
consider autoclass cheeseman searches probable clustering available data set clustering highest jd djh 
independence assumptions autoclass computation djh includes easily seen mechanisms simple bayes classifier supervised contexts 
compared proposed derivations decision tree selection measures bayesian mml measures autoclass proposed error rate simplicity classification cost external objective criteria comparisons 
advantage bayesian mml approaches proper selection prior biases require separate strategy resampling pruning strategies adapted variable frontier identification 
objective function cluster formation serves cease hierarchical decomposition 
know experimental studies bayesian mml techniques accuracy cost dimensions outlined expect perform quite 
final comments external validation criteria proposal external validation criteria clustering error rate classification cost stem larger implicit long standing bias ai learning systems serve ends artificial autonomous agent 
certainly cobweb family systems trace ancestry systems lebowitz kolodner autonomous agency primary theme fisher anderson expresses similar concerns 
short view clustering means organizing memory observations autonomous agent begs question agent tasks memory organization intended support 
pattern completion error rate simplicity cost obvious candidate criteria 
underlying assumption article criteria appropriate externally validating clusterings data analysis contexts clustering external human analyst exploited analyst purposes hypothesis generation 
traditional criteria cluster evaluation contexts include measures intra cluster cohesion observations clusters similar inter cluster coupling observations differing clusters dissimilar 
criteria proposed article traditional criteria certainly related 
consider derivation portion category utility measure begins expected number variable values correctly predicted prediction guided clustering fc cn optimization hierarchical clusterings correct variable cn correct variable correct predictions variable jc ij jc times ij correct prediction jc ij jc theta ij jc ij jc final steps derivation assume variable value predicted probability ij jc probability prediction correct derivation category utility assumes probability matching prediction strategy gluck gluck 
favoring partitions improve prediction variables hierarchical clustering category utility tends result hierarchies variable frontiers described section near top clustering tends reduce post validation classification cost 
category utility motivated measure rewards cohesion clusters decoupling clusters noted section measure motivated desire reduce error rate indirectly classification cost 
general measures motivated desire reduce error rate favor cohesion decoupling stems aspects pattern completion task lebowitz medin 
assign observation cluster known variable values observation best facilitated variable value high variables clusters decoupled 
having assigned observation cluster cluster definition predict values variables known observation description 
process successful variables predictable clusters clusters cohesive 
fact designing measures cohesion decoupling mind undoubtedly results useful clusterings purposes pattern completion explicit goal designer 
external validation criteria error rate cost correlated traditional criteria cohesion coupling criteria 
part stems ai machine learning bias systems designed evaluated specific performance task mind 
addition plethora measures assessing cohesion coupling system assessed relative variant 
variation difficult assess similarities differences systems 
article suggests pattern completion error rate cost relatively unbiased alternatives comparative studies 
inversely direct measures error rate classification cost holdout objective function guide search space clusterings 
expensive 
cheaply computed objective function designed external error rate cost evaluation mind undoubtedly objective function reflects cohesion coupling 

importantly prediction cobweb performed probability maximizing strategy frequent value variable cluster predicted 
fisher discusses advantage constructing clusters implicit probability matching strategy cases clusters exploited probability maximizing strategy 

mml bayesian approaches autoclass support probabilistic assignment observations clusters importance decoupling cohesion remain 
fisher course computed error rate identified variable frontiers simplified performance task variable independently masked predicted test observations 
unreasonable generic method computing error rate different domains may suggest different computations variables simultaneously unknown analyst may interested subset variables 
addition proposed simplicity number leaves expected classification cost external validation criteria 
section suggests criteria probably necessary addition error rate discriminate poor clusterings judged objective function 
general desirable realizations error rate simplicity cost vary domain interpretation tasks analyst 
short analyst task largely making inferences clustering error rate cost components information analyst glean clustering required part analyst extract information 
probably case expressed components precisely way cognitively implemented analyst 
article fisher cheeseman anderson viewed attempts formally tentatively describe analyst criteria cluster evaluation criteria prescribe autonomous artificial agent confronted task 
issues important issues clustering address depth 
possible advantage overlapping clusters lebowitz martin 
assumed tree structured clusterings store observation cluster clusters related proper subset relation descends path tree 
cases lattices levinson wilcox levinson carpineto romano generally directed acyclic graphs dag may better representation scheme 
structures allow observation included multiple clusters cluster need subset 
may better provide analyst multiple perspectives data 
example animals partitioned clusters corresponding mammals birds may partitioned clusters corresponding 
tree require partitions carnivore subordinate mammals birds classes subordinate partition necessarily distributed descendents mammal mammal reptile top level clusters ideally represent clusters partition 
dag allows perspectives coexist relative equality making perspectives explicit analyst 
assumed variables nominally valued 
numerous adaptations basic pu function functions discretization strategies accommodate numeric variables michalski stepp gennari reich cheeseman biswas 
basic sorting procedure iterative optimization techniques data described part numerically valued variables regardless approach takes 
identification optimization hierarchical clusterings numeric variable frontiers holdout done mean value variable node generating predictions identifying variable frontier set clusters collectively minimize measure error mean squared error 

concluding remarks partitioned search space hierarchical clusterings phases 
phases opinion desirable characteristics data analysis standpoint inexpensive generation initial clustering suggests form structure data absence iterative optimization background clusterings better quality retrospective simplification generated clusterings 
evaluated iterative optimization strategies operate independent objective function 
varying degrees inspired previous research hierarchical redistribution appears novel iterative optimization technique clustering appears quite 
novel aspect resampling means validating clusters simplifying hierarchical clusterings 
experiments section indicate optimized clusterings provide greater data compression unoptimized clusterings 
surprising pu compresses data reasonable manner optimally issue 
recommendations research 

suggested experiments alternative objective functions including bayesian mml measures inspired variable selection measures decision tree induction 

may cost quality benefits applying optimization strategies intermittent points hierarchical sorting 

holdout method identifying variable frontiers pruning suggests strategy akin fold cross validation clusters data identifying variable frontiers facilitating pruning 

analyses classification cost purposes external validation probably best expressed terms expected number variables focusing method gennari 
sum proposed criteria internal external validation experimental comparisons various approaches dimensions 
ideally researchers explore objective functions search control strategies pruning techniques kind experimental comparisons particularly external criteria error rate simplicity classification cost de comparisons supervised systems prominent unsupervised contexts 
fisher varma arthur diana gordon comments 
reviewers editor supplied extensive helpful comments 
supported nag nasa ames research center 
abbreviated discussion article results appear fisher published aaai press 
ahn medin 

stage categorization model family resemblance sorting 
proceedings eleventh annual conference cognitive science society pp 

ann arbor mi lawrence erlbaum 
anderson 

iterative bayesian algorithm categorization 
fisher pazzani langley 
eds concept formation knowledge experience unsupervised learning 
san mateo ca morgan kaufmann 
biswas weinberg li 

iterate conceptual clustering method knowledge discovery databases 
braunschweig day 
eds innovative applications artificial intelligence oil gas industry 
editions 
biswas weinberg yang koller 

conceptual clustering exploratory data analysis 
proceedings eighth international machine learning workshop pp 

san mateo ca morgan kaufmann 
carpineto romano 

galois order theoretic approach conceptual clustering 
proceedings tenth international conference machine learning pp 

amherst ma morgan kaufmann 
cheeseman kelly self stutz taylor freeman 

autoclass bayesian classification system 
proceedings fifth international machine learning conference pp 

ann arbor mi morgan kaufmann 
gluck 

explaining basic categories feature predictability information 
psychological bulletin 
de da veiga 

data analysis biomedical research novel methodological approach implementation conceptual clustering algorithm portuguese 
ph thesis universidade de coimbra de atica inform atica da de 


description contrasting incremental concept formation 
kodratoff 
ed machine learning ewsl lecture notes artificial intelligence pp 

springer verlag 
devaney ram 

personal communication oct 
duda hart 

pattern classification scene analysis 
new york ny wiley sons 
optimization hierarchical clusterings everitt 

cluster analysis 
london heinemann 
fayyad 

induction decision trees multiple concept learning 
ph thesis university michigan ann arbor mi department computer science engineering 
fisher 

optimization simplification hierarchical clusterings 
proceedings international conference knowledge discovery data mining pp 

menlo park ca aaai press 
fisher 

database management analysis tools machine induction 
journal intelligent information systems 
fisher xu reich chen biswas weinberg 

applying ai clustering engineering tasks 
ieee expert 
fisher xu 

ordering effects clustering 
proceedings ninth international conference machine learning pp 

san mateo ca morgan kaufmann 
fisher 

knowledge acquisition incremental conceptual clustering 
machine learning 
fisher 

knowledge acquisition incremental conceptual clustering 
ph thesis university california irvine ca department information computer science 
fisher 

noise tolerant conceptual clustering 
proceedings international joint conference artificial intelligence pp 

detroit mi morgan kaufmann 
fisher langley 

structure formation natural categories 
bower 
ed psychology learning motivation vol 

san diego ca academic press 
fisher schlimmer 

concept simplification prediction accuracy 
proceedings fifth international conference machine learning pp 

ann arbor mi morgan kaufmann 
gennari 

focused concept formation 
proceedings sixth international workshop machine learning pp 

san mateo ca morgan kaufmann 
gennari langley fisher 

models incremental concept formation 
artificial intelligence 
gluck 

information uncertainty utility categories 
proceedings seventh annual conference cognitive science society pp 

hillsdale nj lawrence erlbaum 
fisher yun 

concept formation incremental conceptual clustering 
proceedings international joint conference artificial intelligence pp 

san mateo ca morgan kaufmann 
hanson stutz cheeseman 

bayesian classification correlation inheritance 
proceedings th international joint conference artificial intelligence pp 

san mateo ca morgan kaufmann 
iba 

heuristic approach discovery macro operators 
machine learning 
iba gennari 

learning recognize movements 
fisher pazzani langley 
eds concept formation knowledge experience unsupervised learning 
san mateo ca morgan kaufmann 


hierarchical clustering composite objects variable number components 
preliminary papers fifth international workshop artificial intelligence statistics pp 



incremental conceptual clustering line application 
ph thesis stockholm university stockholm sweden department computer systems sciences 
kolodner 

reconstructive memory computer model 
cognitive science 
lebowitz 

correcting erroneous generalizations 
cognition brain theory 
lebowitz 

experiments incremental concept formation 
machine learning 
levinson 

self organizing retrieval system graphs 
proceedings national conference artificial intelligence pp 

san mateo ca morgan kaufmann 
lopez de mantaras 

distance attribute selection measure decision tree induction 
machine learning 
martin 

acquiring combining overlapping concepts 
machine learning 
mckusick langley 

constraints tree structure concept formation 
proceedings international joint conference artificial intelligence pp 

san mateo ca morgan kaufmann 
mckusick thompson 

cobweb portable implementation tech 
rep 
fia 
moffett field ca ai research branch nasa ames research center 
optimization hierarchical clusterings medin 

structural principles categorization 
shepp 
eds perception cognition development pp 

hillsdale nj lawrence erlbaum 
michalski stepp 

automated construction classifications conceptual clustering versus numerical taxonomy 
ieee transactions pattern analysis machine intelligence 
michalski stepp 

learning observation conceptual clustering 
michalski carbonell mitchell 
eds machine learning artificial intelligence approach 
san mateo ca morgan kaufmann 
mingers 

empirical comparison pruning methods decision tree induction 
machine learning 
mingers 

empirical comparison selection measures decision tree induction 
machine learning 


branch bound incremental conceptual clusterer 
machine learning 
quinlan 

induction decision trees 
machine learning 
quinlan 

simplifying decision trees 
international journal man machine studies 
quinlan 

programs machine learning 
san mateo ca morgan kaufmann 
reich 

formation concepts design 
fisher pazzani langley 
eds concept formation knowledge experience unsupervised learning 
san mateo ca morgan kaufmann 
utgoff 

improved algorithm incremental induction decision trees 
proceedings eleventh international conference machine learning pp 

san mateo ca morgan kaufmann 
wallace dowe 

intrinsic classification mml program 
proceedings th australian joint conference artificial intelligence pp 

une nsw australia world scientific 
weiss kulikowski 

computer systems learn 
san mateo ca morgan kaufmann 
wilcox levinson 

self organized knowledge base recall design discovery organic chemistry 
pierce 
eds artificial intelligence applications chemistry 
washington dc american chemical society 

