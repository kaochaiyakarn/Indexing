lexical modeling non native speech automatic speech recognition karen james glass spoken language systems group laboratory computer science massachusetts institute technology cambridge ma usa examines recognition non native speech jupiter speaker independent spontaneous speech conversational system 
non native speech domain limited varied speaker accent specific methods impractical 
chose model non native data single model 
particular describes attempt better model non native lexical patterns 
patterns incorporated applying context independent phonetic confusion rules probabilities estimated training data 
approach word error rate non native test set reduced 

speech recognition accuracy observed drastically lower non native speakers target language native speakers 
research nonnative accent modeling dialect specific modeling shows large gains performance achieved acoustics pronunciation new accent dialect taken account :10.1.1.49.1074
non native accents problematic dialects larger number non native accents language variability speakers non native accent potentially greater speakers dialect due di erent levels familiarity target language individual tendencies 
previous non native speech involved modeling particular accent particular speaker 
described deals speech recognizer jupiter speaker independent spontaneous speech conversational system interacts non native speakers diverse backgrounds 
furthermore relatively small amount non native jupiter data corpus contains nonnative utterances contain vocabulary words compared native utterances vocabulary 
situation impractical model non native accent separately classify ac supported national science foundation graduate research fellowship 
iri 
cents 
possibilities speaker adaptation limited speaker interaction system typically utterances long 
main goal discover performance gains obtained modeling non native speech single model 
sections describe methods automatically discover non native pronunciation variations results recognition experiments applying methods 

lexical modeling section explore modifications lexicon incorporate non native pronunciations 
ideally collect entire word pronunciations train probabilities real non native data 
instances word training set attempt derive rules data apply baseline lexicon 
methods findings simple type rule context independent phonetic confusions 
context dependent rules contain information larger required number parameters di cult train limited amount training data 

modeling pronunciation patterns finite state transducers recognition engine finite state transducer version summit segment recognition system 
baseline recognizer search space represented composition fst phonetic graph associated acoustic scores lexicon language model 
non native pronunciation rules incorporated introducing additional fst lexicon phonetic graph modified search space represented rc represent number phenomena arc weights represent probabilities phenomena 
case context independent confusion rules consists single state self loops representing allowed confusions 
portion may look ix iy ix ix ix ch sh sh sh arc labeled indicates lexical label realized surface label probability fst indicates example lexical sh realized surface sh probability ch probability 
insertions deletions represented transitions null unit 
full contain arc lexical label lexical pronunciation surface realization 
refer confusion fst 
arc probabilities estimated training data 
ect composing add arc set parallel arcs corresponding possible realizations lexical phone arc example lexicon containing word chicago pronunciation may look sh chicago ix kcl aa ow contains confusions shown plus lexical labels sh chicago ch chicago ix iy kcl aa ow approach similar respects previous lexical modeling 
levinson obtained word hypotheses aligning outputs phonetic recognizer lexicon grammar 
confusion weights determined acoustic similarity measure 
teixeira similar current approach pronunciation weights estimated training data 
riley train probabilistic context dependent phoneme phone mappings obtain phonetic lexicon native speakers 
approach similar subword lexical modeling framework phonological rules trainable probabilities 

estimation confusion probabilities order estimate confusion probabilities need phonetic transcription utterance training set aligned corresponding pronunciation word appears lexicon 
approach transcriptions generated automatically see section aligned lexicon automatic string alignment procedure 
maximum likelihood ml estimates confusion probabilities computed frequencies confusions alignments 
substitutions deletions lexical label occurs times confusion occurs times ml estimate probability case insertions hand need take priori probability insertion account 
ml estimate priori probability ins tot number insertions tot total number aligned phones 
insertion occurs estimated probability inserted phone ins ns ins ns ins number insertions 
total estimated probability insertion ins ins ins ns ins tot 
computational details composition lexicon larger lexicon factor ns number surface labels 
time space requirements recognizer prohibitively large 
precomputing composition precompute compose result dynamically recognition 
dynamic composition portions search space rc created necessary expand hypotheses considered 
order limit size rc prune including arcs probabilities threshold 
addition experiments reduce time space requirements narrower beam recognition search baseline recognizer 
initially increases error rate allows experiment larger range sizes 
due memory computational constraints smooth probabilities account sparse training data 
ensure baseline pronunciation word allowed include self confusions minimum probability 

experiments section describes recognition experiments performed 
experiments vocabulary non native utterances divided utterance training set utterance development set parameter tuning utterance test set 
recognizer uses diphone acoustic models features mfcc averaged regions near boundary segmentation 
modeled diagonal gaussian mixtures components model trained set containing native utterances non native training set 
basic lexical units consist phone labels 
lexicon contains words alternative pronunciations 
language model word trigram 
configuration similar 
baseline recognizer achieves word error rate wer non native test set native test set 

probability estimation phonetic recognition hypotheses set experiments transcription training utterance simply best hypothesis produced phonetic recognizer 
phonetic recognizer uses acoustic models word recognizer 
order constrain transcriptions little possible minimizing obvious transcription errors phone bigram language model task 
alignments performed equal weights substitutions deletions insertions 
refer phonetic recognition pr method 
similar methods derive transcriptions training pronunciation rules 
method non native training set obtain cpr containing confusions maximum phone labels including null label 
tested derived cpr varying pruning thresholds 
threshold expressed negative log probability higher threshold larger 
shows results obtained non native test set 
identity mapping contains half confusions cpr shows series results di erent beam widths recognition viterbi search 
tested beam widths development set wer reductions obtained baseline improvements test set 
minimum wer 
increase wer may indicate low probability arcs trained adding increases words 
lowest wer obtained series experiments 
di erence significant matched pair sentence segment test 

probability estimation forced paths pr method results large accuracy limited alignments 
furthermore reason believe created way unnecessarily large phonetic recognizer information word recognizer available 
pr method build word recognizer need word sequence constrained match particular string phones 
word baseline word error rate non native test set function pruning threshold pr method 
recognizer entire phonetic graph disposal search alternate phones better match lexicon necessary 
reason may need expand lexicon great extent 
alternate approach uses entire phone graph generate forced transcriptions lexicon consisting known word string utterance expanded pre existing words transcription best path fst rfp phone graph represents known word transcription utterance baseline pronunciations 
cpr pruned threshold padded minimum probability 
pruning threshold chosen transcriptions computed reasonable time real time allowing confusions cpr refer forced path fp method 
method obtain cfp containing confusions 
large decrease relative pr method 
visual inspection cfp confusion statistics appear conform better expectations probability mass concentrated confusions expected non native confusions iy ih uw uh receive higher probability estimates pr method 
shows wer obtained non native test set 
contains confusions cfp case narrower search beam 
experiments development set recognition wider beam took prohibitively long time wer lower narrower beam larger 
reason wer higher baseline low 
word error rate baseline word error rate non native test set function pruning threshold fp method 
lowest wer 
di erence baseline significant level matched pair sentence segment test 
aimed non native speakers tested recognizer cfp native test set 
interestingly configuration yields wer native speakers baseline recognizer 
encouraging sign combining native non native models 

described demonstrates pronunciation patterns non native speakers group modeled automatically trained phone confusions represented simple finite state transducer 
order methods practical necessary computational improvements reduce running time memory requirements 
possible extensions 
example may possible improve performance iteratively training smoothing confusion probabilities 
initial attempts iterative training smoothing yielded improvements 
interesting train context dependent rules may feasible training data available grouping phone labels classes existing data 
emphasized immediate goal improve recognition non native speakers group 
possibilities accent speaker specific modeling limited domain jupiter additional gains may obtained instantaneous incremental adaptation user interaction speaker clustering training 
obvious avenue combination native non native models single recognizer performance population close possible population optimized models 

acknowledgments lee provided helpful suggestions fst confusion probabilities 
sally lee initial tagging accented jupiter callers feasible divide data accent categories 
digalakis neumeyer 
development dialect specific speech recognizers adaptation methods 
proc 
icassp 
glass hazen 
realtime telephone speech recognition jupiter domain 
proc 
icassp 
glass hazen 
telephone conversational speech recognition jupiter domain 
proc 
icslp 
hanna stewart ming 
application improved dp match automatic lexicon generation 
proc 
eurospeech 
humphries woodland pearce 
accent specific pronunciation modelling robust speech recognition 
proc 
icslp 
levinson miller 
continuous speech recognition phonetic transcription 
proc 
icassp 
liu fung 
fast accent identification accented speech recognition 
proc 
icassp 

analysis modeling non native speech automatic speech recognition 
master thesis mit august 
nguyen ph 


chien 
best supervised unsupervised adaptation native non native speakers 
proc 
icassp 
pallet fisher fiscus 
tools analysis benchmark speech recognition tests 
proc 
icassp 
riley 
automatic generation detailed pronunciation lexicons 

lee paliwal editors automatic speech speaker recognition 
kluwer academic publishers boston 
lau meng 
new framework speech analysis morpho phonological modelling 
proc 
icslp 
teixeira 
recognition non native accents 
proc 
eurospeech 
schwartz makhoul 
adaptation algorithms bbn phonetically tied mixture system 
proc 
arpa spoken language systems technology workshop 
