measuring vc dimension learning machine vladimir vapnik esther levin yann le cun bell laboratories corner road holmdel nj method measuring capacity learning machines described 
method fitting theoretically derived function empirical measurements maximal difference error rates separate data sets varying sizes 
experimental measurements capacity various types linear classifiers 

theoretical experimental studies shown influence capacity learning machine generalization ability vapnik baum haussler le cun weigend rumelhart huberman guyon abu mostafa 
learning machines small capacity may require large training sets approach best possible solution lowest error rate test sets 
high capacity learning machines hand may provide better asymptotical solutions lower test error rate large training sets may require large amounts training data reach acceptable test performance 
training set size difference training error test error larger high capacity machines 
theory learning vc dimension predicts behavior difference training error test error function training set size characterized single quantity vc dimension characterizes machine capacity vapnik 
introduce empirical method measuring capacity learning machine 
method formula maximum deviation frequency errors produced machine separate data sets function capacity machine size data sets 
main idea capacity learning machine measured finding capacity produces best fit formula set experimental measurements frequency errors data sets varying sizes 
paradigm learning examples learning machine learn approximate possible unknown target rule input output relation training set labeled examples 
input output pairs composing training set ae assumed drawn independently unknown distribution function jx 
describes region interest input space distribution jx describes target input output relation 
learning machine characterized set binary classification functions ff ff ff parameter specifies function set admissible parameters appear neural computation 
copyright fl realize indicator functions 
goal learning choose function ff set minimizes probability error probability disagreement value output learning machine ff ff ej 
gamma ff expectation taken respect probability distribution 
problem distribution unknown way assess ff frequency errors computed training set ff 
gamma ff learning algorithm called principle empirical risk minimization consists picking function ff minimizes number error training set 
vapnik shown algorithms minimize empirical risk knowledge quantities allows establish upper bound probability error ff 
quantity capacity learning machine measured vc dimension set functions implement 
second frequency errors training set empirical risk ff 
third size training set probability gamma bound ff ff ff simultaneously valid ff including ff minimizes empirical risk ff 
function form ff ln gamma ln ff ln gamma ln universal constant 
interpreted confidence interval difference training error true error 
regimes behavior function simplifies 
regime training error happens small large capacity small training set size easy task approximated ff ln gamma ln training error large near approximated ff ln gamma ln unfortunately theoretical estimates vc dimension obtained handful simple classes functions notably class linear discriminant functions 
set linear discriminant functions defined ff delta ff ff delta ff dot product vectors ff threshold function ae vc dimension set linear discriminant functions inputs equal vapnik 
attempts obtain exact value vc dimension classes decision rules encountered substantial difficulties 
estimated values appear exceed real large amount 
poor estimates result crudely overestimated evaluations confidence interval article considers possibility empirically estimating vc dimension learning machine 
idea possible inspired observations 
shown vapnik chervonenkis learning algorithm minimizes empirical risk minimizes error training set consistent sided uniform convergence condition holds lim ff ff gamma ff words sided uniform convergence set function frequencies probabilities necessary sufficient condition consistency learning process 
kolmogorov smirnov law distribution maximal deviation distribution function empirical distribution function random variable 
result formulated follows 
set functions ff gamma ff ff gamma equality ff ff gamma ff expf gamma lg gamma gamma expf gamma lg holds sufficiently large ff ef ff ff ff 
equality independent probability measure note second term small compared 
vapnik chervonenkis shown set indicator functions ff ff finite vc dimension rate uniform convergence bounded follows ff ff gamma ff min exp ae ln gamma oe universal constants 
inequality independent probability measure 
direct consequence ffi exists ffi inequality ff ff gamma ff expf gamma gamma ffi lg holds 
vapnik chervonenkis shown 
result improved devroye 
interestingly inequality close kolmogorov smirnov equality obtained simple set functions 
means upper bound asymptotically close exact value 
tightening upper bound value theoretically difficult term multiplies main term exponential large values suppose exists value constant upper bound independent probability measure tight 
furthermore suppose bound tight large numbers observations smaller numbers statisticians commonly asymptotic kolmogorov smirnov test starting learning processes usually involves hundreds observations 
case expect function phi defined phi sup ff ff gamma ff ff ff gamma ff gd independent probability measure small values assuming know functional form phi experimentally estimate expected maximal deviation empirical risk expected risk various values measure fitting phi measurements 
remainder concerned finding appropriate functional form phi applying measuring vc dimension various learning machines 
learning algorithm said consistent probability error training set converges true probability error size training set goes infinity 
practice measuring maximum difference training set error error infinite test set convenient measure maximum difference error rates measured separate sets ff ff gamma ff ff ff frequencies error calculated different samples size introduce approximate functional form right hand side phi relation phi wide range values construct approximation determine section different bounds bound valid large bound valid small section introduce notion effective vc dimension reflects weak properties unknown probability distribution 
effective vc dimension exceed vc dimension show functional form bounds obtained vc dimension valid effective vc dimension 
consequence bounds obtained effective vc dimension tighter 
section introduce single functional form inspired bound bounds small large hypothesize function describes expectations term effective vc dimension 
assuming hypothesis true section proposes method measuring effective vc dimension learning machine 
section demonstrate proposed method measuring vc dimension various classes linear decision rules 
estimation maximum deviation 
section establish formal definition vc dimension give bounds maximum deviation frequencies error separate sets 
definition vc dimension set indicator functions ff ff maximal number vectors set shattered ff ff vectors said shattered ff ff possible partition vectors classes partitions exists function ff implements partition sense ff ff estimate bound expectation random variable need define random independent sample vectors class 

denote set samples size 
denote ff frequency erroneous classifications vectors half sample obtained decision rule ff ff 
gamma ff denote ff frequency erroneous classification vectors obtained decision rule ff ff 
gamma ff study properties random variable sup ff ff gamma ff maximal deviation error frequencies half samples set functions 
exists upper bound expectation random value varying sample size consider cases 
case 
case trivial bound sup ff ff gamma ff case small 
supremum difference ff gamma ff attained function ff ff ff 
consider event ffi fz ff gamma ff ff gamma ff ffi ff ff ff denote probability event ffi 
appendix prove theorem theorem 
set indicator functions ff ff vc dimension bound conditional expectation ff ff gamma ff jb ffi ffi ln gamma ln ffi valid 
fact deviation frequencies half samples exceed obtain ff ff gamma ff ffi ffi ln gamma ln ffi gamma ffi bound ffi small ffi probability ffi close 
situation arise ratio large large 
case ffi close large large bound ff ff gamma ff ln valid constant 
shall bound small constructing empirical estimate expectation 
case large 
appendix show set indicator functions vc dimension bound holds ff ff gamma ff ln constant 
bound true shall large bound case valid 
conducted experiments described section find 
effective vc dimension 
definition vc dimension depend input probability measure 
purpose introduce concept effective vc dimension reflects weak dependence properties probability measure 
probability measure subset set probability measure close gamma small set indicator functions ff ff vc dimension defined set functions ae vc dimension appendix prove theorem 
theorem 
inequality el el fulfilled bounds ff ff gamma ff jb ffi ffi ln gamma ln ffi ff ff gamma ff ln lh ln valid 

inequalities true note case left hand side inequalities depend right hand side 
tightest inequality achieved smallest definition 
effective vc dimension set ff ff measure minimal vc dimension set functions defined subsets ae measure gamma small value 
previous section obtain effective vc dimension inequality true ff ff gamma ff ffi ffi ln gamma ln ffi gamma ffi case ffi close quantity small large obtain ff ff gamma ff ln constant 
bounds form bounds simplify notation sections denote effective vc dimension 
law largest deviations 
section gave bounds different cases expectation largest deviation set decision rules 
section give single functional form reflects properties bounds 
conjecture functional form approximates expectation largest deviation 
bounds previous section summarized follows ff ff gamma ff ln small ln large 
consider continuous approximation right side bound phi ln gammak gammak ln function phi free parameters third parameter chosen conditions continuity point phi 
note function phi structure confidence interval frequency ff replaced constant 
small phi behaves ln gamma large ln gamma constants determine regions large small values hypothesis exist constants weakly dependent properties input probability measure function phi approximates expectation sufficiently ff ff gamma ff phi constants empirically fitting phi experimental data machine known 
assume constants obtained universal function phi obtained way measure capacity learning machines 
measuring effective vc dimension classifier function phi approximates expectation sufficient accuracy effective vc dimension set indicator functions realizable classifier estimated basis experiment 
generate random independent set size generator random vectors possibly non deterministic generator labels jx 
sample measure quantity sup ff ff gamma ff approximating expectation average independently generated sets size 
repeating procedure various values obtain set estimates 
effective vc dimension set functions ff ff approximated finding integer parameter provides best fit phi arg min gamma phi accuracy obtained vc dimension estimate fact validity approach depend crucially function phi describes expectation 
estimate expectation largest deviation empirically able define fixed sample value largest deviation 
done considering modified training set labels half set reversed denotes opposite class 
shown appendix evaluating done minimizing functional ff gamma ff 
gamma ff empirical measurement effective vc dimension section illustrate method measuring vc dimension applying various types linear classifiers 
method relies assumptions checked experimentally ffl expected deviation ef largely independent generator distribution jx 
ffl expected deviation ef depends choice learning machine parameter ffl expected deviation ef described function phi equation fixed parameters ffl values free parameters constant values wide classes learning machines 
experiments learning machine implement various subsets set linear classification functions conducted check hypotheses 
vc dimension set linear classifiers bias known theory equal dimension input space experiments described conducted 
additional experiments various values ranging performed similar results 
efficient algorithm known minimizing classification error linear threshold units trained machine minimizing mean squared error labels output unit hard threshold replaced sigmoid function procedure ensure classification error minimized known empirically give approximate solutions separable non separable cases 
training output labels computed thresholding output sigmoid 
independence average deviations task difficulty set experiments performed assess influence difficulty task important property conditional probability distribution jx deviation 
ensembles training sets generated expected frequency errors linear classifier respectively 
randomly selected set dimensional input vectors coordinates independently drawn uniform distribution gamma interval 
labels experiments generated picking dimensional vector coefficients ff random generating labels conditional probability distributions jx ae ff delta gamma ff delta jx ae ff delta gamma ff delta jx ae ff delta gamma ff delta corresponds linearly separable task noise random classification task random labels generalization expected task intermediate difficulty 
shows average values deviations function size half set conditional probabilities 
average value obtained realizations deviations 
seen results cases practically coincide demonstrating independence difficulty task 
experiments conditional probability jx 
estimation free parameters phi value free parameters phi eq determined fit equation set experimentally obtained values average maximal deviation produced learning machine known capacity assuming optimal values obtained universal simply consider constant measurement capacity learning machines 
shows approximation empirical data phi parameters set 
note function phi describes average maximal deviation full range training set sizes experiments 
experiments various input sizes yielded consistent values parameters 
simpler functional form deviation phi phi ln gamma inspired bound small describes data small 
shows approximation empirical data phi 
values obtained experiment experiments described 
control experiments validation proposed method series control experiments conducted 
experiments described method measure effective vc dimension learning machines compared theoretically known values 
learning machine composed fixed preprocessor transformed dimensional input vector dimensional vector linear projection subspace dimension resulting vector fed linear classifier inputs 
easy see theoretical value effective vc dimension reduced linear classifier inputs 
table shows estimated vc dimension indicates strong agreement estimated effective vc dimension theoretically predicted dimension 
effective vc dim 
estimated vc dim 
table true measured values vc dimension control experiments figures demonstrate function phi estimate effective describes experimental data 
smoothing experiments section measure effect smoothing input capacity 
contrary previous section effect predicted theoretically 
previous section learning machine incorporated linear preprocessor transformed smoothing operation expf gamma ij jj gamma ij ae gamma gamma described previously classifier trained gradient descent small weight decay term added reason weight decay explained 
parameter fi determines amount smoothing large fi components vector virtually independent vc dimension learning machine close value fi decreases strong correlations input variables start appear 
causes distribution preprocessed vectors small variance directions 
intuitively appropriate weight decay term dimensions effectively invisible linear classifier reducing measured effective vc dimension 
measured vc dimension decreases fi 
fi takes non zero values theoretically known evaluation vc dimension success method measured experimental data described functional phi 
figures show average maximal deviation function different smoothing parameters fi demonstrates function phi estimate effective vc dimension approximates experimental data 
shows estimated value vc dimension function smoothing parameter fi 
shown exist different regimes behavior maximum possible deviation frequencies error different sets examples 
small values 
maximal deviation small values behaves log large values behaves log 
introduced concept effective vc dimension takes account weak properties probability distribution input space 
prove effective vc dimension place vc dimension formulae obtained previously 
provides tighter bounds maximal deviation 
functional forms bounds regimes propose single formula contains free parameters 
shown value parameters evaluated experimentally 
show formula estimate effective vc dimension 
illustrate method applying various learning machines linear classifiers 
experiments show agreement model experimental data cases 
interestingly appears set linear classifiers values parameters universal 
universality confirmed results obtained version linear classifiers trained real life tasks image classification 
excellent fit values constants obtained classifiers trained minimizing empirical risk subject various types constraints 
included simple constraints limit norm weight vector equivalent weight decay complex ones improve invariance classifier distortions input image limiting norm lie derivatives cortes guyon 
extension multilayer networks faces difficulties 
theory derived methods minimize empirical risk 
existing learning algorithms multilayer nets viewed minimizing empirical risk entire set functions implementable network 
energy surface multiple local minima initial parameter value picked search confined subset possible functions realizable network 
capacity subset capacity set may explain neural nets large numbers parameters better theoretically expected 
second number local minima may change number examples capacity search subset may change number observations 
may require theory considers notion non constant capacity associated active subset functions 
appendix 
proof theorem 
random independent sample 
call classes equivalence set ff ff sample subset indicator functions ff ff take value vectors 
denote number different classes equivalence sample delta delta consider event ffl fz sup ff ff gamma ff fflg fz ff gamma ff fflg event ffi defined equation 
lemma true lemma 
ffl ffi delta expf gamma ffi proof lemma follow scheme proof theorem vapnik pp 

write probability evident form ffl ffi ff gamma ff gamma fflg ff gamma ff ff gamma ff gamma ffi gdp space samples ff ff ff ff function attains maximum deviation frequencies half samples 
denote 
set possible permutations elements sample 
denote ff ff gamma ff oe ff ff gamma ff obvious equality ffl ffi ff gamma fflg ff oe ff gamma ffi gdp 

ff gamma fflg ff oe ff gamma ffi gdp true 
note fixed inequality true ff gamma fflg ff oe ff gamma ffi ff delta ff gamma fflg ff oe ff gamma ffi bound estimate integrand right hand side equality 
exceed value ff delta 

ff gamma fflg ff oe ff gamma ffi expression brackets fraction number permutations events ff gamma ff ffl ff gamma ff ff gamma ff ffi hold simultaneously possible 
permutations 
equal value gamma gammak gammam ff ff summation performed satisfying inequalities gamma gamma ffl gamma gamma ffi gamma gamma estimate value gamma 
derivation estimate gamma repeats derivation similar estimates vapnik pp 

ln gamma gamma gammam gamma number gamma gammam gamma gamma gamma gamma add number integer satisfying condition gamma ffl obtain ln gamma gamma ffi gamma inequality obtain ln gamma gammaffi substitute estimate value gamma right hand side inequality 
obtain pfa ffl ffi ff delta expf gamma ffi gdp delta expf gamma ffi lemma proved 
note inequality delta max delta true 
function known growth function estimated means vc dimension vapnik 
le pfa ffl ffi 
le gamma pfa ffl jb ffi 
le gamma ffi estimate conditional expectation ffl jb ffi ff ff gamma ff ffi estimation divide interval area parts trivial estimate conditional probability ffl jb ffi area estimate 
obtain ffl jb ffi dffl le ffi expf gamma ffi le ffi expf gamma ffi ul minimum right hand side inequality attained ln gamma ln ffi ffi way obtain estimate ffl jb ffi ln gamma ln ffi ffi theorem proved 
want obtain estimate expectation random value arbitrary inequality vapnik ff ff gamma ff fflg le expf gammaffl lg case get ff ff gamma ff jg dffl le expf gammaffl le expf le lu expf ln obtain estimate ff ff gamma ff jg ln lh ln ln appendix 
proof theorem 
prove part theorem 
lemma proved appendix inequality ffl ffi delta expf true 
follows ffl jb ffi delta ffi expf gamma fflffi delta dp expf gamma ffi divide integration domain parts gamma gamma gamma sample space elements belong expf gamma fflffi delta dp expf gamma fflffi gamma delta dp gamma delta dp conditions theorem area gamma bound delta le holds area gamma bound delta le holds 
follows ffl jb ffi expf gamma ffi le le measure gamma note gamma gamma lj gamma measure set condition theorem inequality le le holds 
derive le le substituting obtain ffl jb ffi 
le expf gamma ffi appendix obtain estimate expectation 
derive estimate similarly 
appendix estimation maximum error rate difference half samples estimate value maximal error rate difference learning machine half sets maximal deviation 
denote sequence obtained set equation changing values half set 
gamma training set learning machine 
case training results minimization ff 
gamma ff 
gamma ff easy see minimum obtained function ff maximizes deviation 
ff binary values 
gamma ff gamma 
gamma ff follows ff gamma 
gamma ff 
gamma ff minimizing ff equivalent maximizing ff 
gamma ff gamma 
gamma ff ff gamma ff summarize maximization deviation obtained training learning machine training set value maximal deviation related minimum training functional ff gamma ff abu mostafa 

hints vc dimension 
neural computation 
baum haussler 

size net gives valid 
neural computation 
cortes 

communication 
devroye 

bounds uniform deviations empirical measures 
journal multivariate analysis 
guyon vapnik boser bottou solla 

structural risk minimization character recognition 
moody hanson lippmann editors advances neural information processing systems denver morgan kaufman 
le cun denker solla howard jackel 

optimal brain damage 
touretzky editor advances neural information processing systems denver morgan kaufman 
vapnik 

estimation dependencies empirical data 
springer verlag new york 
vapnik chervonenkis 

uniform convergence relative frequencies events probabilities 
theory probability applications 
vapnik chervonenkis 

necessary sufficient conditions consistency empirical risk minimization method 
pattern recognition image analysis 
weigend rumelhart huberman 

generalization weight elimination application forecasting 
lippmann moody touretzky editors advances neural information processing systems denver morgan kaufman 
measured values average deviation shown function size half sample normalized vc dimension 
values obtained experiments different conditional probabilities jx 
symbol denotes values measured symbol denotes values measured symbol fl denotes values measured values average deviation fitted phi parameter values 
values average deviation fitted phi 
values average deviation measured control experiments fitted phi measured value vc dimension shown function 

values average deviation measured experiments different values smoothing parameter fi fitted phi measured value vc dimension shown function 
fi 
estimated vc dimension 
fi 
estimated vc dimension 
fi 
estimated vc dimension 
fi 
estimated vc dimension 
vc dim 
estimated effective vc dimension function smoothing parameter fi 

