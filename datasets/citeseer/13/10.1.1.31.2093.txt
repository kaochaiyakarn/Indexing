fl kluwer academic publishers boston 
manufactured netherlands 
variational methods graphical models michael jordan jordan cs berkeley edu department electrical engineering computer sciences department statistics university california berkeley ca zoubin ghahramani zoubin gatsby ucl ac uk gatsby computational neuroscience unit university college london wc ar tommi jaakkola tommi ai mit edu artificial intelligence laboratory mit cambridge ma lawrence saul research att edu labs research florham park nj editor david heckerman 
presents tutorial variational methods inference learning graphical models bayesian networks markov random fields 
number examples graphical models including qmr dt database sigmoid belief network boltzmann machine variants hidden markov models infeasible run exact inference algorithms 
introduce variational methods exploit laws large numbers transform original graphical model simplified graphical model inference efficient 
inference model provides bounds probabilities interest original model 
describe general framework generating variational transformations convex duality 
return examples demonstrate variational algorithms formulated case 
keywords graphical models bayesian networks belief networks probabilistic inference approximate inference variational methods mean field methods hidden markov models boltzmann machines neural networks 
problem probabilistic inference graphical models problem computing conditional probability distribution values nodes hidden unobserved nodes values nodes evidence observed nodes 
letting represent set hidden nodes letting represent set evidence nodes wish calculate hje hje general exact inference algorithms developed perform calculation jensen shachter andersen szolovits shenoy algorithms take systematic advantage conditional independencies joint distribution inferred pattern missing edges graph 
wish calculate marginal probabilities graphical models particular probability observed evidence 
viewed function parameters graphical model fixed important quantity known likelihood 
suggested eq 
evaluation likelihood closely related calculation hje 
inference algorithms simply compute numerator denominator eq 
divide fact generally produce likelihood product calculation hje 
algorithms maximize likelihood related quantities generally calculation hje subroutine 
cases exact algorithms provide satisfactory solution inference learning problems cases discuss time space complexity exact calculation unacceptable necessary recourse approximation procedures 
context junction tree construction example time complexity exponential size maximal clique junction tree 
see natural architectural assumptions necessarily lead large cliques 
cases complexity exact algorithms manageable reason consider approximation procedures 
note particular exact algorithms numerical representation joint probability distribution associated graphical model put way algorithms complexity regardless particular probability distribution consideration family distributions consistent conditional independencies implied graph 
may situations nodes clusters nodes nearly conditionally independent situations node probabilities determined subset neighbors node situations small subsets configurations variables contain probability mass cases achieved exact algorithm may worth computational cost 
variety approximation procedures developed attempt identify exploit situations 
examples include pruning algorithms kjaerulff bounded conditioning method horvitz suermondt cooper search methods henrion localized partial evaluation method draper hanks 
virtue methods closely tied exact methods able take full advantage conditional independencies 
virtue vice exponential growth complexity exact algorithms 
related approach approximate inference arisen applications graphical model inference error control decoding mceliece mackay cheng 
particular kim pearl algorithm singly connected graphical models pearl successfully iterative approximate method inference non singly connected graphs 
approach design approximation algorithms involves making monte carlo methods 
variety monte carlo algorithms developed see neal applied inference problem graphical models dagum luby fung gilks thomas spiegelhalter jensen kong kjaerulff pearl 
advantages algorithms include simplicity implementation theoretical guarantees convergence 
disadvantages monte carlo approach algorithms slow converge hard diagnose convergence 
chapter discuss variational methods provide approach design approximate inference algorithms 
variational methodology yields deterministic approximation procedures generally provide bounds probabilities interest 
basic intuition underlying variational methods complex graphs probabilistically simple particular graphs dense connectivity averaging phenomena come play rendering nodes relatively insensitive particular settings values neighbors 
advantage averaging phenomena lead simple accurate approximation procedures 
important emphasize various approaches inference outlined means mutually exclusive exploit complementary features graphical model formalism 
best solution problem may involve algorithm combines aspects different methods 
vein variational methods way emphasizes links exact methods 
see exact methods appear subroutines variational approximation cf 
jaakkola jordan saul jordan 
acknowledged outset art science current understanding variational methods applied probabilistic inference 
variational transformations form large open ended class approximations general mathematical picture transformations exploited yield bounds probabilities graphical models systematic algebra allows particular variational transformations matched optimally particular graphical models 
provide illustrative examples general families graphical models variational methods applied successfully provide general mathematical framework encompasses particular examples able provide assurance framework transfer easily examples 
section brief overview exact inference graphical models basing discussion junction tree algorithm 
section presents examples graphical models provide motivation variational methodology provide examples return develop detail proceed chapter 
core material variational approximation section 
sections fill details focusing sequential methods block methods respectively 
sections return examples variational approximations case 
section presents directions research 

directed graph parameterized associating local conditional probability node 
joint probability product local probabilities 

exact inference section provide brief overview exact inference graphical models represented junction tree algorithm relationships junction tree algorithm exact inference algorithms see shachter andersen szolovits see dechter shenoy developments exact inference 
intention provide complete description junction tree algorithm introduce moralization triangulation steps algorithm 
understanding steps create data structures determine run time inference algorithm suffice purposes 
comprehensive junction tree algorithm see jensen 
graphical models come basic flavors directed graphical models undirected graphical models 
directed graphical model known bayesian network specified numerically associating local conditional probabilities nodes acyclic directed graph 
conditional probabilities specify probability node values parents js represents set indices parents node represents corresponding set parent nodes see fig 

obtain joint probability distribution nodes graph sn take product local node probabilities js inference involves calculation conditional probabilities joint distribution 
undirected graphical model known markov random field specified numerically associating potentials cliques graph 
potential function set configurations clique setting 
undirected graph parameterized associating potential clique graph 
cliques example fs fs fs potential assigns positive real number configuration corresponding clique 
joint probability normalized product clique potentials 
values nodes clique associates positive real number configuration 
subset nodes forms clique associated potential oe see fig 

joint probability distribution nodes graph obtained product clique potentials oe total number cliques normalization factor obtained summing numerator configurations fsg oe keeping statistical mechanical terminology refer sum partition function 
junction tree algorithm compiles directed graphical models undirected graphical models subsequent inferential calculation carried undirected formalism 
step converts directed graph undirected graph called moralization 
initial graph undirected simply skip moralization step 
understand moralization note directed undirected cases joint probability distribution obtained product local functions 
directed case functions node conditional probabilities js 
fact probability nearly qualifies potential function certainly real valued function configurations set variables fs problem variables appear 
simplest non triangulated graph 
graph cycle chord 
adding chord nodes renders graph triangulated 
clique 
parents common child necessarily linked 
able utilize node conditional probabilities potential functions marry parents nodes undirected edges 
drop arrows edges graph 
result moral graph represent probability distribution original directed graph undirected formalism 
second phase junction tree algorithm somewhat complex 
phase known triangulation takes moral graph input produces output undirected graph additional edges possibly added 
graph special property allows recursive calculation probabilities take place 
particular triangulated graph possible build joint distribution proceeding sequentially graph conditioning blocks interconnected nodes predecessor blocks sequence 
simplest graph possible cycle cycle nodes shown fig 

try write joint probability sequentially example bja cjb djc see problem 
particular depends unable write joint probability sequence conditionals 
graph triangulated cycles chord chord edge non neighboring nodes 
graph fig 
triangulated triangulated adding chord fig 

graph write joint probability sequentially dja cjb 
generally graph triangulated possible arrange cliques graph data structure known junction tree 
junction tree running intersection property node appears cliques tree appears cliques lie path cliques 
property important consequence general algorithm probabilistic inference achieving local consistency cliques 
cliques assign marginal probability nodes common 
junction tree running intersection property local consistency implies global consistency 
probabilistic calculations performed junction tree involve marginalizing rescaling clique potentials achieve local consistency neighboring cliques 
time complexity performing calculation depends size cliques particular discrete data number values required represent potential exponential number nodes clique 
efficient inference critical obtain small cliques 
remainder investigate specific graphical models consider computational costs exact inference models 
cases able display obvious triangulation able lower bound size cliques triangulated graph considering cliques moral graph 
need consider specific algorithms triangulation discussion triangulation algorithms see kjaerulff 

examples section examples graphical models exact inference generally infeasible 
example involves diagnostic system fixed graphical model answer queries 
remaining examples involve estimation problems graphical model fit data subsequently prediction diagnosis 

qmr dt database qmr dt database large scale probabilistic database intended diagnostic aid domain internal medicine 
provide brief overview qmr dt database details see shwe middleton heckerman henrion horvitz lehmann cooper 
qmr dt database bipartite graphical model upper layer nodes represent diseases lower layer nodes represent symptoms see fig 

approximately disease nodes symptom nodes database 
evidence set observed symptoms henceforth refer observed symptoms findings represent vector findings symbol symbol denotes vector diseases 
nodes binary components binary random variables 
making conditional independencies implied bipartite form graph marginalizing unobserved symptom nodes obtain joint probability diseases findings jd jd diseases symptoms 
structure qmr dt graphical model 
shaded nodes represent evidence nodes referred findings 
prior probabilities diseases obtained shwe archival data 
conditional probabilities findings diseases jd obtained expert assessments noisy model 
conditional probability ith symptom absent jd expressed follows jd gamma gamma ij ij parameters obtained expert assessments 
considering case diseases absent see parameter interpreted probability ith finding disease 
effect additional disease contribute additional factor gamma ij probability ith finding absent 
find useful rewrite noisy model exponential form jd gamma ij gamma ij gamma ln gamma ij transformed parameters 
note probability positive finding follows jd gamma gamma ij gamma forms express noisy model generalized linear model 
form joint probability distribution products local probabilities jd eq 
see negative findings benign respect inference problem 
particular product exponential factors linear diseases cf 
eq 
yields joint probability exponential expression linear diseases 
negative finding incorporated joint probability linear number operations 
products probabilities positive findings hand yield cross products terms problematic exact inference 
cross product terms input hidden output 
layered graphical structure neural network 
input nodes output nodes comprise set evidence nodes 
couple diseases responsible explaining away phenomena arise noisy model see pearl 
unfortunately coupling terms lead exponential growth inferential complexity 
considering set standard diagnostic cases cpc cases see shwe jaakkola jordan median size maximal clique moralized qmr dt graph nodes 
considering triangulation step see diagnostic calculation qmr dt model generally infeasible 

neural networks graphical models neural networks layered graphs endowed nonlinear activation function node see fig 

consider activation functions bounded zero obtained logistic function gammaz 
treat neural network graphical model associating binary variable node interpreting activation node probability associated binary variable takes values 
example logistic function write js gamma ij gamma ij parameters associated edges parent nodes node bias parameter associated node sigmoid belief network introduced neal 
advantages treating neural network manner include ability perform diagnostic calculations handle missing data treat unsupervised learning footing supervised learning 
realizing benefits requires inference problem solved efficient way 
fact easy see exact inference infeasible general layered neural network models 
node neural network generally parents nodes preceding layer 
moralized neural network graph links hidden output 
moralization neural network 
output nodes evidence nodes training 
creates probabilistic dependencies hidden nodes captured edges added moralization 

boltzmann machine 
edge nodes associated factor exp ij contributes multiplicatively potential cliques containing edge 
node contributes factor exp potential 
nodes layer see fig 

links necessary exact inference general clear particular training neural network output nodes evidence nodes hidden units penultimate layer probabilistically dependent ancestors preceding hidden layers 
hidden units particular hidden layer time complexity inference ignoring additional growth clique size due triangulation 
neural networks dozens hundreds hidden units commonplace see training neural network exact inference generally feasible 

boltzmann machines boltzmann machine undirected graphical model binary valued nodes restricted set potential functions see fig 

particular clique potentials formed products boltzmann factors exponentials terms quadratic hinton sejnowski 
clique potential product factors expf ij factors expf 
pair nodes appear multiple overlapping cliques 
pair assume expression expf ij appears factor clique potential 
similarly factors expf assumed appear clique potential 
product clique potentials cf 
eq 
ij set ij nodes neighbors graph convention allows sum indiscriminately pairs respect clique boundaries 
refer negative exponent eq 
energy 
definition joint probability eq 
general form boltzmann distribution 
saul jordan pointed exact inference certain special cases boltzmann machine trees chains pairs coupled chains tractable proposed decimation algorithm purpose 
general boltzmann machines decimation immune exponential time complexity plagues exact methods 
despite fact boltzmann machine special class undirected graphical model special class virtue parameterization virtue conditional independence structure 
exact algorithms decimation junction tree algorithm solely graphical structure boltzmann machine efficient boltzmann machines general graphical models 
particular triangulate generic boltzmann machines including layered boltzmann machines grid boltzmann machines obtain intractably large cliques 
sampling algorithms traditionally attempt cope intractability boltzmann machine hinton sejnowski 
sampling algorithms overly slow considered faster mean field approximation peterson anderson 
describe mean field approximation boltzmann machines special form variational approximation approach provides lower bounds marginal probabilities 
discuss general variational algorithm provides upper lower bounds probabilities marginals conditionals boltzmann machines jaakkola jordan 

hidden markov models section briefly review hidden markov models 
hidden markov model hmm example graphical model exact inference tractable purpose discussing hmms lay groundwork discussion 
hmm represented graphical model 
left right spatial dimension represents time 
output nodes evidence nodes training process state nodes hidden 
intractable variations hmms sections 
see smyth heckerman jordan fuller discussion hmm graphical model 
hmm graphical model form chain see fig 

consider sequence multinomial state nodes assume conditional probability node immediate predecessor gamma independent preceding variables 
index thought time index 
chain assumed homogeneous matrix transition probabilities jx gamma invariant time 
require probability distribution initial state hmm model involves set output nodes emission probability law jx assumed time invariant 
hmm trained treating output nodes evidence nodes state nodes hidden nodes 
expectation maximization em algorithm baum petrie soules weiss dempster laird rubin generally update parameters algorithm involves simple iterative procedure having alternating steps run inference algorithm calculate conditional probabilities gamma update parameters weighted maximum likelihood weights conditional probabilities calculated step 
easy see exact inference tractable hmms 
moralization triangulation steps vacuous hmm time complexity read fig 
directly 
see maximal clique size dimensionality state node 
inference scales length time series 

factorial hidden markov models problem domains natural additional structural assumptions state space transition probabilities available 

factorial hmm chains 
transition matrices associated horizontal edges output probabilities determined matrices associated vertical edges 
simple hmm framework 
number structured variations hmms considered years see smyth generically variations viewed dynamic belief networks dean kanazawa kanazawa koller russell 
consider particularly simple variation hmm theme known factorial hidden markov model ghahramani jordan williams hinton 
graphical model factorial hmm fhmm shown fig 

system composed set chains indexed state node mth chain time represented transition matrix mth chain represented view effective state space fhmm cartesian product state spaces associated individual chains 
transition probability system obtained product intra chain transition probabilities jx gamma jx gamma symbol stands tuple 
ghahramani jordan utilized linear gaussian distribution emission probabilities fhmm 
particular assumed jx sigma sigma matrices parameters 


triangulation fhmm component chains 
moralization step links states single time step 
triangulation step links states diagonally neighboring time steps 
fhmm natural model systems hidden state realized joint configuration uncoupled set dynamical systems 
fhmm able represent large effective state space smaller number parameters single unstructured cartesian product hmm 
example chains chain nodes states effective state space size transition probabilities represented compactly parameters 
single unstructured hmm require parameters transition matrix case 
fact output function states chains implies states stochastically coupled outputs observed 
investigate implications fact time complexity exact inference fhmm 
fig 
shows triangulation case chains fact optimal triangulation 
cliques hidden states size time complexity exact inference number states chain assume chain number states simplicity 
fig 
shows case triangulation chains triangulation optimal creates cliques size 
note particular graph fig 
cliques size triangulation cycles chord 
general case difficult see cliques size created number chains complexity exact inference fhmm scales 
single unstructured cartesian product hmm having number states fhmm states complexity scales exact inference fhmm somewhat costly exponential growth complexity case shows exact inference infeasible general fhmms 


triangulation state nodes chain fhmm component chains 
observation nodes omitted interest simplicity 

graph triangulation chain fhmm 

higher order hidden markov models related variation hmms considers higher order markov model state depends previous states single previous state 
case readily shown time complexity exponential discuss higher order hmm chapter variational algorithm higher order hmm see saul jordan 

hidden markov decision trees consider model decision tree endowed markovian dynamics jordan ghahramani saul 
decision tree viewed graphical model modeling decisions tree multinomial random variables level decision tree 
referring fig 
focusing particular time slice shaded node top diagram represents input vector 
unshaded nodes input nodes decision nodes 
decision nodes conditioned input entire sequence 
hidden markov decision tree 
shaded nodes fu fy represent time series element input output pair 
linking inputs outputs sequence decision nodes correspond branches decision tree 
decisions linked horizontally represent markovian temporal dependence 
preceding decisions vertical arrows diagram 
terms traditional decision tree diagram dependence provides indication path followed data point drops decision tree 
node bottom diagram output variable 
decisions decision tree conditional current data point decisions previous moment time obtain hidden markov decision tree 
fig 
horizontal edges represent markovian temporal dependence 
note particular dependency assumed level specific probability decision depends previous decision level decision tree 
sequence input vectors corresponding sequence output vectors inference problem compute conditional probability distribution hidden states 
problem intractable general seen noting includes fhmm special case 

basics variational methodology variational methods approximation methods wide variety settings including finite element analysis quantum mechanics sakurai statistical mechanics parisi statistics 
cases application variational methods converts complex problem simpler problem simpler problem generally characterized decoupling degrees freedom original problem 
decoupling 
variational logarithm function 
linear functions gamma form family upper bounds logarithm exact particular value achieved expansion problem include additional parameters known variational parameters fit problem hand 
terminology comes roots techniques calculus variations 
start systematically calculus variations jump intermediate point emphasizes important role convexity variational approximation 
point view turns particularly suited development variational methods graphical models 

examples considering simple example 
particular express logarithm function ln min fx gamma ln gamma expression variational parameter required perform minimization value expression readily verified derivative respect solving substituting 
situation best appreciated geometrically show fig 

note expression braces eq 
linear slope clearly concavity logarithm line having slope value intercept line touches logarithm single point 
gamma ln gamma eq 
precisely intercept 
range family lines forms upper envelope logarithm function 
ln gamma ln gamma variational transformation provides family upper bounds logarithm 
minimum bounds exact value logarithm 
pragmatic justification transformation converted nonlinear function linear function 
cost obtained free parameter set value obtain upper bound logarithm set obtain bound 
recover exact value logarithm optimal choice consider second example directly relevant graphical models 
binary valued nodes common represent probability node takes values monotonic nonlinearity simple function linear function values parents node 
example logistic regression model gammax seen previously eq 

weighted sum values parents node 
logistic function convex concave simple linear bound 
logistic function log concave 
function gamma ln gammax concave function readily verified calculating second derivative 
bound log logistic function linear functions bound logistic function exponential 
particular write min fx gamma binary entropy function gamma ln gamma gamma ln gamma 
explain binary entropy function arises suffices think simply appropriate intercept term log logistic function 
take exponential sides noting minimum exponential function commute min gammah variational transformation logistic function examples plotted fig 

note value obtain upper bound logistic function values gammah choices provide better bounds 
advantages transformation eq 
significant context graphical models 
particular obtain joint probability graphical 
variational transformation logistic function 
model required take product local conditional probabilities cf 
eq 

conditional probabilities represented logistic regression obtain products functions form gammax 
product simple form 
augment network representation including variational parameters representing logistic function eq 
see bound joint probability obtained products exponentials 
tractable computationally particularly exponents linear 
convex duality find variational transformations systematically 
variational transformations utilized literature graphical models examples general principle convex duality 
general fact convex analysis rockafellar concave function represented conjugate dual function follows min gamma allow vectors 
conjugate function obtained dual expression min gamma relationship easily understood geometrically shown fig 

plot linear function particular value short vertical 
conjugate function obtained minimizing deviations represented dashed lines 
segments represent values gamma 
clear need shift linear function vertically amount minimum values gamma order obtain upper bounding line slope touches single point 
observation justifies form conjugate function minimum differences gamma explains conjugate function appears intercept eq 

easy exercise verify conjugate function logarithm ln conjugate function log logistic function binary entropy 
focused upper bounds section framework convex duality applies equally lower bounds particular convex max gamma max gamma conjugate function 
focused linear bounds section convex duality restricted linear bounds 
general bounds obtained transforming argument function interest value function jaakkola jordan 
example concave write min fx gamma conjugate function 
transformation yields quadratic bound 
worth noting transformations combined logarithmic transformation utilized earlier obtain gaussian representations upper bounds 
useful obtaining variational approximations posterior distributions jaakkola jordan 
summarize general methodology suggested convex duality 
wish obtain upper lower bounds function interest 
function convex concave simply calculate conjugate function 
function convex concave look invertible transformation renders function convex concave 
may consider transformations argument function 
calculate conjugate function transformed space transform back 
approach useful need find transform logarithm inverse useful algebraic properties 

approximations joint probabilities conditional probabilities discussion far focused approximations local probability distributions nodes graphical model 
approximations translate approximations global probabilities interest particular conditional distribution hje interest inference problem marginal probability interest learning problems 
focus directed graphs concreteness 
suppose lower bound upper bound local conditional probabilities js 
assume forms js js providing upper lower bounds respectively generally different variational parameterizations appropriate upper lower bounds 
consider upper bounds 
product upper bounds upper bound js js inequality holds arbitrary settings values variational parameters eq 
hold subset subset held fixed implies upper bounds marginal probabilities obtained sums variational form right hand side equation 
example letting disjoint partition fhg fhg js see examples discussed choose variational forms js summation carried efficiently key step developing variational method 
eq 
eq 
upper bounds hold settings values variational parameters hold particular optimizing settings parameters 
treat right hand side eq 
right hand side eq 
function minimized respect case optimization process induce interdependencies parameters interdependencies desirable critical obtaining variational bound marginal probability interest 
particular best global bounds obtained probabilistic dependencies distribution reflected dependencies approximation 
clarify nature variational bounds note important distinction joint probabilities eq 
marginal probabilities eq 

eq 
allow variational parameters set optimally value argument possible principle find optimizing settings variational parameters recover exact value joint probability 
assume local probabilities js represented exactly variational transformation examples discussed section 
eq 
hand generally able recover exact values marginal optimizing variational parameters depend argument consider example case node parents range fhg summands right hand side eq 
involve evaluating local probability js different values parents variational parameter depends general expect obtain exact representation js summand 
summands eq 
necessarily bounds exact values 
observation provides bit insight reasons variational bound expected tight circumstances loose 
particular js nearly constant range operating point variational representation fairly insensitive setting example right hand side logarithm fig 
bounds may expected tight 
hand conditions expect bound loose 
situation complicated interdependencies induced optimization process 
return issues discussion 
discussed upper bounds similar comments apply lower bounds marginal probabilities obtained lower bounds joint distribution 
conditional distribution hje hand ratio marginal distributions hje 
obtain upper lower bounds conditional distribution upper lower bounds numerator denominator 
generally speaking obtain upper lower bounds denominator labor essentially finished numerator involves fewer sums 
case numerator involves sums simply function evaluation 
worth noting variational methods interest simply tractable approximations methods provide strict bounds sampling methods 
way obtain variational approximation bound marginal probability substitute variational parameters obtained conditional probability distribution 
example obtain lower bound likelihood fitting variational parameters 
substitute parameters parameterized variational form utilize variational form efficient inference engine calculating approximation hje 
sections illustrate general variational framework applied number worked examples 
examples involve architectures practical interest provide concrete examples variational methodology 
certain degree examples serve case histories generalized related architectures 
important emphasize necessarily straightforward develop variational approximation new architecture 
ease utility applying methods outlined section depend architectural details including choice node probability functions graph topology particular parameter regime model operated 
particular certain choices node conditional probability functions lend readily variational transformations useful algebraic properties 
certain architectures simplify readily variational transformation particular marginal bounds eq 
simple functions cases complex 
issues currently understood development effective variational approximations cases require substantial creativity 

sequential block methods consider somewhat detail variational methods applied probabilistic inference problems 
basic idea suggested wish simplify joint probability distribution transforming local probability functions 
appropriate choice variational transformation simplify form joint probability distribution simplify inference problem 
transform nodes 
cost performing transformations obtain bounds approximations probabilities exact results 
option transforming nodes important implies role exact methods subroutines variational approximation 
particular partial transformations graph may leave original graphical structure intact introduce new graphical structure exact methods fruitfully applied 
general wish variational approximations limited way transforming graph simplified graph exact methods applied 
general yield tighter bounds algorithm transforms entire graph regard computationally tractable substructure 
majority variational algorithms proposed literature date divided main classes sequential block 
sequential approach nodes transformed order determined inference process 
approach advantage flexibility generality allowing particular pattern evidence determine best choices nodes transform 
cases particularly obvious substructures graph amenable exact methods advantageous designate advance nodes transformed 
see block approach particularly natural setting parameter estimation 

sequential approach sequential approach introduces variational transformations nodes particular order 
goal transform network resulting transformed network amenable exact methods 
see examples certain variational transformations understood graphically sparsification nodes removed graph 
sufficient number variational transformations introduced resulting graph sufficiently sparse exact method applicable 
operational definition sparseness obtained running greedy triangulation algorithm upper bounds run time junction tree inference algorithm 
basically ways implement sequential approach untransformed graph introduce variational transformations node time completely transformed graph reintroduce exact conditional probabilities node time 
advantage approach graph remains tractable times feasible directly calculate quantitative effect transforming node 
approach graph intractable search way assessing transformation qualitative effect graphical sparseness 
sequential approach best context specific example 
section return qmr dt network show sequential variational approach inference network 

qmr dt network jaakkola jordan application sequential variational methods qmr dt network 
seen qmr dt network bipartite graph conditional probabilities findings noisy model eq 
negative findings eq 
positive findings note symptom nodes findings symptoms observed simply marginalized joint distribution omission impact inference 
discussed negative findings difficulties inference exponential form probability eq 
effects negative findings disease probabilities handled linear time 
assume updates associated negative findings focus problem performing inference positive findings 
repeating eq 
convenience representation probability positive finding jd gamma gamma ij gamma function gamma gammax log concave case logistic function able express variational upper bound terms exponential linear function 
particular gamma gammax gammaf conjugate function follows gamma ln ln plugging argument eq 
eq 
noting need different variational parameter transformed node obtain jd ij gammaf gammaf theta ij final equation displays effect variational transformation 
exponential factor outside product simply constant 
product taken nodes parent set node case graph moralized exact computation contributions associated nodes uncoupled 
factor exp ij simply constant multiplied probability previously associated node 
coupling nodes taken products untransformed noisy 
graphical effect variational transformation shown fig 
variational transformation ith finding graph 
particular example graph rendered singly connected exact inference algorithm invoked 
recall marginalizing unobserved symptoms simply removes graph 
sequential methodology utilized jaakkola jordan begins completely transformed graph exact conditional probabilities selected nodes 
choose ordering reinstate nodes jaakkola jordan heuristic basing choice effect likelihood bound node individually starting completely transformed state 
despite suboptimality heuristic yielded approximation orders magnitude accurate algorithm random ordering 
ordering algorithm proceeds follows choose node ordering consider diseases symptoms 
qmr dt graph lightly shaded finding subjected variational transformation 
effect equivalent node graph 
effect links associated node current graph 
resulting graph amenable exact methods reinstate node iterate 
run exact method 
choose parameters approximation tight possible 
difficult verify products expression eq 
yield bound convex function parameters jaakkola jordan 
standard optimization algorithms find choices shows results jaakkola jordan approximate inference cpc cases mentioned earlier 
cases sufficiently small number positive findings exact algorithm run provide gold standard comparison 
leftmost shows upper lower bounds log likelihood cases 
jaakkola jordan sorted cases log likelihood execution time 
exact values variational upper lower bounds log likelihood tractable cpc cases 
mean correlation approximate exact posterior marginals function execution time seconds 
solid line variational estimates dashed line likelihood weighting sampling 
lines sampling result represent standard errors mean independent runs sampler 
calculated approximate posterior marginals diseases 
correlations marginals gold standard shown rightmost 
plots accuracy run time runs positive findings treated exactly 
note accurate values obtained second 
shows results state art sampling algorithm likelihood weighted sampler shwe cooper 
sampler required significantly computer time variational method obtain roughly comparable accuracy 
jaakkola jordan results entire corpus cpc cases 
variational method yielded reasonably accurate estimates posterior probabilities diseases lengthy runs sampler basis comparison second computer time 

boltzmann machine consider different example 
discussed boltzmann machine special subset class undirected graphical models potential functions composed products quadratic linear boltzmann factors 
jaakkola jordan introduced sequential variational algorithm approximate inference boltzmann machine 
method discuss section yields upper lower bounds marginal conditional probabilities interest 
recall form joint probability distribution boltzmann machine ij obtain marginal probabilities joint distribution calculate sums exponentials quadratic energy functions 
obtain conditional probabilities hje take ratios sums numerator requires fewer sums denominator 
general sum partition function sum configurations fsg 
focus upper lower bounds partition function general case allows calculate bounds marginals conditionals interest 
approach perform sums sum time introducing variational transformations ensure resulting expression stays computationally tractable 
fact step process describe transformed potentials involve quadratic boltzmann factors 
exact methods viewed creating increasingly higher order terms marginalizing sums performed 
transformed boltzmann machine remains boltzmann machine 
consider lower bounds 
write partition function follows fsg jk sk jk sk attempt find tractable lower bound inner summand right hand side 
difficult show expression log convex 
bound logarithm ln jk sk fj kg jk ln ij fj kg jk ln ij fj kg jk ij sum term right hand side sum pairs equal delta binary entropy function variational parameter associated node line simply pulled outside sum terms involving second line performed sum values lower bound expression eq 
need lower bound term ln right hand side 
variational bounds related expression treating logistic function recall eq 

upper bound case translates lower bound current case ln gammax gammax bound utilized eq 

consider graphical consequences bound eq 
see fig 

note nodes graph node neighbors boltzmann factors unaltered see terms bound 
graph unaltered nodes 
term parentheses see neighbors node endowed new linear terms importantly nodes linked done exact marginalization 
neighbors linked previously remain linked jk parameter 
node absent transformed partition function absent graph left trace new linear boltzmann factors associated neighbors 
summarize effects transformation noting transformed graph new boltzmann machine fewer node parameters jk jk ij 
transformation boltzmann machine approximate marginalization node case lower bounds 
boltzmann machine transformation 
boltzmann machine transformation 
pairwise parameters jk equal remained unaltered 
suggested lines linear coefficients changed nodes neighbors note constant term keep track 
term interesting interpretation return boltzmann machine context block methods 
upper bounds obtained similar way 
break partition function sum particular node sum configurations remaining nodes sns lines ensuing derivation leading eq 
identical 
complete derivation find upper bound ln 
jaakkola jordan proposed quadratic bounds purpose 
particular noted ln ln gammax ln gammax concave function verified second derivative respect 
implies ln quadratic upper bound form ln gamma appropriately defined conjugate function 
upper bounds eq 
obtain ln jk sk fj kg jk ij ij gamma variational parameter associated node 
transformation boltzmann machine approximate marginalization node case upper bounds 
boltzmann machine transformation 
boltzmann machine transformation 
dashed edges suggest neighbors linked linked new parameter values 
suggested lines neighbors new linear coefficients 
edges parameters unaltered 
graphical consequences transformation somewhat different lower bounds see fig 

considering terms bound see case graph unaltered nodes graph node neighbors neighbors previously linked remain linked 
quadratic term gives rise new links previously unlinked neighbors node alters parameters previously linked neighbors 
nodes acquires new linear term 
expanding eq 
collecting terms see approximate marginalization yielded boltzmann machine parameters jk jk ji ik ij ij ij constant term gamma 
graphical consequences lower upper bound transformations computational consequences 
particular lower bound transformation introduces additional links nodes somewhat natural combine transformations exact methods 
particular algorithm simply nodes tractable structure tree revealed point exact algorithm called subroutine 
upper bound transformation hand introducing links neighbors node reveal tractable structure readily 
vantage mitigated fact upper bound tighter bound jaakkola jordan 

block approach alternative approach variational inference designate advance set nodes transformed 
principle view block approach line application sequential approach 
case lower bounds advantages gained developing methodology specific block transformation 
section show natural global measure approximation accuracy obtained lower bounds block version variational formalism 
method meshes readily exact methods cases tractable substructure identified graph 
approach saul jordan refined version mean field theory markov random fields developed number studies ghahramani jordan ghahramani hinton jordan 
block approach identifying substructure graph interest know amenable exact inference methods generally efficient approximate inference methods 
example pick tree set chains original graph 
wish simplified structure approximate probability distribution original graph 
consider family probability distributions obtained simplified graph variational parameters 
choose particular approximating distribution simplifying family making particular choice variational parameters 
sequential approach new choice variational parameters time new evidence available 
formally represent joint distribution graphical model interest represents nodes graph disjoint subsets representing hidden nodes evidence nodes respectively 
wish approximate conditional probability hje 
introduce approximating family conditional probability distributions hje variational parameters 
graph representing generally graph representing generally sub graph 
family approximating distributions choose particular distribution minimizing kullback leibler kl divergence qkp respect variational parameters argmin hje hje probability distributions kl divergence defined follows qkp fsg ln minimizing values variational parameters define particular distribution hje treat best approximation hje family hje 
simple justification kl divergence measure approximation accuracy yields best lower bound probability evidence likelihood family approximations hje 
bound logarithm jensen inequality follows ln ln fhg ln fhg hje delta hje fhg hje ln hje difference left right hand sides equation easily seen kl divergence qkp 
positivity kl divergence cover thomas right hand side eq 
lower bound 
choosing eq 
obtain tightest lower bound 

convex duality kl divergence justify choice kl divergence making appeal convex duality theory linking block approach sequential approach jaakkola 
consider simplicity case discrete valued nodes distribution hje viewed vector real numbers configuration variables treat vector vector valued variational parameter eq 

log probability ln viewed vector real numbers defined set configurations treat vector variable eq 

define ln 
verified expression ln ln ln fhg ln convex values ln 
direct substitution eq 
min fhg hje ln gamma ln minimizing respect ln conjugate function seen negative entropy function fhg hje ln hje 
eq 
lower bound log likelihood follows ln fhg hje ln gamma hje ln hje identical eq 

see principle recover exact log likelihood allowed range probability distributions hje 
ranging parameterized family hje obtain tightest lower bound available family 

parameter estimation variational methods neal hinton pointed lower bound eq 
useful role play context maximum likelihood parameter estimation 
particular link lower bound parameter estimation em algorithm 
augment notation include parameters specification joint probability distribution sj 
designate subset nodes observed evidence 
marginal probability ej thought function known likelihood 
em algorithm method maximum likelihood parameter estimation log likelihood 
making convexity relationship ln ej ln ej described previous section 
section showed function fhg hje ln ej gamma hje ln hje lower bound log likelihood probability distribution hje 
showed difference ln ej bound kl divergence hje hje 
suppose allow hje range possible probability distributions minimize kl divergence 
standard result cf 
cover thomas kl divergence minimized choosing hje hje minimal value zero 
verified substituting hje right hand side eq 
recovering ln ej 
suggests algorithm 
starting initial parameter vector iterate steps known expectation step maximization step 
maximize bound respect probability distributions second fix maximize bound respect parameters 
formally step argmax step argmax coordinate ascent 
related traditional presentation em algorithm dempster laird rubin noting fixed right hand side eq 
function ln ej term 
maximizing respect step equivalent maximizing function fhg hje ln ej maximization function known complete log likelihood em literature defines step traditional presentation em 
return situation unable compute full conditional distribution hje 
cases variational methodology suggests consider family approximating distributions 
longer able perform true em iteration avail hje perform coordinate ascent lower bound 
variational strategy minimizing kl divergence respect variational parameters define approximating family exactly restricted form coordinate ascent argument 
follow step step increases lower bound respect parameters 
point view viewed computationally tractable approximation em algorithm exploited number architectures including sigmoid belief network factorial hidden markov model hidden markov decision tree architectures discuss sections helmholtz machine dayan hinton neal zemel hinton dayan frey neal 

examples return problem picking tractable variational parameterization graphical model 
wish pick simplified graph rich provide distributions close true distribution simple exact algorithm utilized efficiently calculations approximate distribution 
similar considerations hold variational parameterization variational parameterization representationally rich approximations available simple procedure minimizes kl divergence hope finding parameters getting stuck local minimum 
necessarily possible realize desiderata simultaneously number cases relatively simple variational approximations yield reasonably accurate solutions 
section discuss examples 

mean field boltzmann machine section discussed sequential variational algorithm yielded upper lower bounds boltzmann machine 
revisit boltzmann machine context block approach discuss lower bounds 
relate approaches 
recall joint probability boltzmann machine written follows sj ij ij nodes neighbors graph 
consider representation conditional distribution hje boltzmann machine 
nodes contribution ij reduces constant vanishes normalize 
quadratic contribution linear contribution associate node linear terms associated nodes constants vanish 
summary express conditional distribution hje follows hje ij sums restricted range nodes updated parameters include contributions associated evidence nodes ij updated partition function follows fhg ae ij oe sum boltzmann machine subset mean field approximation peterson anderson boltzmann machines particular form variational approximation completely factorized distribution approximate hje 
consider simplest possible approximating distribution obtained dropping edges boltzmann graph see fig 

choice hje represent variational parameters little choice variational parameterization represent large approximating family possible endow degree freedom variational parameter written follows hje gamma gammas product taken hidden nodes forming kl divergence fully factorized distribution distribution eq 
obtain qkp ln gamma ln gamma gamma ij gamma ln 
node boltzmann machine markov blanket 
approximating mean field distribution graph edges 
mean field equations yield deterministic relationship represented dotted lines variational parameters nodes markov blanket node sums range nodes deriving result fact distribution independent random variables mean values take derivatives kl divergence respect noting independent set derivative zero obtain equations oe ij oe gammaz logistic function define ij equal ji eq 
defines set coupled equations known mean field equations 
equations solved iteratively fixed point solution 
note variational parameter updates value sum variational parameters markov blanket cf 
fig 

viewed variational form local message passing algorithm 
peterson anderson compared mean field approximation gibbs sampling set test cases ran times faster yielding roughly equivalent level accuracy 
cases mean field approximation known break 
cases include sparse boltzmann machines boltzmann machines frustrated interactions networks potential functions embody constraints neighboring nodes simultaneously satisfied see 
case sparse networks exact algorithms provide help observation led exact algorithms subroutines structured mean field approach pursued saul jordan 
consider parameter estimation problem boltzmann machines 
writing lower bound eq 
case ln ej ij gamma ln gamma ln gamma ln gamma derivative respect ij yields gradient simple hebbian term contribution derivative ln respect ij hard show derivative hs brackets signify average respect unconditional distribution sj 
gradient algorithm performing approximate step delta ij gamma hs unfortunately assumption calculations boltzmann distribution intractable graph consideration intractable compute unconditional average 
appeal mean field theory compute approximation hs factorized distribution nodes step difference gradients different bounds longer guaranteed increase serious problem particularly salient unsupervised learning problems 
data set interest heterogeneous collection sub populations unsupervised classification problems unconditional distribution generally required multiple modes 
unfortunately factorized mean field approximation unimodal poor approximation multi modal distribution 
approach problem utilize multi modal distributions mean field framework example jaakkola jordan discuss mixture models approximating distributions 
issues find satisfactory treatment context directed graphs see section 
particular gradient directed graph cf 
eq 
require averages unconditional distribution 
consider relationship mean field approximation lower bounds obtained sequential algorithm section 
fact run algorithm nodes eliminated graph obtain bound identical mean field bound jaakkola 
see note boltzmann machine nodes eliminated quadratic linear terms constant terms remain 
recall section constant arises node removed refers value updated absorb linear terms previously eliminated nodes 
recall update ij removal particular node neighbor 
collecting updates summing nodes find resulting constant term follows ij gamma ln gamma ln gamma differs lower bound eq 
term ln disappears maximize respect 
neural networks discussed section sigmoid belief network essentially directed neural network graphical model semantics 
utilize logistic function node probability function js gamma ij gamma assume ij parent 
particular ij ji 
noting probabilities case case written single expression follows js ij ij obtain representation joint distribution sj ij ij wish calculate conditional probabilities joint distribution 
seen cf 
fig 
inference general sigmoid belief networks intractable sensible consider variational approximations 
saul jaakkola jordan saul jordan explored viability simple completely factorized distribution 
set hje gamma gammas attempt find best approximation varying parameters computation kl divergence qkp proceeds case mean field boltzmann machine 
entropy term ln 
energy term ln logarithm eq 
averaging respect putting results obtain ln ej ij gamma ln ij ae gamma ln gamma ln gamma deltai denotes average respect distribution abused notation defining values set instantiated values 
note despite fact factorized unable calculate average ln denotes ij important term arises directly directed nature sigmoid belief network arises denominator sigmoid factor necessary define sigmoid local conditional probability 
deal term saul 
introduced additional variational transformation due seung viewed refined form jensen inequality 
particular hln omega ln gamma ff hz ln gamma gamma hz ln gamma gamma variational parameter 
note inequality reduces standard jensen inequality 
final result utilized directly eq 
provide tractable lower bound log likelihood variational parameter optimized variational parameters 
saul jordan show limiting case networks hidden node large number parents central limit theorem invoked parameter probabilistic interpretation approximate expectation oe oe delta logistic function 
fixed values parameters differentiating kl divergence respect variational parameters obtain consistency equations oe ij ji gamma ji ji expression depends node child parents parents node term sum contributions parents node second term sum contributions children node see consistency equation node involves contributions markov blanket node see fig 

case boltzmann machine find variational parameters linked markov consistency equation eq 
interpreted local message passing algorithm 
saul jaakkola jordan saul jordan show update variational parameters papers utilize parameters 
node sigmoid belief network machine markov blanket 
mean field equations yield deterministic relationship represented dotted lines variational parameters nodes markov blanket node slightly different ways obtain different update equations 
related variational approximation sigmoid belief network including upper lower bounds jaakkola jordan 
compute gradient respect parameters ij fixed variational parameters result obtained saul jordan takes form delta ij gamma gamma ij gamma gamma note need calculate variational parameters unconditional distribution sj case boltzmann machine fact noted neal 
note interesting appearance regularization term second term equation weight decay term maximal non extreme values variational parameters parameters bounded zero 
computationally motivated approximation maximum likelihood estimation fact form penalized maximum likelihood estimation 
saul 
tested sigmoid belief network handwritten digit recognition problem obtaining results competitive supervised learning systems 
important advantage graphical model approach ability deal missing data 
saul jordan report degradation performance missing pixels digits slight 
comparative empirical sigmoid belief networks related architectures including comparisons gibbs sampling see frey hinton dayan 

factorial hidden markov models factorial hidden markov model fhmm multiple chain structure see fig 

notation developed earlier 

fhmm 
variational approximation fhmm obtained picking tractable substructure fhmm graph 
parameterizing graph leads family tractable approximating distributions 
see section joint probability distribution fhmm fx fy gj jx gamma jfx computation probability distribution generally infeasible saw earlier clique size large fhmm chain structure moralized triangulated 
necessary consider approximations 
fhmm natural substructure base variational algorithm 
particular chains compose fhmm individually tractable 
removing edges naive mean field approximation discussed previous sections reasonable remove edges necessary decouple chains 
particular remove edges link state nodes output nodes see fig 

edges moralization process longer links state nodes longer creates large cliques 
fact moralization process graph fig 
vacuous triangulation 
cliques graph size number states single chain 
iteration approximate inference runs time number chains length time series 
consider express variational approximation graph fig 
approximation 
idea introduce free parameter approximating probability distribution edge dropped 
free parameters denote essentially serve surrogates effect observation time state component optimize divergence qkp respect parameters interdependent deterministic interdependence viewed approximation probabilistic dependence captured exact algorithm moralization process 
referring fig 
write distribution factorized form fx jx gamma vector variational parameters define transition matrix product exact transition matrix variational parameter jx gamma jx gamma similarly initial state probabilities family distributions respects conditional independence statements approximate graph fig 
provides additional degrees freedom variational parameters 
ghahramani jordan equations result minimizing kl divergence approximating probability distribution eq 
true probability distribution eq 

result summarized follows 
architectures discussed equation variational parameter function terms markov blanket corresponding node 
particular update depends parameters linking variational parameters time update depends expected value states expectation taken distribution chains decoupled expectations running exact algorithms example forward backward algorithm hmms separately chain 
expectations course depend current values parameters cf 
eq 
dependence effectively couples chains 
summarize fitting variational parameters fhmm iterative phase procedure 
phase exact algorithm run subroutine calculate expectations hidden states 
done independently chains making current values parameters second phase parameters updated expectations computed phase 
procedure returns phase iterates 
ghahramani jordan reported results fitting fhmm bach data set merz murphy 
showed significantly larger effective state spaces fit fhmm unstructured hmm performance terms probability test set order magnitude larger fhmm 
evidence overfitting seen hmm states evidence overfitting fhmm seen states 

forest chains approximation 
parameterizing graph leads approximating family distributions 

hidden markov decision trees final example return hidden markov decision tree described briefly discuss variational approximation architecture 
discussed essentially markov time series model probability model time step probabilistic decision tree hidden decision nodes 
markovian dependence obtained separate transition matrices different levels decision tree giving model factorized structure 
variational approach fitting closely related fitting fhmm additional choices variational approximation 
particular substructures worth considering dropping vertical edges recover decoupled set chains 
fhmm chains handled forward backward algorithm 
dropping horizontal edges recover decoupled set decision trees 
calculate probabilities trees posterior propagation algorithm described jordan 
approach refer forest chains approximation shown fig 

fhmm write variational approximation forest chains approximation respecting conditional independencies approximating graph incorporating variational parameters obtain extra degrees freedom see jordan details 
consider forest trees approximation horizontal links eliminated see fig 

decision tree fully connected graph essentially naive mean field approximation hypergraph 
possible develop variational algorithm analogous viterbi algorithm hmms 
particular utilize approximation assigns probability single path state space 
kl divergence distribution particularly easy evaluate entropy contribution kl divergence ln term zero 
evaluation energy ln term reduces substituting states chosen path distribution 

forest trees approximation 
parameterizing graph leads approximating family distributions 
resulting algorithm involves subroutine standard viterbi algorithm run single chain chains held fixed 
subroutine run chain turn 
jordan 
performance bach chorales essentially fhmm 
advantage greater interpretability runs resulted coarse fine ordering temporal scales markov processes top bottom tree 

discussion described variety applications variational methods problems inference learning graphical models 
hope convinced reader variational methods provide powerful elegant tool graphical models algorithms result simple intuitively appealing 
important emphasize research variational methods graphical models quite origin open problems unresolved issues 
section discuss number issues 
broaden scope presentation discuss number related strands research 

related research methods discussed involve deterministic iterative approximation algorithms 
interest discuss related approximation schemes non deterministic non iterative 

recognition models helmholtz machine algorithms core nonlinear optimization problem 
particular having introduced variational parameters sequentially block left bound eq 
optimized 
optimization bound generally achieved fixed point iteration gradient algorithm 
iterative optimization process induces interdependencies variational parameters give best approximation marginal conditional probability interest 
consider particular problem directed graphical model unsupervised learning 
common approach unsupervised learning consider graphical models oriented generative direction point hidden variables observables 
case predictive calculation elementary 
calculation hje hand diagnostic calculation proceeds backwards graph 
diagnostic calculations generally non trivial require full power inference algorithm 
alternative approach solving iteratively approximation diagnostic calculation learn generative model recognition model approximates diagnostic distribution hje 
associate different parameters generative model recognition model rely parameter estimation process bring parameterizations register 
basic idea helmholtz machine dayan hinton 
key advantage recognition model approach calculation hje reduced elementary feedforward calculation performed quickly 
disadvantages approach 
particular lack iterative algorithm helmholtz machine unable deal naturally missing data phenomena explaining away couplings hidden variables change function conditioning variables 
cases clear natural parameterization recognition model induced generative model particular linear models factor analysis general difficult insure models matched appropriately 
problems addressed combining recognition model approach iterative variational approach essentially treating recognition model cache storing initializations variational parameters 

sampling methods section remarks relationships variational methods stochastic methods particular gibbs sampler 
setting graphical models classes methods rely extensive message passing 
gibbs sampling message passing particularly simple node learns current instantiation markov blanket 
samples node estimate distribution markov blanket roughly speaking determine statistics 
advantage scheme limit samples guaranteed converge correct statistics 
disadvantage samples may required 
message passing variational methods quite different 
purpose couple variational parameters node markov blanket 
messages come form samples form approximate statistics summarized variational parameters 
example network binary nodes gibbs sampler circulating messages binary vectors correspond instantiations markov variational methods circulating real valued numbers correspond statistics markov 
may reason variational methods converge faster gibbs sampling 
course disadvantage schemes necessarily converge correct statistics 
hand provide bounds marginal probabilities quite difficult estimate sampling 
sampling methods suited estimating statistics individual hidden nodes ill equipped compute marginal probabilities 
interesting direction research consider combinations sampling methods variational methods 
initial direction done hinton sallans ghahramani discuss brief gibbs sampling point view variational approximation 

bayesian methods variational inference applied general problem bayesian parameter estimation 
quite generally treat parameters additional nodes graphical model cf 
heckerman treat bayesian inference footing generic probabilistic inference graphical model 
probabilistic inference problem intractable variational approximations useful 
variational method known ensemble learning originally introduced way fitting ensemble neural networks data setting parameters thought different member ensemble hinton van camp 
je represent variational approximation posterior distribution je 
ensemble fit minimizing appropriate kl divergence kl qkp je ln je je line argument section know minimization equivalent maximization lower bound 
particular copying argument section find minimizing kl divergence yields best lower bound quantity ln ln ej logarithm marginal likelihood key quantity bayesian model selection model averaging 
ensemble learning approach applied mixture experts architectures waterhouse mackay robinson hidden markov models mackay 
interesting aspect applications assume particular parametric family nonparametric assumption factorizes specific way 
variational minimization determines best family factorization prior 
jaakkola jordan developed variational methods bayesian inference variational approach find analytically tractable approximation logistic regression gaussian prior parameters 

perspective key issue faces developers variational methods issue approximation accuracy 
develop intuition variational methods perform perform poorly examining properties certain studied cases 
case fully factorized approximations undirected graphs starting point statistical mechanics literature approximation give exact results 
cases include densely connected graphs uniformly weak non negative couplings neighboring nodes parisi 
mean field equations networks unique solution determines statistics individual nodes limit large graphs 
kearns saul utilized large deviation methods study approximation accuracy bounds likelihood dense directed graphs 
characterizing accuracy terms number parents node assumed constant layered graph shown gap variational upper lower bounds converges rate ln 
approach utilizes general form upper lower bounds local conditional probabilities depend convexity properties 
result expected robust general family variational methods faster rates may obtainable convexity variational approximations discussed current 
general graphical models conditions convergence fully factorized variational approximations may favorable 
general nodes may small markov blanket may strongly dependent particular neighbors variational transformations nodes yield poor bounds 
globally strong probabilistic dependencies model posterior multiple modes limiting case deterministic relationships readily create switching automata multiple modes 
fully factorized approximations necessarily unimodal fail cases 
handling cases requires making methods transform subset nodes incorporating exact inferential procedures subroutines handle untransformed structure 
exact methods capture strong dependencies leaving weaker dependencies variational transformations 
practice achieving kind division labor requires strong dependencies identified advance inspection graph identified context sequential variational method simple greedy calculations 
alternative approach handling multiple modes utilize mixture models approximating distributions distributions block variational methods 
see jaakkola jordan bishop lawrence jaakkola jordan discussion approach 
important stress difference joint distributions conditional distributions hje 
situations classification problems represents category label joint distribution multi modal conditional distribution hje 
factorized approximations may sense inference problems poor approximations joint probability model 
key issue broadening scope variational methods 
restricted set variational techniques convexity transformations 
techniques applicable appropriate convexity properties need identified 
relatively easy characterize small classes models properties lead simple approximation algorithms case local conditional probabilities log concave generalized linear models generally easy develop variational algorithms kinds graphical models 
broader characterization variational approximations needed systematic algebra needed match approximations models 
open problems include problem combining variational methods sampling methods search methods problem making informed choices node ordering case sequential methods development upper bounds block framework combination multiple variational approximations model development variational methods architectures combine continuous discrete random variables 
similar open problems exist sampling methods methods incomplete pruned versions exact methods 
difficulty providing solid theoretical foundations cases lies fact accuracy contingent large degree actual conditional probability values underlying probability model discrete properties graph 
acknowledgments wish brendan frey david heckerman kjaerulff peter dayan helpful comments manuscript 
appendix section calculate conjugate functions logarithm function log logistic function 
ln min fx gamma ln xg derivative respect setting zero yields gamma substituting back eq 
yields ln justifies representation logarithm eq 

log logistic function gamma ln gammax min fx ln gammax derivative respect setting zero yields gammax gammax obtain ln gamma ln gammax ln gamma plugging expressions back eq 
yields gamma ln gamma gamma ln gamma binary entropy function 
justifies representation logistic function eq 

notes 
presentation take point view moralization triangulation combined local message passing algorithm sufficient exact inference 
possible show certain conditions steps necessary exact inference 
see jensen jensen 

identify ith node random variable associated node 

define clique subset nodes fully connected maximal additional node added subset subset remains fully connected 

note particular fig 
moralization fig 

acronym qmr dt refers decision theoretic version quick medical 

particular pattern missing edges graph implies diseases diseases symptoms conditionally independent 

jaakkola jordan calculated median pairwise cutset size 
value rules exact cutset methods inference qmr dt 

possible consider general boltzmann machines multivalued nodes potentials exponentials arbitrary functions cliques 
models essentially equivalent general undirected graphical model eq 
represent zero probabilities 

note treat general marginal probability necessarily assume jointly exhaust set nodes particular recognition model utilized helmholtz machine layered graph weak conditional independence assumptions possible principle capture fairly general dependencies 


finite element procedures 
englewood cliffs nj prentice hall 
baum petrie soules weiss 

maximization technique occurring statistical analysis probabilistic functions markov chains 
annals mathematical statistics 
bishop lawrence jaakkola jordan 

approximating posterior distributions belief networks mixtures 
jordan kearns solla eds advances neural information processing systems mit press cambridge ma 
cover thomas 

elements information theory 
new york john wiley 
dagum luby 

approximating probabilistic inference bayesian belief networks np hard 
artificial intelligence 
dayan hinton neal zemel 

helmholtz machine 
neural computation 
dean kanazawa 

model reasoning causality persistence 
computational intelligence 
dechter 

bucket elimination unifying framework probabilistic inference 
jordan ed learning graphical models 
cambridge ma mit press 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society 
draper hanks 

localized partial evaluation belief networks 
uncertainty artificial intelligence proceedings tenth conference 
san mateo ca morgan kaufmann 
frey hinton dayan 

wake sleep algorithm learn density estimators 
touretzky mozer hasselmo eds advances neural information processing systems 
cambridge ma mit press 
fung 

backward simulation bayesian networks 
uncertainty artificial intelligence proceedings tenth conference 
san mateo ca morgan kaufmann 


limitations deterministic boltzmann machine learning 
network 
ghahramani hinton 

switching state space models 
technical report 
toronto department computer science university toronto 
ghahramani jordan 

factorial hidden markov models 
machine learning 
gilks thomas spiegelhalter 

language program complex bayesian modelling 
statistician 
heckerman 

tutorial learning bayesian networks 
jordan ed learning graphical models 
cambridge ma mit press 
henrion 

search methods bound diagnostic probabilities large belief nets 
uncertainty artificial intelligence proceedings seventh conference 
san mateo ca morgan kaufmann 
hinton sejnowski 

learning relearning boltzmann machines 
rumelhart mcclelland eds parallel distributed processing volume cambridge ma mit press 
hinton van camp 

keeping neural networks simple minimizing description length weights 
proceedings th annual workshop computational learning theory new york ny acm press 
hinton dayan frey neal 

wake sleep algorithm unsupervised neural networks 
science 
hinton sallans ghahramani 

hierarchical community experts 
jordan ed learning graphical models 
cambridge ma mit press 
horvitz suermondt cooper 

bounded conditioning flexible inference decisions scarce resources 
conference uncertainty artificial intelligence proceedings fifth conference 
mountain view ca association uai 
jaakkola jordan 

computing upper lower bounds likelihoods intractable networks 
uncertainty artificial intelligence proceedings twelth conference 
san mateo ca morgan kaufmann 
jaakkola 

variational methods inference estimation graphical models 
unpublished doctoral dissertation massachusetts institute technology cambridge ma 
jaakkola jordan 

recursive algorithms approximating probabilities graphical models 
mozer jordan petsche eds advances neural information processing systems 
cambridge ma mit press 
jaakkola jordan 

bayesian logistic regression variational approach 
madigan smyth eds proceedings conference artificial intelligence statistics ft lauderdale fl 
jaakkola jordan 


improving mean field approximation mixture distributions 
jordan ed learning graphical models 
cambridge ma mit press 
jaakkola jordan 


variational methods qmr dt database 
submitted journal artificial intelligence research 
jensen kong kjaerulff 

blocking large probabilistic expert systems 
international journal human computer studies 
jensen jensen 

optimal junction trees 
uncertainty artificial intelligence proceedings tenth conference 
san mateo ca morgan kaufmann 
jensen 

bayesian networks 
london ucl press 
jordan 

statistical approach decision tree modeling 
warmuth ed proceedings seventh annual acm conference computational learning theory 
new york acm press 
jordan ghahramani saul 

hidden markov decision trees 
mozer jordan petsche eds advances neural information processing systems 
cambridge ma mit press 
kanazawa koller russell 

stochastic simulation algorithms dynamic probabilistic networks 
uncertainty artificial intelligence proceedings eleventh conference 
san mateo ca morgan kaufmann 
kjaerulff 

triangulation graphs algorithms giving small total state space 
research report department mathematics computer science aalborg university denmark 
kjaerulff 

reduction computational complexity bayesian networks removal weak dependences 
uncertainty artificial intelligence proceedings tenth conference 
san mateo ca morgan kaufmann 
mackay 

ensemble learning hidden markov models 
unpublished manuscript 
cambridge department physics university cambridge 
mceliece mackay cheng 

turbo decoding instance pearl belief propagation algorithm 
submitted ieee journal selected areas communication 
merz murphy 

uci repository machine learning databases 
irvine ca department information computer science university california 
neal 

connectionist learning belief networks artificial intelligence 
neal 

probabilistic inference markov chain monte carlo methods 
technical report crg tr 
toronto department computer science university toronto 
neal hinton 

view em algorithm justifies incremental sparse variants 
jordan ed learning graphical models 
cambridge ma mit press 
parisi 

statistical field theory 
redwood city ca addison wesley 
pearl 

probabilistic reasoning intelligent systems networks plausible inference san mateo ca morgan 
peterson anderson 

mean field theory learning algorithm neural networks 
complex systems 
rockafellar 

convex analysis 
princeton university press 


variational methods statistics 
new york academic press 
sakurai 

modern quantum mechanics 
redwood city ca addison wesley 
saul jordan 

learning boltzmann trees 
neural computation 
saul jaakkola jordan 

mean field theory sigmoid belief networks 
journal artificial intelligence research 
saul jordan 

exploiting tractable substructures intractable networks 
touretzky mozer hasselmo eds advances neural information processing systems 
cambridge ma mit press 
saul jordan 

mean field learning algorithm unsupervised neural networks 
jordan ed learning graphical models 
cambridge ma mit press 
seung 

annealed theories learning 
oh kwon cho eds neural networks statistical mechanics perspectives 
singapore world scientific 
shachter andersen szolovits 

global conditioning probabilistic inference belief networks 
uncertainty artificial intelligence proceedings tenth conference 
san mateo ca morgan kaufmann 
shenoy 

valuation systems bayesian decision analysis 
operations research 
shwe cooper 

empirical analysis likelihood weighting simulation large multiply connected medical belief network 
computers biomedical research 
shwe middleton heckerman henrion horvitz lehmann cooper 

probabilistic diagnosis reformulation internist qmr knowledge base 
meth 
inform 
med 
smyth heckerman jordan 

probabilistic independence networks hidden markov probability models 
neural computation 
waterhouse mackay robinson 

mixtures experts 
touretzky mozer hasselmo eds advances neural information processing systems 
cambridge ma mit press 
williams hinton 

mean field networks learn discriminate temporally distorted strings 
touretzky elman sejnowski hinton 
eds proceedings connectionist models summer school 
san mateo ca morgan kaufmann 
