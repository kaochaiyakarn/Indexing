accepted nips conference nov denver reinforcement learning methods continuous time markov decision problems steven bradtke computer science department university massachusetts amherst ma bradtke cs umass edu michael duff computer science department university massachusetts amherst ma duff cs umass edu semi markov decision problems continuous time generalizations discrete time markov decision problems 
number reinforcement learning algorithms developed solution markov decision problems ideas asynchronous dynamic programming stochastic approximation 
td learning real time dynamic programming 
reviewing semi markov decision problems bellman optimality equation context propose algorithms similar named adapted solution semi markov decision problems 
demonstrate algorithms applying problem determining optimal control simple queueing system 
conclude discussion circumstances algorithms may usefully applied 
number reinforcement learning algorithms ideas asynchronous dynamic programming stochastic approximation developed solution markov decision problems 
sutton td watkins learning real time dynamic programming rtdp :10.1.1.117.6173:10.1.1.132.7760
learning widely domain application limited processes modeled discrete time markov decision problems mdp 
derives analogous algorithms semi markov decision problems smdp extending domain applicability continuous time 
effort originally motivated desire apply reinforcement learning methods problems adaptive control queueing systems problem adaptive routing computer networks particular 
apply new algorithms known problem routing heterogeneous servers 
conclude discussion circumstances algorithms may usefully applied 
semi markov decision problems semi markov process continuous time dynamic system consisting countable state set finite action set suppose system originally observed state action accepted nips conference nov denver applied 
semi markov process evolves follows ffl state chosen transition probabilities xy ffl reward rate ae defined transition occurs ffl conditional event state time transition occurs probability distribution xy form smdp find policy minimizes expected infinite horizon discounted cost value state ae dt psi denote respectively state action time fixed policy value state satisfy xy ae xy tj xy df xy tj defining ae xy tj expected reward received transition state state action fl df xy tj expected discount factor applied value state transition state action clear equation nearly identical value function equation discrete time markov reward processes fl xy xy 
transition times identically smdp standard discrete time mdp results 
similarly value function associated optimal policy mdp satisfies bellman optimality equation max fl xy optimal value function smdp satisfies version bellman optimality equation max xy ae xy xy df xy rl methods continuous time mdp accepted nips conference nov denver temporal difference learning smdp sutton td stochastic approximation method finding solutions system equations :10.1.1.132.7760
having observed transition state state sample reward td updates value function estimate direction sample value 
td update rule mdp ff gamma ff learning rate 
sequence value function estimates generated td converge true solution probability appropriate conditions ff definition mdp 
td learning rule smdp intended solve system equations sequence sampled state transitions ff gamma gammafi fi gammafi gamma sampled transition time state state time units gammae gammafi fi sample reward received time units gammafi sample discount value state transition time time units 
td learning rule smdp straightforward define 
learning smdp watkins define function corresponding policy fl xy notice action 
action chosen policy function corresponds optimal policy 
represents total discounted return expected action taken state policy followed 
equation rewritten fl xy satisfies bellman style optimality equation fl xy max learning described watkins uses stochastic approximation iteratively refine estimate function learning rule similar td 
sampled transition state state selection sampled reward function estimate updated ff fl max gamma functions may defined smdp 
optimal function smdp satisfies equation xy ae xy xy max df xy rl methods continuous time mdp accepted nips conference nov denver leads learning rule smdp ff gamma gammafi fi gammafi max gamma rtdp adaptive rtdp smdp td learning algorithms model free rely stochastic approximation asymptotic convergence desired function respectively 
convergence typically slow 
realtime dynamic programming rtdp adaptive rtdp system model speed convergence :10.1.1.117.6173
rtdp assumes system model known priori adaptive rtdp builds model interacts system 
discussed barto asynchronous dp algorithms computational advantages traditional dp algorithms system model 
inspecting equation see model needed rtdp smdp domain consists parts 
state transition probabilities xy 
expected reward transition state state action 
expected discount factor applied value state transition state state action fl 
process dynamics governed continuous time markov chain model needed rtdp analytically derived 
general model difficult analytically derive 
cases adaptive rtdp incrementally build system model direct interaction system 
version adaptive rtdp algorithm smdp described 
set set start state 
initialize fl 
repeat forever actions compute xk px theta xk fl perform update xk min xk select action ak perform ak observe transition xk time units 
update sample reward gammae gammafi fi xk xk ak sample discount factor gammafi update fl 
adaptive rtdp smdp 
fl estimates maintained adaptive rtdp fl 
notice action selection procedure line left unspecified 
rtdp adaptive rtdp choose greedy action 
estimate system model base decisions estimate initially quite inaccurate 
adaptive rtdp needs explore choose actions currently appear optimal order ensure estimated model converges true model time 
rl methods continuous time mdp accepted nips conference nov denver experiment routing heterogeneous servers consider queueing system shown 
arrivals assumed poisson rate arrival customer routed queues servers service times exponentially distributed parameters respectively 
goal compute policy minimizes objective function dt psi scalar cost factors denote number customers respective queues time pair state system time state space problem countably infinite 
actions available state arrival occurs route queue route queue 
routing queueing systems 
known problem optimal policy threshold policy set states optimal route queue characterized monotonically nondecreasing threshold function jn case policy simply join shortest queue function line slicing state space 
applied smdp version learning problem attempt find optimal policy subset state space 
system parameters set fi 
feedforward neural network trained backpropagation function approximator 
learning take exploratory actions order adequately sample available state transitions 
decision time selected action applied state boltzmann distribution rfa ag gammaq xk tk gammaq xk tk computational temperature 
temperature initialized relatively high value resulting uniform distribution prospective actions 
gradually lowered computation proceeds raising probability selecting actions lower application better values 
limit action greedy respect function estimate selected 
temperature learning rate ff decreased time search converge method 
shows results obtained learning problem 
square denotes state visited running axis axis 
color square represents probability choosing action route arrivals queue 
black represents probability white represents probability 
optimal policy black diagonal white diagonal arbitrary colors diagonal 
unsatisfactory feature algorithm performance convergence slow schedules governing decrease boltzmann temperature learning rate ff involve design parameters may result faster convergence 
known optimal policies type structural property holds may extreme practical utility fact constraining value functions way representing combination appropriate basis vectors realize enforce structural property 
rl methods continuous time mdp accepted nips conference nov denver results learning experiment 
panel represents policy total updates panel represents policy total updates panel represents policy total updates 
discussion proposed extending applicability known reinforcement learning methods developed discrete time mdp continuous time domain 
derived semi markov versions td learning rtdp adaptive rtdp straightforward way discrete time analogues 
convergence proofs new algorithms proofs difficult obtain limit problems finite state spaces 
proof convergence new algorithms complicated fact general state spaces involved infinite convergence proofs traditional reinforcement learning methods assume state space finite 
ongoing directed applying techniques complicated systems examining distributed control issues investigating methods incorporating prior knowledge structured function approximators 
professor andrew barto bob crites members adaptive networks laboratory 
supported national science foundation ecs professor barto 
barto bradtke singh 
learning act real time dynamic programming 
artificial intelligence 
accepted 
bertsekas 
dynamic programming deterministic stochastic models 
prentice hall englewood cliffs nj 
bradtke 
incremental dynamic programming line adaptive optimal control 
phd thesis university massachusetts 
darken chang moody 
learning rate schedules faster stochastic gradient search 
neural networks signal processing proceedings ieee workshop 
ieee press 
dayan sejnowski 
td convergence probability 
machine learning 
press 

contraction mappings theory underlying dynamic programming 
siam review april 
hajek 
optimal control interacting service stations 
ieee tac 
rl methods continuous time mdp accepted nips conference nov denver jaakkola jordan singh 
convergence stochastic iterative dynamic programming algorithms 
neural computation 
submitted 
ross 
applied probability models optimization applications 
holden day san francisco 
sutton :10.1.1.132.7760
learning predict method temporal differences 
machine learning 
tsitsiklis 
asynchronous stochastic approximation learning 
technical report lids laboratory information decision systems mit cambridge ma 
watkins 
learning delayed rewards 
phd thesis cambridge university cambridge england 
rl methods continuous time mdp 
