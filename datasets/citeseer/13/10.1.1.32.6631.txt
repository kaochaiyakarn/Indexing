large scale discriminative training speech recognition woodland cambridge university engineering department street cambridge cb pz uk 
dp eng cam ac uk describes evaluates large scale lattice framework discriminative training large vocabulary speech recognition systems gaussian mixture hidden markov models hmms 
concentrates maximum mutual information estimation mmie criterion train hmm systems conversational telephone speech transcription hours training data 
experiments represent largest scale application discriminative training techniques speech recognition authors aware led significant reductions word error rate triphone hmms compared best models trained maximum likelihood estimation 
mmie implementation techniques ensuring improved generalisation interactions maximum likelihood adaptation discussed 
furthermore variations mmie training scheme introduced aim reducing training 

model parameters hmm speech recognition systems normally estimated maximum likelihood estimation mle 
speech really statistics assumed hmm model correctness infinite training set global maximum likelihood estimate optimal sense unbiased minimum variance 
estimating parameters hmm speech recognisers training data unlimited true data source hmm 
case examples constructed alternative discriminative training schemes maximum mutual information estimation mmie provide better performance mle 
mle training model parameters adjusted increase likelihood word strings corresponding training utterances account probability possible word strings 
contrast mle discriminative training schemes take account possible competing word hypotheses try reduce probability incorrect hypotheses recognition errors directly 
discriminative schemes widely noted conventional hmm training schemes find local maximum likelihood function 
small vocabulary recognition tasks relatively small number competing hypotheses training viable :10.1.1.16.9409
large vocabulary tasks especially large datasets main problems generalisation unseen data order increase test set performance mle providing viable computation framework estimate confusable hypotheses perform parameter estimation 
computation problem ameliorated lattice discriminative training framework compactly encode competing hypotheses 
allowed investigation maximum mutual information estimation mmie techniques large vocabulary tasks large data sets variation method described described 
large vocabulary tasks held discriminative techniques mainly produce hmms fewer parameters increase absolute performance mle systems 
key issue generalisation affected amount training data available number hmm parameters estimated training scheme 
discriminative training schemes try generate training set confusions improve generalisation :10.1.1.16.9409
similarly case mmie training increased set training set confusions improve generalisation 
availability large training sets acoustic modelling computational power exploit primary motivation carry current investigation largescale discriminative training 
introduces mmie training criterion optimisation extended baum welch algorithm 
lattices mmie training described particular methods introduced 
sets experiments conversational telephone transcription show mmie training successfully applied range training set sizes 
effect methods improve generalisation interaction maximum likelihood adaptation variations basic training scheme avoid training discussed 

mmie criterion mle increases likelihood training data correct transcription training data models classes participate parameter reestimation 
mmie training proposed alternative mle maximises mutual information training word sequences observation sequences 
language model lm parameters fixed training literature mmie criterion equivalent conditional maximum likelihood estimation proposed 
increases posteriori probability word sequence corresponding training data training data 
technique normally referred mmie term 
training observations fo corresponding transcriptions fw mmie objective function log jm mw composite model corresponding word sequence probability sequence determined language model 
summation denominator taken possible word sequences allowed task replaced jm den jm den encodes full acoustic language model recognition 
noted optimisation requires maximisation numerator term identical mle objective function simultaneously minimising denominator term den 
denominator includes possible word sequences including correct objective function maximum value zero 
minimisation denominator ordinarily involve doing recognition pass training data iteration mmie training 
viable small vocabulary tasks computationally expensive large vocabulary tasks instance cross word context dependent acoustic models conjunction long span language model 
approximation denominator required computational load feasible 
notable feature mmie objective function gives greater weight training utterances low posterior probability correct word sequence 
feature discussed contrasts situation mle training utterances equally weighted 
argued mmie may give undue weight outlier training utterances attempts modify criterion training utterances far decision boundary similar way minimum classification error mce training result improved recognition performance 

extended baum welch algorithm mmie objective function optimised standard gradient methods slow converge second order information may impractical large systems 
version extended baum welch algorithm optimisation 
algorithm uses re estimation formulae reminiscent standard baum welch algorithm mle training 
shown re estimation formula form converge give local optimum sufficiently large value constant mean variance updates continuous density hmms formula lead closed form solution re estimation means variances 
discrete approximation gaussian distribution normandin showed mean particular dimension gaussian state mixture component jm corresponding variance jm assuming diagonal covariance matrices re estimated jm num jm den jm jm num jm den jm jm num jm den jm jm jm num jm den jm jm equations sums data squared data respectively weighted occupancy mixture component state gaussian occupancies summed time jm superscripts num den refer model corresponding correct word sequence recognition model word sequences respectively 
setting key issue update equations setting constant value set large training slow stable small updates may increase objective function iteration 
useful lower bound value ensures variances remain positive 
lower bound constraint shown lead system quadratic inequalities find suitable value fact set twice value 
furthermore single global value lead slow convergence phone specific value 
preliminary experiments reported convergence speed improved set gaussian level gaussian specific jm 
set maximum twice value necessary ensure positive variance updates dimensions gaussian ii global constant multiplied denominator occupancy den jm bulk experiments value 
section values investigated value termed 
setting computing value jm twice minimum value positive variances gaussian setting half maximum value jm den jm gaussians 
scheme results way setting fairly task hmm set independent 
experiments increased training progressed 
mixture weight transition probability updates originally proposed re estimation formula mixture weight parameters jm follows directly jm jm jm constant chosen mixture weights positive 
derivative jm jm num jm den jm extremely sensitive small valued parameters 
alternative robust approximation derivative suggested jm num jm num den jm den method example 
unfortunately update rule lead instability training proceeds alternative sought 
alternative mixture weight update rule suggested free smoothing constants informal experiments shown normally results faster increase mmie objective function approach derivative approximation 
particular state mixture weight update consists finding mixture weights jm maximise function num jm log jm den jm jm jm subject sum constraint 
jm original weights jm mixture component occupancies 
proof maximising increase objective function uses assumption mixture weight varied mixture component occupancies obtained forward backward alignment vary factor ratio new old mixture weights 
purposes proof mixture weight occupancies assumed independent parameters hmm 
optimisation may performed generic function optimisation routine 
experiments reported iterative scheme involves repeatedly mixture weight turn finding optimal value weight assuming relative values fixed maintaining sum constraint 
update equation single row transition matrix performed way mixture weight update 
note mixture weights transition probabilities denominator occupancies zero update equivalent standard mle update 
noted decision tree tied state mixture gaussian hmms experiments reported effect mmie training mixture weights mixture weight update relatively unimportant 
course hmm system tied mixture models mixture weight update rule greater significance 

lattice mmie training previous parameter re estimation formulae section require generation occupation weighted data counts numerator terms rely correct word sequence denominator terms recognition model 
calculation denominator terms directly computationally expensive approximations denominator suggested 
early suggested best lists calculated mle model set approximate set possible sentences mmie training 
moderately complex tasks long sentences small number probable sentences included 
alternative type lattice structure represent various alternatives 
looped lattice model proposed include pronunciation particular word 
approach evaluated word task hours training data 
sophisticated approach word lattices fully encode sequential acoustic language model constraints 
lattices generated htk large vocabulary recognition system 
htk lattices composed nodes represent ends words particular points time arcs connect represent particular word pronunciations 
denominator lattices contain repeated arcs nodes encode slightly different start times different start context dependent hmms due variant previous words 
lattices generated mle hmm set repeatedly iterations mmie training 
technique uses lattices collecting numerator statistics represent possibility alternative pronunciations 
cases denominator lattices contain correct sequence denominator lattice formed merging recogniser lattice numerator lattice 
word lattices numerator denominator technique performed iteration forwardbackward pass word lattice node arc level generate posterior probability particular lattice arc occurring 
viterbi state level segmentation arc arc posterior probability calculate statistics re estimation formulae 
method train hmm sets word vocabulary tasks north american business news corpus cross word triphone acoustic models gram lms hours training data 

improving mmie generalisation key issue mmie training discriminative training general generalisation performance difference training set test set accuracy 
mmie training greatly reduces training set error mle baseline reduction error rate independent test set normally compared mle generalisation performance poorer 
furthermore statistical modelling approaches complex model poorer generalisation 
fairly complex models needed obtain optimal performance mle difficult improve mmie training 
widely thought major application discriminative training techniques large vocabulary recognition tasks reduce error rates relatively parameters improve best achievable error rates mle training aimed challenging view 
number approaches try improve generalisation performance mmie type training schemes discussed 
methods involve trying increase amount confusable data processed training way 
frame discrimination fd technique previously investigated discussed 
experimented techniques aimed improving generalisation weaker language models acoustic model scaling 
frame discrimination frame discrimination fd replaces recognition model probability denominator gaussians parallel 
fd removes constraints gaussian sequences phone unigram gaussian level language model training set occurrences 
model lexical lm provides far confusable states particular utterance 
turn expected reduces training set performance compared mmie improves generalisation 
shown improvements obtained fd reported mmie models task setup 
argued fd generalises confusable data set modelling confusions practice arise perform poorly challenging recognition tasks greater inherent acoustic 
reported fd didn improve error rates mle trained models broadcast news recognition task 
weakened language models shown improved test set performance obtained unigram lm mmie training bigram trigram recognition 
aim provide focus discrimination provided acoustic model loosening language model constraints 
way confusable data generated improves generalisation 
unigram lm mmie training investigated 
acoustic model scaling combining likelihoods hmm acoustic model lm usual scale lm log probability 
necessary primarily due invalid modelling assumptions hmm underestimates probability acoustic vector sequences leading wide dynamic range likelihood values 
alternative lm scaling multiply acoustic model log likelihood values inverse lm scale factor acoustic model scaling 
produce effect language model scaling considering single word sequence viterbi decoding 
likelihoods different sequences added forward backward algorithm denominator effects lm acoustic model scaling different 
language model scaling particular state sequence tends dominate likelihood point time dominates sums path likelihoods 
acoustic scaling paths fairly similar likelihoods non negligible contribution summations 
acoustic model scaling tends increase confusable data set training broadening posterior distribution state occupation den jm update equations 
increase confusable data leads improved generalisation performance 
noted acoustic scaling similar reasons finding word posterior probabilities lat unigram mmie training confusable data constrained word lattices generated trigram lm 
acoustic model lm scaling effects identical viterbi path components acoustic model log likelihood scaled including contribution transition probabilities 
posterior decoding confidence estimation 

current lattice training methods lattice training technique various differences detail 
furthermore variants current scheme investigated 
step generate word level lattices normally mle trained hmm system bigram lm appropriate training set 
step normally performed just experiments section word lattices generated real time rt 
second step generate phone marked lattices label word lattice arc phone model sequence viterbi segmentation points 
word lattices particular hmm set may different generate original word level lattices 
implementation phone marked lattices encode lm probabilities mmie training may different lm generate original word level lattices 
stage typically took xrt generate triphone marked lattices experiments section speed process considerably increased 
phone marked lattices numerator denominator training audio segment alternative implementations generate occupation probabilities associated weighted data statistics needed updates 
full search implementation aims perform full forward backward pass state level constrained lattice 
pruning performed phone marked lattice segmentation points extended short period direction 
alternative exact match case state level forward backward pass context dependent model instance lattice performed solely viterbi segmentation points model 
cases search optimised far possible combining redundantly repeated models requires conversion model level lattice 
recognition experiments model level lattices typically average lattice density arcs 
different optimisations possible cases discussed 
details full search implementation full search case model level lattice compacted combining instances model occur position word overlap time 
single instance model created start times minimum maximum original models 
set arcs entering leaving new combined arc set run times measured intel pentium iii running mhz 
typically ms start phone 
union original sets transitions duplicates removed 
process lattice reduction repeated merges possible decreases average lattice density order magnitude 
full forward backward search resulting lattice performed time information phone extended small margin pruning 
acoustic likelihood scaling performed directly scaling values state output distribution log probability densities 
typically full search method takes xrt iteration experiments section 
details exact match implementation exact match approach calculates likelihood phone segment lattice start times accumulates statistics updates forward backward algorithm 
possible advantages approach 
firstly forwardbackward pass necessary model start times matter times appears lattice exact match typically runs twice quickly full search method 
secondly acoustic log likelihoods scaled keeps multiple parallel confusable models retaining sharp transitions states 
fact segmentation times phone marked lattices treated constants multiple iterations mmie training lead reduced accuracy 

mmie experiments hub data section describes series mmie training experiments cambridge university htk cu htk system transcription conversational telephone data switchboard call home english corpora hub data 
experiments performed preparation nist march hub evaluation 
experiments investigated effect different training set hmm set sizes types acoustic likelihood scaling unigram lms training possible interactions mmie training maximum likelihood linear regression adaptation 
experiments section full search lattice training implementation value set updates 
effect alternatives discussed section 
basic cu htk hub system cu htk hub system continuous mixture density tied state cross word context dependent hmm system htk hmm toolkit 
full system operates multiple passes complex acoustic language models unsupervised adaptation passes 
incoming speech parameterised cepstral coefficients second derivatives form dimensional vector ms 
cepstral mean variance normalisation vocal tract length normalisation performed conversation side training test 
hmms constructed decision tree state clustering triphone models 
lexicon experiments vocabulary vocabulary core dictionary limsi wsj lexicon 
system uses word gram lms estimated interpolation hub acoustic training transcriptions broadcast news texts 
experiments reported trigram lms stated 
system operates multiple passes 
triphone models word lattice generation 
lattices recognition passes system development 
lattice rescoring generate results 
baseline models hub training test data different training sets different test sets mmie experiments 
different training sets ranging hours hours size investigate mmie approach scales large training sets allowing experiments run 
characteristics training sets shown table 
set defined bbn transcriptions train sets transcriptions provided mississippi state university msu 
training sets contain data switchboard swb corpus train sets contain call home english che data 
train sub set subset train covers training speakers swb portion train subset che 
training total conversation sides set time hrs swb che train sub train table hub training sets 
test sets subset hub evaluation set eval sub containing conversation sides switchboard ii swb data che evaluation data set eval containing sides swb che sides total hours data march evaluation data set eval sides swb che sides 
training number gaussians gaussians set speech states state hour train sub train table hub triphone model sets baseline gender independent sets triphone hmms created training set trained mle 
number clustered speech states triphone model set number gaussians state average number gaussians trained hour training data table 
note versions mle model set 
experiments hours training initially investigated mmie training gaussian state hmms best mle trained models 
lattices generated training set bigram lm 
bigram best hypotheses word error rate wer lattice wer 
mmie wer iteration acoustic scaling lm scaling mle table hour experiments mixture component models eval sub comparison acoustic model language model scaling 
gaussian state results table compare acoustic language model scaling iterations mmie training 
seen acoustic scaling helps avoid training best wer iterations 
training set lattices regenerated single mmie iteration gave wer showing technique effective reducing training set error 
regenerated lattices better subsequent training iterations just initially generated word lattices 
advantage mmie training gaussian state system small system fewer gaussians state investigated 
shown table gaussian system approximately ratio parameters training data train sub system 
mmie wer iteration lattice bigram lattice unigram mle table hour experiments mixture component models eval sub comparison lattice lms 
results mmie training gaussian state system acoustic scaling shown table show best performance mmie iterations 
furthermore gain mle system absolute bigram lm absolute unigram lm gaussian state hmm set slightly outperforms gaussian system 
furthermore seen weakened lm unigram improves performance little fact gain unigram greater acoustic scaling performed acoustic scaling weakened lm increase amount diversity confusable data 
experiments hours training effect hour train sub set investigated tests performed eval sub eval sets 
case phone marked denominator lattices 
results mmie training shown table 
mmie wer iteration eval sub eval mle table word error rates eval sub eval train sub training 
seen peak improvement comes iterations case larger reduction error rate seen gaussian state experiments absolute eval sub absolute eval 
word error rate best hypothesis original bigram word lattices measured training data 
mmie models obtained iterations portion training data gave error rate mmie provided sizeable reduction training set error 
experiments training set baseline model set section 
triphone experiments hours training performance smaller training sets led investigate mmie training available hub data hour train set 
train set contains segments numerator denominator word level lattices created trained segment phone marked lattices generated 
mmie wer iteration eval sub eval mle table word error rates train training che data weighting 
experimented data weighting setup mmie training 
rationale test data sets contain equal amounts switchboard che data training set balanced 
gave higher weighting che data training 
results experiments eval sub eval test sets shown table 
seen data weighting improvement wer absolute eval sub absolute eval 
data weighting gives absolute eval variable results eval sub 
data weighting applied mle training eval sub mle baseline improves absolute 
concluded extra weight placed poorly recognised data mmie training relative mle reduces need data weighting technique 
model training cu htk hub system uses models investigated mmie training models full train set 
decision tree state clustering process includes questions regarding phone context word boundaries 
baseline system uses speech states gaussians state give gaussians hour training data 
mmie training triphone generated word lattices phone marked lattices regenerated models necessary prune word lattices 
results mmie trained eval sub set shown table 
note experiments previous ones reported include pronunciation probabilities 
mmie wer iteration eval sub mle table mmie results eval sub 
pronunciation probabilities 
mmie training runs discussed largest wer reduction absolute comes iterations training 
mmie training working reductions error rate quite large triphone models 
may extra pruning required phone marked lattices hmm parameters estimate 
interaction mllr results models adapted particular conversation side 
model adaptation parameter transformation maximum likelihood linear regression mllr established technique important investigate interaction mmie trained models transformation parameters estimated mle 
measure mllr adaptation performance mmie mle models che data weighting full decode test data rescoring lattices gram language model 
output pass estimate global speech mllr block diagonal mean diagonal variance transform 
data available separate transform estimated silence models output respective non adapted pass adaptation supervision 
adapted models second full decode pass 
results experiments shown table 
adaptation wer eval mle mmie mllr table effect mllr mle mmie trained models 
results show mmie models absolute better mle models mllr better mllr 
case mllr just mmie trained models relatively small number parameters estimated mllr global transforms keep gaussians configuration optimised mmie 
march cu htk hub system mmie triphone models included march cu htk hub evaluation system 
system incorporates numerous changes compared described mmie models system gave greatest benefit 
initial lattices generated gender independent mmie triphone hmms vocabulary gram language model 
subsequent passes data mmie triphones mle soft tied triphones 
model sets pronunciation probabilities iterative mllr adaptation combined global full variance transform 
final system output model set generated minimise expected word error rate confusion networks 
output mmie mle model stages combined confusion network combination give final output 
eval data system gives error rate march evaluation data eval lowest error rate obtained evaluation statistically significant margin 

investigation mmie training scheme section properties mmie training scheme section investigated number variations 
include effect acoustic likelihood eval test set consistently yields lower error rates eval recognition systems 
scaling number confusable states exact match full search lattice processing methods effect different values global constant optimisation test set performance brief investigation modified objective function 
increased confusion data acoustic model scaling illustrate effect acoustic scaling language model scaling distribution posterior probability state occupation average number states posterior probability greater computed full search exact match lattice search procedures 
results shown table 
search scaling type acoustic lm num den num den full search exact match table average number states posterior probability occupation greater acoustic scaling 
expected acoustic likelihood scaling significantly broadens posterior probability distribution 
noteworthy exact match procedure reduces number confusable states quite markedly models computed outside lattice arc viterbi segmentation points 
objective function optimisation generalisation iteration mmie training starting mle trained models full search full search exact match exact match exact match mmie criterion optimisation 
increase mmie objective function corresponding test set error rate eval sub measured full search exact match schemes values global smoothing constant 
experiments hour train sub training setup acoustic scaling 
change objective function training proceeds shown corresponding error rates 
consistent difference wer iteration mmie training word error rate eval sub full search full search exact match exact match exact match error rates mmie training variants 
full search exact match implementation search ran significantly faster 
problem training easily occurs second iteration mmie training yields results 
higher value global smoothing constant increases training results objective function optimised poorer final value danger training 
underlying problem improving objective function past certain point causes test set accuracy deteriorate 
criterion objective function alternative solution training modify objective function 
particular interpolation mmie mle objective functions gives type criterion examined 
function investigated objective function implemented simply appropriate scaling mmie numerator statistics 
exact match method train sub training set 
evaluation eval sub test set showed error rate converged objective function optimised yield error th iteration 
models gave test set accuracy conventional mmie training noted model parameters changed mle parameter values pure mmie ones accuracy means standard deviations mle values compared standard deviations similarly performing pure mmie models 

discussion discussed discriminative training large vocabulary hmm speech recognition training set size level task difficulty previously attempted 
shown significant reductions word error rates obtained transcription conversational telephone speech 
mmie objective function reviewed key issues application large vocabulary tasks discussed efficiency objective function optimisation generalisation test data 
extended baum welch algorithm gaussian specific constants shown iterations updating sufficient obtain performance large range data set sizes model types 
furthermore novel updating formula mixture weight parameters introduced 
weakened language model unigram importantly acoustic likelihood scaling investigated methods increasing amount relevant confusable data mmie training 
techniques improve generalisation allow better performance obtained mmie training complex models 
contrast previously held beliefs possible mmie training challenging large vocabulary tasks reduce error rates best mle models just provide performance reduced number parameters 
lattice approach calculating statistics related objective function denominator specific implementations lattice search described 
methods previous discriminative training algorithms perform full forward backward pass model level 
differ constraints model boundaries comparable error rate exact match scheme lower computational cost 
mmie training effective clear overtraining easily occur 
possible solution modify objective function aid generalisation directly 
method doing interpolation mmie mle objective functions effective 
intend investigate modifications improve generalisation performance 
concentrated mmie objective function discussed directly applied objective functions 
general formulation lattice discriminative training proposed measures mce lattice framework 
mmie training scheme applied transcription hub data training sets hours size triphone models resulted absolute reduction word error rate 
trained mmie triphone hmms march cu htk hub system lowest error rate evaluation statistically significant margin 
method computationally expensive feasible investigate mmie training scale 
believe exciting research largescale discriminative training done 
acknowledgments part supported 
dan holds studentship foundation 
bahl brown de souza mercer 
maximum mutual information estimation hidden markov model parameters speech recognition proc 
icassp pp 
tokyo 
chou 
lee 
juang 
minimum error rate training best string models 
proc 
icassp pp 
minneapolis 
chow 
maximum mutual information estimation hmm parameters continuous speech recognition best algorithm 
proc 
icassp albuquerque 
woodland 
large vocabulary decoding confidence estimation word posterior probabilities 
proc 
icassp istanbul 
woodland 
posterior probability decoding confidence estimation system combination 
proc 
speech transcription workshop college park 
gales woodland 
mean variance adaptation mllr framework 
computer speech language vol 
pp 

gales 
maximum likelihood linear transformations hmm speech recognition 
computer speech language vol 
pp 

gopalakrishnan nadas nahamoo 
decoder selection proc 
icassp pp 
new york 
gopalakrishnan nadas nahamoo 
inequality rational functions applications statistical estimation problems 
ieee trans 
information theory vol 
pp 

hain woodland niesler whittaker 
htk system transcription conversational telephone speech 
proc 
icassp pp 
phoenix 
hain woodland 
cu htk march hub transcription system 
proc 
speech transcription workshop college park 
hochberg foote silverman 
hidden markov model neural network training techniques connected alpha digit speech recognition 
proc 
icassp pp 
toronto 
kapadia valtchev young 
mmi training continuous parameter recognition timit database 
proc 
icassp pp 
minneapolis 
kapadia 
discriminative training hidden markov models 
ph thesis cambridge university engineering dept woodland 
maximum likelihood linear regression speaker adaptation continuous density hmms 
computer speech language vol 
pp 
luo jelinek 
probabilistic classification hmm states large vocabulary continuous speech recognition proc 
icassp pp 
phoenix 
brill stolcke 
finding consensus words lattice word error minimization 
proc 
eurospeech pp 
budapest 
merialdo 
phonetic recognition hidden markov models maximum mutual information training 
proc 
icassp pp 
new york 
nadas 
decision theoretic formulation training problem speech recognition comparison training unconditional versus conditional maximum likelihood 
ieee trans 
assp vol 
pp 

nadas nahamoo 
training algorithm speech recognition 
ieee trans 
assp vol 
pp 

normandin 
improved mmie training algorithm speaker independent small vocabulary continuous speech recognition 
proc 
icassp pp 
toronto 
normandin 
hidden markov models maximum mutual information estimation speech recognition problem 
ph thesis mcgill university montreal 
normandin 
mmie training large vocabulary continuous speech recognition 
proc 
pp 
yokohama 
woodland 
frame discrimination training hmms large vocabulary speech recognition 
proc 
icassp pp 
phoenix 
woodland 
investigation frame discrimination continuous speech recognition 
technical report cued infeng tr cambridge university engineering dept schluter 
comparison discriminative training criteria 
proc 
icassp pp 
seattle 
schluter muller wessel ney 
interdependence language models discriminative training 
proc 
ieee workshop pp 
keystone colorado 
valtchev 
discriminative methods hmm speech recognition 
ph thesis cambridge university engineering dept valtchev woodland young 
discriminative training large vocabulary speech recognition 
proc 
icassp pp 
atlanta 
valtchev odell woodland young 
mmie training large vocabulary speech recognition systems 
speech communication vol 
pp 

woodland odell valtchev young 
htk large vocabulary speech recognition system 
proc 
icassp pp 
detroit 
woodland hain moore niesler whittaker 
htk broadcast news transcription system development results 
proc 
darpa broadcast news workshop pp 
morgan kaufmann 
young odell woodland 
tree state tying high accuracy acoustic modelling 
proc 
arpa human language technology workshop pp 
morgan kaufmann 
