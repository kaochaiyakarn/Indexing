information spiking neuron charles stevens anthony zador salk institute mnl la jolla ca zador salk edu generally agreed neurons transmit information synaptic inputs spike trains code information transmitted understood 
upper bound information encoded obtained hypothesizing precise timing spike conveys information 
develop general approach quantifying information carried spike trains hypothesis apply leaky integrate fire model neuronal dynamics 
formulate problem terms probability distribution interspike intervals isis assuming spikes detected arbitrary finite temporal resolution 
absence added noise variability isis encode information information rate simply entropy isi distribution gammap log times spike rate 
provides exact expression information rate 
methods developed determine experimentally information carried spike trains lower bound information rate provided stimulus reconstruction method tight 
preliminary series experiments methods estimate information rates hippocampal neurons slice response somatic current injection 
pilot experiments suggest information rates high bits spike 
information rate spike trains cortical neurons spike trains communicate neurons 
output neuron stochastic function input neurons 
interest know neuron telling neurons inputs 
information spike train provide signal 
consider noise added signal produce total input 
passed possibly stochastic functional produce output spike train 
assume information contained spike train represented list spike times extra information contained properties spike height width 
note characteristics spike train mean instantaneous rate derived representation derivative property turns relevant formulation specialized appropriately 
interested mutual information input signal ensemble output spike train ensemble 
defined terms entropy signal entropy spike train joint entropy gamma note mutual information symmetric joint entropy 
note signal spike train completely independent mutual information joint entropy just sum individual entropies 
completely line intuition case spike train provide information signal 
information estimation stimulus reconstruction bialek colleagues bialek reconstruction method obtain strict lower bound mutual information experimental setting 
method expression mathematically equivalent eq 
involving conditional entropy signal spike train gamma gamma est est upper bound conditional entropy obtained reconstruction est signal 
entropy estimated second order statistics reconstruction error gammas est property gaussian upper bound 
intuitively equation says information gained spike train observing stimulus just initial uncertainty signal absence knowledge spike train minus uncertainty remains signal spike train known second equation says second uncertainty greater particular estimate optimal estimate 
information estimation spike train reliability adopted different approach equivalent expression mutual information gamma zjs term entropy spike train second zjs conditional entropy spike train signal intuitively inverse repeatability spike train repeated applications signal 
eq 
advantage spike train deterministic function input permits exact calculation mutual information 
follows important difference conditional entropy term eq 
deterministic stochastic component zjs stochastic component 
absence added noise discrete entropy zjs eq 
reduces 
isis independent simply expressed terms entropy discrete isi distribution gamma log nh number spikes probability spike occurred interval deltat deltat 
assumption finite timing precision deltat keeps potential information finite 
advantage considering isi distribution full spike train distribution univariate multivariate estimating requires data 
conditions isis independent 
correlations isis arise stimulus spike generation mechanism 
shall guarantee correlations arise spike generator considering forgetful integrate fire model information previous spike eliminated spike 
limit temporally uncorrelated stimuli stimuli drawn white noise ensemble sure isis independent eq 
applied 
presence noise evaluated give gamma js js conditional entropy isi signal js gamma js log js js probability obtaining isi response particular stimulus presence noise 
conditional entropy thought quantification reliability spike generating mechanism average trial trial variability spike train generated response repeated applications stimulus 
maximum spike train entropy follows useful compare information rate neuron limiting case exponential isi distribution maximum entropy point process rate papoulis 
provides upper bound information rate possible spike train spike rate temporal precision 
re gamma rt exponential distribution mean spike rate assuming temporal precision deltat entropy spike log deltat entropy time rate rh log deltat example hz deltat sec gives bits second spike second bits spike 
discretize hz spike train msec bins possible transmit bits second 
reduce bin size fold rate increases log bit spike bits spike double lose bit get bit note different firing rate hz halving bin size increases entropy spike bit spike spike rate twice high bit second increase information rate 
model consider functional describing forgetful leaky model spike generation 
suppose add noise signal threshold sum produce spike train 
specifically suppose voltage neuron obeys gammav membrane time constant white gaussian distributions mean variance oe voltage reaches threshold time neuron emits spike time resets initial condition language neurobiology model thought tuckwell limiting case neuron leaky spike generating mechanism receiving excitatory inhibitory synaptic inputs 
note input white correlations spike train induced signal neuron resets spike correlations induced mechanism 
isis independent eq 
applied 
estimate mutual information ensemble input signals ensemble outputs model isis independent construction need evaluate js determine distribution isis js conditional distribution isis ensemble signals 
note corresponds passage time distribution ornstein uhlenbeck process tuckwell 
neuron model considering regimes determined relation asymptotic membrane potential absence threshold threshold 
regime threshold crossings occur signal variance zero oe 
subthreshold regime threshold crossings occur oe 
limit ae mean firing rate low compared integration time constant occur subthreshold regime isi distribution exponential coefficient variation cv unity cf 
koch 
low rate regime firing deterministically poisson mean distinguish usual usage poisson neuron stochastic situation instantaneous firing rate parameter probability firing interval depends stimulus 
case exponential isi distribution arises deterministic mechanism 
border regimes threshold just equal asymptotic potential explicit exact solution entire isi distribution sugiyama gamma oe gamma gamma exp gamma oe gamma special case absence fluctuations oe membrane potential hovers just subthreshold 
neurophysiological interpretation excitatory inputs just balance inhibitory inputs neuron hovers just verge firing 
information rates noisy noiseless signals compare information rate neuron balance point maximum entropy spike train 
simplicity brevity consider zero noise case 
fig 
shows information spike function firing rate calculated eq 
varied changing signal variance oe assume spikes resolved temporal resolution msec isi distribution bins msec wide 
dashed line shows theoretical upper bound exponential distribution limit approached neuron operating far threshold poisson limit 
model upper bound information spike monotonically decreasing function spike rate model achieves upper bound mean isi just equal membrane time constant 
model information saturates low firing rates exponential distribution information increases bound 
high firing rates information goes zero firing rate fast individual isis resolved temporal resolution 
fig 
shows information rate information second neuron balance point goes maximum firing rate increases 
maximum occurs lower firing rate exponential distribution dashed line 
bounding information rates stimulus reconstruction construction eq 
gives exact expression information rate model 
compare lower bound provided stimulus reconstruction method eq 
bialek 
assess tight lower bound provides 
fig 
shows lower bound provided reconstruction solid line reliability dashed line methods function firing rate 
firing rate increased increasing mean input stimulus noise set 
low firing rates estimates nearly identical high firing rates reconstruction method substantially underestimates information rate 
amount underestimate depends model parameters decreases noise added stimulus 
tightness bound empirical question 
bialek colleagues show conditions experiments underestimate factor clear potential underestimate different conditions different systems greater 
discussion generally agreed spike trains encode information neuron inputs clear information encoded 
idea mean firing rate encodes signal variability mean effectively noise 
alternative view variability encodes signal information encoded precise times spikes occur 
view information expressed terms interspike interval isi distribution spike train 
encoding scheme yields higher information rates mean rate interval longer typical isi considered 
quantified information content spike trains hypothesis simple neuronal model 
consider model construction isis independent information rate bits sec computed directly information spike bits spike spike rate spikes sec 
information spike turn depends temporal precision spikes resolved precision infinite information content infinite message example encoded decimal expansion precise arrival time single spike reliability spike transduction mechanism entropy isi distribution 
low firing rates neuron subthreshold limit isi distribution close theoretically maximal exponential distribution 
interest information theoretic analyses neural code attributed seminal bialek colleagues bialek rieke measured information rate sensory neurons number systems 
results broad agreement considered information rate linear filtered threshold crossing model 
developed functional expansion term describes limit spike times isis independent second term correction correlations 
model differs model mainly reset spike 
consequently natural model gaussian signal noise convolved linear filter times resulting waveform crosses threshold called spikes 
representation spike train model sequence firing times model natural representation sequence tn isis 
choice convenience representations equivalent 
models complementary 
model results obtained colored signals noise conditions awkward model 
model contrast class highly correlated spike trains conveniently considered awkward model 
isi condition required model restrictive independent spike condition model spikes independent iff isis isi distribution exponential 
particular high firing rates isi distribution far exponential spikes far independent isis independent 
assumed input white entropy infinite mutual information grow bound temporal precision spikes resolved improves 
spike train transmitting minute fraction total available information 
signal saturates capacity spike train 
clear real neurons behave implausible typical cortical neuron receives synaptic inputs information rate input target information rate impinging target fold greater neglecting synaptic unreliability decrease substantially capacity 
preliminary series experiments reliability method estimate information rate hippocampal neuronal spike trains slice response somatic current injection stevens zador unpublished 
conditions isis appear independent method developed applied 
pilot experiments information rates high bits spike observed 
bialek rieke de van 

reading neural code 
science 


optimization principles neural code 
hasselmo editor advances neural information processing systems vol 

mit press cambridge ma 
papoulis 

probability random variables stochastic processes nd edition 
mcgraw hill 
rieke de van bialek 

neural coding 
mit press 
koch 

highly irregular firing cortical cells inconsistent temporal integration random 
neuroscience 
sugiyama moore 

solutions stochastic model neuronal spike production 
mathematical biosciences 
tuckwell 

theoretical neurobiology vols 
cambridge 
information balance point firing rate hz information rate balance point 
top information spike decreases monotonically spike rate solid line 
bounded entropy exponential limit dashed line highest entropy isi distribution mean rate limit approached neuron subthreshold regime 
information rate goes firing rate order temporal resolution deltat 
information spike balance point nearly optimal 
msec deltat msec bottom information second conditions 
information rate balance point solid curve exponential distribution dashed curve pass maximum maximum greater occurs higher rate 
firing rates smaller rates indistinguishable 
msec deltat msec spike rate hz estimating information stimulus reconstruction 
information rate estimated reconstruction method solid line exact information rate dashed line shown function firing rate 
reconstruction method significantly underestimates actual information particularly high firing rates 
firing rate varied mean input parameters membrane time constant msec spike bin size deltat msec signal variance oe threshold 
