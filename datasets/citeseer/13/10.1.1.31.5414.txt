non linear neurons low noise limit factorial code maximizes information transfer jean pierre nadal laboratoire de physique statistique ecole normale sup erieure rue paris cedex france nestor parga departamento de te universidad aut de madrid blanco madrid spain investigate consequences maximizing information transfer simple neural network input layer output layer focussing case non linear transfer functions 
assume receptive fields synaptic transfer functions adapted environment 
main result bounded invertible transfer functions case vanishing additive output noise input noise maximization information linsker principle leads factorial code solution required redundancy reduction principle barlow 
show result valid linear generally unbounded transfer functions provided optimization performed additive constraint written sum terms specific output neuron 
study effect non zero input noise 
find order input noise assumed small compared small output noise results valid provided output noise uncorrelated neuron 
biophysics neurophysiological processes short title information maximization non linear neurons appear network index ps nadal physique ens fr pages infomax applied non linear neurons low noise limit leads redundancy reduction 
laboratoire associ au 
aux universit es paris vi paris vii 
theoretical approaches analysis sensory systems understand organization architecture receptive fields provide models self organization account epigenetic developement 
natural consider function processing stages sensory processing case visual system retina layers realize particular neural representation code environment 
possible approach assume observed nervous systems self organization process result optimization cost function characterizes quality code 
defines program choose cost function possibly set relevant constraints derive set optimal network self organization algorithms order reach optimal solution compare biological data 
question define relevant set cost function 
long time ago suggested information theory provide appropriate tools 
particular general ideas developped barlow origin theoretical experimental studies see 
barlow insists need building neural representation easily subsequent processing 
leads idea factorial code output unit statistically independent unit 
network independent features mixed input signal 
means minimize redundancy neural code fact quantified terms information theoretic criterion 
modeling layers visual system redundancy reduction principle studied barlow case discrete noiseless coding atick continuous noisy neural states 
approach systematically developped order account color scale invariant stereo perception 
demanding requirement system simply maximize amount information output conveys input signal 
suggests particular way modelling transfer function sensory neuron adapted particular environment animal lives 
remarkable experimental tests performed particular van hateren indicating validity hypothesis 
appropriate cost function taken mutual information output input network 
idea information preservation developped linsker name infomax principle model layers visual system 
atick linsker van hateren studies network linear neurons studied maximization chosen cost function performed synaptic receptive fields study done gaussian input distribution analytical results obtained 
pointed particular prediction criteria similar especially large signal noise ratios depending particular constraint chosen differ large noise 
note alternative approaches necessarily information theoretic criteria 
particular may ask possibility reconstructing signal neural representation 
experimentally shown detailed statistics observed spikes reconstruct little error input signal 
note reconstruction decoding task viewed supervised learning task dual coding task 
may ask optimal network criterion minimize quadratic error reconstructed signal true signal 
shown optimization leads different predictions derived information theoretic criteria linsker barlow particular small signal noise ratios 
consider maximization information transfer infomax principle linsker relationship redundancy reduction barlow 
main concern study network non linear transfer functions 
papers quoted dealt linear neurons 
may fewer systematic studies devoted non linear processing 
works optimization transfer function study input distributions transfer function optimally adapted redundancy reduction binary generally discrete coding networks binary neurons studied tools statistical mechanics effect weak non linearity neurons non linear transfer functions limit large output noise 
said easy see large noise limit optimal transfer function step function neuron mcculloch pitts neuron threshold chosen way statistically neuron equally 
consider opposite limit small output noise 
discuss specific realistic case note small noise situation considered theoretical studies applied modelling layers visual pathway 
previous studied case noiseless perceptron binary mcculloch pitts neurons 
showed particular network infomax redundancy reduction principles equivalent 
consider case neurons arbitrary invertible transfer functions presence small output noise 
ask consequence maximizing information transfer optimization synaptic transfer functions 
outcome precisely partly elucidate origin similarity results obtained infomax redundancy reduction principles 
organized follows 
section formalize information processing problem case single neuron non linear bounded transfer function 
rederive standard result giving low noise limit optimal transfer function signal distribution 
section generalize derivation case network outputs neuron having non linear bounded transfer function 
obtain main result showing limit vanishing additive output noise optimal information transfer obtained factorial code 
section extend approach unbounded particular linear transfer functions showing result bounded transfer functions remains certain conditions 
show section results presence small input noise 
discuss results particular case gaussian signal distributions compute information capacity network particular small noise limit 
perspectives section 
transfer information single neuron section review basic properties information transfer single neuron non linear sigmoidal transfer function instant time signal activates sensory units leading total postsynaptic potential neuron connected input units 
output neuron nonlinear transfer function simplicity assume bounded invertible comment shortly non invertible cases 
assumed input noise presence additive output noise necessarily gaussian probability distribution noise strength measured say noise variance gamma brackets means averaging distribution 
interest amount information conveyed signal choose transfer function order maximize information transfer 
basic result information transfer maximum vanishing noise limit output distribution uniform maximum entropy distribution 
achieved known image processing sampling histogram equalization performed fig 
dh psi psi probability distribution induced input distribution 
optimal transfer function adapted particular input probability distribution shown psi fi cosh fi 
elegant derivation just said see 
physical meaning easily understood large amount information obtained discriminate finely input signal 
potential distribution known sample values values observed nearby values psi large discriminate slope transfer function large way outputs far apart possible 
argument correct noise infinitely small non zero provides resolution scale output 
experimental evidence adaptations sensory systems leading 
order deal section output neurons instructive formalize little information transfer problem 
consider neuron output activity 
absence input noise postsynaptic potential assumed deterministic function input signal illustrative purpose think type shown neuron receiving inputs denoting activation jth input unit input signal postsynaptic potential input hj phi phi phi phi phi phi phi 
gamma gamma gamma gamma gamma gamma gamma 
jn generic model output activity obtained applying non linear transfer function postsynaptic potential 
simplest neuron model potential linear combination input components 
associated synaptic efficacy total postsynaptic potential follows partly section independent particular model considered transformation signal potential include non linearities delays synaptic transmission 
general goal choose synaptic parameters depend transfer function order maximize information transfer 
section consider optimization transfer function 
set couplings input signal distribution induces probability distribution psi postsynaptic potential quantity interest mutual information random variables absence input noise equal mutual information psi ln conditional probability observing knowing input resulting output distribution gamma dh psi noise additive mutual information rewritten gamma term differential entropy probability distribution gamma dv ln second term depends noise distribution gamma dz ln point notice additive noise level absence input noise maximization mutual information equivalent maximization entropy output distribution 
case gaussian noise equal ln gaussian distribution largest entropy distributions variance ln goes zero second term goes infinity 
matters correction infinite constant finite limit zero noise limit obtained dh psi ffi gamma change variable dv dh psi dv dh result written gammad psi psi dh psi ln psi consider derivative probability distribution 
recognizes psi kullback distance relative entropy probability distribution psi probability distribution quantity positive null zero probability distributions identical possibly zero measure set 
maximizing mutual information equivalent low noise limit minimizing kullback distance input distribution defined derivative transfer function particular gets result announced 
chosen positive derivative replace absolute value result read dh psi alternative solution possibility chose gamma psi 
signal distribution single bump case gaussian obtains cell activity shut large inputs 
output neurons consider generalization result network number outputs 
ith output neuron postsynaptic potential acts transfer function possibly different 
noted atick realizes factorial code possible find couplings psi psi apply preceeding reasoning output neuron 
result optimal set transfer functions simply psi show conditions input noise small additive output noise factorial code individual adaptations gives precisely maximum information transfer 
working hypothesis output activities arbitrary noise distribution need independent random variables 
define noise strength total variance 
gamma pt noise additive mutual information written gamma 
limit change variable dv dh psi dv dh gives gammad psi psi psi ln psi finds direct generalization giving mutual information constant equal minus kullback distance potential distribution probability defined product fact important consequences 
main consequence announced mutual information maximized synaptic realizing factorial code individual adaptations transfer functions 
obtain particular remarkable fact infomax principle linsker redundancy reduction principle barlow precisely requires build factorial code lead identical predictions receptive fields working hypotheses zero input noise low output noise 
note maximization mutual information predicts receptive fields transfer functions 
notice factorial code optimize information transfer 
example gaussian input distribution number output units choice different principal components give optimal information transfer 
degeneracy comes absence input noise scale compare different directions 
expect account small amount input noise largest principal components selected 
obvious factorial code remain optimal choice 
considered effect small input noise analysis section 
consequence algorithmic point view optimization respect couplings adaptation transfer functions may considered separately deal linear part processing transformation input asking factorial code potential distribution compute transfer functions 
remarkable receptive fields predicted analysis purely linear system non linear processing taken account 
application linear processing principle redundancy reduction la barlow done atick precisely low noise limit understood just practical way finding code maximize information transfer 
point dealing separately linear non linear parts processing leads optimal solution possible find factorial code potential distribution 
case obvious strategy efficient 
interesting study non gaussian distributions 
main result section specific case non linear bounded transfer functions 
know happens purely linear generally unbounded transfer functions 
consider section 
unbounded transfer functions assume transfer functions restricted class unbounded functions 
optimization mutual information solution introduce constraint order defined problem 
known linear processing case mutual information equal logarithm signal noise ratio upper bound exists restrict optimization say value signal variance 
case constraint couplings potentials distribution outputs distribution 
expect optimum strongly depend choice constraint 
study effect different kind constraints 
simplicity consider unique output neuron define cost function adding kullback distance term enforces constraint ae psi gamma ae gamma constraint may potential example gamma gamma psi dh psi function output gamma gamma psi dv dh psi constraints rewrite cost function ae dh psi ln psi phi gamma ln distribution phi defined phi normalization factor ensures dh phi particular case constraint output distribution depend dv case sees optimization task equivalent effective bounded transfer function derivative phi 
easily check recovers standard formula mutual information linear channel 
generalization outputs straightforward 
constraints gamma psi psi gamma psi gets expression analogous phi shows general case phi factorial distribution optimal code factorial 
constraint written sum individual constraints phi phi phi optimal solution factorial code 
result allows understand similarities differences works different criteria 
forthcoming compare model discussed van hateren information maximization criterion proposed atick approach reduction redundancy 
conclude section maximizing mutual information linear generally unbounded transfer functions leads factorial code constraint additive 
note additive constraints usually considered 
account input noise order correction want see effect non zero input noise strength delta 
pay attention fact limits delta commute 
finite noise inputs outputs 
consider case zero output noise finite input noise going noisy postsynaptic potential output reversible change variable mutual information equal linear system noise case considerations preceeding section apply 
section interested opposite limit want perturbation calculation section order delta goes infinity 
obtained computing delta expansion finite value limit 
see relevant small parameter fact delta output consider case output neuron output noise input noise variance delta gamma delta assume gaussian input output noise 
small delta expansion gaussian gammah delta delta ffi delta ffi delta write conditional probability jh jh delta jh jh exp gamma gamma gets algebra mutual information written delta mutual information absence input noise finite value correction gamma dv ln dh psi dh jh gamma dh psi output distribution absence input noise 
goes zero term takes asymptotic expression obtained section gamma ln gamma psi term finite limit delta 

relevant term second 
write order delta gamma delta dh psi optimization respect fo transfer function gives psi delta psi psi gamma psi psi dh psi optimal transfer function gamma ln gamma delta psi expected psi distributions equivalent 
particular case gaussian input psi gammah 

gets psi 
optimal solution maximizes potential variance result identical obtained optimization linear neuron limit small noise 
small input noise optimal transfer function psi delta see text 
input distribution psi shown 
full line resulting transfer function small correction artificially enhanced delta dashed line optimal transfer function absence noise 
outputs consider case output neurons 
assume gaussian input output noise output ith neuron output noise input noise correlation matrix delta gamma 
delta ii delta 
assume uncorrelated output noise gamma 
ffi conditional probability value delta jk gets expression gamma ln psi ii gamma ii dh psi limit gets generalization gamma delta ii dh psi delta value delta 
sees non diagonal terms introducing correlations appear order delta 
leading order delta optimal solution factorial code 
choice input distribution psi output neuron optimal transfer function equation replacing psi marginal distribution psi delta ii delta set optimal transfer functions mutual information gamma delta ii psi psi dh psi consequence expression order maximize mutual information find couplings realizing factorial code psi minimized 
note result valid uncorrelated output noise generalization calculation correlated noise straightforward finds term order delta introduces correlations output units 
contrast zero input noise case correlations output noise matter 
discussion comparison linear case gaussian input distribution illustrate result consider particular case neurons type shown synaptic pg 
assume gaussian input distribution correlation matrix jk gaussian noise inputs ij delta ffi correlation matrix ii ffi ij ffi jj ii gets optimization transfer functions equ 
gamma ln ln jrj ii gamma delta jj ii jrj ii maximization couplings leads directions vectors largest eigenvectors correlation matrix note solution independent scale jj ii couplings scale exists transfer function 
fixing norm couplings equivalent fixing range potentials transfer function goes 
expression mutual information obtained optimizing transfer function compared redundancy reduction cost function set linear neurons conditions weak noise 
precisely consider linear network noise output output noise 
particular limit considered corresponds large output noise compared coupling strength 
set global scale couplings norm order may choose defined output noise variance cost function defined measure redundancy gamma amount information conveyed ith neuron 
redundancy zero factorial code minimized appropriate constraint notations written ln det delta uu ii delta uu ii order delta limit small output noise finds term order delta disappears equal gamma delta uu gamma uu ii simply order gamma ln ii minus second term 
take account constraint 
leads expression closely related choice constraint output variances 
defining ae lagrange needed enforce global constraint maximize ln ii gamma ae ii note criterion care true positive 
give simple example consider input signal states signal redundantly encoded binary units xor representation coded equal probability equal probabilities 
information obtained looking single unit loss information bit 
comparing expression sees ae plays role similar parameter delta appears low noise limit maximization mutual information network non linear transfer functions leads essentially predictions receptive fields redundancy reduction criterion barlow atick applied network linear neurons constraint output variances 
information capacity address question information capacity network 
discussed adaptation network environment input distribution 
problem information capacity determine environment network efficient able extract information signal optimization parameters synaptic transfer functions 
problem analogous channel capacity information theory consider family channels system consider allowed adapt environment relevant information capacity largest channel capacity accessible family networks characterized choice synaptic transfer functions 
knowing information capacity know environment network optimization able extract information consider family networks having output neurons bounded invertible transfer functions section 
optimization requires factorial code information capacity equal times capacity single neuron 
consider case single output neuron 
transfer function fixed vanishing noise limit information capacity equal mutual information obtained relation fulfilled optimal input distribution equals derivative transfer function gamma ln 
discussion relation standard transfer functions 
allow optimization function take account input noise 
optimal environment psi minimal 
optimization problem ill defined adding constraint input distribution 
search optimal input distribution differential entropy gamma dh psi ln psi minimization psi imposes flat distribution optimum psi interval gammaa obtained capacity gamma ln gamma delta gamma particular input distribution term order delta equation transfer function zero 
associated optimal transfer function ramp goes linearly interval gammaa 
interesting compare mutual information gaussian input distribution variance giving entropy ln 
seen psi write mutual information gauss optimization network gauss gamma ln gamma delta gamma dependence gauss larger gauss 
chosen optimal distribution having variance solution case leading gamma ln gamma delta 
compared information gaussian distribution variance gauss gamma ln gamma delta 
case relative 

surprising find flat distribution optimal vanishing noise limit optimal output distribution flat interval optimal input distribution rescaled version distribution 
contrast case linear channel case optimal input gaussian distribution 
considered problem maximizing information transfer network neurons inputs outputs focussing case non linear transfer functions 
assumed transfer functions synaptic adapted environment 
main consequence analysis limit small additive output noise smaller input noise infomax principle linsker redundancy reduction criterion barlow equivalent non linear processing taken account 
result linear processing optimization performed constraint written sum terms depending output unit 
explains results obtained atick linsker similar 
detail comparison finite noise forthcoming 
practical consequence optimization receptive fields synaptic transfer functions done separately may look linear transformation realizes factorial code adapt transfer functions independently output neuron 
course true factorial code exists 
step strategy valid considers sufficient act input distribution gaussian paying attention mean covariance input distribution see detailed discussions motivation approximation 
absence input noise factorial code maximize information transfer 
provided codes exist number output units smaller number inputs codes exist may say optimized network extracting qualitative information looking statistically independent features 
presence small input noise provide scale measuring input signal network extract quantitative information looking relevant features 
main results valid input distribution mainly discussed consequences assuming factorial code particular considered case gaussian distribution 
clearly interesting study case non gaussian distributions particular empirical distributions derived analysis natural scenes 
note analysis done time domain 
case maximizing information lead decorrelation case meaning source separation 
algorithm proposed source separation ad hoc cost function related statistical correlations set neuron units 
interesting see similar algorithms defined gradient descent information theoretic criterion mutual information non linear output units redundancy reduction cost function linear units 
conversely interesting see efficient empirical algorithms developped source separation adapted decorrelation spatial domain known algorithms spatial inputs assume gaussian distribution 
source separation algorithms proposed odor coding algorithms olfactory system insects approach similar barlow atick visual system 
quite plausible unique framework say maximization mutual information study encoding spatio temporal signals leading joined decorrelation space time 
partly supported french spanish program picasso 
attneave informational aspects visual perception 
psychological review 
barlow coding sensory messages 
thorpe editors current problems animal behaviour pages 
cambridge university press 
shannon weaver mathematical theory communication 
university illinois press urbana 
blahut principles practice information theory 
addison wesley cambridge ma 
simple coding procedure enhances neuron information capacity 

van hateren theoretical predictions spatiotemporal receptive fields fly experimental validation 
comp 
physiology 
linsker self organization perceptual network 
computer 
barlow adaptation decorrelation cortex 
durbin mitchison editors computing neuron pages 
addisonwesley cambridge ma 
bialek editor 
princeton lectures biophysics 
world scientific pub singapore 
bialek editor 
princeton lectures biophysics 
nec princeton 
atick information theory provide ecological theory sensory processing 
network 
atick redlich retina know natural scenes 
neural comp 
atick li redlich understanding retina color coding principles 
neural comp 
li atick theory striate cortex 
neural comp 
li atick efficient stereo coding multiscale representation 
neural comp appear 
bialek rieke de van reading neural code 
science 
linsker sensory processing information theory 
grassberger nadal editors statistical physics statistical inference back pages 
kluwer acad 
pub dordrecht 
ruderman designing receptive fields highest fidelity 
network 
nadal 
parga duality learning machines bridge supervised unsupervised learning 
neural comp 
linsker deriving receptive fields optimal encoding criterion 
hanson cowan lee giles editors neural information processing systems pages 
morgan kaufmann san mateo 
information entropy maximization transmission neuron nonlinearity 
appear 
barlow mitchison finding minimum entropy codes 
neural comp 
redlich redundancy reduction strategy unsupervised learning 
neural comp 
bialek zee understanding efficiency human perception 
phys 
rev lett 
nadal 
parga information processing perceptron unsupervised learning task 
network 
schuster learning maximizing information transfer nonlinear noisy neurons noise breakdown 
phys 
rev 
russ image processing handbook 
crc press 
barlow possible principles underlying transformation sensory messages 
editor sensory communication page 
press cambridge ma 
nadal 
parga infomax constrained variances noise reduction decorrelation 
preparation 
atick redlich theory early visual processing 
neural comp 
field relations statistics natural images response properties cortical cells 
opt 
soc 
am 
ruderman bialek statistics natural images scaling woods 
cowan tesauro alspector editors neural information processing systems pages 
morgan kaufmann san mateo 
jutten herault signal proc 
hopfield 
proc 
natl 
acad 
sci 
usa 
blind separation sources nonlinear neural algorithm 
neural networks 
linsker local synaptic learning rules suffice maximize mutual information linear network 
neural comp 
atick redlich convergent algorithm sensory receptive field development 
neural comp 

fort 
coding odor quality roles convergence inhibition 
network appear 

