review communicated steven nowlan unifying review linear gaussian models sam roweis computation neural systems california institute technology pasadena ca zoubin ghahramani department computer science university toronto toronto canada factor analysis principal component analysis mixtures gaussian clusters vector quantization kalman filter models hidden markov models unified variations unsupervised learning single basic generative model 
achieved collecting disparate observations derivations previous authors introducing new way linking discrete continuous state models simple nonlinearity 
nonlinearities show independent component analysis variation basic generative model 
show factor analysis mixtures gaussians implemented autoencoder neural networks learned squared error plus regularization term 
introduce new model static data known sensible principal component analysis novel concept spatially adaptive observation noise 
review literature involving global local mixtures basic models provide pseudocode inference learning basic models 
unifying review common statistical techniques modeling multidimensional static data sets multidimensional time series seen variants underlying model 
show include factor analysis principal component analysis pca mixtures gaussian clusters vector quantization independent component analysis models ica kalman filter models known linear dynamical systems hidden markov models hmms 
relationships models noted passing literature 
example hinton revow dayan note fa pca closely related digalakis rohlicek ostendorf relate forward backward algorithm hmms address roweis zoubin gatsby ucl ac uk 
gatsby computational neuroscience unit university college london queen square london ar neural computation massachusetts institute technology sam roweis zoubin ghahramani kalman filtering 
article unify disparate observations previous authors rubin thayer digalakis hinton elliott moore ghahramani hinton hinton ghahramani review algorithms instances single basic generative model 
unified view allows show interesting relations previously disparate algorithms 
example factor analysis mixtures gaussians implemented autoencoder neural networks different nonlinearities learned squared error cost penalized regularization term 
ica seen nonlinear version factor analysis 
framework possible derive new model static data pca sensible probabilistic interpretation novel concept spatially adaptive observation noise 
review literature involving global local mixtures basic models provide pseudocode appendix inference learning basic models 
basic model basic models discrete time linear dynamical systems gaussian noise 
models assume state process question summarized time vector state variables causes observe directly 
system produces time step output observable vector access 
state assumed evolve simple order markov dynamics output vector generated current state simple linear observation process 
state evolution observation processes corrupted additive gaussian noise hidden 
continuous valued state variable basic generative model written xt axt wt axt yt cxt vt cxt state transition matrix observation measurement generative matrix 
vector vector random variables representing state evolution observation noises respectively independent vectors column vectors 
denote transpose vector matrix notation xt determinant matrix denoted matrix inversion symbol means distributed multivariate normal gaussian distribution mean covariance matrix written gaussian evaluated point denoted unifying review linear gaussian models values noise sources temporally white uncorrelated time step time step spatially gaussian distributed zero mean covariance matrices denote respectively 
written place wt vt emphasize noise processes knowledge time index 
restriction zero mean noise sources loss generality 
state evolution noise gaussian dynamics linear xt order gauss markov random process 
noise processes essential elements model 
process noise state xt shrink exponentially zero blow exponentially direction leading eigenvector similarly absence observation noise state longer hidden 
illustrates basic model engineering system block form network form common machine learning 
notice degeneracy model 
structure matrix moved matrices means loss generality models identity matrix 
course restricted way values yt observed free whiten rescale 
components state vector arbitrarily reordered corresponds swapping columns typically choose ordering norms columns resolves degeneracy 
network diagram unfolded time give separate units time step 
diagrams standard method illustrating graphical models known probabilistic independence networks category models includes markov networks bayesian belief networks formalisms pearl lauritzen spiegelhalter whittaker smyth 
graphical model representation dependency structure variables multivariate probability distribution 
node corresponds random variable absence arc variables corresponds particular conditional independence relation 
graphical models assumption weakly motivated central limit theorem strongly analytic tractability 
specifically add st dimension state vector fixed unity 
augmenting extra column holding noise mean extra row zeros unity bottom right corner takes care nonzero mean 
similarly adding extra column takes care nonzero mean 
particular covariance matrix symmetric positive semidefinite diagonalized form rotation matrix eigenvectors diagonal matrix eigenvalues 
model identity matrix generate exactly equivalent model new state vector etx new covariance identity matrix sam roweis zoubin ghahramani linear dynamical system generative model 
block unit delay 
covariance matrix input noise covariance matrix output noise network model smaller circles represent noise sources units linear 
outgoing weights drawn hidden unit 
model equivalent kalman filter model linear dynamical system 
scope review important point provide general framework working models consider 
review unify extend known statistical models signal processing algorithms focusing variations linear graphical models gaussian noise 
main idea models equations hidden state sequence xt informative lower dimensional projection explanation complicated observation sequence yt 
aid dynamical noise models states summarize underlying causes data succinctly observations 
reason state dimensions smaller number observables words assume precisely model matrices full rank problem inferring unifying review linear gaussian models rank introduced full rank 
probability computations popularity linear gaussian models comes fortunate analytical properties gaussian processes sum independent gaussian distributed quantities gaussian distributed output linear system input gaussian distributed gaussian distributed 
means assume initial state system gaussian distributed states xt observations yt gaussian distributed 
fact write explicit formulas conditional expectations states observables xt xt axt xt yt xt cxt yt 
furthermore markov properties model gaussian assumptions noise initial distributions easy write expression joint probability sequence states outputs xt xt yt xt 
negative log probability cost just sum matrix quadratic forms log yt cxt yt cxt log state sequence consecutive observations defined long notion related observability systems theory goodwin sin 
reason dynamic models useful state spaces larger dimension observations case single state vector provides compact representation sequence observations 
words convolution gaussians gaussian 
particular convolution false statement sum gaussians gaussian fourier domain equivalent statement multiplication gaussians gaussian longer normalized 
sam roweis zoubin ghahramani xt axt xt axt log log log 
learning estimation problems latent variable models wide spectrum application data analysis 
scenarios know exactly hidden states supposed just want estimate 
example vision problem hidden states may location pose object tracking problem states may positions momenta 
cases write priori observation state evolution matrices knowledge problem structure physics 
emphasis accurate inference unobserved information data example image object radar observations 
scenarios trying discover explanations causes data explicit model causes 
observation state evolution processes entirely unknown 
emphasis robustly learning parameters model observed data assign high likelihood 
speech modeling example situation goal find economical models perform recognition tasks particular values hidden states models may meaningful important 
goals estimating hidden states observations model learning model parameters typically manifest solution distinct problems inference system identification 
inference filtering smoothing 
problem inference filtering smoothing asks fixed model parameters said unknown hidden state sequence observations 
question typically precise ways 
basic quantity able compute total likelihood observation sequence possible 
marginalization requires efficient way integrating summing joint probability easily computed equation similar formulas possible paths state space 
integral available simple compute conditional distribution proposed hidden state sequence observations unifying review linear gaussian models dividing joint probability total likelihood observations 
interested distribution hidden state particular time filtering attempt compute conditional posterior probability xt yt observations including time smoothing compute distribution xt xt entire sequence observations 
possible ask conditional state expectation observations extend time steps partial smoothing time steps current time partial prediction 
conditional calculations closely related computation equation intermediate values recursive method compute equation give desired distributions equations 
filtering smoothing extensively studied continuous state models signal processing community starting seminal works kalman kalman bucy rauch rauch tung literature known machine learning community 
discrete state models literature stems baum colleagues baum petrie baum baum petrie soules weiss baum hmms viterbi optimal decoding 
book elliott colleagues contains thorough mathematical treatment filtering smoothing general models 
learning system identification 
second problem interest linear gaussian models learning system identification problem observed sequence sequences outputs find parameters maximize likelihood observed data computed equation 
learning problem investigated extensively neural network researchers static models discrete state dynamic models hmms general bayesian belief networks 
corresponding area study control theory known system identification investigates learning continuous state models 
linear gaussian models approaches system identification sam roweis zoubin ghahramani ljung clarify relationship models review article focus system identification methods expectation maximization em algorithm 
em algorithm linear gaussian dynamical systems originally derived shumway stoffer reintroduced extended neural computation field ghahramani hinton 
digalakis 
similar extension speech processing community 
mention book elliott 
covers learning context 
basis learning algorithms authors powerful em algorithm baum petrie dempster laird rubin 
objective algorithm maximize likelihood observed data equation presence hidden variables 
denote observed data hidden variables parameters model 
maximizing likelihood function equivalent maximizing log likelihood log log dx 
distribution hidden variables obtain lower bound log dx log dx log dx log dx log dx middle inequality known jensen inequality proved concavity log function 
define energy global configuration log readers may notice lower bound negative quantity known statistical physics free energy expected energy minus entropy neal hinton 
em algorithm alternates maximizing respect distribution parameters respectively holding fixed 
starting initial parameters step qk arg max unifying review linear gaussian models step arg max qk 
easy show maximum step results exactly conditional distribution qk point bound equality qk 
maximum step obtained maximizing term equation entropy depend step arg max log dx 
expression associated em algorithm obscures elegant interpretation em coordinate ascent neal hinton 
step step change guaranteed decrease likelihood combined em step 
heart em learning procedure idea solutions filtering smoothing problem estimate unknown hidden states observations current model parameters 
fictitious complete data solve new model parameters 
estimated states obtained inference algorithm usually easy solve new parameters 
linear gaussian models typically involves minimizing quadratic forms equation done linear regression 
process repeated new model parameters infer hidden states 
shall review details particular algorithms various cases touch general point causes confusion 
goal maximize total likelihood see equation equivalently maximize total log likelihood observed data respect model parameters 
means integrating summing ways generative model produced data 
consequence em algorithm maximization find needing compute maximize expected log likelihood joint data expectation taken distribution hidden values predicted current model parameters observations 
appears maximizing incorrect quantity doing fact guaranteed increase keep quantity interest iteration algorithm 
continuous state linear gaussian systems having described basic model learning procedure focus specific linear instances model hidden state variable continuous noise processes gaussian 
allow sam roweis zoubin ghahramani elucidate relationship factor analysis pca kalman filter models 
divide discussion models generate static data generate dynamic data 
static data temporal dependence information lost permuting ordering data points yt dynamic data time ordering data points crucial 
static data modeling factor analysis spca pca 
situations reason believe assume point data set generated independently identically 
words natural temporal ordering data points merely form collection 
cases assume underlying state vector dynamics matrix zero matrix simply constant take loss generality zero vector corrupted noise 
new generative model cx 
notice xt driven noise yt depends xt temporal dependence disappeared 
motivation term static notations 
longer separate distribution initial state 
model illustrated 
analytically integrate equation obtain marginal distribution gaussian 
things important notice 
degeneracy mentioned persists structure means loss generality restricting diagonal 
furthermore arbitrary sharing scale diagonal typically restrict columns unit vectors identity matrix resolve degeneracy 
follows assume loss generality 
second covariance matrix observation noise restricted way model capture interesting informative projections state 
ifr restricted learning simply choose set sample covariance data trivially achieving maximum likelihood model explaining diagonalize rewrite covariance degeneracy clear ce ce diagonal simply replace ce 
unifying review linear gaussian models static generative model continuous state covariance matrix input noise covariance matrix output noise network model smaller circles represent noise sources units linear 
outgoing weights drawn hidden unit 
model equivalent factor analysis spca pca models depending output noise covariance 
factor analysis diagonal 
spca pca lim structure data noise 
remember model reduced single gaussian distribution better having covariance model equal sample covariance data 
note restricting making diagonal constitute loss generality original model equations 
intuitive spatial way think static generative model 
white noise generate spherical ball density dimensional state space 
ball stretched rotated dimensional observation space matrix looks dimensional 
convolved covariance density described get final covariance model 
want resulting ellipsoidal density close possible ellipsoid sample covariance data 
restrict shape covariance constraining force interesting information appear result 
observe varieties filtering smoothing reduce problem static model time dependence 
seeking posterior probability single hidden state corresponding single observation 
inference easily done sam roweis zoubin ghahramani linear matrix projection resulting density gaussian cx cct ct cc obtain expected value unknown state estimate uncertainty value form covariance computing likelihood data point merely evaluation gaussian equation 
learning problem consists identifying matrices family em algorithms various cases discussed detail review 
factor analysis 
restrict covariance matrix controls observation noise diagonal words covariance ellipsoid axis aligned set state noise identity matrix recover exactly standard statistical model known maximum likelihood factor analysis 
unknown states called factors context matrix called factor loading matrix diagonal elements known 
see everitt brief clear 
inference calculation done exactly equation 
learning algorithm loading matrix exactly em algorithm take care constrain properly easy diagonal unconstrained maximum likelihood estimate see rubin thayer ghahramani hinton 
completely free procedure called exploratory factor analysis build priori zeros confirmatory factor analysis 
exploratory factor analysis trying model covariance structure data pk free parameters free parameters full covariance matrix 
key assumption 
factor analysis attempts explain covariance structure observed data putting variance unique coordinate matrix putting correlation structure observation response wold 
essence factor analysis considers axis rotation original data arrived special observation noise called sensor noise independent coordinates axes 
original scaling coordinates unimportant 
change units measured components factor analysis merely rescale corresponding entry correction comes degeneracy unitary transformations factors 
see example everitt 
unifying review linear gaussian models row achieve new model assigns rescaled data identical likelihood 
hand rotate axes measure data easily fix things noise constrained axis aligned covariance diagonal 
em factor analysis criticized quite slow rubin thayer 
standard method fitting factor analysis model quasi newton optimization algorithm fletcher powell empirically converge faster em 
em algorithm efficient way fitting factor analysis model wish emphasize factor analysis latent variable models reviewed em provides unified approach learning 
online learning shown possible derive family em algorithms faster convergence rates standard em algorithm kivinen warmuth bauer koller singer :10.1.1.30.7849
spca pca 
restricting merely diagonal require multiple identity matrix words covariance ellipsoid spherical model call sensible principal component analysis spca roweis 
columns span principal subspace subspace pca call scalar value diagonal global noise level 
note spca uses pk free parameters model covariance 
inference done equation learning em algorithm take trace maximum likelihood estimate learn noise level see roweis 
factor analysis spca considers original axis rotation data arrived unimportant measurement coordinate system rotated spca left multiply rotation likelihood new data change 
hand original scaling coordinates privileged spca assumes observation noise variance directions measurement units observed data 
rescale components model easily corrected spherical covariance 
spca model similar independently proposed probabilistic principal component analysis tipping bishop 
go take limit lim keeping diagonal elements finite obtain standard principal component analysis pca model 
directions columns isotropic scaling data space arbitrary just easily take limit diagonal elements infinite holding finite take limits 
idea noise variance infinitesimal compared scale data 
sam roweis zoubin ghahramani known principal components 
inference reduces simple squares projection lim ct cc 
noise infinitesimal posterior states collapses single point covariance zero 
em algorithm learning roweis learn pca just diagonalize sample covariance data take leading eigenvectors multiplied eigenvalues columns approach give step problems 
em learning algorithm amounts iterative procedure finding leading eigenvectors explicit diagonalization 
important final comment regular pca define proper density model observation space ask directly likelihood assigned model data 
examine quantity proportional negative log likelihood limit zero noise 
sum squared deviation data point projection 
cost learning algorithm ends minimizing available evaluation pca model fits new data 
critical failings pca translating points arbitrary amounts inside principal subspace effect model error 
recall rank left multiplication ct cct appears defined cct invertible exactly equivalent left multiplication ctc ct singular value decomposition idea defining inverse diagonal singular value matrix inverse element zero case remains zero 
intuition cct truly invertible directions invertible exactly ct project 
computationally hard diagonalize invert large matrices 
requires enormous amount data large sample covariance matrix full rank 
working patterns large thousands number dimensions want extract tens principal components naively try diagonalize sample covariance data 
techniques snapshot method sirovich attempt address require diagonalization matrix number data points 
em algorithm approach solves problems requiring explicit diagonalization whatsoever inversion matrix 
guaranteed converge true principal subspace subspace spanned principal components 
empirical experiments roweis indicate converges iterations ratio leading eigenvalues near unity 
unifying review linear gaussian models time series modeling kalman filter models 
term dynamic data refer observation sequences temporal ordering important 
data want ignore state evolution dynamics provides aspect model capable capturing temporal structure 
systems described original dynamic generative model shown equations known linear dynamical systems kalman filter models extensively investigated engineering control communities decades 
emphasis traditionally inference problems famous discrete kalman filter kalman kalman bucy gives efficient recursive solution optimal filtering likelihood computation problems rts recursions rauch rauch solve optimal smoothing problem 
learning unknown model parameters studied shumway stoffer known ghahramani hinton digalakis 
parameters unknown 
illustrates model appendix gives pseudocode implementation 
extend spatial intuition static case dynamic model 
point state space surrounded ball density described stretched observation space convolved observation noise covariance described 
static case centered ball density origin state space center state space ball flows time step time step 
flow field described eigenvalues eigenvectors matrix move new point flow field center ball point pick new state 
new state flow new point apply noise 
identity matrix zero matrix flow move state just evolves random walk noise set discrete state linear gaussian models consider simple modification basic continuous state model state time takes finite number discrete values 
real world processes especially distinct modes operation better modeled internal states continuous 
possible construct models mixed continuous discrete state 
state evolution order markovian dynamics observation process linear additive gaussian noise 
modification involves winner take nonlinearity wta defined wta vector new vector unity position largest coordinate input zeros positions 
discrete state generative model simply xt wta axt wt wta axt yt cxt vt cxt sam roweis zoubin ghahramani wta discrete state generative model dynamic data 
wta block implements winner take nonlinearity 
block unit delay 
covariance matrix input noise covariance matrix output noise network model smaller circles represent noise sources hidden units winner take behaviour indicated dashed lines 
outgoing weights drawn hidden unit 
model equivalent hidden markov model tied output covariances 
longer known state transition matrix see matrix shortly 
vector vector temporally white spatially gaussian distributed noises independent initial state generated obvious way wta soon see loss generality restricted identity matrix 
discrete state generative model illustrated 
static data modeling mixtures gaussians vector quantization 
just continuous state model consider situations unifying review linear gaussian models natural ordering data set matrix zero matrix 
discrete state case generative model wta cx 
state generated independently fixed discrete probability histogram controlled mean covariance 
specif probability assigned gaussian ically ej region space jth coordinate larger 
ej unit vector jth coordinate direction 
notice obtain nonuniform priors wta nonlinearity require nonzero mean noise 
state chosen corresponding output generated gaussian mean jth column covariance exactly standard mixture gaussian clusters model covariances clusters constrained 
probabilities ej correspond mixing coefficients clusters columns cluster means 
constraining various ways corresponds constraining shape covariance clusters 
model illustrated 
compute likelihood data point explicitly perform sum equivalent integral equation contains terms ej ci ei ci ci denotes ith column varieties inference filtering simply seeking set discrete probabilities ej words need probabilistic classification 
problem easily solved computing responsibilities cluster data point ej ej ej ei cj ej ci ei continuous static case dispense special treatment initial state 
sam roweis zoubin ghahramani wta static generative model discrete state 
wta block implements winner take nonlinearity 
covariance matrix input noise covariance matrix output noise network model smaller circles represent noise sources hidden units winner take behaviour indicated dashed lines 
outgoing weights drawn hidden unit 
model equivalent mixture gaussian clusters tied covariances vector quantization vq lim cj ci 
mean state vector data point exactly vector responsibilities data point 
quantity defines entire posterior distribution discrete hidden state data point 
measure randomness uncertainty hidden state evaluate entropy normalized entropy discrete distribution corresponding 
may related variance posterior factor analysis analogy deceptive 
defines entire distribution variance measure needed 
learning consists finding cluster means columns covariance mixing coefficients easily done em corresponds exactly maximum likelihood competitive learning duda hart nowlan entropy distribution divided logarithm lies zero 
unifying review linear gaussian models clusters share covariance 
introduce extensions model remove restriction 
continuous state case consider limit observation noise infinitesimal compared scale data 
results standard vector quantization model 
inference classification problem solved nearest neighbor rule euclidean distance multiple identity matrix mahalanobis distance unscaled matrix 
similarly pca observation noise disappeared posterior collapses mass cluster closest corresponding uncertainty entropy zero 
learning em equivalent batch version means algorithm proposed lloyd 
pca vector quantization define proper density observation space 
examine sum squared deviation point closest cluster center quantity proportional likelihood limit zero noise 
batch means algorithms minimize cost lieu maximizing proper likelihood 
time series modeling hidden markov models 
return fully dynamic discrete state model introduced equations 
key observation dynamics described equation exactly equivalent traditional discrete markov chain dynamics state transition matrix tij xt ej xt ei easy see compute equivalent state transition matrix tij probability assigned gaussian mean ith column covariance region space jth coordinate larger 
true transition matrix rows sum unity exist matrices dynamics equivalent 
similarly initial probability mass function easily computed desired histogram states exist achieve 
similar degeneracy exists discrete state model model structure noise covariance means columns chosen set equivalent transition probabilities tij loss generality restrict identity matrix means columns harder see 
sketch proof loss generality set covariance identity matrix 
set dot product mean vector vector having unity positions zero moving direction change probabilities 
degrees freedom mean probability model 
set mean randomly projection unity direction 
move mean line defined constraint probabilities remain constant probabilities desired value 
repeat set correctly 
sam roweis zoubin ghahramani set probabilities 
equivalently restrict mean set probabilities initial state 
generative model equivalent standard hmm emission probability densities constrained covariance 
likelihood filtering computations performed socalled forward alpha recursions complete smoothing done forward backward alpha beta recursions 
em algorithm learning exactly known baum welch reestimation procedure baum petrie baum 
important peculiar consequence discretizing state affects smoothing problem 
state sequence formed probable state posterior distribution time computed forward backward recursions observed data model parameters single state sequence produced observed data 
fact sequence states obtained concatenating states individually maximum posterior probability time step may zero probability posterior 
creates need separate inference algorithms find single state sequence observations 
algorithms filtering smoothing called viterbi decoding methods viterbi 
need similar decoding continuous state case 
turns due smooth unimodal nature posterior probabilities individual states continuous case posteriors gaussian sequence maximum posteriori states exactly single state trajectory regular kalman filter rts smoothing recursions suffice 
possible see example rabiner juang learn discrete state model parameters results viterbi decoding forward backward smoothing words maximize joint likelihood observations single state sequence total likelihood summed possible paths state space 
independent component analysis great deal interest blind source separation problem attempts recover number source signals observations resulting signals knowledge original sources independent 
square linear version problem observation process characterized entirely square invertible matrix words observation streams sources delay echo convolutional distortion 
experience shown surprising result nongaussian distributed sources problem solved prior knowledge sources widely believed proved theo unifying review linear gaussian models see mackay high kurtosis source distributions easily separated :10.1.1.48.120:10.1.1.48.120
focus modified classic version due bell sejnowski baram roth original independent component analysis algorithm comon 
bell sejnowski derived information maximization perspective modified algorithm obtained defining particular prior distribution components vector xt sources deriving gradient learning rule maximizes likelihood data yt limit zero output noise amari cichocki yang pearlmutter parra mackay :10.1.1.48.120:10.1.1.48.120
algorithm originally derived unordered data extended modeling time series pearlmutter parra 
show generative model underlying ica obtained modifying slightly basic model considered far 
modification replace wta nonlinearity introduced general nonlinearity operates componentwise input 
generative model static data cx 
role nonlinearity convert gaussian distributed prior nongaussian prior 
loss generality set covariance structure obtained linear transformation random variable linear transformation subsumed nonlinearity 
assuming generative nonlinearity invertible differentiable choice generative nonlinearity results corresponding prior distribution source probability density function px 
important distinguish generative nonlinearity nonlinearity ica learning rule 
call learning rule nonlinearity clarify distinction nonlinearities 
classic ica defined square invertible limit vanishing noise lim conditions posterior density delta function ica algorithm defined terms learning recognition unmixing weights generative mixing weights gradient learning rule increase likelihood wy sam roweis zoubin ghahramani learning rule nonlinearity derivative implicit log px log prior dx mackay :10.1.1.48.120:10.1.1.48.120
generative nonlinearity results nongaussian prior px turn results nonlinearity maximum likelihood learning rule 
somewhat generative models perspective ica discussed terms learning rule nonlinearity implicit prior sources 
popular choice ica learning rule nonlinearity tanh function corresponds heavy tailed prior sources mackay px :10.1.1.48.120:10.1.1.48.120
cosh equation obtain general relationship cumulative distribution function prior sources unit variance noise erf monotonic erf error function du 
relationship solved obtain expression example px cosh find setting ln tan erf causes generative model equations generate vectors component distributed exactly cosh 
nonlinearity shown 
ica seen linear generative model nongaussian priors hidden variables nonlinear generative model gaussian priors hidden variables 
possible derive em algorithm ica observation noise nonzero fewer sources observations 
complication posterior distribution product nongaussian prior gaussian likelihood term difficult evaluate 
posterior step consists maximizing expected log joint probability function step arg max log log yi indexes data points denotes expectation respect posterior distribution yi yi 
term unifying review linear gaussian models nonlinearity equation converts gaussian distributed source distributed cosh 
depend second term quadratic derivatives respect obtain linear system equations solved usual manner yi xx 
similar step derived generative model linear step requires evaluating second moments posterior distribution xx 
necessary know posterior moments computed 
may computed gibbs sampling certain source priors table lookup closed form computation 
particular moulines 
attias schreiner independently proposed gaussian mixture model prior component source posterior distribution gaussian mixture evaluated analytically derive em algorithm mixing matrix source densities 
caveat number gaussian components posterior grows exponentially number sources limits applicability method models sources 
limit zero noise em updates derived manner degenerate decrease likelihood contradict convergence proof em algorithm 
increase likelihood explain uses em fit standard zero noise ica model 
source modeled mixture gaussians sources components mixture 
sam roweis zoubin ghahramani alternatively compute posterior distribution product gaussian prior nongaussian likelihood 
may easy may wish resort gibbs sampling geman geman markov chain monte carlo methods neal 
option employ deterministic trick bishop williams context generative topographic map gtm probabilistic version kohonen self organized topographic map 
approximate gaussian prior finite number points trick 
words wj wj finite sample 
generative model takes points maps fixed nonlinearity adaptable linear mapping adds gaussian noise covariance produce 
generative model constrained mixture gaussians constraint comes fact way centers move varying computing posterior amounts computing responsibility gaussians data point 
responsibilities problem linear means solved equation 
traditional zero noise limit ica responsibilities select center closest data point exactly manner standard vector quantization 
ica potentially implemented em limit zero output noise 
network interpretations regularization early modern history neural networks realized pca implemented linear autoencoder network baldi hornik 
data fed input target network network parameters learned squared error cost function 
section show factor analysis mixture gaussian clusters implemented manner albeit different cost function 
understand probabilistic model learned autoencoder useful recognition generation decomposition autoencoder hinton zemel hinton dayan revow 
autoencoder takes input produces internal representation hidden layer generates output reconstruction input 
call mapping hidden output layers generative part network generates data usually compact representation noise model 
conversely call mapping input hidden units recognition part network unifying review linear gaussian models generative weights recognition weights network state inference learning parameters static data model 
input clamped input units bottom mean posterior estimated state appears hidden units 
covariance state posterior constant easily computed weights known 
inference computation trivial linear projection learning weights inference network difficult 
input hidden weights constrained function hidden output weights network trained autoencoder self supervised learning 
outgoing weights drawn input unit hidden unit 
produces representation hidden variables input 
usually assumed deterministic think recognition network computing posterior mean hidden variables input 
generative model factor analysis assumes hidden states observables normally distributed get posterior probabilities hidden states equation 
assume generative weight matrix hidden units outputs noise model output gaussian covariance posterior mean hidden variables ct cct hidden units compute posterior mean exactly linear weight matrix input hidden units 
notice tied need estimate learning 
denote expectations posterior state distribution example dx 
theory em algorithm see section know way maximize likelihood maximize expected value log joint probability posterior distribution hidden variables log sam roweis zoubin ghahramani changing signs ignoring constants equivalently minimize cost function cx cx log ctr cx log log trace 
defined posterior covariance step reorganized terms fact ct cx trace 
terms cost function equation just squared error cost function evaluated respect gaussian noise model covariance exactly terms minimized fitting standard neural network gaussian noise model 
term regularization term accounts posterior variance hidden states inputs 
take derivatives cost function differentiate respect usual em algorithm differentiate cost posterior distribution hidden variables 
derivatives respect premultiplying obtain weight change rule cx 
term usual delta rule 
second term simply term decaying columns respect posterior covariance hidden variables 
intuitively higher uncertainty hidden unit outgoing weight vector shrunk zero 
summarize factor analysis implemented tying recognition weights generative weights particular regularizer addition squared reconstruction error learning 
analyze mixture gaussians model manner 
recognition network meant produce mean hidden variable inputs 
assume discrete hidden variable represented unit vector mean just vector probabilities settings inputs responsibilities 
assuming equal mixing coefficients ej ei ij responsibilities pca assumes infinitesimal noise posterior distribution hidden states zero variance regularizer vanishes 
unifying review linear gaussian models defined equation ej exp cj cj exp ci ci exp jy exp iy defined ct cj 
equation describes recognition model linear followed softmax nonlinearity written full matrix notation 
words simple network exact inference linear input hidden weights softmax hidden units 
appealing em algorithm obtain cost function minimized implement mixture gaussians 
log probability data hidden variables written log cx cx log const 
previous derivation obtain cost function log trace xt second order term xt evaluates matrix diagonal zero 
factor analysis depends input 
summarize mixture gaussians model implemented 
recognition part network linear followed softmax nonlinearity 
cost function usual squared error penalized regularizer exactly form factor analysis 
similar network interpretations obtained probabilistic models 
comments extensions advantages theoretical practical unified treatment unsupervised methods reviewed 
theoretical viewpoint treatment emphasizes techniques inference various models essentially just correspond probability propagation particular model variation 
similarly learning procedures application em derivation assumes tied covariance equal mixing coefficients 
slightly complex equations result general case 
sam roweis zoubin ghahramani algorithm increase total likelihood observed data iteratively 
furthermore origin zero noise limit algorithms vector quantization pca easy see 
unified treatment highlights relationship similar questions different models 
example picking number clusters mixture model state dimension dynamical system number factors principal components covariance model number states hmm really question 
practical standpoint unified view models allows apply known solutions hard problems area similar problems 
example framework obvious deal properly missing data solving learning inference problems 
topic understood static models little rubin tresp ahmad ghahramani jordan typically addressed linear dynamical systems literature 
example easy design models having mixed continuous discrete state vector example hidden filter hmms fraser directly addressed individual literatures discrete continuous models 
practical advantage ease natural extensions basic models developed 
example hierarchical mixture experts formalism developed jordan jacobs consider global mixtures model variants discussed 
fact mixtures considered mixtures linear dynamical systems known switching state space models see shumway stoffer ghahramani hinton mixtures factor analyzers ghahramani hinton pca hinton mixtures hmms smyth 
mixture constrained mixtures gaussians clusters gives mixture model mk components possible covariance matrices 
tied covariance approach popular speech modeling reduce number free parameters 
corresponds full unconstrained mixture gaussians model clusters 
possible consider local mixtures conditional probability yt xt longer single gaussian complicated density mixture gaussians 
constrained mixture gaussians model way get full mixture 
hmms known extension usually standard approach emission density modeling rabiner juang 
possible constrained mixture models output density model hmm see example saul uses factor analysis hmm output density 
aware considers variation continuous state cases static dynamic data 
important natural extension spatially adaptive observation noise 
idea observation noise different statis unifying review linear gaussian models tics different parts state observation space described single matrix discrete mixture models idea known achieved giving mixture component private noise model 
continuous state models idea relatively unexplored interesting area investigation 
crux problem parameterize positive definite matrix space 
propose simple ways achieve 
possibility replacing single covariance shape observation noise conic linear blending basis covariance shapes 
case linear dynamical systems factor analysis amounts novel type model local covariance matrix computed conic linear combination canonical covariance matrices tensor product current state vector equivalently noiseless observation cx master noise tensor approach drop conic restriction allow general linear combinations add multiple identity matrix resulting noise matrix order positive definite 
third approach represent covariance shape compression elastic sphere due spatially varying force field 
representation easier parameterization field unconstrained hard learn local field measurements effective covariance shape 
bishop sec 
considered simple nonparametric methods estimating input dependent noise levels regression problems 
goldberg williams bishop explored idea context gaussian processes 
interesting consider happens dynamic models output noise tends zero 
words dynamic analogs pca vector quantization 
linear dynamical systems hmms causes state longer hidden 
linear dynamical systems optimal observation matrix performing pca data principal components columns hmms vector quantization data codebook vectors columns 
observation matrices state longer hidden 
remains identify order markov dynamics state space simple ar model continuous case firstorder markov chain discrete case 
zero noise limits interesting models right valuable choices initialization learning linear dynamical systems hmms 
conic linear combination coefficients positive 
mixtures gaussians hidden markov models kind linear blending merely selects jth submatrix tensor discrete state ej 
way recover conventional full unconstrained mixture gaussians hidden markov model emission density cluster state private covariance shape observation noise 
sam roweis zoubin ghahramani appendix appendix review detail inference learning algorithms models 
goal enable readers implement algorithms pseudocode provided 
class model solution inference problem em algorithm learning model parameters 
appendix adopt notation transpose vector matrix written xt denote length time series 
define binary operator element wise product equal size vectors matrices 
comments symbol 
factor analysis spca pca 
inference 
factor analysis related models posterior probability hidden state observations gaussian 
inference problem consists computing mean covariance gaussian cov cc return observation noise matrix assumed diagonal smaller dimension efficiently computed matrix inversion lemma computing log likelihood observation evaluation gaussian cc sensible pca spca algorithm special case factor analysis observation noise assumed spherically symmetric inference spca identical inference factor analysis 
traditional pca algorithm obtained limiting case factor analysis lim inverse computing factor analysis longer defined 
limit defined posterior collapses single point cov 
projection principal components return unifying review linear gaussian models learning 
em algorithm learning parameters factor analyzer factors zero mean data set yn column matrix data point initialize compute sample covariance change log likelihood step nv step set diagonal elements rii ii return obvious interpretation inference function applied entire matrix observations 
depend computed efficiently matrix form 
data appear outer products run factor analysis learning just sample covariance 
note log likelihood computed cc log cc const 
em algorithm spca identical em algorithm factor analysis observation noise covariance spherically symmetrical step changed jj em algorithm pca obtained similar manner initialize change squared reconstruction error step step return sam roweis zoubin ghahramani pca probability model assumes zero noise likelihood undefined convergence assessed monitoring squared reconstruction error 
mixtures gaussians vector quantization 
inference 
discussing inference problem mixtures gaussians discuss inference problem vector quantization limiting case 
hidden variable mixture gaussians discrete variable take values 
represent variable vector length setting hidden variable corresponds value unity dimension zero 
probability distribution discrete hidden variable degrees freedom sum fully described mean 
inference problem limited computing posterior mean data point model parameters prior mean matrix columns means settings observation noise covariance matrix 
compute responsibilities ci ci exp return measure randomness hidden state obtained evaluating entropy discrete distribution corresponding 
standard vq corresponds limiting case lim equal priors inference case performed known nearest neighbor rule 
nearest neighbor ci ci ej arg min return unifying review linear gaussian models ej unit vector jth coordinate direction 
squared distances generalized mahalanobis metric respect matrix nonuniform priors easily incorporated 
case pca posterior distribution zero entropy 
learning 
em algorithm learning parameters mixture gaussian ml soft competitive learning initialize change log likelihood initialize step xi yi xi step cj xij yi cj yi cj return assumed common covariance matrix gaussians extension different covariances gaussian straightforward 
means vector quantization learning algorithm results take appropriate limit algorithm means initialize change squared reconstruction error step initialize xi yi xi sam roweis zoubin ghahramani step return cj linear dynamical systems 
inference 
inference linear dynamical system involves computing posterior distributions hidden state variables sequence observations 
factor analysis hidden state variables assumed gaussian fully described means covariance matrices 
algorithm computing posterior means covariances consists parts forward recursion uses observations yt known kalman filter kalman backward recursion uses observations yt yt rauch 
combined forward backward recursions known kalman rauch tung rts smoother 
describe smoothing algorithm useful define quantities xs respectively mean covariance matrix xt observations ys xt xt vt vt full smoother estimates 
learn matrix em necessary compute covariance time xt xt kalman smoother kalman filter forward pass axt avt kt cv xt xt kt yt cx vt initialize vt av rauch recursions backward pass jt xt jt xt ax vt jt vt vt jt vt av unifying review linear gaussian models return xt vt vt learning 
em algorithm learning linear dynamical system shumway stoffer ghahramani hinton assuming simplicity single sequence observations initialize set change log likelihood step initialize yt xt vt xt vt xt vt step return hidden markov models 
inference 
forward backward algorithm computes posterior probabilities hidden states hmm forms basis inference required em 
standard definitions xt yt yt yt xt sam roweis zoubin ghahramani vectors length case observations yt real valued dimensional vectors probability density observation corresponding state output model assumed single gaussian mean cxt covariance parameters model transition matrix initial state prior probability vector observation mean matrix observation noise matrix tied states forward backward algorithm forward pass bt exp yt ci yt ci bt backward pass bt bt ij tij return definitions equations correspond running algorithm scaling factors factors essential numerical stability algorithm long sequences vanishingly small 
furthermore compute log likelihood sequence log yt log useful function return 
learning 
assume simplicity single sequence observations wish learn parameters hmm 
em algorithm learning parameters known unifying review linear gaussian models baum welch algorithm baum welch initialize change log likelihood step step tij tij ti cj yt cj yt cj return acknowledgments carlos sanjoy mahajan erik fruitful discussions early stages anonymous referees helpful comments geoffrey hinton john hopfield providing outstanding intellectual environments guidance 
supported part center systems engineering part national science foundation engineering research center program eec natural sciences engineering research council canada nserc award 
supported ontario information technology research centre 
amari cichocki yang 

new learning algorithm blind signal separation 
touretzky mozer hasselmo eds advances neural information processing systems pp 

cambridge ma mit press 
attias schreiner 

blind source separation deconvolution dynamic component analysis 
neural computation 
baldi hornik 

neural networks principal components analysis learning examples local minima 
neural networks 
baram roth 

density shaping neural networks application classification estimation forecasting tech 
rep tr cis 
haifa israel center intelligent systems technion israel institute technology 
bauer koller singer 

update rules parameter estimation bayesian networks 
proceedings thirteenth conference uncertainty artificial intelligence uai 
san mateo ca morgan kaufmann 
baum 

inequality associated maximization technique sta sam roweis zoubin ghahramani tistical estimation probabilistic functions markov process 
inequalities 
baum 

inequality applications statistical estimation probabilistic functions markov processes model ecology 
bulletin american mathematical society 
baum petrie 

statistical inference probabilistic functions finite state markov chains 
annals mathematical statistics 
baum petrie weiss 

maximization technique occurring statistical analysis probabilistic functions markov chains 
annals mathematical statistics 
bell sejnowski 

information maximization approach blind separation blind deconvolution 
neural computation 
bishop 

neural networks pattern recognition 
oxford clarendon press 
bishop williams 

gtm principled alternative self organizing map 
neural computation 
comon 

independent component analysis new concept 
signal processing 


remarks filtering semi markov data tech 
rep 
beaulieu france institute de recherche en informatique systems 
dempster laird rubin 

maximum likelihood incomplete data em algorithm discussion 
journal royal statistical society 
digalakis rohlicek ostendorf 

ml estimation stochastic linear system em algorithm application speech recognition 
ieee transactions speech audio processing 
duda hart 

pattern classification scene analysis 
new york wiley 
elliott moore 

hidden markov models estimation control new york springer verlag 
everitt 

latent variable models 
london chapman hill 
fletcher powell 

rapidly convergent descent method minimization 
computing journal 
fraser 

forecasting probability densities hidden markov models mixed states 
weigend gershenfeld eds time series prediction forecasting understanding past pp 

reading ma addison wesley 
geman geman 

stochastic relaxation gibbs distributions bayesian restoration images 
ieee transactions pattern analysis machine intelligence 
ghahramani hinton 

parameter estimation linear dynamical systems tech 
rep crg tr 
toronto department computer science university toronto 
available ftp ftp cs toronto edu pub zoubin 
ghahramani hinton 

switching state space models tech 
rep unifying review linear gaussian models crg tr 
toronto department computer science university toronto 
submitted publication 
ghahramani hinton 

em algorithm mixtures factor analyzers tech 
rep crg tr 
toronto department computer science university toronto 
available ftp ftp cs toronto edu pub zoubin 
ghahramani jordan 

supervised learning incomplete data em approach 
cowan tesauro alspector eds advances neural information processing systems pp 

san mateo ca morgan kaufmann 
goldberg williams bishop 

regression input dependent noise gaussian process treatment 
kearns jordan solla eds advances neural information processing systems pp 

cambridge ma mit press 
goodwin sin 

adaptive filtering prediction control 
englewood cliffs nj prentice hall 
hinton dayan revow 

manifolds images handwritten digits 
ieee transactions neural networks 
hinton ghahramani 

generative models discovering sparse distributed representations 
philosophical transactions royal society 
hinton revow dayan 

recognizing handwritten digits mixtures linear models 
tesauro touretzky leen eds advances neural information processing systems pp 

cambridge ma mit press 
hinton zemel 

minimum description length helmholtz free energy 
cowan tesauro alspector eds advances neural information processing systems pp 

san mateo ca morgan kaufmann 
jordan jacobs 

hierarchical mixtures experts em algorithm 
neural computation 


contributions maximum likelihood factor analysis 

kalman 

new approach linear filtering prediction problems 
trans 
am 
soc 
mech 
eng series journal basic engineering 
kalman bucy 

new results linear filtering prediction theory 
trans 
am 
soc 
mech 
eng series journal basic engineering 
kivinen warmuth 

exponentiated gradient versus gradient descent linear predictors 
journal information computation 
kohonen 

self organized formation topologically correct feature maps 
biological cybernetics 
lauritzen spiegelhalter 

local computations probabilities graphical structures application expert systems 
royal statistical society 
little rubin 

statistical analysis missing data 
new sam roweis zoubin ghahramani york wiley 
ljung 

theory practice recursive identification 
cambridge ma mit press 
lloyd 

square quantization pcm 
ieee transactions information theory 


fixpoint property wold iterative estimation method principal components 
krishnaiah ed multivariate analysis 
new york academic press 
mackay 

maximum likelihood covariant algorithms independent component analysis tech 
rep draft 
cambridge cavendish laboratory university cambridge 
moulines cardoso 

maximum likelihood blind separation deconvolution noisy signals mixture models 
proceedings international conference acoustics speech signal processing vol 
pp 

neal 

probabilistic inference markov chain monte carlo methods tech 
rep crg tr 
toronto department computer science university toronto 
neal hinton 

view em algorithm justifies incremental sparse variants 
jordan ed learning graphical models pp 

dordrecht ma kluwer 
nowlan 

maximum likelihood competitive learning 
lippmann moody touretzky eds advances neural information processing systems pp 

san mateo ca morgan kaufmann 
pearl 

probabilistic reasoning intelligent systems networks plausible inference 
san mateo ca morgan kaufmann 
pearlmutter parra 

maximum likelihood blind source separation context sensitive generalization ica 
mozer jordan petsche eds advances neural information processing systems pp 

cambridge ma mit press 
rabiner juang 

hidden markov models 
ieee assp magazine 
rauch 

solutions linear smoothing problem 
ieee transactions automatic control 
rauch tung 

maximum likelihood estimates linear dynamic systems 
aiaa journal 
roweis 

em algorithms pca spca 
kearns jordan solla eds advances neural information processing systems 
cambridge ma mit press 
tech report cns tr computation neural systems calif institute technology 
rubin thayer 

em algorithms ml factor analysis 
psychometrika 
saul 

modeling acoustic correlations factor analysis 
kearns jordan solla eds advances neural information processing systems pp 

cambridge ma mit press 
shumway stoffer 

approach time series smoothing forecasting em algorithm 
journal time series analysis unifying review linear gaussian models 
shumway stoffer 

dynamic linear models switching 
journal american statistical association 
sirovich 

turbulence dynamics coherent structures 
quarterly applied mathematics 
smyth 

clustering sequences hidden markov models 
tesauro touretzky leen eds advances neural information processing systems pp 

cambridge ma mit press 
smyth heckerman jordan 

probabilistic independence networks hidden markov probability models 
neural computation 
tipping bishop 

mixtures probabilistic principal component analyzers 
neural computation 
tech 
report tr ncrg neural computing research group aston university 
tresp ahmad 

training neural networks deficient data 
cowan tesauro alspector eds advances neural information processing systems pp 

san mateo ca morgan kaufmann 
viterbi 

error bounds convolutional codes asymptotically optimal decoding algorithm 
ieee transactions information theory 
whittaker 

graphical models applied multivariate statistics 
chichester england wiley 
received september accepted april 
