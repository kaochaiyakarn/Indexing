discriminative training gaussian mixtures image object recognition schluter ney lehrstuhl fur informatik vi rwth aachen university technology aachen informatik rwth aachen de 
discriminative training procedure gaussian mixture densities 
conventional maximum likelihood ml training mixtures proved efficient object recognition class treated separately training 
discriminative criteria offer advantage class data aim optimizing class separability 
results postal service usps handwritten digits database compare discriminative results obtained ml training 
compare best results reported groups proving state art 
years gaussian mixture densities image object recognition proved efficient 
known object recognition tasks usps handwritten digits database obtained results comparable superior results reported support vector machines artificial neural nets decision trees 
drawback conventional ml training mixture densities fact class handled separately training 
opposite maximum mutual information criterion mmi optimizes posteriori probabilities training samples class separability 
deal mmi criterion gaussian mixture densities 
results obtained usps database compare results obtained ml training 
improve best ml result error usps show discriminative criteria number model parameters needed achieve results drastically reduced 
discriminative criteria efficient realizing fast classifiers real time environments 
chapters shortly describe usps database experiments feature reduction approach 
chapter gaussian mixtures densities context bayesian decision rule ml training approach 
discriminative training procedure dealt chapter 
drawing chapter results chapter 
postal service database usps database ftp ftp mpg de pub bs data known handwritten digit recognition database 
contains training objects test objects 
characters isolated represented theta pixels sized grayscale image see 
experiments order lose information pixel feature yielding dimensional feature vector 
usps recognition task known hard human error rate testing data 
experiments created fig 

example images taken usps database additional virtual training data shifting image pixel directions 
doing hand get precise estimation model parameters hand obviously incorporate invariance slight translations 
procedure leads training samples train system 
note translated images size theta pixel want guarantee pixel belonging digit gets shifted image 
feature reduction reduce number model parameters estimated transforming data low dimensional feature space advisable 
propose modified subspace method step training data estimate whitening transformation matrix pp 
data transformed called white class conditional covariance matrix sigma gamma kn gamma kn matrix identity number training samples xn observation training sample kn mean vector class kn xn belongs 
second step generate prototype vectors form gamma number classes mean vector class mean vector 
transform vectors orthonormal basis 
avoid numerical instabilities classical approach caused rounding errors done singular value decomposition pp 

yields maximum gamma base vectors dimensionality subspace spanned prototypes shown equal gamma pp 
projecting original feature vectors subspace obtain reduced feature vectors 
proposed method proved robust conventional linear discriminant analysis lda pp gives results compared gamma lda features 
case fewer gamma features wanted lda second step previously reduced features 
features needed advisable create called clustering training data 
case usps database obtained best results creating class resulting feature vectors dimensional 
note created clustering data done algorithms described 
gaussian mixture densities classify observation ir bayesian decision rule pp gamma 
argmax fp xjk priori probability class xjk class conditional probability observation class classifier decision 
parameter represents set parameters class conditional probabilities jk 
xjk known choose models estimate parameters training data 
experiments set class modell xjk gaussian mixture densities 
gaussian mixture defined linear combination gaussian component densities xj ki sigma ki fc ki ki sigma ki xjk ki delta xj ki sigma ki number component densities model class ki weight coefficients ki ki ki mean vector sigma ki covariance matrix component density class avoid problems estimating covariance matrix high dimensional feature space keep number parameters estimated small possible pooled covariance matrices experiments class specific variance pooling estimate single sigma class sigma ki sigma global variance pooling estimate single sigma sigma ki sigma furthermore diagonal covariance matrix variance vector 
mean loss information hand mixture density form arbitrarily precise approximate density function hand covariance matrix previously whitened data known diagonal 
ml parameter estimation done expectation maximization em algorithm combined linde gray clustering procedure 
note global variance pooling maximum approximation em algorithm experiments 
information ml parameter estimation reader referred 
discriminative training assume training data form kn xn observation training sample ng kn corresponding class label kn posteriori probability class observation xn shall denoted kjx 
similarly jk represent class conditional priori probabilities 
priori probabilities supposed see chapter 
maximum mutual information criterion defined expression log jk jk mmi criterion aims maximize sum logarithms posteriori probabilities jx 
maximization mmi criterion defined tries simultaneously maximize class conditional probabilities training samples minimize weighted sum class conditional probabilities competing classes 
mmi criterion optimizes class separability 
mmi reestimation formulae mixture density parameters global variance pooling 
mmi parameter optimization mixture density parameters calculated maximum approximation approximate sums probabilities maximum addend 
performing extended baum welch parameter optimization mmi criterion yields reestimation formulae means ki global diagonal variances oe mixture weights ki gaussian mixture densities details topic reader referred 
note ease representation skip dimension index formulae 
ki gamma ki dc ki ki gamma ki dc ki oe oe ki ki kd gamma ki gamma ki dc ki kd ki ki gamma ki dc ki gamma iteration constant gamma ki gamma discriminative averages functions training observations defined gamma ki ffi ffi kn gamma kjx xn gamma gamma ki ffi kronecker delta training observation xn class kn ffi best fitting component density class ffi kn kn fast reliable convergence mmi criterion choice iteration constant crucial 
exists proof convergence size iteration constant guaranteeing convergence yields impractical small stepsizes slow convergence 
practice fastest convergence obtained iteration constants chosen denominators reestimation equations variances kept positive delta max phi dmin ki fi gamma gamma ki psi dmin max gamma gamma ff gamma gamma ki gamma gamma ki ki ki oe gamma ff ki fi gamma ki gamma gamma ki ki oe gamma ff dmin denotes estimation minimal iteration constant guaranteeing positivity variances iteration factor controls convergence iteration process high values leading low step sizes 
constants fi chosen prevent overflow caused low valued denominators 
experiments parameter initialization done ml training chose fi max gamma ki results chapter results proposed classifier usps database compare results obtained ml approach 
furthermore compare best results obtained state art classifiers support vector machines artificial neural nets decision trees 
experiments dimensionality feature space reduced described chapter yielding feature space dimension 
comparison results obtained ml mmi respectively shown table 
table 
comparison ml mmi iterations results global variance pooling respect total number component densities component ml error rate mmi error rate densities train test train test draw training procedures models parameters 
improvements get smaller number model parameters increasing clear mmi training drastically reduces number parameters needed obtain results 
instance error rate total component densities goes ml mmi relative improvement nearly 
obtain similar error rate ml component densities needed 
discriminative training criteria efficient realizing fast recognizers real time environments 
best results far obtained ml training combined creation virtual test samples 
test sample multiplied shifting directions 
yields instances test sample classified separately 
classifier combination schemes case product rule come final decision original test sample 
basic idea method able classifier combination rules benefits having create multiple classifiers 
simply create virtual test samples 
approach ml error rate goes 
comparison results reported state art methods table 
note considered research groups exactly training test sets 
constraint comparison training classification methods possible 
groups instance improved recognition performance adding machine printed digits training set 
iteration index fig 

mmi convergence behaviour different single densities table 
results reported usps database method error rate human performance decision tree layer neural net layer neural net lenet support vectors invariant support vectors mmi mixtures ml mixtures mmi mixtures product rule ml mixtures product rule discriminative training methods guarantee convergence realistic conditions interesting investigate convergence behaviour 
shows mmi convergence behaviour single densities different choices iteration factor seen choice yields fast unstable convergence 
lead smooth convergence experiments leads significantly faster convergence 
discriminative training criterion gaussian mixture densities image object recognition 
improve best ml result usps database mmi criterion able produce results parameters 
furthermore noted just begun discriminative criteria object recognition 
experience speech recognition raises hope able improve best results near 
instance reestimation formula mixture weights ki known converge slowly 
currently implementing modified reestimation formulae known give better convergence 
includes realizing discriminative criteria minimum classification error criterion 

ney mit levi 
may eds dagm symposium pp stuttgart germany 

simard le cun denker efficient pattern recognition new transformation distance hanson cowan giles eds advances neural information processing systems morgan kaufmann san mateo ca pp 


fukunaga statistical pattern recognition academic press san diego ca 

press teukolsky vetterling flannery numerical recipes university press cambridge 

duda hart pattern classification scene analysis john wiley sons 

dempster laird rubin maximum likelihood incomplete data em algorithm journal royal statistical society pp 


linde und gray algorithm vector quantizer design ieee transactions communications vol 
pp 

normandin maximum mutual information estimation hidden markov models automatic speech speaker recognition 
lee paliwal eds kluwer academic publishers norwell ma pp 

schluter comparison discriminative training criteria proc 
ieee int 
conf 
acoustics speech signal processing seattle washington pp may 

baum inequality applications statistical estimation probabilistic functions markov processes model ecology bulletin american mathematical society vol pp 

kittler combining classifiers ieee transactions pattern analysis machine intelligence vol 
pp 
march 

drucker schapire simard boosting performance neural networks international journal pattern recognition artificial intelligence vol pp 


vapnik nature statistical learning theory springer new york pp 

scholkopf support vector learning oldenbourg verlag munich 

scholkopf simard smola vapnik prior knowledge support vector kernels jordan kearns solla eds advances neural information processing systems mit press pp 

article processed macro package llncs style 
