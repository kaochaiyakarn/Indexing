ieee transactions knowledge data engineering appear final draft guide literature learning probabilistic networks data wray buntine literature review discusses different methods general rubric learning bayesian networks data includes overlapping general probabilistic networks 
connections drawn statistical neural network uncertainty communities different methodological communities bayesian description length classical statistics 
basic concepts learning bayesian networks introduced methods reviewed 
methods discussed learning parameters probabilistic network learning structure learning hidden variables 
presentation avoids formal definitions theorems plentiful literature illustrates key concepts simplified examples 
keywords bayesian networks graphical models hidden variables learning learning structure probabilistic networks knowledge discovery 
probabilistic networks probabilistic graphical models representation variables problem probabilistic relationships 
bayesian networks popular kind probabilistic network different applications including fault diagnosis medical expert systems software debugging 
review learning focus mainly bayesian networks directed graphs 
probabilistic networks increasingly seen convenient high level language structuring confusing equations 
explicit representation dependencies independencies variables ignores specific numeric functional details 
depending interpretation represent causality 
probabilistic networks broad sense independently developed number communities genetics social science statistics factor multi dimensional contingency tables artificial intelligence model probabilistic intelligent systems decision theory model complex decisions 
area considered review graphical modeling social science rich development application strong interactions artificial intelligence statistical communities 
networks general play role high level language seen artificial intelligence statistics lesser degree neural networks biological views offer alternative interpretation 
see survey ripley 
networks build complex models simple components 
networks broader sense include prob current address avenue suite berkeley ca 
email wray com url www com wray graphical models kind considered neural networks decision trees 
probabilistic networks distinguishing characteristic specify probability distribution clear semantics allow processed order diagnosis learning explanation inference tasks necessary intelligent systems 
instance new research area considered briefly section probabilistic network input specification compiler generates learning algorithm 
compilation easier network defines probability distribution 
learning probabilistic networks particular interest earlier artificial intelligence building expert systems involved tedious process manual knowledge acquisition 
tedium spurred developments continued independently machine learning originally focused learning rule systems uncertainty artificial intelligence focused developing coherent probabilistic knowledge structures elicitation suffered pitfalls 
instance henrion cooley give detailed case study heckerman developed similarity networks allow complex network elicited simply expect 
interest artificial intelligence learning probabilistic networks result marriage machine learning uncertainty artificial intelligence 
neural network learning developed concurrently exclusively learning data 
networks computational side neural networks interested information processing opposed biological modeling increasingly moving direction probabilistic models 
overlap learning probabilistic networks neural networks 
statistics general inference techniques developed applied learning probabilistic networks 
computer scientists instance artificial intelligence contributed terms combining scaling techniques generalizing classes representations 
examples variety probabilistic networks applications learning 
learning probabilistic networks includes number complications learning structure parameters structure hidden variables values data values variable missing 
review describes current literature addressing various tasks reviews major ieee transactions knowledge data engineering appear final draft gies applied describes major algorithms 
available software learning bayesian networks discussed review 
extensive list software general inference probabilistic networks maintained world wide web 
list relevant online tutorial articles slides mentioned available 
area considered review empirical evaluation learning algorithms probabilistic networks 
empirical evaluation learning algorithms fraught difficulties 
notwithstanding interesting empirical studies appear :10.1.1.156.9918
ii 
probabilistic networks section introduces bayesian networks general probabilistic networks 
tutorial articles bayesian networks see 
artificial intelligence perspective see 
statistical graphical models general see tutorial see 
bayesian networks bayesian methods learning see 
kinds networks include markov undirected networks markov random fields considered widely image analysis spatial statistics neural networks 
section introduces bayesian networks simple example illustrates richness representation additional examples 
consider bayesian networks discrete variables 
simplest form consist network structure associated conditional probability tables 
example adapted 
structure network structure represented directed acyclic graph dag fig 

network occupation climate age disease symptoms fig 

simple bayesian network definition equivalent functional decomposition joint probability full variable names abbreviated age occ clim disease symptoms age occ clim occ clim turn equivalent set conditional independence statements occ 
age clim 
symptoms 
occ disease table probability tables age age age disease symptoms stomach myocardial stomach pain chest pain bjc reads independent 
take node symptoms example 
node parent disease ancestors age occ clim 
reads assumption symptoms dependent age occupation climate indirectly influence disease 
network substructure definition translates third independence statement 
bayesian networks simplify full joint probability distribution set variables age occ clim disease symptoms show independencies variables 
conditional probability tables parameters conditional probability tables needed specify probability distribution network 
structure fig 
see equation tables age occ clim occ clim need specified 
tables may specified form implicitly parametric probability distribution explicitly tables 
tables age 
notice age real valued variable discretized create binary variable 
symptoms valued discrete variable disease 
assumptions network leads equation smaller tables large joint table variables required 
networks provide way simplifying representation probability distribution 
extensions variables treated simple discrete variables conditional probabilities example simple tables general variety variables functions bayesian networks 
variables real valued integer valued multivariate 
real valued variable may probability density function gaussian 
giving probability table mean variance gaussian functions parent variables 
buntine guide literature learning graphical models data constructions allow bayesian networks represent standard statistical models regression gaussian error log linear models 
furthermore graphical models restricted directed 
undirected arcs problems diagnosis association symptoms represented image analysis associations regions image 
combination directed undirected graphical models developed lauritzen wermuth forms rich representation language 
combinations see 
example richness consider feed forward neural networks 
connections feed forward neural networks fig 
shows transformation feed forward neural network predicting real valued variables probabilistic network 
fig 
shows feed forward network sigmoid sigmoid sigmoid sigmoid gaussian gaussian fig 

feed forward network bayesian network form fig 
shows corresponding probabilistic network bivariate gaussian error distribution grafted output nodes network 
feed forward neural network lower nodes filled indicate input nodes 
bivariate gaussian represented probabilistic network nodes directed arc equivalent representation undirected arc transformation bayesian network needs qualified ways 
notice interior nodes bayesian network labeled transfer function typically feed forward network 
nodes double ovals single ovals 
short hand say variable deterministic function inputs probabilistic function 
neural networks usually weight associated arc giving sense strength association 
probabilistic networks arc indicates form probabilistic dependence correlation weights associated node parameterize functions node 
furthermore probabilistic network explicitly includes measured output variables network neural network includes predicted output variables probabilistic network explicitly represents error function neural network leaves unspecified 
summary bayesian network indicates class fig 

simple clustering model output variables gaussian distribution variables deterministic sigmoid functions hidden variables forth 
sophisticated dynamic networks recurrent neural networks roughly thought flexible non linear extension probabilistic models kalman filters hidden markov models 
networks feed forward neural networks relationship probabilistic networks development 
connections statistics pattern recognition whittaker wermuth lauritzen provide rich set examples modeling statistical hypotheses graphical models mixed graphs incorporating undirected directed networks 
consider clustering style unsupervised learning 
bayesian network drawn clustering algorithm autoclass assumed observed variables independent hidden class 
clustering cases grouped coherent manner 
probabilistic network fig 

suggests way doing 
discrete variable class introduced termed latent hidden variable 
value appears data indicates unknown class case belongs 
advantage construction class value known case probability distribution simple independent needing real valued parameters define 
model called mixture model joint probability mixture data obtained different classes 
visual illustration power mixture models consider real valued variables bivariate gaussian places oval shaped cloud centered point 
mixture bivariate gaussians illustrated fig 
clouds points 
mixture contains classes density quite complex 
popular models pattern recognition speech recognition control kalman filter hidden markov model hmm modeled bayesian networks 
simple hidden markov model fig 

sequence observations phonemes utterance 
indicated shaded nodes observe observe observe shading indicates variables observed 
observations dependent hidden states hidden hidden hidden underlying system 
observations phonemes hidden states may letters underlying word spoken course hidden observer 
kinds models ieee transactions knowledge data engineering appear final draft fig 

data dimensional mixture gaussians hidden observe hidden observe hidden observe fig 

simple hidden markov model 
dynamic sense network set repeated units expanded time instance forecasting 
causal networks useful trick elicitation bayesian networks assume arcs represent causality 
consider network reproduced fig 

imagine environmental variables causing disease turn causes symptoms nice way explaining particular graph expert 
bayesian networks interpretation referred causal networks 
causality fundamental importance science notion intervention 
identifying observed probabilities relating smoking sex lung cancer interesting task real goal study establish act changing smoking habits change susceptibility lung cancer 
kind action external intervention variables 
causal model expected stable acts external intervention drawn valid 
probabilistic interpretation networks review assumption cases got passive observation independently identically distributed examples 
networks represent causality manner networks different interpretation probabilistic networks considered 
causality networks learning covered review 
learning identification causality considered 
table ii sample database relational table case iii 
simple examples basic concepts example learning consider data binary variables data take form table simple example table ii 
rows table give cases different patients 
typically hundreds thousands cases exist relational database 
table ii case variables measured values recorded 
values variable true indicated false indicated variable value 
represents missing value means value variable unknown 
missing values common domains especially variables expensive measure 
hypothesis space example bayesian networks match problem fig 

consider structure fig 

bayesian networks variables denote represents variables independent 
structure probability tables needed 
variables binary probabilities specified real numbers 
denote tables parameter set structure denoted probability tables cjb denoted needed 
parameter set specified value cjb specified values instance jb jb 
consider conditional probability distributions complete network sm probability table xjy subset real space jxj gamma jy dimensions jxj number values variable fully connected network matching table ii variables connected buntine guide literature learning graphical models data real values calculated gamma 
network binary variables needs gamma real values specify conditional probability tables 
realvalued node conditional probability distribution gaussian parents require real values specify mean covariance matrix 
general real values specify conditional probability tables explicitly table implicitly formula referred parameters network 
simple counting argument shows different networks just variables fig 

happens equivalent sense represent equivalent independence statements 
networks different equivalence classes networks variables 
instance consider networks fig 

networks functional decompositions respectively labeled bja cjb bjc ajb bja basic algebra laws conditional probability show bayesian networks equivalent functional decompositions equivalent independence properties bayesian network different 
structures said equivalent probability models 
properties equivalence relation worked general bayesian networks discussed section 
gamma different undirected arcs place network variables means gamma different undirected networks variables 
variables ordered ahead time arc point variable ordering gamma different directed networks 
ordering allowed vary equivalent probability models 
sample likelihood maximum likelihood approach starting point statistical theory introduced 
fix structure sm parameters model matching problem table ii calculate likelihood sample follows case js case probabilities case js calculated probability tables formulation assumes case independent true model sm independently identically distributed 
true model unknown model believed represent process generating data assumed exist purposes modeling reasonable approximation exists 
instance structure fig 
case js ja jb terms right equation corresponding entries probability tables quantity equation called sample likelihood 
maximum likelihood approach fixed structure sm chooses parameters maximize sample likelihood 
important notice structure maximum likelihood calculation 
probability appearing likelihood case function parameters conditional probability table variable parameters bayesian network structure partitioned different parameters node represents parameters conditional probability table variable sample likelihood ja jb notice product separate terms maximum likelihood optimization decomposed maximum likelihood optimization different variable sets individually 
represented sample bja sample cjb show local maximum likelihood problems node 
sample likelihood said decompose bayesian networks deterministic variables missing hidden values undirected arcs :10.1.1.156.9918
decomposition applies network incrementally modified instance search 
parameters describe probability tables binary variables table ii equation corresponds product 
instance pa gamma na counts pa na give occurences respectively data 
case binomial maximum likelihood observed frequency pa na pa likewise variables entries tables 
important common assumption computing sample likelihood complete data assumption 
holds case missing values 
ieee transactions knowledge data engineering appear final draft unrealistic assumption 
instance data comes historical medical database expensive measurements taken recorded considered critical diagnosis 
complete data assumption simplifies calculation sample likelihood network 
instance consider model fig 
consider likelihood case 
suppose variable missing value 
case js ft fg cj ja terms right equation simply corresponding entries probability tables notice summation outside 
summations longer simple closed form solution maximizing sample likelihood 
furthermore optimization problem longer decomposes demonstrated equation 
hidden variables lead problem violate complete data assumption summations appear sample likelihood 
concept central subsequent techniques family statistical distributions known exponential family 
context probabilistic networks appears 
family includes gaussian bernoulli poisson general functional form xj exp lends convenient computational properties including compact storage training sample simple calculation derivatives fitting guaranteed linear size sample 
needs familiar features exponential family order understand developments learning probabilistic models 
properties sample likelihood impact complete data assumption exact solutions maximum likelihood equations forth follow directly standard results exponential family effort usually expended formulating probabilistic network member exponential family standard results exponential family follow 
basic statistical considerations suppose structure sm network discrete gaussian variables fixed 
remains learn parameters probability tables considered earlier data sample likelihood wellbehaved differentiable function parameters 
called parametric problem 
non parametric problem contrast potentially infinite number parameters coherent likelihood function defined un parameterized 
clear literature cases model non parametric manner parametric basis classification trees example 
consider problem learning structures remember finite number 
fixed network structure distinct set parameters 
allowing set different structures parameters full probability density single natural global real valued parameterization different parameterizations depending structure 
problems referred semiparametric qualifications apply 
course clever mathematician coerce full specification network parameters single real number 
artificial construct complex non continuous derivatives 
furthermore structures fig 
probability distributions represented structure set measure zero probability distributions structure set measure zero offering structures valid alternatives set measure zero ignored 
refer combination detail structure neat parametric model structures form nested hierarchies subset measure zero parametric structure problem 
learning network structures data termed model selection problem sense network corresponds distinct model selected data 
non parametric methods model selection active research areas modern statistics 
researchers statistics focused model uncertainty accepted selection single best model exponential sized family models case learning bayesian networks infeasible 
selecting single best model looks subset reasonable models attempting quantify uncertainty 
complexity learning network learning involves choosing possibly exponential number network structures giving values possibly exponential number real values 
problem 
basic results computational learning theory show difficult terms number cases required training time space required optimization 
aspects referred sample complexity computational complexity respectively 
learning roughly distinct phases cases obtained learn small sample medium sample large sample phases 
initially purposes subspace measure zero integrated area relative full space zero 
usually means space lower dimension 
line measure zero finite plane rectangle finite plane non zero measure 
dimensional slice cube measure zero full threedimensional cube 
buntine guide literature learning graphical models data small sample learning corresponds going biases priors 
large sample learning close true model possible high probability close measured reasonable utility criteria mean square error kullback leibler distance 
learning possible reasonable algorithms asymptotically converge truth 
small large sample phase medium sample phase algorithms perform better depending particular biases align true model 
term biases loose sense 
cases obtained learn performance may increase gradually jumps algorithm better approximates truth 
illustrated learning curve fig 
plots error idealized algorithm gains cases represented sample size 
asymptotic error bayes optimal error small sample medium sample large sample fig 

idealized learning curve 
error example approaches bayes optimal error rate 
lower bound error rate achieved algorithm instance predicting coin tosses fair coin bayes optimal error rate 
theory learning curves developed instance 
suppose hypothesis space family probabilistic networks results computational learning theory show conditions transition large sample phase sample size max dim logk sample size sample complexity 
discrete bayesian networks discussed earlier term exponential number variables second term quadratic 
course ignores issue computational complexity 
exponential number networks surprising formulations learning bayesian network np complete problem 
formulations learning viewed maximization problem find network maximizing quality measure 
case sample likelihood scores usually decompose sample likelihood see instance :10.1.1.156.9918
optimization problem find network variables maximizing function form quality quality sample network influences quality measure parents function parents quality measure may log probability log likelihood complexity measure minimized 
measures discussed section viii 
maximization problem instance maximum branchings problem see discussion general allowing quality function nodes np complete variables network restricted parents :10.1.1.156.9918
polynomial variable parent 
variation problem discussed find best networks terms quality measure :10.1.1.156.9918
bayesian networks search problem confounded existence equivalent networks 
experience existing systems shows standard search algorithms greedy algorithms iterated local search algorithms perform 
basic greedy search explored 
furthermore search problem adapts nicely branch bound standard methods information theory provide bounds savings exhaustive search appear orders magnitude 
iv 
parameter fitting fixed graphical structure sm parameter fitting problem learn parameters data 
mathematics fitting parameters bayesian markov network extension standard fitting procedures statistics 
fitting algorithms exist bayesian networks general probabilistic networks cases complete missing data 
see whittaker extensive discussion review methods theory 
case bayesian network complete data distributions nodes discrete probability tables gaussians fast close form solutions exist computed time proportional size data set 
example consider fitting model fig 
data table 
probabilities oe model occurs sample likelihood form oe gamma oe maximum oe maximum likelihood solution parameters equal observed frequency relevant probabilities cases variety iterative algorithms exist fast closed form solutions subroutine 
common techniques shall explain expectation maximization em algorithm ieee transactions knowledge data engineering appear final draft iterative proportional fitting ipf algorithm 
exponential family important 
maximum likelihood approaches suffer called sparse data instance may undefined table counts total zero 
consider model fig 
consider estimating jc 
notice instances sample maximum likelihood estimate probability undefined sample likelihood exist 
binary variables fully connected bayesian network variables directly connected clearly need greater gamma cases sample maximum likelihood estimate defined 
related problem problem fitting 
suppose sparse data problem 
observe maximum likelihood estimate 
equal data observed cases variable value cases 
reasonable true value chance data 
estimate upper bound probability 
definition maximum likelihood value estimate true sample likelihood 
sample size gets larger larger estimate gradually converge true value assured cases large sample properties maximum likelihood theory see 
small samples maximum likelihood value may larger true likelihood general maximum likelihood solution attempt fit data possible instance regression degree polynomials fit data points exactly data points reasonably attempt fit degree polynomial assume remaining lack fit due noise data 
maximum likelihood parameter values said fit data 
known problem supervised learning instance addressed pruning methods classification trees 
bayesian maximum posterior map approach extends maximum likelihood approach introducing prior probability 
introductions simplified bayesian approach extensions 
approach places probability distribution unknown parameters reasons axioms probability theory 
likelihood augmented prior gives initial belief seeing data 
consider just column data table ii consider parameter giving probability bayes theorem sample numerator contains sample likelihood prior denominator obtained integrating numerator sample computations simplified cases exponential family mentioned previously gaussians bernoulli forth 
example fig 

fig 

priors likelihoods posteriors left graph shows different priors 
priors beta distributions parameters ff marked plot 
second prior ff mild preference prior agnostic 
middle graph shows likelihoods different samples counts sample size right graph shows resulting posterior theta posteriors resulting 
cluster peaks top posteriors prior ff 
notice agnostic prior ff influenced likelihood posterior peaks mild prior reflect shape prior quite strongly 
maximum posterior value value maximum curve 
notice effected prior likelihood 
general algorithms exist addressing parameter fitting problems probabilistic networks missing latent variables large samples recursive incremental techniques special nodes subjective priors table iii lists major techniques application 
introductions new extensions examples means thorough list area 
common versions em ipf algorithms mean field theory exponential family generalizations exist 
conjunction methods large number optimization techniques finding map computing various quantities laplace approximation 
optimization techniques specific parameter fitting learning 
includes fisher scoring method approximate newton raphson algorithm stochastic optimization computes gradients subsamples individual cases time 
variations method popular neural networks having feature early methods proven yield computational buntine guide literature learning graphical models data algorithm problems refs 
map general laplace nd order approx 
em missing hidden values ipf undirected network mean field approximate moments gibbs approximate moments mcmc approximate moments table iii general algorithms parameter fitting savings studies 
extension parameter fitting handle sequential line learning missing data described 
uses bayesian methods overcome problems sparse data defining dirichlet prior entries probability tables 
full implementation described 
extensions gaussians popular nodes types bayesian network 
combined structure elicitation techniques parameter fitting prove powerful applications instance dynamic models medical domain 
structure identification methods ignoring issue sample size moment difficult question particular network structures latent variables identifiable limit probability 
assuming large amounts data accurately estimate various probabilities true probabilistic network reconstructed sense learning algorithm sufficiently large sample invariably return hypothesis graphical structure parameters close truth 
question formalized addressed angles computational learning theory name identification learnability statistics name consistency 
situation fig 

bayesian networks question confounded existence equivalence classes graphs example redundant model hidden latent variables 
instance consider networks fig 

bayesian networks equivalent probability models bayesian network different 
bayesian networks equivalent sample likelihoods distinguished data additional criteria knowledge bayesian network identified data 
theoretical tool analyze identifiability equivalence graphical models latent variables involving causality variables manipulated 
thorough treatment issues equivalence latent variables causality appears 
cases class equivalent graphs reconstructed data cases latent variables properties identified uniquely 
identification methods lead earliest algorithms learning structure data related approach combines cross validation address model selection 
identification methods tetrad ii successor tetrad 
theory network identification data network equivalence precursor techniques learning medium sized samples fig 

network equivalence important concept bayesian techniques learning bayesian networks data advanced priors bayesian networks :10.1.1.156.9918
discussed 
vi 
diagnostics elicitation assessment day day practice learning data analysis may learning algorithm core lot involves modeling assessment building model trying find going data expert opinions 
relevant learning comes statisticians generally experience decision analysts methods constructing systems working experts 
basic problem elicitation twist problem knowledge acquisition expert systems 
ffl medium sample regime applies frequently data complemented prior knowledge constraints reliable useful results obtained 
ffl prior knowledge obtained domain experts manual process knowledge elicitation 
ffl domain experts poor judging limitations capabilities estimating probabilities 
common mistakes beginners assume expert claims valid 
applications issues crucial learning problem come neat wrapper instructions assembly data variables try tree program 
learning problem usually embedded larger problem 
domain expert may needed just circumscribe learning component variables predicted forth 
crucial success learning algorithm incidental 
number techniques exist interface learning knowledge acquisition 
diagnostics measures evaluate particular model assumptions 
sensitivity analysis measures sensitivity results study model assumptions techniques taught engineers wiggle inputs model case learning means constraints priors watch output ieee transactions knowledge data engineering appear final draft model 
assessment elicitation usual process discussed manual knowledge acquisition interviewing expert order obtain prior estimates relevant quantities 
elicitation evaluation probabilistic networks developed area refinement networks learning possible discussed priors 
vii 
learning structure data earliest result structure learning chow liu algorithm learning trees data 
algorithm learns bayesian network shape tree 
variables trees exponential number bayesian networks 
sample complexity log sample complexity tree learning feasible small samples 
furthermore computational complexity searching tree shaped network requires quadratic number network evaluations 
herskovits cooper demonstrated problem significant size complex structure learning possible quite reasonable sample sizes case despite faced potentially exponential sample complexity np complete search problem 
early structure learning identification results discussed previous section instance 
problems learning structure bayesian network suffer samples smaller 
happens fitting structure space similar overfitting parameter space discussed previously 
maximum likelihood hypothesis testing methods provide techniques comparing structure shall add arc 
model better model done instance likelihood ratio test 
repeated test lead problems chance hypothesis tests confidence level fail times hundreds tests may need learning network structure data 
comparable problem statistics literature variable subset selection regression 
problem seeks find subset variables base linear regression 
pitfalls hypothesis testing context discussed 
basic problem model selection focuses choosing single best model 
discrete variables problem learning bayesian networks complete data related problem learning classification trees exemplified cart algorithm statistics id artificial intelligence 
relationship holds sample likelihood binary classification tree represented product independent binomial distributions just sample likelihood bayesian networks binary variables described section iii 
problems similar parametric structure 
classification tree problem long history studied perspective applied statistics artificial intelligence bayesian statistics minimum description length mdl genetic algorithms computational learning theory 
adaptation successful tree algorithm algorithm learning bayesian networks appears relationship approaches discussed :10.1.1.52.1068
adaptation quite direct constructor algorithm adapts technique cart algorithm trees 
variety heuristic techniques developed trees including handling missing values discretization real valued attributes find way algorithms probabilistic networks 
viii 
statistical methodology learning structure researchers applied standard statistical methodology fitting models handling fitting 
appropriate discuss standard methodologies done section 
problem fitting encountered addressed earliest methods 
important note role statistical methodology convert learning problem optimization problem 
statistical methodologies despite wide philosophical differences reduce learning problem kind optimization problem practitioner left wondering differences 
important note structure learning built form parameter learning sub problem 
general different structure learning methods extensions general algorithms summarized table iii 
cases simple placing model selection wrapper parameter fitting system cases sophistication layered top 
unfortunate different competing statistical methodologies exist address essentially problem 
partly stems apparent impossibility handling smaller sample learning problems objective manner difficulty establishing basis statistical methodology judged 
see instance efforts compare different learning algorithms consider statistical methodology higher level abstraction learning algorithm 
discussion bayesian perspective issues learning appears touching prior probabilities subjective statistical analysis 
different disciplines addressed problems parallel attempted extend classical maximum likelihood hypothesis testing approaches statistics 
methodology comes cast standard claims dogma paradoxes counter claims 
useful familiar different approaches mappings approximations better understand differences difficult confusing state literature 
methodology particular strengths suitable certain conditions ease implementation adequate large samples buntine guide literature learning graphical models data appropriate engineer availability software training 
forth 
believe methodology superior respects 
comments review colored bayesian perspective 
tried keep comments realm generally believed knowledgeable area merely repeating dogma community 
section methodologies 
include appropriate tutorial 
really hundreds different methodologies small cluster researchers 
list presents different corners continuum 
maximum likelihood minimum cross entropy methods maximum likelihood approach says find network structure sm maximum likelihood parameters largest argmax sm max sm minimum cross entropy approach says find structure minimum cross entropy data smallest 
approaches equivalent known suffer fitting discussed section iv 
true model single equivalent representative hypothesis space maximum likelihood approach consistent sense limit large sample converge truth 
maximum likelihood method viewed simplification approaches important starting point 
large sample regime best strategy maximum likelihood approach avoid mathematical implementation details complex approaches 
results computational learning theory bounding onset large sample phase useful deciding 
bayesian networks maximum likelihood approach applied 
herskovits cooper major breakthrough learning bayesian networks 
clear mdl bayesian methods extend maximum likelihood approach applied detail 
hypothesis testing approaches hypothesis testing standard model selection strategy classical statistics 
probabilistic networks methods developed variety statistical software exists 
mentioned problem viable approach small number hypotheses tested 
clever greedy search techniques help reducing number hypothesis tests required 
way thinking deal multiple hypotheses hypothesis testing return set possible models expecting isolate single 
strategy resembles bayesian approach multiple models considered 
discussed context probabilistic networks 
extended likelihood approaches number extensions maximum likelihood approach proposed overcome problem fitting overcome problems inherent hypothesis testing 
approaches replace sample likelihood modified score maximized 
examples include penalized likelihood akaike information criteria aic bayesian information criteria bic 
typically involves minimizing formula bic formula bic sm gamma log sm dim log maximum likelihood estimate fixing structure sm sample size dim dimensionality 
bic criteria related variations bayesian avoid specification prior similar variations minimum information complexity approaches described 
examples undirected probabilistic networks bic criteria appear 
minimum information complexity approaches different schools general rubric minimizing information complexity measure code length instance minimum description length mdl minimum message length minimum complexity 
simple approximation mdl equivalent bic variations involve statistical quantities fisher information hypothesis dependent complexity measures chosen particularly domain 
approaches popular engineers computer scientists learn coding information theory undergraduates 
perspective methods related bayesian map methods subtle differences 
advantage proponents claim approach particularly mdl school requires prior objective 
instances corresponding implicit prior constructed code 
authors approach bayesian methods disguise anti bayesian colleagues 
search bounds instance area information complexity approach takes advantage techniques developed information theory 
suzuki developed branch bound technique learning bayesian networks information theoretic bounds 
bayesian networks mdl applied 
resampling approaches modern statistics developed variety resampling schemes addressing fitting parametric situations ieee transactions knowledge data engineering appear final draft learning networks 
resampling refers fact pseudo samples created original sample 
popular approach cross validation applied 
resampling schemes great success applied multivariate statistics see instance tutorial 
strength lies fact reliable black box method requiring complex mathematical treatment bayesian minimum complexity methods 
resampling schemes provide benchmark comparison complex schemes additional mathematical implementation pitfalls 
theoretical justification large sample empirical successes small sample case wide range problems 
bayesian approaches rich variety bayesian methods depending approximations shortcuts previous methodologies reproduced form bayesian approximation 
full form bayesian approach requires specification prior probability tutorial list see 
general bayesian methods learning bayesian networks 
advanced introductions reviews bayesian methods learning 
bayesian approach different approximations 
simplest map approach seeks find structure sm maximizing log probability log sm sample log sm log term called evidence differs likelihood 
evidence average sample likelihood likelihood earlier techniques js relative value calculated sm sample sample base structure called bayes factor variety techniques approximations exist computing 
basic technique bayesian learning bayesian network structures complete data uses standard bayesian methods worked form :10.1.1.52.1068:10.1.1.156.9918
certainly techniques standard bayesian manipulations obvious students bayesian theory 
general case exponential family worked 
summaries line thesis covering issues :10.1.1.156.9918
full bayesian approach predictive returning single best network aim perform prediction estimate probabilities new cases 
instance interested probability new cases sample new 
general estimated averaging predictions possible networks probability identity new sm new sample situation represented fig 

approaches sample gibbs sampler new data sample new data new data sample fig 

averaging multiple bayesian networks matches intuition different networks quite reasonable hedge bets combine 
practice full summation possible approximations 
bayesian methods learning probabilistic networks general sense :10.1.1.52.1068:10.1.1.52.1068
computational aspects finding best networks discussed :10.1.1.156.9918
related concern combine posterior network probabilities efficiently compute conditional posterior probabilities 
general bayesian algorithm family inference applies context parameter fitting structure learning markov chain monte carlo mcmc family algorithms 
extensive review 
family uses kind trick 
suppose wish sample distribution 
general complex distribution convenient sampling algorithm may known 
complete data assumption violated instance discussed section iii quite easy get sample likelihood distribution network parameters posterior distribution network parameters may convenient functional form sample exactly kind problem mcmc methods designed 
instance estimate posterior predictions learning complex parametric systems sigmoidal feed forward neural networks 
sample gibbs sampler simplest kind mcmc method start repeatedly re sample variable turn current conditional distribution read buntine guide literature learning graphical models data sampled ajb bja cja ajb bja probabilistic networks ideal framework developing mcmc methods conditional distributions generated automatically network 
mcmc methods parameter fitting sample different network parameters structure learning sample different possible probabilistic network structures 
mcmc methods learning probabilistic networks discussed 
madigan raftery refer mcmc methods averaging multiple probabilistic networks full predictive approach markov chain monte carlo model composition mc 
key distinction bayesian non bayesian methods priors 
priors unfortunately complex mathematically poorly chosen priors bayesian method perform poorly methods real danger case bayesian networks semi parametric nature 
informative priors noninformative priors :10.1.1.52.1068:10.1.1.156.9918
fundamental assumption equivalent network structures equivalent priors parameters :10.1.1.52.1068:10.1.1.156.9918
instance consider structures fig 

prior probability js virtue equivalence converted prior change variables jacobian transformation js js det notice prior constructed prior necessarily equal prior js 
assumption prior equivalence sets priors equal applicable network causal interpretation 
gives set functional equations prior satisfy 
basic theory properties priors bayesian networks discussed extending techniques :10.1.1.156.9918
ability variety informative subjective priors bayesian networks strengths 
informative priors include constraints preferences structure network preferences probabilities expert generate imaginary data :10.1.1.52.1068:10.1.1.156.9918
example language chain graphs extension bayesian networks 
potential bayesian networks basis knowledge refinement suggested applications offers integrated approach development maintenance intelligent systems long considered potential fruits artificial intelligence :10.1.1.52.1068:10.1.1.156.9918
ix 
learning structure exact algorithm handling incomplete data missing values 
problems involved exact methods previously explained 
impractical larger problems serve tool benchmark non trivial sized problems approximate algorithms exist instance mentioned table iii 
simple clustering algorithms learn bayesian networks single latent hidden variable root network 
kinds problems addressed limited sense years ai statistics community 
bayesian method 
likewise 
missing values handled known em algorithm accurately gibbs sampling 
versions clustering algorithms search possible structures 
algorithms fit neatly categories 
learning markov undirected networks data related early boltzmann machine neural networks 
earlier bayesian methods require input strict ordering variables identification algorithms require :10.1.1.52.1068
thought combination bayesian identification algorithms 
bayesian methods equivalent things large sample case independence tests identification algorithms strict ordering entirely necessary bayesian algorithms :10.1.1.156.9918
variety hybrid algorithms exist provide rich source ideas development 
constructing learning software variety network structures latent variables different parametric nodes logistic poisson forms bugs program generate gibbs samplers automatically 
effectively allows data analysis algorithms compiled specifications probabilistic network technique addresses number non trivial data analysis problems 
unfortunately gibbs sampling thought domain specific optimization time intensive convergence may slow methods need developed approach widely applicable 
algorithm schemas table iii applied compilation framework may possible construct efficient algorithms automatically 
exposition techniques algorithms learning bayesian networks decomposition exact bayes factors differentiation readily automated 
heckerman mamdani wellman real world applications bayesian networks communications acm vol 

ieee transactions knowledge data engineering appear final draft verma pearl equivalence synthesis causal models bonissone 
spirtes glymour scheines causation prediction search springer verlag new york 
heckerman shachter definition graphical representation causality besnard hanks 
pearl graphical models causality intervention statistical science vol 
pp 

lauritzen spiegelhalter local computations probabilities graphical structures application expert systems discussion journal royal statistical society vol 
pp 

wright correlation causation journal agricultural research vol 
pp 

pearl probabilistic reasoning intelligent systems morgan kaufmann 
howard matheson influence diagrams principles applications decision analysis howard matheson eds 
strategic decisions group menlo park ca 
glymour scheines spirtes kelly discovering causal structure morgan academic press san diego ca 
mishra shenoy attitude formation models insights tetrad cheeseman pp 

scheines inferring causal structure unmeasured variables cheeseman pp 

ripley network methods statistics probability statistics optimization kelly ed pp 

wiley sons new york 
hertz krogh palmer theory neural computation addison wesley 
quinlan programs machine learning morgan kaufmann 
hayes roth waterman lenat eds building expert systems addison wesley 
michie current developments expert systems applications expert systems quinlan ed 
addison wesley london 
quinlan compton horn lazarus inductive knowledge acquisition case study applications expert systems quinlan ed 
addison wesley london 
henrion cooley experimental comparison knowledge engineering expert systems decision analysis sixth national conference artificial intelligence seattle american association artificial intelligence pp 

heckerman probabilistic similarity networks networks vol 
pp 

neal connectionist learning belief networks artificial intelligence vol 
pp 

saul jaakkola jordan mean field theory sigmoid belief networks technical report computational cognitive science mit 
buntine operations learning graphical models journal artificial intelligence research vol 
pp 
jair mirrored sites including url www cs washington edu research jair home html 
tanner tools statistical inference springer verlag new york second edition 
kass raftery bayes factors model uncertainty journal american statistical association vol 
pp 

bernardo smith bayesian theory john wiley chichester 
buntine graphical models discovering knowledge advances knowledge discovery data mining fayyad piatetsky shapiro smyth eds 
mit press 
software belief networks world wide web site url bayes stat washington edu belief html 
association uncertainty artificial intelligence home page berkeley world wide web site url www org 
michie spiegelhalter taylor eds machine learning neural statistical classification ellis hertfordshire england 
lauritzen thiesson spiegelhalter diagnostic systems created model selection methods case study cheeseman pp 

remco bouckaert properties bayesian belief network learning algorithms de mantaras poole 
singh algorithm construction bayesian network structures data heckerman mamdani pp 

aliferis cooper evaluation algorithm inductive learning bayesian belief networks simulated data sets de mantaras poole pp 

cooper herskovits bayesian method induction probabilistic networks data report smi section medical informatics university pittsburgh january 
bouckaert bayesian belief networks inference construction phd thesis wiskunde en informatica utrecht universiteit june 
heckerman geiger chickering learning bayesian networks combination knowledge statistical data technical report msr tr revised microsoft research advanced technology division july appear machine learning journal :10.1.1.156.9918
bo thiesson block recursive models induced relevant knowledge observations statistical techniques computational statistics data analysis vol 
pp 

shachter heckerman thinking backwards knowledge acquisition ai magazine vol 

fall pp 

charniak bayesian networks tears ai magazine vol 
pp 

henrion breese horvitz decision analysis expert systems ai magazine vol 
pp 

whittaker graphical models applied multivariate statistics wiley 
edwards graphical modelling springerverlag 
heckerman bayesian networks knowledge representation learning advances knowledge discovery data mining fayyad piatetsky shapiro smyth eds 
mit press extended version available msr tr microsoft research advanced technology division 
ripley spatial statistics wiley new york 
lauritzen dawid larsen 
independence properties directed markov fields networks vol 
pp 

lauritzen wermuth graphical models associations variables qualitative quantitative annals statistics vol 
pp 

buntine chain graphs learning besnard hanks 
tino horne giles finite state machines recurrent neural networks automata dynamical systems approaches technical report institute advanced computer studies university maryland published progress neural networks special volume temporal dynamics timevarying pattern recognition eds ablex publishing 
wermuth lauritzen substantive research hypotheses conditional independence graphs graphical chain models journal royal statistical society vol 

hanson stutz cheeseman bayesian classification correlation inheritance ijcai 
dean wellman planning control morgan kaufmann san mateo california 
poland decision analysis continuous discrete variables mixture distribution approach phd thesis department engineering economic systems stanford university stanford ca 
buntine guide literature learning graphical models data dagum horvitz uncertain reasoning forecasting international journal forecasting submitted 
pearl causal diagrams empirical research technical report cognitive systems laboratory computer science department university california los angeles appear biometrika 
pearl verma theory inferred causation principles knowledge representation reasoning proceedings second international conference allen fikes sandewall eds pp 

morgan kaufmann san mateo ca 
pearl identification nonparametric structural equations technical report cognitive systems laboratory computer science department university california los angeles march 
heckerman bayesian approach learning causal networks besnard hanks 
spirtes meek richardson causal inference presence latent variables selection bias besnard hanks pp 

dawid lauritzen hyper markov laws statistical analysis decomposable graphical models annals statistics vol 
pp 

lam bacchus causal information local measures learn bayesian networks heckerman mamdani pp 

heckerman geiger learning bayesian networks unification discrete gaussian domains besnard hanks 
barndorff nielsen information exponential families statistical theory john wiley sons new york 
breiman friedman olshen stone classification regression trees wadsworth belmont 
model selection wiley 
small sample large sample statistical model selection criteria cheeseman pp 

raftery bayesian model selection social research discussion gelman rubin hauser sociological methodology marsden ed 
cambridge mass 
madigan raftery model selection accounting model uncertainty graphical models occam window journal american statistical association vol 
pp 

haussler kearns seung tishby rigorous learning curve bounds statistical mechanics proceedings seventh acm conference computational learning theory warmuth ed 
pp 
morgan kaufmann 
haussler decision theoretic generalizations pac model neural net learning applications information control vol 
pp 
sept 
chickering learning bayesian networks np complete submitted proceedings ai statistics 

learning robust learning product distributions research report nr 
revised may fachbereich informatik universitat dortmund 
suzuki efficient mdl learning procedure branch bound technique technical report comp institute electronics information communication engineers 
edwards hierarchical interaction models journal royal statistical society vol 

effective implementation iterative proportional fitting procedure computational statistics data analysis vol 
pp 

lauritzen em algorithm graphical association models missing data computational statistics data analysis vol 
pp 

dempster laird rubin maximum likelihood incomplete data em algorithm journal royal statistical society vol 
pp 

casella berger statistical inference wadsworth brooks cole belmont ca 
heckerman tutorial learning bayesian networks technical report msr tr microsoft research advanced technology division march 
howard decision analysis perspectives inference decision experimentation proceedings ieee vol 

catlett russell decision theoretic subsampling induction large databases machine learning proc 
tenth international conference amherst massachusetts morgan kaufmann 
azevedo shachter laplace method approximations probabilistic inference belief networks continuous variables de mantaras poole pp 

bo thiesson accelerated quantification bayesian networks incomplete data proceedings international conference knowledge discovery data mining fayyad uthurusamy eds appear 
ghahramani factorial learning em algorithm advances neural information processing systems nips tesauro touretzky leen eds 
morgan kaufmann 
york madigan markov chain monte carlo methods hierarchical bayesian expert systems cheeseman pp 

gilks thomas spiegelhalter language program complex bayesian modelling statistician vol 
pp 

neal probabilistic inference markov chain monte carlo methods technical report crg tr dept computer science university toronto 
radford neal bayesian learning neural networks phd thesis university toronto graduate department computer science october available ftp ftp cs toronto edu pub radford thesis ps mccullagh nelder generalized linear models chapman hall london second edition 
robbins munro stochastic annals mathematical statistics vol 
pp 

mller efficient training feed forward neural networks phd thesis aarhus university aarhus denmark 
david rumelhart james mcclelland pdp research group eds parallel distributed processing mit press 
spiegelhalter lauritzen sequential updating conditional probabilities directed graphical structures networks vol 
pp 

olesen lauritzen jensen systems creating adaptive causal probabilistic networks dubois pp 

parameter adjustment bayesian networks 
generalized noisy gate heckerman mamdani pp 

provan tradeoffs constructing evaluating temporal influence diagrams heckerman mamdani pp 

ben david learning limit non uniform ffl ffi learning proceedings sixth acm workshop computational learning theory pitt ed 
pp 
morgan kaufmann 
spirtes verma equivalence causal models latent variables report cmu phil philosophy carnegie mellon university 
verma pearl algorithm deciding set observed independencies causal explanation dubois 
chain graph markov property scandinavian journal statistics vol 
pp 

geiger verma pearl identifying independence bayesian networks networks vol 
pp 

andersson madigan perlman markov equivalence chain graphs undirected graphs acyclic digraphs technical report department statistics university washington seattle wa december 
spirtes glymour algorithm fast recovery sparse causal graphs social science computing reviews vol 
pp 

fung crawford system induction probabilistic models eighth national conference artificial ieee transactions knowledge data engineering appear final draft intelligence boston massachusetts american association artificial intelligence pp 

geiger heckerman characterization dirichlet distribution application learning bayesian networks besnard hanks 
winkler quantification judgment methodological suggestions siam journal computing vol 
pp 

spiegelhalter bull bull assessment criticism improvement imprecise subjective probabilities medical expert system henrion pp 

morgan henrion uncertainty guide dealing uncertainty quantitative risk policy analysis cambridge university press 
kahneman tversky judgement uncertainty heuristics biases cambridge university press cambridge 
langley simon applications machine learning rule induction cacm appear 
spiegelhalter dawid lauritzen cowell bayesian analysis expert systems statistical science vol 
pp 

spiegelhalter cowell learning probabilistic expert systems bernardo pp 

cowell dawid spiegelhalter sequential model criticism probabilistic expert systems ieee transactions pattern analysis machine intelligence vol 
pp 

laskey sensitivity analysis probability assessments bayesian networks heckerman mamdani pp 

chow liu approximating discrete probability distributions dependence trees ieee transactions information theory vol 
pp 

herskovits cooper entropy driven system construction probabilistic expert systems databases bonissone pp 

srinivas russell agogino automated construction sparse bayesian networks henrion pp 

buntine learning classification trees hand pp 

rissanen stochastic complexity statistical enquiry world scientific 
wallace patrick coding decision trees machine learning vol 
pp 

buntine theory refinement bayesian networks uncertainty artificial intelligence proceedings seventh conference ambrosio smets bonissone eds los angeles ca :10.1.1.52.1068
buntine classifiers theoretical empirical study ijcai 
quinlan unknown attribute values induction proceedings sixth international machine learning workshop ed cornell new york morgan kaufmann 
fayyad irani multi valued interval discretization continuous valued attributes classification learning international joint conference artificial intelligence chambery france ijcai pp 
morgan kaufmann 
ron kohavi george john richard long david manley karl pfleger mlc machine learning library tools artificial intelligence 
pp 
ieee computer society press available anonymous ftp stanford edu pub ronnyk mlc ps 
kullback information theory statistics john wiley sons new york 
geiger entropy learning algorithm bayesian conditional trees dubois pp 

edwards fast model selection procedure large families models journal american statistical association vol 
pp 

poland shachter approaches probability model selection de mantaras poole pp 

rissanen stochastic complexity journal royal statistical society vol 
pp 

wallace freeman estimation inference compact encoding journal royal statistical society vol 
pp 

barron cover minimum complexity density estimation ieee transactions information theory vol 

oliver baxter mml bayesianism similarities differences technical report monash university melbourne 
smyth admissible stochastic complexity models classification problems hand pp 

lam bacchus learning bayesian belief networks approach mdl principle computational intelligence vol 

suzuki construction bayesian networks databases mdl scheme heckerman mamdani pp 

efron tibshirani statistical data analysis computer age science vol 
pp 

ron kohavi study cross validation bootstrap accuracy estimation model selection international joint conference artificial intelligence montreal ijcai morgan kaufmann 
buntine prior probabilities tutorial slides available url www com wray 
cooper herskovits bayesian method induction probabilistic networks data machine learning vol 
pp 

shachter eddy influence diagram approach medical technology assessment influence diagrams belief nets decision analysis oliver smith eds pp 

wiley 
learning probabilistic expert systems workshop probabilistic expert systems ed roma october pp 

york bayesian methods analysis misclassified incomplete multivariate discrete data phd thesis university washington seattle wa 
madigan york bayesian graphical models discrete data technical report department statistics university washington seattle wa november submitted international statistical review 
madigan raftery york bradshaw strategies graphical model selection cheeseman pp 

madigan raftery eliciting prior information enhance predictive performance bayesian graphical models communications statistics appear 
york madigan lie estimation proportion double sampling incorporating covariates accounting model uncertainty applied statistics vol 
pp 

minimal assumption distribution propogation belief networks heckerman mamdani pp 

ripley stochastic simulation john wiley sons 
heckerman geiger chickering learning bayesian networks combination knowledge statistical data de mantaras poole 
cooper method learning belief networks contain hidden variables journal intelligent information systems appear 
proceedings workshop knowledge discovery databases 
titterington smith makov statistical analysis finite mixture distributions john wiley sons chichester 
cheeseman self kelly taylor freeman stutz bayesian classification seventh national conference artificial intelligence saint paul minnesota american association artificial intelligence pp 

thomas spiegelhalter gilks bugs program perform bayesian inference gibbs sampling bernardo pp 

gilks clayton spiegelhalter best mcneil kirby modelling complexity buntine guide literature learning graphical models data applications gibbs sampling medicine journal royal statistical society vol 
pp 

buntine networks learning th session international statistical institute beijing china invited lecture 
bonissone ed proceedings sixth conference uncertainty artificial intelligence cambridge massachusetts 
besnard hanks eds uncertainty artificial intelligence proceedings eleventh conference montreal canada 
cheeseman eds selecting models data artificial intelligence statistics iv springer verlag 
lopez de mantaras poole eds uncertainty artificial intelligence proceedings tenth conference seattle wa 
heckerman mamdani eds uncertainty artificial intelligence proceedings ninth conference washington dc 
ijcai ed international joint conference artificial intelligence sydney 
morgan kaufmann 
dubois wellman ambrosio smets eds uncertainty artificial intelligence proceedings conference stanford ca 
henrion shachter kanal lemmer eds uncertainty artificial intelligence elsevier science publishers amsterdam 
bernardo berger dawid smith eds bayesian statistics oxford university press 
hand ed artificial intelligence frontiers statistics chapman hall london 
wray buntine received sc 
degree mathematics university queensland ph degree computer science university technology sydney 
currently senior scientist undertaking consulting exploratory data analysis software development 
part survey written principle investigator research institute advanced computing science nasa ames research center 
research interests include experimental theoretical methods machine learning statistics neural networks knowledge discovery 
wrote decision tree learning package ind currently writing compiler probabilistic networks 
