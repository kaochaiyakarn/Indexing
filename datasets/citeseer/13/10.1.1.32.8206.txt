journal artificial intelligence research submitted published hierarchical reinforcement learning maxq value function decomposition thomas dietterich tgd cs orst edu department computer science oregon state university corvallis presents new approach hierarchical reinforcement learning decomposing target markov decision process mdp hierarchy smaller mdps decomposing value function target mdp additive combination value functions smaller mdps 
decomposition known maxq decomposition procedural semantics subroutine hierarchy declarative semantics representation value function hierarchical policy 
maxq unifies extends previous hierarchical reinforcement learning singh kaelbling dayan hinton 
assumption programmer identify useful subgoals define subtasks achieve subgoals 
defining subgoals programmer constrains set policies need considered reinforcement learning 
maxq value function decomposition represent value function policy consistent hierarchy 
decomposition creates opportunities exploit state abstractions individual mdps hierarchy ignore large parts state space 
important practical application method 
defines maxq hierarchy proves formal results representational power establishes conditions safe state abstractions 
presents online model free learning algorithm maxq proves converges probability kind locally optimal policy known recursively optimal policy presence kinds state abstraction 
evaluates maxq representation maxq series experiments domains shows experimentally maxq state abstractions converges recursively optimal policy faster flat learning 
fact maxq learns representation value function important benefit possible compute execute improved non hierarchical policy procedure similar policy improvement step policy iteration 
demonstrates effectiveness non hierarchical execution experimentally 
concludes comparison related discussion design tradeoffs hierarchical reinforcement learning 
fl ai access foundation morgan kaufmann publishers 
rights reserved 
dietterich 
area reinforcement learning bertsekas tsitsiklis sutton barto studies methods agent learn optimal near optimal plans interacting directly external environment 
basic methods reinforcement learning classical dynamic programming algorithms developed late bellman howard 
reinforcement learning methods offer important advantages classical dynamic programming 
methods online 
permits focus attention parts state space important ignore rest space 
second methods employ function approximation algorithms neural networks represent knowledge 
allows generalize state space learning time scales better 
despite advances reinforcement learning shortcomings 
biggest lack fully satisfactory method incorporating hierarchies reinforcement learning algorithms 
research classical planning shown hierarchical methods hierarchical task networks currie tate macro actions fikes hart nilsson korf state abstraction methods sacerdoti knoblock provide exponential reductions computational cost finding plans 
basic algorithms probabilistic planning reinforcement learning flat methods treat state space huge flat search space 
means paths start state goal state long length paths determines cost learning planning information rewards propagated backward paths 
researchers singh lin kaelbling dayan hinton hauskrecht parr russell sutton precup singh experimented different methods hierarchical reinforcement learning hierarchical probabilistic planning 
research explored different points design space hierarchical methods systems designed specific situations 
lack crisp definitions main approaches clear understanding relative merits different methods 
formalizes clarifies approach attempts understand compares techniques 
approach called maxq method provides hierarchical decomposition reinforcement learning problem set subproblems 
simultaneously provides decomposition value function problem set value functions subproblems 
declarative semantics value function decomposition procedural semantics subroutine hierarchy 
decomposition subproblems advantages 
policies learned subproblems shared reused multiple parent tasks 
second value functions learned subproblems shared subproblem reused new task learning value function new task accelerated 
third state abstractions applied value function represented compactly sum separate terms depends subset state variables 
compact representation value function require data learn learning faster 
maxq hierarchical reinforcement learning previous research shows important design decisions constructing hierarchical reinforcement learning system 
provide overview results review issues see maxq method approaches 
issue specify subtasks 
hierarchical reinforcement learning involves breaking target markov decision problem hierarchy subproblems subtasks 
general approaches defining subtasks 
approach define subtask terms fixed policy provided programmer learned separate process 
option method sutton precup singh takes approach 
second approach define subtask terms nondeterministic finite state controller 
hierarchy machines ham method parr russell takes approach 
method permits programmer provide partial policy constrains set permitted actions point specify complete policy subtask 
third approach define subtask terms termination predicate local reward function 
define means subtask completed final reward completing subtask 
maxq method described follows approach building previous singh kaelbling dayan hinton dean lin 
advantage option partial policy approaches subtask defined terms amount effort course action terms achieving particular goal condition 
option approach simple form described requires programmer provide complete policies subtasks difficult programming task real world problems 
hand termination predicate method requires programmer guess relative desirability different states subtask terminate 
difficult dean lin show guesses revised automatically learning algorithm 
potential drawback hierarchical methods learned policy may suboptimal 
hierarchy constrains set possible policies considered 
constraints poorly chosen resulting policy suboptimal 
learning algorithms developed option partial policy approaches guarantee learned policy best possible policy consistent constraints 
termination predicate method suffers additional source suboptimality 
learning algorithm described converges form local optimality call recursive optimality 
means policy subtask locally optimal policies children 
exist better hierarchical policies policy subtask locally suboptimal policy optimal 
example subtask buying milk performed distant store larger problem involves buying film store 
problem avoided careful definition termination predicates local reward functions added burden programmer 
interesting note problem recursive optimality noticed previously 
previous dietterich focused subtasks single terminal state cases problem arise 
second design issue employ state abstractions subtasks 
subtask employs state abstraction ignores aspects state environment 
example robot navigation problems choices route take reach goal location independent robot currently carrying 
exceptions state abstraction explored previously 
see maxq method creates opportunities exploit state abstraction abstractions huge impact accelerating learning 
see important design tradeoff successful state abstraction requires subtasks defined terms termination predicates option partial policy methods 
maxq method employ termination predicates despite problems create 
third design issue concerns non hierarchical execution learned hierarchical policy 
kaelbling point value function learned hierarchical policy evaluated incrementally yield potentially better non hierarchical policy 
dietterich sutton 
generalized show arbitrary subroutines executed non hierarchically yield improved policies 
order support non hierarchical execution extra learning required 
ordinarily hierarchical reinforcement learning states learning required higher levels hierarchy states subroutines terminate plus possible initial states 
support non hierarchical execution learning required states levels hierarchy 
general requires additional exploration additional computation memory 
consequence hierarchical decomposition value function maxq method able support form execution see problems improvement non hierarchical execution worth added cost 
fourth final issue form learning algorithm employ 
important advantage reinforcement learning algorithms typically operate online 
finding online algorithms general hierarchical reinforcement learning difficult particularly termination predicate family methods 
singh method relied subtask having unique terminal state kaelbling employed mix online batch algorithms train hierarchy options framework usually assumes policies subproblems need learned 
best previous online algorithms learning algorithm parr russell partial policy method feudal algorithm dayan hinton 
unfortunately method requires flattening hierarchy undesirable consequences 
feudal algorithm tailored specific kind problem converge defined optimal policy 
general algorithm called maxq fully online learning hierarchical value function 
algorithm enables subtasks hierarchy learned simultaneously online 
show experimentally theoretically algorithm converges recursively optimal policy 
show substantially faster flat non hierarchical learning state abstractions employed 
maxq hierarchical reinforcement learning remainder organized follows 
introducing notation section define maxq value function decomposition section illustrate simple example markov decision problem 
section presents analytically tractable version maxq learning algorithm called maxq algorithm proves convergence recursively optimal policy 
shows extend maxq produce maxq algorithm shows extend theorem similarly 
section takes issue state abstraction formalizes series conditions state abstractions safely incorporated maxq representation 
state abstraction give rise hierarchical credit assignment problem briefly discusses solution problem 
section presents experiments example domains 
experiments give idea generality maxq representation 
provide results relative importance temporal state abstractions importance non hierarchical execution 
concludes discussion design issues briefly described particular addresses tradeoff method defining subtasks termination predicates ability exploit state abstractions 
readers may disappointed maxq provides way learning structure hierarchy 
philosophy developing maxq share reinforcement learning researchers notably parr russell draw inspiration development belief networks pearl 
belief networks introduced formalism knowledge engineer describe structure networks domain experts provide necessary probability estimates 
subsequently methods developed learning probability values directly observational data 
methods developed learning structure belief networks data dependence knowledge engineer reduced 
likewise require programmer provide structure hierarchy 
programmer need important design decisions 
see maxq representation computer program rely programmer design modules indicate permissible ways modules invoke 
learning algorithms fill implementations module way program 
believe approach provide practical tool solving large real world mdps 
believe help understand structure hierarchical learning algorithms 
hope subsequent research able automate currently requiring programmer 

formal definitions introducing definitions markov decision problems semi markov decision problems 
markov decision problems employ standard definition markov decision problems known markov decision processes 
restrict attention situations agent dietterich interacting fully observable stochastic environment 
situation modeled markov decision problem mdp hs defined follows ffl finite set states environment 
point time agent observe complete state environment 
ffl finite set actions 
technically set available actions depends current state suppress dependence notation 
ffl action performed environment probabilistic transition current state resulting state probability distribution js 
ffl similarly action performed environment transition agent receives real valued possibly stochastic reward expected value js 
simplify notation customary treat reward time action initiated may general depend ffl starting state distribution 
mdp initialized state probability 
policy mapping states actions tells action perform environment state consider settings episodic infinite horizon 
episodic setting rewards finite zero cost absorbing terminal state 
absorbing terminal state state actions lead back state probability zero reward 
technical reasons consider problems deterministic policies proper deterministic policies non zero probability reaching terminal state started arbitrary state 
believe condition relaxed verified formally 
episodic setting goal agent find policy maximizes expected cumulative reward 
special case rewards non positive problems referred stochastic shortest path problems rewards viewed costs lengths policy attempts move agent path minimum expected cost 
infinite horizon setting rewards finite 
addition discount factor fl agent goal find policy minimizes infinite discounted sum rewards 
value function policy function tells state expected cumulative reward executing policy starting state random variable tells reward agent receives time step policy define value function episodic setting fr delta delta delta js discounted setting value function flr fl delta delta delta fi fi fi maxq hierarchical reinforcement learning see equation reduces previous fl 
mdps sum may converge fl 
value function satisfies bellman equation fixed policy js theta js quantity right hand side called backed value performing action state possible successor state computes reward received value resulting state weights probability optimal value function value function simultaneously maximizes expected cumulative reward states bellman proved unique solution known bellman equation max js theta js may optimal policies achieve value 
policy chooses achieve maximum right hand side equation optimal policy 
denote optimal policy note optimal policies greedy respect backed value available actions 
closely related value function called action value function function watkins 
function gives expected cumulative reward performing action state policy 
function satisfies bellman equation js theta js flq optimal action value function written satisfies equation js js fl max note policy greedy respect optimal policy 
may optimal policies differ break ties actions identical values 
action order denoted total order actions mdp 
anti symmetric transitive relation true iff strictly preferred ordered greedy policy greedy policy breaks ties 
example suppose best actions state 
ordered greedy policy choose 
note may optimal policies mdp ordered greedy policy unique 
dietterich semi markov decision processes order introduce prove properties maxq decomposition need consider simple generalization mdps semi markov decision process 
discrete time semi markov decision process smdp generalization markov decision process actions take variable amount time complete 
particular random variable denote number time steps action takes executed state extend state transition probability function joint distribution result states number time steps action performed state js 
similarly expected reward changed js 
straightforward modify bellman equation define value function fixed policy js js fl change expected value right hand side taken respect fl raised power reflect variable amount time may elapse executing action note expectation linear operator write bellman equations sum expected reward performing action expected value resulting state example rewrite equation js fl expected reward performing action state expectation taken respect results generalized apply discrete time decision processes 
consequence talks executing primitive action just easily talk executing hand coded openloop subroutine 
subroutines learned execution interrupted discussed section 
applications robot control limited sensors open loop controllers useful hide example see ar ari 
note episodic case difference mdp semi markov decision process discount factor fl optimal policy optimal value function depend amount time action takes 
reinforcement learning algorithms reinforcement learning algorithm algorithm tries construct optimal policy unknown mdp 
algorithm access unknown mdp 
formalization slightly different standard formulation smdps separates js tjs cumulative distribution function probability terminate time units real valued integer valued 
case important consider joint distribution need consider actions arbitrary real valued durations 
maxq hierarchical reinforcement learning reinforcement learning protocol 
time step algorithm told current state mdp set actions executable state 
algorithm chooses action mdp executes action causes move state returns real valued reward absorbing terminal state set actions contains special action reset causes mdp move initial states drawn known learning algorithms learning watkins watkins dayan sarsa rummery niranjan 
apply algorithms case action value function represented table entry pair state action 
entry table initialized arbitrarily 
learning algorithm observed chosen received observed performs update gamma ff gamma ff fl max gamma ff learning rate parameter 
jaakkola jordan singh bertsekas tsitsiklis prove agent follows exploration policy tries action state infinitely lim ff lim ff converges optimal action value function probability 
proof holds settings discussed episodic infinite horizon 
sarsa algorithm similar 
observing choosing observing observing choosing algorithm performs update gamma ff gamma ff flq gamma ff learning rate parameter 
key difference value chosen action appears right hand side place learning uses value best action 
singh 
provide important convergence results fixed policy employed choose actions sarsa converge value function policy provided ff decreases equations 
second called glie policy employed choose actions sarsa converge value function optimal policy provided ff decreases equations 
glie policy defined follows definition glie greedy limit infinite exploration policy policy satisfying 
action executed infinitely state visited infinitely 

limit policy greedy respect value function probability 
dietterich taxi domain 

maxq value function decomposition center maxq method hierarchical reinforcement learning maxq value function decomposition 
maxq describes decompose value function policy collection value functions individual subtasks recursively 
motivating example discussion concrete consider simple example 
shows grid world inhabited taxi agent 
specially designated locations world marked ed lue 
taxi problem episodic 
episode taxi starts randomly chosen square 
passenger locations chosen randomly passenger wishes transported locations chosen randomly 
taxi go passenger location source pick passenger go destination location destination put passenger 
keep things uniform taxi pick drop passenger located destination 
episode ends passenger deposited destination location 
primitive actions domain navigation actions move taxi square north south east west pickup action putdown action 
reward gamma action additional reward successfully delivering passenger 
reward gamma taxi attempts execute putdown pickup actions illegally 
navigation action cause taxi hit wall action op usual reward gamma 
simplify examples section primitive actions deterministic 
actions stochastic order create greater challenge learning algorithms 
seek policy maximizes total reward episode 
possible states squares locations passenger counting starting locations taxi destinations 
task simple hierarchical structure main sub tasks get passenger deliver passenger 
subtasks turn involves maxq hierarchical reinforcement learning subtask navigating locations performing pickup putdown action 
task illustrates need support temporal abstraction state abstraction subtask sharing 
temporal abstraction obvious example process navigating passenger location picking passenger temporally extended action take different numbers steps complete depending distance target 
top level policy get passenger deliver passenger expressed simply temporal abstractions employed 
need state abstraction obvious 
consider subtask getting passenger 
subtask solved destination passenger completely irrelevant affect pickup decisions 
importantly navigating target location source destination location passenger target location important 
fact cases taxi carrying passenger cases irrelevant 
support subtask sharing critical 
system learn solve navigation subtask solution shared get passenger deliver passenger subtasks 
show maxq method provides value function representation learning algorithm supports temporal abstraction state abstraction subtask sharing 
construct maxq decomposition taxi problem identify set individual subtasks believe important solving task 
case define tasks ffl navigate 
subtask goal move taxi current location target locations indicated formal parameter ffl get 
subtask goal move taxi current location passenger current location pick passenger 
ffl put 
goal subtask move taxi current location passenger destination location drop passenger 
ffl root 
taxi task 
subtasks defined subgoal subtask terminates subgoal achieved 
defining subtasks indicate subtask subtasks primitive actions employ reach goal 
example navigate subtask primitive actions north south east west 
get subtask navigate subtask pickup primitive action 
information summarized directed acyclic graph called task graph shown 
graph node corresponds subtask primitive action edge corresponds potential way subtask call child tasks 
notation formal actual source tells formal parameter bound actual parameter 
suppose subtasks write policy computer program achieve subtask 
refer policy subtask subroutine view parent subroutine invoking child subroutine ordinary dietterich pickup get root put navigate putdown south east west north destination source task graph taxi problem 
subroutine call return semantics 
policy subtask gives policy taxi mdp 
root subtask executes policy calling subroutines policies get put subtasks 
get policy calls subroutines navigate subtask pickup primitive action 

call collection policies hierarchical policy 
hierarchical policy subroutine executes enters terminal state subtask 
definitions formalize discussion far 
maxq decomposition takes mdp decomposes finite set subtasks fm convention root subtask solving solves entire original mdp 
definition subtask tuple ht defined follows 
termination predicate partitions set active states set terminal states policy subtask executed current state time subtask executed mdp enters state terminates immediately executing subtask see 

set actions performed achieve subtask actions primitive actions set primitive actions mdp subtasks denote indexes refer actions children subtask sets define directed graph subtasks graph may contain cycles 
stated way subtask invoke recursively directly indirectly 
child subtask formal parameters interpreted subtask occurred multiple times occurrence possible tuple actual maxq hierarchical reinforcement learning values bound formal parameters 
set actions may differ state set actual parameter values technically function actual parameters 
suppress dependence notation 

pseudo reward function specifies deterministic pseudo reward transition terminal state pseudo reward tells desirable terminal states subtask 
typically employed give goal terminal states pseudo reward non goal terminal states negative reward 
definition pseudo reward zero non terminal states pseudo reward learning mentioned section 
primitive action primitive subtask maxq decomposition executable terminates immediately execution pseudo reward function uniformly zero 
subtask formal parameters possible binding actual values formal parameters specifies distinct subtask 
think values formal parameters part name subtask 
practice course implement parameterized subtask parameterizing various components task 
specifies actual parameter values task define parameterized termination predicate parameterized pseudo reward function 
simplify notation rest usually omit parameter bindings 
noted parameter subtask takes large number possible values equivalent creating large number different subtasks need learned 
create large number candidate actions parent task learning problem difficult parent task 
definition hierarchical policy set containing policy subtasks problem subtask policy takes state returns name primitive action execute name subroutine bindings formal parameters invoke 
terminology sutton precup singh subtask policy deterministic option probability terminating state denote fi parameterized task policy parameterized takes state bindings formal parameters returns chosen action bindings formal parameters 
table gives pseudo code description procedure executing hierarchical policy 
hierarchical policy executed stack discipline similar ordinary programming languages 
denote contents pushdown stack time subroutine invoked name actual parameters pushed stack 
subroutine terminates name actual parameters popped stack 
notice line subroutine stack terminates subroutines dietterich table pseudo code execution hierarchical policy 
procedure state world time state execution stack time empty stack observe push nil stack invoke root task parameters repeat top primitive action top name current subroutine gives parameter bindings fa action fa gives parameter bindings chosen policy push fa stack nil pop primitive action top stack 
execute primitive action observe receive reward js subtask terminated terminated subtask highest closest root stack 
top pop pop resulting execution stack 
empty immediately aborted control returns subroutine invoked terminated subroutine 
useful think contents stack additional part state space problem 
hierarchical policy implicitly defines mapping current state current stack contents primitive action action executed yields resulting state resulting stack contents added state information stack hierarchical policy non markovian respect original mdp 
hierarchical policy maps states stack contents actions value function hierarchical policy assign values combinations states stack contents definition hierarchical value function denoted hs ki gives expected cumulative reward hierarchical policy starting state stack contents hierarchical value function exactly learned ron parr algorithm discuss 
focus learning projected value functions subtasks hierarchy 
maxq hierarchical reinforcement learning definition projected value function hierarchical policy subtask denoted expected cumulative reward executing policies descendents starting state terminates 
purpose maxq value function decomposition decompose projected value function root task terms projected value functions subtasks maxq decomposition 
decomposition projected value function defined hierarchical policy projected value function show value function decomposed hierarchically 
decomposition theorem theorem task graph tasks hierarchical policy subtask defines semi markov decision process states actions probability transition function js expected reward function projected value function child task state primitive action defined expected immediate reward executing js js 
proof consider subroutines descendents task task graph 
subroutines executing fixed policies specified hierarchical policy probability transition function js defined stationary distribution child subroutine set states set actions obvious 
interesting part theorem fact expected reward function smdp projected value function child task see write value efr flr fl delta delta delta js sum continues subroutine task enters state suppose action chosen subroutine subroutine invoked executes number steps terminates state js 
rewrite equation gamma fl fl fi fi fi fi fi summation right hand side equation discounted sum rewards executing subroutine starting state terminates words projected value function child task second term right hand side equation value current task discounted fl current state subroutine terminates 
write form bellman equation js fl dietterich form equation bellman equation smdp term expected reward 
obtain hierarchical decomposition projected value function switch action value representation 
need extend notation handle task hierarchy 
expected cumulative reward subtask performing action state hierarchical policy subtask terminates 
action may primitive action child subtask 
notation re state equation follows js fl right term equation expected discounted reward completing task executing action state term depends summation away dependence define equal term definition completion function expected discounted cumulative reward completing subtask invoking subroutine subtask state reward discounted back point time begins execution 
js fl definition express function recursively re express definition composite js js primitive refer equations decomposition equations maxq hierarchy fixed hierarchical policy equations recursively decompose projected value function root projected value functions individual subtasks individual completion functions fundamental quantities stored represent value function decomposition just values non primitive subtasks values primitive actions 
easier programmers design debug maxq decompositions developed graphical representation call maxq graph 
maxq graph taxi domain shown 
graph contains kinds nodes max nodes nodes 
max nodes correspond subtasks task decomposition max node primitive action max node subtask including root task 
primitive max node stores value 
nodes correspond actions available subtask 
node parent task state maxq hierarchical reinforcement learning source destination putdown east south west north pickup maxq graph taxi domain 
subtask stores value 
children node unordered order drawn imply order executed 
child action may executed multiple times parent subtask completed 
addition storing information max nodes nodes viewed performing parts computation described decomposition equations 
specifically max node viewed computing projected value function subtask 
primitive max nodes information stored node 
composite max nodes information obtained asking node corresponding 
node parent task child task viewed computing value 
asking child task projected value function adding completion function 
dietterich example consider situation shown denote suppose passenger wishes go hierarchical policy evaluating optimal policy denoted omit superscript reduce clutter notation 
value state cost unit move taxi unit pickup passenger units move taxi unit putdown passenger total units reward gamma 
passenger delivered agent gets reward net value 
shows maxq hierarchy computes value 
compute value root consults policy finds root get 
asks node compute root get 
completion cost root task performing get root get cost units deliver customer net reward gamma completing get subtask 
just reward completing get ask estimate expected reward performing get 
policy dictates navigate subroutine invoked bound consults node compute expected reward 
knows completing navigate task action pickup required complete get navigate gamma 
asks compute expected reward performing navigate location policy chooses north action asks compute value 
looks completion cost finds navigate north navigate task completed performing north action 
consults determine expected cost performing north action 
primitive action looks expected reward gamma 
series recursive computations conclude follows ffl navigate north gamma ffl navigate gamma ffl get navigate gamma gamma gamma perform navigate plus gamma complete get 
ffl get gamma ffl root get gamma gamma perform get plus complete root task collect final reward 
result value root decomposed sum terms plus expected reward chosen primitive action root north navigate north get navigate root get gamma gamma maxq hierarchical reinforcement learning pickup putdown north east south west computing value state maxq hierarchy 
value node shown left node 
numbers show values returned graph 
general maxq value function decomposition form gamma am am path max nodes chosen hierarchical policy going root primitive leaf node 
summarized graphically 
summarize presentation section theorem theorem ng hierarchical policy defined maxq graph subtasks root node graph 
exist values internal max nodes primitive leaf max dietterich 
ae ae ae ae xxxxxx 
am gamma am am gamma am maxq decomposition denote sequence rewards received primitive actions times 
nodes computed decomposition equations expected discounted cumulative reward policy starting state proof proof induction number levels task graph 
level compute values primitive decomposition equations 
apply decomposition equations compute apply equation theorem conclude gives value function level obtain value function entire hierarchical policy 
important note representation theorem mention function pseudo reward learning 
theorem captures representational power maxq decomposition address question learning algorithm find policy 
subject section 

learning algorithm maxq decomposition section presents central contributions 
discuss optimality criteria employed hierarchical reinforcement learning 
introduce maxq learning algorithm learn value functions policies maxq hierarchies pseudo rewards pseudo rewards zero 
central theoretical result maxq converges recursively optimal policy maxq hierarchy 
followed brief discussion ways accelerating maxq learning 
section concludes description maxq learning algorithm handles non zero pseudo reward functions 
maxq hierarchical reinforcement learning kinds optimality order develop learning algorithm maxq decomposition consider exactly hoping achieve 
course mdp find optimal policy maxq method hierarchical reinforcement learning general programmer imposes hierarchy problem 
hierarchy constrains space possible policies may possible represent optimal policy value function 
maxq method constraints take forms 
subtask possible primitive actions may permitted 
example taxi task navigate north south east west actions available pickup putdown actions allowed 
second consider max node child nodes fm policy learned involve executing learned policies child nodes 
policy child node executed run enters state policy learned pass subset terminal state sets ft ham method shares constraints addition imposes partial policy node policy subtask deterministic refinement non deterministic initial policy node option approach policy constrained 
approach non primitive levels hierarchy subtasks lower level children primitive actions complete policies programmer 
learned policy upper level constructed concatenating lower level policies order 
purpose imposing constraints policy incorporate prior knowledge reduce size space searched find policy 
constraints may impossible learn optimal policy 
learn optimal policy best target learn best policy consistent represented hierarchy 
definition hierarchically optimal policy mdp policy achieves highest cumulative reward policies consistent hierarchy 
parr proves learning algorithm converges probability hierarchically optimal policy 
similarly fixed set options sutton precup singh prove smdp learning algorithm converges hierarchically optimal value function 
incidentally show primitive actions available trivial options smdp method converges optimal policy 
case hard say formal options speed learning process 
may fact hinder hauskrecht 
maxq decomposition represent value function hierarchical policy easily construct modified version algorithm apply learn hierarchically optimal policies maxq hierarchy 
decided pursue weaker form optimality reasons clear proceed 
form optimality called recursive optimality 
dietterich south east north simple mdp left associated maxq graph right 
policy shown left diagram recursively optimal hierarchically optimal 
shaded cells indicate points locally optimal policy globally optimal 
definition recursively optimal policy markov decision process maxq decomposition fm hierarchical policy subtask corresponding policy optimal smdp defined set states set actions state transition probability function js reward function sum original reward function js function 
note state transition probability distribution js subtask defined locally optimal policies subtasks descendents maxq graph 
recursive optimality kind local optimality policy node optimal policies children 
reason seek recursive optimality hierarchical optimality recursive optimality possible solve subtask context executed 
context free property easier share re subtasks 
turn essential successful state abstraction 
proceed describe learning algorithm recursive optimality see recursive optimality differs hierarchical optimality 
easy construct examples policies recursively optimal hierarchically optimal vice versa 
consider simple maze problem associated maxq graph shown figures 
suppose robot starts left room reach goal right room 
robot actions north south east actions deterministic 
robot receives reward gamma move 
define subtasks maxq hierarchical reinforcement learning ffl exit 
task terminates robot exits left room 
set function terminal states states indicated 
ffl 
task terminates robot reaches goal arrows show locally optimal policy room 
arrows left seek exit left room shortest path specified set pseudo reward function 
arrows right follow shortest path goal fine 
resulting policy hierarchically optimal optimal 
exists hierarchical policy exit left room upper door 
maxq value function decomposition represent value function policy policy locally optimal example states shaded region follow shortest path doorway 
example illustrates recursively optimal policy hierarchically optimal hierarchically optimal policy recursively optimal 
consider moment see way fix problem 
value upper starred state optimal hierarchical policy gamma value lower starred state gamma 
changed values zero recursively optimal policy hierarchically optimal globally optimal 
words programmer guess right values terminal states subtask recursively optimal policy hierarchically optimal 
basic idea pointed dean lin 
describe algorithm initial guesses values starred states updates guesses computed values starred states resulting policy 
proved converge hierarchically optimal policy 
drawback method requires repeated solution resulting hierarchical learning problem yield speedup just solving original flat problem 
parr proposed interesting approach constructs set different functions computes recursively optimal policy subtask 
method chooses functions way hierarchically optimal policy approximated desired degree 
unfortunately method quite expensive relies solving series linear programming problems requires time polynomial parameters including number states js subtask 
discussion suggests principle possible learn values pseudo reward function practice rely programmer specify single pseudo reward function subtask 
programmer wishes consider small number alternative pseudo reward functions handled defining small number subtasks identical functions permitting learning algorithm choose gives best recursively optimal policy 
experiments employed simplified approach defining subtask define predicates termination predicate goal predicate goal predicate defines subset terminal states goal states pseudo reward 
terminal states fixed constant dietterich pseudo reward gamma set better terminate goal state non goal state 
problems tested maxq method worked 
experiments maxq easy mistakes defining goal defined carefully easy create set subtasks lead infinite looping 
example consider problem 
suppose permit fourth action west mdp define termination goal predicates right hand room satisfied iff robot reaches goal exits room 
natural definition quite similar definition left hand room 
resulting locally optimal policy room attempt move nearest locations goal upper door lower door 
easily see states near goal policies constructed loop forever trying leave left room entering right room trying leave right room entering left room 
problem easily fixed defining goal predicate right room true robot reaches goal avoiding undesired termination bugs hard complex domains 
worst case possible programmer specify pseudo rewards recursively optimal policy arbitrarily worse hierarchically optimal policy 
example suppose change original mdp state immediately left upper doorway gives large negative reward gammal robot visits square 
rewards gamma policy exits room lower door 
suppose programmer chosen force robot exit upper door assigning pseudo reward gamma leaving lower door 
case recursively optimal policy leave upper door suffer large gammal penalty 
making arbitrarily large difference hierarchically optimal policy recursively optimal policy arbitrarily large 
maxq learning algorithm understanding recursively optimal policies learning algorithms 
called maxq applies case pseudo reward function zero 
prove convergence properties show extended give second algorithm maxq works general pseudo reward functions 
table gives pseudo code maxq 
maxq recursive function executes current exploration policy starting max node state performs actions reaches terminal state point returns count total number primitive actions executed 
execute action maxq calls recursively line 
recursive call returns updates value completion function node uses count number primitive actions appropriately discount value resulting state leaf nodes maxq updates estimated step expected reward 
value ff learning rate parameter gradually decreased zero limit 
maxq hierarchical reinforcement learning table maxq learning algorithm 
function maxq state primitive execute receive observe result state gamma ff delta ff delta return count false choose action current exploration policy maxq recursive call observe result state gamma ff delta ff delta fl count count return count maxq main program initialize arbitrarily maxq root node starting state things specified order algorithm description complete 
keep pseudo code readable table show ancestor termination handled 
recall action termination predicates subroutines calling stack checked 
termination predicate satisfied calling stack unwound highest terminated subroutine 
cases values updated subroutines interrupted follows 
subroutine invoked subroutine termination condition satisfied subroutine update value 
second specify compute line stored max node 
computed modified versions decomposition equations max composite primitive equations reflect important changes compared equations 
equation defined terms value best action action chosen fixed hierarchical policy 
second superscripts current value function fixed hierarchical policy compute equations perform complete search paths maxq graph starting node leaf nodes 
table dietterich table pseudo code greedy execution maxq graph 
function primitive max node return hv ii hv hg argmax return hv hg hg gives pseudo code recursive function implements depthfirst search 
addition returning returns action leaf node achieves value 
information needed maxq useful consider non hierarchical execution learned policy 
search computationally expensive problem research develop efficient methods computing best path graph 
approach perform best search bounds values subtrees prune useless paths maxq graph 
better approach computation incremental state environment changes nodes values changed result state change re considered 
possible develop efficient bottom method similar rete algorithm successors soar architecture forgy tambe rosenbloom 
third thing specified complete definition maxq exploration policy require ordered glie policy 
definition ordered glie policy glie policy greedy limit infinite exploration converges limit ordered greedy policy greedy policy imposes arbitrary fixed order available actions breaks ties favor action appears earliest order 
need property order ensure maxq converges uniquely defined recursively optimal policy 
fundamental problem recursive optimality general max node choice different locally optimal policies policies adopted descendent nodes 
different locally optimal policies achieve locally optimal value function give rise different probability transition functions js 
result semi markov decision problems defined level node maxq graph differ depending various locally optimal policies chosen node differences may lead better worse policies higher levels maxq graph difference inside subtask practice designer maxq graph need design pseudo reward function subtask ensure locally optimal policies maxq hierarchical reinforcement learning equally valuable parent subroutine 
carry formal analysis just rely arbitrary tie breaking mechanism 
establish fixed ordering max nodes maxq graph left right depth numbering break ties favor lowest numbered action defines unique policy max node 
consequently induction defines unique policy entire maxq graph 
call policy subscript denote recursively optimal quantities ordered greedy policy 
corresponding value function denote corresponding completion function action value function 
prove maxq algorithm converges theorem hs episodic mdp deterministic policies proper discounted infinite horizon mdp discount factor fl 
maxq graph defined subtasks fm pseudo reward function zero ff sequence constants max node lim ff lim ff ordered glie policy node state assume immediate rewards bounded 
probability algorithm maxq converges unique recursively optimal policy consistent proof proof follows argument similar introduced prove convergence learning sarsa bertsekas tsitsiklis jaakkola 
employ result stochastic approximation theory state proof lemma proposition bertsekas tsitsiklis consider iteration gamma ff ff ur fr gamma ff ff ig entire history iteration 
ff satisfy conditions noise terms satisfy jf norm jj delta jj exist constants jf jj 
exists vector positive vector scalar fi gamma jj gamma jj 
alternatively break ties stochastic policy chose randomly tied actions 
dietterich exists nonnegative random sequence converges zero probability ju jjr jj converges probability 
notation jj delta jj denotes weighted maximum norm jjajj max ja structure proof theorem inductive starting leaves maxq graph working root 
employ different time clock node count number update steps performed maxq node 
variable refer time clock current node prove base case primitive max node note line maxq just standard stochastic approximation algorithm computing expected reward performing action state converges conditions 
prove recursive case consider composite max node child node js transition probability distribution performing child action state time exploration policy descendent nodes node 
inductive assumption maxq applied converge unique recursively optimal value function probability 
furthermore maxq ordered glie policy descendents converge executing greedy policy respect value functions js converge js unique transition probability function executing child locally optimal policy remains shown update assignment line maxq algorithm converges optimal function probability 
prove apply lemma 
identify lemma state action pair 
vector completion cost table fixed update steps 
vector optimal completion cost fixed 
define mapping uc js fl max update mdp assuming descendent value functions transition probabilities js converged 
apply lemma express update formula form update rule lemma 
state results performing state line written gamma ff delta ff delta fl max gamma ff delta ff delta uc maxq hierarchical reinforcement learning fl max gamma js fl max js fl max gamma js fl max difference doing update node single sample point drawn js doing update full distribution js 
value captures difference doing update current probability transitions js current value functions children doing update optimal probability transitions js optimal values children 
verify conditions lemma 
condition assumed conditions theorem ff ff 
condition satisfied sampled js expected value difference zero 
condition follows directly fact jc jv bounded 
show bounded episodic case discounted case follows 
episodic case assumed policies proper 
trajectories terminate finite time finite total reward 
discounted case infinite sum rewards bounded step rewards bounded 
values computed temporal averages cumulative rewards received finite number bounded updates means variances maximum values bounded 
condition condition weighted max norm pseudo contraction 
derive starting weighted max norm learning 
known weighted max norm pseudo contraction bertsekas tsitsiklis episodic case deterministic policies proper discount factor fl infinite horizon discounted case fl 
exists positive vector scalar fi gamma jj gamma jj operator tq js fl js max show derive pseudo contraction update operator plan show express operator learning terms operator updating values 
replace tq pseudo contraction equation dietterich learning uc show weighted max norm pseudo contraction weights fi 
recall eqn 

furthermore operator performs updates optimal value functions child nodes write 
children node converged function version bellman equation mdp written js fl max noted plays role immediate reward function node operator rewritten tq js fl max replace obtain tq js fl max note depend move outside expectation obtain tq js fl max uc abusing notation slightly express vector form tq uc 
similarly write vector form substitute formulas max norm pseudo contraction formula eqn 
obtain jjv uc gamma jj gamma jj weighted max norm pseudo contraction gamma jj gamma jj condition satisfied 
easy verify important condition 
assumption ordered glie policies child nodes converge probability locally optimal policies children 
js converges js probability converges probability child actions ju converges zero probability 
trivially construct sequence ju bounds convergence ju jjc jj maxq hierarchical reinforcement learning verified conditions lemma conclude converges probability 
induction conclude holds nodes maxq including root node value function represented maxq graph converges unique value function recursively optimal policy important aspect theorem proves learning take place levels maxq hierarchy simultaneously higher levels need wait lower levels converged learning 
necessary lower levels eventually converge locally optimal policies 
techniques speeding maxq algorithm maxq extended accelerate learning higher nodes graph technique call states updating 
action chosen max node state execution move environment sequence states subroutines markovian resulting state reached started executing action state state including execute version line maxq intermediate states shown replacement pseudo code gamma ff delta ff delta fl gammaj max implementation composite action executed maxq constructs linked list sequence primitive states visited 
list returned composite action terminates 
parent max node process state list shown 
parent max node concatenates state lists receives children passes parent terminates 
experiments employ states updating 
kaelbling introduced related powerful method accelerating hierarchical reinforcement learning calls goals updating 
understand method suppose primitive action composite tasks invoked primitive action 
goals updating primitive action executed equivalent line maxq applied composite task invoked primitive action 
sutton precup singh prove composite tasks converge optimal values goals updating 
furthermore point exploration policy employed choosing primitive actions different policies subtasks learned 
straightforward implement simple form goals updating maxq hierarchy case composite tasks invoke primitive actions 
primitive actions executed state update value parent tasks invoke additional care required implement goals updating non primitive actions 
suppose executing exploration policy sequence world states actions obtained gamma gamma composite task terminated state gamman gamman gamma sequence actions executed subtask children 
words suppose dietterich possible parse state action sequence terms series subroutine calls returns invocation subtask possible parent task invokes update value gamman 
course order updates useful exploration policy ordered glie policy converge recursively optimal policy subtask descendents 
follow arbitrary exploration policy produce accurate samples result states drawn js 
simple case described sutton precup singh exploration policy different policies subtasks learned 
considerably reduces usefulness goals updating completely eliminate 
simple way implementing non primitive goals updating perform maxq learning usual subtask invoked state returned update value potential calling subtasks implemented complexity involved identifying possible actual parameters potential calling subroutines 
maxq learning algorithm shown convergence maxq design learning algorithm arbitrary pseudo reward functions 
just add maxq effect changing mdp different reward function 
pseudo rewards contaminate values completion functions computed hierarchy 
resulting learned policy recursively optimal original mdp 
problem solved learning completion function inside max node separate completion function outside max node 
quantities inside node written tilde quantities outside node written tilde 
outside completion function completion function discussing far 
computes expected reward completing task performing action state learned policy computed completion function parent tasks compute expected reward performing action starting state second completion function completion function inside node order discover locally optimal policy task function incorporate rewards real reward function js pseudo reward function 
line choose best action hg execute 
note return external value hg chosen action 
employ different update rules learn completion functions 
function learned update rule similar learning rule line maxq 
function learned update rule similar sarsa purpose learn value function policy discovered optimizing pseudo code resulting algorithm maxq shown table 
key step lines 
line maxq updates value greedy action resulting state 
update includes pseudo reward maxq hierarchical reinforcement learning table maxq learning algorithm 
function maxq state seq sequence states visited executing primitive execute receive observe result state gamma ff delta ff delta push seq count false choose action current exploration policy maxq sequence states visited executing action 
reverse order observe result state argmax gamma ff delta ff delta fl gamma ff delta ff delta fl append front seq return seq maxq line maxq updates greedy action greedy action value function 
update course include pseudo reward function 
important note appears pseudo code refers value function state executing max node computed recursively exactly way maxq 
note pseudo code incorporates states updating call maxq returns list states visited execution updates lines performed states 
list states ordered states updated starting state visited working backward starting state helps speed algorithm 
maxq converged resulting recursively optimal policy computed node choosing action maximizes breaking ties fixed ordering established ordered glie policy 
reason gave name max nodes nodes represent subtasks learned policies maxq graph 
node parent node stores computes invoking child max node max node takes maximum values computes computes best action dietterich corollary conditions theorem maxq converges unique recursively optimal policy mdp defined maxq graph pseudo reward functions ordered glie exploration policy proof argument identical tedious proof theorem 
proof convergence values identical original proof values relies proving convergence new values follows weighted max norm pseudo contraction argument 

state abstraction reasons introduce hierarchical reinforcement learning important reason create opportunities state abstraction 
introduced simple taxi problem pointed subtask ignore certain aspects state space 
example performing taxi navigation decisions regardless passenger taxi 
purpose section formalize conditions safe introduce state abstractions show convergence proofs maxq extended prove convergence presence state abstraction 
specifically identify conditions permit safe state abstractions 
section taxi problem running example see conditions permit reduce number distinct values stored order represent maxq value function decomposition 
establish starting point compute number values stored taxi problem state abstraction 
maxq representation tables functions internal nodes functions leaves 
leaf nodes store store values node states locations possible destinations passenger possible current locations passenger special locations inside taxi 
second root node children requires theta values 
third nodes actions requires values total 
actions consider target parameter take possible values 
effectively combinations states values action total values represented 
total maxq representation requires separate quantities represent value function 
place number perspective consider flat learning representation store separate value primitive actions possible states total values 
see state abstraction maxq representation requires times memory flat table 
conditions permit state abstraction introduce conditions permit state abstractions 
condition give definition prove lemma states condition satisfied value function corresponding class policies maxq hierarchical reinforcement learning represented abstractly versions functions 
condition provide rules identifying condition satisfied give examples taxi domain 
introducing definitions notation 
definition mdp maxq graph defined suppose state written vector values set state variables 
max node suppose state variables partitioned sets function projects state values variables combined called state abstracted maxq graph 
cases state variables partitioned write mean state represented vector values state variables vector values state variables similarly write jx place js respectively 
definition policy hierarchical policy mdp maxq graph associated abstraction functions hierarchical policy policy corresponding subtask satisfies condition states 
stochastic policy exploration policy interpreted mean probability distributions choosing actions states 
order maxq converge presence state abstractions require times instantaneous exploration policy hierarchical policy 
way achieve construct exploration policy uses information relevant state variables deciding action perform 
boltzmann exploration state abstracted values ffl greedy exploration counter exploration abstracted states exploration policies 
counter exploration full state space exploration policy 
introduced notation describe analyze abstraction conditions 
identified different kinds conditions abstractions introduced 
kind involves eliminating irrelevant variables subtask maxq graph 
form abstraction nodes leaves maxq graph tend relevant variables nodes higher graph relevant variables 
kind abstraction useful lower levels maxq graph 
second kind abstraction arises funnel actions 
macro actions move environment large number initial states small number resulting states 
completion cost subtasks represented number values proportional number resulting states 
funnel actions tend appear higher maxq graph form abstraction useful near root graph 
third kind abstraction arises structure maxq graph 
exploits fact large parts state space subtask may reachable termination conditions ancestors maxq graph 
dietterich describing abstraction conditions type 
conditions second type 
describe condition third type 
condition max node irrelevance condition arises set state variables irrelevant max node 
definition max node irrelevance max node maxq graph mdp set state variables irrelevant node state variables partitioned sets stationary hierarchical policy executed descendents properties hold ffl state transition probability distribution js node factored product distributions jx jx delta jx give values variables give values variables ffl pair states child action 
note conditions hold stationary policies executed descendents subtask discuss strong requirements satisfied practice 
prove conditions sufficient permit tables represented state abstractions 
lemma mdp full state maxq graph suppose state variables irrelevant max node associated abstraction function projects remaining relevant variables hierarchical policy 
action value function node represented compactly value completion function equivalence class states share values relevant variables 
specifically computed follows jx delta fl arbitrary value irrelevant state variables maxq hierarchical reinforcement learning proof define new mdp node follows ffl states fx sg 
ffl actions ffl transition probabilities jx ffl reward function policy decisions states defined policy 
action value function unique solution bellman equation jx delta fl compare bellman equation js delta fl note 
furthermore know distribution factored separate distributions rewrite jx jx delta fl right sum depend sum evaluates eliminated give jx delta fl note equations identical expressions values 
solution bellman equation unique conclude rewrite right hand side obtain jx delta fl course primarily interested able discover represent optimal policy node corollary shows optimal policy policy represented abstractly 
dietterich corollary consider conditions lemma change hierarchical policy executed descendents node node ordering actions 
optimal ordered policy node policy action value function represented abstractly 
proof define policy ae optimal ordered policy mdp corresponding optimal action value function 
argument solution optimal bellman equation original mdp 
means policy defined 
ae optimal ordered policy construction policy 
stated max node irrelevance condition appears quite difficult satisfy requires state transition probability distribution factor components possible hierarchical policies 
practice condition satisfied 
example consider navigate subtask 
source destination passenger irrelevant achievement subtask 
policy successfully completes subtask value function regardless source destination locations passenger 
abstracting away passenger source destination obtain huge savings space 
requiring values represent functions task require values actions locations possible values 
advantages form abstraction similar obtained boutilier dearden goldszmidt belief network models actions exploited simplify value iteration stochastic planning 
way understanding conditions definition express form decision diagram shown 
diagram shows irrelevant variables affect rewards directly indirectly affect value function optimal policy 
rule noticing cases abstraction condition holds examine subgraph rooted max node set state variables irrelevant leaf state transition probabilities reward functions pseudo reward functions termination conditions subgraph variables satisfy max node irrelevance condition lemma mdp associated maxq graph max node partition state variables set state variables irrelevant node ffl primitive leaf node descendent jx jx jx jx jx ffl internal node equal node descendent termination predicate true iff 
maxq hierarchical reinforcement learning dynamic decision diagram represents conditions definition 
probabilistic nodes represent state variables time nodes represent state variables time square action node chosen child subroutine utility node represents value function child action 
note may influence affect affect proof show hierarchical policy give rise smdp node transition probability distribution factors reward function depends definition hierarchical policy choose actions information primitive probability transition functions factor independent component termination conditions nodes variables probability transition function jx factor jx jx 
similarly reward functions equal rewards received subtree leaves pseudo rewards depend variables variables irrelevant max node taxi task primitive navigation actions north south east west depend location taxi location passenger 
function termination condition node depend location taxi parameter 
lemma applies passenger source destination irrelevant node 
condition leaf irrelevance second abstraction condition describes situations apply state abstractions leaf nodes maxq graph 
leaf nodes obtain stronger result lemma slightly weaker definition irrelevance 
dietterich definition leaf irrelevance set state variables irrelevant primitive action maxq graph states expected value reward function js js depend values state variables words pair states differ values variables js js js js condition satisfied leaf lemma shows represent value function compactly 
lemma mdp full state maxq graph suppose state variables irrelevant leaf node associated abstraction function projects remaining relevant variables represent state abstracted value function 
proof definition leaf irrelevance states differ irrelevant state variables value 
represent unique value 
rules finding cases leaf irrelevance applies 
rule shows probability distribution factors leaf irrelevance 
lemma suppose probability transition function primitive action js factors jx jx jx reward function satisfies js jx 
variables irrelevant leaf node proof plug definition simplify 
js js jx jx jx jx jx jx jx jx expected reward action depends variables variables second rule shows reward function primitive action constant apply state abstractions js factor 
lemma suppose js reward function action mdp equal constant entire state irrelevant primitive action maxq hierarchical reinforcement learning proof js js js depend entire state irrelevant primitive action lemma satisfied leaf nodes north south east west taxi task step reward constant gamma 
requiring values store functions need values action 
similarly expected rewards pickup putdown actions require values depending corresponding actions legal illegal 
require values values 
condition result distribution irrelevance consider condition results funnel actions 
definition result distribution irrelevance 
set state variables irrelevant result distribution action policies executed node descendents maxq hierarchy holds pairs states differ values state variables js js condition satisfied subtask value parent task represented compactly lemma mdp full state maxq graph suppose set state variables irrelevant result distribution action child max node ij associated abstraction function ij define completion cost function ij states ij proof completion function fixed policy defined follows js delta fl consider states ij ij result distribution irrelevance transition probability distributions 
right hand sides value conclude dietterich define completion function represent quantity 
undiscounted cumulative reward problems definition result distribution irrelevance weakened eliminate number steps 
needed pairs states differ irrelevant state variables js js 
undiscounted case lemma holds revised definition 
appear result distribution irrelevance condition rarely satisfied find cases condition true 
consider example get subroutine taxi task 
matter location taxi state taxi passenger starting location get finishes executing taxi just completed picking passenger 
starting location irrelevant resulting location taxi js get js get states differ taxi location 
note maximizing discounted reward taxi location irrelevant probability get terminate exactly steps depend location taxi differ states different values produce different amounts discounting ignore taxi location representing completion function get 
undiscounted case applying lemma represent root get distinct values equivalence classes states source locations times destination locations 
quantities representation 
note state variables may irrelevant result distribution subtask may important subtask taxi task location taxi critical representing value get irrelevant result state distribution get irrelevant representing root get 
maxq decomposition essential obtaining benefits result distribution irrelevance 
funnel actions arise hierarchical reinforcement learning problems 
example actions move robot doorway move car entrance ramp freeway property 
result distribution irrelevance condition applicable situations long undiscounted setting 
condition termination fourth condition closely related funnel property 
applies subtask guaranteed cause parent task terminate goal state 
sense subtask environment set states described goal predicate parent task 
lemma termination 
task maxq graph states goal predicate true pseudo reward function 
suppose child task state hierarchical policies js maxq hierarchical reinforcement learning possible state results applying goal predicate true 
policy executed node completion cost zero need explicitly represented 
proof action executed state guaranteed result state true 
definition goal states satisfy termination predicate task terminate 
true terminal pseudo reward zero completion function zero 
example taxi task states taxi holding passenger put subroutine succeed result goal terminal state root 
termination predicate put passenger destination location implies goal condition root 
means root put uniformly zero states put terminated 
easy detect cases termination condition satisfied 
need compare termination predicate subtask goal predicate parent task 
implies second termination lemma satisfied 
condition shielding shielding condition arises structure maxq graph 
lemma shielding 
task maxq graph state paths root graph node subtask possibly equal termination predicate true nodes need represent values state proof order task executed state exist path ancestors task leading root graph ancestor tasks terminated 
condition lemma guarantees false task executed state values need represented 
termination condition shielding condition verified analyzing structure maxq graph identifying nodes ancestor tasks terminated 
taxi domain simple example arises put task terminated states passenger taxi 
means need represent root put states 
result combined termination condition need explicitly represent completion function put 
applying abstraction conditions obtain safe state abstractions taxi task ffl north south east west 
terminal nodes require quantity total values 
leaf irrelevance 
dietterich ffl pickup putdown require values legal illegal states total 
leaf irrelevance 
ffl require values values locations 
max node irrelevance 
ffl requires values possible source locations 
passenger destination max node irrelevant taxi starting location result distribution irrelevant navigate action 
ffl requires possible values possible source locations possible taxi locations 
passenger destination max node irrelevant 
ffl requires possible values source locations destination locations 
result distribution irrelevance 
ffl requires values possible destination locations 
passenger source destination max node irrelevant taxi location result distribution irrelevant navigate action 
ffl requires possible values taxi locations possible destination locations 
passenger source max node irrelevant 
ffl requires values 
termination shielding 
gives total distinct values values required flat learning 
see applying state abstractions maxq representation give compact representation value function 
key thing note state abstractions value function decomposed sum terms single term depends entire state mdp value function depend entire state mdp 
example consider state described figures 
showed value state passenger destination taxi decomposed root north navigate north get navigate root get state abstractions see term right hand side depends subset features ffl north constant ffl navigate north depends taxi location passenger source location 
ffl get navigate depends source location 
ffl root get depends passenger source destination 
maxq hierarchical reinforcement learning maxq decomposition features irrelevant value function depends entire state 
prior knowledge required part programmer order identify state abstractions 
suffices know qualitative constraints step reward functions step transition probabilities termination predicates goal predicates pseudo reward functions maxq graph 
specifically max node irrelevance leaf irrelevance conditions require simple analysis step transition function reward pseudo reward functions 
opportunities apply result distribution irrelevance condition identifying funnel effects result definitions termination conditions operators 
similarly shielding termination conditions require analysis termination predicates various subtasks 
applying conditions introduce state abstractions straightforward process model step transition reward functions learned abstraction conditions checked see satisfied 
convergence maxq state abstraction shown state abstractions safely introduced maxq value function decomposition conditions described 
conditions guarantee value function fixed hierarchical policy represented show recursively optimal policies represented show maxq learning algorithm find recursively optimal policy forced state abstractions 
goal section prove results ordered recursively optimal policy policy represented state abstractions maxq converge policy applied maxq graph safe state abstractions 
lemma mdp full state maxq graph state maxq graph abstractions satisfy conditions 
ordering actions maxq graph 
statements true ffl unique ordered recursively optimal policy defined policy depends relevant state variables node see definition ffl functions represent projected value function proof abstraction lemmas tell ordered recursively optimal policy functions represent value function 
heart lemma claim 
forms abstraction shielding termination place restrictions policies ignore proof 
proof induction levels maxq graph starting leaves 
base case consider max node children primitive actions 
case policies executed children max node 
variables irrelevant node apply abstraction lemmas represent value function policy node just policies 
consequently value dietterich function optimal policy node represented property states 
impose action ordering compute optimal ordered policy 
consider actions prefers suppose tie function state values actions maximize state 
optimal ordered policy choose states just established values 
tie exist optimal ordered policy choice states 
optimal ordered policy node policy 
turn recursive case max node inductive assumption ordered recursively optimal policy descendent nodes consider locally optimal policy node set state variables irrelevant node corollary tells states 
similarly set variables irrelevant result distribution particular action lemma tells thing 
ordering argument ordered optimal policy node 
induction proves lemma 
lemma established combination mdp maxq graph action ordering defines unique recursively optimal ordered policy 
ready prove maxq converge policy 
theorem hs episodic mdp deterministic policies proper discounted infinite horizon mdp discount factor fl 
maxq graph defined subtasks fm pseudo reward functions 
state abstracted maxq graph defined applying state abstractions node conditions 
ordered glie exploration policy node state decisions depend relevant state variables node unique hierarchical policy defined probability algorithm maxq applied converges provided learning rates ff satisfy equation step rewards bounded 
proof repeating entire proof maxq describe change state abstraction 
forms state abstraction refer states values inferred structure maxq graph need represented 
values updated maxq ignore 
consider forms state abstraction turn 
considering primitive leaf nodes 
leaf node set state variables leaf irrelevant states maxq hierarchical reinforcement learning differ values leaf irrelevance probability transitions js js need expected reward performing states 
maxq visits state know value part state abstracted away 
draws sample jx receives reward jx updates estimate line maxq 
probability maxq visiting part state line maxq computing stochastic approximation jx jx write jx jx leaf irrelevance inner sum value states call value 
gives equal distribution 
maxq converges leaf irrelevance abstractions 
turn forms abstraction apply internal nodes max node irrelevance result distribution irrelevance 
consider smdp defined node abstracted maxq graph time maxq 
ordinary smdp transition probability function jx reward function maxq draws samples state transitions drawn distribution js original state space 
prove theorem show drawing second distribution equivalent drawing distribution 
max node irrelevance know policies applied node descendents transition probability distribution factors js jx jx exploration policy policy js factors way 
means components state affect components sampling js discarding values gives samples jx 
maxq converge max node irrelevance abstractions 
consider result distribution irrelevance 
child node suppose set state variables irrelevant result distribution smdp node wishes draw sample jx know current value irrelevant part current state 
matter result distribution irrelevance means possible values jx 
maxq converge result distribution irrelevance abstractions 
dietterich cases maxq converge locally optimal ordered policy node maxq graph 
lemma produces locally optimal ordered policy smdp node induction maxq converge unique ordered recursively optimal policy defined maxq mdp ordered exploration policy hierarchical credit assignment problem situations introduce state abstractions properties described permit 
consider modification taxi problem 
suppose taxi fuel tank time taxi moves square costs unit fuel 
taxi runs fuel delivering passenger destination receives reward gamma trial ends 
fortunately filling station taxi execute action fill fuel tank 
solve modified problem maxq hierarchy introduce subtask goal moving taxi filling station filling tank 
child invokes navigate bound location filling station move taxi filling station 
fuel possibility run fuel means include current amount fuel feature representing value internal nodes value leaf nodes maxq graph 
unfortunate intuition tells amount fuel influence decisions inside navigate subtask 
taxi fuel reach target case chosen navigation actions depend fuel taxi fuel fail reach regardless navigation actions taken 
words navigate subtask need worry amount fuel fuel action navigate take get fuel 
top level subtasks monitoring amount fuel deciding go go pick passenger go deliver passenger 
intuition natural try abstracting away amount remaining fuel navigate subtask 
doesn taxi runs fuel gamma reward nodes explain reward received consistent way setting tables predict negative reward occur values ignore amount fuel tank 
stated formally difficulty max node irrelevance condition satisfied step reward function js actions depends amount fuel 
call hierarchical credit assignment problem 
fundamental issue maxq decomposition information rewards stored leaf nodes hierarchy 
separate basic rewards received navigation gamma action reward received exhausting fuel gamma 
reward leaves depend location taxi max node irrelevance condition satisfied 
maxq hierarchical reinforcement learning way programmer manually decompose reward function indicate nodes hierarchy receive reward 
js js decomposition reward function js specifies part reward handled max node modified taxi problem example decompose reward leaf nodes receive original penalties fuel rewards handled 
lines maxq algorithm easily modified include js 
domains believe easy designer hierarchy decompose reward function 
straightforward problems studied 
interesting problem research develop algorithm solve hierarchical credit assignment problem autonomously 

non hierarchical execution maxq hierarchy point focused exclusively representing learning hierarchical policies 
optimal policy mdp strictly hierarchical 
kaelbling introduced idea deriving non hierarchical policy value function hierarchical policy 
section exploit maxq decomposition generalize ideas apply recursively levels hierarchy 
describe methods non hierarchical execution 
method dynamic programming algorithm known policy iteration 
policy iteration algorithm starts initial policy repeats steps policy converges 
policy evaluation step computes value function current policy policy improvement step computes new policy rule argmax js js howard proved optimal policy guaranteed improvement 
note order apply method need know transition probability distribution js reward function js 
know js js maxq representation value function perform step policy iteration 
start hierarchical policy represent value function maxq hierarchy learned maxq 
perform step policy improvement applying equation computed maxq hierarchy compute 
corollary argmax js js value function computed maxq hierarchy primitive action 
optimal policy strictly better state proof direct consequence howard policy improvement theorem 
unfortunately iterate policy improvement process new policy hierarchical policy representable dietterich table procedure executing step greedy policy 
procedure repeat hv ai execute primitive action resulting state terms local policies node maxq graph 
step policy improvement give significant improvements 
approach non hierarchical execution ignores internal structure maxq graph 
effect maxq hierarchy just viewed way represent representation give step improved policy second approach non hierarchical execution borrows idea learning 
great representation value functions compute step policy improvement knowing js simply new policy argmax 
gives step greedy policy computed step lookahead 
maxq decomposition perform policy improvement steps levels hierarchy 
defined function need 
table function current state conducts search paths max node leaves maxq graph finds path best value maximum sum values path plus value leaf 
equivalent computing best action greedily level maxq graph 
addition returns primitive action best path 
action primitive action executed learned hierarchical policy executed starting current state second method non hierarchical execution maxq graph call state execute primitive action returned 
pseudo code shown table 
call policy computed hierarchical greedy policy denote hg superscript indicates computing greedy action time step 
theorem shows give better policy original hierarchical policy 
theorem maxq graph representing value function hierarchical policy terms computed 
hg value computed line hg resulting policy 
define hg value function hg states case hg hg proof sketch left inequality equation satisfied construction line 
see consider original hierarchical policy maxq hierarchical reinforcement learning viewed choosing path maxq graph running root leaf nodes sum values chosen path plus value leaf node 
contrast performs traversal paths maxq graph finds best path path largest sum leaf values 
hg large 
establish right inequality note construction hg value function policy call hg chooses action greedily level maxq graph recursively follows 
consequence fact line right hand side represents cost completing subroutine policy 
table written execute execute hg opportunity improve hg time step 
hg underestimate actual value hg note theorem works direction 
says find state hg greedy policy hg strictly better optimal policy structure maxq graph prevents considering action primitive composite improve policy improvement theorem howard primitive actions eligible chosen guarantee suboptimal hierarchically greedy policy strict improvement 
contrast perform step policy improvement discussed start section corollary guarantees improve policy 
see general methods non hierarchical execution better 
method operates level individual primitive actions able produce large improvements policy 
contrast hierarchical greedy method obtain large improvements policy changing actions subroutines chosen near root hierarchy 
general hierarchical greedy execution probably better method 
course value functions methods computed better estimated value executed 
sutton 
simultaneously developed closely related method nonhierarchical execution macros 
method equivalent special case maxq hierarchy level subtasks 
interesting aspect permits greedy improvements levels tree influence action chosen 
care taken applying theorem maxq hierarchy values learned maxq 
online algorithm maxq correctly learned values states nodes maxq graph 
example taxi problem value put learned special locations put subtask executed passenger taxi usually means get just completed taxi passenger source location 
exploration children put tried states 
putdown usually fail receive negative reward navigate eventually succeed lengthy exploration dietterich take taxi destination location 
states updating values put navigate learned states path passenger destination values putdown action learned passenger source destination locations 
train maxq representation hierarchical execution maxq switch hierarchically greedy execution results quite bad 
particular need introduce execution early exploration policy actively exploring 
theory glie exploration policy ceases explore practice want find policy quickly just asymptotically 
course alternative hierarchically greedy execution learning 
remember higher nodes maxq hierarchy need obtain samples js child action hierarchical greedy execution interrupts child reached terminal state state way subtask appears better samples obtained 
important purely hierarchical execution training transition greedy execution point 
approach taken implement maxq way specify number primitive actions taken hierarchically hierarchical execution interrupted control returns top level new action chosen greedily 
start set large execution completely hierarchical child action invoked committed execute action terminates 
gradually reduce point hierarchical greedy execution 
time reaches time boltzmann exploration temperature exploration effectively halted 
experimental results show generally gives excellent results little added exploration cost 

experimental evaluation maxq method performed series experiments maxq method goals mind understand expressive power value function decomposition characterize behavior maxq learning algorithm assess relative importance temporal abstraction state abstraction non hierarchical execution 
section describe experiments results 
fickle taxi task experiments performed modified version taxi task 
version incorporates changes task described section 
navigation actions noisy probability moves intended direction probability moves right intended direction probability moves left 
purpose change create realistic difficult challenge learning algorithms 
second change taxi picked passenger moved square away passenger source location passenger changes destination location probability 
maxq hierarchical reinforcement learning purpose change create situation optimal policy hierarchical policy effectiveness non hierarchical execution measured 
compared different configurations learning algorithm flat learning maxq learning form state abstraction maxq learning state abstraction maxq learning state abstraction greedy execution 
configurations controlled parameters 
include initial values functions learning rate employed fixed learning rate cooling schedule boltzmann exploration glie policy employed non hierarchical execution schedule decreasing number steps consecutive hierarchical execution 
optimized settings separately configuration goal matching exceeding primitive training actions possible best policy code hand 
boltzmann exploration established initial temperature cooling rate 
separate temperature maintained max node maxq graph temperature reduced multiplying cooling rate time subtask terminates goal state 
process optimizing parameter settings algorithm time consuming flat learning maxq 
critical parameter schedule cooling temperature boltzmann exploration cooled rapidly algorithms converge suboptimal policy 
case tested different cooling rates 
choose different cooling rates various subtasks started fixed policies random hand coded subtasks subtasks closest leaves 
chosen schedules subtasks allowed parent tasks learn policies tuned cooling rates 
nice effect method cooling temperature subtask terminates naturally causes subtasks higher maxq graph cool slowly 
meant results obtained just cooling rate max nodes 
choice learning rate easier determined primarily degree stochasticity environment 
tested different rates configuration 
initial values functions set knowledge problems experiments required 
took care tuning parameters experiments normally take real application wanted ensure method compared best possible conditions 
general form results particularly speed learning wide ranges cooling rate learning rate parameter settings 
parameters selected tuning experiments 
flat learning initial values states learning rate boltzmann exploration initial temperature cooling rate 
initial values signature debugging detect weight modified 
maxq learning state abstraction initial values learning rate boltzmann exploration initial temperature cooling rates 
dietterich mean cumulative reward primitive actions maxq maxq maxq flat greedy comparison performance hierarchical maxq learning state abstractions state abstractions state abstractions combined hierarchical greedy evaluation flat learning 
maxq learning state abstraction initial values learning rate boltzmann exploration initial temperature cooling rates 
maxq learning non hierarchical execution settings state abstraction 
addition initialized decreased trial reached 
trials execution completely greedy 
shows averaged results training runs 
training run involves performing repeated trials convergence 
different trials execute different numbers primitive actions just plotted number primitive actions horizontal axis number trials 
thing note forms maxq learning better initial performance flat learning 
constraints introduced maxq hierarchy 
example agent executing navigate subtask attempt pickup putdown passenger actions available navigate 
similarly agent attempt putdown passenger picked passenger vice versa termination conditions get put subtasks 
second thing notice state abstractions maxq learning takes longer converge flat curve crosses maxq abstraction maxq hierarchical reinforcement learning curve 
shows state abstraction cost learning huge number parameters maxq representation really worth benefits 
suspect consequence model free nature maxq algorithm 
maxq decomposition represents information redundantly 
example cost performing put subtask computed root get put 
model algorithm compute learned model maxq learn separately experience 
third thing notice state abstractions maxq converges quickly hierarchically optimal policy 
seen clearly focuses range reward values neighborhood optimal policy 
see maxq abstractions attains hierarchically optimal policy approximately steps flat learning requires roughly twice long reach level 
flat learning course continue onward reach optimal performance maxq hierarchy best hierarchical policy slow respond fickle behavior passenger changes destination 
thing notice greedy execution maxq policy able attain optimal performance 
execution greedy temporary drop performance maxq learn values new regions state space visited recursively optimal policy 
despite drop performance greedy maxq recovers rapidly reaches hierarchically optimal performance faster purely hierarchical maxq learning 
added cost terms exploration introducing greedy execution 
experiment presents evidence favor claims hierarchical reinforcement learning faster flat learning second state abstraction required maxq learning performance third non hierarchical execution produce significant improvements performance little added exploration cost 
kaelbling hdg method second task consider simple maze task introduced leslie kaelbling shown 
trial task agent starts state move randomly chosen goal state usual north south east west operators employed deterministic operators 
small cost move agent minimize undiscounted sum costs 
goal state different locations different mdps 
kaelbling hdg method starts choosing arbitrary set landmark states defining voronoi partition state space manhattan distances landmarks states belong voronoi cell iff nearest landmark 
method defines subtask landmark subtask move state current voronoi cell neighboring voronoi cell landmark optimal policies subtasks computed 
hdg policies subtasks solve markov decision problem moving landmark state landmark state subtask solutions macro actions subroutines 
computes value function mdp 
dietterich mean cumulative reward primitive actions optimal policy hier optimal policy maxq greedy maxq flat maxq close view previous 
shows horizontal lines indicating optimal performance hierarchically optimal performance domain 
readable applied step moving average data points average runs 
possible destination location voronoi cell landmark hdg method computes optimal policy getting combining subtasks hdg method construct approximation optimal policy follows 
addition value functions discussed agent maintains functions nl name landmark nearest state list landmarks cells immediate neighbors cell combining agent build list state current landmark landmarks neighboring cells 
landmark agent computes sum terms expected cost reaching landmark expected cost moving landmark landmark goal cell expected cost moving goal cell landmark goal state 
note terms exact estimates term computed landmark subtasks subroutines 
means corresponding path pass intermediate landmark states going directly goal landmark 
maxq hierarchical reinforcement learning kaelbling navigation task 
circled state landmark state heavy lines show boundaries voronoi cells 
episode start state goal state chosen random 
start state shown black square goal state shown black hexagon 
term typically overestimate required distance 
note choices intermediate landmarks need explicitly included computation best action agent enters cell containing goal 
information agent chooses move best landmarks agent goal voronoi cell case agent moves goal state 
example term cost reaching landmark row column 
term cost getting row column landmark row column going landmark 
case best landmark landmark path go directly row column row column 
term 
term cost getting row column goal 
sum 
comparison optimal path length 
kaelbling experiments employed variation learning learn terms computed regular intervals floyd warshall sources shortest paths algorithm 
shows maxq approach solving problem 
task root takes argument specifies goal cell 
subtasks dietterich gl gl nl gl north south east west gl maxq graph hdg navigation task 
ffl go landmark nearest goal location 
termination predicate subtask true agent reaches landmark nearest goal 
goal predicate termination predicate 
ffl go landmark termination predicate true agent reaches landmark agent outside region defined voronoi cell neighboring voronoi cells 
goal predicate subtask true condition 
ffl go goal location termination predicate subtask true agent goal location agent outside voronoi cell nl contains goal predicate subtask true agent goal location 
maxq hierarchical reinforcement learning maxq decomposition essentially kaelbling method somewhat redundant 
consider state agent inside voronoi cell goal states hdg decomposes value function terms 
similarly maxq decomposes terms ffl cost getting landmark represented sum 
ffl gl cost getting landmark landmark gl nearest goal 
ffl root gl cost getting goal location reaching gl cost completing root task reaching gl 
agent inside goal voronoi cell hdg maxq store essentially information 
hdg stores maxq breaks terms sums quantities compute value 
note maxq decomposition stores information twice specifically cost getting goal landmark gl goal stored root gl 
compare amount memory required flat learning hdg maxq 
locations possible actions possible goal states flat learning store values 
compute quantity hdg store values actions state respect landmark landmarks nl 
gives total values stored 
compute quantity hdg store landmark information shortest path landmark 
landmarks 
consider landmark row column 
neighboring landmarks constitute macro actions agent perform move landmark 
nearest landmark goal cell landmarks gives total values stored 
similar computations landmarks give total values stored 
compute quantity hdg store information square inside voronoi cell get squares inside voronoi cell 
requires values 
grand total hdg huge savings flat learning 
consider maxq hierarchy state abstractions 
ffl expected reward primitive action state 
states primitive actions requires values 
reward constant gamma apply leaf irrelevance store single value 
ffl primitive actions 
requires amount space kaelbling representation combined represents exactly information 
requires values 
state abstractions applied 
dietterich ffl gl cost completing task going landmark primitive actions deterministic terminate location need store pair gl 
exactly kaelbling quantity requires values 
primitive actions stochastic kaelbling original store value possible terminal state action 
actions terminate target landmark states bordering set voronoi cells neighbors cell requires values 
kaelbling stores values effectively making assumption fail reach landmark approximation introduce maxq representation choice state abstraction node 
ffl cost completing task executing primitive actions quantity hdg representation requires amount space values 
ffl root cost reaching goal reached landmark nearest goal 
maxq represent combinations goal landmarks goals 
requires values 
note values values primitive actions 
means maxq representation stores information twice hdg representation stores term 
ffl root 
cost completing root task executed task 
primitive action deterministic zero reached goal 
apply termination condition store values 
primitive actions stochastic store value possible state borders voronoi cell contains goal 
requires different values 
kaelbling hdg representation value function ignoring probability terminate non goal state 
maxq exact representation value function ignore possibility 
incorrectly apply termination condition case maxq representation function approximation 
stochastic case state abstractions maxq representation requires values 
safe state abstractions requires values 
approximations employed kaelbling equivalently primitive actions deterministic maxq representation state abstractions requires values 
numbers summarized table 
see unsafe state abstractions maxq representation requires slightly space hdg representation redundancy storing root 
example shows hdg task start fully general formulation provided maxq impose assumptions obtain method similar hdg 
maxq formulation guarantees value function hierarchical policy represented exactly 
assumptions introduce approximations maxq hierarchical reinforcement learning table comparison number values stored represent value function hdg maxq methods 
hdg maxq hdg maxq maxq maxq item item values abs safe abs unsafe abs root root total number values required value function representation 
useful general design methodology building application specific hierarchical representations 
long term goal develop methods new application require inventing new set techniques 
shelf tools maxq specialized imposing assumptions state abstractions produce efficient special purpose systems 
important contributions hdg method introduced form non hierarchical execution 
soon agent crosses voronoi cell current subtask reaching landmark cell interrupted agent recomputes current target landmark 
effect reaches goal voronoi cell agent aiming landmark outside current voronoi cell 
agent aims sequence landmark states typically visit states way goal 
states just provide convenient set intermediate targets 
shortcuts hdg compensates fact general overestimated cost getting goal computed value function policy agent goes landmark 
effect obtained hierarchical greedy execution maxq graph directly inspired hdg method 
note storing nl nearest landmark function hdg method detect efficiently current subtask interrupted 
technique works navigation problems space distance metric 
contrast performs kind polling checks primitive action interrupt current subroutine invoke new 
important goal research maxq find general purpose mechanism avoiding unnecessary polling mechanism discover efficiently evaluable interrupt conditions 
shows results experiments hdg maxq learning algorithm 
employed parameters flat learning initial values learning rate initial temperature cooling rate maxq state abstractions initial values gamma learning rate initial dietterich mean cumulative reward primitive actions flat maxq maxq comparison flat learning maxq learning state abstraction 
average runs 
temperature cooling rates maxq state abstractions initial values gamma learning rate initial temperature cooling rates 
hierarchical greedy execution introduced starting primitive actions trial reducing trial actions trials execution completely greedy 
confirms observations experiments fickle taxi task 
state abstractions maxq converges slowly flat learning 
state abstractions converges roughly times fast 
shows close view allows compare differences final levels performance methods 
see maxq state abstractions able reach quality hand coded hierarchical policy presumably exploration required achieve state abstractions maxq able slightly better hand coded policy 
hierarchical greedy execution maxq able reach goal fewer action average approaches performance best hierarchical greedy policy computed value iteration 
notice best performance obtained hierarchical greedy execution best recursively optimal policy match optimal performance 
flat maxq hierarchical reinforcement learning mean cumulative reward primitive actions optimal policy hierarchical greedy optimal policy hierarchical hand coded policy flat maxq maxq maxq greedy expanded view comparing flat learning maxq learning state abstraction hierarchical greedy execution 
average runs 
learning achieves policy reaches goal state average fewer primitive action 
notice taxi domain added exploration cost shifting greedy execution 
kaelbling hdg extended generalized moore baird kaelbling sparse mdp task get start state desired goal state 
key success approach landmark subtask guaranteed terminate single resulting state 
possible identify sequence intermediate landmark states assemble policy visits sequence 
moore baird kaelbling show construct hierarchy landmarks airport hierarchy planning process efficient 
note subtask terminate single state general mdps airport method combinatorial explosion potential intermediate states need considered 
parr russell hierarchies machines dissertation ron parr considered approach hierarchical reinforcement learning programmer encodes prior knowledge form hierarchy finite state controllers called ham hierarchy machines 
hierarchy dietterich goal vertical hallway horizontal hallway intersection parr maze problem left 
start state upper left corner states lower right hand room terminal states 
smaller diagram right shows hallway intersection structure maze 
executed procedure call return discipline provides partial policy task 
policy partial machine include non deterministic choice machine states machine lists options action specify chosen 
programmer puts choice states point know action performed 
partial policy parr goal find best policy making choices choice states 
words goal learn hierarchical value function hs mi state external environment contains internal state hierarchy contents procedure call stack values current machine states machines appearing stack 
key observation necessary learn value function choice states hs mi 
parr algorithm learn decomposition value function 
flattens hierarchy create new markov decision problem choice states hs mi 
hierarchical primarily sense programmer structures prior knowledge hierarchically 
advantage parr method find optimal hierarchical policy subject constraints provided programmer 
disadvantage method executed non hierarchically produce better policy 
parr illustrated maze shown 
maze large scale structure series hallways intersections small scale structure series obstacles avoided order move hallways intersections 
maxq hierarchical reinforcement learning trial agent starts top left corner move state bottom right corner room 
agent usual primitive actions north south east west 
actions stochastic probability succeed probability action move left probability action move right north action move east probability west probability 
action collide wall obstacle effect 
maze structured series rooms containing block states various obstacles 
rooms parts hallways connected rooms opposite sides 
rooms intersections hallways meet 
test representational power maxq hierarchy want see represent prior knowledge parr able represent ham 
describing parr ham maze task maxq hierarchy captures prior knowledge 
parr top level machine consists loop single choice state chooses possible child machines east south est north 
loop terminates agent reaches goal state 
invoke particular machine hallway specified direction 
start state consider south east 
machine begins executing agent intersection 
thing tries exit intersection hallway specified direction attempts traverse hallway reaches intersection 
invoking machine 
machine returns invokes machine 
machine returns returns 
machines identical termination conditions 
machines consist loop choice state chooses possible subroutines 
simplify description suppose east chosen east 
possible subroutines east north east south east north east south 
machine moves direction encounters wall part obstacle part walls maze 
moves perpendicular direction reaches wall 
wall ways agent trapped corner walls directions longer wall direction case machine terminates second case resumes moving direction machine moves step backwards direction opposite moves steps direction moves may may succeed actions stochastic may walls blocking way 
actions carried case machine returns 
machines terminate reach hall intersection 

author ron parr providing details ham task 
dietterich finite state controllers define highly constrained partial policy 
machines contain choice states 
choice points choose direction move decide call call perpendicular direction tell machines try move forward 
inv go room maxq graph parr maze task 
shows maxq graph encodes similar set constraints policy 
subtasks defined follows maxq hierarchical reinforcement learning ffl root 
exactly machine 
choose direction invoke go 
terminates agent enters terminal state 
goal condition course 
ffl go 
go direction leaving room 
parameter bound identification number corresponding current room agent located 
go terminates agent enters room hallway direction leaves desired hallway wrong direction 
goal condition go satisfied agent reaches desired intersection 
ffl 
terminates agent exited room goal condition agent exit room direction ffl 
terminates agent exited current hall intersection 
goal condition agent entered desired intersection direction ffl sniff 
encodes subtask equivalent machine 
sniff child subtasks simply internal states 
necessary subtask maxq framework contain internal state finite state controller ham representation contain internal states necessary 
particular state moving forward state wall sideways 
ffl 
equivalent part 
terminates wall front agent direction goal condition termination condition 
ffl 
equivalent part 
moves direction wall direction ends stuck corner walls directions 
goal condition termination condition 
ffl back 
attempts encode information machine case maxq hierarchy capture information 
simply executes sequence primitive actions step back steps direction 
internal states maxq allow 
back subtask subgoal moving agent square backwards squares direction order determine achieved subgoal remember position started execute bound parameters back 
back terminates achieves desired change position runs walls prevent achieving subgoal 
goal condition termination condition 
ffl 
moves agent step backwards direction opposite needs starting position order tell succeeded 
terminates moved unit direction wall direction 
goal condition termination condition 
dietterich ffl 
moves agent steps direction needs starting positions order tell succeeded 
terminates moved units direction wall direction 
goal condition termination condition 
ffl move 
parameterized primitive action 
executes primitive move direction terminates immediately 
see major differences maxq representation ham representation 
ham finite state controller contain internal states 
convert maxq subtask graph separate subtask internal state ham 
second ham terminate amount effort performing actions maxq subtask terminate change state world 
impossible define maxq subtask performs steps terminate regardless effects steps adding kind counter state mdp 
third difficult formulate termination conditions maxq subtasks ham machines 
example ham necessary specify machine terminates entered different intersection executed 
important maxq method maxq subtask learns value function policy independent parent tasks 
example requirement enter different intersection learning algorithms maxq prefer take step backward return room go action started easier terminal state reach 
problem arise ham approach policy learned subtask depends flattened hierarchy machines returning state go action started help solve problem reaching goal state lower right corner 
construct maxq graph problem introduced programming tricks binding parameters aspects current state order serve kind local memory subtask began executing having parameterized primitive action order able pass parameter value specifies primitive action perform employing inheritance termination conditions subtask maxq graph inherits termination conditions ancestor tasks 
agent middle executing action leaves intersection subroutine terminates termination condition satisfied 
behavior similar standard behavior maxq 
ordinarily ancestor task terminates descendent tasks forced return updating values 
inheritance termination conditions hand descendent tasks forced terminate updating values 
words termination condition child task logical termination conditions ancestors plus termination condition 
inheritance easier write maxq graph parents need pass children information necessary children define complete termination goal predicates 
maxq hierarchical reinforcement learning essentially opportunities state abstraction task irrelevant features state 
opportunities apply shielding termination properties 
particular guaranteed cause parent task terminate require stored values 
states subtasks terminated go east state wall east side room values need stored 
applying state elimination conditions maxq representation task requires space flat representation 
exact computation difficult applying maxq learning maxq representation required values flat learning requires fewer values 
parr states method requires values 
test relative effectiveness maxq representation compare maxq learning flat learning 
large negative values states acquire particularly early phases learning unable get boltzmann exploration bad experience cause action receive low value tried 
experimented ffl greedy exploration counter exploration 
ffl greedy exploration policy ordered glie policy random action chosen probability ffl ffl gradually decreased time 
counter exploration policy keeps track times action executed state choose action state selects action executed fewest times actions executed times 
switches greedy execution 
genuine glie policy 
parr employed counter exploration policies experiments task 
domains conducted experimental runs testing boltzmann ffl greedy counter exploration determine best parameters algorithm 
flat learning chose parameters learning rate ffl greedy exploration initial value ffl ffl decreased successful execution max node initial values gamma 
maxq learning chose parameters counter exploration learning rate equal reciprocal number times action performed initial values values selected carefully provide underestimates true values 
example initial values gamma worst case completing task takes steps complete subsequent task complete go parent task 
performance quite sensitive initial values potential drawback maxq approach 
plots results 
see maxq learning converges times faster flat learning 
know maxq converged recursively optimal policy 
comparison show performance hierarchical policy coded hand hand coded policy knowledge contextual information choose operators policy surely better best recursively optimal policy 
learning converge policy equal slightly better hand coded policy 
experiment demonstrates maxq representation capture prior knowledge represented hierarchy 
dietterich mean reward trial primitive steps maxq learning flat learning hand coded hierarchical policy comparison flat learning maxq learning parr maze task 
shows maxq representation requires care design goal conditions subtasks 
domains addition domains discussed developed maxq graphs singh flag task treasure hunter task described tadepalli dietterich dayan hinton feudal learning task 
tasks easily naturally placed maxq framework fit easily parr russell maze task 
maxq able exactly duplicate singh decomposition value function exactly amount space represent value function 
maxq duplicate results tadepalli dietterich maxq explanation method considerably slower requires substantially space represent value function 
feudal task maxq able give better performance feudal learning 
reason feudal learning subroutine decisions function learned level hierarchy information estimated costs actions descendents 
contrast maxq value function decomposition permits max node decisions sum completion function costs estimated descendents 
course maxq maxq hierarchical reinforcement learning supports non hierarchical execution possible feudal learn value function decomposition 

discussion concluding wish discuss issues design tradeoffs hierarchical reinforcement learning methods automatically learning improving maxq hierarchies 
design tradeoffs hierarchical reinforcement learning discussed issues concerning design hierarchical reinforcement learning architectures method defining subtasks state abstraction non hierarchical execution design learning algorithms 
subsection want highlight tradeoff issues 
maxq defines subtasks termination predicate pseudo reward function drawbacks method 
hard programmer define correctly essentially requires guessing value function optimal policy mdp states subtask terminates 
second leads seek recursively optimal policy hierarchically optimal policy 
recursively optimal policies may worse hierarchically optimal ones may giving substantial performance 
return drawbacks maxq obtains important benefit policies value functions subtasks context free 
words depend parent tasks larger context invoked 
understand point consider mdp shown 
clear optimal policy exiting left hand room exit subtask depends location goal 
top right hand room agent prefer exit upper door bottom right hand room agent prefer exit lower door 
define subtask exiting left hand room pseudo reward zero doors obtain policy optimal case policy re cases 
furthermore policy depend location goal 
apply max node irrelevance solve exit subtask location robot ignore location goal 
example shows obtain benefits subtask reuse state abstraction define subtask termination predicate pseudo reward function 
termination predicate pseudo reward function provide barrier prevents communication value information exit subtask context 
compare parr ham method 
algorithm finds best policy consistent hierarchy 
achieve permit information propagate exit subtask exit finite state controller environment 
means state reached leaving exit subtask different values depending location goal different values propagate back exit subtask 
represent different values exit subtask know dietterich location goal 
short achieve hierarchically optimal policy exit subtask general represent value function entire state space 
state abstractions employed losing hierarchical optimality 
see direct tradeoff achieving hierarchical optimality employing state abstractions 
methods hierarchical optimality freedom defining subtasks partial policies ham approach 
safely employ state abstractions subtasks general reuse solution subtask multiple contexts 
methods recursive optimality hand define subtasks method pseudo reward functions maxq fixed policies options framework isolates subtask context 
return apply state abstraction learned policy reused contexts optimal 
interesting iterative method described dean lin viewed method moving tradeoff 
dean lin method programmer initial guess values terminal states subtask doorways 
initial guess locally optimal policies subtasks computed 
locally optimal policy parent task computed holding subtask policies fixed treating options 
point algorithm computed recursively optimal solution original problem initial guesses 
solving various subproblems sequentially offline algorithm dean lin suggested maxq learning algorithm 
method dean lin 
computes new values terminal states subtask learned value function entire problem 
allows update guesses values terminal states 
entire solution process repeated obtain new recursively optimal solution new guesses 
prove process iterated indefinitely converge hierarchically optimal policy provided course state abstractions subtasks 
suggests extension maxq learning adapts values online 
time subtask terminates update function computed value terminated state 
precise subtask terminates state update equal max 
represented full state subtask employing state abstractions need average value average taken states weighted probability visiting states 
easily accomplished performing stochastic approximation update form gamma ff ff time subtask terminates 
algorithm expected converge best hierarchical policy consistent state abstractions 
suggests problems may worthwhile learn recursively optimal policy aggressive state abstractions learned value function initialize maxq representation detailed representation states 
progressive refinements state space guided monitoring maxq hierarchical reinforcement learning degree values vary state large variance means state abstractions failing important distinctions values states refined 
kinds adaptive algorithms take longer converge basic maxq method described 
tasks agent solve times lifetime worthwhile learning algorithms provide initial useful solution gradually improve solution optimal 
important goal research find methods diagnosing repairing errors sub initial hierarchy ultimately optimal policy discovered 
automated discovery abstractions approach taken rely programmer design maxq hierarchy including termination conditions pseudo reward functions state abstractions 
results particularly concerning state abstraction suggest ways able automate construction hierarchy 
main purpose hierarchy create opportunities subtask sharing state abstraction 
closely related 
order subtask shared different regions state space case value function different regions identical additive offset 
maxq framework additive offset difference values parent task 
way find reusable subtasks look regions state space value function exhibits additive offsets 
second way search structure step probability transition function js 
subtask useful enables state abstractions max node irrelevance 
formulate problem identifying region state space conditioned region js factors equation 
top divide conquer algorithm similar decision tree algorithms able 
third way search funnel actions looking bottlenecks state space policies travel 
useful discovering cases result distribution irrelevance 
ways difficult kinds state abstractions discover arbitrary subgoals introduced constrain policy sacrifice optimality 
example algorithm automatically decide impose landmarks hdg task 
detecting large region state space bottlenecks variations reward function 
problem discovering hierarchies important challenge provided guidelines constitute state abstractions serve objective functions guiding automated search abstractions 

concluding remarks introduced new representation value function hierarchical reinforcement learning maxq value function decomposition 
proved maxq decomposition represent value function hierarchical policy dietterich finite horizon undiscounted cumulative reward criterion infinite horizon discounted reward criterion 
representation supports subtask sharing re value function decomposed value functions individual subtasks 
introduced learning algorithm maxq learning proved converges probability recursively optimal policy 
argued recursive optimality weaker hierarchical optimality global optimality important form optimality permits subtask learn locally optimal policy ignoring behavior ancestors maxq graph 
increases opportunities subtask sharing state abstraction 
shown maxq decomposition creates opportunities state abstraction identified set properties max node irrelevance leaf irrelevance result distribution irrelevance shielding termination allow ignore large parts state space subtasks 
proved maxq converges presence forms state abstraction showed experimentally state abstraction important practice successful application maxq learning taxi hdg tasks 
different methods deriving improved non hierarchical policies maxq value function representation formalized conditions methods improve hierarchical policy 
verified experimentally non hierarchical execution gives improved performance fickle taxi task achieves optimal performance hdg task gives substantial improvement 
argued tradeoff governing design hierarchical reinforcement learning methods 
design spectrum context free methods maxq learning 
provide support state abstraction subtask sharing learn recursively optimal policies 
spectrum context sensitive methods options framework early dean lin 
methods discover hierarchically optimal policies cases globally optimal policies drawback easily exploit state abstractions share subtasks 
great speedups enabled state abstraction argued context free approach preferred relaxed needed obtain improved policies 
author gratefully acknowledges support national science foundation number iri office naval research number air force office scientific research number spanish government program de en regimen de en espa na 
addition author indebted colleagues helping develop clarify ideas including leslie kaelbling bill langford wes rich sutton prasad tadepalli sebastian thrun 
particularly want eric chown encouraging study feudal reinforcement learning ron parr providing details ham machines sebastian thrun encouraging write single comprehensive 
andrew moore maxq hierarchical reinforcement learning action editor sets anonymous reviewers previous drafts suggestions careful reading improved 
bellman 

dynamic programming 
princeton university press 
bertsekas tsitsiklis 

neuro dynamic programming 
athena scientific belmont ma 
boutilier dearden goldszmidt 

exploiting structure policy construction 
proceedings fourteenth international joint conference artificial intelligence pp 

currie tate 

plan open planning architecture 
artificial intelligence 
dayan hinton 

feudal reinforcement learning 
advances neural information processing systems pp 

morgan kaufmann san francisco ca 
dean lin 

decomposition techniques planning stochastic domains 
tech 
rep cs department computer science brown university providence rhode island 
dietterich 

maxq method hierarchical reinforcement learning 
fifteenth international conference machine learning pp 

morgan kaufmann 
fikes hart nilsson 

learning executing generalized robot plans 
artificial intelligence 
forgy 

rete fast algorithm pattern object pattern match problem 
artificial intelligence 
hauskrecht meuleau kaelbling dean boutilier 

hierarchical solution markov decision processes macro actions 
proceedings fourteenth annual conference uncertainty artificial intelligence uai pp 
san francisco ca 
morgan kaufmann publishers 
howard 

dynamic programming markov processes 
mit press cambridge ma 
jaakkola jordan singh 

convergence stochastic iterative dynamic programming algorithms 
neural computation 
kaelbling 

hierarchical reinforcement learning preliminary results 
proceedings tenth international conference machine learning pp 
san francisco ca 
morgan kaufmann 
dietterich ar ari 

module reinforcement learning real robot 
machine learning 
knoblock 

learning abstraction hierarchies problem solving 
proceedings eighth national conference artificial intelligence pp 
boston ma 
aaai press 
korf 

macro operators weak method learning 
artificial intelligence 
lin 

reinforcement learning robots neural networks 
ph thesis carnegie mellon university department computer science pittsburgh pa moore baird kaelbling 

multi value functions efficient automatic action hierarchies multiple goal mdps 
proceedings international joint conference artificial intelligence pp 
san francisco 
morgan kaufmann 
parr 

flexible decomposition algorithms weakly coupled markov decision problems 
proceedings fourteenth annual conference uncertainty artificial intelligence uai pp 
san francisco ca 
morgan kaufmann publishers 
parr 

hierarchical control learning markov decision processes 
ph thesis university california berkeley california 
parr russell 

reinforcement learning hierarchies machines 
advances neural information processing systems vol 
pp 
cambridge ma 
mit press 
pearl 

probabilistic inference intelligent systems 
networks plausible inference 
morgan kaufmann san mateo ca 
rummery niranjan 

online learning connectionist systems 
tech 
rep cued tr cambridge university engineering department cambridge england 
sacerdoti 

planning hierarchy abstraction spaces 
artificial intelligence 
singh jaakkola littman ari 

convergence results single step policy reinforcement learning algorithms 
tech 
rep university colorado department computer science boulder appear machine learning 
singh 

transfer learning composing solutions elemental sequential tasks 
machine learning 
sutton singh precup ravindran 

improved switching temporally actions 
advances neural information processing systems vol 
pp 

mit press 
maxq hierarchical reinforcement learning sutton barto 

reinforcement learning 
mit press cambridge ma 
sutton precup singh 

mdps semi mdps learning planning representing knowledge multiple temporal scales 
tech 
rep university massachusetts department computer information sciences amherst ma 
appear artificial intelligence 
tadepalli dietterich 

hierarchical explanation reinforcement learning 
proceedings fourteenth international conference machine learning pp 
san francisco ca 
morgan kaufmann 
tambe rosenbloom 

investigating production system representations non combinatorial match 
artificial intelligence 
watkins 

learning delayed rewards 
ph thesis king college oxford 
reprinted mit press 
watkins dayan 

technical note learning 
machine learning 

