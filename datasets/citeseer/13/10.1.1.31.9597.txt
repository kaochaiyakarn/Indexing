article communicated steven nowlan erkki oja natural gradient works efficiently learning shun ichi amari riken frontier research program japan parameter space certain underlying structure ordinary gradient function represent steepest direction natural gradient 
information geometry calculating natural gradients parameter space perceptrons space matrices blind source separation space linear dynamical systems blind source deconvolution 
dynamical behavior natural gradient online learning analyzed proved fisher efficient implying asymptotically performance optimal batch estimation parameters 
suggests plateau phenomenon appears backpropagation learning algorithm multilayer perceptrons disappear serious natural gradient 
adaptive method updating learning rate proposed analyzed 
stochastic gradient method widrow amari rumelhart hinton williams popular learning method general nonlinear optimization framework 
parameter space euclidean riemannian metric structure cases 
cases ordinary gradient give steepest direction target function steepest direction natural contravariant gradient 
riemannian metric structures introduced means information geometry amari murray rice amari amari 
article gives natural gradients explicitly case space perceptrons neural learning space matrices blind source separation space linear dynamical systems blind multichannel source deconvolution 
extended version earlier article amari including new results 
natural gradient learning compared conventional gradient learning 
asymptotic behavior online natural gradient learning studied purpose 
training examples online learning appear 
asymptotic performance online learning better optimal batch procedure examples reused 
prove natural gradient online learning gives fisher efficient estimator sense neural computation massachusetts institute technology shun ichi amari asymptotic statistics loss function differentiable asymptotically equivalent optimal batch procedure see amari opper 
loss function nondifferentiable accuracy asymptotic online learning worse batch learning factor see example van den 
shown amari 
dynamic behavior natural gradient boltzmann machine excellent 
easy calculate natural gradient explicitly multilayer perceptrons 
preliminary analysis yang amari simple model shows performance natural gradient learning remarkably free trapped plateaus give rise slow convergence backpropagation learning method saad solla 
suggests riemannian structure eliminate plateaus serious 
online learning flexible track slow fluctuations target 
online dynamics analyzed amari researchers 
sompolinsky seung seung sompolinsky proposed adaptive method adjusting learning rate see amari 
generalize idea evaluate performance riemannian metric errors 
article organized follows 
natural gradient defined section 
section formulates natural gradient various problems stochastic descent learning 
section gives statistical analysis efficiency online learning section devoted problem adaptive changes learning rate 
calculations riemannian metric explicit forms natural gradients sections 
natural gradient parameter space function defined 
euclidean space orthonormal coordinate system squared length small incremental vector dw connecting dw dw components dw 
coordinate system squared length quadratic form dw gij 
curved manifold orthonormal linear coordinates length dw written equation 
space natural gradient works efficiently learning riemannian space 
show sections parameter spaces neural networks riemannian character 
matrix gij called riemannian metric tensor depends general reduces gij ij euclidean orthonormal case unit matrix case 
steepest descent direction function defined vector dw minimizes dw dw fixed length constraint dw sufficiently small constant 
theorem 
steepest descent direction riemannian space ij inverse metric gij conventional gradient wn superscript denoting transposition 
proof 
put dw search minimizes dw constraint 
lagrangean method ai ta ga 
shun ichi amari gives ga determined constraint 
call natural gradient riemannian space 
represents steepest descent direction 
tensorial notation contravariant form space euclidean coordinate system orthonormal 
suggests natural gradient descent algorithm form wt wt wt learning rate determines step size 
natural gradient learning consider information source generates sequence independent random variables zt subject probability distribution 
random signals zt processed processor neural network set adjustable parameters loss function signal processed processor parameter risk function average loss denotes expectation respect learning procedure search optimal minimizes 
stochastic gradient descent learning method formulated general wt wt tc wt zt wt natural gradient works efficiently learning learning rate may depend suitably chosen positive definite matrix see amari 
natural gradient online learning method proposed put equal riemannian structure defined 
give number examples studied detail 
statistical estimation probability density function 
case statistical estimation assume statistical model problem obtain probability distribution approximates unknown density function best way estimate true obtain optimal approximation observed data 
typical loss function log 
expected loss log eq log hz hz entropy depending minimizing equivalent minimizing kullback leibler divergence log dz probability distributions 
true distribution written equivalent obtain maximum likelihood estimator riemannian structure parameter space statistical model defined fisher information rao amari log log gij wi wj component form 
invariant metric statistical model campbell amari 
learning equation see equation gives sequential estimator wt 
multilayer neural network 
consider multilayer feedforward neural network specified vector parameter wn parameter composed modifiable connection weights thresholds 
input applied network processes calculates outputs 
input subject unknown probability shun ichi amari distribution 
consider teacher network receiving generates corresponding output subject conditional probability distribution 
task obtain optimal examples student network approximates behavior teacher 
denote loss input signal processed network having parameter typical loss output teacher 
consider statistical model neural networks output noisy version multivariate gaussian noise zero mean unit covariance matrix putting input output pair model specifies probability density cq exp normalizing constant loss function see equation rewritten const log log 
sequence examples xt natural gradient online learning algorithm written wt wt xt wt 
information geometry amari shows riemannian structure parameter space multilayer networks fisher information matrix log gij wi wj 
show calculate gij inverse section 
blind separation sources 
consider signal sources produce independent signals si discrete times 
assume si independent different times natural gradient works efficiently learning expectations si 
joint probability density function written product form 
consider case direct access source signals observe instantaneous mixtures xi aij nonsingular mixing matrix depend xm observed mixtures 
blind source separation problem recovering original signals observed signals jutten 
know trivial 
blind implies know mixing matrix probability distribution densities ri si 
typical algorithm solve problem transform wtx wt estimate modified learning equation wt wt tf xt wt 
special matrix function satisfying density functions equation wt equation converge equation necessary sufficient stability equilibrium considered 
operator maps matrix matrix 
shun ichi amari satisfies equation 
equilibrium stability different 
natural gradient alter stability equilibrium positive definite 
loss function expectation target function minimized typical function obtained gradient respect 
obtained heuristic arguments 
amari cardoso press gave complete family satisfying equation elucidated statistical efficiency related algorithms 
statistical point view problem estimate observed data 
probability density function written px wx specified estimated unknown function form 
statistical model said semiparametric difficult problem solve bickel klassen ritov wellner includes unknown function infinite degrees freedom 
apply information geometrical theory estimating functions amari problem 
gradient loss function see equation gradient respect matrix natural gradient 
operator transforming matrix matrix matrix 
metric space gl nonsingular matrices 
give explicit form section lie group structure 
inverse explicitly 
important problem stability equilibrium learning dynamics 
solved riemannian structure amari chen press see cardoso laheld 
algorithms proved amari certain conditions 
blind source deconvolution 
original signals mixed instantaneously past signals prob natural gradient works efficiently learning lem called blind source deconvolution equalization 
introducing time delay operator mixing matrix filter denoted ak matrices 
observed mixtures aks 
recover original independent sources finite impulse response model degree original signals recovered adaptively modified xt xt wt 
xt xt loss function includes past signals 
summarize past signals current state variable online learning algorithm 
loss function obtained maximum entropy method bell sejnowski independent component analysis comon statistical likelihood method 
order obtain natural gradient learning algorithm xt xt wt need define riemannian metric space matrix filters multiterminal linear systems 
study initiated amari 
possible define obtain explicitly see section 
preliminary investigation performance natural gradient learning algorithm undertaken douglas amari amari 

shun ichi amari natural gradient gives fisher efficient online learning algorithms section studies accuracy natural gradient learning statistical point view 
statistical estimator gives asymptotically best result said fisher efficient 
prove natural gradient learning attains fisher efficiency 
consider multilayer perceptrons example 
study case realizable teacher behavior teacher 
dt xt independent input output examples generated teacher network having parameter minimizing log loss log training data dt obtain wt minimizes training error train xt yt 
equivalent maximizing likelihood xt 
wt maximum likelihood estimator 
cram rao theorem states expected squared error unbiased estimator satisfies wt wt inequality holds sense positive definiteness matrices 
estimator said efficient fisher efficient satisfies equation equality large maximum likelihood estimator fisher efficient implying best estimator attaining cram rao bound asymptotically lim te wt wt inverse fisher information matrix gij defined equation 
examples time case online learning 
wt online estimator time time estimator wt modified give new estimator wt current observation xt 
old observations xt reused obtain wt learning rule written wt xt wt 
natural gradient works efficiently learning process wt markovian 
learning rule chosen behavior estimator wt better optimal batch estimator wt restriction 
gradient online learning rule wt wt tc xt yt wt proposed positive definite matrix dynamical behavior studied amari learning constant fixed 
heskes kappen obtained similar results ignited research online learning 
satisfies condition say positive constant stochastic approximation guarantees wt consistent estimator converging fisher efficient general 
arises question exists learning rule gives efficient estimator 
exists asymptotic behavior online learning equivalent best batch estimation method 
article answers question affirmatively giving efficient online learning rule see amari see opper 
consider natural gradient learning rule wt wt xt yt wt 
theorem 
learning rule see equation natural gradient online estimator wt fisher efficient 
proof 
denote covariance matrix estimator wt vt wt wt 
shows expectation squared error 
expand xt wt xt yt xt yt wt wt 
subtracting sides equation expectation square sides vt vt vt xt shun ichi amari xt wt wt converges guaranteed stochastic approximation certain conditions see kushner clark 
solution equation written asymptotically vt proving theorem 
theory extended applicable unrealizable teacher case order obtain efficient result optimal batch procedure 
locally equivalent newton raphson method 
results stated terms generalization error covariance estimator obtain universal results see amari amari murata 

cases blind source separation deconvolution models semiparametric including unknown function see equation 
cases cram rao bound necessarily hold 
theorem hold cases 
holds estimate true source probability density functions define loss function 
equation hold 
stability true solution necessarily guaranteed 
amari chen cichocki press analyzed situation proposed universal method attaining stability equilibrium solution 
adaptive learning constant dynamical behavior learning rule see equation studied amari small constant 
case wt fluctuates local optimal value large expected value variance wt studied trade convergence speed accuracy convergence demonstrated 
current wt far optimal desirable relatively large accelerate convergence 
close natural gradient works efficiently learning small preferred order eliminate fluctuations 
idea adaptive change discussed amari called learning learning rules sompolinsky 
see proposed rule adaptive change applicable pattern classification problem expected loss differentiable article generalizes idea general case differentiable analyzes behavior riemannian structure 
propose learning scheme wt wt xt yt wt exp xt yt wt constants 
assume training data generated realizable deterministic teacher holds optimal value 
see murata ller amari general case 
try analyze dynamical behavior learning continuous version algorithm sake simplicity dt wt tg wt xt yt wt dt xt zt wt 
order show dynamical behavior wt averaged version equations respect current input output pair xt yt 
averaged learning equation amari written dt wt tg wt wt dt wt denotes average current 
asymptotic evaluations wt wt wt wt wt wt 
dt wt wt shun ichi amari dt wt wt introduce squared error variable wt wt riemannian magnitude wt easy show dt dt 
behavior equations interesting 
origin attractor 
basin attraction boundary fractal structure 
anyway starting adequate initial value solution form coefficients determined gives ab ab 
proves convergence rate generalization error optimal order estimator wt converging adaptive shows nice characteristic target teacher slowly fluctuating changes suddenly 
natural gradient space perceptrons riemannian metric inverse calculated section obtain natural gradient explicitly 
analog simple perceptron input output behavior natural gradient works efficiently learning gaussian noise subject 
conditional probability density applied exp 
distribution inputs assumed normal distribution 
joint distribution 
order calculate metric equation explicitly put euclidean norm 
theorem 
theorem 
fisher information metric ww exp exp 
proof 
log log log wi log xi nf xi 
shun ichi amari fisher information matrix gij log wi log wj xixj taken account 
written form xx 
order show equation calculate quadratic form arbitrary gw 
subject subject 
noting exp confirms equation put arbitrary unit vector orthogonal euclidean sense 

independent subject exp 
natural gradient works efficiently learning equation determined quadratic forms proves equation 
obtain natural gradient necessary explicit form calculate explicitly perceptron case 
theorem 
inverse fisher information metric ww 
easily proved direct calculation gg natural gradient learning equation wt wt yt wt xt wt xt wt wt wt xt wt xt wt 
show geometrical characteristics parameter space perceptrons 
volume vn manifold simple perceptrons measured dw vn determinant gij represents volume density riemannian metric 
interesting see manifold perceptrons finite volume 
bayesian statistics considers randomly chosen subject prior distribution 
choice jeffrey prior noninformative prior 
vn jeffrey prior calculated follows 
theorem 
tively jeffrey prior volume manifold respec vn dw vn respectively area unit sphere 
shun ichi amari fisher metric calculated multilayer perceptrons 
consider multilayer perceptron having hidden units sigmoidal activation functions linear output unit 
input output relation vi wi conditional probability wm cexp vi wi 
total parameter consist wm 
calculate fisher information matrix consists blocks corresponding wi wi log nvi wi easily obtain block submatrix corresponding wi log log wi wi wi wi 
exactly simple perceptron case factor vi diagonal block log wi log wj wi wj 
case form coefficients cij dij calculated explicitly similar methods 
block wi block calculated similarly 
inversion easy simple cases 
requires inversion ofa dimensional matrix 
better direct inversion original dimensional matrix yang amari performed preliminary study performance natural gradient learning algorithm simple multilayer perceptron 
result shows natural gradient learning free plateau phenomenon 
learning trajectory trapped plateau takes long time get 
natural gradient works efficiently learning natural gradient space matrices blind source separation define riemannian structure space nonsingular matrices forms lie group denoted gl purpose introducing natural gradient learning rule blind source separation problem 
dw small deviation matrix dw 
tangent space tw gl linear space spanned small deviations called lie algebra 
need introduce inner product defining squared norm dw ds dw dw dw multiplying right mapped ww unit matrix dw mapped dw dx dx 
shows deviation dw equivalent deviation dx correspondence multiplication lie group invariance requires metric kept invariant correspondence inner product dw equal inner product wy dw dw wy 
wy principle derive natural gradient amari cichocki yang see yang amari detail 
give analysis dx 
define inner product dx dx tr dx dx 
riemannian metric structure dw dw tr dw 
write metric tensor component form 
quantity having indices gij kl ds gij kl gij kl jm lm shun ichi amari jm components may appear straightforward obtain explicit form natural gradient fact calculated shown 
theorem 
natural gradient matrix space 
proof 
metric euclidean inverse identity 
mapping dw dx natural gradient learning rule terms dx written dx dt tg continuous time version 
equation dx dt dw dt 
gradient calculated wt wt natural gradient learning rule dw dt proves equation 
dx forms basis tangent space integrable find matrix function satisfies equation 
basis called nonholonomic basis 
locally defined basis convenient purpose 
calculate natural gradient explicitly 
put log det log fi yi wx fi yi adequate probability distribution 
expected loss natural gradient works efficiently learning represents entropy output componentwise nonlinear transformation nadal parga bell sejnowski 
independent component analysis mutual information criterion gives similar loss function comon amari see oja karhunen 
fi true probability density function ith source negative log likelihood 
natural gradient calculated follows 
calculate differential dl dw dlog det dlog fi yi due change dw 
log det log det dw log det log det dw log det dx 
similarly dy log fi yi dxy column vector ym yi dy log fi yi 
gives natural gradient learning equation dw dt ty 
efficiency equation studied statistical information geometrical point view amari amari cardoso press 
calculate hessian natural frame dx dx dxy diagonal matrix diagonal entries yi dyi 
expectation explicitly calculated amari press 
hessian decomposed diagonal elements diagonal blocks see cardoso laheld 
stability learning rule easily checked 
terms dx solve fundamental problems efficiency stability learning algorithms blind source separation amari cardoso press amari press 
shun ichi amari natural gradient systems space problem define riemannian structure parameter space systems time shift operator 
amari point view information geometry amari murray rice 
show ideas see douglas amari douglas cichocki yang preliminary studies 
case multiterminal deconvolution typical loss function log det yi log fi yi dyi yi marginal distribution derived past sequence matrix convolution equation 
type loss function obtained maximization entropy independent component analysis maximum likelihood 
gradient ml wm ml 
order calculate natural gradient need define riemannian metric manifold linear systems 
geometrical theory manifold linear systems amari defines riemannian metric pair dual affine connections space linear systems 
dw dw mz small deviation 
postulate inner product dw dw invariant operation matrix filter dw dw dw dw wy natural gradient works efficiently learning system matrix 
put general system necessarily belonging fir identity system including terms 
tangent vector dw mapped dx dw 
inner product defined dx dx ij ij elements matrix 
natural gradient manifold systems follows 
ij theorem 
natural gradient manifold systems operator operated adequately 
proof omitted 
remarked belong class fir systems satisfy causality condition 
order obtain online learning algorithm need introduce time delay map space causal fir systems 
article shows principles involved details published separate article amari douglas cichocki 
shun ichi amari article introduces riemannian structures parameter spaces multilayer perceptrons blind source separation blind source deconvolution means information geometry 
natural gradient learning method introduced shown statistically efficient 
implies optimal online learning efficient optimal batch learning fisher information matrix exists 
suggested natural gradient learning easier get plateaus conventional stochastic gradient learning 
acknowledgments cichocki back yang riken frontier research program discussions 
amari 

theory adaptive pattern classifiers 
ieee trans ec 
amari 

neural theory association concept formation 
biological cybernetics 
amari 

differential geometrical methods statistics 
lecture notes statistics 
new york springer verlag 
amari 

differential geometry parametric family invertible linear systems riemannian metric dual affine connections divergence 
mathematical systems theory 
amari 

universal theorem learning curves 
neural networks 
amari 

learning statistical inference 
arbib ed handbook brain theory neural networks pp 

cambridge ma mit press 
amari 

neural learning structured parameter spaces natural riemannian gradient 
mozer jordan th 
petsche eds advances neural processing systems 
cambridge ma mit press 
amari 

information geometry 
contemporary mathematics 
amari 

blind source separation 
unpublished manuscript 
amari cardoso 
press 
blind source separation semi parametric statistical approach 
ieee trans 
signal processing 
amari chen cichocki 
press 
stability analysis learning algorithms blind source separation 
neural networks 
amari cichocki yang 

new learning algorithm blind signal separation nips vol 
cambridge ma mit press 
amari douglas cichocki yang 

multichannel blind deconvolution equalization natural gradient 
signal processing natural gradient works efficiently learning advance wireless communication workshop paris 
amari 

information geometry estimating functions semiparametric statistical models bernoulli 
amari 

information geometry boltzmann machines 
ieee trans 
neural networks 
amari murata 

statistical theory learning curves entropic loss criterion 
neural computation 
seung sompolinsky 

local global convergence line learning 
phys 
rev lett 
bell sejnowski 

information maximization approach blind separation blind deconvolution 
neural computation 
bickel klassen ritov wellner 

efficient adaptive estimation semiparametric models 
baltimore johns hopkins university press 
campbell 

relation information theory differential geometric approach statistics 
information sciences 
cardoso laheld 

equivariant adaptive source separation 
ieee trans 
signal processing 


statistical decision rules optimal inference russian 
moscow nauka translated english rhode island ams 
comon 

independent component analysis new concept 
signal processing 
douglas cichocki amari 

fast convergence filtered regressor algorithms blind equalization 
electronics letters 
heskes kappen 

learning process neural networks 
physical review 
jutten 

blind separation sources adaptive algorithm neuromimetic architecture 
signal processing 
kushner clark 

stochastic approximation methods constrained unconstrained systems 
berlin springer verlag 
murata ller amari 

adaptive line learning changing environments 
mozer jordan th 
petsche eds neural processing systems 
cambridge ma mit press 
murray rice 

differential geometry statistics 
new york chapman hall 
nadal parga 

nonlinear neurons low noise limit factorial code maximizes information transfer 
network 
oja karhunen 

signal separation nonlinear hebbian learning 

eds computational intelligence dynamic systems perspective pp 

new york ieee press 
opper 

online versus offline learning random examples general results 
phys 
rev lett 
rao 

information accuracy attainable estimation statistical parameters 
bulletin calcutta mathematical society 
rumelhart hinton williams 

learning internal representations error propagation 
parallel distributed processing vol 
pp 

cambridge ma mit press 
shun ichi amari saad solla 

line learning soft committee machines 
phys 
rev 
sompolinsky seung 

line learning dichotomies algorithms learning curves 

oh 
eds neural networks statistical mechanics perspective pp 

proceedings ctp joint workshop theoretical physics 
singapore world scientific 
ya 


foundation theory learning systems 
new york academic press 
van den 

unsupervised learning examples line versus line 
phys 
rev lett 
widrow 

statistical theory adaptation 
oxford pergamon press 
yang amari 

application natural gradient training multilayer perceptrons 
unpublished manuscript 
yang amari 
press 
adaptive line learning algorithms blind separation maximum entropy minimal mutual information 
neural computation 
received january accepted may 
