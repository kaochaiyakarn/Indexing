recursive distributed representations jordan pollack laboratory ai research computer information science department ohio state university neil avenue columbus oh pollack cis ohio state edu long standing difficulty connectionist modeling represent variable sized recursive data structures trees lists fixed width patterns 
presents connectionist architecture automatically develops compact distributed representations compositional structures efficient accessing mechanisms 
patterns stand internal nodes fixed valence trees devised recursive back propagation layer autoassociative encoder networks 
resulting representations novel combine apparently aspects features pointers symbol structures 
form bridge data structures necessary high level cognitive tasks associative pattern recognition machinery provided neural networks 
pollack 
major stumbling blocks application connectionism higherlevel cognitive tasks natural language processing inadequacy representations 
local distributed representations far unsuitable capturing dynamically allocated variable sized symbolic data structures traditionally ai 
limitation shows fact pure connectionism generated somewhat unsatisfying systems domain example parsers fixed length sentences embedded structures 
attacks connectionism aimed precisely question representational adequacy 
minsky papert example neural network learning machines stopped need ai focus knowledge representation principle machine learn recognize possesses potentially scheme representing xiii 
fodor pylyshyn arguments connectionism belief connectionist machines potential representing combinatorial syntactic constituent structure exhibit semantic systematicity thought processes 
agreeing thoroughly compositional symbolic structures important show connectionist architecture discover compact distributed representations 
recursive auto associative memory raam uses backpropagation non stationary environment devise patterns stand internal nodes fixed valence trees 
representations discovered merely connectionist implementations classic concatenative data structures fact new interesting potentially useful 
rest organized follows 
background connectionist representational schemes raam architecture described experiments 
discussion generative capacity architecture analysis new representations potential applications 
hybrid connectionist symbolic models potential powerful representations insist neural plausibility constraints create limitations place 
recursive distributed representations 
background connectionist representations normal computer programs long sequential data structures arrays lists primitives 
built notion address contents sequences addresses sequences quite simple computer programs represent manipulate tree graph structures 
representing lists trees trivial problem connectionist networks adjacent randomly addressed memory cells permit real time dynamic creation new units 
earliest modern connectionism inappropriate analogy semantic networks neural networks 
links represented logical relations concepts 
links represented weighted paths activation energy flowed 
needless say connectionist networks concept mapped single neuron unit representational capacity logically powerful cousins 
furthermore local representational schemes efficiently represent sequential information 
standard approach involves converting time space duplicating sub networks fixed set buffers sequential input 
early connectionist mcclelland rumelhart word recognition model modern efforts approach able represent process sequences longer predetermined bound 
way overcome length limitation sliding input buffer 
systems capable processing sequences longer predetermined bound really representing 
distributed representations focus research including reported circulation hinton report discussing properties representations entity represented pattern activity distributed computing elements computed element involved representing different entities 
obvious natural distributed representation feature microfeature system traditionally linguistics 
example connectionist model representation kawamoto lexical access 
entire feature system needed represent single concept attempts pollack representing structures involving concepts managed system 
example features needed represent nurse features needed represent elephant attempt represent nurse riding elephant may come white elephant large nurse legs 
solve problem feature superposition full size constituent buffers agent action object 
buffer reside feature pattern filling roles nurse riding elephant 
unfortunately dichotomy representation structure concatenation representation element structure features type system represent embedded structures john saw nurse riding elephant 
solution feature buffer dichotomy problem anticipated sketched hinton involved having reduced description nurse riding elephant fit constituent buffers patterns john saw 
immediately obvious develop reduced descriptions 
avant connectionist representations coarse coding allows multiple semi independent representational elements simultaneously superposition feature vector 
multiple elements conventional groupings elements interpreted larger structures 
example touretzky developed coarse coded memory system production system primitive lisp data structuring system called combination simple tree manipulations 
representation triples symbols elements represented patterns bits small sets triples reliably represented 
interpreting set triples pseudo cons cells limited representation sequences trees achieved 
similarly past tense model rumelhart mcclelland developed implicitly sequential representation set formed overlapping triples interpreted sequence 
instructive view basic idea representational scheme encoding sequence tokens 
unordered set overlapping subsequences breadth tokens recursive distributed representations 



coarse coded memory simultaneously represent set subsequences represent longer sequence 
limits type representation cost representation goes exponentially breadth particular breadth may sequences internal duplication 
sets count multiple occurrences elements 
system example represented spellings words sets letter pairs able represent word yoyo breadth increased system able represent words duplicate triples banana 
touretzky rumelhart mcclelland coarse coded representations fairly successful circumscribed tasks remain problems large amount human effort involved design compression tuning representations clear translate effort domains 
coarse coding requires expensive complex access mechanisms networks clause spaces 
coarse coded symbol memories simultaneously instantiate small number representational elements triples tokens spurious elements introduced furthermore assume possible tokens need combined 
utilize binary codes large set units hundreds thousands 
mode aggregating larger structures basic elements cause problems 
contrast distributed representations devised raam architecture demonstrate better properties encodings developed mechanically adaptive network 
point banana problem rumelhart mcclelland actual representation phonological orthographic pinker prince discovered words internal duplication language 
rosenfeld touretzky provide nice analysis coarse coded symbol memories 
pollack access mechanisms simple deterministic 
potentially large number primitive elements selectively combine constituent structures 
triples symbols need represented 
representations utilize real values units tens 
aggregation mode compositional 

recursive auto associative memory problem attack representation variable sized symbolic sequences trees numeric fixed width form suitable association categorization pattern recognition neural style processing mechanisms 

example binary tree 
consider hypothetical mechanisms translate directions symbolic trees numeric vectors 
compressor encode small sets fixed width patterns single patterns size 
recursively applied bottom fixed valence tree labeled terminals leaves resulting fixed width pattern representing entire structure 
binary tree shown terminals fixed width pattern take steps 
compressed pattern compressed pattern compressed reconstructor decode fixed width patterns parts determine parts decoded 
recursively applied top resulting reconstruction original tree 
example decoded decoded mechanisms hypothetical clear physically build computationally simulate devices patterns look 
answer question just assume mechanisms built recursive distributed representations output units input units output units input units left right right left 
proposed feedforward networks compressor reconstructor working binary trees 
standard modern connectionist substrate layered fully connected feed forward networks semi linear units 
binary trees bit patterns leaves compressor single layer network inputs outputs 
reconstructor single layer network inputs outputs 
schematics shown 
hidden units output units input units left right right left 
single network composed compressor reconstructor 
answer second regarding patterns look develop strategy letting connectionist network devise representations 
consider assume reader familiar standard backpropagation technique adjusting weights attempt re presentation mathematics 
crucially depend default assumptions full connectedness 
relying standard defaults hope keep focus issue representation 
pollack simultaneously training mechanisms single network shown 
looks network encoder problem 
backpropagation quite successful problem self supervised auto associative mode layer network 
network trained reproduce set input patterns input patterns desired target patterns 
learning network develops compressed code hidden units input patterns 
example training network reproduce bit patterns usually results bit binary code hidden units 
order find codes trees auto associative architecture recursively name 
extending simple example bit patterns network trained reproduce follows input pattern hidden pattern output pattern represents time epoch training 
assuming back propagation converges limit sum squares differences desired actual outputs go fact representation tree virtue fact compressor deterministic algorithm transforms tree representation reconstructor deterministic algorithm transforms representation back tree 
way representations devised subtrees case 
note rumelhart demonstrated network successful uses include network network 
numbers correspond number units input hidden output layers network 
recursive distributed representations demonstrated strategy works collection trees just single tree 
details form bridge theory practice 
initially random values hidden units part training environment 
weights network evolve patterns comprise training environment 
form non stationary moving target learning explored 
stability convergence network sensitive learning parameters 
explication rumelhart parameters learning rate controls gradient descent step size momentum integrates effects previous steps 
parameters set low change hidden representations invalidate decreasing error granted change weights high change takes place 
experiments described usually set larger experiments 
learning curve flattens slowly increased 
induction relied outside mechanical framework learning 
induction global success arising local improvements similar bucket brigade principle classifier systems 
training strategy reconstructs terminals fact equal limit allows strategy 
back propagation really run forever standard sigmoidal activation function impossible achieve perfect encoding described 
practical way decide training necessary 
back propagation produce binary outputs tolerance conventionally set training output value training pattern desired bit 
nonterminal patterns may binary far permissive tolerance 
order successfully reconstruct tolerance example similar second tolerance real valued non terminals experiments pollack set 
name architecture recursive auto associative memory raam accurately reflects codes developed auto associative memory compressed 
reflect separate mechanisms happen simultaneously trained 
mechanisms require support form control memory ability simple neural networks thresholds 
order encode tree bottom compressor needs stack store temporary results 
order decode tree top reconstructor needs external stack store intermediate patterns 
furthermore needs mechanism perform terminal testing 
experiments assumed terminal test merely threshhold test binary ness checks values pattern alternatively train simple classifier conventional computer programs test membership set perform error detection correction 

sequential raam sequences represented left branching binary trees nil alternative version raam architecture works developing representations access mechanisms sequences 
units units units units stack stack top stack stack top 
inverse sequencing mechanisms single layered networks 
compressor combines dimensional representation sequence stack new element top returning new dimensional vector recursive distributed representations reconstructor decodes back components 
architecture fact simpler mechanism trees 
compressed representations side stored externally 
constraint size representations higher dimension assumed compressed patterns terminal symbols shows single layer compressor reconstructor networks sequential raam viewed single network input output units hidden units 
vector numbers assumed stand nil empty sequence 
experiments vectors chosen generated intermediate state 
earlier logic network trained patterns input pattern hidden pattern output pattern xy xy xyz xy expected back propagation converges xyz representation sequence 
way representations developed prefixes sequence case 

experiments recursive auto associative memories 
proof concept demonstrate raam works practical assumptions discover compositional representations simple access mechanisms small sequential raam 
training set consisted possible sequences bits 
network empty pattern representations shown developed 
representations prefixes shown 
network clearly developed tri state shift register feature corresponds inverse bit second inverse bit third bit encoded 
pollack empty 
representations developed raam complete set bit patterns length 
square represents number 
shift register simply concatenates bits classical means serially constructing accessing obviously compositional representation 
finite piece hardware built hold certain number bits degrades rapidly filled 
interesting area explore involves pattern spaces underlying regularities depend representing possible combinations sub patterns 
conditions adaptive connectionist mechanism expected display desirable properties content sensitivity graceful degradation 

letter sequences second experiment involves learning represent sequences letters 
trying represent possible sequences letters certainly give rise shift register limited subset english words chosen 
electronic spelling dictionary words containing letters selected prefixes bar removed resulting list 
note training representation developed prefix recursive distributed representations air ana ani banana barn bin brain bran inn nab rain ran rib rib ran rain nab inn bran brain bin barn banana ani ana air 
representations developed raam letter sequences 
terminal coded bit pattern empty vector raam encode words 
note banana troublesome implicit sequential representation breadth 
shows representations letter sequences cluster diagram shows decaying sum representation information older elements gets lost sequential representation devoting resources keeping older elements alive 
resources build letter shift register network take easy solution path need represent letter words 
pollack inn air ana ani nab rain ran rib bin bran brain banana barn 
clustering letter sequence representations 

learning formed syntactic trees tree syntactic parse tree sentence little boy ran street terminals stand respectively determiner adjective noun verb preposition 
consider simple contextfree grammar rule expansion exactly constituents np vp np np ap np pp pp np vp np pp ap ap set strings language defined grammar easy derive bracketed binary trees training set 
set strings chart parser yielded set trees recursive distributed representations ap pp vp np 
representations binary trees training set devised raam manually clustered phrase type 
terminal represented bit code padded zeros 
raam devised representations shown 
dn dn dn dn dn dn dn dn dn 
clustering syntactic patterns 
tree representation labeled phrase type grammar sorted type 
raam clearly developed representation pollack similarity members type 
example third feature clearly distinguishing sentences non sentences fifth feature separates adjective phrases tenth feature appears distinguish prepositional noun phrases rest 
cluster patterns reveals similarity patterns generally follows phrase type breakup reflects depth trees 

learning represent propositions 
tree representations common data structures semantic syntactic structures 
final experiment sets propositional representations exploited merely demonstrates architecture capable working just binary trees 
table 
collection sentences propositional experiment 
pat loved mary john loved pat john saw man hill telescope mary ate spaghetti chopsticks mary ate spaghetti meat pat ate meat pat knew john loved mary pat thought john knew mary loved john pat hoped john thought mary ate spaghetti john hit man long telescope pat hoped man telescope saw pat hit man thought mary loved john short man thought saw john saw pat starting somewhat random collection sentences raam devise compact representations corresponding propositional forms 
sentences training shown table 
terminals raam bit patterns symbols appear sentences minus determiners pronouns plus new symbols subject raiser representations sentences mod specify adjectives triples 
metrics course classified np 
surprising combined 
course binary trees symbols distinguished nil element sufficient arbitrary tree representations 
recursive distributed representations table 
bit patterns terminal symbols word thing human prep verb bits bits bits bits bits hill street telescope chopsticks meat spaghetti man john mary pat mod long short knew hoped thought loved hit ate saw table 
ternary trees propositional experiment 
loved pat mary loved john pat saw telescope john man hill ate chopsticks mary spaghetti ate mary spaghetti meat ate pat meat knew pat loved john mary thought pat knew john loved mary john hoped pat thought john ate mary spaghetti hit mod telescope long john man hit john man mod telescope long hoped pat saw man telescope pat hit pat man thought man loved mary john saw mod man short thought man saw man john pat similarity bit binary representation devised terminals dividing classes thing human prep verb bit class counter shown table 
empty spots zeros 
sentence manually translated ternary tree sentence pollack saw man 
pat mod man short man thought man saw 
thought mary saw 
saw man john hit pat man 
man thought man loved 
thought man loved 
ate pat meat ate mary 
ate 
mary loved john pat loved pat mary hoped pat saw 
saw man 
pat man scope hit john man man scope 
hit 
john man hoped pat thought 
thought john ate 
ate mary thought pat knew 
knew john loved 
loved mary john knew pat loved 
loved john mary saw 
john man man hill 
representations ternary semantic trees training set devised raam manually clustered 
symbolic trees abbreviated fit 
readings shown table 
representation meant capture flavor recursive action agent object case system 
raam learned construct representations recursively encode decode trees respective parts 
shown pictorially clustered 
recursive distributed representations hit mod telescope long mod man short thought man saw man john saw telescope john man hill ate mary spaghetti meat hoped pat thought john ate mary spaghetti hoped pat saw man telescope pat hit john man mod telescope long knew pat loved john mary thought pat knew john loved mary john knew john loved mary john hit pat man thought man loved mary john man thought man loved mary john thought john ate mary spaghetti thought man loved mary john thought man saw man john ate chopsticks mary spaghetti ate mary spaghetti ate pat meat saw telescope ate chopsticks spaghetti meat man mod telescope long man hill man telescope mod telescope long mod man short hit mod telescope long john man loved john mary loved pat mary loved john pat loved mary john saw man telescope pat saw mod man short thought man saw man john pat saw man john 
clustering semantic patterns 
discussion 
studies generalization important question recursive auto associative memories capable productive forms generalization 
turned shift register example just memorizing training set finding convenient mapping structures unassigned vertices highdimensional hypercube ultimately uninteresting 
luckily turns case 
straightforward matter enumerate set sequences trees raam capable representing training set 
taken encoder decoder networks form recursive formedness test follows take patterns trees encode pattern new higher level tree decode pollack back patterns sub trees 
reconstructed subtrees tolerance tree considered formed 
procedure tree raams program start set terminals pool formed patterns exhaustively randomly combine pairs adding new formed patterns pool 
sequential raams pool begun just pattern empty sequence program merely attempts compose terminal pattern pool adding new prefixes pool 
running generator network formed syntactic tree experiment yielded formed trees shown table 
really grammatical rule allows np combine 
new instances np new vp twelve new 
clearly sort generativity memorization going infinite manner 
new instances syntactic classes formed recombination parts 
sequential raam letter sequences quite bit productive 
able represent new sequences letters approximately third including names electronic spelling dictionary brian rina barbara 
novel sequences reflect low order letter transition statistics indicating process powerful rote list memorization powerful arbitrary random access sequential storage place 
tendency especially raam decode novel trees back existing members training set 
example pattern encoded thought john knew pat loved mary john reconstructed thought pat knew john loved mary john original trees 
lack productivity probably attributable problem input patterns similar hamming distance john pat bit 
raam productive hoped quite systematic bit simplification formedness test guarantee pattern new tree fully decoded 
tolerance kept low full tree recoverable 
recursive distributed representations table 
additional trees represented raam fodor pylyshyn definition mean say thought systematic 
just don find people understand sentence john loves girl sentence girl loves john don find people think thought john loves girl think think thought girl loves john 
cases loved chosen set john mary pat man able reliably represented training set 
pollack 
improving generalization capacity productive capacity systems 
ought way acquire theoretically ability represent infinite numbers similar structures recursive distributed representations 
simplest formulation layer fully connected semi linear network arbitrary training sets shown limited capacity form small number new useful representations composed existing constituents better training environments different mathematical assumptions needed 
similarity difference relationships terminal patterns affects productivity raam 
case semantic triples fact terminals class john mary assigned similar patterns lead ability systematically problem single bit errors reconstruction damaging 
hand expect fully random patterns generalize 
brings question design compressible representations 
sort representations devised raam non terminal patterns lead best possible compression generalization properties adopted terminals 
secondly achieve truly infinite representational capacity fixed width patterns necessary theoretically consider underlying mathematical basis connectionist networks freed default implementational assumptions back propagation floating point calculations linear combinations sigmoids 
hand considered real numbers biologically computationally problematic 
unbounded number bits trivially compressed real number leading unbounded storage communication costs 
simulated connectionist system real numbers able bits precise output values properly paying 
binary code system able exploit redundancy sparseness regularity environment 
hand certainly reasonable assume rational numbers competence theory 
question answer similarity preserving mapping complex structured representations high dimensional spatial representations 
recursive distributed representations 
analysis representations prescription engineering recursive distributed representations insights 
top bottom constraints forge representations 
bottom constraint pattern completely determined constituents knowledge eventually fixed network weights trees similar constituents similar 
top constraint redundant information compressed similar structures np combine vp possible siblings pattern similar 
working drive similarity system wide goal minimizing error serves constrain apart patterns different trees environment 
result pressures representations consist types features categorical features identified earlier able separate classes distinctive features vary discriminate members class 
categorical features developed syntactic tree experiment clear examination small classifier 
patterns tree training set input input output network trained discriminate classes np vp pp ap table 
weights single layer classifier network rounded integers 
np vp pp ap strength bias table shows weights network rounded integers 
columns correspond categories rows correspond features 
bias inputs category units shown row sums absolute values weights row 
looking column labeled np example clear ninth tenth features strongly code np eighth fifth pollack features code np 
looking column labeled vp third features code second tenth 
strength row indicates categorical distinctive feature tenth feature example strongly codes np pp vp ap features connect strongly seventh ninth discriminations categories 
regard binary versus real question raised earlier raam may build hybrid code 
strong binary distinctions categorical judgements weaker analog distinctions discriminating labeling members categories 

geometric interpretation alternative means understanding representations may come geometry 
terminal patterns vertices dimensional hypercube contains non terminal patterns 
binary trees raam finding consistent invertible mapping works way composable pairs vertices internal points composed 
view image raam trained trees points floor cube 

perspective diagram dimensional codes developed trees 
recursive distributed representations shows perspective plot dimensional hypercube codes developed trees 
long pair composable points mental left right hands see triangles falling forward reduce scale 
saund investigated non recursive auto association method dimensionality reduction asserted order map constrained form small dimensional parametric surface larger dimensional space 
consider just auto associator 
really invertible mapping certain points unit square points unit line 
order network develop parametric dimensional curve space set connected splines 
points need encoded parametric curve get cover 
limit especially dense patches space need covered longer dimensional curve space filling curve fractal dimension 
notions associative reconstructive memories fractal dimensions discussed 

applications 
associative inference raam devise representations trees numeric vectors attacked fixed width techniques neural networks lead fast inference structural transformation engines 
question course patterns trees operated systematic fashion decoded 
simple demonstration possibility 
raam propositional triples able represent cases loved possible build associative network perform simple implication loved loved 
trivial shifting task performed explicit concatenative representation 
bit triples compressed dimensional pattern vectors quite simple job 
task find associator transform compressed representation antecedent 
loved mary john compressed representation pollack consequent 
loved john mary 
back propagation feed forward network trained pairs patterns tolerance able successfully transform remaining pairs 
system need follow long chains implications 
showing certain conditions feed forward networks hidden layers compute arbitrary non linear mappings 
anticipate sequential application associative inference able compiled slow training fast networks layers 
consider homogenous coordinate transformations computer graphics linear nature primitive operations scaling rotation translation allows sequence compiled single matrix multiplication 
field ai date produced compiling methods rival speedup interesting ai problems nonlinear interesting ai representations numeric 
point suitable representations efficient non linear mapping engines generate significant speed improvements inferential processing 

massively parallel parsing revisited introduced noting natural language processing posed problems connectionism precisely representational adequacy problem 
build parser generator having internal representations 
raams devise compositional representations shown experiment semantic triples target patterns recurrent networks accept sequences words input 
feasibility study concept performed sequential cascaded network higher order network restricted topology sigma pi 
basically cascaded network consists subnetworks function network ordinary feed forward network weights dynamically computed purely linear context network outputs determine weight function net 
sequential cascaded network outputs function network directly fed back inputs context network 
network trained presentations initial context input sequences desired final state 
recursive distributed representations table 
bit input patterns connectionist parser 
word class identity john man pat mary telescope spaghetti chopsticks hill meat ate hit saw loved hoped thought knew long short new bit similarity encoding created words appearing sentences making identical 
bits define class second bits distinguish members 
patterns displayed table 
sequential cascaded network consisting function network context network trained sequences bit patterns corresponding sentences table 
initial context vectors zeroes desired final states compressed dimensional representations devised raam trees table including 
system closest thing barely adequate connectionist system processing language variable length sequence words network returns linear time dimensional vector decoded meaning raam operated associative inference engines 
hand system extreme deficiencies evaluated cognitive model 
produce single tree sentence handles small corpus sentences 
simplifying assumption internal representations devised target patterns questionable 
system pollack interesting aspects 
fact runs linear time outputs compositional representation sentences automatically performs prepositional phrase attachment correctly parses mary ate spaghetti meat chopsticks examples pronoun resolution automatically replaces proper filler 
connectionist parser deal embedded structures resorting external symbolic computational power 

great deal research conducted area conversion small feasibility studies falsifiable cognitive models reliably engineered artifacts 
immediate concerns include understanding convergence stability properties moving target learning strategy empirical analytical studies called 
similarly relationship termination condition depth capacity raam needs better understood 
developing complete understanding representations mechanisms developed 
outcome general representational scheme analytically derived particular representational task relying slow gradient descent learning 

conundrum theories human machine learning came mental procedure mental representation 
minsky papert claimed representational egg come procedural chicken fodor pylyshyn claimed intimately know egg extension exclusive class fertile 
flip side course perfect egg may impossible chicken formal representational theory specified consideration genesis may learnable mechanism principle 
points biologically certified way dilemma evolution 
representations associated procedures develop slowly responding constraints changing environment 
constraint recursive distributed representations representations fit fixed width patterns interacts constraint patterns compose certain formed ways giving rise fixed width patterns capture structural similarity spatial distance 
raam architecture inspired powerful ideas 
due hinton showed properly constrained connectionist network develop semantically interpretable representations hidden units 
second old idea sufficiently powerful form learning machine learn efficiently perform task example design 
taken ideas suggest task specified example requires embedded representations network able develop representations 
turns single task requires representations 
tasks construct representations access 
address machines tasks string concatenation array indexing computationally primitive natural fall far notice 
natural neural networks need examined anew 
resulting task specific mechanisms compressor reconstructor form reconstructive memory system traces actual memory contents stored reliable created domain knowledge 
systematic patterns developed raam new kind representation recursive distributed representation instantiate hinton notion reduced description mentioned earlier 
combine apparently aspects understood representations act feature vectors fixed width simple measures similarity pointers simple efficient procedures contents fetched 
act compositional symbol structures simple associative procedures reconstructor pattern classifiers pattern transformers clearly sensitive internal structure 
feature vectors representations recursively combine constituent structures statistically inferred formedness constraints 
pointers symbols contain information suitable similarity measurements nearest neighbor judgements 
symbol structures easily compared taken apart order worked 
recursive distributed representations may lead reintegration syntax pollack semantics low level 
currently symbolic systems information free atoms physically combine bit pointer concatenation completely unrestricted fashion 
domain syntax required restrict molecules fact set semantically interpretable ones 
recursive distributed representations undergo symbols contain meanings physically combine systematic fashion 
real atoms molecules time 
partially supported state new mexico ohio office naval research 
tony plate implemented back propagation copy cluster program 
comments chandrasekaran hinton mcclelland touretzky helped improve presentation 

cottrell connectionist parsing proceedings seventh annual conference cognitive science society irvine ca 

context free parsing connectionist networks tr university rochester computer science department rochester 

selman rule processing connectionist system natural language understanding csri university toronto computer systems research institute toronto canada 

hanson connectionist network learns natural language grammar exposure natural language sentences proceedings ninth conference cognitive science society seattle 

mcclelland kawamoto mechanisms sentence processing assigning roles constituents parallel distributed processing experiments microstructure cognition vol 
mcclelland rumelhart pdp research group ed mit press cambridge 

pollack waltz natural language processing spreading activation lateral inhibition proceedings fourth annual cognitive science conference ann arbor mi 

waltz pollack massively parallel parsing strongly interactive model natural language interpretation cognitive science 

lehnert case problem solving large knowledge base learned cases proceedings sixth national conference artificial intelligence seattle 

berg parallel natural language processing architecture distributed control proceedings ninth annual conference cognitive science society seattle 

minsky papert perceptrons mit press cambridge ma 
recursive distributed representations 
fodor pylyshyn connectionism cognitive architecture critical analysis cognition 

rumelhart hinton williams learning internal representations error propagation parallel distributed processing experiments microstructure cognition vol 
rumelhart mcclelland pdp research group ed mit press cambridge 

mcclelland rumelhart interactive activation model effect context perception part 
account basic findings psychology review 

allen studies natural language back propagation institute electrical electronics engineers international conference neural networks san diego ii 

sejnowski rosenberg parallel networks learn pronounce english text complex systems 

charniak santos context free connectionist parser connectionist really context free advances connectionist neural computation theory pollack ed ablex norwood nj 

hinton distributed representations cmu cs carnegie mellon university computer science department pittsburgh pa 

kawamoto dynamic processes re solution lexical ambiguity doctoral dissertation department psychology brown university providence 

hinton representing part hierarchies connectionist networks proceedings tenth annual conference cognitive science society montreal 

touretzky hinton symbols neurons details connectionist inference architecture proceedings ninth international joint conference artificial intelligence los angeles ca 

touretzky reconciling connectionism recursive nature stacks trees proceedings th annual conference cognitive science society amherst ma 

touretzky representing transforming recursive objects neural network trees grow boltzmann machines proceedings institute electrical electronics engineers international conference systems man cybernetics atlanta ga 

rumelhart mcclelland learning past tenses english verbs parallel distributed processing experiments microstructure cognition vol 
mcclelland rumelhart pdp research group ed mit press cambridge 

pinker prince language connectionism analysis parallel distributed processing model language cognition 

mozer inductive information retrieval parallel distributed computation technical report institute cognitive science ucsd la jolla 

rosenfeld touretzky capacity models coarse coded symbol memories complex systems 

ackley hinton sejnowski learning algorithm boltzmann machines cognitive science 

cottrell munro zipser learning internal representations gray scales images example extensional programming proceedings seventh annual conference cognitive science society seattle 

elman finding structure time report center research language ucsd san diego 

miikkulainen dyer forming global representations back propagation proceedings institute electrical electronics engineers second annual international pollack conference neural networks san diego 

plaut nowlan hinton experiments learning back propagation cmu cs computer science dept carnegie mellon university pittsburgh 

holland holyoak nisbett thagard induction processes inference learning discovery mit press cambridge 

jordan serial order parallel distributed processing approach ics report institute cognitive science ucsd la jolla 

saund dimensionality reduction constraint vision proceedings ninth annual conference cognitive science society seattle 

mandelbrot fractal geometry nature freeman san francisco 

pollack implications recursive distributed representations advances neural information processing systems touretzky ed morgan kaufman los ca 

hornik stinchcombe white multi layer feedforward networks universal approximators neural networks appear 

lippman computing neural networks institute electrical electronics engineers assp magazine april 

lapedes farber neural nets los alamos 

pollack cascaded back propagation dynamic connectionist networks proceedings ninth conference cognitive science society seattle 

williams logic activation functions parallel distributed processing experiments microstructure cognition vol 
rumelhart mcclelland pdp research group ed mit press cambridge 

hinton learning distributed representations concepts proceedings eighth annual conference cognitive science society amherst ma 
