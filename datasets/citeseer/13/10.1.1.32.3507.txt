efficient locally weighted polynomial regression predictions andrew moore robotics institute school computer science carnegie mellon university pittsburgh pa cs cmu edu jeff schneider robotics institute carnegie mellon university pittsburgh pa cs cmu edu kan deng robotics institute carnegie mellon university pittsburgh pa cs cmu edu locally weighted polynomial regression lwpr popular instance algorithm learning continuous non linear mappings 
inputs datapoints computational expense predictions daunting 
discuss drawbacks previous approaches dealing problem new algorithm multiresolution search augmented kd tree 
needing rebuild tree fast predictions arbitrary local weighting functions arbitrary kernel widths arbitrary queries 
begins new faster algorithm exact lwpr predictions 
introduce approximation achieves magnitude speedup negligible accuracy losses 
increasing certain approximation parameter achieves greater speedups correspondingly larger accuracy degradation 
useful operations early stages model selection locating optima fitted surface 
show approximations permit real time query specific optimization kernel width 
conclude brief discussion potential extensions tractable instance learning datasets large fit computer main memory 
locally weighted polynomial regression locally weighted polynomial regression lwpr form instance memory algorithm learning continuous non linear mappings real valued input vectors real valued output vectors 
particularly appropriate learning complex highly non linear functions inputs noisy data 
popularized statistics literature past decades cleveland grosse atkeson enjoying increasing applications learning robot dynamics moore schaal atkeson learning process models 
classical bayesian linear regression analysis tools extended locally weighted framework hastie tibshirani providing confidence intervals predictions gradient estimates noise estimates important learned mapping controller atkeson schneider 
review lwpr 
linear regression input output 
global linear regression left finds line minimizes sum squared residuals 
represented fi fi fi fi minimize gamma gamma fi gamma fi locally weighted linear regression prediction query point query supplied 
linear map constructed strongly influenced datapoints lie close query point scaled euclidean distance metric 
achieved prediction weighting datapoint distance query point close query gets weight point far away gets weight zero 
common weighting function gaussian weight datapoint exp query query local linear model vary query get curve weighted squares query left graph shows global linear regression progress sum squares unweighted residuals minimized 
right graph shows locally weighted linear regression 
weighted sum squared residuals minimized thickness lines indicates strength weight 
parameter called kernel width bandwidth determines quickly weights decline value moves away query 
finding line parameters fi fi minimize global sum squared residuals minimize locally weighted sum squared residuals gamma fi gamma fi right side shows effect 
near query marked large residuals penalized strongly far query penalty negligible 
query moved weights datapoints change different local linear map 
provided strong stomach matrix manipulation ideas easily extended datasets inputs local polynomial models linear models 
suppose wish estimate local multivariate polynomial fi fi fi query point query function produces jth term polynomial 
example inputs quadratic local model equation written compactly fi vector polynomial terms input 
weight kth datapoint computed decaying function euclidean distance query fi chosen minimize gamma fi 
happily minimization requires gradient descent obtained directly fi gamma theta matrix row column matrix remember number terms defined ij succinctly may write efficient lwpr predictions lwpr powerful flexible tool fitting multivariate noisy data non linearities 
direct implementation algorithm requires prediction entire dataset scanned weights computed datapoints datapoints weighted contribution added matrices 
datapoints terms means nm multiplications build matrices operations compute fi vector 
computational expense isn inconvenience 
devil stick robot schaal atkeson able predictions second inputs outputs row column matrix kth row wk theta xk tm xk 
need constructed explicitly 
data points dsp board 
cases graphing optimizing model performing test set validation model faster predictions desirable 
number datapoints may larger 

previous researchers provided kinds answers question 
provides fourth described shortly 
common solution editing small subset datapoints 
results algorithm different properties information fine detail necessarily lost 
solution caching multivariate spline model built data loaded 
grosse variable resolution kd tree structure multilinear interpolation tree leaves 
unfortunately continuous interpolation dimensions expensive computation memory 
quinlan uses caching method ignores continuity storing separate discontinuous linear maps leaves 
downside caching solutions record fitted surface 
scheme retains information necessary full local regression analysis providing noise estimates confidence intervals prediction 
third solution retain information uses technique called range searching friedman preparata shamos 
possible arrange data way query point distance datapoints distance query returned needing search entire dataset 
works small number dimensions kernel width small tiny fraction datapoints non zero weight query 
dimensions savings uniformly distributed datasets fewer millions points disappointing 
worse datasets best kernel width broad meaning significant fraction data data non zero weight 
case avoiding zero weight datapoints help 
main idea deng moore multiresolution data structure increased speed kernel regression known locally weighted averaging 
extend method arbitrary locally weighted polynomials give number empirical evaluations resulting algorithms 
show apparently excessive approximation reduces prediction times gives performance 
investigate fast predictions permit prediction kernel width 
algorithm builds enhanced kd tree data 
imagine inputs datapoints distributed leftmost 
root node tree records bounding box data shown rectangle 
root children half data bounding boxes part 
turn children 
continues recursively leaf nodes contain just point 
decide input attribute split 
decision trees breiman quinlan induction sole purpose splits increase computational efficiency alter inductive bias 
believe choice numerous kd tree splitting criteria critical purpose choose split middle dimension widest spread method deng moore 
nd node kd tree 
records ffl unweighted nd matrix summing unweighted data nd 
ffl unweighted nd vector summing unweighted data nd 
ffl value data node 
needed predictions described necessary computing confidence intervals noise estimates 
kd tree built perform cheap computation weighted arbitrary queries arbitrary monotonically nonincreasing weighting functions arbitrary kernel widths 
brevity consider query query suppose require weighted matrix points node nd 
weighted nd nd obvious method ask children compute values weighted left nd weighted right nd sum 
course gain computation speed prediction root add nodes 
may able cut computation node 
get savings spot weights nd near identical weights current query kernel width weighting function 
happen reasons ffl points nd far query zero weight 
ffl points close providing room weight variation 
ffl weight function varies negligibly region nd 
wide kernel region 
tree level tree level tree level tree level bounding boxes increasing depths kd tree 
close query may constant value example 
computing maximum variation weights points node nd easy 
know location query know bounding hyperrectangle current node 
simple algorithm costing number tree dimensions compute shortest largest possible distances point node 
values assumption weight function non increasing compute minimum maximum possible weights min wmax datapoint node nd 
exact lwpr multiresolution tools simple tree cutoff rule 
computing weighted weighted nd matrix compute min wmax different recursively call weighted computation child nodes sum results 
identical write min wmax return weighted nd nd nd unweighted nd approximate lwpr see method provide large computational savings whilst computing exactly local linear model regular lwpr 
examine approximation reduce costs 
suppose prepared sacrifice original weighting function approximation differs original ffl 
concession brings tremendous rewards 
simple implementation cutoff rule wmax gamma min ffl simply return unweighted nd min wmax weights dangerous 
lwpr query far away datapoints total sum weights prediction may small ffl approximation may wildly different predictions non approximate case especially thousands datapoints weight weight ffl 
problem solved setting ffl fraction total sum weights involved regression ffl small fraction cutoff wmax gamma min course don know value prediction computing number datapoints operation 
estimate lower bound computation far accumulated sum weights sofar datapoints node nd minimum weight nd min sofar min tricks summarized 
compute approximation weighted sum weighted nd nd nd sofar 
compute min wmax retrieve unweighted nd node nd 

wmax gamma min sofar min 
return weighted nd min wmax unweighted nd 
weighted left nd left child nd sofar update sofar include extra weight added left child 
weight distance weight distance left side weighting function 
right side approximation tolerance ffl 
weighted right nd right child nd sofar return weighted nd weighted left nd weighted right nd trivial bookkeeping passes accumulated sofar value 
details interesting details summarize briefly 
ffl ensure numerical stability algorithm attributes pre scaled hypercube centered origin 
ffl exposition discussed constructing matrix 
constructed exactly way 
ffl function approximated linear quadratic attributes nonlinear perform lwpr distance metric contains attributes cause non linearities 
dimensionality kd tree distance metric number attributes 
ffl cost building tree log 
built lazily growing ondemand queries occur datapoints added theta tree depth time occasional rebalancing may needed 
tree occupies space 
huge memory savings possible nodes fewer datapoints split retain datapoints linked list 
ffl searching left child advantageous search node closest query 
strengthens sofar bound 
ffl ball trees omohundro play similar role kd tree range searching possible hierarchy balls containing sufficient statistics datapoints contain beneficially place bounding boxes 
ffl algorithms modified permit nearest neighbors query receive weight matter far query 
approximate algorithms approximate set nearest neighbors 
useful discussed experiments 
empirical evaluation evaluated algorithms comparison 
regular direct computation tk direct computation obvious useful tweak 
wk don bother operation adding sum 
tree near exact tree algorithm set gamma 
approx approximate tree algorithm 
fast wildly approximate tree algorithm 
gives extremely rough approximation weight function 
trivial dataset 
local linear regression applied gaussian weight function kernel width exp gamma gamma query theta middle line predicted value top bottom lines show confidence intervals provided locally weighted regression analysis 
tree algorithm 
approx algorithm gives indistinguishable graph 
fast algorithm gets similar predictions noticeable small discontinuities 
discontinuities cause serious problems user trying width tree local weighted linear predict 
univariate dataset fitted locally weighted linear regression tree algorithm 
width fast local weighted linear predict 
univariate dataset fitted locally weighted linear regression fast algorithm 
gradient width tree local weighted linear gradient 
gradient estimates method 
gradient width fast local weighted linear gradient 
gradient estimates method 
estimate derivatives surface 
derivatives estimated subtracting close predictions derivatives estimated accurately fi vector identified local regression 
case derivative simply fi estimate equation 
derivatives evaluated way shown figures 
examine prediction non trivial dataset 
abalone available uci repository inputs datapoints task predict number rings 
experiments removed datapoints random test set examined algorithm performing predictions variables scaled kernel width 
table shows regular method took second prediction 
saved 
tree reduced regular time producing identical predictions shown identical mean absolute errors regular tree 
approx algorithm gives fold saving compared tree fast algorithm times faster 
price approx fast pay terms predictive accuracy 
compare standard error dataset output variable mean predicted value tree error approx error fast error 
notice small insignificant penalty relative percentage variance explained 
results run testset size 
representative 
table reader containing averages confidence intervals runs different randomly chosen test sets 
bottom row shows error approx fast relative regular algorithm confidently estimated small 
examine algorithms applied collection uci repository datasets robot dataset described atkeson 
shows results datasets local model locally weighted linear regression kernel width unit scaled input attributes 
shows results variety different local polynomial models described 
pattern computational savings serious accuracy penalties consistent earlier experiment 
space give results numerous experiments graphing performance dimensionality dataset size kernel width polynomial type forth 
longer technical report forthcoming 
prediction time optimization kernel width lwpr examples fixed kernel widths 
datasets adaptive kernel width dependent current query desirable 
point issues arise statistical issue evaluate different kernel widths example confidence interval width resulting prediction estimate local variance estimate local data density computational cost searching best kernel width chosen criterion 
interested regular tree approx fast predict build weighted regression matrices solve matrices multiplications mean abs error low quartile abs err hi quartile abs err table costs errors predicting abalone dataset 
algorithm regular tree approx fast sigma sigma sigma sigma sigma mean sigma sigma sigma sigma sigma excess error compared regular sigma sigma sigma sigma sigma table build weighted regression matrices errors errors relative regular 
confidence intervals provided experiments 
expense error heart mbl inputs datapoints regular tree approx fast pool mbl inputs datapoints regular tree approx fast energy mbl inputs datapoints regular tree approx fast abalone mbl inputs datapoints regular tree approx fast mpg mbl inputs datapoints regular tree approx fast breast mbl inputs datapoints regular tree approx fast expense error heart mbl inputs datapoints regular tree approx fast pool mbl inputs datapoints regular tree approx fast energy mbl inputs datapoints regular tree approx fast abalone mbl inputs datapoints regular tree approx fast mpg mbl inputs datapoints regular tree approx fast breast mbl inputs datapoints regular tree approx fast performance uci datasets robot dataset 
heart breast classification problems approximated regression output 
threshold predictions give class error denotes fraction testset misclassified 
regression tasks error mean absolute prediction error 
cases std error denotes error produced default rule predict mean output 
expense prediction building regression matrices 
tests kernel width 
experiments variety lwpr models selected cross validation 
heart kernel regression kernel width kw ignoring input 
pool local weighted lw quadratic regression kw 
energy lw quadratic regression cross terms 
abalone lw linear regression ignoring attribute completely including attributes distance metric 
mpg uses attributes distance metric 
breast uses subset available inputs 
kernel width 
locally weighted quadratic 
dataset points 
points inputs range 
outputs computed sin noise 
points inputs range jxj outputs computed sin noise 
noise std dev 
dataset crafted fixed kernel width method see local quadratic polynomial kernel width fitting outer data inner data poorly 
results text datapoints inputs separation classes functions jxj denoting distance origin 
fixed variable variable width kernel width width goal weight width mean goal mean algorithm mean error weight error error predict regular tree approx fast table prediction time optimization kernel width 
results explained text 
computational issue resort simple criterion local weight 
shows input dataset variable kernel width desirable 
experiment input dataset constructed similar spirit 
evaluated test set points see fixed kernel width better mean error table columns 
chose simplest imaginable adaptive kernel width prediction algorithm top level prediction inner loop predictions kernel widths gamma gamma gamma choose predict kernel width produces weight closest fixed goal weight 
dense data small kernel width chosen sparse data kernel wide 
results striking 
middle columns table reveal wide range goal weights test set error achieved 
rightmost columns show approximate methods continue win computationally 
computational efficiency required ability tree methods cut computation wide distance metrics 
experiment extended 
search goal weights need build goal 
search best kernel width binary chop log kernel width 
enormous datasets 
striking feature approximate algorithms complete locally weighted lookups needing inspect individual datapoints 
regression matrices built entirely nodes summarize relevant statistics datapoints 
datapoints exist bottom tree visited put main memory 
rarely visited portions tree transferred disk queried extremely local lookups 
interesting speculate extending large databases billions records numeric information 
may possible build multiresolution structure records parent records recursively record sets statistics summarizing descendants 
may greatly speed queries ask statistics clusters records local query record 
date devised methods efficiently building structures records maintaining top tree main memory 
svd trees 
uses kd trees curse dimensionality remains 
hundreds input attributes query hopes cutoffs matrix construction levels tree attributes split 
investigating new data structure help combat problem called singular value decomposition tree 
set datapoints node linearly dependent subspace linear transformation recorded map child nodes datapoints new lower dimensional coordinate system 
sponsored national science foundation career award andrew moore 
mary soon lee reviewers valuable comments 
atkeson moore schaal 

locally weighted learning 
ai review 
atkeson moore schaal 

locally weighted learning control 
accepted publication ai review 
breiman friedman olshen stone 

classification regression trees 
wadsworth 
cleveland 

locally weighted regression approach regression analysis local fitting 
journal american statistical association 
deng moore 

multiresolution instance learning 
appear ijcai 
morgan kaufmann 
friedman bentley finkel 

algorithm finding best matches logarithmic expected time 
acm trans 
mathematical software 
grosse 

loess multivariate smoothing moving squares 
ward editors approximation theory vi 
academic press 
hastie tibshirani 

generalized additive models 
chapman hall 
moore 

fast robust adaptive control learning forward models 
moody hanson lippman editors advances neural information processing systems 
morgan kaufmann 
omohundro 

efficient function constraint classification learning 
lippmann moody touretzky editors advances neural information processing systems 
morgan kaufmann 
preparata shamos 

computational geometry 
springer verlag 
quinlan 

learning efficient classification procedures application chess games 
michalski carbonell mitchell editors machine learning artificial intelligence approach 
tioga publishing palo alto 
quinlan 

combining instance model learning 
machine learning proceedings tenth international conference 
schaal atkeson 

robot juggling implementation memory learning 
control systems magazine 
schneider 

exploiting model uncertainty estimates safe dynamic control learning 
neural information processing systems 
morgan kaufmann 
