de informatique ecole cnrs ura mixture approach universal model selection olivier liens mixture approach universal model selection olivier liens september laboratoire informatique de ecole normale sup erieure rue ulm paris cedex tel dmi ens fr mixture approach universal model selection olivier 
build model selection algorithm mean kullback risk upper bounded mean risk best model applied half sample augmented explicit penalty term order sample size 
estimator true selection rule adaptive convex combination models mixture approach inspired context tree weighting method information theory willems shtarkov universal data compression algorithm stationary binary sources 
algorithm universal respect statistical mean kullback risk sense optimal exchangeable sample distribution 
give application estimation probability measure adaptive histograms 
detail factorised computation mixture weights models performing number operations order log subdivisions underlying histograms nested cells 
shows case dense family models true selection rule built second step 
give upper bound mean square hellinger distance estimator sample distribution tends zero optimal rate explicit multiplicative constant case square distance kullback divergence comparable 

purpose adapt methods results information theory statistical estimation probability distribution 
measure quality estimators respect kullback divergence respect hellinger distance 
precisely interested universal model selection algorithms true sample distribution supposed exchangeable seek adaptive way select efficient family estimators values family models 
mixture approach consists mixing estimators appropriate adaptive weights choosing 
comes coding theory double mixture codes proved nearly optimal redundancy stationary source 
codes called universal special knowledge source 
design study date september 
key words phrases 
adaptive statistical model selection adaptive mixtures models adaptive histograms context tree weighting method mean kullback risk 
am glad massart joint organisation seminar adaptive statistics ecole normale sup erieure beautiful course lectures gave subject semester numerous fruitful discussions occasion 
implementation known remarkable breakthrough works willems shtarkov context tree weighting method 
coding algorithm performs implicit estimation probability distribution source applying methods information theory statistics requires thinking redundancy criterion coding theory coincide kullback contrast function cesaro mean kullback risks subsamples growing sizes 
main result explicit universal model selection method 
call progressive mixture method combines bayesian estimators subsamples increasing lengths basic implementation alternatives lower algorithmic complexity mentioned 
true selection rule needed possible approach mixture estimator distribution drawn models second step 
gives efficient upper bound mean square hellinger distance true sample distribution closure family models 
illustrate general approach show lead efficient algorithms develop case estimation probability measure adaptive histograms 
point true selection rule penalised maximum likelihood estimator reach performance progressive mixture estimator weak assumptions give toy counter example increase average risk due model selection order true selection rule order progressive mixture estimator size sample 
bears resemblance known result game theory saying general optimal strategies mixed 
reason failure true model selection consider situations true sample distribution necessarily lies closure family models 
situation common practice signal image analysis models far need capture complexity data 
interesting note possible build adaptive estimators mean risk exchangeable sample distributions 
analysis understood penalised minimum contrast adaptive estimators concentration theorems product measures see massart mixture approach opens different line proofs 

universal mixture estimator model selection kullback divergence measurable space 
bn product sigma algebra pn probability distribution bn 
canonical process permutation oe sn ng oex exchanged process oex oe assume pn exchangeable 
means oe sn bn pn pn oex universal model selection consider countable family estimators 
precisely assume probability measure map 
ajx measurable 
assume dominating measure delta gamma replacing necessary gamma assume independent assume measurable version dq delta 
model selection problem kullback risk solve approximately knowing sample minimisation problem inf epn pn delta delta probability measures ae ae kullback divergence function called relative entropy ae log dae dae ae build approximate mixture solution problem 
probability distribution positive integer 
consider estimator gamma xn xn dq delta delta immediately definition independent choice dominating measure theorem 
previous assumptions epn pn delta delta inf ae epn pn delta delta gamma log oe 
supplementary observations choose estimators computed plays role test set 

see examples possible balance terms sum right hand side previous equation appropriate choice 
course interesting case pn product measure 
proof requires pn exchangeable 
practice support finite finite number models 
proof 
easy see pn delta delta gamma log pn dx independent way pn delta delta gamma log pn dx equation equivalent gamma epn log inf gammae pn log gamma log fact gamma log convex function see gamma epn log gamma gamma epn log xn xn fact pn exchangeable swap xm mth term sum 
simplify notations introduce xn get gammae pn log gamma gamma epn log xn xn gamma gamma epn log xn gamma log gamma epn log xn eventually exchange xn right hand side get gammae pn log gammae pn log gamma log infimum right hand side ends proof theorem 
universal model selection 
progressive mixture estimators progressive mixture estimator previous section drawback requiring lengthy computations gamma mixtures computed 
number reduced log gamma proceed way assume gamma define backward induction sequence probability measures ff 
ff ff gamma xn ff gamma xn ff 
ff ff xk ff xk ff consider estimator ff qm dq delta delta easy show induction gamma epn log gamma ep log xn inf gammae log log theorem 
previous assumptions epn pn delta delta inf ae epn pn delta delta gamma log oe 
randomised progressive mixture estimators way modify progressive mixture estimator choice number observations model selection 
consider integer sample integers drawn respect uniform distribution fk ng 
put xn dq easy see theorem 
epn pn delta delta inf ae epn pn delta delta gamma log oe various combinations improvements progressive mixture estimator easily imagined 

mixture counter example similarity give example show true selection rule selects distribution models family models substituted progressive mixture estimator theorem 
supplementary assumptions structure family models added 
consider sample binary variables simple models containing distribution 
real number bernoulli distribution parameter ffi gamma ffi consider models containing distribution second distribution best estimators models obviously constant estimators assume true distribution gamma central limit theorem shows large positive constant ff independent pn gamma ff maximum likelihood estimator choose probability equal ff 
gamma gamma gamma log universal model selection notations theorem maximum likelihood estimator epn delta delta inf ae epn delta delta ff log oe compared theorem applied gives epn delta delta inf ae epn delta delta log oe symmetry problem shows selection rule observations better maximum likelihood estimator 
reason counter example works course map 
inf phi psi differentiable extremal point non zero directional derivatives 
words true selection rule gave increase risk order possible estimate precision order clearly contradiction central limit theorem 

universal bayesian estimation bernoulli variable section show bayesian estimate corresponding priori uniform mixture parameter universal 
technique proof borrowed subtle proof provided estimator 
notations analogous section parameter continuous space 
consider uniform mixture equal lebesgue measure 
put omega gamma 


gamma bayesian estimator corresponding known laplace estimator ffi ffi order study performance fact true distribution assumed exchangeable 
introduce random variables gamma gammae pn log gamma epn log gammae pn log log epn inf gamma log omega log inf epn log proved theorem 
exchangeable distribution pn epn pn delta delta inf epn pn delta delta 
progressive mixture estimator prior distribution gamma got upper bound penalty equal log easy consequence fact log gamma gamma log see proof 
case continuous parameter space universal upper bound prove bayesian estimator better progressive mixture estimator 

laplace estimator known long time know result give new 
thing say source inspiration proof 

generalisation random variables finite number values consider case delta delta delta dg 
distribution uniform lebesgue probability measure parameter space theta 
uniform prior theta omega theta delta delta delta 
delta delta delta 
delta delta delta universal model selection 
exchangeable distribution pn gammae pn log gammae pn gammae pn log epn inf theta gamma log omega log inf theta epn log theorem 
exchangeable distribution pn dg epn pn delta delta inf theta epn pn delta delta 
estimation adaptive histograms consider section measurable space probability measure space 
assume sample distribution pn omega exchangeable 
countable family subdivisions call subdivision partition finite number measurable sets 
parameter theta consider measure density respect measure consider estimator density dq delta theta theta previous section see xn jsj gamma gammae pn log inf theta epn log consider progressive mixture estimator prior probability distribution gamma xn proved section gamma epn log inf ae inf theta epn log gamma log oe want balance penalty terms take fi gives penalty fi gamma log fi gamma proposition 
take fi gamma get gamma epn log inf inf theta epn log fi log fi fi 
result course true randomised progressive mixture estimators 

binary trees histograms restrict families subdivisions leading fast algorithms 
cases progressive mixture estimators computed efficiently adapting context tree weighting algorithm willems shtarkov 
exactly base estimation approximation gamma jsj gamma gamma jsj gamma gamma jsj gamma advantage approximation seen computation mixtures 
step explain prove loose doing 
define randomised universal model selection progressive mixture xn ir sample uniform distribution fk ng independent 
get statistical estimator normalise estimator dx gamma dx gamma log gamma log replacing proof section see gammae log inf gammae log gamma log inf gammae log gamma gamma log gamma gamma log inf inf theta log gamma gamma log describe family prior distribution assume purpose divided binary tree nested cells fi notation cells indexed set finite binary strings call words 
assume ffl empty string ffl word 
example case unit interval choose gammak gammal gammak length string recall complete prefix code finite set finite binary strings ffl word prefix word 
ffl maximal inclusion 
complete prefix code seen complete binary tree 
see suffices arrange binary tree considering sons 
prefix code leaves finite subtree infinite tree complete prefix code leaves finite complete subtree interior node sons 
easy see complete prefix code fi sg subdivision write word prefix equivalently subdivision corresponding refinement subdivision corresponding set complete prefix codes consider ae family subdivisions fs sg survival probability ae consider priori distribution ae js gamma gamma ae js sj distribution genealogy tree branching process particle gives birth sons probability ae generation dies probability gamma ae located genealogy case dies probability 
proved theorem theorem 
previous assumptions notations modified randomised progressive mixture estimator satisfies gamma log inf inf theta log gamma gamma log ae gamma ae going see compute 
compute mixtures type wm gammak gamma jsj gamma see advantage product factors functions 
possible computation 
universal model selection tree corresponding set prefixes words finite sigma algebra generated subdivision define backward recursion gamma gamma gammak gamma gamma ae gamma gamma gammak gamma easy check proposition 
stands empty string 
compute need perform number operations order jt sj gamma 
compared number models jsj 
backward recursion jsj case complete tree depth ffi example cells dyadic intervals complete tree depth ffi defines uniform subdivision step size gammaffi see easily ffi gamma 
gamma gamma ff ff ff 
case ffi gamma jsj ff ffi gamma gamma ff modified randomised progressive mixture estimator perform best ffi gamma models time order ffi ffi number trials randomised length second subsample 
justification complexity estimation add modify observation point want compute update computations ffi nested cells 
gamma observations introduce progressively computations consider case falls ffi finer cells random lengths second subsample accounts ffi factor 
cells balanced reasonable choice take ffi order kept bounded results complexity order log 
section generalised general trees 
flexibility choice possible 
refer comparison generalisations context tree weighting method 

model selection dense family models notations section assume delta mm set probability distributions forming model indexed 
assume pn product measure pn omega simplicity straightforward generalisations case exchangeable left reader 
assume ffl inf mm 
assume sample distribution closure family models mm respect kullback distance 
words family models assumed dense set possible sample distributions 
ffl inf inf mm family positive weights 
assumption give suggestive result 
easily seen replaced arbitrary function section 
seen delta inf inf mm gamma log hellinger distance probability measures gamma known true distance 
select delta mm minimising inf mm delta fl fl family positive weights arbitrary positive function fl 
reader obvious modifications required case infimum reached previous equation 
mm epn delta fl eh delta eh delta delta fl eh delta eh delta fl eh delta fl eh delta fl gamma log fl proved theorem 
rate approximation adaptive estimator mean square hellinger distance upper bounded gamma delta inf mm gamma log fl universal model selection 
theorem shows cases rate decrease optimal 
case order infimum reached gamma chosen order fl chosen order gammac possible choose log order 

choice fl possible practice select model simple possible prefer choice fl increasing dimension model fast possible 
usually dimension related choose fl order 

progressive mixture estimator interesting theoretical point view proves possible perform best countable family estimators loss performance due fact splitting sample due presence extra term gammak log arbitrary probability distribution 
remarkable achieved special assumption estimators true sample distribution 
fast practical algorithm case computation mixtures 
treated case adaptive histograms 
may suggest variety applications worked broad definition histograms 
planning give applications generalisations mixture approach regression classification problems near 
barron massart risk bounds model selection preprint de mod statistique universit paris sud orsay 
massart model selection adaptive estimation preprint de mod statistique universit paris sud orsay 
massart minimum contrast estimators preprint de mod statistique universit paris sud orsay 
feder merhav hierarchical universal coding ieee trans 
inform 
theory vol sept 
ryabko twice universal coding probl 
inform 
vol pp 
july sept 
willems shtarkov context tree weighting method basic properties ieee trans 
inform 
theory vol may 
willems shtarkov context weighting general finite context sources ieee trans 
inform 
theory vol sept 
intelligence artificielle math ematiques laboratoire de math ematiques de ecole normale sup erieure du rue ulm paris france 
