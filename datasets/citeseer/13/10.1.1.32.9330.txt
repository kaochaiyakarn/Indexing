nips classifiers sequential inference punyakanok dan roth department computer science university illinois urbana champaign urbana il cs uiuc edu cs uiuc edu study problem combining outcomes different classifiers way provides coherent inference satisfies constraints 
particular develop general approaches important subproblem identifying phrase structure 
markovian approach extends standard hmms allow rich observation structure general classifiers model state observation dependencies 
second extension constraint satisfaction formalisms 
develop efficient combination algorithms models study experimentally context shallow parsing 
situations necessary decisions depend outcomes different classifiers way provides coherent inference satisfies constraints sequential nature data domain specific constraints 
consider example problem chunking natural language sentences goal identify kinds phrases noun phrases verb phrases sentences 
task sort involves multiple predictions interact way 
example way address problem utilize classifiers phrase type recognizes phrase 
clearly constraints predictions instance phrases overlap probabilistic constraints order phrases lengths 
mentioned problem instance general class problems identifying phrase structure sequential data 
develops general approaches class problems utilizing general classifiers performing inferences outcomes 
formalisms directly applies natural language problems shallow parsing computational biology problems identifying splice sites problems information extraction :10.1.1.53.2725
approach markovian framework 
case classifiers functions observation sequence outcomes represent states study markov models inference procedures differ type classifiers details probabilistic modeling 
critical shortcoming framework attempts maximize likelihood state sequence true performance measure interest derivative 
second approach extends constraint satisfaction formalism deal variables associated costs shows model classifier combination problem 
approach general constraints incorporated flexibly algorithms developed closely address true global optimization criterion interest 
approaches develop efficient combination algorithms general classifiers yield inference 
approaches studied experimentally context shallow parsing task identifying syntactic sequences sentences useful large scale language processing applications including information extraction text summarization 
working concrete task allows compare approaches experimentally phrase types base noun phrases nps phrases svs differ significantly statistical properties including length internal dependencies 
robustness approaches deviations assumptions evaluated 
main methods projection markov models pmm constraint satisfaction classifiers cscl shown perform task predicting np sv phrases cscl method tried tasks 
cscl performs better pmm tasks significantly harder sv task 
attribute cscl ability cope better length phrase long term dependencies 
experiments snow classifier provide way combine scores probabilistic framework exhibit improvements standard hidden markov model hmm allowing states depend richer structure observation classifiers 
identifying phrase structure inference problem considered formalized identifying phrase structure input string 
input string phrase substring consecutive input symbols external mechanism assumed consistently stochastically annotate substrings phrases goal come mechanism input string identifies phrases string 
identification mechanism works classifiers attempt recognize input string local signals indicative existence phrase 
assume outcome classifier input symbol represented function local context input string aid external information inferred classifiers indicate input symbol inside outside phrase io modeling indicate input symbol opens closes phrase oc modeling combination 
focuses oc modeling shown robust io especially fairly long phrases 
case classifiers outcomes combined determine phrases input string 
process needs satisfy constraints resulting set phrases legitimate 
types constraints length order formalized incorporated approaches studied 
goal fold learn classifiers recognize local signals combine way respects constraints 
call inference algorithm combines classifiers outputs coherent phrase structure combinator 
performance process measured accurately retrieves phrase structure input string 
quantified terms recall percentage phrases correctly identified precision percentage identified phrases correct phrases 
assume single type phrase input symbol phrase outside 
methods extended deal kinds phrases string 
case natural language processing words sentence additional information include morphological information part speech tags semantic class information wordnet information assumed encoded observed sequence 
markov modeling hmm probabilistic finite state automaton models probabilistic generation sequential processes 
consists finite set states set observations initial state distribution state transition distribution sjs observation distribution ojs 
sequence observations generated picking initial state state produces observation ojs transits new state sjs 
state produces observation process goes reaches designated final state 
supervised learning task observation sequence supervised corresponding state sequence 
allows estimate hmm parameters new observation sequence identify corresponding state sequence 
supervision supplied see sec 
local signals state sequence recovered 
constraints incorporated hmm constraining state transition probability distribution sjs 
example set sjs transition allowed 
hidden markov model combinator recover state sequence hmm wish estimate required probability distributions 
sec 
assume local signals indicate state 
classifiers states outcomes 
formally assume time step sequence 
order information hmm framework compute ojs observing conditional probability ojs directly training data compute classifiers output 
notice hmm assumption probability distributions stationary 
assume obtain classifier need assume distributions 
calculated sjs sjs required distributions hmm need harder approximate treated constant goal find sequence states observations compared sequences 
scheme combine classifiers predictions finding sequence observation sequence dynamic programming 
incorporate classifiers opinions recursive step computing js max sjs js max sjs derived hmm assumptions utilizes classifier outputs allowing extend notion observation 
sec 
estimate observation sequence significantly improve performance 
projection markov model combinator hmms observations allowed depend current state long term dependencies modeled 
equivalently constraints structure restricted having stationary probability distribution state previous 
attempt relax allowing distribution state depend addition previous state observation 
formally independence assumption js js observation sequence find state sequence maximizing js jo js jo model generalizes standard hmm combining state transition probability observation probability function 
state sequence recovered dynamic programming viterbi algorithm modify recursive step max sjs 
model classifiers decisions incorporated terms sjs 
learn classifiers follow projection approach separate sjs functions previous states jsj classifiers projected previous states separately trained 
name projection markov model pmm 
simpler classifiers hope performance improve 
question constitutes observation issue 
sec 
exhibits contribution estimating wider window observation sequence 
related attempts combine classifiers neural networks hmms speech recognition works decade 
similar pmm maximum entropy classifiers 
cases attempt combine classifiers markov models motivated attempt improve existing markov models belief yield better generalization pure observation probability estimation training data 
motivation different 
starting point existence general classifiers provide local information input sequence constraints outcomes goal classifiers infer phrase structure sequence way satisfies constraints 
markov models possibility mentioned earlier optimizes real performance measure interest 
technically novelty worth mentioning wider range observations single observation predict state 
certainly violates assumption underlying hmms improves performance 
constraints satisfaction classifiers section describes different model extension boolean constraint satisfaction csp formalism handle variables outcome classifiers 
assume observed string local classifiers loss generality take distinct values indicating phrase second indicating closing oc modeling 
classifiers provide output terms probability observation 
extend csp formalism deal probabilistic variables generally variables cost follows 
set boolean variables associated problem jv constraints encoded clauses standard csp modeling boolean csp cnf conjunctive normal form formula problem simply find assignment satisfies optimization problem 
associate cost function variable try find solution minimum cost efficient way general scheme encoding phrases variables 
set possible phrases 
non overlapping constraints encoded overlaps yields quadratic number variables constraints binary encoding restriction phrases overlap 
satisfying assignment resulting cnf formula computed polynomial time corresponding optimization problem np hard 
specific case phrase structure find optimal solution linear time 
solution optimization problem corresponds shortest path directed acyclic graph constructed observations symbols legitimate phrases variables csp edges cost edges weights 
construction graph takes quadratic time corresponds constructing cnf formula 
hard see details omitted path graph corresponds satisfying assignment shortest path corresponds optimal solution 
time complexity algorithm linear size graph 
main difficulty determine cost function confidence classifiers 
experiments revealed algorithm robust reasonable modifications cost function 
natural cost function classifiers probabilities define phrase interpretation error selecting error selecting allowing overlap constant biases minimization prefers selecting phrases minimize 
shallow parsing shallow parsing tasks order evaluate approaches 
shallow parsing involves identification phrases words participate syntactic relationship 
observation shallow syntactic information extracted local information examining pattern nearby context local part speech information motivated learning methods recognize patterns 
study identification types phrases base noun phrases np subject verb sv patterns 
chose differ significantly structural statistical properties allows study robustness methods assumptions 
previous problem evaluation concerned identifying layer np sv phrases embedded phrases 
oc modeling learn classifiers predicting open location close location 
technical reasons cases separated inside outside phrase 
consequently classifier may output possible outcomes noi open open inside open outside nci resp 
diagram captures order constraints 
modeling problem modification earlier topic quite successful compared learning methods attempted problem 
nci noi state transition diagram phrase recognition problem 
classification classifier learn states function observation snow multi class classifier specifically tailored large scale learning tasks 
snow learning architecture learns sparse network linear functions targets states case represented linear functions common features space 
snow successfully variety tasks natural language visual processing 
typically snow classifier predicts mechanism activation value target classes 
activation value computed sigmoid function linear sum 
current study normalize activation levels targets sum output outcomes targets states 
verified experimentally training data output state distribution function processing details omitted 
possible account classifiers suggestions inside phrase details omitted 
experiments experimented nps svs show results different representations observations different feature sets classifiers part speech pos information pos additional lexical information words 
result interest 
recall 
precision 
precision recall 
data sets standard data sets problem taken wall street journal corpus penn treebank 
np training test corpus prepared sections section respectively sv phrase corpus prepared sections training section testing 
model study different classifiers 
simple classifier corresponds standard hmm ojs estimated directly data 
observations terms lexical items data sparse yield robust estimates entries left empty 
nb naive bayes snow classifiers feature set conjunctions size pos tags pos words resp 
window size 
table results different methods np sv recognition method np sv model classifier pos tags pos tags words pos tags pos tags words snow hmm nb simple snow pmm nb simple snow cscl nb simple important observation sv identification task significantly difficult np task 
consistent models feature sets 
comparing different models feature sets clear simple hmm formalism competitive models 
interesting significant sensitivity feature base classifiers despite violation probabilistic assumptions 
easier np task hmm model competitive classifiers nb snow 
particular fact significant improvements probabilistic methods achieve input snow confirms claim output snow reliably probabilistic classifier 
pmm cscl perform predicting np sv phrases cscl methods tried tasks 
nps svs cscl performs better significantly harder sv task 
attribute cscl ability cope better length phrase long term dependencies 
addressed problem combining outcomes different classifiers way provides coherent inference satisfies constraints 
viewed concrete instantiation learning reason framework 
focus important subproblem identification phrase structure 
probabilistic framework extends hmms ways approach extension csp formalism 
cases developed efficient combination algorithms studied empirically 
csp formalisms support desired performance measure complex constraints dependencies flexibly markovian approach 
supported experimental results show cscl yields better results particular complex case sv phrases 
side effect exhibits general classifiers probabilistic framework 
includes extensions deal general constraints exploiting general probabilistic structures generalizing csp approach 
acknowledgments research supported nsf iis iis 
abney 
parsing chunks 
berwick editors principle parsing computation psycholinguistics pages 
kluwer dordrecht 
appelt hobbs bear israel tyson 
fastus finite state processor information extraction real world text 
proc 
ijcai 
argamon dagan 
memory approach learning shallow natural language patterns 
journal experimental theoretical artificial intelligence special issue memory learning 
burge karlin 
finding genes genomic dna 
current opinion structural biology 
cardie pierce 
error driven pruning treebanks grammars base noun phrase identification 
proceedings acl pages 
carlson rosen roth 
snow learning architecture 
technical report uiucdcs uiuc computer science department may 
church 
stochastic parts program noun phrase parser unrestricted text 
proc 
acl conference applied natural language processing 

gene identification problem overview developers 
computers chemistry 
freitag mccallum 
information extraction hmms shrinkage 
papers aaai workshop machine learning information extraction 
golding roth 
winnow approach context sensitive spelling correction 
machine learning 

evaluation techniques automatic semantic extraction comparing semantic window approaches 
acl workshop acquisition lexical knowledge text 
grishman 
nyu system muc syntax 
sundheim editor proceedings sixth message understanding conference 
morgan kaufmann publishers 
gusfield pitt 
bounded approximation minimum cost sat problems 
algorithmica 
harris 
occurrence transformation linguistic structure 
language 
haussler 
computational 
trends biochemical sciences supplementary guide bioinformatics pages 
khardon roth 
learning reason 
acm sept 
mackworth 
constraint satisfaction 
shapiro editor encyclopedia artificial intelligence pages 
volume second edition 
marcus santorini marcinkiewicz 
building large annotated corpus english penn treebank 
computational linguistics june 
mccallum freitag pereira 
maximum entropy markov models information extraction segmentation 
proceedings icml 
appear 
morgan bourlard 
continuous speech recognition 
ieee signal processing magazine 
mu punyakanok roth 
learning approach shallow parsing 
emnlp vlc 
rabiner 
tutorial hidden markov models selected applications speech recognition 
proceedings ieee 
ramshaw marcus 
text chunking transformation learning 
proceedings third annual workshop large corpora 
roth 
learning resolve natural language ambiguities unified approach 
proceedings national conference artificial intelligence pages 
roth 
yang ahuja 
learning recognize objects 
cvpr ieee conference computer vision pattern recognition pages 
valiant 
projection learning 
proceedings conference computational learning theory pages 
