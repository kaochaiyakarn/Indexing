optimal brain damage yann le cun john denker sara solla bell laboratories holmdel information theoretic ideas derive class practical nearly optimal schemes adapting size neural network 
removing unimportant weights network improvements expected better generalization fewer training examples required improved speed learning classification 
basic idea second derivative information tradeoff network complexity training set error 
experiments confirm usefulness methods real world application 
successful applications neural network learning real world problems achieved highly structured networks large size example waibel lecun 
applications complex networks presumably larger structured 
design tools techniques comparing different architectures minimizing network size needed 
importantly number parameters systems increases overfitting problems may arise devastating effects generalization performance 
introduce new technique called optimal brain damage reducing size learning network selectively deleting weights 
show automatic network minimization procedure interactive tool suggest better architectures 
basic idea possible take perfectly reasonable network delete half weights wind network works just better 
applied situations complicated problem solved system optimal limited amount training data 
known theory denker baum haussler solla experience lecun fixed amount training data networks weights generalize 
hand networks weights power represent data accurately 
best generalization obtained trading training error network complexity 
technique reach tradeoff minimize cost function composed terms ordinary training error plus measure network complexity 
schemes proposed statistical inference literature see akaike rissanen vapnik nn literature rumelhart chauvin hanson pratt mozer smolensky 
various complexity measures proposed including vapnik chervonenkis dimensionality vapnik chervonenkis description length rissanen 
time honored albeit inexact measure complexity simply number non zero free parameters measure choose see 
free parameters connections constrained networks connections controlled single parameter 
cases statistical inference literature priori heuristic information dictates order parameters deleted example family polynomials smoothness heuristic may require high order terms deleted 
neural network obvious order parameters deleted 
simple strategy consists deleting parameters small saliency deletion effect training error 
things equal small magnitude parameters saliency reasonable initial strategy train network delete small magnitude parameters order 
deletion network retrained 
course procedure iterated limit reduces continuous weight decay training disproportionately rapid decay small magnitude parameters 
fact network minimization schemes implemented non proportional weight decay rumelhart chauvin hanson pratt gating coefficients mozer smolensky 
generalization performance reported increase significantly somewhat small problems examined 
drawbacks techniques require fine tuning pruning coefficients avoid catastrophic effects learning process significantly slowed 
methods include implicit hypothesis appropriate measure network complexity number parameters number units network 
main points move approximation magnitude equals saliency propose theoretically justified saliency measure 
technique uses second derivative objective function respect parameters compute saliencies 
method validated handwritten digit recognition network trained backpropagation lecun 
optimal brain damage objective functions play central role field reasonable define saliency parameter change objective function caused deleting parameter 
prohibitively laborious evaluate saliency directly definition temporarily deleting parameter reevaluating objective function 
fortunately possible construct local model error function analytically predict effect perturbing parameter vector 
approximate objective function taylor series 
perturbation ffiu parameter vector change objective function ffie ffiu ii ffiu ij ffiu ffiu jj ffiu components ffiu components gradient respect ij elements hessian matrix respect ij goal find set parameters deletion cause increase problem practically insoluble general case 
reason matrix enormous theta terms parameter network difficult compute 
introduce simplifying approximations 
diagonal approximation assumes ffie caused deleting parameters sum ffie caused deleting parameter individually cross terms neglected third term right hand side equation discarded 
extremal approximation assumes parameter deletion performed training converged 
parameter vector local minimum term right hand side equation neglected 
furthermore local minimum ii non negative perturbation parameters cause increase stay 
thirdly quadratic approximation assumes cost function nearly quadratic term equation neglected 
equation reduces ffie ii ffiu computing second derivatives need efficient way computing diagonal second derivatives ii procedure derived lecun basis fast back propagation method extensively various applications becker lecun lecun lecun 
procedure similar backpropagation algorithm computing derivatives 
outline procedure details 
assume objective function usual mean squared error mse generalization additive error measures straightforward 
expressions apply single input pattern afterward averaged training set 
network state computed standard formulae ij state unit total input weighted sum squashing function ij connection going unit unit shared weight network single parameter control connections ij set index pairs 
chain rule diagonal terms kk ij summand expanded basic network equations ij second derivatives back propagated layer layer li need boundary condition output layer specifying second derivative respect layer weighted sums gamma gamma units output layer 
seen computing diagonal hessian order complexity computing gradient 
cases second term right hand side equations involving second derivative neglected 
corresponds known levenberg marquardt approximation interesting property giving guaranteed positive estimates second derivative 
recipe procedure carried follows 
choose reasonable network architecture 
train network reasonable solution obtained 
compute second derivatives kk parameter 
compute saliencies parameter kk 
sort parameters saliency delete low saliency parameters 
iterate step deleting parameter defined setting freezing 
variants procedure devised decreasing values parameters simply setting allowing deleted parameters adapt set 
experiments simulation results section obtained back propagation applied handwritten digit recognition 
initial network highly constrained sparsely connected having connections controlled free parameters 
trained database segmented handwritten zipcode digits printed digits containing approximately training examples test examples 
details obtained companion lecun 
magnitude parameters parameters objective function db versus number parameters lower curve magnitude parameter deletion upper curve 
predicted actual objective function versus number parameters 
predicted value lower curve sum saliencies deleted parameters 
shows objective function increases right left number remaining parameters decreases 
clear deleting parameters order saliency causes significantly smaller increase objective function deleting magnitude 
random deletions tested sake comparison performance bad curves shown scale 
shows objective function increases right left number remaining parameters decreases compared increase predicted quadratic extremum diagonal approximation 
obtained approximately deleted parameters approximately parameters 
point curves split reasons diagonal terms equation disproportionately important number deleted parameters increases higher quadratic terms important larger valued parameters deleted 
parameters parameters objective function db versus number parameters retraining upper curve retraining lower curve 
curves training set test set 
shows log mse training set test set retraining 
performance training set test set retraining stays parameters total deleted 
interactive tool network design analysis 
contrasts usual view weight deletion automatic procedure 
specifically prepared charts depicting saliency parameters digit recognition network reported year lecun 
surprise large groups parameters 
able second layer reducing number parameters factor 
training set mse increased factor generalization mse increased 
category classification error test set decreased indicates mse optimal objective function task 
motivated architectural changes seen comparing parameter network lecun parameter network lecun 
outlook optimal brain damage interactively reduce number parameters practical neural network factor 
obtained additional factor delete parameters automatically 
network speed improved significantly recognition accuracy increased slightly 
emphasize starting point state art network 
easy start foolish network large improvements technique help improve network particularly valuable 
believe techniques scratch surface applications second derivative information 
particular able move approximation complexity equals number free parameters second derivative information 
derive improved measure network information content complexity 
allows compare network architectures task contact notion minimum description length mdl rissanen 
main idea simple network description needs small number bits generalize correctly complex network presumably extracted essence data removed redundancy 
acknowledgments postal service contractors providing database 
rich howard larry jackel helpful comments 
especially david rumelhart sharing unpublished ideas 
akaike 

statistical models time series analysis 
proceedings icassp pages tokyo 
ieee 
baum haussler 

size net gives valid 
neural computation 
becker lecun 

improving convergence back propagation learning sec ond order methods 
touretzky hinton sejnowski editors proc 
connectionist models summer school pages san mateo 
morgan kaufman 
chauvin 

back propagation algorithm optimal hidden units 
touretzky editor advances neural information processing systems volume denver 
morgan kaufmann 
denker schwartz solla howard jackel hopfield 

large automatic learning rule extraction generalization 
complex systems 
hanson pratt 

comparisons constraints minimal network construction back propagation 
touretzky editor advances neural information processing systems volume denver 
morgan kaufmann 
lecun 

de apprentissage connectionist learning models 
phd thesis universit curie paris 
lecun 

generalization network design strategies 
pfeifer fogelman steels editors connectionism perspective zurich switzerland 
elsevier 
extended version published technical report university toronto 
lecun boser denker henderson howard hubbard jackel 

backpropagation applied handwritten zip code recognition 
neural computation 
lecun boser denker henderson howard hubbard jackel 

handwritten digit recognition backpropagation network 
touretzky editor advances neural information processing systems nips denver morgan kaufman 
mozer smolensky 

skeletonization technique trimming fat network relevance assessment 
touretzky editor advances neural information processing systems volume denver 
morgan kaufmann 
rissanen 

stochastic complexity statistical inquiry 
world scientific singapore 
rumelhart 

personal communication 
solla schwartz tishby levin 

supervised learning theoretical framework 
touretzky editor advances neural information processing systems volume denver 
morgan kaufman 
vapnik 

inductive principles search empirical dependences 
proceedings second annual workshop computational learning theory pages 
morgan kaufmann 
vapnik chervonenkis 

uniform convergence relative frequencies events probabilities 
th 
prob 
applications 
waibel 

consonant recognition modular construction large phonemic time delay neural networks 
touretzky editor advances neural information processing systems volume pages denver 
morgan kaufmann 
