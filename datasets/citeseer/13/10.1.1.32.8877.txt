october analysis belief propagation turbo decoding graph gaussian densities benjamin van roy stanford university stanford edu motivated success decoding turbo codes provide analysis belief propagation algorithm turbo decoding graph gaussian densities 
context able show certain conditions algorithm converges somewhat surprisingly density generated belief propagation may differ significantly desired posterior density means densities coincide 
computation posterior distributions tractable densities gaussian belief propagation setting may appear unwarranted 
primary motivation studying belief propagation context stems desire enhance understanding algorithm dynamics non gaussian setting gain insights excellent performance turbo codes 
densities gaussian belief propagation may provide efficient alternative traditional inference methods 
key words approximate inference belief network belief propagation gaussian densities turbo decoding 
probability distributions provide tool characterizing beliefs unobserved quantities relationships 
observations beliefs change posterior distributions evolve reflect improved understanding 
unfortunately process inference computing posterior distributions entails integration high dimensional spaces typically intractable 
exception arises densities gaussian 
case posterior distributions gaussian computed efficiently represented compactly terms means covariances 
case admits efficient computation arises conditional independencies random variables form convenient pattern 
belief networks markov random fields offer approaches characterizing conditional independencies terms directed undirected graphs respectively 
case graph singly connected cycles belief propagation efficient inference algorithm applicable 
distributions interest gaussian accommodate singly connected graphs 
cases exact inference typically intractable approximations called 
surprisingly belief propagation developed singly connected graphs shown deliver impressive performance applications involving graphs cycles 
notable example turbo decoding algorithm turbo codes 
turbo decoding algorithm approximation method delivered impressive performance certain coding applications 
inference task originally addressed turbo decoding algorithm involves computing distribution underlying message receiving encoded transmission noisy communication channel 
structure encoding scheme turbo codes leads efficient transmission rates leaves decoder job solving intractable inference problem 
turbo decoding algorithm proven effective approximation method task 
initial development supported mathematical theory spectacular empirical success received surprise excitement intrigue 
turns turbo decoding algorithm equivalent belief propagation 
connection noted frey kschischang mceliece 
particular mceliece mackay cheng interpretation turbo decoding algorithm application belief propagation graph cycles 
belief propagation developed singly connected graphs application presence cycles done turbo decoding supported pre existing principles 
excitement spawned success turbo decoding algorithm came reexamination iterative decoding algorithms codes graphs message passing algorithms 
message passing algorithms proposed decades earlier coding literature bear similarities turbo decoding algorithm 
designed decoding low density parity check codes message passing algorithms turned correspond belief propagation graphs cycles 
furthermore empirical study establishes message passing algorithms share impressive performance demonstrated turbo decoding 
kschischang frey shown iterative decoding algorithms belief propagation various message passing algorithms unified single framework involving distributed marginalization algorithm functions characterized factor graphs 
show algorithms artificial intelligence signal processing digital communications developed independently fit naturally framework 
similar unifying framework proposed 
signs promise belief propagation graphs cycles inference problems arising coding 
positive results generated example empirical case studies motivated applications image processing medical decision making 
case studies algorithm fails factors influencing performance understood 
analytical focused identifying suitable classes problems understanding properties foster success 
analyses focusing context coding extend early gallager shed light success turbo decoding message passing algorithms 
rough terms thrust line research involves establishing cycles arising relevant coding applications generally long showing allows belief propagation singly connected graphs additional specialized context low density parity check codes strengthens results 
line analytical aimed understanding behavior belief propagation general graphs cycles 
starting point researchers studied case involving graph single cycle 
case useful right exact inference tractable presence single cycle 
study case lead concise results enhance state understanding 
particular results pertaining case single cycle include 
belief propagation converges unique stationary point 

random variables binary valued component wise maximum likelihood estimates offered resulting approximation concur true maximum likelihood values 
unfortunately line analysis employed case single cycle immediately extend graphs multiple cycles 
study belief propagation new angle analyzing dynamics restrictive setting densities gaussian 
focus attention case dependence structure random variables similar appeared original turbo decoding application 
graph captures dependency referred turbo decoding graph 
case exact inference tractable belief propagation entirely necessary 
belief propagation may provide efficient method solving certain inference problems context 
primary motivation studying gaussian case provide setting amenable streamlined analysis 
clear understanding may offer insights behavior belief propagation general settings possibly shed light success turbo codes 
contributions analysis include certain concise results concerning belief propagation densities gaussian 
belief propagation initialized gaussian densities iterate gaussian lemma 

associated sequence covariance matrices converges unique stationary point theorem 

certain conditions sequence mean vectors converges unique stationary point theorem proposition 

belief propagation converges mean resulting approximation coincides true posterior density theorem 
note distribution gaussian mean corresponds maximum likelihood value result parallels aforementioned result concerning case graph single cycle binary variables 
preparing aware related initiatives involving analysis belief propagation densities gaussian graphs possess cycles 
weiss freeman studying case dimensional lattice 
able show belief propagation converges mean resulting approximation coincides true posterior distribution 
weiss freeman derived equations characterizing dynamics means covariance matrices generated belief propagation 
time frey studied case involving graphical structures generalize employed turbo decoding 
derived equation satisfied stationary points provided analysis relating convergence means spectral radius particular matrix analyze related matrix section 
conducted empirical study 
coincidentally short papers describing weiss freeman frey summarizing results simultaneously submitted conference 
organized follow 
section provide working definition belief propagation algorithm 
lend concreteness definition example section situation belief propagation efficient traditional inference methods 
section discuss specialization belief propagation gaussian case 
convergence analysis section 
section prove mean approximation generated belief propagation coincides desired posterior distribution 
presenting experimental results close concluding section 
belief propagation turbo decoding graph consider random variable takes values independent components 
denote prior density random variables conditionally independent example represent outcomes independent transmissions signal memoryless communication channel 
turbo decoding graph depicting dependence random variables bayesian network factor graph representation 
definition belief propagation exploit dependence structure random variables 
observed want infer posterior density conditioned 
obtained computing densities turbo decoding graph bayesian network representation factor graph representation 
function resp 
corresponds conditional density resp 
function gi corresponds prior density xi 
conditioned second conditioned 
normalizing operator defined dx multiplication division carried pointwise 
unfortunately known computation intractable 
burden associated storing manipulating high dimensional densities appears primary obstacle 
motivates idea limiting attention densities factor 
context convenient define operator generates density factors possessing marginals density 
particular operator defined xi xi ai density xi xi xi xn 
aim computing proxy unfortunately problem intractable 
belief propagation viewed iterative algorithm approximating operators defined gp density belief propagation applicable cases computation operations tractable 
algorithm generates sequences initialized densities useful approximation factor 
hope converges example example hidden markov model 
bn preceding definition relied operators subroutines 
sake concreteness discuss section certain situations computation tractable 
situations belief propagation may constitute legitimate approximation scheme 
describe example terms markov random fields reviewing semantics graphical modeling framework 
markov random field undirected graph node corresponding random variable 
arcs convey information conditional independencies 
particular mutually exclusive sets nodes separates random variables corresponding conditionally independent corresponding conditioned corresponding term separates refers fact path node node visits node markov random field singly connected belief propagation offers efficient approach inference 
particular variables observed posterior distribution remaining variables furthermore marginal distributions individual variables efficiently computed 
common class markov random fields accommodates efficient inference class hidden markov models 
depicts markov random field associated simple hidden markov model 
nodes labeled corresponding random variables 
easy see graph singly connected common inference problem computing posterior distribution 
conditioned 
bn efficiently solved belief propagation 
presence cycles inference complicated intractable 
describe class problems belief propagation may constitute useful approximation scheme 
consider singly connected markov random fields nodes 
nodes correspond components dimensional random vectors correspond 
graph belief propagation offers efficient inference observed 
consider augmented markov random field containing nodes corresponding components random vector arcs include connecting components connecting components example markov random field 
note presence cycles 

furthermore additional arcs connect component components 
illustrated example possess cycles 
presence cycles traditional exact inference method requires construction junction tree nodes tree correspond cliques triangulated graph 
resulting clique generally large due presence cycles 
running time algorithms exponential clique size exact inference typically infeasible problems 
presence cycles render inference tasks intractable forms inference performed efficiently 
example efficiently computed observation posterior distribution belief propagation 
possible nodes corresponding ignored remaining nodes form singly connected graph 
similarly efficiently inferred 
observed posterior distribution observe inference complex 
context belief propagation may provide suitable approximation algorithm 
ideally algorithm generate marginal distributions individual components conditioned simultaneous observation 
assume prior distribution factors equivalently components initially independent 
density posterior density conditioned prior density 
consequently density factors appropriately altering priors keeping fixed conditional probabilities conditioned applying belief propagation efficiently compute 
turn enables efficient computation pointwise multiplication normalization tractable functions factor 
operator similarly accommodates efficient computation 
markov random field tractable implementations application belief propagation feasible 
belief propagation generate useful approximations separate issue 
gaussian case remainder focus setting joint distribution gaussian 
context application belief propagation may appear unwarranted tractable algorithms computing conditional distributions priors gaussian 
primary motivation provide setting amenable streamlined analysis concise results 
worth noting belief propagation may provide efficient means traditional algorithms solving certain gaussian inference problems 
discuss possibility concluding section 
define notation facilitate exposition 
denote set covariance matrices diagonal positive definite 
denote set gaussian densities covariance matrices write denote gaussian density mean vector covariance matrix matrix denote diagonal matrix entries equal diagonal elements gaussian density 
diagonal matrices write dii dii dii dii pairs diagonal matrices write similarly write matrices nonsingular define matrix abbreviate denote matrix 
vectors assumed column vectors explicitly stated 
random variables jointly gaussian densities gaussian 
define means covariance matrices satisfying 
assumptions concerning parameters 
assumption identity matrix 
positive definite 
positive definite 
assumption simplifies exposition sacrifice generality 
problem nondegenerate gaussian prior transformed meet requirement appropriate translation scaling coordinate system 
second assumption implies observations provide information pertinent component final assumption hand requires observation rules possible outcomes outcome possible observation prior posterior probabilities may differ substantially 
mean covariance matrix determined 
nature dependence identified lemma reused various purposes subsequent sections 
lemma positive definite 
positive definite uv result follows simple algebra omit proof 
respect course implications turns initialized gaussian densities iterates generated belief propagation fact simplifies analysis algorithm dynamics need attend sequences means covariance matrices 
particular define sequences fact iterates remain gaussian consequence lemma proof provided appendix lemma set closed 
follows lemma domain mappings act densities represented terms operations mean vectors covariance matrices 
provide characterizations operations form lemma 
concise statement lemma define notation 
functions defined ad furthermore functions defined da da ad ad 
lemma follows 
lemma dg dg dg dg dg 
lemma dynamics belief propagation characterized postpone proof lemma appendix convergence analysis immediate consequences lemma guide general structure convergence analysis 
covariance matrices generated belief propagation evolve independently mean vectors 
fact leads studying dynamics covariance matrices paying attention mean vectors 
show sequence covariance matrices converges unique stationary point 
denoting stationary points allows approximate dynamics means large second consequence lemma functions affine arguments renders convergence analysis amenable tools linear systems 
unfortunately sequences covariance matrices sequences means converge 
provide conditions convergence guaranteed 
stability particular matrix constitutes sufficient condition global convergence mean vectors 
show set lead stability matrix invariant certain type transformation 
addition facilitate understanding provide simpler conditions matrix stable 
preview state rough terms conditions ensures convergence 
complementary proposition 
diagonal nearly diagonal proposition 
conditioned words matrix ratio largest smallest eigenvalue large 
proposition analyzing conditions customized argument 
unified approach offers interpretable means distinguishing convergent cases desirable finding approach remains open problem 
move formal statements results corresponding analyses 
subsection addresses convergence sequences covariance matrices dynamics mean vectors treated section 
convergence covariance matrices defining clear lemma theorem establishes sequence converges point independent initial iterate 
theorem operator possesses unique fixed point furthermore denoting fixed point lim operator uniquely determined follows theorem unique fixed point completely determined covariance matrices conditional densities respectively 
ease exposition explicit dependence 
proof theorem relies lemma 
lemma captures essential properties operator proof result appendix lemma continuity function continuous 
monotonicity 
boundedness exist matrices scaling lemma establishes convergence sequence covariance matrices initialized identity matrix 
lemma sequence converges fixed point proof lemma 
follows monotonicity lemma 
bounded pair matrices lemma sequence converge furthermore continuous lemma limit fixed point 
lemma limit exists lemma establishes unique fixed point 
lemma unique fixed point proof lemma fixed point different fixed point 
follows lemma 
monotonicity lemma sup note defined positive definite 
furthermore 
follows lemma addition due monotonicity lemma 
implies existence contradicts definition 
follows unique fixed point proof theorem lemma established uniqueness fixed point lemma asserts converges fixed point 
complete proof theorem need show converges follows monotonicity convergence lemma 
particular converges 
general case convergence follows fact consequence lemmas 
considering starting point sequence leads preceding case established convergence 
address case 
sup lemma follows monotonicity lemma fixed point 
uniqueness fixed point converges continuous limit viable limit 
converge monotonicity 
complete proof consider case arbitrary pair case exist matrices di 
monotonicity previous arguments establish converge consequently converge 
convergence mean vectors sequences covariance matrices sequences mean vectors converge 
section establish sufficient conditions ensure convergence 
show convergence guaranteed stability certain matrix defined ac unfortunately matrix factors influencing stability difficult interpret 
consequently remainder section devoted understanding properties give rise stable establishing interpretable conditions ensure stability convergence mean vectors 
stating proving result linking convergence stability purpose theorem associated analysis denote spectral radius matrix 
theorem exist vectors sequence converges 
theorem provides sufficient necessary condition convergence 
matrix difficult interpret condition offers little insight factors influencing convergence 
proving theorem provide subsequent subsections interpretable conditions 
move prove theorem 
rely lemma somewhat standard flavor 
state result provide proof appendix lemma ak sequence matrices converges bk sequence vectors converges consider sequence vectors xk xk bk 
exists vector sequence xk converges 
proof theorem recall lemma mean vectors evolve rewrite 
highlight relation dynamics addressed lemma introduce additional notation 
ck rk tk defined ck tk rk ck tk ck ck rk theorem asserts converges 
follows matrices ck tk ck rk converge 
furthermore limit convergence ck tk ck mam matrix nonsingular matrix 
result follows lemma 
region convergence know theorem sufficient necessary condition convergence mean vectors 
denote set satisfying assumption 
interpreted region space symmetric positive definite matrices belief propagation converges 
section show invariant certain type transformation 
result provide information shape section demonstrate certain classes behaved symmetric positive definite matrices belong proceed main result section introduce notation 
symmetric matrix min max denote smallest largest eigenvalues respectively 
proposition 

satisfy assumption 
denote unique fixed point sequence covariance matrices generated belief propagation covariance matrices respectively 
proof hard verify follows definition ac ac ac ac unique fixed point theorem follows equivalently ac ac similar argument shows follows theorem unique fixed point sequence covariance matrices generated belief propagation covariance matrices respectively 
ac desired result follows 
previous proposition provides information shape collection inverses covariance matrices previous proposition suggests open set centered consists rays emanating boundary set 
consequently conjecture region convergence star shaped center origin 
currently formal proof result plan pursue 
result appears resemble result reported theorem stability fixed points general turbo decoding 
sufficient conditions previous section showed covariance matrices lead convergence mean vectors invariant certain type transformation 
result provides information shape region convergence 
unfortunately help determining particular pair covariance matrices lead convergent sequence mean vectors 
section offer sufficient conditions ensure stability matrix convergence mean vectors 
proofs conditions quite complicated defer appendices 
focus insights derived conditions 
condition expressed proposition proof appendix proposition symmetric positive definite matrix positive definite max max max max note positive definite eigenvalues 
implies range allowable proposition includes zero 
follows open set containing equal close mean vectors converge 
general proposition shows mean vectors converge covariance matrices complementary sense total variance measured large 
degree complementarity captured parameter 
increases variance decreases relative increases 
correspond situation additional errors introduced resulting greater uncertainty expected value increase variance 
proposition tells belief propagation converges provided corresponding increase precision associated estimate expected value decrease variance 
furthermore start fairly certain estimate max see belief propagation converge despite wide range variation covariance matrices range inverse proportional largest eigenvalue 
dealing gaussians components conditioned independent equivalently factors easy see belief propagation factors follows density converges implies 
furthermore note 
independence components conditioned leads analogous outcome 
gaussian case independence corresponds fact covariance matrix diagonal 
argument discussed context general distributions implies belief propagation converges diagonal 
stronger result formalized proposition establishes convergence holds range matrices nearly diagonal proof proposition appendix proposition li ui defined li min ui max 

li ui discuss certain interpretation proposition 
diagonal li ui 
extreme case leaves leeway requirement li ui 
analogous extreme case arises diagonal 
diagonal grow bounded arbitrarily large 
event viewed measure far diagonal alternatively correlated components observation 
furthermore product li ui combines measure requirement product allows covariance matrix diagonal 
proposition places constraints convergence guaranteed discussed covariance matrices nearly diagonal satisfy constraints 
proposition extends result showing elements small relative diagonal elements belief propagation converges 
proof proposition follows directly proposition refer reader appendix proposition satisfying assumption exist discuss interpretation result coding context 
covariance written matrices follows lemma interpreted covariance matrix conditional density prior density variance similar interpretation applies increases uncertainty prior estimate random variable decreases 
thought signal noise ratio recall represent covariances conditional density respectively 
covariances encapsulate correlations information bits conditioned observed transmissions 
correlations determined encoding scheme channel characteristics 
viewing perspective result proposition implies encoding scheme channel characteristics threshold signal noise ratio exceeds threshold belief propagation converges 
result appears related results reported 
observed agrawal vardy codes finite length thresholds signal noise ratio higher turbo decoding converges signal noise ratio algorithm diverges 
expect similar result hold context gaussian densities 
unfortunately currently formal proof result 
plan pursue 
propositions show covariance matrices close diagonal mean vectors converge 
identify additional situation convergence occurs involves covariance matrices conditioned result stated proposition proof appendix proposition 
min max min max immediate corollary proposition max min 
means converge covariance matrices conditioned example divergence section provide example demonstrates possibility divergent sequence mean vectors 
note matrix variable ui proposition bounded 
implies belief propagation converge matrices 
consider matrices particular choice turns information hard verify 
mean vectors diverge 
analysis fixed point established covariance matrices generated belief propagation converge certain conditions means 
section show limits convergence may provide useful information relating desired posterior density 
particular turns somewhat surprisingly mean approximation resulting belief propagation coincides formalize result limiting means covariance matrices denoted furthermore limiting densities 
theorem establishes main result section mean density generated belief propagation coincides desired posterior density theorem denote limit sequence covariance matrices suppose sequences mean vectors tively 
converge ac 
respec proof theorem relies lemma provides equation relating means associated fixed points 
hard show ac ac statement defined 
lemma denote limit sequence covariance matrices suppose sequences mean vectors tively 
ac converge ac proof 
belief propagation follows 
respec denote fixed points result consequence lemma fact alter mean density 
proof theorem lemma mean ac show expressions equal 
multiplying equations lemma appropriate matrices obtain follows ac ac 
ac implies note ac 
ac 
ac follows 
relative errors mean iteration number relative errors mean iteration number relative errors covariance iteration number relative errors mean iteration number evolution errors representative runs belief propagation 
experimental results limits convergence belief propagation provide approximation established mean approximation coincides desired posterior density 
expect covariance matrix approximates bear relation unfortunately illustrated experimental results section expectations appear inaccurate 
performed experiments involving dimensional gaussian densities dimensional instance 
covariance matrices generated independent random matrices elements drawn uniform distribution 
means generated independent random vectors elements drawn uniform distribution 
shows evolution errors representative runs belief propagation dimensional problems 
graph plots relative root mean squared error mean mean approximation generated belief propagation kth iteration 
relative root mean squared error referring root mean squared difference vectors divided root mean squared value vector 
indicated analysis belief relative errors mean trial number relative errors mean trial number relative errors covariance trial number relative errors mean trial number errors iterations 
densities generated th iteration proxy fixed points belief propagation 
propagation converges error converges zero 
second chart plots relative root mean squared error covariance 
matrices diagonal treat diagonal elements components vectors measuring root mean squared error 
covariances converge agreement theorem ultimate errors far zero 
final graphs plot relative root mean squared errors means respectively 
means converge ultimate errors large 
plots data different experiments involving dimensional problems 
experiment belief propagation executed iterations 
measuring errors densities generated iterations assumed equal stationary points horizontal axes labeled indices problem instances 
graphs exhibit phenomenon observed case dimensional problems errors mean close zero errors associated statistics vary dramatically 
worth noting reported experiments sequences mean vectors appeared converge 
larger problems observed divergent cases rare 
suggests may sufficient conditions convergence satisfied 
finding conditions remains open problem 
closing remarks shown densities gaussian belief propagation converges mean associated limit convergence coincides desired posterior density 
intriguing note context communications objective choose code word comes close transmitted code natural way involves assigning code word maximizes conditional density highest chance correct 
gaussian case studied corresponds mean quantity computed correctly belief propagation 
interesting explore generalizations line analysis 
direction expand arguments encompass belief propagation general network topologies gaussian densities 
interesting probably challenging pursuit develop theory pertaining general non gaussian densities 
note suggest context gaussian densities belief propagation may prove useful 
reconsider example coupled hidden markov model described section 
suppose prior distributions associated coupled hidden markov model gaussian 
computation conditional mean tractable traditional methods belief propagation may provide efficient alternative explain 
densities gaussian mean conditioned computation mean may carried inversion relevant symmetric positive definite matrices takes order operations 
additional computation required obtain 
consider alternative approach uses belief propagation 
recall belief propagation computes sequences discussed section computation iteration done efficiently belief propagation procedure requires operations iteration 
algorithm converges mean resulting approximation coincides 
algorithm converges iterations comes close limit point obtain approximation sn operations 
large result substantial computational savings 
unfortunately bound proximity mean density generated belief propagation iterations 
experimental results suggest belief propagation converges fairly quickly 
notion belief propagation compute mean posterior distribution quickly traditional approaches raises tantalizing possibility algorithm potential variants able accelerate solution similar tasks numerical computation 
acknowledgments research supported part stanford university award 
authors grateful david forney michael jordan pointers literature turbo decoding inference early stages research 
michael saunders useful discussions linear algebra 
anonymous reviewers detailed thoughtful comments suggestions 
lemmas matrix algebra section collect useful lemmas matrix algebra 
results appendices 
lemma states inequality due bellman 
lemma symmetric positive definite matrix ax easy see matrix inversion operator commute 
lemma reflects possible consequences reordering 
lemma symmetric positive definite matrix proof letting ei ei unit vector ith component equal aii ii ax inequality follows lemma 
follows ii aii immediately leads desired result 
lemma states inequality due bergstrom 
lemma symmetric positive definite matrices 
denote sub matrices symmetric positive definite obtained deleting th row column 
denotes determinant matrix lemma reflects potential consequences distributing certain combination matrix inversions operator sum 
lemma symmetric positive definite matrices 
proof nonsingular matrix known ii follows lemma equivalently ii ii ii proof lemmas appendix contains proof lemmas 
prove result prove lemma 
lemma positive definite diagonal matrices 
proof suffices prove result 
proof similar 
positive definite assumption defined positive definite addition follows lemma implies equivalently definition follows 
implies positive definite diagonal matrix 
proof lemma 
proof suffices prove result 
proof similar 
recall density consider density 
positive definite assumption positive definite 
follows lemma implies follows definition lemma matrix defined positive definite 
application lemma implies gaussian density covariance matrix simply 
lemma tells mean density equivalently fg 
follows fact mean density written fg simply 
obvious lemma direct corollary lemma 
proof lemma continuity continuity operator domain follows immediately definition 
monotonicity note 
starting definition implies lemma appendix asserts diagonal follows 
implies equivalently 
analogous argument shows 

boundedness follows definition assumption know positive definite 
follows lemma appendix diagonal positive definite follows analogous argument shows scaling establishing definition application lemma appendix implies implies positive definite assumption implies bound implies 
follows analogous argument shows result follows 
proof lemma proof lemma relies results 
standard flavor state proof 
lemma yk sequences non negative real numbers 
lim yk 
yk kyk lim lemma matrix exist constant proof lemma 
proof assume 
sequence xk defined 
follows xk xk xk xk xk ak xk bk 
recursion show implies xk xk ak xk bk xk xk ak xk bk ak xk xk ak xk bk ak xk xk ak xk bk inequality follows lemma 
define sequence zk zk ak xk xk ak xk bk definition zk follows zk zk ak xk xk ak xk bk ak zk ak xk bk inequality follows fact xk xk zk 
sequence ak converges follows lim ak 
sequence xk converges 
lim ak xk bk 
follows lemma sequence zk converges 
xk xk zk sequence xk converges follows sequence xk converges 
established convergence sequence xk 
proof case similar 
modification result lemma 
case sufficiently large easy see argument works case 
proof proposition proof proposition relies lemmas 
relates stability sub matrices 
lemma 
proof recall matrix ac diagonal matrix defined defined ac definition implies mt easy see mt ac matrices ac ac ac symmetric ab ac ac mt ac ac ac ac equality follows symmetry ac 
note eigenvalues invariant similarity transformations analogous argument shows ac ac mt ac result follows fact mt positive reals 
lemma generalizes result case positive definite matrices 
lemma suppose symmetric positive definite matrices 
eigenvalue 
proof note defined symmetric positive definite matrix 
eigenvalue associated eigenvector may complex vector 
definition cu ba pre multiplying equation follows pre multiply equation uh conjugate transpose linear algebra real symmetric matrix vector possibly complex real number 
real number 
implies real vector 
equation written symmetric positive definite matrices follows 
final lemma shows convergence mean vectors 
lemma sequence covariance matrices converge unique fixed point 

proof recall theorem sequence covariance matrices converges unique fixed point show fixed point follows satisfy ac equal follows symmetry lemma positive definite 
follows lemma appendix implies 
recall matrix defined ac ac fact assumption lemma positive definite follows lemma eigenvalues matrix 
implies equivalently 
follows lemma 
ac 
proof proposition hard verify satisfy assumption 
denotes unique fixed point sequences covariance matrices belief propagation covariance matrices lemma 
follows theorem satisfies equation matrix defined ca ca defined standard fact linear algebra symmetric positive definite matrix ii max lemma follows positive definite 
equality follows definition similar argument shows follows theorem unique fixed point sequences covariance matrices generated belief propagation covariance matrices respectively 
know theorem associated sequence mean vectors converge ac immediate definition ac ac ac lemma desired result follows 
proof proposition proof proposition relies lemmas 
deals eigenvalues product symmetric positive definite matrix positive definite diagonal matrix quantities appear definition li ui 
lemma symmetric positive definite matrix positive definite diagonal matrix 
min min du au max max du au proof suffices prove result min proof max similar 
symmetric positive definite matrix orthogonal matrix positive definite diagonal matrix 
real number matrix defined reader easily verify normal rules exponentiation apply case 
linear algebra symmetric positive definite matrix min min kv min kv eigenvalues preserved similarity transformation min min min da min da da equality follows fact da symmetric positive definite matrix 
min min min min da da aa du au equality follows identification fact nonsingular 
ratio right hand side equation scale invariant min min du au second lemma provides bound fixed point lemma proof fixed point follows definition ac 
lemma positive definite diagonal matrix 
follows lemma appendix application lemma appendix implies result established entirely analogous means 
equipped lemmas move prove proposition 
proof proposition recall matrix defined ac find upper bound eigenvalues ac matrix transpose possess eigenvalues 
suffices consider eigenvalues eigenvalue 
definition implies follows lemma implies min ac associated eigenvector max positive definite lemma min max min max lemma definition proposition follows min max 
arbitrary conclude eigenvalues 
analogous argument shows eigenvalues ac bounded 
fact li ui non negative lemma follows consequently li ui ac ac desired result follows lemma 
proof proposition inf max symmetric positive definite matrices follows defined 
furthermore easy verify satisfy assumption follows lemma appendix max max max max similar argument shows max max 

application lemma appendix implies min min follows lemma appendix min follows similar argument shows min min min desired result follows proposition 
proof proposition min 

max 
symmetric positive definite matrix max denote largest diagonal element follows min min ku max max ku min max max application lemma inequality implies max max max min max min max min max min analogous argument shows min min min max max max max max min max li ui defined proposition 
min max follows min max max min max 
min exactly argument show suppose max 
min min max min 
max multiplying inequality max max min min equivalently implies max min max min max max min min max max min min max min max min max max 
min min li ui 
desired result follows proposition 
agrawal vardy turbo decoding algorithm phase trajectories appear ieee trans 
information theory 
aji horn mceliece convergence iterative decoding graphs single cycle proc 
ciss princeton march 
aji mceliece generalized distributive law appear ieee trans 
information theory 
bellman notes matrix theory iv inequality due bergstrom am 
math 
monthly vol 
pp 

benedetto turbo codes results parallel concatenated coding schemes ieee trans 
information theory vol 
pp 
mar 
berrou glavieux near shannon limit error correcting coding turbo codes proc 
int 
conf 
commun geneva switzerland may pp 

el gamal analyzing turbo decoder gaussian approximation submitted ieee trans 
information theory 
forney kschischang marcus iterative decoding tail biting information theory workshop 
san diego feb pp 

frey turbo factor analysis proc 
advances neural information processing systems dec 
frey turbo factor analysis submitted neural computation may 
frey kschischang probability propagation iterative decoding proc 
th annual allerton conf 
communications control computing pp 
october 
gallager low density parity check codes 
cambridge ma mit press 
jensen bayesian networks 
new york springer verlag 
kschischang frey iterative decoding compound codes probability propagation graphical models ieee journal selected areas commun vol 
jan 
kschischang frey 
loeliger factor graphs sum product algorithm appear ieee trans 
information theory 
lauritzen spiegelhalter local computation probabilities graphical structures application expert systems journal royal statistical society series vol 
pp 

luby mitzenmacher shokrollahi spielman analysis low density codes improved designs irregular graphs proc 
th acm stoc may 
mackay error correcting codes sparse matrices ieee trans 
information theory vol 
pp 
mar 
mceliece mackay 
cheng turbo decoding instance pearl belief propagation algorithm ieee journal selected areas commun vol 
pp 
feb 
mceliece coding theory probability propagation loopy bayesian networks invited talk th conf 
uncertainty artificial intelligence aug 
murphy weiss jordan loopy belief propagation approximate inference empirical study proceedings th conf 
uncertainty artificial intelligence aug 
pearl probabilistic reasoning intelligent systems networks plausible inference 
san mateo ca morgan kaufmann 
richardson geometry turbo decoding dynamics appear ieee trans 
information theory 
richardson shokrollahi urbanke design provably low density parity check codes submitted ieee trans 
information theory 
richardson urbanke capacity low density parity check codes message passing decoding submitted ieee trans 
information theory 
van roy analysis turbo decoding gaussian densities proc 
advances neural information processing systems dec 
strang linear algebra applications 
orlando fl harcourt brace jovanovich 
weiss correctness local probability propagation graphical models loops neural computation pp 
weiss freeman correctness belief propagation gaussian graphical models arbitrary topology proc 
advances neural information processing systems dec 
weiss freeman correctness belief propagation gaussian graphical models arbitrary topology available www cs berkeley edu pdf 
wiberg codes decoding general graphs 
phd thesis link ping university sweden 
wiberg 
loeliger tter codes iterative decoding general graphs european trans 
vol 
pp 
sep oct 
biographies benjamin van roy benjamin van roy received sb degree computer science engineering sm phd degrees electrical engineering computer science massachusetts institute technology 
currently assistant professor departments management science engineering electrical engineering stanford university courtesy appointment department computer science 
research interests revolve problems information processing decision making complex systems 
received awards including digital equipment scholarship mit george newton award best undergraduate electrical engineering laboratory project mit morris levin memorial award outstanding master thesis mit george award best doctoral dissertation computer science nfs career award 
software product developed team named call center magazine product year 
stanford named frederick fellow david ii faculty scholar 
received ba degree mathematics university california berkeley 
currently graduate student department management science engineering stanford university 
research interests include tractable inference complex systems decentralized decision making 
received university california scholarship uc berkeley roberts prize truly exceptional scholarship mathematics stanford graduate fellowship 

