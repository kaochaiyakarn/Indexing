training algorithms hidden markov models entropy distance functions yoram singer laboratories mountain avenue murray hill nj singer research att com manfred warmuth computer science department university california santa cruz ca manfred cse ucsc edu new algorithms parameter estimation hmms 
adapting framework supervised learning construct iterative algorithms maximize likelihood observations attempting stay close current estimated parameters 
bound relative entropy hmms distance measure 
result new iterative training algorithms similar em baum welch algorithm training hmms 
proposed algorithms composed step similar expectation step baum welch new update parameters replaces maximization re estimation step 
algorithm takes negligibly time iteration approximated version uses expectation step baum welch 
evaluate experimentally new algorithms synthetic natural speech pronunciation data 
sparse models models relatively small number non zero parameters proposed algorithms require significantly fewer iterations 
preliminaries numbers name states hmm 
state special initial state state special final state 
state sequence denoted starts initial state returns ends final state 
observations symbols numbers mg observation sequences denoted discrete output hidden markov model hmm parameterized matrices matrix dimension gamma denotes probability moving state state second matrix dimension probability outputting symbol state set parameters hmm denoted 
initial state distribution vector represented row 
hmm probabilistic generator sequences 
starts initial state 
iteratively final state reached 
current state state chosen transition probabilities current state row matrix 
arriving state symbol output output probabilities state row matrix 
sj denote probability likelihood hmm generates observation sequence path starting state state sj jsj jxj jsj def jxj gamma sake brevity omit conditions assume hmms absorbing state path final state non zero probability 
similar parameter estimation algorithms derived ergodic hmms 
absorbing hmms induce probability state observation sequences sj 
likelihood observation sequence obtained summing possible hidden paths state sequences xj sj 
obtain likelihood set observations simply multiply likelihood values individual sequences 
seek hmm maximizes likelihood set observations equivalently maximizes log likelihood ll jx ln xj 
simplify notation denote generic parameter ranges total number parameters clamped zero 
denote total number parameters leave fixed correspondence entries unspecified 
indices naturally partitioned classes corresponding rows matrices 
denote class parameters belongs vector 
parameters row matrices 
clear context denote class parameters row number state associated class 
rewrite sj number times parameter path observation sequence 
note value depend actual parameters 
compute partial derivatives likelihood log likelihood notation 
sj delta delta delta gamma gamma gamma delta delta delta sj ll jx sj xj jx sj xj jx sjx xj jx xj def sjx expected number occurrences transition output corresponds paths produce 
values calculated expectation step expectation maximization em training algorithm hmms known baum welch forwardbackward algorithm 
sections additional expectations def sj def 
note summation legal arbitrary length expected number times state visited 
entropic distance functions hmms training algorithms framework kivinen warmuth motivating iterative updates :10.1.1.30.7849
assume done number iterations current parameters 
assume set observations processed current iteration 
batch case set changes line case typically single observation 
new parameters stay close incorporates knowledge obtained past iterations maximize log current date set maximizing loglikelihood maximize gamma see motivation :10.1.1.30.7849
measures distance old new parameters trade factor 
maximizing usually difficult distance function loglikelihood depend 
approximate log likelihood order taylor expansion add lagrange multipliers constraints parameters class sum ll gamma ll gamma commonly distance function relative entropy :10.1.1.30.7849
calculate relative entropy hmms need sum possible hidden state sequence leads definition dre def xj ln xj xj sj ln sj sj divergence difficult calculate convex function 
avoid computational difficulties non convexity dre upper bound relative entropy log sum inequality dre dre def sj ln sj sj sj ln sj ln ln sj ln note distance function dre hmm viewed joint distribution observation sequences hidden state sequences 
simplify bound relative entropy lemma proof omitted 
lemma absorbing hmm parameter 
gives new formula dre ln rewritten dre dre ln equation difficult solve variables depend new set parameters known 
approximate dre distance function dre ln new parameter updates distance functions discussed previous section 
derive main update distance function 
done replacing dre setting derivatives resulting 
gives set equations ig xj jx gamma ln gamma equivalent xj jx gamma ln solve replace normalization factor ensures sum parameters exp xj jx exp xj jx re estimation rule entropic update hmms 
derive alternate update 
mixture weights approximate original mixture weights dre lead state dependent learning rate parameters class 
computation time limited see discussion expectations approximated readily available 
possible choice sample expectations xj jx approximation 
weights needed calculating gradient evaluated expectation step baum welch 
xj def xj approximation leads distance function xj jx dre xj jx ln results update call approximated entropic update hmms exp xj xj exp xj xj current set parameters learning rate obtain new set parameters iteratively evaluating right hand side entropic update approximated entropic update 
calculate expectations xj done expectation step baum welch 
weights xj obtained averaging xj 
lets evaluate right hand side approximated entropic update 
entropic update slightly involved requires additional calculation 
recall expected number times state visited data 
compute expectations need sum possible sequences state observation pairs 
probability outputting possible symbols state sum calculating reduces evaluating probability reaching state possible time sequence length 
absorbing hmms approximated efficiently dynamic programming compute summing probabilities legal state sequences length cn typically proved sufficient obtain accurate approximations 
time complexity calculating depends number states regardless dimension output vector training data subtle improvement possible update treating transition probabilities output probabilities differently 
transition probabilities updated 
state probabilities recomputed new parameters possible state probabilities depend transition probabilities output probabilities 
output probabilities updated place 
relation em convergence properties show em algorithm hmms derived framework 
approximate relative entropy distance see dre def gammap distance approximate dre dre def xj jx gamma minimizing version distance function derivation steps approximated entropic update arrive call approximated update hmms gamma xj xj setting results update xj xj maximization re estimation step em algorithm 
omitted due lack space shown entropic updates update improve likelihood iteration 
updates belong family generalized em gem algorithms guaranteed converge local maximum additional conditions 
furthermore infinitesimal analysis second order approximation likelihood function local maximum similar shown approximated update contraction mapping close local maximum exists learning rate results faster rate convergence :10.1.1.18.5213
experiments artificial natural data order test actual convergence rate algorithms compare baum welch created synthetic data hmms 
experiments mainly sparse models models parameters clamped zero 
previous suggest entropic updates perform better sparse models 
dense models generate data algorithms showed performance 
training algorithms started randomly chosen dense model 
comparing algorithms initial model 
due different trajectories parameter space algorithm may converge different local maximum 
clarity presentation show results cases updates converged maximum occur hmm generating data sparse examples typically tens observations non zero parameter 
tested entropic updates updates 
learning rates greater speed convergence 
entropic updates converge equally fast synthetic data generated hmm 
natural data entropic update converges slightly faster approximated version 
update benefits learning rates larger 
update need carefully necessarily ensure non new parameters 
problems exaggerated data generated hmm 
entropic updates experiments natural data 
order fair comparison tune learning rate set 
give comparison entropic update approximated entropic update baum welch left hmm generate random observation sequences parameters average transition observation vector parameters hmm non zero 
performance entropic update approximated entropic update practically updates clearly outperform baum welch 
reason performance entropic updates observations generated hmm 
case approximating expectations sample expectations reasonable 
results suggest valuable alternative baum welch predetermined sparse potentially biased hmm large number parameters clamped zero 
suggest starting full model entropic updates find relevant parameters 
approach demonstrated right part 
example data generated sparse hmm states possible output symbols 
hmm parameters nonzero 
log 
log baum welch parameters non zero hmm generating data initialized random non zero values 
log likelihood entropic update baum welch parameters initialized randomly 
curves show entropic update compensates inferior initialization iterations see horizontal line point requires iterations converge compared baum welch prior knowledge non zero parameters 
contrast baum welch started full model convergence slower entropic update 
iteration approx 
entropic update entropic update em baum welch iteration em baum welch random init 
entropic update random init 
em baum welch sparse init 
comparison entropic updates baum welch 
tested updates speech pronunciation data 
natural speech word pronounced differently different speakers 
common practice construct set stochastic models order capture variability possible pronunciations 
alternative pronunciations word 
problem studied previously state merging algorithm hmms subclass probabilistic finite automata 
purpose experiments discussed compare algorithms entropic updates compare entropic updates baum welch 
resulting hmm pronunciation models usually sparse 
typically phonemes non zero output probability state average number states practice follow states 
entropic updates may provide alternative algorithms 
timit texas instruments mit database 
database contains acoustic waveforms continuous speech phone labels alphabet phones constitute temporally aligned phonetic transcription uttered words 
purpose building pronunciation models acoustic data ignored partitioned phonetic labels words appeared data 
data filtered partitioned words occurring times dataset training evaluation partition 
occurrences word training data learning algorithm remaining evaluation 
built word pronunciation models training fully connected hmm number states set times longest sample denoted nm 
models evaluated calculating log likelihood averaged different random parameter initializations hmm phonetic transcription word test set 
table give negative log likelihood achieved test data average number iterations needed training 
differences log likelihood small means results interpreted caution 
entropic update obtained highest likelihood test data needing number iterations 
approximated entropic update baum welch achieve similar results test data requires iterations 
checking resulting models reveals reason entropic update achieves higher likelihood values better job setting irrelevant parameters zero faster 
negative log likelihood iterations states nm nm nm nm nm nm baum welch approx 
eu entropic update table comparison entropic updates baum welch speech pronunciation data 
research showed framework kivinen warmuth derive parameter updates algorithms hmms :10.1.1.30.7849
view hmm joint distribution observation sequences hidden state sequences bound relative entropy distance new old parameter settings 
approximate relative entropy distance replace exact state expectations sample approximation fix learning rate framework yields alternative derivation em algorithm hmms 
em update uses sample estimates state expectations hard line setting 
contrast line versions updates easily derived observation sequence time 
alternative gradient descent methods estimating parameters hmms 
methods usually employ exponential parameterization soft max parameters see 
case learning set mixture coefficients exponential parameterization led algorithm slower convergence rate compared algorithms derived entropic distances 
clear case hmms 
goals perform comparative study different updates emphasis line versions 
acknowledgments anders krogh showing simple derivative calculations fernando pereira sakakibara interesting discussions 
baldi chauvin 
smooth line learning algorithms hidden 
neural computation 
baum andt 
petrie 
statistical inference finite state 
annals mathematical 
cover thomas 
elements information theory 
wiley 
dempster laird rubin 
maximum incomplete data em algorithm 
journal royal statistical society 
helmbold schapire singer warmuth 
comparison new old algorithms mixture estimation problem 
proceedingsof eighth theory pages 
kivinen warmuth :10.1.1.30.7849
exponentiated gradient versus gradient descent linear predictors 
computation 
appear 
rabiner juang 
hidden markov models 
ieee assp magazine 
ron singer tishby 
learnability usage acyclic probabilistic finite automata 
proc 
eighth annual workshop computational learning theory 
stolcke omohundro 
hidden markov model induction bayesian model merging 
advances neural information processing systems volume 
morgan kaufmann 
xu jordan :10.1.1.18.5213
convergence properties em algorithm gaussian mixtures 
neural computation 
