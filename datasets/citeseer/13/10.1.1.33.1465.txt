entropy subspace clustering mining numerical data cheng chun hung supervised prof ada wai chee fu submitted division department computer science engineering partial fulfillment requirements degree master philosophy chinese university hong kong entropy subspace clustering mining numerical data submitted cheng chun hung degree master philosophy chinese university hong kong mining numerical data relatively difficult problem data mining 
clustering techniques 
consider database numerical attributes transaction viewed multi dimensional vector 
studying clusters formed vectors discover certain behaviours hidden data 
traditional clustering algorithms find clusters full space data sets 
results high dimensional clusters poorly comprehensible human 
important task setting ability discover clusters embedded subspaces high dimensional data set 
problem known subspace clustering 
follow basic assumptions previous clique 
number subspaces clustering large criterion called coverage proposed clique pruning 
addition coverage identify new useful criteria problem propose entropy method called enclus handle criteria 
major contributions identify new meaningful criteria high density correlation dimensions goodness clustering subspaces introduce entropy provide evidence support closure properties entropy prune away insignificant subspaces efficiently enclus sig propose mechanism mine minimal correlated subspaces interest strong clustering enclus int experiments carried show effectiveness proposed method 
iii acknowledgments prof ada fu supervisor guidance patience 
research done reasonably insightful advice know little research 
past years guidance covered undergraduate final year project addition dissertation improved research writing skills significantly firmly believe invaluable rest life 
prof wong prof raymond yeung marked term papers taught useful courses 
prof yi zhang helping working mathematical proofs algorithms 
gratitude goes members research groups chun hing cai kin pong chan wang wai kwong po shan kam wai ching wong wong 
shared experiences doing research lot inspiration 
research nearly successful help 
wish express colleagues kam wing chu wong chi wing fu yuen tsui hong ki chu fai fung 
lot useful suggestions technical problems encountered years research 
intelligent capable people 
iv contents ii acknowledgments iv tasks data mining 
classification 
estimation 
prediction 
market basket analysis 
clustering 
description 
problem description 
motivation 
terminology 
outline thesis 
survey previous data mining 
association rules variations 
rules containing numerical attributes 
clustering 
clique algorithm 
entropy subspace clustering criteria subspace clustering 
criterion high density 
correlation dimensions 
entropy numerical database 
calculation entropy 
entropy clustering criteria 
entropy coverage criterion 
entropy density criterion 
entropy dimensional correlation 
enclus algorithms framework algorithms 
closure properties 
complexity analysis 
vi mining significant subspaces 
mining interesting subspaces 
example 
experiments synthetic data 
data generation hyper rectangular data 
data generation linearly dependent data 
effect changing thresholds 
effectiveness pruning strategies 
scalability test 
accuracy 
real life data 
census data 
stock data 
comparison clique 
subspaces uniform projections 
problems hyper rectangular data 
miscellaneous enhancements extra pruning 
multi resolution approach 
vii multi threshold approach 
bibliography appendix differential entropy vs discrete entropy relation differential entropy discrete entropy 
mining quantitative association rules approaches 
performance 
final remarks 
viii list tables comparison clustering algorithms 
notations discussion entropy clustering criteria 
notation complexity analysis 
notations algorithm 
setting synthetic data 
values entropy interest interest gain subspaces example 
default parameters experiments 
subspaces highest interest dimensions census database 
subspaces lowest interest dimensions census database 
mnemonic census data sets 
subspaces highest interest dimensions stock database 
subspaces lowest interest dimensions stock database 
parameters comparison experiment 
ix example illustrating problem hyper rectangular data 
list figures example cluster 
apriori algorithm 
mdl pruning 
example data sets equal coverage different densities 
problem independent dimensions 
area cluster vs entropy 
lattice variables 
algorithm mining significant subspaces 
algorithm mining interesting subspaces 
example illustrated lattice 
entropy threshold vs running time 
interest threshold vs running time enclus sig 
pass vs percentage subspaces pruned 
scalability test dimensionality data set 
xi scalability test number transactions data set 
performance clique different thresholds 
comparison algorithms clique 
stock price cheung kong hsbc normalized :10.1.1.40.6757
example illustrating problem hyper rectangular data 
performance algorithms extra pruning 
data sets equal coverage density different number clusters 
example explaining multi resolution method works 
correlated variables 
xii chapter modern technology provides efficient low cost methods data collection 
raw data rarely direct benefit higher level management decision making intelligent analysis 
data mining knowledge discovery databases exploration analysis large data sets discover meaningful patterns rules 
aims construction automatic semi automatic tools analysis data sets 
contrary top processes hypothesis testing past behaviour verify disprove preconceived ideas data mining bottomup process priori assumptions data 
describes knowledge discovery non trivial process extraction implicit previously unknown potentially useful information databases 
tasks data mining tasks data mining phrased terms tasks classification estimation prediction market basket analysis clustering description 
single algorithm equally applicable tasks 
need different tools techniques deal different tasks 
tasks chapter briefly described section 
classification classification task examining features object assigning predefined set classes classifying group animals fishes birds mammals 
major characteristics classification defined definition classes 
training set consists preclassified examples 
classification referred supervised learning 
estimation estimation similar classification 
classification deals discrete outcomes estimation deals continuously valued outcomes 
estimation approach advantage individual record rank ordered 
instance limited advertising budget target customers services 
neural networks suited task estimation 
prediction prediction different classification estimation objects classified predicted behaviours estimated values 
historical data build model predicts behaviours 
wait see accuracy created model 
chapter market basket analysis classical problem data mining mining binary association rule 
large amount research data mining went problem 
originated analysis buying patterns supermarkets 
called market basket analysis 
example binary association rule rule says high probability people buying apples buy oranges 
rule associated pair values confidence factor support respectively example 
confidence factor probability rule true support fraction transactions database contains items rule 
clustering clustering task segmenting heterogeneous population number homogeneous groups objects 
clustering different classification depend predefined set classes 
training set called unsupervised learning 
description description task describing happens complicated database 
description help understand people processes products chapter salary age example cluster 
database 
hopefully gives explanation behaviours 
description convey important insights directions look explanations 
problem description thesis attempt mine numerical data clustering techniques 
particular subspace clustering problem 
consider database consisting numerical attributes 
transaction database viewed multi dimensional vector 
clustering discover homogeneous groups objects values vectors 
study behaviour objects looking shapes number clusters 
see example 
cluster describes relationship age salary 
clustering algorithms suitable problem 
satisfy special requirements order useful 
important requirement ability discover clusters embedded subspaces high dimensional data 
space dimensions formed set attributes space dimensions formed subset called subspace chapter conversely called instance suppose attributes clusters may exist inside subspace formed independent case noise variable 
high dimensional information hard interpret desirable clustering algorithm cluster subspace ab full space abc 
real life databases usually contain attributes proper cluster full space knowing existence cluster full space little user 
ability discover embedded clusters important 
problem called subspace clustering 
motivation mining binary association rule extensively studied years databases real world usually numerical attributes addition binary attributes 
unfortunately mining numerical data difficult problem relatively little done topic 
previous includes 
mining clusters preferable multi dimensional quantitative association rules association rules consist antecedent consequent parts 
learn statistics possible find correlation different factors raw data find direction implication risky conclude causal relationship raw data 
clustering method finds correlations inferring causal relationship 
important requirement mentioned previous section ability discover embedded cluster 
data mining definition deals huge amount data measured gigabytes terabytes 
chapter traditional clustering algorithms elegant accurate involve complicated mathematical computations 
methods shown handle problem sizes hundreds thousands transactions far sufficient data mining applications 
algorithms means assume data sets placed main memory 
algorithms require tremendous amount disk accesses assumption hold 
need algorithm gives reasonable performance high dimensionality large data sets 
prefer clustering algorithms assume restrictive shapes clusters 
clustering algorithms clarans birch assume clusters convex shape 
adopt definition cluster limitation 
algorithm assumptions distribution data sensitive existence outliers 
require user specify parameters user difficulty decide 
instance means algorithm requires user specify number clusters known user 
practice need repeat algorithm different guesses obtain best result 
meaningful effective way convey resulting clusters user purpose data mining 
solution problem consist steps find subspaces clustering 
identify clusters selected subspaces 
result user 
shall focus step 
propose entropy approach tackle problem 
chapter terminology despite lot efforts keep terminology consistent thesis cases resort different terms meaning 
thesis terms attribute variable dimension interchangeably 
terms sound natural context database information theory clustering respectively 
outline thesis rest thesis organized follows 
chapter discuss related similar problems 
chapter points new criteria clustering explains needed 
define measure entropy numerical database discuss suitable measure listed criteria 
chapter describes proposed method enclus clustering details 
variations algorithm enclus sig enclus int 
common framework discussed complexity derived 
chapter look experimental results 
synthetic real life data experiments 
comparison previous clique 
chapter discusses miscellaneous enhancements applicable enclus 
chapter gives 
part result thesis published proceedings acm sigkdd international conference knowledge discovery data mining kdd 
chapter survey previous chapter introduce previous related problem 
section go research data mining 
cover research clustering section 
clustering techniques data mining problem studied people different disciplines 
focus methods proposed database research community 
data mining give brief review research data mining 
mining association rules hottest topics field data mining 
explain meaning association rules variations rules containing numerical attributes 
introduce famous apriori algorithm mining binary association rules variation association rule incorporates statistical measure correlation 
move study implication rules 
implication rule changes definition association rules give meaningful rules 
chapter survey previous association rules variations research association rules extended modified definition association rules introduce new types rules 
apriori classical algorithm mining association rules 
apriori algorithm problem mining association rules introduced 
original form association rules consists binary attributes 
newer studies extend association rules contain non binary attributes original association rule known binary association rule 
example rule section 
apriori algorithm algorithms ais setm introduced problem efficient apriori 
formal statement problem follows 
apriori algorithm shares setting 
set literals called items 
set transactions transaction set items associated transaction unique identifier called id say transaction contains set items association rule implication form ae ae 
rule support transaction set transactions contain confidence support support 
task discover rules support confidence exceeding predefined thresholds 
problem mining association rules divided subproblems 

find sets item itemsets transaction support predefined threshold called minimum support 
items known chapter survey previous algorithm apriori gamma apriori gen gamma new candidates forall transaction subset candidate contained forall candidates count fc count minsup answer apriori algorithm 
large itemsets 

large item sets generate association rules 
rules confidence level predefined threshold called minimum conference 
step straightforward trivial terms computational time 
focus step 
apriori algorithm solving step 
apriori iterative algorithm 
pass simply scans database find large itemsets 
subsequent pass apriori gen function involved generate candidate itemsets large itemsets previous pass gamma apriori gen function join prune step 
join step gamma self joins form insert select item item item item gamma gamma item item item gamma item gamma item gamma item gamma prune step itemsets having gamma subset gamma deleted 
chapter survey previous function subset returns candidate itemsets contained transaction subset function implemented efficiently storing candidate itemsets hash tree traversing hash tree subset function involved 
apriori algorithm shown outperform ais setm 
similar algorithm aprioritid apriorihybrid proposed 
aprioritid better performance higher passes 
apriorihybrid combines apriori aprioritid 
uses apriori earlier passes switches aprioritid passes 
generalizing association rules correlation identifies problem original definition association rules proposes correlation rule mining 
generalize association rules consider presence absence items 
measure significance association test correlation classical statistics 
correlation upward closed itemset lattice gives useful pruning criteria 
example illustrating problem original definition association rules 
table rows represent buying buying tea respectively 
similarly columns represent buying buying coffee respectively 
row col support rule fairly high 
confidence quite high 
may conclude rule valid 
chapter survey previous priori probability customer buys coffee customer known buy tea buy coffee general population 
calculating correlation theta theta know negative correlation rule misleading 
solution problem employ test correlation classical statistics 
test capable testing positive negative correlations 
chi test look proof closure property correlated items 
probability event occurs gamma probability event occur 
want show items correlated superset items correlated 
correlation upward closed itemset lattice 
proof contradiction 
proof suppose correlated 
ab abc ab derive similar formulae ab 
imply independent contradiction 
closure property itemsets interest form border itemset lattice 
border encodes useful information interesting itemsets 
algorithm uses closure property prune away itemsets border itemset lattice provide extra information 
algorithm apriori replaces test chapter survey previous confidence test correlation adds pruning criteria border property 
introduce test independence 
fi set items 
series trials denote number times item occurs 
fi theta theta fi set possible transaction values forms dimensional table called contingency table 
value denotes cell denote number transactions falling cell expectation calculated assumption independence 
single item gamma theta theta theta calculate statistic gamma variables really independent value 
higher cutoff value obtained distribution table required significance reject independence assumption 
proposes measure interest 
interest defined interest allows detection negative correlation 
illustrate con chapter survey previous sider example interest values 
cell extreme interest indicating having property correlated having property kind negative association usually mined classical framework 
experiment shows correlation gives meaningful rules new pruning criteria causes improvement performance reducing number candidate itemsets 
implication rules insufficiency measures association rules identified 
new measure conviction proposed replacement confidence 
rules measured conviction called implication rules 
definition conviction measure advantages ffl flaw confidence mentioned 
ffl interest measure departure independence 
measure implication definition completely symmetrical 
conviction problem 
chapter survey previous ffl rules hold time rules highest possible conviction value 
rules may interest value slight larger 
rules containing numerical attributes studies association rules focus database binary categorical attributes 
earliest extending association rules contain numerical attributes 
consider association rules form balance implies bank customers balances fall range tend card loan high probability 
kind rule dimensional numerical attribute rule 
follow goes extending rule form age balance es dimensional numerical attributes involved 
implies bank customers ages balances fall planar region tend card loan high probability 
application dimensional association rule construction decision tree 
node decision tree consider family grid regions plane associated pair attributes 
chapter survey previous data split classes data inside data outside detailed algorithms generation rules decision trees geometry 
give basic idea 
mine dimensional association rule divide data equidepth buckets bn numerical attribute balance bucket balance second bucket consider rules ranges combinations consecutive buckets 
denote size number tuples satisfying objective condition consider sequence points 
rule range bm slope gives confidence rule coordinate qm minus coordinate gives support rule 
apply methods geometry find rules sufficient support confidence 
move dimensional association rule 
simplify problem classes regions rectangle admissible region considered 
admissible regions connected monotone regions intersection vertical line 
dimensional rule data divided equi depth buckets 
suppose numeric attributes distribute values equal depth buckets values image theta cells dimensional plane 
regions union cells considered 
employ methods geometry find regions sufficient support confidence 
dimensional association rule applied construction chapter survey previous decision tree reduce size tree 
node decision tree data split sets inside region outside choice region different sole dimension rule goal optimization longer confidence support entropy splitting 
algorithm applies method geometry perform optimization 
clustering section describe previous done clustering problem 
focus database research community clustering extensively studied people different disciplines 
general overview clustering problem please refer books clustering 
clarans randomized search reduce search space means approach 
dbscan relies density notion clusters designed discover clusters arbitrary shape 
spatial data structure efficient retrieval data sets 
assumption points inside cluster uniformly distributed 
algorithm employs test statistics verify distribution clusters 
clarans dbscan targeted spatial data 
incremental dbscan improves dbscan algorithm handle update database efficiently 
takes advantage density nature dbscan insertion deletion object affects cluster membership neighborhood object 
algorithm considerably faster dbscan database updated frequently 
chapter survey previous uses multidimensional grid structure variation grid file organize value space surrounding pattern values 
patterns grouped blocks clustered respect blocks topological neighbour search algorithm 
traditional methods means medoid point mean medoid represent cluster calculating distance point cluster 
cure extends representing cluster certain fixed number points 
parameter set adjust representative points means graph theory algorithm minimum spanning tree mst special cases cure 
result algorithm recognizes non spherical clusters particularly sensitive outliers 
birch dynamical incremental method cluster incoming points 
important idea birch summarize cluster points clustering feature vector 
summary uses storage storing data points cluster 
cf tree built splits dynamically 
clusters stored leaf nodes 
scalable clustering framework applies means algorithm 
clusters compressed sufficient statistics identical clustering feature vector birch 
resolves huge memory requirement means suitable large data sets 
algorithms satisfies important requirement ability identify clusters embedded subspaces high dimensional data 
clique published algorithm aware satisfies requirement 
follow closely problem setting clique shall describe details 
introducing clique give comparison features clustering algorithms table 
chapter survey previous name algorithm spherical clusters sensitive outliers affected input order discover embedded clusters mst means clarans dbscan incremental dbscan birch cure clique table comparison clustering algorithms 
chapter survey previous clique algorithm introduce target problem assumptions clique 
set data points parameters 
discretize data space non overlapping rectangular units obtained partitioning dimension intervals equal length 
unit dense fraction total data points contained unit greater threshold clusters unions connected dense units subspace 
need identify dense units different subspaces 
clique algorithm divided steps find dense units identify subspaces containing clusters 
identify clusters selected subspace 
generate minimal description clusters disjunctive normal form 
theoretically possible create histogram spaces identify dense units 
method computationally infeasible number dimensions large 
reduce search space bottom algorithm exploits monotonicity clustering criterion respect dimensionality collection points cluster dimensional space part cluster gamma dimensional projections space 
algorithm iterative find dimensional dense units making pass data 
having determined gamma dimensional dense units gamma candidate dimensional units determined candidate generation procedures 
pass data determine candidate units dense algorithm iterates increasing dimensionality terminates new candidates 
candidate generation procedure similar adopted wellknown apriori algorithm mining association rules 
self joins form join condition units share gamma dimensions 
represents identifier ith dimension unit represents chapter survey previous subspace coverage selected subspaces pruned subspaces average selected subspaces average pruned subspaces mdl pruning 
interval ith dimension 
insert select 
gamma gamma gamma gamma gamma gamma 
gamma gamma gamma gamma gamma gamma gamma gamma discard dense units projection gamma dimensions included gamma number dimensions increases method may produce large amount dense units subspace pruning may effective 
clique uses new criteria pruning subspace coverage 
coverage subspace count fraction database covered dense units count number points fall inside subspaces high coverages selected low coverages pruned away 
minimal chapter survey previous code length method chooses subspaces contain clusters 
subspaces sorted coverage 
want divide subspaces selected set prune set subspaces high coverages selected low coverages pruned away see 
code length calculated follows ji jn gamma cl log ji log jx gamma log jn log jx gamma choose value code length minimized optimal cut point 
belong belong set subspaces contain clusters dense units set subspaces discarded save memory 
note possible legitimate clusters minimal code length method 
subspaces containing clusters identified clusters subspace determined 
recall clusters connected dense units 
simply depth search algorithm find connected components 
final step generate minimal cluster descriptions 
description form dnf expression 
age salary age salary 
equivalent union hyper rectangular regions 
regions greedy growth method 
start dense unit greedily grow maximal region dimension 
process repeated union regions cover cluster 
need remove redundant regions 
achieved repeatedly removing smallest redundant region maximal region chapter survey previous removed 
break ties arbitrarily process removing redundant region 
give dnf expression describing clusters 
chapter entropy subspace clustering chapter discuss properties subspaces clustering define criteria subspace clustering 
propose entropy originates information theory measure quality clustering 
support entropy measure showing handle proposed subspace clustering criteria 
criteria subspace clustering factors considered clustering algorithm data mining 
mentioned efficiency shape clusters sensitivity outliers requirements parameters 
clustering algorithm assume certain set criteria cluster criteria clustering set data 
addition clustering problem handle problem determining subspaces clustering 
need addition criteria determining clustering different sets data better 
clique definition coverage measurement goodness clustering 
reasonable criterion chapter entropy subspace clustering density density example data sets equal coverage different densities 
subspace distinguished clusters high coverage subspace close random data distribution low coverage 
believe criteria needed 
criterion add criterion high density 
criterion high density suppose coverage measurement goodness 
problem case illustrated 
shows probability density function random variable value coverage represented area shade portion coverage fraction database covered dense units 
example cases coverage 
contradicts intuition points case closely packed qualified cluster 
correlation dimensions third criterion consider related correlation dimensions 
note finding subspaces clustering may helpful want dimensions subspace correlated 
reason chapter entropy subspace clustering problem independent dimensions 
subspace may contain clusters may interesting dimensions independent 
example shows scenario 
example data points projected lies projected lies data objects distributed theta joint space 
points uniformly distributed theta cluster looking joint space gives knowledge looking dimensions independently 
require dimensions subspace correlated 
note say correlated mean dimensions completely independent need mean strong correlation 
having identified number criteria clustering shall find metric measure criteria simultaneously 
subspace clustering criteria high score metric 
set threshold measurement find subspaces exceed threshold 
metric entropy shall discuss section 
chapter entropy subspace clustering entropy numerical database propose entropy method 
method motivated fact subspace clusters typically lower entropy subspace clusters 
introduce concept entropy 
entropy measure uncertainty random variable 
discrete random variable set possible outcomes probability mass function random variable entropy defined expression 
gamma log base log unit entropy bit 
natural log unit entropy nat 
note nat bits 
variable calculate joint entropy measure uncertainty 
gamma xn xn log probability uniformly distributed uncertain outcome 
entropy highest case 
hand data points highly skewed probability mass function know variable fall small set outcomes uncertainty entropy low 
calculation entropy similar clique divide dimension intervals equal length delta high dimensional space partitioned form grid 
suppose data set scanned count number points contained cell chapter entropy subspace clustering entropy nats area area vs entropy area cluster vs entropy 
grid 
density cell 
set cells density cell terms percentage data contained define entropy data set gamma log data points uniformly distributed uncertain particular point lie 
entropy highest 
data points closely packed small cluster know particular point fall small area cluster uncertainty entropy low 
shows result experiment studying relationship area cluster dimensional space theta 
smaller area cluster closely packed points lower entropy 
size interval delta carefully selected 
interval size small cells average number points cell small 
hand interval size large may able capture differences density different regions space 
unfortunately knowing distribution data sets difficult chapter entropy subspace clustering estimate minimal average number points required cell correct result 
inappropriate assume distribution exactly studying 
suggest points cell average considered minimum sample size large sample procedures 
size interval delta set accordingly 
entropy clustering criteria section propose criteria goodness clustering high coverage high density dimensional correlation 
section discuss entropy relate criteria chosen selection subspaces 
list symbols discussion table 
total number units total number dense units threshold density dense unit percentage data coverage percentage data covered dense units densities dense units densities non dense units table notations discussion entropy clustering criteria 
entropy coverage criterion investigate relationship entropy coverage consider case 
assuming ratio densities units constant densities units coverage percentage data covered dense units particular subspace 
original authors clique define number objects covered dense units 
definition slightly different 
chapter entropy subspace clustering follows gamma dp dc dp dc dp dc dp dc gamma want establish relationship certain conditions entropy decreases coverage increases dh dc 
theorem dh dc dp dc dpn dc 
proof gamma log gamma log gamma log differentiate entropy respect coverage 
dh dc gamma dp dc log dp dc gamma dp dc log dp dc gamma dp dc log gamma gamma dp dc log gamma dp dc log gamma dp dc log gamma log dp dc dpn dc result follows proof completed 
chapter entropy subspace clustering necessary sufficient condition desirable property hold 
condition complicated difficult understand 
investigation needed comprehensive 
theorem suppose dp dc dp dc min ik max jn 
dh dc proof dp dc dpn dc min ik dp dc dp dc delta max jk dp dc dpn dc min ik max jn theorem applies proof completed 
conditions theorem hold coverage increased increasing densities denser units decreasing densities non dense units 
true conditions supportive evidence entropy reflect coverage clustering subspace 
entropy density criterion example shown entropy case lower case suggests case better cluster 
see entropy better capture intuition clustering compared mere metric coverage 
examine relationship entropy density chapter entropy subspace clustering consider case 
assume density dense units equal ff density non dense units equal fi 
total number dense units total number non dense units gamma gamma log gamma log log gamma kff log ff gamma fi log fi assuming ff fi change continuously entropy differentiable function density 
theorem dh dff ff fi 
proof note kff gamma fi gamma dfi dff differential entropy respect density ff dh dff gamma log ff gamma dfi dff log fi gammak log ff gamma log fi log fi ff shows dh dff ff fi 
proof completed 
value negative entropy decreases ff increases 
chapter entropy subspace clustering entropy relate measurement density clustering subspace 
entropy dimensional correlation problem correlated dimensions easily handled entropy independence dependence dimensions detected relationships entropy 
iff independent iff function traditionally correlation numerical variables measured correlation coefficient detect correlation entropy 
entropy algorithm entropy detect correlation introduces negligible computational overhead 
set variables correlated equation satisfied 
express precisely define term interest 
interest fx gamma equation satisfied interest greater 
thesis define degree correlation interest 
higher interest stronger correlation 
avoid correlation occurred random consider variables correlated interest exceeds predefined threshold 
thesis correlation subspace defined terms interest 
interests dimensional subspaces 
definition interest equivalent mutual information individual dimensions subspace xn 
term interest mutual information individual dimensions simplify terminology 
chapter entropy subspace clustering advantage entropy coverage discover correlation coverage 
relationships equation exist coverage 
propose measure interest gain measures increase correlation adding new dimension subspace 
discussed section 
chapter enclus algorithms chapter introduce proposed method enclus details 
variations enclus enclus sig enclus int discussed section respectively 
strategy solving subspace clustering consists main steps 
find subspaces clustering entropy method 

identify clusters subspace 

result users 
step step adopt method clique existing clustering algorithms 
examine step research 
framework algorithms proposed algorithms framework similar introduces algorithm mining correlation rules 
apriori algorithm chapter enclus algorithms mining association rule 
apriori start finding large itemsets 
results generate candidate itemsets checked database determine large itemsets 
process repeated increasing itemset sizes large itemset 
algorithm mining correlation rules extends framework apriori pair downward upward closure properties 
contrast downward closure property adopted apriori 
downward closure property subspace satisfies property subspaces 
upward closure property subspace satisfies property 
downward closure property pruning property 
subspace satisfy property cross know satisfy property 
upward closure property contrast constructive property 
subspace satisfies property satisfy property 
upward closure property useful pruning 
trick find minimal correlated subspaces 
know subspace correlated minimal correlated 
upward closure pruning property 
suppose downward closure property upward closure property outline algorithm follows 

start finding dimensional subspaces satisfying enter dimensional candidate set 

subsequent pass form candidate set dimensional subspaces 
set contains subspace gamma dimensional projections satisfying 
candidate examined 
satisfying go result chapter enclus algorithms set 

go back step empty candidate sets 
method variations 
algorithm enclus sig follows framework 
variation enclus int downward closure utilized 
consider upward closure property upward closure property removed outline algorithm 
closure properties propose closure properties section 
previously term clustering indicate subspace contains set clusters intuitive sense 
shall give term concrete definition means entropy 
need set threshold 
subspace entropy considered clustering 
similarly define subspace interest defined section threshold ffl correlated 
note downward closure property entropy 
non negativity shannon information measures 
correctness bottom approach property 
lemma downward closure dimensional subspace clustering gamma dimensional projections space 
proof subspace clustering 
values entropy conditional entropy mutual information conditional mutual information non negative 
true differential entropy value differential entropy may positive negative 
chapter enclus algorithms gamma gamma jx gamma non negativity gamma dimensional projection clustering 
proof repeated gamma dimensional projections 
section discuss criterion dimensional correlation 
section examine entropy measure dimensional correlation 
show upward closure property criterion 
lemma upward closure set dimensions correlated proof suppose correlated superset recall define correlation subspace interest interest ffl interest gamma gamma jx gamma gamma gamma jx gamma interest ffl chapter enclus algorithms subspace correlated 
complexity analysis section examine worst case complexity algorithm 
note target problem apriori np hard algorithm framework run polynomial time theoretically 
hope algorithm run faster theoretical bound practice 
performance evaluation algorithm experiment chapter 
notation analysis table 
number transactions database total dimensionality database number intervals dimension divided table notation complexity analysis 
pass database scanned calculation entropy 
requires 
section calculation entropy requires summing terms pass worst case dc candidate subspaces 
pass requires dc delta 
totally passes 
dc delta nd dc delta nd gamma worst case complexity nd 
practically number passes candidate subspaces generated algorithms chapter enclus algorithms lattice variables 
smaller values 
complexity derived represents worst case scenario 
mining significant subspaces current number iterations set dimensional candidate subspaces set dimensional significant subspaces ns set dimensional subspaces clustering minimal correlated table notations algorithm 
call subspaces clustering minimal correlated significant subspaces 
due upward closure property subspaces interested form border 
border stores necessary information 
refer example 
subspaces dotted lines clustering downward closed 
subspaces solid line correlated upward closed 
border fx stores significant subspaces minimal correlated subspaces clustering 
chapter enclus algorithms algorithm enclus sig ffl dimensional subspaces 
subspace delta cal density cal entropy delta 
interest ffl ns ns candidate gen ns go step 
go step 
result algorithm mining significant subspaces 
chapter enclus algorithms details algorithm called enclus sig 
table lists notations 
description procedures algorithm follows 
cal density build grid count number points fall cell grid described section 
density cell estimated 
cal entropy delta calculate entropy density information obtained scanning data set 
candidate gen ns generate candidate subspaces dimensions ns join step prune step candidate generate function 
join step expressed pseudo code 
joins subspaces having common gamma dimensions 
insert select dim dim dim dim ns ns dim dim dim gamma dim gamma dim dim prune step subspace having dimensional projection outside ns removed 
mining interesting subspaces correlation usually detected low dimension mining high dimensional clusters avoided 
low dimensional clusters easier interpret time mining high dimensional clusters saved 
consider interested chapter enclus algorithms algorithm enclus int ffl dimensional subspaces 
subspace delta cal density cal entropy delta 
interest gain ffl ni ni candidate gen ni go step 
go step 
result algorithm mining interesting subspaces 
chapter enclus algorithms non minimal correlated subspaces 
instance correlated may interested subspace abc abc strongly correlated 
measure increase correlation define term interest gain interest gain subspace defined follows 
interest gain fx interest fx gamma max fx gamma fx interest gain dimensional subspace defined 
interest gain dimensional subspace interest subspace minus maximum interest gamma dimensional projections 
words increase interest adding extra dimension 
new goal mining subspaces entropy exceeds interest gain exceeds new threshold ffl call subspaces interesting subspaces 
mining significant subspace algorithm modified slightly mine interesting subspaces 
shows modified algorithm enclus int 
relax pruning criteria candidates longer running time expected 
worst case complexity enclus int enclus sig computation interest gain negligible consider extra pruning capacity enclus sig worst case 
example give example illustrate algorithm works 
example clusters noise generated predefined positions space 
definition interest gain equivalent mutual information original subspace gamma new dimension gamma 
term interest gain mutual information original subspace new dimension simplify terminology 
chapter enclus algorithms generated data dimensions points uniformly distributed inside cluster 
clusters points plus noise data points 
values dimension uniformly distributed independent dimensions regarded noise attribute 
positions clusters shown table 
clusters subspaces parameters ffl ffl delta set respectively 
know clusters located result algorithms compared setting 
mine significant subspaces 
initial iteration onedimensional subspaces added candidate set definition interest dimensional subspaces go ns fx ns fx second iteration candidate set generated candidate generation procedure self joins ns table see subspaces satisfy entropy threshold fx satisfies interest threshold ffl 
subspaces go rest goes ns fx fx ns fx candidate generation function gives empty candidate set subspaces ns common dimension 
algorithm chapter enclus algorithms example illustrated lattice 
terminates 
result significant subspaces discovered far 
result fx result set correctly tells clusters subspaces dimension noise attribute 
form clusters dimension 
example illustrated lattice 
subspaces dotted lines clustering downward closure subspaces solid line correlated upward closure 
subspaces marked boxes form border 
significant subspaces 
subspaces lie borders clustering correlated included result minimal correlated 
setting example clusters contained subspace enclus sig subspace minimal correlated 
need enclus int algorithm wish chapter enclus algorithms cluster 
number points noise table setting synthetic data 
find cluster 
set parameter ffl 
iterations identical mining sequential subspaces 
fx ni fx fx fx ni fx dimensional candidate sets qualifies interesting subspace 
qualify interest gain exceed threshold ffl pruned away entropy interest gain exceed thresholds 
candidate set empty algorithm terminates 
subspaces result set consistent initial setting 
notice result lies borders lattice 
means interesting spaces clustering correlated 
fx fx chapter enclus algorithms subspace entropy nats interest nats interest gain nats table values entropy interest interest gain subspaces example 
ni fx result fx example see enclus int discovers subspaces generates candidates running time longer 
algorithm depends interested non minimal correlated subspaces 
chapter experiments evaluate performance accuracy algorithms implemented algorithms sun ultra workstation gnu compiler 
synthetic data real life data experiments 
goal analyse performance accuracy algorithms different settings 
compare algorithms clique 
synthetic data set experiments high dimensional synthetic data generated contains clusters embedded subspaces 
generate kinds synthetic data hyper rectangular data linearly dependent data 
data generation hyper rectangular data data generator allows user specify dimensionality data number subspaces containing clusters dimensionality clusters number clusters subspace number transactions supporting cluster 
table example input data generator 
method chapter experiments data generation design data generator resembles works :10.1.1.131.5152
hyper rectangular data tests problems may arise 
problems discussed section 
subspace containing clusters insert hyper rectangular clusters 
uniform hyper rectangular cluster inserted subspace pruned away due independence see section 
data generation linearly dependent data experiments focus linearly dependent data 
kind data contains linearly dependent variables 
linearly dependent variables set variables linear combination variables 
example 
example set linearly dependent variables 
variables random variables uniformly distributed :10.1.1.40.6757
ideally subspace clustering algorithm report subspace abc user 
experiments see subspace clustering algorithms successfully 
specified data dimensions transactions experiments 
sets linearly dependent variables contain dimensional subspaces 
default parameters shown table 
set parameters obtained trial error 
suitable discovery dimensional clusters 
chapter experiments parameter value delta ffl ffl table default parameters experiments 
entropy threshold omega nats enclus sig epsilon enclus int entropy threshold vs running time 
effect changing thresholds shows performance algorithms different values 
smooth curve increases certain value candidates higher dimension introduced impose considerable amount extra computation 
dimensional subspaces discovered 
see running time algorithm enclus sig ceases increase high point pruning power entropy negligible pruning attributed upward closure property independent 
algorithm enclus int running time keeps increasing entropy utilized pruning 
enclus int recovers chapter experiments interest threshold epsilon nats omega interest threshold vs running time enclus sig 
dimension clusters embedded 
enclus sig represents correlated variables number dimensional subspaces 
shows performance enclus sig different values ffl 
running time ceases increase certain point 
pruning power upward closure property negligible pruning done entropy 
ffl increases clusters expressed higher dimensional subspaces performance suffers pruning done 
dimensional subspaces discovered ffl reaches 
obviously ffl set large subspace discovered exist set variables having high correlation 
high interest threshold ffl recommended 
performed similar set experiments enclus int 
performance enclus int different values ffl nearly identical entropy pruning 
chapter experiments pass vs percentage subspaces pruned 
effectiveness pruning strategies pruning power algorithm illustrated 
methods compared naive algorithm examines possible subspaces 
result see methods achieve significant reduction number candidates passes 
enclus sig prunes candidates enclus int 
explains enclus sig runs faster enclus int 
experiment carried dimensional data set 
scalability test result scalability test shown 
expected enclus sig outperforms enclus int enclus sig finds minimal correlated subspaces enclus int spend extra time mine non minimal correlated subspaces 
gap enclus sig enclus int increases dimensionality suggests pruning power upward closure significant 
run experiment dimensions 
higher dimensions computation time increase 
suggest extra pruning strategies improve performance chapter experiments dimensionality enclus sig enclus int scalability test dimensionality data set 
number transactions enclus sig epsilon enclus int scalability test number transactions data set 
section 
shows scalability algorithms databases transactions 
experiment algorithm scales linearly number transactions sole effect changing number transactions time reading database 
number passes remains constant time reading database increases linearly number transactions 
result consistent complexity analysis done section chapter experiments points worst case complexity nd 
complexity experimental analysis algorithms scale linearly exponentially accuracy investigate accuracy algorithms performed experiment data set containing dimensional clusters disjoint subspaces 
total dimensionality data set 
enclus int successfully discovers dimensional subspaces contains embedded clusters reporting false alarms dimensional subspaces 
enclus sig expresses correlated variables number dimensional subspaces 
examine higher dimensional subspaces minimal correlated 
real life data section apply algorithms real life data sets order verify validity results algorithms 
sets data census data hong kong stock price data 
census data census database available web site choose person record data 
consists kinds attributes 
categorical choose numerical attributes data set analysis 
algorithm enclus int url www umn edu 
chapter experiments subspace sei sei sei age table subspaces highest interest dimensions census database 
subspace table subspaces lowest interest dimensions census database 
enclus sig low dimension allows see interesting subspaces 
show dimensional subspaces highest lowest interests table respectively 
mnemonic table 
subspaces low interest usually target algorithms 
contrast subspaces high interest 
result see algorithms discover meaningful subspaces 
subspaces high interest clustering subspaces low interest 
stock data section algorithm study stock price hong kong stock market 
study limited stocks hang seng index constituent stocks 
remaining stocks omitted chapter experiments mnemonic variable name number family members household number children household number children age household age child household age youngest child household age age number children born year immigration speaks english education attainment recode occupational income score sei duncan socioeconomic index weeks worked year usual hours worked week total personal income non farm business income table mnemonic census data sets 
missing data 
high interest threshold ffl expect price movements stocks highly correlated 
subspaces highest lowest interests shown table respectively 
algorithm produces meaningful results 
instance subspace cheung kong henderson land ppt clustering real estate stocks 
price movement stocks affect 
similarly subspace cheung kong hsbc henderson land contains leading stocks 
contrast subspaces low interest contain stocks different industries 
experiments sets real life data conclude results produced algorithms valid meaningful 
chapter experiments stock name cheung kong henderson land ppt cheung kong hsbc henderson land cheung kong hutchison ppt hsbc hutchison pacific table subspaces highest interest dimensions stock database 
stock name hk electric land hk electric bank east asia hk telecom land hk telecom bank east asia pacific table subspaces lowest interest dimensions stock database 
comparison clique section algorithms compared clique algorithm 
clique algorithm implemented platform enclus 
recall steps clique 
fair comparison clique discover subspaces clustering step 
discover actual position clusters step 
explained section performance algorithm depends values thresholds 
result similar evaluation clique shown see performance clique varies greatly threshold special attention setting thresholds paid comparison experiment 
experiment threshold values set way target subspaces discovered algorithms 
running time implementation dependent advised look growth rate running time absolute value 
chapter experiments tau clique performance clique different thresholds 
algorithm thresholds enclus sig ffl enclus int ffl clique table parameters comparison experiment 
threshold values listed table 
data set experiment transactions synthetic data different dimensionality 
reasons described section section simple hyper rectangular data generated experiment 
subspace chosen dimensional clusters embedded subspace 
algorithms discover subspace successfully 
shows performance 
scales non linearly dimensionality 
running time dimensional case times running time dimensional case enclus sig enclus int clique respectively 
suggests algorithms better scalability clique 
suspect enclus algorithms run faster clique candidate sets enclus subspaces units clique 
potential number dense units larger chapter experiments dimensionality enclus sig enclus int clique comparison algorithms clique 
potential number subspaces enclus fewer candidates high dimensions 
enclus sig uses closure properties pruning effective closure property clique 
pruning power just running time enclus clique topic research 
special attention paid ensure fairness 
number dense units larger potential number subspaces base calculation pruning ratio clique larger 
result misleading clique may candidates despite better pruning ratio 
mdl pruning clique sacrifices accuracy better pruning 
enclus algorithms sacrifice accuracy 
pruning ratios directly comparable 
subspaces uniform projections clique studied hyper rectangular synthetic data 
experiments observe clique capable dealing data sets subspaces clustering projections chapter experiments cheung kong stock price cheung kong hsbc normalized :10.1.1.40.6757
look uniform 
subspaces missed clique 
kind data sets linearly dependent data 
example section variables subspace ab uniform 
pruned away clique low coverages due uniform distribution 
unfortunately inhibit discovery subspace abc 
test clique linearly dependent data 
generate data set containing sets linearly dependent variables 
clique unable discover different values threshold tried 
hand enclus sig enclus int handle successfully subspace containing linearly dependent variables give high interest 
uniform distribution lower subspaces give high entropy subspaces pruned away algorithms downward closure property keeps potential subspaces entropy exceeds threshold 
entropy subspace exceeds threshold impossible clustering 
try clique stock data set data set closely chapter experiments cluster 
number points table example illustrating problem hyper rectangular data 
cluster cluster cluster cluster cluster cluster example illustrating problem hyper rectangular data 
resembles linearly dependent data 
example shows stock price cheung kong hsbc 
shows close relationship stock prices 
dimensional projection strictly uniform show particular clusters 
run clique stock data set 
dimensional subspace highest coverage hsbc hk electric hang seng 
interest ranks th candidate dimensional subspaces 
highest interest 
high interest subspaces algorithm missed clique 
problems hyper rectangular data method generating hyper rectangular data previous experiments :10.1.1.131.5152
manipulation data awkward cluster cross subspaces 
chapter experiments look setting table example 
example intend embed clusters subspaces 
clusters may arise subspaces gives graphical illustration 
cluster cluster subspace uniform subspace similar argument holds cluster 
entropy subspaces particular low clusters looks uniform 
hand subspaces say cluster looks bar 
result contain clusters intended 
phenomenon subspace clustering algorithm discovers subspaces expected 
tested enclus clique set hyper rectangular data 
set dimensional data transactions generated 
embed clusters dimensional subspaces 
enclus clique give satisfactory accuracy hyper rectangular data 
enclus int recovers target subspaces dimensions introduces subspaces 
result clique deviates expected result detailed analysis 
recover target subspaces introduces lot subspaces 
stops dimensional subspace discovered 
result confusing clique tested hyper rectangular data reported accuracy 
replicate result simple hyper rectangular data sets having low number subspaces containing clusters experiment previous section 
chapter miscellaneous enhancements previous chapter discuss details algorithm enclus 
chapter go look miscellaneous enhancements applicable enclus 
include extra pruning strategies adjustment clustering criteria 
target provide better performance meaningful comprehensible results 
extra pruning attempt propose new pruning method improving performance 
experiments notice useless subspaces pruned away late passes 
suggests rooms reduction number candidate subspaces 
way doing making assumption 
assumption subspace significant interesting projection dimensions interest ffl ffl 
assumption set new threshold ffl 
rationale assumption observation data sets set chapter miscellaneous enhancements variables correlated subset shows degree correlation 
verified real life data sets experiments 
subspace correlated projections independent 
assumption true 
instance hold linearly dependent data 
assumption subspace interest ffl pruned away 
simple modification algorithm incorporate pruning techniques 
line enclus sig enclus int replaced follows 

interest ffl examine performance algorithm extra pruning technique hyper rectangular synthetic data 
owing problems described section clusters embedded dimensional subspaces 
generate transactions different dimensionality 
parameters ffl set 
result experiment shown 
observe algorithms extra pruning outperforms large margin 
data set target subspaces successfully recovered algorithms 
accuracy achieved assumption hold data set 
extra pruning applied linearly dependent data assumption hold 
multi resolution approach possible improvement enclus consideration number clusters 
recall means method problem determining chapter miscellaneous enhancements dimensionality enclus sig enclus int enclus sig extra pruning enclus int extra pruning performance algorithms extra pruning 
data sets equal coverage density different number clusters 
number clusters generally large number clusters lower average distance points cluster centroids giving dilemma forming cluster point give optimal distance measurement 
see number clusters valid consideration determination goodness clustering 
data sets possible coverage density sets set contains large number clusters set contains small number clusters 
shows example 
intuitive set smaller number clusters considered chapter miscellaneous enhancements clusters fall different grids 
entropy higher 
clusters fall grid 
entropy lower 
entropy entropy nats example explaining multi resolution method works 
set better clustering 
unfortunately algorithm take account 
handle criterion multi resolution approach calculating entropy 
repeat calculating entropy different values size interval delta 
coarse resolution entropy value favours small number clusters 
data points fall cells subspace small number clusters fall different cells subspace large number clusters 
entropy value lower smaller number clusters 
see illustration 
just coarse resolution capture difference densities different regions 
multiresolution approach 
extend algorithm having various entropy thresholds entropy different resolutions 
words multiple downward closure properties 
chapter miscellaneous enhancements correlated variables multi threshold approach multi threshold approach different thresholds difference passes 
multi threshold approach useful clique enclus 
look case clique 
shows problem case clique 
variables correlated 
distribution uniform considered individually joint distribution 
depending threshold units considered dense 
case cluster dimensional space missed clique 
avoid situation threshold set low 
similar problem occurs values variables distribute setting queen problem see 
set low avoid missing dimensional clusters 
setting low create problems 
firstly generate tremendous amount dense units introduces large memory overhead hampers performance greatly 
dimensional subspace memory requirement storing count dense units 
secondly produce coverages uniformly distributed subspaces 
undesirable intend lower coverage uniformly distributed chapter miscellaneous enhancements subspaces 
problem solved multiple thresholds allowed 
multiple thresholds explicitly set clique 
minimal description length mdl method clique effectively assigns different thresholds different dimensionalities 
multi threshold approach useful enclus 
maintain downward closure property entropy threshold pass 
low dimension subspace tends low entropy 
result low dimensional subspace entropy threshold may useful users 
avoided give lower threshold lower passes 
violate downward closure property essential enclus 
propose post processing method 
multiple entropy thresholds set run enclus highest entropy threshold 
result users checked threshold level 
subspaces satisfy threshold corresponding level removed 
clumsy set multiple thresholds human 
may mdl pruning method clique entropy coverage 
effect mdl pruning enclus studied left 
chapter propose tackle problem mining numerical data clustering techniques transaction attributes seen data point dimensional space 
large databases typically large number attributes patterns occur subsets attributes important 
mining clusters subspaces important problem 
proposed solution consists steps identification subspaces containing clusters discovery clusters selected subspaces presentation users 
concentrate subproblem identifying subspaces containing clusters works done better known previous method clique 
propose criteria goodness clustering subspaces coverage density correlation 
proposed method measure entropy information theory typically gives lower value subspace clustering 
entropy decision trees data mining knowledge previous problem subspace clustering 
justify approach establishing relationship entropy criteria 
algorithm enclus sig incorporates idea pair downward upward closure properties problem mining correlation rules 
approach shown effective reduction search space 
problem downward closure property entropy upward closure property dimensional correlation entropy 
closure properties algorithm pruning power 
algorithm enclus int relaxes upward closure property non minimal correlated subspaces mined 
experiments carried show proposed algorithm successfully identify significant interesting subspaces pruning effective efficient 
algorithms compared clique better performance 
accuracy enclus higher forms data 
propose miscellaneous enhancements enclus powerful 
bibliography agrawal srikant 
fast algorithms mining association rules 
proceedings th vldb conference pages 
rakesh agrawal johannes gehrke dimitrios gunopulos prabhakar raghavan 
automatic subspace clustering high dimensional data data mining applications 
proceedings acm sigmod conference management data montreal canada 
rakesh agrawal tomasz imielinski arun swami 
mining association rules sets items large databases 
acm sigmod washington dc usa pages 
aho hopcroft ullman 
design analysis computer algorithms 
addison 
backer 
computer assisted reasoning cluster analysis 
prentice hall 
michael berry gordon 
data mining techniques marketing sales customer support 
wiley 
bradley usama fayyad cory reina 
scaling clustering algorithms large databases 
proceedings international conference knowledge discovery data mining kdd aaai press 
bradley mangasarian nick street 
clustering concave minimization 
mozer jordan petsche editors advances neural information processing systems pages cambridge ma 
mit press 
sergey brin rajeev motwani craig silverstein 
market baskets generalizing association rules correlations 
proceedings acm sigmod conference management data 
sergey brin rajeev motwani jeffrey ullman shalom tsur 
dynamic itemset counting implication rules 
proceedings acm sigmod conference management data 
david chiu andrew wong 
synthesizing knowledge cluster analysis approach event covering 
ieee transactions sytems man cybernetics vol 
smc march april pages 
thomas cover joy thomas 
elements information theory 
wiley series telecommunications 
csisz ar korner 
information theory coding theorems discrete memoryless system 
academic press 
jay devore 
probability statistics engineering sciences 
duxbury press th edition 
martin ester hans peter kriegel jorg sander michael xu 
incremental clustering mining data warehousing environment 
proceedings th vldb conference new york usa 
martin ester hans peter kriegel jorg sander xu 
algorithm discovering clusters large spatial databases noise 
proceedings international conference knowledge discovery data mining kdd aaai press pages 
fayyad piatetsky shapiro 
advances knowledge discovery data mining 
aaai mit press 
fukuda morimoto morishita 
data mining dimensional optimized association rules scheme algorithms visualization 
proceedings acm sigmod conference management data 
fukuda morimoto morishita 
constructing efficient decision trees optimized numeric association rules 
proceedings nd vldb conference mumbai bombay india 
fukuda morimoto morishita 
mining optimized association rules numeric attributes 
proceedings fifteenth acm sigact sigmod sigart symposium principles database systems june 
clark glymour david madigan pregibon padhraic smyth 
statistical themes lessons data mining 
data mining knowledge discovery 
sudipto guha rajeev rastogi kyuseok shim 
cure efficient clustering algorithm large databases 
proceedings acm sigmod conference management data montreal canada june 
john hartigan 
clustering algorithms 
wiley 
swami 
set oriented mining rules 
technical report rj ibm almaden research center san joe california 
chun hung cheng ada fu yi zhang 
entropy subspace clustering mining numerical data 
proceedings acm sigkdd international conference knowledge discovery data mining kdd san diego 
leonard kaufman peter rousseeuw 
finding groups data cluster analysis 
wiley 
heikki mannila hannu toivonen 
algorithm finding interesting sentences extended 
proceedings th internation conference database theory pages 
pierre 
clustering techniques 
generation computer systems pages 
raymond ng jiawei han 
efficient effective clustering methods spatial data mining 
proceedings th vldb conference santiago chile 
nievergelt hinterberger 
grid file adaptable symmetric multikey file structure 
acm transactions database system pages 
quinlan 
induction decision trees 
machine learning pages 
kluwer academic publishers 
quinlan 
programs machine learning 
morgan kaufmann 
erich 
grid clustering efficient hierarchical clustering method large data sets 
proceedings internation conference pattern recognition icpr pages 
jan van der 
information theory 
cambridge university press 
xu martin ester hans peter kriegel jorg sander 
distribution clustering algorithm mining large spatial databases 
proceedings th international conference data engineering icde 
mohamed za 
comparative study clustering methods 
generation computer systems pages 
tian zhang raghu ramakrishnan miron livny 
birch efficient data clustering method large databases 
proceedings acm sigmod conference management data montreal canada pages june 
appendix differential entropy vs discrete entropy entropy introduced chapter designed discrete variables 
differential entropy continuous version entropy 
support set random variable probability density function random variable differential entropy defined follows gamma log dx variable define joint differential entropy measure uncertainty 
gamma sn log dx ideal case natural differential entropy discrete entropy criteria focus mining knowledge numerical data 
decision differential entropy 
differential entropy non negativity property 
important downward closure property lemma hold chosen differential entropy 

calculation differential entropy requires probability density function available 
raw data construct probability density function high dimensional data computationally expensive 
undesirable differential entropy algorithm need justify discrete entropy place differential entropy handle section 
relation differential entropy discrete entropy described section calculate entropy data partitioning grid 
dealing dimension effectively converts random variable quantized version delta proven entropy delta relates differential entropy manner 
theorem density random variable riemann integrable delta log delta delta entropy bit quantization continuous random variable approximately delta differ approximately constant log delta compare values entropy quantized variables comparing values differential entropy 
similar argument applies higher dimensions 
interval size delta carefully chosen delta gives approximation 
see section discussion topic 
appendix mining quantitative association rules chapter discuss mining quantitative association rules 
done switching subspace clustering problem 
briefly describe chapter 
problem mining quantitative association rule consider database numerical attributes boolean attribute want find rules form numerical attribute boolean attribute 
numerical attributes span dimensional space 
region dimensional space 
transaction mapped point space 
call right side arrow condition objective condition 
meaning rule denotes satisfied satisfies objective condition certain probability 
example age education length service income binary association rules rule associated support confidence values 
definitions similar counterpart binary association rules 
dimensionality rule number attributes contained condition 
previous proposed algorithms finding quantitative association rules dimension dimensions 
region dimension rules restricted special forms rectangles admissible regions 
propose algorithms solving problem higher dimensionality 
types regions restrictive 
types regions considered 
flexible form regions mining kind region efficient 
meaningful human 
path form region joined line branches 
connected region connected 
flexible path path special case 
clusters clusters defined connected regions 
approaches assume going mine dimensional rule chosen numerical attributes objective condition discretize space spanned attributes store support confidence cell multidimensional array 
regions mined greedy approaches 
pick cells 
time pick cell highest confidence 
confidence weighted average confidence individual cells confidence region monotonically decreases add new cells 
support increases 
adding new cells sufficient support confidence 
path mined depth search 
depth search try possible paths 
sufficient support confidence go result set 
exhaustive search time consuming propose pruning techniques 
calculate confidence bound 
current confidence drops bound know valid path impossible terminate tree search 
second may visit configuration tree search record visited configurations avoid repeated visits 
connected regions mined complicated algorithm 
configurations connected regions exhaustive search question 
propose iterative approximate algorithm 
chooses seeds cells 
subsequent pass tries grow neighbouring cells 
confidence improves pass improvement confidence impossible 
region support confidence goes result set 
region confidence support tries gather support neighbouring cells 
clusters mined combining method connected regions 
cluster connected region algorithm connected regions discover possible clusters 
greedy approach adopted 
add cluster result set decreasing order confidence support confidence 
performance performance proposed algorithms studied experiments 
greedy algorithm uses trivial computational time algorithms path connected regions worth studying 
algorithms scale exponentially number cells running time algorithm mining path rises faster connected regions 
algorithm mining path exhaustive search algorithm take pruning strategies account 
algorithm mining connected regions run faster produce regions higher support confidence 
attributed fact connected region restrictive form region path 
suggests connected region algorithm produces approximate solutions 
final remarks major problem algorithms target numerical attributes objective condition mind applying algorithms 
real database different attributes know attributes go rule 
problem exists previous studies quantitative association rules 
discussed section clusters better representation knowledge association rule 
switch study subspace clustering problem helps find useful subspaces processes 

