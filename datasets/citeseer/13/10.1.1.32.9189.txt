learning bandit problems michael duff department computer science university massachusetts amherst ma duff cs umass edu multi armed may viewed structured markov decision processes mdp potentially state sets 
particularly elegant methodology computing optimal policies developed ago gittins gittins jones 
gittins approach reduces problem finding optimal policies original mdp sequence low dimensional stopping problems solutions determine optimal policy called gittins indices 
shown gittins index process state may interpreted particular component maximum value function associated restart process simple mdp standard solution methods computing optimal policies successive approximation apply 
explores problem learning gittins indices line aid process model suggests utilizing process learning agents solve respective restart state subproblems includes example online reinforcement learning approach applied problem stochastic scheduling instance drawn wide class problems may formulated bandit problems 
reinforcement learning algorithms method temporal differences td sutton learning watkins originally advanced models animal learning motivated inspired behavioral paradigms classical instrumental conditioning :10.1.1.132.7760
algorithms subsequently proved useful solving certain problems prediction control encountered general adaptive real time systems agents embedded stochastic environments 
supporting theory applications reached stage development relatively mature 
connections established stochastic dynamic programming heuristic search barto mathematical framework grounded classical theory stochastic approximation led new improved proofs convergence jaakkola tsitsiklis 
researchers customarily focused attention asymptotic learning maximally efficient strategies optimal learning strategies 
successful applications large complex problems computational effort required traditional engineering methods unduly burdensome unjustified cases approximate models underlying systems known tesauro crites 
examines class problems called bandit problems considerable practical significance 
basic version problem concerns collection statistically independent reward processes family alternative bandit processes decision maker time selects process activate 
activated process yields immediate reward changes state processes remain frozen current states yield reward 
decision maker goal splice individual reward processes sequence rewards having maximum expected discounted value 
size state sets associated bandit problems may typically magnitude overwhelm straightforward methods solution 
large state sets possess particular cartesian product structure independence various control actions may exploit defining characteristics 
fact proof bandit problems decomposed simpler lowdimensional subproblems effect rendering prob lems previous approaches exponential complexity problems solutions linear complexity rigorously established gittins gittins jones 
show learning integrated gittins approach solve bandit problems online model free way 
reviewing bandit problem formulation notes complexity computing optimal policies family alternative bandit processes modeling family straightforwardly large markov decision process 
followed discussion gittins approach comparatively efficient elegant method number interesting interpretations allows learning applied 
main contribution appears section central conceptual argument summarized implementational details reinforcement learning algorithm 
followed examples discussion important generalizations basic bandit formulation cases practical interest 
concludes observing archetypal multi armed bandit problem policies map histories arm selections captures essence problem optimal learning algorithm section may interpreted method learning learn optimally 
bandit problems suppose exist stochastic processes fx values members countable set 
stage decision maker chooses action ng 
supposing state xn evolves random disturbance depending prior disturbances 
example markov transitions state evolution governed rfx yg prespecified transition matrix 
case considered henceforth 
goal choose sequence actions fa maximize expected value discounted infinite horizon expected return fl delta bounded reward function fl task task task activation switch bandit problem schematic 
discount factor 
decision maker essentially switching component processes reward streams activating process causes change state states remain frozen 
literature component process referred bandit process entire collection candidate processes termed family alternative bandit processes 
highly influential robbins robbins initiated systematic study bandit problems emphasized strategies asymptotic loss tends zero 
bellman bellman adopted bayesian formulation infinite horizon discounted case gittins jones gittins jones generalized bellman process model 
term multi armed bandit refers bayesian adaptive control problem selecting sequence plays slot machine arms corresponding different unknown probability distributions payoff 
may identify conditional distributions success respective arms observed past history stage process state fx 
action pulling arm elicits immediate reward payoff bayes rule update markov arm success probability 
slot machine example highlights key feature multi armed bandit problems may prudent sacrifice short term reward information gain allow informed decisions 
whittle whittle claimed ban versions problem time option retiring permanently receiving time reward fl provides useful parametrization certain derivations interpretations gittins index reviewed section note sufficiently small retirement option excluded 
dit problem embodies essential form conflict evident human action 
exploration versus exploitation trade recurring theme sequential experimentation adaptive control general bandit problem challenging 
final observation consider bandit problem component processes tasks state space state bandit problem process element action activating task generates immediate reward causes state change markovian fashion 
ignoring special structural constraints satisfied bandit problem process simply standard markov decision process mdp potentially large state set 
transition matrices jsj jsj standard methods computing optimal policies complexity order roughly jsj cn small constant 
non activated tasks change state rewards received depend state active task 
features may naturally lead conjecture existence defined optimal policies determination requires order jsj 
researchers mean say example bandit problem solved puzzling researchers years gittins jones 
gittins index consider version multi armed bandit problem decision maker added option stage permanently retiring receiving retirement reward fl rich structure bandit problem turns imply exist functions task map task states numbers indices optimal policies form retire max fg activate task max fg interpretation index profitability activating task known gittins index 
order gain insight meaning gittins index method calculating consider bandit problem single task standard stopping problem 
optimal value function viewed function fixed large values sufficiently small constant value independent optimal policy excludes retirement 
extremes may shown convex monotonically non decreasing gittins index indifference threshold bertsekas 
minimal value 
fact gittins index minimal value minfm jv mg interpretation index provides indifference threshold state retiring activating task state rigorous proof fact policies determined indices defined way optimal scope see gittins varaiya bertsekas rigorous proofs 
whittle proof whittle index rule yields optimal policy reveals way interesting relationship exists optimal value function multi task optimal value functions component bandit processes interpretation index may derived ross considering single task problem intial state retirement reward optimal policy indifferent continuing retiring 
follows positive random retirement time stopping time sense stochastic process theory discounted return prior fl equality holding optimal continuation policy 
max discounted return prior gamma fl integer valued positive random variable said stopping time sequence fx event tg independent fair charge prevailing charge fair charge prevailing charge associated example bandit process trajectory 
gamma fl max discounted return prior discounted time prior calculate index suffices find stopping time maximum reward unit time discounted prior maximal 
weber provides intuitive proof weber optimality gittins index rule notion fair game interpretation index equivalent previously mentioned indifference threshold interpretation 
similar view varaiya index interpreted winning bid auction right pipe gittins candidate bandit processes standard bandit process 
discussion follows weber 
suppose bandit process decision maker gambler may choose activate play process pay fixed prevailing charge play 
bandit state may define fair charge value prevailing charge optimal play bandit fair game sup sup gamma fl gamma jx stopping time defined policy bandit process state evolves fair charge 
event fair charge current state dips prevailing charge case gambler normally playing imagine prevailing charge reset fair charge 
sequence prevailing charges bandit process non increasing number plays gambler experiences continued play fair game 
case multiple bandit processes policy playing bandit greatest prevailing charge equivalently fair charge gambler interleaves prevailing charges component bandit streams non increasing sequence 
nature discounting policy maximizes expected total discounted charge paid gambler 
gambler engaged playing fair game policy maximizes expected total discounted reward 
restart state problems gittins index restrict attention moment reward structure associated single task states consider restart problem 
state option continuing state accumulating discounted rewards instantaneously teleporting state accumulating discounted rewards 
problem find policy optimizes expected discounted value state 
dynamic programming equation optimal value function problem may written fl jk continue fl ik restart signifies th component optimal value function restart state problem 
particular th component satisfies fl ik may interpreted maximum value state corresponding embedded single state semi markov decision chain satisfies max phi discounted reward prior fl psi stopping time process period chooses restart state restart problem see 
comparing equation equation previous section optimal continuation policy concludes may identified gittins index state state set states optimal restarting set entered optimal restart see 
number transitions taken reach restarting set starting state optimal stopping time associated gittins index 
suggest calculating gittins indices solving restart set restart problem 
corresponding restart problems successive approximation fl fl 
fl ik 
vn fl nk state corresponds restart subproblem solved way yielding th component gittins index state line estimation gittins indices learning multi armed bandit problem stated section section gittins index approach constructing optimal policies approach reduces bandit problem sequence low dimensional stopping problems section asserted gittins index state may characterized component optimal value function associated stopping problem simple mdp 
reinforcement learning methods learning adaptive model free algorithms applied online computing optimal policies mdp 
learning watkins originally advanced sample monte carlo extension successive approximation solving bellman equation alternative motivation justification algorithm rigorous proofs convergence appeal results theory stochastic approximation see jaakkola tsitsiklis 
follows principle learning applied calculate gittins indices provides model free means learning solve bandit problems online 
principle theory reinforcement learning implies learning converge correct optimal values associated various restart mdp 
practical terms reasonable question meaning restart action example context stochastic scheduling 
simply reset task desired state omnipotent act 
desires learning updates backups performed states arise naturally sample paths bandit problem process 
consider step task activated changes state state state generating reward simple transition yields data value continue action action state note data restart problems task 
observe transition supplies information restart action restart subproblem states task 
follows observing state transition reward active task states allows learning backups performed backup state action continue restart problem continue data state action restart restart problem restart data 
remains define task activations way achieves adequate sampling states actions 
reasonable ways doing boltzman distribution action selection method proposed 
suppose multi task bandit process state xn 
current estimate gittins index task state state action continue restart problem task define action selection boltzman distribution task ig boltzman temperature 
summary stage ffl select task activate boltzman distribution 
ffl observe state transition immediate reward elicited activating task 
ffl perform backups number states activated task state action continue restart problem task gamma ff state action continue restart problem task ff fl max fc rg state action restart problem task state action restart restart problem task gamma ff state action restart restart problem task ff fl max fc rg state action restart problem task respectively denote admissible actions continue restart 
alternative processes tasks possible states nn values calculated stored 
note substantial reduction nn values required approach straightforward mdp formulation 
state transition gives rise backups effective parallelism may viewed reducing computational complexity 
calculate gittins indices algorithm solves nn mdp number tasks theta number restart problems task size task associated restart problems solved parallel simple mdp admissible actions state 
examples confirm algorithm works consider simple bandit problem shown 
problem tasks states 
transition probabilities rewards label arcs discount factor chosen fl 
optimal policy may calculated solving state mdp discussed section applying modelbased successive approximation scheme offline 
optimal policy activate task state activate task 
plots convergence gittins indices true values reinforcement learning task task simple bandit problem 
task state task state task state task state convergence indices state transitions index convergence gittins indices 
algorithm proposed section 
optimal policy activate task task leaves state 
consequently boltzman temperature lowered increasing number transitions greedy policy respect index estimates increasing proportion transition samples drawn task activations 
miscellaneous parameters govern rate boltzman temperature reduction optimized sense purpose example simply demonstrate line algorithm works 
meaningful example bandit problem static stochastic scheduling 
consider scenario trial fixed number tasks completed task service time determined respective distribution function example consider problem task scheduling task task model constant hazard rate 

task model non constant hazard rate 
geometric service time rf sg ae gamma ae gamma see 
task constant hazard rate ae known order minimize mean flow time mean waiting time optimal policy activate tasks decreasing order parameter 
may unreasonable presume distributions known priori 
case reinforcement learning algorithm section applied online calculate respective gittins indices directly building explicit task model 
non constant hazard rate cases handled defining task models suitable markov reward chains see 
example consider task model case task increasing hazard rate rf sg gamma gamma ae gamma gamma gamma ae gamma mean weighted defined weighted sum task finishing times divided number tasks 
trials trials gittins index surface plotted function state axis task axis 
probability task completion increases number task activations 
model simple generalization constant hazard rate case probability decreases simple exponential fashion respect number task activations 
experiment performed tasks modeled ae discount factor fl set 
reinforcement learning algorithm applied online trial way results plots gittins index surface estimate vertical axis versus task axis page ranging task task task state axis left right ranging state state stages learning process 
may seen index values gradually un roll task axis values initialized zero 
low numbered states sampled frequently consequently index values converge rapidly rarely encountered example state task 
known analytical means problem optimal schedule schedule tasks non preemptively highest hazard rate 
plots appear converging indices give rise policy index estimates rarely encountered task states slowly rising true values 
commonly encountered bandit states gittins surface estimate yields optimal scheduling policy relatively early learning process 
note pursue straightforward mdp approach mentioned section entail state set size order theta theta theta theta theta theta theta theta transition matrices corresponding dimension assuming knows effective range states task 
important stress scheduling literature assumed service time distributions known contribution reinforcement learning algorithm assumption 
problem stochastic scheduling cases constant monotone hazard rates analytically tractable resulting policies usually somewhat intuitive stated simply 
arbitrary non monotone hazard rates things wellunderstood reinforcement learning approach preclude application cases 
book gittins gittins contains applications bandit formulation job scheduling resource allocation sequential random sampling random search 
traced chain reasoning ffl multi armed bandit markov decision process mdp possessing special decompositional cartesian product structure 
ffl optimal policies bandit processes constructed efficiently calculating gittins indices 
ffl gittins index task state th component optimal value function restart problem 
ffl restart process standard mdp 
ffl optimal policies mdp computed online model free way learning 
ffl learning applied online process model compute solutions bandit problems 
implementational details practical algorithm section 
alternative state process resulting algorithm computes parallel desired gittins indices solving action mdp size proof convergence follows existing convergence proofs learning conventional mdp jaakkola tsitsiklis 
advantage reinforcement learning methods mentioned far methods may inherit computational advantage conventional model methods particularly large problems 
aspect discussed barto duff 
model process processes real time dynamic programming barto applied full model backups performed states encountered sample paths 
indirect methods adaptive real time dynamic programming adaptively construct model controlled process base control policies value function update computations latest model see gullapalli barto convergence proof 
number generalizations basic bandit formulation extreme practical interest scheduling 
example gittins gittins examined issue existence index theorems bandit problems precedence contraints focus constraints tree structure 
whittle whittle studied bandit problems new tasks arrive index results preserved arrival process poisson bernoulli 
case costs addressed 
server processor available enabling process active time general quite strong additional conditions required index theorem hold 
reinforcement learning algorithm section undergone preliminary empirical testing convergence utilization function approximators representing values thoughtful selection learning rate parameters raises interesting issue example section considered problem stochastic scheduling specific instance general bandit problem formulation 
general bandit problems archetypes optimal learning problems goal collect information inform behavior yield largest expected reward actions taken entire duration learning process 
reader urged recall slot machine interpretation multi armed bandit problem stated section 
algorithm solving bandit problems sense entitled learning learn optimally 
reinforcement learning algorithm surely optimal boltzman distribution scheme action selection practical provisional inspired informed bandit problem mode analysis 
envision problem optimally learning learn optimally 
learn optimally 
regress stated entirely meaningful watkins observed citing mcnamara houston learning optimal respect prior assumptions concerning probability distributions environments animal decision maker may encounter 
professor andrew barto members adaptive networks laboratory 
supported part national science foundation ecs professor barto 
barto sutton watkins 
learning sequential decision making gabriel moore eds 
learning computational neuroscience foundations adaptive networks mit press pp 
barto bradtke singh 
real time learning control asynchronous dynamic programming 
computer science department university massachusetts tech 
rept 

barto duff 
monte carlo matrix inversion reinforcement learning neural information processing systems 
bellman 
problem sequential design experiments 
bertsekas 
dynamic programming deterministic stochastic models hall 
crites 
multiagent reinforcement learning applied elevator control 
preparation 
gittins 
multi armed bandit allocation indices wiley 
gittins jones 
dynamic allocation index sequential design experiments progress statistics eds pp 

stochastic scheduling precedence relations switching costs appl prob 
gittins 
single machine scheduling precedence relations linear discounted costs oper 
res 

gullapalli barto 
convergence indirect adaptive asynchronous value iteration algorithms neural information processing systems 
varaiya 
multi armed bandit problem revisited opt 

applic 
jaakkola jordan singh 

convergence stochastic iterative dynamic programming algorithms neural information processing systems 

multi armed bandit problem decomposition computation 
math 


mcnamara houston 
optimal foraging learning 
journal theoretical biology 
robbins 
aspects sequential design experiments bull 
amer 
math 
soc 
ross 
stochastic dynamic programming academic press 
sutton 
learning predict method temporal differences machine learning 
tesauro 
practical issues temporal difference learning machine learning 
tsitsiklis 
asynchronous stochastic approximation learning machine learning 
varaiya 
extensions multi armed bandit problem discounted case ieee tac 

queueing networks prentice hall 
watkins 
learning delayed rewards 
phd thesis cambridge university 
weber 
index multi armed annals applied probability 
whittle 
optimization time dynamic programming stochastic control vol 
wiley 
