decision theoretic generalization line learning application boosting yoav freund robert schapire labs park avenue florham park nj research att com december part consider problem dynamically resources set options worst case line framework 
model study interpreted broad extension studied line prediction model general decision theoretic setting 
show multiplicative rule littlestone warmuth adapted model yielding bounds slightly weaker cases applicable considerably general class learning problems 
show resulting learning algorithm applied variety problems including gambling multiple outcome prediction repeated games prediction points second part apply multiplicative weight update technique derive new boosting algorithm 
boosting algorithm require prior knowledge performance weak learning algorithm 
study generalizations new boosting algorithm problem learning functions range binary arbitrary finite set bounded segment real line 
gambler frustrated persistent horse racing losses friends winnings decides allow group fellow bets behalf 
decides fixed sum money race money friends doing 
certainly knew ahead time friends win naturally friend handle 
lacking attempts allocate race way total winnings season reasonably close won bet friends 
describe simple algorithm solving dynamic allocation problems show solution applied great assortment learning problems 
appeared journal computer system sciences 
extended appeared proceedings second european conference computational learning theory barcelona march 
surprising applications derivation new algorithm boosting converting weak pac learning algorithm performs just slightly better random guessing arbitrarily high accuracy 
formalize line allocation model follows 
allocation agent options strategies choose number integers time step allocator decides distribution strategies amount allocated strategy 
strategy suffers loss determined possibly adversarial environment 
loss suffered delta average loss strategies respect chosen allocation rule 
call loss function mixture loss 
assume loss suffered strategy bounded loss generality 
condition assumptions form loss vectors manner generated adversary choice may depend allocator chosen mixture goal algorithm minimize cumulative loss relative loss suffered best strategy 
attempts minimize net loss la gamma min la delta total cumulative loss suffered algorithm trials strategy cumulative loss 
section show littlestone warmuth weighted majority algorithm generalized handle problem prove number bounds net loss 
instance results shows net loss algorithm bounded ln put way average trial net loss decreasing rate ln increases difference decreases zero 
results line allocation model applied wide variety learning problems describe section 
particular generalize results littlestone warmuth cesa bianchi problem predicting binary sequence advice team experts 
authors proved worst case bounds making line randomized decisions binary decision outcome space discrete loss prove slightly weaker bounds applicable bounded loss function decision outcome spaces 
bounds express explicitly rate loss learning algorithm approaches best expert 
related generalizations expert prediction model studied vovk kivinen warmuth haussler kivinen warmuth 
authors focused primarily multiplicative weight update algorithms 
chung generalization giving problem game theoretic treatment 
boosting returning horse racing story suppose gambler grows choosing experts wishes create computer program accurately predict winner horse race usual information number races won horse betting odds horse 
create program asks favorite expert explain betting strategy 
surprisingly expert unable articulate grand set rules selecting horse 
hand data specific set races expert trouble coming rule thumb set races bet horse won races bet horse favored odds 
rule thumb obviously rough inaccurate unreasonable expect provide predictions little bit better random guessing 
furthermore repeatedly asking expert opinion different collections races gambler able extract rules thumb 
order rules thumb maximum advantage problems faced gambler choose collections races expert extract rules thumb expert useful 
second collected rules thumb combined single highly accurate prediction rule 
boosting refers general problem producing accurate prediction rule combining rough moderately inaccurate rules thumb 
second part analyze new boosting algorithm inspired methods solving line allocation problem 
formally boosting proceeds follows booster provided set labeled training examples label associated instance instance horse racing example observable data associated particular horse race outcome winning horse race 
round booster distribution set examples requests unspecified oracle weak hypothesis rule thumb low error ffl respect ffl pr id 
distribution specifies relative importance example current round 
rounds booster combine weak hypotheses single prediction rule 
previous boosting algorithms freund schapire new algorithm needs prior knowledge accuracies weak hypotheses 
adapts accuracies generates weighted majority hypothesis weight weak hypothesis function accuracy 
binary prediction problems prove section error final hypothesis respect set examples bounded exp gamma fl ffl gamma fl error tth weak hypothesis 
hypothesis entirely random guesses error fl measures accuracy tth weak hypothesis relative random guessing 
bound shows consistently find weak hypotheses slightly better random guessing error final hypothesis drops exponentially fast 
note bound accuracy final hypothesis improves weak hypotheses improved 
contrast previous boosting algorithms performance bound depended accuracy accurate weak hypothesis 
time weak hypotheses accuracy performance new algorithm close achieved best known boosting algorithms 
section give extensions boosting algorithm multi class prediction algorithm hedge fi parameters fi initial weight vector number trials 
choose allocation 
receive loss vector environment 

suffer loss delta 
set new weights vector fi line allocation algorithm 
problems example belongs possible classes just 
give extension regression problems goal estimate real valued function 
line allocation algorithm analysis section algorithm called hedge fi line allocation problem 
algorithm analysis direct generalizations littlestone warmuth weighted majority algorithm 
pseudo code hedge fi shown 
algorithm maintains weight vector value time denoted omega ff times weights nonnegative 
weights initial weight vector nonnegative sum 
conditions initial weight vector may arbitrary may viewed prior set strategies 
bounds strongest strategies receiving greatest initial weight want choose initial weights give weight strategies expect perform best 
naturally reason favor strategies set initial weights equally note weights trials need sum 
algorithm allocates strategies current weight vector normalizing 
time hedge fi chooses distribution vector loss vector received weight vector updated multiplicative rule delta fi generally shown analysis applicable minor modification alternative update rule form delta fi fi function parameterized fi satisfying fi fi gamma gamma fi 
analysis analysis hedge fi mimics directly littlestone warmuth 
main idea derive upper lower bounds imply upper bound loss algorithm 
upper bound 
lemma sequence loss vectors ln gamma gamma fi hedge fi proof convexity argument shown ff gamma gamma ff ff 
combined equations implies fi gamma gamma fi gamma gamma fi delta applying repeatedly yields gamma gamma fi delta exp gamma gamma fi delta lemma follows immediately 
hedge fi gamma ln gamma fi note equation fi fi needed complete analysis 
theorem sequence loss vectors ng hedge fi gamma ln gamma ln fi gamma fi generally nonempty set ng hedge fi gamma ln gamma ln fi max gamma fi proof prove general statement equation follows special case fig 
equation fi fi max theorem follows immediately equation 
simpler bound states hedge fi perform worse best strategy sequence 
difference loss depends choice fi initial weight strategy 
weight set equally bound hedge fi min ln fi ln gamma fi depends logarithmically bound reasonable large number strategies 
complicated bound generalization simpler bound especially applicable number strategies infinite 
naturally uncountable collections strategies sum appearing equation replaced integral maximum supremum 
bound equation written hedge fi min ln ln fi gamma fi gamma fi 
vovk analyzes prediction algorithms performance bounds form proves tight upper lower bounds achievable values vovk results show constants achieved hedge fi optimal 
theorem algorithm line allocation problem arbitrary number strategies 
suppose exist positive real numbers number strategies sequence loss vectors lb min ln fi ln fi gamma fi gamma fi proof appendix 
choose fi far analyzed hedge fi choice fi proved reasonable bounds choice fi 
practice want choose fi maximally exploit prior knowledge may specific problem hand 
lemma helpful choosing fi bounds derived 
lemma suppose fi 
gammal ln fi gamma fi proof sketch shown gamma ln fi gamma fi fi fi 
applying approximation choice fi yields result 
lemma applied bounds bounds form lemma 
example suppose strategies know prior bound loss best strategy 
combining equation lemma hedge fi min ln ln fi ln 
general know ahead time number trials upper bound cumulative loss strategy dividing sides equation obtain explicit bound rate average trial loss hedge fi approaches average loss best strategy hedge fi min ln ln gives worst case rate convergence ln close zero rate convergence faster roughly ln 
lemma applied bounds theorem obtain analogous results 
bound equation improved special cases loss function prediction outcome function special form see example 
general case improve square root term ln constant factor 
corollary lower bound cesa bianchi 
theorem analyze line prediction problem seen special case line allocation model 
applications framework described point quite general applied wide variety learning problems 
consider set chung 
decision space delta space outcomes omega gamma bounded loss function delta theta omega 
results require bounded rescaling assume range 
time step learning algorithm selects decision ffi delta receives outcome omega gamma suffers loss ffi 
generally may allow learner select distribution space decisions case suffers expected loss decision randomly selected expected loss ffi decide distribution assume learner access set experts 
time step expert produces distribution delta suffers loss 
goal learner combine distributions produced experts suffer expected loss worse best expert 
results section provide method solving problem 
specifically run algorithm hedge fi treating expert strategy 
time step hedge fi produces distribution set experts construct mixture distribution outcome loss suffered hedge fi define loss suffered learner delta exactly mixture loss analyzed section 
bounds section applied current framework 
instance applying equation obtain theorem loss function set experts sequence outcomes expected loss hedge fi described min ln ln assumed bound expected loss best expert fi ln 
example 
ary prediction problem delta omega kg ffi ffi 
words problem predict sequence letters alphabet size loss function mistake 
probability respect prediction disagrees 
cumulative loss learner expert expected number mistakes entire sequence 
case theorem states expected number mistakes learning algorithm exceed expected number mistakes best expert ln possibly loss best expert bounded ahead time 
bounds type previously proved binary case littlestone warmuth algorithm 
algorithm improved vovk cesa bianchi 
main result section proof bounds shown hold bounded loss function 
example 
loss function may represent arbitrary matrix game rock scissors 
delta omega fr sg loss function defined matrix ffi decision ffi represents learner play outcome adversary play ffi learner loss learner loses round wins round round tied 
instance scissors cut cumulative loss learner expert expected number losses series rounds game play counting ties half loss 
results show repeated play expected number rounds lost algorithm converge quickly expected number lost best experts particular sequence moves played adversary 
example 
suppose delta omega finite represents game matrix example 
suppose create expert decision ffi delta expert recommends playing ffi game theoretic terminology experts identified pure strategies 
von neumann classical min max theorem states fixed game matrix exists distribution actions called mixed strategy achieves min max optimal value expected loss adversarial strategy 
min max value called value game 
suppose algorithm hedge fi choose distributions actions playing matrix game repeatedly 
case theorem implies gap learner average round loss larger best pure strategy maximal gap decreases zero rate log deltaj 
expected loss optimal mixed strategy fixed convex combination losses pure strategies smaller loss best pure strategy particular sequence events 
conclude expected trial loss hedge fi upper bounded value game plus log deltaj 
words algorithm perform worse algorithm uses optimal mixed strategy game better adversary play optimally 
holds true learner knows game played unknown learner adversarial opponent complete knowledge game played algorithm learner 
algorithms similar properties weaker convergence bounds devised blackwell hannan 
details see related 
example 
suppose delta omega unit ball ffi gamma jj 
problem predict location point loss suffered euclidean distance predicted point ffi actual outcome 
theorem applied probabilistic predictions allowed 
setting natural require learner expert predict single point measure space possible points 
essentially problem tracking sequence points loss function measures distance predicted point 
see handle problem finding deterministic predictions notice loss function ffi convex respect ffi jj affi gamma ffi gamma jj gamma jj gamma gamma jj omega gamma follows 
time learner predicts weighted average experts predictions ffi prediction ith expert time regardless outcome equation implies gamma jj jj gamma jj theorem provides upper bound right hand side inequality obtain upper bounds left hand side 
results case give explicit bounds total error distance predicted observed points learner relative best team experts 
dimensional case case previously analyzed littlestone warmuth improved kivinen warmuth 
result depends convexity bounded range loss function ffi respect ffi 
applied example squared distance loss function ffi gamma jj log loss function ffi gamma ln ffi delta cover design universal investment portfolios 
case delta set probability vectors points omega constant cases listed superior algorithms analyses known 
weaker specific cases emphasized results far general applied settings exhibit considerably structure horse racing example described 
boosting section show algorithm section line allocation problem modified boost performance weak learning algorithms 
briefly review pac learning model see instance kearns vazirani detailed description 
set called domain 
concept boolean function 
concept class collection concepts 
learner access oracle provides labeled examples form chosen randomly fixed unknown arbitrary distribution domain target concept 
amount time learner output hypothesis 
value interpreted randomized prediction label probability probability gamma 
assume direct access bias prediction results extended case random mapping 
error hypothesis expected value xd jh gamma chosen interpreted stochastic prediction simply probability incorrect prediction 
strong pac learning algorithm algorithm ffl ffi access random examples outputs probability gamma ffi hypothesis error ffl 
running time polynomial ffl ffi relevant parameters size examples received size complexity target concept 
weak pac learning algorithm satisfies conditions ffl gamma fl fl constant decreases polynomial relevant parameters 
weaklearn denote generic weak learning algorithm 
schapire showed weak learning algorithm efficiently transformed boosted strong learning algorithm 
freund boost algorithm considerably efficient schapire 
algorithms calling weak learning algorithm weaklearn multiple times time presenting different distribution domain combining generated hypotheses single hypothesis 
intuitive idea alter distribution domain way increases probability harder parts space forcing weak learner generate new hypotheses mistakes parts 
important practical deficiency boost majority algorithm requirement bias fl weak learning algorithm weaklearn known ahead time 
worst case bias usually unknown practice bias achieved weaklearn typically vary considerably distribution 
unfortunately boost majority algorithm take advantage hypotheses computed weaklearn error significantly smaller presumed worst case bias gamma fl 
section new boosting algorithm derived line allocation algorithm section 
new algorithm nearly efficient boost 
boost majority accuracy final hypothesis produced new algorithm depends accuracy hypotheses returned weaklearn able fully exploit power weak learning algorithm 
new algorithm gives clean method handling real valued hypotheses produced neural networks learning algorithms 
new boosting algorithm boosting roots pac model remainder adopt general learning framework learner receives examples chosen randomly fixed unknown distribution theta set possible labels 
usual goal learn predict label instance start describing new boosting algorithm simplest case label set consists just possible labels 
sections give extensions algorithm general label sets 
freund describes frameworks boosting applied boosting filtering boosting sampling 
boosting sampling framework natural framework analyzing batch learning learning fixed training set stored computer memory 
assume sequence training examples labeled instances drawn randomly theta distribution boosting find hypothesis consistent sample 
general hypothesis accurate training set accurate examples outside training set problem referred fitting 
overfitting avoided restricting hypothesis simple 
come back problem section 
new boosting algorithm described 
goal algorithm find final hypothesis low error relative distribution training examples 
distribution theta set nature distribution instances training set controlled learner 
ordinarily distribution set uniform algorithm maintains set weights training examples 
iteration distribution computed normalizing weights 
distribution fed weak learner weaklearn generates hypothesis hope small error respect distribution 
new hypothesis boosting algorithm generates weight vector process repeats 
iterations final hypothesis output 
hypothesis combines outputs weak hypotheses weighted majority vote 
call algorithm adaboost previous algorithms adjusts adaptively errors weak hypotheses returned weaklearn 
weaklearn pac weak learning algorithm sense defined ffl gamma fl assuming examples generated appropriately 
bound error need known ahead time 
results hold ffl depend performance weak learner distributions generated boosting process 
parameter fi chosen function ffl updating weight vector 
update rule reduces probability assigned examples hypothesis prediction increases probability examples prediction poor 
note adaboost boost majority combines weak hypotheses summing probabilistic predictions 
drucker schapire simard experiments performed boosting improve performance real valued neural network observed summing outcomes networks selecting best prediction performs better selecting best prediction network combining majority rule 
interesting new boosting algorithm final hypothesis uses combination rule observed better practice previously lacked theoretical justification 
introduced successful experiments conducted adaboost including authors drucker cortes jackson craven quinlan breiman 
analysis comparing figures obvious similarity algorithms hedge fi adaboost 
similarity reflects surprising dual relationship line allocation model problem boosting 
put way direct mapping reduction boosting problem line allocation problem 
reduction naturally expect correspondence relating strategies weak hypotheses trials associated loss vectors examples training set 
reduction reversed strategies correspond examples trials learning algorithms generalized distribution directly 
instance gradient algorithms probability associated example scale update step size example 
algorithm generalized way training sample re sampled generate new set training examples distributed distribution 
computation required generate re sampled example takes log time 
furthermore boolean range shown update rule exactly removes advantage hypothesis 
error distribution exactly 
algorithm adaboost input sequence labeled examples distribution examples weak learning algorithm weaklearn integer specifying number iterations initialize weight vector 
set 
call weaklearn providing distribution get back hypothesis 

calculate error ffl jh gamma 
set fi ffl gamma ffl 

set new weights vector fi gammay output hypothesis ae log fi log fi adaptive boosting algorithm 
associated weak hypotheses 
reversal definition loss hedge fi loss small ith strategy suggests action tth trial adaboost loss gamma jh gamma appearing weight update rule step small tth hypothesis suggests bad prediction ith example 
reason hedge fi weight associated strategy increased strategy successful adaboost weight associated example increased example hard 
main technical difference algorithms adaboost parameter fi longer fixed ahead time changes iteration ffl ahead time information ffl gamma fl fl directly apply algorithm hedge fi analysis follows fix fi gamma fl set gamma jh gamma adaboost equal weight assigned hypotheses 
delta exactly accuracy distribution assumption fl 
letting fi straightforward show gamma jy gamma gamma fi fi fi fi fi gamma fi fi fi fi fi definition 
theorem delta fl delta gamma ln fl fl fl gamma ln fi gamma ln gamma fl fl fl fl 
implies error ffl gammat fl boosting algorithm adaboost advantages direct application hedge fi 
giving refined analysis choice fi obtain significantly superior bound error ffl 
second algorithm require prior knowledge accuracy hypotheses weaklearn generate 
measures accuracy iteration sets parameters accordingly 
update factor fi decreases ffl causes difference distributions increase 
decreasing fi increases weight ln fi associated final hypothesis 
intuitive sense accurate hypotheses cause larger changes generated distributions influence outcome final hypothesis 
give analysis performance adaboost 
note theorem applies hypotheses ffl 
theorem suppose weak learning algorithm weaklearn called adaboost generates hypotheses errors ffl ffl defined step 
error ffl pr id final hypothesis output adaboost bounded ffl ffl gamma ffl proof adapt main arguments lemma theorem 
defined 
similar equation update rule step implies fi gammay gamma gammafi gamma jh gamma gamma gamma ffl gamma fi combining inequality get gamma gamma ffl gamma fi final hypothesis defined mistake instance fi gammay fi gamma 
final weight instance fi gammay combining equations lower bound sum final weights sum final weights examples incorrect fi ffl delta fi ffl error combining equations get ffl gamma gamma ffl gamma fi fi factors product positive minimize right hand side minimizing factor separately 
setting derivative tth factor zero find choice fi minimizes right hand side fi ffl gamma ffl 
plugging choice fi equation get equation completing proof 
bound error theorem written form ffl gamma fl exp gamma kl jj gamma fl exp gamma fl kl jj ln gamma ln gamma gamma kullback leibler divergence ffl replaced gamma fl case errors hypotheses equal gamma fl equation simplifies ffl gamma fl exp gammat delta kl jj gamma fl exp gamma fl form chernoff bound probability coin flips turn heads tosses random coin probability heads gamma fl 
bound asymptotic behavior bound boost majority algorithm 
equation get number iterations boosting algorithm sufficient achieve error ffl kl jj gamma fl ln ffl fl ln ffl note errors hypotheses generated weaklearn uniform theorem implies final error depends error weak hypotheses 
previous bounds errors boosting algorithms depended maximal error weakest hypothesis ignored advantage gained hypotheses errors smaller 
advantage relevant practical applications boosting expects error learning algorithm increase distributions fed weaklearn shift away target distribution 
generalization error come back discussing error final hypothesis outside training set 
theorem guarantees error sample small quantity interests generalization error error instance space ffl pr 
order ffl close empirical error ffl training set restrict choice way 
natural way doing context boosting restrict weak learner choose hypotheses simple class functions restrict number weak hypotheses combined choice class weak hypotheses specific learning problem hand reflect knowledge properties unknown concept 
choice various general methods devised 
popular method upper bound vc dimension concept class 
method called structural risk minimization 
see vapnik book extensive discussion theory structural risk minimization 
purposes quote vapnik theorem theorem vapnik class binary functions domain vc dimension distribution pairs theta 
define generalization error respect ffl pr sample training set independent random examples drawn theta define empirical error respect sample ffl jfi gj ffi pr jffl gamma ffl ln ln ffi ffi probability computed respect random choice sample 
defined ae class functions theta class functions defined linear threshold functions theta gamma clearly hypotheses generated weaklearn belong class final hypothesis adaboost rounds boosting belongs theta 
theorem provides upper bound vc dimension class final hypotheses generated adaboost terms weak hypothesis class 
theorem class binary functions vc dimension 
theta log base natural logarithm 
hypotheses generated weaklearn chosen class final hypotheses generated adaboost iterations belong class vc dimension log 
proof result vc dimension computation networks proved baum haussler 
view final hypothesis output adaboost function computed layer feed forward network computation units layer weak hypotheses computation unit second layer linear threshold function combines weak hypotheses 
vc dimension set linear threshold functions 
sum computation units vc dimensions classes functions associated unit td 

baum haussler theorem implies number different functions realized theta domain restricted set size em set log number realizable functions smaller implies vc dimension theta smaller guidelines structural risk minimization assuming know reasonable upper bound vc dimension class weak hypotheses 
hypothesis generated running adaboost iterations 
combining observed empirical error bounds theorems compute upper bound generalization error select hypothesis minimizes guaranteed upper bound 
structural risk minimization mathematically sound method upper bounds ffl generated way larger actual value chosen number iterations smaller optimal value leading inferior performance 
simple alternative cross validation fraction training set left outside set generate called validation set 
value chosen error final hypothesis validation set minimized 
extensive analysis relations different methods selecting model complexity learning see kearns 
initial experiments adaboost real world problems conducted drucker cortes indicate adaboost tends fit problems hundreds rounds boosting generalization error continues drop increase 
bayesian interpretation final hypothesis generated adaboost closely related suggested bayesian analysis 
usual assume examples generated distribution theta probabilities subsection taken respect suppose set valued hypotheses goal combine predictions hypotheses optimal way 
instance hypothesis predictions bayes optimal decision rule says predict label highest likelihood hypothesis values predict pr pr predict 
rule especially easy compute assume errors different hypotheses independent target concept assume event conditionally independent actual label predictions hypotheses gamma 
case applying bayes rule rewrite bayes optimal decision rule particularly simple form predict pr ffl gamma ffl pr gamma ffl ffl 
ffl pr 
add set hypotheses trivial hypothesis predicts value 
replace pr ffl logarithm sides inequality rearranging terms find bayes optimal decision rule identical combination rule generated adaboost 
errors different hypotheses dependent bayes optimal decision rule complicated 
practice common simple rule described justification assuming independence 
called naive bayes interesting principled alternative practice algorithm adaboost find combination rule theorem guaranteed non trivial accuracy 
improving error bound show section bound theorem improved factor 
main idea improvement replace hard valued decision soft threshold 
precise log fi log fi weighted average weak hypotheses consider final hypotheses form 
version adaboost hard threshold equals 
section soft threshold functions take values 
mentioned interpret randomized hypothesis probability predicting 
error id jh gamma simply probability incorrect prediction 
theorem ffl ffl theorem defined 
modified final hypothesis defined satisfies gamma gamma fi gammar error ffl bounded ffl gamma ffl gamma ffl instance shown sigmoid function fi gamma gamma satisfies conditions theorem 
proof assumptions error ffl delta jf gamma jr gamma fi gammay definition implies ffl fi gammay fi gamma gamma gamma ffl gamma fi fi gamma steps follow equations respectively 
theorem follows choice fi boosting multi class regression problems far restricted attention binary classification problems set labels contains elements 
section describe possible extensions adaboost multi class case finite set class labels 
give extension regression problem real bounded interval 
start multiple label classification problem 
kg set possible labels 
boosting algorithms output hypotheses error final hypothesis measured usual way probability incorrect prediction 
extension adaboost call adaboost direct 
weak learner generates hypotheses assign instance possible labels 
require weak hypothesis prediction error respect distribution trained 
provided requirement met able prove error combined final hypothesis decreases exponentially binary case 
intuitively requirement performance weak learner stronger desired 
binary case random guess correct probability probability correct random prediction 
requirement accuracy weak hypothesis greater significantly stronger simply requiring weak hypothesis perform better random guessing 
fact performance weak learner measured terms error rate difficulty unavoidable shown informal example schapire consider learning problem suppose easy predict label hard predict label 
hypothesis predicts correctly label guesses randomly guaranteed correct half time significantly beating accuracy achieved guessing entirely random 
hand boosting learner arbitrary accuracy infeasible assumed hard distinguish labeled instances 
natural example problem consider classification handwritten digits ocr application 
may easy weak learner tell particular image hard tell sure 
part problem boosting algorithm focus attention weak learner harder examples way forcing weak learner discriminate particular labels may especially hard distinguish 
second version multi class boosting attempt overcome difficulty extending communication boosting algorithm weak learner 
allow weak learner generate expressive hypotheses output vector single label intuitively component vector represents degree belief correct label components large values close correspond labels considered plausible 
likewise labels considered implausible assigned small value near questionable labels may assigned value near 
labels considered plausible implausible may assigned large small values 
give weak learning algorithm expressive power place complex requirement performance weak hypotheses 
usual prediction error ask weak hypotheses respect sophisticated error measure call pseudo loss 
pseudo loss varies example example round 
iteration pseudo loss function supplied weak learner boosting algorithm distribution examples 
manipulating pseudo loss function boosting algorithm focus weak learner labels hardest discriminate 
boosting algorithm adaboost described section ideas achieves boosting weak hypothesis pseudoloss slightly better random guessing respect pseudo loss measure supplied weak learner 
addition extensions described mention alternative standard approach convert multi class problem binary problems boosting separately binary problems 
standard ways making conversion successful errorcorrecting output coding approach advocated dietterich bakiri 
section extend adaboost boosting regression algorithms 
case error hypothesis defined theta gamma describe boosting algorithm adaboost methods similar adaboost boosts performance weak regression algorithm 
multi class extension direct extension multi class case goal weak learner generate round hypothesis low classification error ffl pr ip 
extended boosting algorithm called adaboost shown differs slightly adaboost 
main difference replacement error jh gamma binary case predicate define holds 
final hypothesis instance outputs label maximizes sum weights weak hypotheses predicting label 
case binary classification weak hypothesis error significantly larger equal value error significantly replaced gamma hypothesis error ffl useless boosting algorithm 
weak hypothesis returned weak learner algorithm simply halts weak hypotheses computed 
theorem suppose weak learning algorithm weaklearn called adaboost generates hypotheses errors ffl ffl ffl defined 
assume ffl 
error ffl pr id final hypothesis output adaboost bounded ffl ffl gamma ffl proof prove theorem reduce setup adaboost instantiation adaboost apply theorem 
clarity mark variables reduced adaboost space 
examples define adaboost example 
define adaboost distribution examples equal adaboost distribution tth round provide adaboost hypothesis defined rule terms tth hypothesis returned adaboost weaklearn 
setup easily proved induction number rounds weight vectors distributions errors computed adaboost adaboost identical ffl ffl fi fi algorithm adaboost input sequence examples labels kg distribution examples weak learning algorithm weaklearn integer specifying number iterations initialize weight vector 
set 
call weaklearn providing distribution get back hypothesis 
calculate error ffl 
ffl set gamma abort loop 

set fi ffl gamma ffl 

set new weights vector fi gamma output hypothesis arg max log fi multi class extension adaboost 
suppose adaboost final hypothesis mistake instance definition ff ff ff ln fi 
implies ff ff fact ff ffl 
definition implies ff ff definition final adaboost hypothesis 
pr id pr id adaboost instance label pr id exactly error applying theorem obtain bound error completing proof 
possible version boosting algorithm allow hypotheses generate predicted class label confidence 
learner suffers loss gamma prediction correct 
details omitted 
second multi class extension section describe second alternative extension adaboost case label space finite 
extension requires elaborate communication boosting algorithm weak learning algorithm 
advantage doing gives weak learner flexibility making predictions 
particular enables weak learner useful contributions accuracy final hypothesis weak hypothesis predict correct label probability greater 
described weak learner generates hypotheses form theta 
roughly speaking measures degree believed correct label associated instance attains value say hypothesis uninformative instance hand deviation strict equality potentially informative predicts labels plausible 
seen information potentially useful boosting algorithm 
formalize goal weak learner defining pseudo loss measures goodness weak hypotheses 
motivate definition consider setup 
fixed training example hypothesis answer gamma binary questions 
incorrect labels ask question label 
words ask correct label discriminated incorrect label assume momentarily takes values 
interpret answer question deems plausible label considered implausible 
likewise answer answers chosen uniformly random 
general case takes values interpret randomized decision procedure 
choose random bit probability 
apply procedure stochastically chosen binary function probability choosing incorrect answer question pr pr gamma answers gamma questions considered equally important natural define loss hypothesis average gamma questions probability incorrect answer gamma gamma gamma gamma discussed section different discrimination questions different importance different situations 
example considering ocr problem described earlier point boosting process example digit recognized 
stage question discriminates correct label clearly important questions discriminate digits 
natural way attaching different degrees importance different questions assign weight question 
instance incorrect label assign weight associate question discriminates label correct label replace average equation average weighted resulting formula called pseudo loss training instance respect gamma function ng theta called label weighting function assigns example training set probability distribution gamma discrimination problems defined 
weak learner goal minimize expected pseudo loss distribution weighting function id seen manipulating distribution instances label weighting function boosting algorithm effectively forces weak learner focus hard instances incorrect class labels hardest eliminate 
conversely pseudo loss measure may easier weak learner get weak advantage 
instance weak learner simply determine particular instance belong certain class idea remaining classes correct depending may gain weak advantage 
theorem main result section shows weak learner boosted consistently produce weak hypotheses pseudo losses smaller 
note pseudo loss achieved trivially uninformative hypothesis 
furthermore weak hypothesis pseudo loss ffl beneficial boosting replaced hypothesis gamma pseudo loss gamma ffl 
example 
simple example illustrating pseudo loss suppose seek oblivious weak hypothesis weak hypothesis value depends class label oblivious hypotheses se generally weak interest may appropriate find best oblivious hypothesis part instance space set instances covered leaf decision tree 
target distribution label weighting function 
notational convenience define gamma setting ffi verified oblivious hypothesis ffi clearly minimized choice ffi 
suppose gamma pr id proportion examples label verified pseudo loss strictly smaller case uniform distribution labels 
contrast weak learner goal minimization prediction error section shown oblivious hypothesis prediction error strictly label covers distribution 
case easier find hypothesis small pseudo loss small prediction error 
hand values quality prediction labels consequence 
particular incorrect label instance order pseudo loss smaller hypothesis predict correct label probability larger means case pseudo loss criterion stringent usual prediction error 
discussed case unavoidable hard binary classification problem embedded multi class problem 
example suggests may significantly easier find weak hypotheses small pseudo loss hypotheses prediction error small 
hand theoretical bound boosting prediction error theorem stronger bound theorem 
empirical tests shown pseudo loss generally successful weak learners restricted hypotheses 
powerful weak learners decision tree learning algorithms little difference pseudo loss prediction error 
algorithm called adaboost shown 
maintain weights instance label gamma fy weak learner provided distribution label weight function computed weight vector shown step 
weak learner goal minimize pseudo loss ffl defined step 
weights updated shown step 
final hypothesis outputs instance label maximizes weighted average weak hypothesis values 
theorem suppose weak learning algorithm weaklearn called adaboost generates hypotheses pseudo losses ffl ffl ffl defined 
error ffl pr id final hypothesis output adaboost bounded ffl gamma ffl gamma ffl proof proof theorem reduce instance adaboost apply theorem 
mark adaboost variables tilde 
training instance incorrect label gamma fy define adaboost instance associated label 
gamma adaboost instances indexed pair 
distribution instances defined gamma 
tth hypothesis provided adaboost reduction defined rule gamma setup verified computed distributions errors identical ffl ffl fi fi suppose example definition ff ff ff ln fi 
implies ff ff gamma ff definition pr id pr id algorithm adaboost input sequence examples labels kg distribution examples weak learning algorithm weaklearn integer specifying number iterations initialize weight vector gamma gamma fy 
set set 
call weaklearn providing distribution label weighting function get back hypothesis theta 

calculate pseudo loss ffl gamma 
set fi ffl gamma ffl 

set new weights vector fi gammah gamma fy output hypothesis arg max log fi second multi class extension adaboost 
adaboost instances label definition error pr gamma pr id applying theorem bound error completes proof 
omit details bound adaboost improved factor manner similar described section 
boosting regression algorithms section show boosting regression problem 
setting label space 
learner receives examples chosen random distribution goal find hypothesis value predicts approximately value seen 
precisely learner attempts find small mean squared error mse gamma methods applied reasonable bounded error measure sake concreteness concentrate squared error measure 
approach classification problems assume learner provided training set examples distributed focus minimization empirical mse gamma techniques similar outlined section true mse equation related empirical mse 
derive boosting algorithm context reduce regression problem binary classification problem apply adaboost 
done reductions proofs theorems mark variables reduced adaboost space 
example training set define continuum examples indexed pairs associated instance label 
recall predicate holds 
obviously infeasible explicitly maintain infinitely large training set see method implemented efficiently 
results section dealt finitely large training sets extension infinite training sets straightforward 
informally instance mapped infinite set binary questions form correct label bigger smaller 
similar manner hypothesis reduced binary valued hypothesis theta defined rule attempts answer binary questions natural way estimated value 
done classification problems assume distribution training set ordinarily uniform reduction distribution mapped density pairs way minimization classification error reduced space equivalent minimization mse original problem 
define jy gamma normalization constant jy gamma straightforward show 
calculate binary error respect density find desired directly proportional mean squared error fi fi fi gamma fi fi fi dy fi fi fi fi fi jy gamma fi fi fi fi fi gamma constant proportionality 
unraveling reduction obtain regression boosting procedure adaboost shown 
prescribed reduction adaboost maintains weight instance label initial weight function exactly density defined 
normalizing weights density defined step provided weak learner step 
goal weak learner find hypothesis minimizes loss ffl defined step 
step weights updated prescribed reduction 
definition ffl step follows directly reduction exactly classification error reduced space 
note similar adaboost adaboost varies distribution examples modifies round round definition loss suffered hypothesis example 
ultimate goal minimization squared error weak learner able handle loss functions complicated mse 
final hypothesis consistent reduction 
reduced weak hypothesis non decreasing function final hypothesis generated adaboost reduced space threshold weighted sum hypotheses non decreasing function output binary implies value exactly value defined 
note computing weighted median weak hypotheses 
impossible maintain weights uncountable set points 
closer inspection seen viewed function piece wise linear function 
linear pieces update step potentially breaks pieces point 
initializing storing updating piece wise linear functions straightforward operations 
integrals appear evaluated explicitly involve integration piece wise linear functions 
algorithm adaboost input sequence examples labels distribution examples weak learning algorithm weaklearn integer specifying number iterations initialize weight vector jy gamma jy gamma 
set dy 
call weaklearn providing density get back hypothesis 
calculate loss ffl fi fi fi fi fi dy fi fi fi fi fi ffl set gamma abort loop 

set fi ffl gamma ffl 

set new weights vector fi 
output hypothesis log fi log fi extension adaboost regression problems 
theorem describes performance guarantee adaboost proof follows reduction described coupled direct application theorem 
theorem suppose weak learning algorithm weaklearn called adaboost generates hypotheses errors ffl ffl ffl defined 
mean squared error ffl id theta gamma final hypothesis output adaboost bounded ffl ffl gamma ffl unfortunate property setup trivial way generate hypothesis loss 
similar situation encountered algorithm adaboost 
remedy problem allow weak hypotheses general class functions 
simple generalization allow weak hypotheses defined functions associates measure confidence prediction reduced hypothesis associate pair functions ae gamma 
hypotheses way ones defined slight variation algorithm adaboost boost accuracy general weak learners details omitted 
advantage variant hypothesis identically zero pseudo loss exactly slight deviations hypothesis encode weak predictions 
method section boosting square loss reasonable bounded loss function theta 
measure discrepancy observed label predicted label instance gamma goal learning find hypothesis small average loss 
assume differentiable respect non increasing non decreasing modify adaboost handle loss function need replace jy gamma initialization step yj 
rest algorithm unchanged modifications needed analysis straightforward 
acknowledgments corinna cortes harris drucker david helmbold keith vovk manfred warmuth helpful discussions 
eric baum david haussler 
size net gives valid generalization 
advances neural information processing systems pages 
morgan kaufmann 
david blackwell 
analog minimax theorem vector payoffs 
pacific journal mathematics spring 
leo breiman 
bias variance arcing classifiers 
unpublished manuscript 
available ftp ftp stat berkeley edu pub users breiman ps 
nicol cesa bianchi yoav freund david helmbold david haussler robert schapire manfred warmuth 
expert advice 
proceedings fifth annual acm symposium theory computing pages 
thomas chung 
approximate methods sequential decision making expert advice 
proceedings seventh annual acm conference computational learning theory pages 
thomas cover 
universal portfolios 
mathematical finance january 
thomas dietterich bakiri 
solving multiclass learning problems errorcorrecting output codes 
journal artificial intelligence research january 
harris drucker corinna cortes 
boosting decision trees 
advances neural information processing systems 
harris drucker robert schapire patrice simard 
boosting performance neural networks 
international journal pattern recognition artificial intelligence 
yoav freund :10.1.1.37.1595
data filtering distribution modeling algorithms machine learning 
phd thesis university california santa cruz 
retrievable ftp cse ucsc edu pub tr ucsc crl ps yoav freund 
boosting weak learning algorithm majority 
information computation appear 
extended appeared proceedings third annual workshop computational learning theory 
yoav freund robert schapire 
experiments new boosting algorithm 
machine learning proceedings thirteenth international conference pages 
yoav freund robert schapire 
game theory line prediction boosting 
proceedings ninth annual conference computational learning theory pages 
james hannan 
approximation bayes risk repeated play 
tucker wolfe editors contributions theory games volume iii pages 
princeton university press 
david haussler kivinen manfred warmuth 
tight worst case loss bounds predicting expert advice 
computational learning theory second european conference eurocolt pages 
springer verlag 
jeffrey jackson mark craven 
learning sparse perceptrons 
advances neural information processing systems 
michael kearns mansour andrew ng dana ron 
experimental theoretical comparison model selection methods 
proceedings eighth annual conference computational learning theory 
michael kearns umesh vazirani 
computational learning theory 
mit press 
kivinen manfred warmuth 
experts predicting continuous outcomes 
computational learning theory eurocolt pages 
springer verlag 
nick littlestone manfred warmuth 
weighted majority algorithm 
information computation 
ross quinlan 
bagging boosting 
proceedings fourteenth national conference artificial intelligence 
robert schapire 
strength weak learnability 
machine learning 
vapnik 
estimation dependences empirical data 
springer verlag 
vovk 
game prediction expert advice 
proceedings eighth annual conference computational learning theory 
vovk 
aggregating strategies 
proceedings third annual workshop computational learning theory pages 
dudley 
special vapnik chervonenkis classes 
discrete mathematics 
proof theorem start brief review framework vovk similar framework section 
framework line decision problem consists decision space delta outcome space omega loss function delta theta omega associates loss decision outcome pair 
trial learning algorithm receives decisions delta experts generates decision ffi delta 
receiving outcome omega gamma learner expert incur loss ffi respectively 
goal learning algorithm generate decisions way cumulative loss larger cumulative loss best expert 
properties assumed hold 
delta compact topological space 

function ffi 
ffi continuous 

exists ffi ffi 

exists ffi ffi 
give vovk main result 
decision problem defined omega gamma delta obey assumptions 
positive real numbers 
say decision problem bounded exists algorithm finite set experts finite sequence trials cumulative loss algorithm bounded ffi min ln number experts 
say distribution simple non zero finite set denoted dom 
set simple distributions delta 
vovk defines function characterizes hardness decision problem fi sup inf ffi delta sup omega ffi log fi dom fi proves powerful theorem theorem vovk decision problem bounded fi fi fi ln fi 
proof theorem proof consists steps define decision problem conforms vovk framework 
show lower bound function fi problem 
show algorithm line allocation problem generate decisions defined problem get theorem lower bound worst case cumulative loss decision problem defined follows 
fix integer set delta sk sk dimensional simplex sk fx 
set omega set unit vectors omega fe ek ith component components 
define loss function ffi ffi delta ffi easily verify definitions conform assumptions 
prove lower bound fi decision problem choose particular simple distribution decision space delta 
uniform distribution unit vectors dom fe ek distribution explicitly calculate fi inf ffi delta sup omega ffi log fi dom fi easy see denominator equation constant dom fi fi gamma probability vector ffi delta exist component ffi inf ffi delta sup omega ffi combining equations get fi ln fi ln gamma gammafi show line allocation algorithm subroutine solving decision problem 
match experts decision problem strategy allocation problem 
iteration decision problem proceeds follows 

experts generates decision sk 
algorithm generates distribution sn 
learner chooses decision ffi 
outcome omega generated 

learner incurs loss ffi delta expert suffers loss delta 
algorithm receives loss vector delta incurs loss delta delta delta ffi delta observe loss incurred learner decision problem equal loss incurred algorithm upper bound form la min ln decision problem bounded 
hand lower bound theorem lower bound fi equation get fi ln fi ln gamma gammafi ln gamma gammafi free parameter denominators equation gamma fi gives statement theorem 

