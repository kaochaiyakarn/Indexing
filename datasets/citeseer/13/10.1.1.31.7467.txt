independent component analysis extended infomax algorithm mixed sub gaussian super gaussian sources te won lee mark girolami terrence sejnowski salk edu ci paisley ac uk terry salk edu howard hughes medical institute computational neurobiology laboratory salk institute la jolla california usa department computing information systems university paisley pa scotland department biology university california san diego la jolla california usa institut fur technische universitat berlin berlin germany neural computation vol number text pages number figures number tables extension infomax algorithm bell sejnowski able blindly separate mixed signals sub super gaussian source distributions 
achieved simple type learning rule derived girolami choosing negentropy projection pursuit index 
parameterized probability distributions super gaussian regimes derive general learning rule preserves simple architecture proposed bell sejnowski optimized natural gradient amari uses stability analysis cardoso laheld switch sub super gaussian regimes 
demonstrate extended infomax algorithm able easily separate sources variety source distributions 
applied high dimensional data electroencephalographic eeg recordings effective separating artifacts eye blinks line noise weaker electrical signals arise sources brain 
blind source separation independent component analysis ica received attention potential signal processing applications speech enhancement systems telecommunications medical signal processing 
goal ica recover independent sources sensor observations unknown linear mixtures unobserved independent source signals 
contrast correlation transformations principal component analysis pca ica reduces statistical dependencies signals attempting signals independent possible 
blind source separation problem studied researchers neural networks statistical signal processing jutten herault comon cichocki bell sejnowski cardoso laheld amari pearlmutter parra deco oja karhunen girolami fyfe 
see nadal parga historical review ica karhunen review different neural blind source separation algorithms 
general ica reviews cardoso lee 

bell sejnowski developed unsupervised learning algorithm entropy maximization single layer feedforward neural network 
algorithm effective separating sources super gaussian distributions sharply peaked probability density functions heavy tails 
illustrated section bell sejnowski algorithm fails separate sources negative kurtosis uniform distribution 
pearlmutter parra developed contextual ica algorithm maximum likelihood estimation mle framework able separate general range source distributions 
motivated computational simplicity information theoretic algorithm preserves simple architecture bell sejnowski allows extension separation mixtures super gaussian sub gaussian sources 
girolami derived type learning rule viewpoint negentropy maximization exploratory projection pursuit epp ica 
algorithms line line 
line algorithms separate mixtures super gaussian sub gaussian sources proposed cardoso comon pham 
extended infomax algorithm preserves simple architecture bell sejnowski learning rule converges rapidly natural gradient proposed amari 
amari relative gradient proposed cardoso laheld 
computer simulations show algorithm successfully separate mixtures sources sound tracks speech sound signals bell sejnowski uniformly distributed sub gaussian noise signals noise source gaussian distribution 
test extended infomax algorithm challenging real world data performed experiments eeg recordings show clearly separate electrical artifacts brain activity 
relative entropy general term negentropy 
negentropy maximization refers maximizing sum marginal 
obtained pearlmutter cs unm edu bap demos html technique shows great promise analyzing eeg recordings makeig jung functional magnetic resonance imaging fmri data mckeown 
organized follows section problem stated simple general learning rule separate sub super gaussian sources 
rule applied simulations real data section 
section brief discussion algorithms architectures potential applications real world problems limitations research problems 
extended infomax algorithm assume dimensional zero mean vector delta delta delta components mutually independent 
vector corresponds independent source signals 
write multivariate vector product marginal independent distributions 
data vector delta delta delta xn observed time point full rank theta scalar matrix 
components observed vectors longer independent multivariate satisfy product equality 
shall consider case number sources equal number sensors components source normally distributed possible extract sources received mixtures comon 
mutual information observed vector kullback leibler kl divergence multivariate density product marginal univariate densities delta delta delta xn gamma gamma delta delta delta gamma delta delta delta xn log delta delta delta xn dx dx delta delta delta dxn simplicity write log dx mutual information positive equal zero components independent cover thomas 
goal ica find linear mapping signals wx statistically independent 
sources recovered scaling permutation 
ways learning 
comon minimizes degree dependence outputs contrast functions approximated edgeworth expansion kl divergence 
higher order statistics approximated cumulants th order 
methods related minimizing mutual information derived infomax approach 
nadal parga showed low noise case maximum mutual information input output neural processor implied output distribution factorial 
roth baram bell sejnowski independently derived stochastic gradient learning rules maximization applied respectively forecasting time series analysis blind separation sources 
similar adaptive method source separation proposed cardoso laheld 
simple general learning rule learning algorithm derived maximum likelihood formulation 
mle approach blind source separation proposed pham pursued pearlmutter parra cardoso 
probability density function observations expressed amari cardoso det jp hypothesized distribution 
log likelihood equation log det log maximizing log likelihood respect gives learning algorithm bell sejnowski deltaw theta gamma gamma gamma gamma delta delta delta gamma un un un efficient way maximize log likelihood follow natural gradient amari deltaw theta gamma proposed amari :10.1.1.31.9597
relative gradient cardoso laheld 
gradient simplifies learning rule equation speeds convergence considerably 
shown general learning algorithm equation derived theoretical viewpoints mle pearlmutter parra infomax bell sejnowski negentropy maximization girolami fyfe 
lee 
review techniques show relation 
parametric density estimate plays essential role success learning rule equation 
local convergence assured derivative log densities sources pham 
choose logistic function tanh tanh learning rule reduces bell sejnowski natural gradient deltaw theta gamma tanh theoretical considerations empirical observations shown algorithm limited separating sources super gaussian distributions 
sigmoid function bell sejnowski provides priori knowledge source distribution supergaussian shape sources 
discuss flexible sigmoid function sigmoid function parameters gamma match source distribution 
idea modeling parametric nonlinearity investigated generalized pearlmutter parra contextual ica cica algorithm 
model parametric form account temporal information choosing weighted sum logistic density functions variable means scales 
moulines 
xu 
model underlying mixtures detailed section bell sejnowski estimated sub gaussian density models extended infomax learning rule oe delta delta delta 
density clearly bimodal 
gaussians show separate sub super gaussian sources 
parametric modeling approaches general computationally expensive 
addition empirical results eeg event related potentials erp contextual ica indicate cica fail find independent components 
conjecture due limited number recorded time points data points erps reliable density estimate difficult 
deriving learning rule separate sub super gaussian sources purpose extended infomax algorithm provide simple learning rule fixed nonlinearity separate sources variety distributions 
way generalizing learning rule sources sub super gaussian distributions approximate estimated edgeworth expansion gram charlier expansion stuart ord proposed girolami fyfe 
girolami parametric density estimate derive learning rule making approximations show 
symmetric strictly sub gaussian density modeled symmetrical form pearson mixture model pearson follows girolami 
gamma oe gamma oe delta oe normal density mean variance oe shows form density oe varying delta delta delta 
gaussian model clearly bimodal 
kurtosis normalized th order cumulant gamma oe th order cumulant girolami depending values oe kurtosis lies gamma 
equation defines strictly sub gaussian symmetric density 
super gaussian gaussian density model super gaussian distribution 
super gaussian model heavier tail normal density 
defining oe applying equation may write gamma oe gamma exp au gamma exp exp au exp definition hyperbolic tangent write oe gamma oe tanh oe setting oe equation reduces gamma tanh learning rule strictly sub gaussian sources equation equation deltaw theta tanh gamma uu case unimodal super gaussian sources adopt density model pg sech pg zero mean gaussian density unit variance 
shows density model 
nonlinearity gamma tanh learning rule super gaussian sources equation equation deltaw theta gamma tanh gamma uu difference super gaussian learning rule equation sub gaussian learning rule equation sign tanh function 
deltaw ae theta gamma tanh gamma uu super gamma gaussian theta tanh gamma uu sub gamma gaussian learning rules differ sign tanh function determined switching criterion 
girolami employs sign kurtosis sources switching criterion 
general definition sub super gaussian sources chose switching criterion stability criteria subsection 
switching nonlinearities switching sub super gaussian learning rule deltaw theta gamma tanh gamma uu ae super gamma gaussian gamma sub gamma gaussian elements dimensional diagonal matrix switching parameter derived generic stability analysis separating solutions employed cardoso laheld pham amari 

stability analysis mean field approximated order perturbation parameters separating matrix 
linear approximation near stationary point gradient mean field stationary point 
real part eigenvalues derivative mean field negative parameters average pulled back stationary point sufficient condition guaranteeing asymptotic stability derived cardoso ef gamma ef tanh substituting equation equation gives sech gamma ef tanh gamma gamma ef tanh delta ensure sign sign gamma ef tanh learning rule equation sign gamma gamma ef tanh delta hyperbolic cauchy density model parametric density model may separation sub super gaussian sources 
define parametric mixture density sech sech gamma function density estimate suited separate super gaussian sources 
example density estimate bimodal suited separate sub gaussian sources 
shows parametric density function parametric density proportional hyperbolic cauchy distribution suited separating supergaussian sources 
parametric density estimator bimodal distribution negative kurtosis suitable separating sub gaussian sources gamma log gamma tanh tanh tanh gamma learning algorithm sub super gaussian sources equation equation deltaw theta tanh gamma tanh gamma tanh gamma dim 
vector elements learning rule reduces deltaw theta gamma tanh exactly learning rule bell sejnowski natural gradient extension 
parametric density bimodal shown learning rule suitable separating signals sub gaussian distributions 
may sign general stability criteria equation equation determine switch example 
compare range kurtosis values parametric mixture density models equation equation 
kurtosis value shown function shaping parameter symmetric pearson density model hyperbolic cauchy mixture density model 
kurtosis pearson model strictly negative kurtosis zero 
kurtosis hyperbolic cauchy model ranges positive negative may separate signals sub super gaussian densities 
see eqs 

symmetric bimodal densities considered sub gaussian case 
shaping parameter kurtosis hyp cauchy mixture pearson mixture kurtosis value shown function shaping parameter pearson density model hyperbolic cauchy density model 
models approach gamma shaping parameter increases 
kurtosis pearson model strictly negative 
kurtosis hyperbolic cauchy model ranges positive negative may single parametric model separate signals sub super gaussian densities 
simulations experimental results extensive simulations experiments performed recorded data verify performance extended infomax algorithm equation 
show algorithm able separate large number sources wide variety sub super gaussian distributions 
compared performance extended infomax learning rule equation original infomax learning rule equation 
second performed set experiments eeg data high dimensional include various noise sources 
mixed sound sources obtained mixed sound sources separated contextual ica described pearlmutter parra 
required transformation restricted rotation contrast nonlinear pca karhunen 
data points passed times learning rule block size batch 
corresponds iterations weight updates 
learning rate fixed 
shows error measure learning 
learning rules converged 
small variations extended infomax algorithm upper curve due adaptation process matrix initialized identity matrix learning process elements converge extract sub super gaussian sources respectively 
simulation example sources close gaussian slight variations density estimation change sign 
annealing learning rate reduced variation 
music signals super gaussians distribution separable original infomax algorithm 
sources separated pass data sec sparc workstation matlab shown table gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma table performance matrix equation mixed sound sources pass data 
pass data close identity matrix rescaling reordering 
experiments simulations momentum term helped accelerate convergence algorithm deltaw gamma ff deltaw ff takes account history ff increased increasing number weight updates ff 
performance learning process monitored error measure proposed amari 
jp ij max jp ik gamma jp ij max jp kj gamma ij elements performance matrix wa 
close permutation scaled identity matrix sources separated 
shows error measure learning process 
error iteration extended infomax original infomax separating mixed sound sources error measure equation separation sound sources 
upper curve performance extended infomax lower curve shows performance original infomax 
separation quality shown table 
compare speed extended infomax algorithm closely related ones separated mixed sound sources extended exploratory projection pursuit network inhibitory lateral connections girolami fyfe 
single feedforward neural network converged times faster architecture learning rate block size 
larger block sizes feedforward network feedback networks increases convergence speed considerably due reliable estimate switching matrix mixed sound sources separated sources sound tracks obtained pearlmutter speech sound signals bell sejnowski uniformly distributed sub gaussian noise signals noise source gaussian distribution 
densities mixtures close gaussian distributions 
parameters learning rate fixed block size data points passes data iterations 
shows performance matrix rows manually reordered normalized unity 
close identity matrix diagonal elements indicate amount error 
simulation employ measure recovery sources 
original infomax algorithm separated positive sources 
failed extract sources including super gaussian sources music low kurtosis respectively 
contrast shows performance matrix extended infomax algorithm close identity matrix 
listening test clear separation sources mixtures 
note sources ranged laplacian distribu source source original recovered kurtosis recovered kurtosis snr number type kurtosis infomax ext 
infomax ext 
infomax music music music music music music music music music music speech speech music speech music speech uni 
noise uni 
noise uni 
noise gauss 
noise table kurtosis original signal sources kurtosis recovered signals original infomax extended infomax 
source signals range highly speech signals gaussian noise kurtosis zero noise sources uniform distribution negative kurtosis 
boxes placed sources failed clearly separate 
addition snr computed extended infomax 
sources sources performance matrix separation sources original infomax algorithm normalizing reordering 
super gaussian sources recovered 
sub gaussian sources gaussian source super gaussian sources remain mixed aliased sources 
total sources extracted channels remained mixed see table 
tions exp speech gaussian noise uniformly distributed noise separated nonlinearity 
simulation results suggest super gaussian sub gaussian density estimates equation equation sufficient separate true sources 
learning algorithms equation equation performed identically 
eeg recordings electroencephalographic eeg recordings brain electrical activity human scalp artifacts line noise eye movements blinks cardiac signals pose serious problems analyzing interpreting recordings 
regression methods partially remove eye movement eeg data berg artifacts electrode noise cardiac signals muscle noise difficult remove 
makeig 
applied ica analysis eeg data original infomax algorithm 
showed artifactual components isolated overlapping eeg signals including alpha theta bursts 
analyzed eeg data collected develop method objectively monitoring operators listening auditory signals makeig 
half hour session subject asked push button detected auditory target stimulus 
eeg collected electrodes located sites international system makeig sampling rate hz 
extended infomax algorithm applied sources sources performance matrix separation sources extended infomax algorithm normalizing reordering 
approximately identity matrix indicates nearly perfect separation 
channels seconds data parameters learning rate fixed passes block size weight updates 
power spectrum computed channel power band hz compute relative power channel separated component 
shows time course channels eeg shows independent components extended infomax algorithm 
observations ica components power spectrum interest ffl alpha bursts hz detected components 
alpha band activity hz occurs eyes closed subject relaxed 
subjects alpha rhythm somewhat different frequencies scalp patterns 
ffl theta bursts hz detected components 
theta band rhythms hz may occur transient losses awareness makeig frontal theta bursts may occur intense concentration 
ffl eye blink isolated component sec 
ffl hz line noise concentrated component see bottom 
top shows power near hz distributed eeg channels predominantly components 
middle shows original infomax concentrate line noise component 
contrast extended infomax bottom panel concentrates mainly sub gaussian component channel 
shows eeg data set channels including eog channels 
eye blinks near sec sec contaminated channels 
shows ica components normalizing components respect contribution raw data 
ica component contained pure eye blink signal 
small periodic muscle spiking temporal sites extracted ica component 
experiments different eeg data sets confirmed separation artifactual signals highly reliable 
particular severe line noise signals decomposed components sub gaussian distributions 
jung 
show eye movement extracted 
discussion applications real world problems results reported separation eye movement artifacts eeg recordings immediate application medical research data 
independently 
reported similar findings eeg recordings fixed point algorithm ica oja 
useful compare ica algorithms data sets assess merits 
compared traditional techniques eeg analysis extended infomax requires supervision easy apply see makeig 
jung 

addition encouraging results eeg data mckeown 
demonstrated successful extended infomax algorithm fmri recordings 
investigated task related human brain activity fmri data 
application considered spatial temporal ica extended infomax algorithm extracted sub gaussian temporal components extracted original infomax algorithm 
limitations research extended infomax learning algorithm assumptions limit effectiveness 
algorithm requires number sensors greater number sources 
case sources sensors theoretical practical interest 
sensors observe sources eeg data time sec eog oz pz cz fz sec portion eeg time series prominent alpha rhythms hz 
location recording electrode scalp indicated left trace 
eog recording taken 
time sec extended ica components ica components extracted eeg data 
components sub gaussian distributions super gaussian distributions 
eye movement artifact seconds 
line noise concentrated component 
prominent rhythms components different time courses scalp distributions 
eeg ica components extended ica components hz amplitudes power ratio component top ratio power near hz components eeg data 
middle ratio power near hz infomax ica components 
bottom ratio power near hz extended infomax ica components 
note difference scale factor original infomax extended infomax 
eog eog pz cz fz fp fp eeg data time sec eeg data set channels including eog channels 
sec sec artifacts severe eye blinks contaminate data set 
eeg data time sec extended ica components extended infomax ica components derived eeg recordings 
eye blinks clearly concentrated component 
component contains steady state signal 
recover sources 
preliminary results lewicki sejnowski suggest overcomplete representation data extent extract independent components priori knowledge source distribution 
applied lee 
separate sources sensors 
second researchers tackled problem nonlinear mixing phenomena 
yang 
taleb jutten lee 
propose extensions linear mixing combined certain nonlinear mixing models 
approaches self organizing feature maps identify nonlinear features data lin cowan pajunen karhunen 
hochreiter schmidhuber proposed low complexity coding decoding approaches nonlinear ica 
third sources may stationary sources may appear disappear move speaker moving room 
cases weight matrix may change completely time point 
challenging problem existing ica algorithms 
method model context switching non stationary mixing matrix unsupervised way proposed lee 

fourth sensor noise may influence separation included model nadal parga moulines attias 
needs done determine effect noise performance 
addition limitations issues deserve research 
particular remains open question extent learning rule robust parametric mismatch limited number data points 
despite limitations extended infomax ica algorithm applications sub gaussian super gaussian sources need separated additional prior knowledge statistical properties 
extended infomax ica algorithm proposed promising generalization satisfies general stability criterion mixed sub gaussian super gaussian sources cardoso laheld 
learning algorithm derived girolami natural gradient extended infomax algorithm shown excellent performance large real data sets derived electrical blood flow measurements functional activity brain 
compared originally proposed infomax algorithm bell sejnowski extended infomax algorithm separates wider range source signals whilst maintaining simplicity 
acknowledgments lee supported german academic exchange program 
girolami supported ncr financial systems knowledge laboratory advanced technology development division dundee scotland 
sejnowski supported howard hughes medical institute 
indebted jean francois cardoso insights helpful comments stability criteria tony bell general comments discussions 
grateful ping jung scott makeig eeg data useful discussions comments olivier helpful comments 
reviewers fruitful comments 
amari 

natural gradient works efficiently learning 
neural computation press 
amari cardoso 

blind source separation semiparametric statistical approach 
ieee trans 
signal processing 
amari chen cichocki 

stability analysis adaptive blind source separation 
neural networks 
amari cichocki yang 

new learning algorithm blind signal separation 
advances neural information processing systems pages 
attias 

blind separation noisy mixtures em algorithm independent factor analysis 
neural computation submitted 
bell sejnowski 

information maximization approach blind separation blind deconvolution 
neural computation 
berg 

dipole models eye movements blinks 

clin 
pages 
cardoso 

blind beamforming non gaussian signals 
iee 
cardoso 

infomax maximum likelihood blind source separation 
ieee signal processing letters 
cardoso 

blind signal processing review 
proceedings ieee 
appear 
cardoso 

unsupervised adaptive filtering chapter entropic contrasts source separation 
haykin editor prentice hall 
appear 
cardoso 
laheld 

equivariant adaptive source separation 
ieee trans 

cichocki unbehauen 

robust learning algorithm blind separation signals 
electronics letters 
comon 

independent component analysis new concept 
signal processing 
cover thomas editors 
elements information theory volume 
john wiley sons new york 
deco 

information theoretic approach neural computing 
springer verlag isbn 


source separation prior knowledge maximum likelihood solution 
proc 
pages 
girolami 

self organizing artificial neural networks signal separation 
ph thesis department computing information systems paisley university scotland 
girolami 

alternative perspective adaptive independent component analysis algorithms 
neural computation appear 
girolami fyfe 

extraction independent signal sources exploratory projection pursuit network lateral inhibition 
proceedings vision image signal processing journal 
girolami fyfe 

generalised independent component analysis unsupervised learning emergent properties 
proc 
icnn pages houston usa 
hochreiter schmidhuber 

feature extraction lococode 
neural computation appear 
oja 

fast fixed point algorithm independent component analysis 
neural computation 
jung humphries lee makeig mckeown sejnowski 

extended ica removes artifacts electroencephalographic recordings 
advances neural information processing systems pages 
jutten herault 

blind separation sources part adaptive algorithm neuromimetic architecture 
signal processing 
karhunen 

neural approaches independent component analysis source separation 
proc 
th european symposium artificial neural networks pages bruges belgium 
karhunen oja wang 

class neural networks independent component analysis 
ieee trans 
neural networks 
lee girolami bell sejnowski 

unifying framework independent component analysis 
international journal mathematical computer models press 
lee koehler 

blind separation nonlinear mixing models 
ieee pages florida usa 
lee lewicki girolami sejnowski 

blind source separation sources mixtures overcomplete representations 
ieee signal processing letters submitted 
lee lewicki sejnowski 
submitted 
unsupervised classification non gaussian mixture models ica 
advances neural information processing systems 
mit press 
lewicki sejnowski 

learning nonlinear overcomplete efficient coding 
advances neural information processing systems pages 
lin cowan 

faithful representation separable input distributions 
neural computation 
makeig bell jung sejnowski 

independent component analysis electroencephalographic data 
advances neural information processing systems pages 
makeig 

changes eeg spectrum predict fluctuations error rate auditory vigilance task 
society psychophysiology volume 
makeig jung bell sejnowski 

blind separation event related brain response spatial independent components 
proc 
national academy sciences 
mckeown makeig brown jung kindermann lee sejnowski 

spatially independent activity patterns functional magnetic resonance imaging data stroop color naming task 
proceedings national academy sciences 
moulines cardoso 

maximum likelihood blind separation deconvolution noisy signals mixture models 
proc 
icassp volume pages munich 
nadal 
parga 

non linear neurons low noise limit factorial code maximizes information transfer 
network 
nadal 
parga 

redundancy reduction independent component analysis conditions cumulants adaptive approaches 
neural computation 
oja 

nonlinear pca learning rule independent component analysis 
neurocomputing 
pajunen karhunen 

maximum likelihood approach nonlinear blind source separation 
icann pages lausanne 
pearlmutter parra 

context sensitive generalization ica 
international conference neural information processing pages 
pearson 

contributions mathematical study evolution 
phil 
trans 
roy 
soc 

pham 


blind separation mixture independent sources quasi maximum likelihood approach 
ieee trans 
signal proc 
roth baram 

multidimensional density shaping sigmoids 
ieee trans 
neural networks 
stuart ord 

kendall advanced theory statistic distribution theory 
john wiley new york 
taleb jutten 

nonlinear source separation post nonlinear mixtures 
esann pages 
oja 

ica fixed point algorithm extraction artifacts eeg 
ieee nordic signal processing symposium pages espoo finland 
xu cheung yang amari 

maximum equalization entropy maximization mixture cumulative distribution functions 
proc 
icnn pages houston 
yang amari cichocki 

information back propagation blind separation sources non linear mixtures 
proc 
icnn pages houston 

