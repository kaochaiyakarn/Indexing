information objective functions active data selection david mackay computation neural systems california institute technology pasadena ca mackay hope caltech edu appeared neural computation pp 
learning efficient actively select particularly salient data points 
bayesian learning framework objective functions discussed measure expected informativeness candidate measurements 
alternative specifications want gain information lead different criteria data selection 
criteria depend assumption hypothesis space correct may prove main weakness 
theories data modelling assume data provided source control 
scenarios able actively select training data 
data measurements relatively expensive slow want know look learn possible 
jaynes bayesian reasoning applied problem centuries ago laplace consequence important discoveries celestial mechanics 
second scenario immense amount data wish select subset data points useful purposes 
scenarios benefit ways objectively estimating utility candidate data points 
problem active learning sequential design extensively studied economic theory statistics el gamal fedorov 
experimental design bayesian framework shannon information objective function studied lindley luttrell 
distinctive feature approach renders optimisation experimental design independent tests address january st darwin college cambridge cb eu mackay cam ac uk applied data loss functions associated decisions 
uses similar information objective functions discusses problem optimal data selection bayesian framework interpolation described previous papers mackay 
results direct analogs fedorov quantities involved different interpretations example fedorov dispersion estimator bayesian posterior variance parameter 
directly stimulated presentation john skilling maxent skilling 
neural networks literature active data selection known query learning concentrated slightly different problems baum hwang 
relates perfectly separable classification problems papers sensible query learning algorithm proposed empirical results algorithm reported baum gives convergence proof 
algorithms human designed clear objective function querying strategy optimises algorithms improved 
contrast discusses noisy interpolation problems derives criteria defined objective functions objective function leads different data selection criterion 
discuss application ideas classification problems mackay 
white study different problem context noise free interpolation assume large amount data gathered principles selecting subset data efficient training entire data set inputs targets consulted iteration decide example add training subset option permitted 
statement problem imagine gathering data form set input output pairs dn fx data modelled interpolant 
interpolation model specifies architecture defines functional dependence interpolant parameters model specifies regulariser prior cost function noise model describing expected relationship may interpolation model may linear non linear previous papers mackay described bayesian framework fitting comparing models assuming fixed data set 
discusses framework interpolation relates task selecting data gather 
criterion informative new datum depend interested 
alternatives spring mind 
decided particular interpolation model wish select new data points maximally informative values model parameters take 

alternatively interested getting globally determined interpolant want able predict value interpolant accurately limited region point input space able sample directly 

lastly unsure models best interpolation model want select data give maximal information discriminate models 
study tasks case wish evaluate utility function input location single measurement scalar 
complex task selecting multiple new data points addressed methods generalised solve task discussed fedorov luttrell 
similar problem choosing vector outputs measured addressed 
third definitions information gain studied lindley 
cases studied fedorov mainly non bayesian terms 
solutions obtained interpolation problem gaussian approximation cases assuming new datum relatively weak piece information 
common active learning utility evaluated assuming probability distributions defined interpolation model correct 
models assumption may achilles heel approach discussed section 
choice bias inferences 
speculate way choose gather data able bias inferences systematically away truth 
case need inferences way undoes biases account gathered data 
orthodox statistics estimators statistical tests depend sampling strategy 
likelihood principle states inferences depend likelihood actual data received data gathered didn 
bayesian inference consistent principle need undo biases introduced data collecting strategy possible biases introduced long perform inference data gathered berger loredo 
models concerned estimating distribution output variables input variables allowed look value datum decide include datum data set 
bias inferences distribution tjx 
choice information measure start need select measure information gained unknown variable receive new datum having chosen measure select expected information gain maximal 
measures information suggested shannon entropy properties sensible information measure known 
explore choice task want gain maximal information parameters interpolant probability distributions parameters receive datum 
change entropy distribution deltas sn gamma sn sn log measure argument log dimensionless 
greater deltas information gained case quadratic models discussed mackay set measure equal prior quantity sn closely related log occam factor 
alternative information measure cross entropy wp log define gammag obtain positive quantity measure information gain informed true distribution 
information measures equal 
intuitively differ measure flat deltas quantifies probability bubble shrinks new datum arrives incorporates measure bubble moves new datum 
probability distribution shrink certain learnt distribution moves region space 
question information measure appropriate potentially complicated fact consistent additive measure information receive datum datum general ab intriguing complication hinder task base decisions expectations deltas see expectation deltas equal purposes distinction 
result holds independent details models study independent gaussian approximation 
measure unimportant follows included avoid committing dimensional crimes 
note sign deltas defined information gain corresponds positive deltas 
occam factor det gamma exp mp zw ff sn log fl notation mackay 
proof deltas evaluate expectation quantities assume probability distribution datum abbreviated comes 
define probability distribution assuming current model complete error bars correct 
means probability distribution total specification model 
conditioning variables right omitted proof 
compare expectations deltas gamma wp wjt log wjt gamma wp wjt log wjt wp wjt log free measure measure 
term gammas gammae dt wp wjt log gammae wp log gammas sn deltas ffl candidate information measures equivalent purposes 
proof implicitly demonstrates deltas independent measure 
properties deltas proved lindley 
rest deltas information measure set constant 
maximising total information gain solve task choose expected information gain maximised 
intuitively expect learn interpolant gathering data location error bars interpolant currently greatest 
quadratic approximation confirm intuition 
notation likelihood data defined terms noise level oe fi gamma fi exp gammafie zd ed gamma zd appropriate normalising constant 
likelihood defined dependent noise level fi gamma correlated noise multiple outputs case fi gamma covariance matrix noise 
treated scalar simplicity 
likelihood data combined prior exp zw regularising constant weight decay rate ff corresponds prior expected smoothness interpolant obtain current probability distribution exp gammam zm ffe fie objective function quadratically approximated near probable parameter vector wmp wmp deltaw deltaw deltaw gamma wmp hessian rrm evaluated minimum wmp quadratic approximation 
minima treated distinct models mackay 
need know entropy gaussian distribution easy confirm gammam flat measure log log det gamma aim minimising size joint error bars parameters det gamma small possible 
expanding wmp wmp delta deltaw dependent sensitivity output variable parameter evaluated wmp imagine choose particular input collect new datum 
datum falls region quadratic approximation applies new hessian approximation rr gamma gg expression neglects terms terms exactly zero linear models discussed mackay necessarily negligible non linear models neural networks 
notice new hessian independent value datum takes specify information gain deltas datum evaluate just calculating see property datum causes maximally informative 
new entropy sn equal gamma log gamma det delta neglecting additive constants 
determinant analytically evaluated fedorov identities gamma gamma gamma fia gamma gg gamma fig gamma det det fig gamma obtain total information gain delta log det log fig gamma product fig gamma term fi tells surprisingly learn information low noise high fi measurement 
second term gamma precisely variance interpolant point datum collected 
result obtain maximal information interpolant take datum point error bars interpolant currently largest assuming noise oe measurements 
rule resulting optimal minimax design criteria fedorov 
interpolation models error bars largest extreme points data gathered 
criterion cases lead repeatedly gather data edges input space considered non ideal behaviour necessarily need introduce ad hoc procedure avoid 
reason want repeated sampling edges want know happens 
accordingly derive criteria alternative objective functions value information acquired interpolant defined region interest 
maximising information interpolant region interest come second task 
assume wish gain maximal information value interpolant particular point quadratic approximation uncertainty interpolant gaussian distribution size error bars terms hessian parameters oe gamma evaluated entropy gaussian distribution log oe const 
measurement sensitivity error bars scaled factor gamma ae ae correlation variables ae gamma oe oe oe oe gamma information gain marginal information gain delta log oe gamma log gamma gamma oe oe oe term gamma maximised sensitivities maximally correlated measured inner product metric defined gamma second task solved case extrapolation single point 
objective function demonstrated criticised section 
generalisation multiple points imagine objective function defined information gained interpolant set points fx points thought representatives region interest example points test set 
case includes generalisation output variable full generalisation optimisation experiment measurements see fedorov luttrell 
preceding objective function information generalised ways lead results 
objective function multiple points obvious objective function joint entropy output variables interested 
set output variables want minimise uncertainty fy runs sequence different input locations set different scalar outputs 
sensitivities outputs parameters covariance matrix values fy gamma matrix disregarding possibility full rank necessitate complex treatment giving similar results joint entropy output variables fy related log det gamma find information gain measurement sensitivity vector identities 
joint information gain delta log det gamma gamma log gamma gamma gamma gamma oe oe row vector gamma measures correlations sensitivities quadratic form vy gamma measures effectively correlations reduce joint uncertainty fy denominator oe oe term favour measurements small uncertainty 
criticism argue joint entropy fy interpolant values appropriate objective function 
simple example illustrate 
imagine number points defining region interest dimensionality parameter space resulting matrix may singular points close typically full rank 
parameter vector values interpolant fy locally linear correspondence 
means change entropy fy identical change entropy lindley 
confirmed substitution gamma gamma ag gamma yields 
datum chosen accordance equation maximise expected joint information gain fy exactly choice result obtained maximising criterion expected total information gain section 
clearly choice independent choice fy region interest 
criticism joint entropy restricted case reason objective function achieve want joint entropy decreased measurements introduce correlations predictions fy measurements reduce individual uncertainties predictions 
don want variables fy strongly correlated arbitrary way want small variance subsequently asked predict value able confident predictions 
second objective function multiple points motivates alternative objective function maximise average information gained 
define mean marginal entropy log oe const probability asked predict oe gamma measurement sensitivity vector obtain mean marginal information gain gamma log gamma gamma oe oe oe mean marginal information gain demonstrated criticised section 
simple variations objective function derived 
minimising mean marginal entropy predictions minimise mean marginal entropy predicted noisy variables modelled deviating additive noise variance oe obtain oe replaced oe oe alternative may lead significantly different choices marginal variances oe fall intrinsic variance oe predicted variable 
take approach loss functions require datum choose minimises expectation mean squared error predictions fy oe obtain objective function leading order deltae gamma oe oe increases bias favour reducing variance variables largest oe optimal design fedorov 
comment case linear models interesting note linear model oe quadratic penalty functions solutions second tasks depend locations data previously gathered actual data gathered ftg oe independent fi gg independent ftg 
complete data gathering plan drawn start 
non linear model decisions data gather affected previous observations 
maximising discrimination models quadratic approximation models slightly different gaussian predictions value datum 
measure datum input value tjh normal oe parameters oe obtained interpolation model best fit parameters wmp hessian sensitivity vector wmp oe gamma fi intuitively expect informative measurement value separated possible scale defined oe oe thought confirm expect gain information oe oe differ significantly points occam factor penalising powerful model significant 
define information gain deltas sn gammas gamma log exact calculations deltas analytically possible assume regime small information gain expect measurement give weak likelihood ratio tjh tjh 
regime gamma oe oe assumption take expectation page algebra leads result deltas oe oe gamma oe gamma oe oe oe terms correspond precisely expectations stated 
term favours measurements separated second term favours places oe oe differ 
third task solved 
fedorov similar derivation uses poor approximation loses second term 
demonstration discussion data set consisting points dimensional interpolation problem interpolated hidden unit neural network 
data generated smooth function adding noise standard deviation oe 
neural network adapted data weight decay terms ff controlled methods mackay noise level fi fixed oe data resulting interpolant error bars shown 
expected total information gain change entropy parameters shown function 
just monotonic function size error bars 
shows expected marginal information gain points interest fx gamma 
notice marginal information gain case peaked near point interest expect 
note height peak greatest gamma interpolant oscillates rapidly lower interpolant smoother 
marginal information gain total information gain equal 
shows mean marginal information gain points interest fx defined set equally spaced points interval gamma interval training data lie 
mean marginal information gain gradually decreases zero away region interest hoped 
region left characteristic period interpolant similar data spacing expected utility oscillates passes existing data points reasonable 
surprising feature estimated utility region lower data points estimated utility smooth region right 
achilles heel methods approach potential weakness may models defined region interest points fx expected marginal information gain measurement blows 
sigma error bars 
occur information gain estimates utility data point assuming model correct know model approximation tool incorrect possible undesirable behaviour result 
simple example illustrates problem obtained consider modelling data straight line unknown parameter 
imagine want select data obtain model predicts accurately assume model right clearly gain information sample largest possible jxj points give largest signal noise ratio determining assume model correct approximation tool common sense tells sample closer models know incorrect marginal information gain interpolant error bars data total information gain marginal information gains mean marginal information gain demonstration total marginal information gain data set interpolant error bars 
expected total information gain marginal information gains 
mean marginal information gain region interest defined equally spaced points interval gamma 
information gains shown scale nats nat log bits 
really right answer wrong question 
task research formulate new question answer appropriate approximation model 
mean marginal information gain promising objective function test 
computational complexity computation suggested objective functions moderately cheap inverse hessian gamma obtained models concerned 
nk process number data points number parameters process may performed order evaluate error bars models evaluate evidence evaluate parameter saliencies enable efficient learning 
cost compared cost locating minimum objective function worst case scales nk result quadratic function 
evaluation mean marginal information gain candidate points requires ck cv time number points interest evaluate gamma evaluate dot product vector 
evaluation mean marginal information gain computationally expensive inverse hessian evaluation 
contexts expensive progress exploring possibility reducing calculations smaller time statistical methods 
question efficiently search informative addressed gradient methods constructed shows information gain locally non convex scale defined inter datum spacing 
specifications information maximised solution obtained 
solutions apply linear non linear interpolation models depend validity local gaussian approximation 
solution analog non bayesian literature fedorov generalisations multiple measurements multiple output variables luttrell 
case function derived predicts information gain measurement function search optimal value large dimensional input spaces may trivial task 
function serve way reducing size large data set omitting data points expected informative 
function form basis stopping rule rule deciding gather data desired exchange rate information gain measurement lindley 
possible weakness information approaches estimate utility measurement assuming model correct 
lead undesirable results 
search ideal measures data utility open 
baum 
neural net algorithms learn polynomial time examples queries ieee trans 
neural networks 
berger 
statistical decision theory bayesian analysis springer 
el gamal 
role priors active bayesian learning sequential statistical decision framework maximum entropy bayesian methods jr eds kluwer 
fedorov 
theory optimal experiments academic press 

hwang choi oh marks ii 
query learning applied partially trained multilayer perceptrons ieee trans 
neural networks 
jaynes 
bayesian methods general background maximum entropy bayesian methods applied statistics ed 
justice lindley 
measure information provided experiment ann 
math 
statist 

loredo 
laplace supernova sn bayesian inference astrophysics maximum entropy bayesian methods ed 
kluwer 
luttrell 
design data sampling schemes inverse problems inverse problems mackay bayesian interpolation neural computation volume 
mackay practical bayesian framework backprop networks neural computation volume 
mackay evidence framework applied classification networks preparation 
white 
active selection training examples network learning noiseless environments dept computer science ucsd tr 
skilling 
bayesian solution ordinary differential equations maximum entropy bayesian methods seattle erickson smith eds kluwer 
allen tom loredo marcus mitchell referees helpful feedback 
supported caltech fellowship studentship serc uk 

