comparative study feature selection text categorization yiming yang school computer science carnegie mellon university pittsburgh pa usa yiming cs cmu edu jan pedersen verity ross dr sunnyvale ca usa verity com comparative study feature selection methods statistical learning text categorization 
focus aggressive dimensionality reduction 
methods evaluated including term selection document frequency df information gain ig mutual information mi test chi term strength ts 
ig chi effective experiments 
ig thresholding neighbor classifier reuters corpus removal removal unique terms yielded improved classification accuracy measured average precision df thresholding performed similarly 
strong correlations df ig chi values term 
suggests df thresholding simplest method lowest cost computation reliably ig chi computation measures expensive 
ts compares favorably methods vocabulary reduction competitive higher vocabulary reduction levels 
contrast mi relatively poor performance due bias favoring rare terms sensitivity probability estimation errors 
text categorization problem automatically assigning predefined categories free text documents 
textual information available online effective retrieval difficult indexing summarization document content 
documents categorization solution problem 
growing number statistical classification methods machine learning techniques applied text categorization years including multivariate regression models nearest neighbor classification bayes probabilistic approaches decision trees neural networks symbolic rule learning inductive learning algorithms :10.1.1.54.6608:10.1.1.39.6139
major characteristic difficulty text categorization problems high dimensionality feature space 
native feature space consists unique terms words phrases occur documents tens hundreds thousands terms moderate sized text collection 
prohibitively high learning algorithms 
neural networks example handle large number input nodes 
bayes belief models example computationally intractable independence assumption true features imposed 
highly desirable reduce native space sacrificing categorization accuracy 
desirable achieve goal automatically manual definition construction features required 
automatic feature selection methods include removal non informative terms corpus statistics construction new features combine lower level features terms higherlevel orthogonal dimensions 
lewis ringuette information gain measure aggressively reduce document vocabulary naive bayes model decision tree approach binary classification 
wiener mutual information statistic select features input neural networks :10.1.1.54.6608:10.1.1.21.466
yang schutze principal component analysis find orthogonal dimensions vector space documents 
yang wilbur document clustering techniques estimate probabilistic term strength reduce variables linear regression nearest neighbor classification 
moulinier inductive learning algorithm obtain features tive normal form news story categorization 
lang minimum description length principle select terms netnews categorization 
feature selection techniques tried thorough evaluations rarely carried large text categorization problems 
due part fact learning algorithms scale high dimensional feature space 
classifier tested small subset native space evaluate full range potential feature selection methods 
theoretical comparison example performance decision tree algorithms solving problems features native space 
analysis scale distant realities text categorization 
focus evaluation comparison feature selection methods reduction high dimensional feature space text categorization problems 
classifiers scaled target space thousands tens thousands categories 
seek answers questions empirical evidence ffl strengths weaknesses existing feature selection methods applied text categorization ffl extend feature selection improve accuracy classifier 
document vocabulary reduced losing useful information category prediction 
section describes term selection methods 
due space limitations include phrase selection approaches principal component analysis :10.1.1.54.6608
section describes classifiers document corpus chosen empirical validation 
section presents experiments results 
section discusses major findings 
section summarizes 
feature selection methods methods included study uses term goodness criterion thresholded achieve desired degree term elimination full vocabulary document corpus 
criteria document frequency df information gain ig mutual information mi statistic chi term strength ts 
document frequency thresholding df document frequency number documents term occurs 
computed document frequency unique term training corpus removed feature space terms document frequency predetermined threshold 
basic assumption rare terms non informative category prediction influential global performance 
case removal rare terms reduces dimensionality feature space 
improvement categorization accuracy possible rare terms happen noise terms 
df thresholding simplest technique vocabulary reduction 
easily scales large corpora computational complexity approximately linear number training documents 
usually considered ad hoc approach improve efficiency principled criterion selecting predictive features 
df typically aggressive term removal widely received assumption information retrieval 
low df terms assumed relatively informative removed aggressively 
re examine assumption respect text categorization tasks 
information gain ig information gain frequently employed criterion field machine learning 
measures number bits information obtained category prediction knowing presence absence term document 
fc denote set categories target space 
information gain term defined gamma log jt log jt log definition general employed binary classification models 
general form text categorization problems usually ary category space may tens thousands need measure goodness term globally respect categories average 
training corpus unique term computed information gain removed feature space terms information gain predetermined threshold 
computation includes estimation conditional probabilities category term entropy computations definition 
probability estimation time complexity space complexity number training documents vocabulary size 
entropy computations time complexity 
mutual information mi mutual information criterion commonly statistical language modelling word associations related applications :10.1.1.54.6608
considers twoway contingency table term category number times occur number time occurs number times occurs total number documents mutual information criterion defined log theta estimated log theta theta natural value zero independent 
measure goodness term global feature selection combine scores term alternate ways avg max max fi mi computation time complexity similar ig computation 
weakness mutual information score strongly influenced marginal probabilities terms seen equivalent form log gamma log terms equal conditional probability rare terms higher score common terms 
scores comparable terms widely differing frequency 
statistic chi statistic measures lack independence compared distribution degree freedom judge 
way contingency table term category number times occur number time occurs number times occurs number times occurs total number documents term goodness measure defined theta ad gamma cb theta theta theta statistic natural value zero independent 
computed category statistic unique term training corpus category combined scores term scores avg max max computation chi scores quadratic complexity similar mi ig 
major difference chi mi normalized value values comparable terms category 
normalization breaks longer accurately compared distribution cell contingency table lightly populated case low frequency terms 
statistic known reliable low frequency terms 
term strength ts term strength originally proposed evaluated wilbur vocabulary reduction text retrieval applied yang wilbur text categorization 
method estimates term importance commonly term appear closely related documents 
uses training set documents derive document pairs similarity measured cosine value document vectors threshold 
term strength computed estimated conditional probability term occurs second half pair related documents occurs half 
arbitrary pair distinct related documents term strength term defined term strength criterion radically different ones mentioned earlier 
document clustering assuming documents shared words related terms heavily overlapping area related documents relatively informative 
method task specific information term category associations 
sense similar df criterion different ig mi statistic 
parameter ts calculation threshold document similarity values 
close documents considered related pair 
arel average number related documents document threshold tuning 
compute similarity scores documents training set try different thresholds similarity values document pairs choose threshold results reasonable value arel 
value arel chosen experimentally optimized performance task 
previous evaluations retrieval categorization document collections arel values yield satisfactory performance 
computation ts quadratic number training documents 
classifiers data classifiers assess effectiveness feature selection methods different ary classifiers nearest neighbor classifier knn regression method named linear squares fit mapping llsf 
input systems document represented sparse vector word weights output systems ranked list categories confidence score category 
category ranking knn categories assigned nearest training documents input 
categories neighbors weighted similarity neighbor input similarity measured cosine document vectors 
category belongs multiple neighbors sum similarity scores neighbors weight category output 
category ranking llsf method regression model words document predict weights categories 
regression coefficients determined solving squares fit mapping training documents training categories 
properties knn llsf suitable experiments systems top performing state art classifiers 
evaluation classification methods reuters newswire collection section break point values knn llsf outperforming systems evaluated collection including symbolic rule learning ripper swap charade decision approach inductive learning sleeping experts typical information retrieval approach named rocchio :10.1.1.39.6139
variation reuters collection training set test set partitioned differently knn break point result neural networks llsf break point :10.1.1.54.6608
systems scale large classification problems 
large mean input output classifier thousands dimensions higher 
want examine degrees feature selection reduction removing standard words extremely aggressive reduction observe effects accuracy classifier entire target space 
examination need scalable system 
knn llsf ary classifier providing global ranking categories document 
allows straight forward global evaluation document categorization performance measuring goodness category ranking document category performance standard applying binary classifiers problem 
classifiers context sensitive sense independence assumed input variables terms output variables categories 
llsf example optimizes mapping document categories treat words separately 
similarly knn treats document single point vector space 
context sensitivity distinction context free methods explicit independence assumptions naive bayes classifiers regression methods 
context sensitive classifier better information provided features context free classifier enabling better observation feature selection 
classifiers differ statistically 
llsf linear parametric model knn non parametric non linear classifier assumptions input data 
evaluation classifiers reduce possibility classifier bias results 
data collections corpora study reuters collection ohsumed collection 
reuters news story collection commonly corpora text categorization research documents full collection half documents human assigned topic labels 
documents topic divided randomly training set test set documents 
partition similar employed differs full collection including newly revised version reuters available www research att com lewis :10.1.1.39.6139
documents stories mean length words standard deviation 
considered categories appear training set 
categories cover topics commodities interest rates foreign exchange 
documents fourteen assigned categories mean categories document 
frequency occurrence varies greatly category category earnings example appears roughly documents platinum assigned training documents 
unique terms collection performing inflectional stemming word removal conversion lower case 
ohsumed bibliographical document collection developed william hersh colleagues oregon health sciences university 
subset medline database consisting medical journals years 
titles abstracts 
refer title plus document 
documents manually indexed subject categories medical subject headings mesh national library medicine 
categories defined mesh categories ohsumed document collection 
documents training set documents test set study 
unique terms training set 
average length document words 
average categories assigned document 
sense ohsumed corpus difficult reuters data noisy 
word category correspondences fuzzy ohsumed 
consequently categorization difficult learn classifier 
empirical validation performance measures apply feature selection documents preprocessing knn llsf 
effectiveness feature selection method evaluated performance knn llsf preprocessed documents 
knn llsf score categories document basis standard definition serious problem collection text categorization evaluation 
large proportion documents test set incorrectly unlabelled 
evaluation results highly questionable non interpretable unlabelled documents discarded analyzed 
ohsumed available anonymous ftp edu directory pub ohsumed 
recall precision performance measures recall categories correct total categories correct precision categories correct total categories categories means categories score threshold 
document recall thresholds 
system assigns decreasing score order categories needed recall achieved computes precision value point 
resulting point precision values averaged obtain single number measure system performance document 
test set documents average precision values individual documents averaged obtain global measure system performance entire set 
specified precision avgp refer point average precision set test documents 
experimental settings applying feature selection documents removed words standard word list 
feature selection methods evaluated number different term removal thresholds 
high threshold possible terms document threshold 
avoid removing terms document added meta rule process 
apply threshold document results nonempty document apply closest threshold results non empty document 
smart system unified preprocessing followed feature selection includes word stemming weighting 
tried term weighting options ltc atc lnc bnn smart notation combine term frequency tf measure inverted document frequency idf measure variety ways 
best results ltc reported section 
primary results displays performance curves knn reuters training documents test documents term selection ig df ts mi chi thresholding respectively 
tested options avg max mi chi better results represented 
displays performance curves llsf reuters 
training part llsf resource consuming approximation llsf number features unique words 
average precision knn vs unique word count ig df ts chi max mi max number features unique words 
average precision llsf vs unique word count ig df ts chi max mi max complete solution save computation resources experiments 
computed largest singular values solving llsf best results similar performance knn appeared singular values 
simplification llsf invalidate examination feature selection focus experiments 
observation merges categorization results knn llsf reuters 
ig df chi thresholding similar effects performance classifiers 
eliminate unique terms improvement loss categorization accuracy measured average precision 
ig thresholding example vocabulary reduced terms reduction avgp knn improved 
chi better categorization results extremely aggressive thresholds ig better 
ts comparable performance term removal knn term removal llsf 
df 
correlation df ig values words reuters df ig plots df 
correlation df chi values words reuters df chi plots aggressive thresholds performance declines faster ig chi df 
mi comparable performance methods 
correlations df ig chi similar performance ig df chi term selection surprising observation previously reported 
natural question corpus statistics correlated 
plots values df ig term reuters collection 
plots values df chi avg correspondingly 
clearly strong correlations df ig chi values term 
figures shows results cross collection examination 
strong correlation df ig observed ohsumed collection 
performance curves knn df versus ig identical collection 
observations reuters ohsumed highly consistent 
different application domains df 
df ig correlation ohsumed df ig plots percentage features unique words 
feature selection performance knn collections ig reuters df reuters ig ohsumed df ohsumed corpus convinced observed effects df ig thresholding general corpus dependent 
discussion reasons bad performance feature selection text categorization tasks 
table compares criteria angles ffl favoring common terms rare terms ffl task sensitive category information task free ffl term absence predict category probability cell contingency table ffl performance knn llsf reuters ohsumed 
table methods excellent performance share bias scoring favor common terms rare terms 
bias obviously df criterion 
necessarily true ig chi definition theory common term zero valued information gain score 
statistically true strong correlations df ig chi values 
mi method opposite bias seen formula log gamma log 
bias extreme near zero 
ts clear bias sense common rare terms high strength 
excellent performance df ig chi indicates common terms informative text categorization tasks 
significant amounts information lost high levels vocabulary reduction possible knn llsf improved categorization 
precise theory ig measures number bits information obtained knowing presence absence term document 
strong df correlations means common terms informative vice versa statement course extend words 
contrary widely held belief information retrieval common terms non informative 
experiments show assumption may apply text categorization 
interesting point table category information feature selection crucial excellent performance 
df task free category information training set performance similar ig chi task sensitive 
mi task sensitive significantly performs ts df task free 
poor performance mi informative 
bias low frequency terms known section theoretical weakness cause significant accuracy loss text categorization empirically examined 
experiments quantitatively address issue cross method comparison cross classifier validation 
bias mi serious problem sensitivity probability estimation errors 
second term formula log gamma log score extremely sensitive estimation errors near zero 
theoretical interest worth analyzing difference information gain mutual information 
ig proven equivalent ft tg fc log table 
criteria performance feature selection methods knn llsf method df ig chi mi ts favoring common terms categories term absence performance knn llsf excellent excellent excellent poor ok formulas show information gain weighted average mutual information weights joint probabilities respectively 
information gain called average mutual information 
fundamental differences ig mi ig information term absence form mi ignores information ig normalizes mutual information scores joint probabilities mi uses scores 
evaluation feature selection methods dimensionality reduction text categorization reduction levels aggressiveness full vocabulary words feature space removing unique terms 
ig chi effective aggressive term removal losing categorization accuracy experiments knn llsf 
df thresholding comparable performance ig chi term removal ts comparable term removal 
mutual information inferior performance compared methods due bias favoring rare terms strong sensitivity probability estimation errors 
discovered df ig chi scores term strongly correlated revealing previously unknown fact importance common terms text categorization 
suggests df thresholding just ad hoc approach improve efficiency assumed literature text categorization retrieval reliable measure informative features 
ig chi computation quadratic measures expensive 
availability simple effective means aggressive feature space reduction may significantly ease application powerful computationally intensive learning methods neural networks large text categorization problems intractable 
jaime carbonell ralf brown cmu pointing implementation error computation information gain providing correct code 
tom mitchell machine learning group cmu fruitful discussions clarify definitions information gain mutual information literature 
apte damerau weiss :10.1.1.39.6139
language independent automated learning text categorization models 
proceedings th annual acm sigir conference 
kenneth ward church patrick hanks 
word association norms mutual information lexicography 
proceedings acl pages vancouver canada 
william cohen yoram singer 
contextsensitive learning text categorization 
sigir proceedings th annual international acm sigir conference research development information retrieval 

masand smith waltz 
trading mips memory knowledge engineering classifying census returns connection machine 
comm 
acm 
deerwester dumais furnas landauer harshman 
indexing latent semantic analysis 
amer soc inf sci pages 
dunning 
accurate methods statistics surprise coincidence 
computational linguistics volume pages 
fano 
transmission information 
mit press cambridge ma 
fuhr lustig tzeras 
air rule multistage indexing systems large subject fields 
editor proceedings riao 
hersh buckley leone 
ohsumed interactive retrieval evaluation new large text collection research 
th ann int acm sigir conference research development information retrieval sigir pages 
koller sahami 
optimal feature selection 
proceedings thirteenth international conference machine learning 
lang 
newsweeder learning filter netnews 
proceedings twelfth international conference machine learning 
david lewis robert schapire james callan ron papka 
training algorithms linear text classifiers 
sigir proceedings th annual international acm sigir conference research development information retrieval 

lewis ringuette 
comparison learning algorithms text categorization 
proceedings third annual symposium document analysis information retrieval sdair 
tom mitchell 
machine learning 
hill 
moulinier 
learning bias issue text categorization problem 
technical report lip universite paris vi page appear 
moulinier ganascia 
text categorization symbolic approach 
proceedings fifth annual symposium document analysis information retrieval 
quinlan 
induction decision trees 
machine learning 
salton 
automatic text processing transformation analysis retrieval information computer 
addison wesley reading pennsylvania 
schutze hull pedersen 
comparison classifiers document representations routing problem 
th ann int acm sigir conference research development information retrieval sigir pages 
tzeras hartman 
automatic indexing bayesian inference networks 
proc th ann int acm sigir conference research development information retrieval sigir pages 
wiener pedersen weigend :10.1.1.54.6608
neural network approach topic spotting 
proceedings fourth annual symposium document analysis information retrieval sdair 
wilbur 
automatic identification words 
inf 
sci 
yang 
expert network effective efficient learning human decisions text categorization retrieval 
th ann int acm sigir conference research development information retrieval sigir pages 
yang 
noise reduction statistical approach text categorization 
proceedings th ann int acm sigir conference research development information retrieval sigir pages 
yang 
sampling strategies learning efficiency text categorization 
aaai spring symposium machine learning information access pages 
yang 
evaluation statistical approach text categorization 
technical report cmu cs computer science department carnegie mellon university 
yang chute 
example mapping method text categorization retrieval 
acm transaction information systems tois pages 
yang wilbur 
corpus statistics remove redundant words text categorization 
amer soc inf sci 
