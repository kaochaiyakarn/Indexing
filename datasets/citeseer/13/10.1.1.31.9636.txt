approximate dimension equalization vector information retrieval fan jiang fan cs duke edu department computer science duke university durham nc usa michael littman cs duke edu labs research florham park nj usa department computer science duke university durham nc usa vector information retrieval methods vector space model vsm latent semantic indexing lsi generalized vector space model gvsm represent queries documents high dimensional vectors learned analyzing training corpus text 
vsm scales large collections represent term term correlations prevents translingual retrieval 
gvsm lsi represent term term correlations scale large retrieval collections 
novel method call approximate dimension equalization ade combines ideas vsm lsi gvsm produce method performs large collections scales computationally represent term term correlations 
compare performance ade methods large small collections monolingual bilingual text 
ade outperforms methods large bilingual collections performs close best cases 

basic vector space methods information retrieval ir vector space model vsm latent semantic indexing lsi generalized vector space model gvsm 
unique strengths 
vsm simple scales extremely gives excellent performance large text collections 
lsi extends vsm learning reduced dimensional representation models term term associations allowing query positive similarity document shares terms 
especially important cross language ir applications 
lsi shown impressive performance text collections performance extremely large diverse text collections lagged vsm 
evidence large number dimensions needed large text collections 
gvsm models term term associations scales easily lsi 
unfortunately performance lags substantially vsm lsi 
argue gvsm uses dimensions lsi puts tremendous weight largest ones resulting effective dimensionality substantially smaller 
show combine ideas vsm lsi gvsm learn vector representation call approximate dimension equalization ade scales supports term term associations 
large monolingual test collections performance approaches vsm 
large cross language collections vsm applied performance substantial improvement gvsm lsi 

vector information retrieval information retrieval ir task locating information items documents relevant user query collection 
cross language translingual information retrieval problem selecting useful documents collections may contain languages 
vector information retrieval methods represent queries documents high dimensional vectors compute similarity vector inner product 
vectors normalized unit length inner product equal measuring cosine angle vectors vector space 
components vectors term weights vsm developed salton salton transformed space dimensions bear new meanings example gvsm wong lsi deerwester :10.1.1.108.8490
mathematically vector ir methods lated way simp delta pp simp represents similarity document query vector representation document vector representation document transformation matrix 
vector equation usually obtained way document collection indexed term document matrix ij thetan ij weight th term th document size vocabulary number documents corpus 
th column matrix dimensional vector term weights 
user query represented similar way 
conventional vsm transformation matrix simply theta identity matrix delta criticism vsm treats terms unrelated occupy orthogonal dimensions semantic space 
nonzero similarity score vectors result nonzero values dimension original documents contain word common 
certainly far ideal example query contains search word computer digital document uses exclusively word digital deemed relevant document talks gardening obvious exact term matching scheme vsm suited cross language information retrieval 
techniques address problem training corpus derive statistical information term term correlations related terms added original query enrich retrieval 
ideally training corpus thesaurus lists word entry related words strong relationships 
extreme thesaurus contain word entries followed related words exactly analogous conventional vsm 
cross language information retrieval bilingual corpus training corresponding documents collections translations related subjects 
generalized vector space model gvsm wong information retrieval known dual space approach sheridan method captures term term correlations documents matching document pairs case query expansion techniques global analysis local feedback deal word mismatch problem vsm xu croft 
cross language retrieval occur 
monolingual retrieval thetan term document matrix training corpus 
matching vector elements rows terms represent transforms new vector elements correspond documents training corpus 
query vector transformed way 
query document similarity measured transformed vectors sim gvsm ml delta aa theta matrix aa nonzero value row column document contains th th terms 
extension gvsm translingual ir proposed yang yang 
bilingual corpus training matrices formed term document matrix training documents language retrieval documents term document matrix training documents language queries 
number unique terms languages different number training documents represented corresponding columns document transformed query compute inner product sim gvsm tl delta ab latent semantic indexing lsi deerwester vector method takes idea training corpora step :10.1.1.108.8490
term document training matrix uses singular value decomposition svd golub van loan factor parts sigmav diag oe oe oe unitary matrices columns left right singular vectors sigma diagonal matrix diagonal elements non negative arranged descending order rank values oe oe known singular values square roots eigenvalues aa columns correspond largest singular values transformation matrix lsi sim lsi ml delta ui identity matrix diagonal elements nonzero 
say performed dimension reduction 
columns transformation matrix amounts orthogonal projection queries documents row space training matrix 
way lsi learns structure training materials uses making new judgments query document relatedness 

dimension equalization obtain lsi similarity formula equation different perspective terms special form original matrix define reduced dimensional form sigma diag oe oe oe closest rank matrix frobenius norm defined sum squares element wise differences golub van loan 
define operation dimension equalization giving columns unit equal weight 
result operation diag uiv uv closest unitary matrix reduced dimensional form ui diag derivation see transformation matrix lsi delta delta ui ui sim lsi ml lsi translingual retrieval simply substitute query side sim lsi tl delta note formulation different original translingual lsi formulation landauer littman landauer littman 
crosslanguage lsi formula includes single svd matrix combines aligned documents training corpora single documents uab sigma ab vectors extended zeros cover terms language 
compared monolingual translingual gvsm delta delta vsm delta delta lsi delta delta table 
basic vector information retrieval methods terms training matrix represents parallel training corpus translingual case 
traditional translingual lsi new approach computes separate svds smaller matrices 
certainly useful combined matrix large analyze svd 
number documents training collection equation dimension reduction new translingual lsi special form procrustes method littman 
shown equations gvsm lsi forms training matrix vsm expressed similar fashion 
view vsm retrieval corpus directly training 
words term document matrix retrieval collection 
transformation matrix delta delta uv uu equality holds column projection column space uu 
methods expressed normal special form summarized table 
note formula give translingual vsm exactly vsm lsi vsm parallel corpus training retrieval case trivial extension monolingual method retrieval task 

comparison singular values singular values training matrix formulae gvsm vsm table give values 
similarly lsi vsm dimensionality chosen equal rank differences observe vsm gvsm lsi practice depend distribution singular values training matrix 
shows singular values small english test documents yang yang 
collection indexed singular value number 
plot singular values english test collection documents 
smart ntc weighting scheme complete spectrum singular values easily matrix 
characteristic plot possesses called low rank plus shift structure reported zha zhang study lsi zha zhang singular values relatively large decreasing sharply leveling noticeably part middle 
plotted singular values number corpora varying size indexing scheme special property 
matrix size gets large compute complete spectrum current software see trend initial dropping leveling singular values 
plotted singular values trec french collection documents okapi term weighting robertson 
imply behavior various ir methods 
transformation matrices appear table left right singular vectors differ singular values sigma 
depicts singular values transformation matrices fourth explained section 
extrapolating studies smaller collections predict ideal singular value plot transformation matrix large collection flat value full rank training matrix larger computed current software 
depicted dotted line 
vsm gvsm lsi match singular values idealized transformation matrix singular value number 
plot singular values trec french corpus documents 
lemma useful judging gives best approximation 
lemma monolingual training corpus singular value decomposition sigmav transformation matrices share left right singular vectors compare matrices similarities obtained comparing pairs documents simp frobenius norm simp gamma equal frobenius norm sigma phi gamma psi proof jj delta jj represent frobenius norm sum squares matrix components trace delta represent trace sum diagonal elements matrix gamma jj jja pp gamma qq ajj jjv sigmau sigmav gamma qq ajj jjv sigma phi gamma sigma psi jj jjv sigma phi gamma psi jj trace sigma phi gamma psi sigma phi gamma psi trace sigma phi gamma psi trace sigma phi gamma psi jj sigma phi gamma psi jj tells closer transformation matrices singular values closer similarities produce 
dimensions matter lsi ade vsm gvsm 
weights orthogonal dimensions semantic space different methods 

dimension weights ak form ak ones correspond largest singular values training matrix expect vsm produce accurate similarity scores followed lsi accurate dimensions highest singular values 
vsm directly translingual retrieval 
section develop novel approximation idealized transformation matrix 

approximate dimension equalization define oe gamma oe left right singular vectors consists equalized dimensions remaining gamma dimensions normalized oe illustrated 
matrix takes advantage special shape singular value plots term documents seen flattens large singular values attaches rest real singular values relatively level long middle portion small tail 
way obtain relatively equalized dimensions monolingual translingual gvsm delta delta vsm delta delta lsi delta delta ade delta delta table 
unified view vector methods information retrieval 
training matrix close dimensions 
monolingual retrieval simply calculate sim ade ml delta translingual retrieval replace front sim ade tl delta call approach approximate dimension equalization ade 
dimension weights ade method plotted clear ade approximates ideal singular values better lsi gvsm 
perspective ade trying extend limited ability lsi compute singular vectors values large training matrix implicitly adding additional ones relatively equal weights 
ade cross language vsm possible obtaining dimensions training matrices equalization 
third uses gvsm approach scalably capturing term term correlations modified prevent handful dimensions 
computationally ade needs compute dimensions training matrix different dimensional lsi 
practical application dense matrix directly formed 
compute forming transformed query vector step oe gamma oe oe gamma oe sigma note intermediate result matrix bigger size number queries number documents terms whichever bigger 
process queries batches tens hundreds matrices large 
sim ade ml delta computed similar way 
cranfield trec ap gvsm vsm lsi ade table 
monolingual average precision results cranfield trec ap collections 

results experiments tested method various document collections different size different languages 
traditional tf theta idf okapi weighting schemes results whichever weighting scheme works better conventional vsm 
table shows results monolingual retrieval different collections cranfield trec ap 
numbers table average precision score queries tested standard ir measure 
cranfield corpus classic ir test collection consists documents test queries 
collection demonstrates ability lsi derive term term correlations reduced dimensions semantic space 
lsi score achieved dimensions full rank 
ade near best score obtained dimensions confirming approximates lsi fewer dimensions 
document trec ap collection subset trec ap collection 
size large compute complete svd term document matrix 
collection singular triplets calculated total number dimensions 
lsi obtain moderate score compared vsm available dimensions 
ade partially bridges gap lsi vsm achieving results close vsm number dimensions lsi 
predicted gvsm behaves low dimensional lsi collections 
second set results listed table shows vector methods monolingual translingual retrieval complete svd computable 
experiments done collection created described yang yang 
training documents addition test documents english spanish documents aligned sets 
table means training done test collection possible monolingual retrieval means 
complete svd computed tested lsi ade different dimensionalities best possible results monolingual translingual gvsm vsm lsi ade table 
results cmu english spanish collection 
monolingual translingual fre ger fre ger ger fre gvsm vsm lsi ade table 
results trec french german corpora trec cross language topics 
able full dimensional cross language lsi cross language vsm discussed earlier 
see lsi gets best retrieval scores reduced dimensions average precision scores row lsi ade achieves similar results lower dimensions 
size retrieval collection large finding complete svd training matrix limitation current computing resource 
longer try various dimensionalities find ideal retrieval 
get ends spectrum hundreds thousands singular triplets computed currently available hardware software full set dimensions implicitly vsm ideal dimensionality seek probably middle 
cases vsm probably method compute dimensions 
comes translingual retrieval vsm longer applicable ade comes handy 
case illustrated table results large trec french german collections documents respectively 
training documents subset roughly aligned ones collection 
computed maximum dimensions little full dimensionality lsi ade lagging vsm monolingual retrieval ade improving lsi 
vsm impossible case cross language retrieval ade obtain decent results territory 
fact results compare favorably best results obtained collection franz 

shows traditional vector methods information retrieval vsm lsi gvsm viewed belonging family algorithms differ singular values transformation matrix 
ignoring computational concerns lsi desirable method vsm projects queries documents row space training corpus vsm takes advantage term term correlations 
training collection size increases number dimensions required lsi computational concerns dominate 
approximate dimension equalization ade new method combines ideas vsm lsi gvsm approximate ideal transformation matrix accurately available computational resources 
results show ade improve lsi monolingual translingual information retrieval 
deerwester deerwester dumais furnas landauer harshman :10.1.1.108.8490

indexing latent semantic analysis 
journal american society information science 
franz franz roukos 

ad hoc multilingual information retrieval ibm 
voorhees harman editors proceedings seventh text retrieval conference trec 
department commerce national institute standards technology 
golub van loan golub van loan 

matrix computations 
johns hopkins university press baltimore maryland nd edition 
landauer littman landauer littman 

fully automatic cross language document retrieval latent semantic indexing 
proceedings sixth annual conference uw centre new oxford english dictionary text research pages 
uw centre new oed text research waterloo ontario 
littman littman jiang keim 

learning language independent representation terms partially aligned corpus 
shavlik editor proceedings fifteenth international conference machine learning pages 
morgan kaufmann 
littman dumais landauer 

automatic language cross language information retrieval latent semantic indexing 
sixth text retrieval conference notebook papers trec pages 
national institute standards technology special publication 
robertson robertson walker jones hancock beaulieu 

okapi trec 
harman editor third text retrieval conference trec pages 
national institute standards technology special publication 
salton salton wang yang 

vector space model information retrieval 
journal american society information science 
sheridan sheridan 

experiments multilingual information retrieval spider system 
frei harman schauble wilkinson editors proceedings th annual international acm sigir conference research development information retrieval new york 
association computing machinery 
wong wong wong 

generalized vector space model information retrieval 
acm sigir conference research development information retrieval sigir pages 
xu croft xu croft 

query expansion local global document analysis 
proceedings th annual international acm sigir conference research development information retrieval sigir pages zurich switzerland 
yang yang brown carbonell lee 

bilingual corpus approaches translingual information retrieval 
nd workshop multilinguality software industry ai contribution 
zha zhang zha zhang 

matrices low rank plus shift structures partial svd latent semantic indexing 
technical report cse pennsylvania state university university park pa 
