switching state space models zoubin ghahramani geoffrey hinton department computer science university toronto king college road toronto canada email zoubin cs toronto edu introduce statistical model times series data nonlinear dynamics iteratively segments data regimes approximately linear dynamics learns parameters regimes 
model combines generalizes widely stochastic time series models hidden markov model linear dynamical system related models widely control econometrics literatures 
derived extending mixture experts neural network model jacobs fully dynamical version expert gating networks recurrent 
inferring posterior probabilities hidden states model computationally intractable exact expectation maximization em applied 
variational approximation maximizes lower bound log likelihood forward backward recursions hidden markov models kalman filter recursions linear dynamical systems 
commonly probabilistic models time series draw lineage hidden markov model hmm stochastic linear dynamical system known state space model ssm 
hidden markov models represent information past sequence single discrete random variable hidden state 
probability distribution state function previous state represented stochastic transition matrix 
knowing state time renders past observations statistically independent 
markov independence property gives model name 
state space model represent information past real valued hidden state vector 
conditioned state vector past observations rendered independent 
dependency state vector previous state vector specified dynamic equations system noise model 
equations linear noise model gaussian state space model known linear dynamical system kalman filter model 
unfortunately real world processes characterized purely discrete purely linear gaussian dynamics 
example industrial plant may multiple discrete modes behavior appropriately described linear dynamics 
similarly pixel intensities image translating object vary linear dynamics subpixel translations image moves larger range dynamics change significantly nonlinearly 
goal model complex dynamical phenomena may characterized discrete continuous dynamics 
effect introduce probabilistic model called switching state space model inspired divide conquer principle underlying mixture experts neural network jacobs 
switching statespace model natural generalization hidden markov model state space model dynamics transition discrete manner linear operating regime 
face large literature models kind econometrics signal processing fields harrison stevens chang athans hamilton shumway stoffer bar shalom li 
extend models allow multiple real valued state vectors draw connections fields literature neural computation derive learning algorithm parameters system structured variational approximation rigorously maximizes lower bound log likelihood model 
organized follows 
section review background material state space models hidden markov models hybrids 
section describe generative model probability distribution defined observation sequences switching state space models 
section describe learning algorithm switching state space models structured variational approximation em algorithm 
section simulation results artificial domain assess quality approximate inference method natural domain 
conclude section 
background state space models state space model sequence dimensional observation vectors fy discrete time index ranges modeled specifying probabilistic relation observations hidden state vector probabilistic relation consecutive hidden state vectors 
hidden state vectors obey markov independence property joint probability sequences states observations factored fx jx jx gamma jx conditional independences specified equation expressed graphically form 
simplest commonly models kind assume table describing variables notation provided appendix directed acyclic graph dag specifying conditional independence relations state space model 
node conditionally independent non descendents parents output conditionally independent variables state conditionally independent gamma gamma transition output functions linear time invariant distribution state observation variables multivariate gaussian 
term statespace model refer simple form model 
models state transition function written ax gamma state transition matrix zero mean gaussian state noise 
equation ensures gamma gaussian 
assumed gaussian 
output function written cx output matrix zero mean gaussian output noise covariance matrix jx gaussian jx gammad jrj gamma exp ae gamma gamma cx gamma gamma cx oe observation vector divided input predictor variables output response variables 
model input output behavior system conditional probability output sequences input sequences linear gaussian ssm modified state transition function ax gamma bu input observation vector fixed input matrix 
problem inference state space model known parameters consists estimating posterior probabilities hidden variables sequence observed variables 
local likelihood functions observations gaussian priors hidden states gaussian resulting posterior gaussian 
special cases inference problem state space models play prominent role engineering literature filtering smoothing prediction anderson moore goodwin sin 
goal filtering compute probability current hidden state sequence inputs outputs time fug 
recursive algorithm perform computation known kalman filter kalman define state ax bu notation fy short hand sequence bucy 
goal smoothing compute probability sequence inputs outputs time kalman filter recursions forward direction compute probability fy fug similar set backward recursions complete computation accounting observations time rauch 
refer combined forward backward recursions smoothing kalman smoothing recursions known rts rauch smoother 
goal prediction compute probability states observations observations upto time fug computed model simulated forward direction equations inputs compute probability density state output time problem learning parameters state space model known engineering system identification problem general form assumes access sequences input output observations 
focus maximum likelihood learning single locally optimal value parameters estimated bayesian approaches treat parameters random variables compute approximate posterior distribution parameters data 
distinguish line line approaches learning 
line recursive algorithms favored real time adaptive control applications obtained computing gradient second derivatives log likelihood ljung 
similar gradient methods obtained line methods 
alternative method line learning expectation maximization em algorithm dempster 
procedure iterates step fixes current parameters computes posterior probabilities hidden states observations step step uses probabilities maximize expected log likelihood parameters step 
linear gaussian state space models step exactly kalman smoothing problem defined step simplifies linear regression problem shumway stoffer digalakis 
details em algorithm state space models ghahramani hinton original shumway stoffer 
worth pointing linear gaussian state space model generalization statistical method known factor analysis 
factor analysis models high dimensional data smaller number latent variables factors everitt 
model relating factors observations exactly specified equation gaussian distributed vector factor values observation vector known factor loading matrix zero mean gaussian distributed noise constraint elements vector uncorrelated 
state space models dynamic generalization factor analysis allow current factor values depend linearly previous factor values 
fewer dimensions factor analysis problem posed 
statespace model equivalent constraint dimensionality product dimension length observation sequence 
constraint derives notion observability linear system theory goodwin sin 
hidden markov models hidden markov models defines probability distribution sequences observations fy distribution sequences obtained specifying probability observations time step discrete hidden state probability transitioning hidden state time step 
markov property joint probability sequences states observation factored exactly manner equation place fs js js gamma js similarly conditional independences hmm expressed graphically form 
state represented single multinomial variable take discrete values kg 
state transition probabilities js gamma specified theta transition matrix 
observables discrete symbols values observation probabilities js fully specified theta observation matrix 
continuous observation vector js modeled different forms gaussian mixture gaussians neural network 
hmms applied extensively problems speech recognition juang rabiner computational biology baldi fault detection smyth 
hmm known parameters sequence observations algorithms commonly solve different forms inference problem rabiner juang 
computes posterior probabilities hidden states recursive algorithm known forward backward algorithm 
computations forward pass exactly analogous kalman filter ssms computations backward pass analogous backward pass kalman smoothing equations 
noted bridle personal communication smyth heckerman jordan forward backward algorithm special case exact inference algorithms general graphical probabilistic models lauritzen spiegelhalter pearl 
observation holds true kalman smoothing recursions 
inference problem commonly posed hmms compute single sequence hidden states 
solution problem viterbi algorithm consists forward backward pass model 
learn maximum likelihood parameters hmm sequences observations known baum welch algorithm baum 
algorithm special case em uses forward backward algorithm infer posterior probabilities hidden states step 
step uses expected counts transitions observations re estimate transition output matrices linear regression equations case observations gaussian distributed 
hmm augmented allow input variables models conditional distribution sequences output observations sequences inputs cacciatore nowlan bengio frasconi meila jordan 
approach bengio frasconi input output hmms iohmms suggests modeling js gamma input separate neural networks setting gamma de composition ensures sum constraint output networks valid probability transition matrix defined point input space 
hybrids time series applications linear gaussian dynamics ssm purely discrete dynamics hmm appropriately model temporal structure data 
consequence fields ranging econometrics control engineering burgeoning literature developed models combine discrete transition structure hmms linear dynamics ssms harrison stevens chang athans hamilton shumway stoffer bar shalom li deng 
models known alternately hybrid models state space models switching jump linear systems 
briefly review main results literature including proposals field neural computation 
engineering literature state estimation state space models switching reviewed bar shalom li 
state estimation problem consists computing mean covariance hidden real valued state vector observations problem 
shortly kalman results linear gaussian state space models attention turned problem state estimation switching parameters 
example fu consider problem state estimation linear state space models receive unobserved state output disturbances coming gaussian mixture distributions markov transition structure 
chang athans derive equations computing conditional mean variance state parameters linear state space model switch arbitrary markovian dynamics 
prior transition probabilities switching process assumed known 
note models sets parameters observation length exact conditional distribution state gaussian mixture components 
conditional mean variance require far computation summary statistics 
shumway stoffer consider problem learning parameters statespace models single real valued hidden state vector switching output matrices 
probability chosing particular output matrix pre specified time varying function independent previous choices 
pseudo em algorithm derived step exact form require computing gaussian mixture components approximated single gaussian time step 
kim extends case state dynamics output matrices switch switching follows markovian dynamics 
kim uses approximation exponential gaussian mixture collapsed gaussians time step 
authors markov chain monte carlo methods state parameter estimation switching models carter kohn general dynamic probabilistic networks dean kanazawa kanazawa 
model nonlinear processes nonlinear generalizations state space model explicitly representing switching state 
conditional mean variance hidden states estimated extended kalman filter ekf goodwin sin 
time step ekf linearizes system dynamics current state estimate uses resulting kalman filter estimate state 
approaches hybrid models approach approximates non gaussian state distribution time step gaussian 
ekf approaches address difficult problems computer vision simultaneous recognition pose estimation rao ballard 
gaussian approximation improved representing non gaussian state distributions set samples stochastically propagated reweighted 
approach successfully applied problem contour tracking computer vision isard blake blake 
explored ekf deriving em algorithm general stochastic nonlinear dynamical systems ghahramani roweis preparation 
switching state space models viewed nonlinear fixed number linearizations fit dynamics 
maximum likelihood criterion learning algorithm iterates selecting points linearization costly fitting linear model 
related proposal comes fraser combine realvalued discrete states system call hidden filter hmm 
simplification authors assume real valued state known deterministic function past observations embedding 
departs significantly fraser ways 
state space embedding learned fixed priori 
second real valued state vector assumed random variable allowing propagation uncertainty state 
real valued state vectors simultaneously representing hypothesis competing explain observations 
regard literature neural computation model generalization mixtures experts architecture jacobs jordan jacobs 
previous dynamical generalizations mixture experts architecture consider case gating network markovian dynamics cacciatore nowlan meila jordan 
limitation generalization entire past sequence summarized value single discrete variable gating activation system experts convey average log bits information past 
generalization consider experts gating network markovian dynamics 
past summarized state composed cross product discrete variable combined real valued state space experts 
provides wider information channel past 
advantage representation real valued state contain componential structure 
attributes position orientation scale object image naturally encoded independent real valued variables accommodated state exponential growth required discrete hmm representations 
proceed definition probabilistic model important place seen generalization mixtures factor analyzers hinton ghahramani hinton 
context literature just reviewed 
hybrid models state space switching jump linear systems assume single real valued state vector 
model considered generalizes multiple real valued state vectors 
learning algorithm parameters model including markov switching parameters 
structured variational approximation saul jordan show algorithm maximizes strict lower bound log likelihood data heuristically motivated pseudo likelihood 
resulting algorithm simple intuitive flavor decouples forward backward recursions hidden markov model kalman smoothing recursions state space model 
states hmm determine soft assignment observation state space model prediction errors state space models determine observation probabilities hmm 
generative model switching state space models sequence observations fy modeled specifying probabilistic relation observations hidden state space comprising real valued state vectors discrete state vector discrete state modeled multinomial variable take values mg reasons obvious refer switch variable 
joint probability observations hidden states factored fs js gamma delta jx gamma delta jx corresponds graphically conditional independences represented 
conditioned setting switch state observable multivariate gaussian output equation state space model probability observation vector jx gamma jrj gamma exp ae gamma gamma gamma gamma oe dimension observation vector observation noise covariance matrix output matrix state space model cf 
equation single linear gaussian state space model 
real valued state vector evolves linear gaussian dynamics state space model differing initial state transition matrix state noise equation 
switch state evolves discrete markov transition structure specified initial state probabilities theta state transition matrix js gamma 
note state vectors concatenated large state vector factorized transition matrices cf 
factorial hidden markov model ghahramani jordan 
obscures decoupled structure model 
graphical model representation switching state space models 
switch variable real valued state vectors 
switching state space model depicted generalization mixture experts 
light arrows correspond connections mixture experts 
switching state space model states experts gating network depend previous states dark arrows 
exact analogy mixture experts architecture modular learning neural networks jacobs 
state space model linear expert gaussian output noise model linear gaussian dynamics 
switch state gates outputs state space models plays role gating network markovian dynamics 
wish consider straightforward extensions model ex differing output covariances state space model ex differing output means state space model model allowed capture observations different operating range ex conditioning sequence observed input vectors fu extensions possible 
learning efficient learning algorithm parameters switching state space model derived generalizing expectation maximization em algorithm baum dempster 
em alternates optimizing distribution hidden states step optimizing parameters distribution hidden states step 
distribution hidden states fs combined state state space models define lower bound log probability observed data log fy gj log fs fs gj dfx log fs fs fs gj fs dfx fs fs log fs gj fs dfx denotes parameters model jensen inequality cover thomas establish 
steps em increase lower bound log probability observed data 
step holds parameters fixed sets posterior distribution hidden states parameters fs fs maximizes respect distribution turning lower bound equality 
step holds distribution fixed computes parameters maximize distribution 
log fy gj start step step affect log steps combined decrease log change parameters produced step distribution produced previous step typically longer optimal procedure iterated 
unfortunately exact step switching state space models intractable 
related hybrid models described section posterior probability real valued states gaussian mixture terms 
seen semantics directed graphs particular separation criterion pearl implies hidden state variables marginally independent conditionally dependent observation sequence 
induced dependency effectively couples real valued hidden state variables discrete switch variable consequence exact posteriors gaussian mixtures exponential number terms 
order derive efficient learning algorithm system relax em algorithm approximating posterior probability hidden states 
basic idea expectations respect intractable setting step tractable distribution approximate results em learning algorithm maximizes lower bound log likelihood 
difference bound log likelihood kullback liebler kl divergence cover thomas kl qkp fs fs log fs fs dfx complexity exact inference approximation determined conditional independence relations parameters choose tractable structure graphical representation eliminates dependencies structure parameters varied obtain tightest possible bound minimizing 
algorithm alternates optimizing parameters intractability step smoothing problem simpler single state switching model noted engineering literature chang athans bar shalom li 
distribution minimize step optimizing parameters distribution hidden states step 
exact em steps increase lower bound log likelihood equality reached step 
refer general strategy parameterized approximating distribution variational approximation refer free parameters distribution variational parameters 
completely factorized approximation statistical physics provides basis simple powerful mean field approximations statistical mechanical systems parisi 
theoretical arguments motivating approximate steps neal hinton 
saul jordan showed approximate steps maximize lower bound log likelihood proposed powerful technique structured variational approximations intractable probabilistic networks 
key insight saul jordan judicious approximation exact inference algorithms tractable substructures intractable network 
general tutorial variational approximations jordan 

parameters switching state space model fa phig state dynamics matrix model output matrix state noise covariance mean initial state covariance initial state tied output noise covariance prior discrete markov process phi js gamma 
inclusion extensions ex ex result substituting adding means input matrices possible approximations posterior distribution hidden variables learning inference switching state space models focus fs zq gamma gamma non negative potential functions define soon zq normalization constant ensuring integrates 
written terms potential functions conditional probabilities corresponds simple graphical model shown 
terms involving switch variables define discrete markov chain terms involving state vectors define uncoupled state space models 
mean field approximations approximated stochastically coupled system removing couplings original system 
specifically removed stochastic coupling chains results fact observation time depends hidden variables time retain coupling hidden variables successive time steps couplings handled exactly forward backward kalman smoothing recursions 
approximation structured sense variables uncoupled 
discrete switching process defined gamma gamma graphical model representation structured variational approximation posterior distribution hidden states switching state space model 
variational parameters distribution 
parameters scale probabilities states switch variable time step plays exactly role observation probability js play regular hidden markov model 
soon see minimizing kl qkp results equation supports intuition 
uncoupled state space models approximation defined potential functions related probabilities original system 
easier express log potentials log log log jx log gamma log jx gamma log jx variational parameters vector plays role similar switch variable component range 
posterior probability depend observation time posterior probability includes term assumes state space model generated responsibility assigned state space model observation vector difference deterministic parameter stochastic random variable 
maximize lower bound log likelihood kl qkp minimized respect variational parameters separately sequence observations 
definition switching state space model equation approximating distribution minimum kl satisfies fixed point equations variational parameters see appendix exp ae gamma gamma gamma gamma aeoe deltai denotes expectation distribution 
intuitively ffl responsibility assigned state space model observation vector ffl function expected squared error state space model generate compute necessary sum variables including done efficiently forward backward algorithm switch state variables playing exactly role observation probability associated setting switch variable 
related prediction error model data intuitive interpretation switch state associated models smaller expected prediction error particular observation favored time step 
forward backward algorithm ensures final responsibilities models obtained considering entire sequence observations 
compute necessary calculate expectations exp ae gamma gamma gamma hx gamma tr gamma hx oe tr matrix trace operator tr ab tr ba 
done efficiently kalman smoothing algorithm state space model model time data weighted responsibilities parameters depend parameters vice versa process iterated iteration involves calls forward backward kalman smoothing algorithms 
learning algorithm switching state space models structured variational approximation summarized 
deterministic annealing kl divergence minimized step variational em algorithm multiple minima general 
way visualize minima consider space possible segmentations observation sequence length segmentation mean discrete partition sequence state space models 
ssms possible segmentations sequence 
segmentation inferring optimal distribution real valued states ssms convex optimization problem real valued states conditionally gaussian 
difficulty kl minimization lies trying find best soft partition data 
combinatorial optimization problems possibility getting trapped local minima reduced gradually annealing cost function 
employ deterministic variant annealing idea making simple modifications variational fixed point equations weighting data equivalent running kalman smoother unweighted data time varying observation noise covariance matrix 
initialize parameters model 
repeat bound log likelihood converged step repeat convergence kl qkp compute prediction error state space model observation compute forward backward algorithm hmm observation probabilities run kalman smoothing recursions data weighted step re estimate parameters state space model data weighted re estimate parameters switching process baum welch update equations learning algorithm switching state space models 
exp ae gamma gamma gamma gamma aeoe temperature parameter initialized large value gradually reduced 
equations maximize modified form bound entropy multiplied ueda nakano 
simulations experiment variational segmentation deterministic annealing goal experiment assess quality solutions variational inference algorithm effect deterministic annealing solutions 
generated sequences length simple model switched ssms 
ssms switching process defined gamma gamma switch state chosen priors transition probabilities phi phi phi phi 
sequences data set shown true state switch variable 
data sequences length true segmentations 
segmentations switch states represented dark light dots respectively 
notice correctly segmenting sequences prior knowledge dynamics processes difficult 
sequence initialized inference algorithms equal responsibilities ssms ran algorithms iterations 
non annealed inference algorithm ran fixed temperature annealed algorithm initialized temperature decayed iteration decay function eliminate effect model inaccuracies gave inference algorithms true parameters generative model 
segmentations non annealed variational inference algorithm showed little similarity true segmentations data 
furthermore algorithm generally underestimated number switches converging solutions switches 
annealed algorithm segmentations similar true segmentations data 
fact annealing significantly increased mutual information estimated true segmentations 
light expect values lower bounds annealing higher annealing 
surprisingly case 
mean bounds annealing lower bits runs statistically significant effect wilcoxon signed rank test small compared standard deviation differences bits 
computed value true segmentation data average bits higher bounds annealing 
different sequences length segmentations shown sequences light dark dots corresponding ssms generating data 
rows true segmentations corresponding segmentations deterministic annealing annealing 
hard segmentations obtained thresholding final values 
sequences ones shown 
experiment modelling respiration patient sleep switching state space models prove useful modelling time series nonlinear dynamics characterized different regimes 
illustrate point examined physiological data set patient tentatively diagnosed sleep medical condition patients intermittently breathing sleep results reflex arousal breath 
data obtained repository time series data sets associated santa fe time series analysis prediction competition weigend gershenfeld described detail 

simply wish highlight fact respiration pattern sleep characterized regimes breathing breathing 
furthermore patient periods normal rhythmic breathing 
trained switching state space models varying random seed number components mixture dimensionality state space component data set consisting consecutive measurements chest volume 
controls trained simple state space models varying data available web www cs colorado edu andreas time series santafe html 
samples training testing 
mutual information bits histogram mutual information bits observation segmentations annealing annealing 
dimension state space simple hidden markov models varying number discrete hidden states 
simulations run convergence iterations whichever came convergence assessed measuring change likelihood bound likelihood consecutive steps em 
likelihood simple ssms hmms calculated test set consisted consecutive measurements chest volume 
switching ssms likelihood intractable calculated lower bound likelihood simple ssms modeled data poorly performance flat values 
large majority runs switching state space model resulted models higher likelihood simple 
consistent exception noted values switching ssm performed identically simple ssm 
exploratory experiments suggest cases single component takes responsibility data model effectively 
may local minimum problem result poor initialization heuristics 
looking learning curves simple switching state space models easy see plateaus solutions simple component ssms switching ssm get caught 
likelihoods hidden markov models comparable best switching state space models 
purely terms coding efficiency advantage switching ssm model data 
useful consider nature solutions learned switching ssm 
illustrate showing segmentation produced fairly typical switching ssm components state space dimension 
thick dots bottom figures indicate responsibility assigned components 
component clearly specialized modeling data periods component models periods rhythmic breathing 
switching components provide somewhat satisfying explanation time respiration time respiration chest volume respiration force patient sleep time segments night measurements sampled hz 
training data 
characterized extended periods small variability chest volume followed bursts 
see behaviour followed normal rhythmic breathing 
test data 
segment find instances approximately rhythmic region 
thick lines bottom plot explained main text 
switching switching switching switching hmms log likelihood test data total runs simple state space models switching state space models differing numbers components hidden markov models 
iterations em learning curves state space model switching state space model 
data discrete components needed comparable hmm 
discussion main draw series experiments correct model parameters problem segmenting switching time series components difficult 
combinatorially alternatives considered energy surface suffers local minima local optimization approaches variational method limited quality initial conditions 
deterministic annealing thought sophisticated initialization procedure hidden states final solution temperature provides initial conditions 
annealing improved quality segmentations 
second series experiments suggests real data set believed switching dynamics switching state space model uncover multiple regimes 
captures regimes generalizes test set better simple linear dynamical model 
similar coding efficiency obtained hidden markov models due discrete nature state space model nonlinear dynamics 
doing hidden markov models discrete states solutions interpretable 
variational approximations provide powerful tool inference learning complex probabilistic models 
seen applied switching state space model incorporate single framework known exact inference methods kalman smoothing forward backward algorithm 
training complex models apparent importance methods model selection initialization 
bayesian approaches offer principled way priori knowledge initialization models selected weighted posteriori probabilities 
summarize switching state space model fully dynamical extension mixture experts network closely related known models econometrics control merges ideas underlying hidden markov models linear systems 
clearly appropriate model time series situations priori knowledge multiple approximately linear dynamical regimes switching state space model exploit knowledge 
example sleep problem may possible train separate models known regimes small labelled portion data full learning algorithm obtain better fit labelled unlabelled data 
variational approximations overcome single difficult problem learning switching ssms inference step intractable 
deterministic annealing improves solutions variational method 
notation symbol size description variables theta observation vector time fy theta sequence observation vectors theta state vector state space model ssm time km theta entire real valued hidden state time theta switch state variable represented discrete variable values mg theta vector model parameters theta state dynamics matrix ssm theta output matrix ssm theta state noise covariance matrix ssm theta initial state mean ssm theta initial state noise covariance matrix ssm theta output noise covariance matrix theta initial state probabilities switch state phi theta state transition matrix switch state variational parameters theta responsibility ssm theta related expected squared error ssm generated miscellaneous matrix transpose jxj matrix determinant hxi expected value distribution dimensions size observation vector length sequence observation vectors number state space models size state vector state space model derivation variational fixed point equations appendix derive variational fixed point equations learning algorithm switching state space models 
plan 
write probability density defined switching state space model 
convenience express probability density log domain associated energy function hamiltonian probability density related hamiltonian usual boltzmann distribution temperature delta expf gammah delta normalization constant required delta integrates unity 
expressing probabilities log domain affect resulting algorithm 
similarly express approximating distribution hamiltonian hq obtain variational fixed point equations setting zero derivatives kl divergence respect variational parameters joint probability observations hidden states switching state space model equation fs js gamma jx gamma jx proceed dissect expression constituent parts 
initial probability switch variable time represented theta vector switch state state 
probability transitioning switch state time gamma switch state time js gamma phi gamma initial distribution hidden state variable state space model gaussian mean covariance matrix gammak jq gamma exp ae gamma gamma gamma gamma oe probability distribution state state space model time state time gamma gaussian mean gamma covariance matrix jx gamma gammak jq gamma exp ae gamma gamma gamma gamma gamma gamma oe write jx gammad jrj gamma exp ae gamma gamma gamma gamma oe terms exponent equal vanish product 
combining negative logarithm obtain hamiltonian switching state space model ignoring constants log jq gamma gamma gamma gamma log jq gamma gamma gamma gamma gamma log jrj gamma gamma gamma gamma log gamma gamma log phi hamiltonian approximating distribution analogously derived definition equation fs zq gamma gamma potentials initial switch state switch state transitions gamma phi gamma potential initial state state space model jx potential state time state time gamma gamma jx gamma jx hamiltonian obtained combining terms negative logarithm hq log jq gamma gamma gamma gamma log jq gamma gamma gamma gamma gamma log jrj gamma gamma gamma gamma log gamma gamma log phi gamma log comparing hq see interaction variables eliminated introducing sets variational parameters responsibilities bias terms discrete markov chain order obtain approximation maximizes lower bound log likelihood minimize kl divergence kl qkp function variational parameters kl qkp fs fs log fs fs dfx hh gamma hq gamma log zq log deltai denotes expectation approximating distribution zq normalization constant define distributions exponential family 
consequence zeros derivatives kl respect variational parameters obtained simply equating derivatives hhi respect corresponding sufficient statistics ghahramani gamma hi hs gamma hi hx gamma hi hp hx gamma hx covariance terms cancel subtract hamiltonians hq gammah gamma gamma gamma gamma gamma log derivatives obtain gamma hi hs gamma log gamma gamma gamma gamma ae gamma hi hx gamma gamma hs gamma hx gamma gamma hi gamma hs gamma get fixed point equation satisfied hs fact hs get 
fu 

state estimation switching environments 
ieee transactions automatic control ac 
anderson moore 

optimal filtering 
prentice hall englewood cliffs nj 


likelihood evaluation state estimation nonlinear state space models 
ph thesis graduate group managerial science applied economics university pennsylvania philadelphia pa baldi chauvin mcclure 

hidden markov models biological primary sequence information 
proc 
nat 
acad 
sci 
usa 
bar shalom li 

estimation tracking 
artech house boston ma 
baum petrie soules weiss 

maximization technique occurring statistical analysis probabilistic functions markov chains 
annals mathematical statistics 
bengio frasconi 

input output hmm architecture 
tesauro touretzky leen editors advances neural information processing systems pages 
mit press cambridge ma 
blake isard reynard 

learning track visual motion contours 
artificial intelligence 
cacciatore nowlan 

mixtures controllers jump linear non linear plants 
cowan tesauro alspector editors advances neural information processing systems pages 
morgan kaufmann publishers san francisco ca 
carter kohn 

gibbs sampling state space models 
biometrika 
bishop ghosh 

mixture experts framework adaptive kalman filtering 
ieee trans 
systems man cybernetics 
chang athans 

state estimation discrete systems switching parameters 
ieee transactions aerospace electronic systems aes 
cover thomas 

elements information theory 
wiley new york 
dean kanazawa 

model reasoning causation 
computational intelligence 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
royal statistical society series 
deng 

stochastic model speech incorporating hierarchical nonstationarity 
ieee trans 
speech audio processing 
digalakis rohlicek ostendorf 

ml estimation stochastic linear system em algorithm application speech recognition 
ieee transactions speech audio processing 
everitt 

latent variable models 
chapman hall london 
fraser 

forecasting probability densities hidden markov models states 
wiegand gershenfeld editors time series prediction forecasting understanding past sfi studies sciences complexity proc 
vol 
xv pages 
addison wesley reading ma 
ghahramani 

structured variational approximations 
technical report ftp ftp cs toronto edu pub zoubin struct ps gz 
ghahramani hinton 

parameter estimation linear dynamical systems 
technical report crg tr ftp ftp cs toronto edu pub zoubin tr 
ps gz ghahramani hinton 

em algorithm mixtures factor analyzers 
technical report crg tr ftp ftp cs toronto edu pub zoubin tr 
ps gz ghahramani jordan 

factorial hidden markov models 
machine learning 
goodwin sin 

adaptive filtering prediction control 
prentice hall 
hamilton 

new approach economic analysis nonstationary time series business cycle 
econometrica 
harrison stevens 

bayesian forecasting discussion 
royal statistical society 
hinton dayan revow 

modeling manifolds images handwritten digits 
submitted publication 
isard blake 

contour tracking stochastic propagation conditional density 
buxton cipolla editors proc 
th european conference computer vision volume pages 
springer verlag berlin 
jacobs jordan nowlan hinton 

adaptive mixture local experts 
neural computation 
jordan ghahramani jaakkola saul 

variational methods graphical models 
jordan editor learning graphical models 
kluwer academic publishers 
jordan jacobs 

hierarchical mixtures experts em algorithm 
neural computation 
juang rabiner 

hidden markov models speech recognition 
technometrics 


recursive estimation dynamic modular rbf networks 
touretzky mozer hasselmo editors advances neural information processing systems pages 
mit press 
kalman bucy 

new results linear filtering prediction 
journal basic engineering asme 
kanazawa koller russell 

stochastic simulation algorithms dynamic probabilistic networks 
besnard hanks editors uncertainty artificial intelligence 
proceedings eleventh conference pages 
morgan kaufmann publishers san francisco ca 
kim 

dynamic linear models markov switching 
econometrics 
lauritzen spiegelhalter 

local computations probabilities graphical structures application expert systems 
royal statistical society pages 
ljung 

theory practice recursive identification 
mit press cambridge ma 
meila jordan 

learning fine motion markov mixtures experts 
touretzky mozer hasselmo editors advances neural information processing systems 
mit press 
neal hinton 

new view em algorithm justifies incremental variants 
technical report unpublished manuscript 
parisi 

statistical field theory 
addison wesley redwood city ca 
pearl 

probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann san mateo ca 
rabiner juang 

hidden markov models 
ieee acoustics speech signal processing magazine 
rao ballard 

class stochastic models invariant recognition motion stereo 
submitted advances neural information processing systems 
rauch 

solutions linear smoothing problem 
ieee transactions automatic control 
goldberger moody mark 

multi channel physiological data description analysis 
weigend gershenfeld editors time series prediction forecasting understanding past sfi studies sciences complexity proc 
vol 
xv pages 
addison wesley reading ma 
saul jordan 

exploiting tractable substructures intractable networks 
touretzky mozer hasselmo editors advances neural information processing systems 
mit press 
shumway stoffer 

approach time series smoothing forecasting em algorithm 
time series analysis 
shumway stoffer 

dynamic linear models switching 
amer 
stat 
assoc 
smyth 

hidden markov models fault detection dynamic systems 
pattern recognition 
smyth heckerman jordan 

probabilistic independence networks hidden markov probability models 
neural computation 
ueda nakano 

deterministic annealing variant em algorithm 
tesauro touretzky alspector editors advances neural information processing systems pages 
morgan kaufmann 
weigend gershenfeld 

time series prediction forecasting understanding past 
sfi studies sciences complexity proc 
vol 
xv 
addison wesley reading ma 

