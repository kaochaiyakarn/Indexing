convergence properties softassign quadratic assignment algorithm anand rangarajan department diagnostic radiology yale university anand med yale edu alan yuille smith eye research institute san francisco ca yuille ski org eric mjolsness dept computer science engineering university california san diego cs ucsd edu softassign quadratic assignment discrete time continuous state synchronous updating optimizing neural network 
effectiveness shown traveling salesman problem graph matching graph partitioning thousands simulations associated study convergence properties 
construct discrete time lyapunov functions cases exact approximate doubly stochastic constraint satisfaction show convergence fixed point 
combination convergence properties experimental success softassign algorithm technique choice neural qap optimization 
discrete time optimizing neural networks topic neural computation 
discrete state hopfield model hopfield considerable effort spent analyzing convergence properties discrete time networks especially dimensions continuous versus discrete state synchronous versus sequential update hopfield peterson fogelman soulie marcus blum wang waugh koiran wang 
relative continuous time counterparts discrete time optimizing networks easily implemented digital computers temporal step size parameter needed implementing continuous time networks 
advantage partially offset problem constraint satisfaction discrete time networks 
continuous time lyapunov functions shown exist quadratic assignment qap optimizing networks gee gee prager wolfe yuille kosowsky discrete time counterparts 
quadratic assignment networks important subsume traveling salesman problem tsp peterson graph partitioning van den bout miller iii convergence properties softassign quadratic assignment algorithm peterson graph isomorphism rangarajan graph matching gold rangarajan embody doubly stochastic constraint satisfaction 
softassign quadratic assignment algorithm discrete time continuous state synchronous updating neural network 
despite embodying fairly complex doubly stochastic constraint satisfaction subnetwork lineage earlier discrete time optimizing neural networks 
effectiveness shown qap problems tsp graph partitioning graph matching gold rangarajan rangarajan gold rangarajan linear problems point matching rangarajan linear assignment kosowsky yuille existence discrete time lyapunov function shown 
demonstrate existence discrete time lyapunov function softassign quadratic assignment neural 
section considering simpler case exact doubly stochastic constraint satisfaction 
directly leads general discrete time lyapunov function broadly applicable choice neuronal activation function 
contrast section show case approximate doubly stochastic constraint satisfaction discrete time lyapunov function easily constructed exponential neuronal activation function 
softassign quadratic assignment algorithm quadratic assignment problem qap stated follows min qap gamma aibj ai bj ai bj ai ai ai subject ai ai ai quadratic assignment benefit matrix linear assignment benefit matrix desired theta permutation matrix 
binary constraint relaxed positivity constraint doubly stochastic min gamma aibj ai bj ai bj ai ai ai subject ai ai ai stands minimizing space doubly stochastic matrices yield permutation matrix 
yuille kosowsky shown positive definite rewritten matrix minima permutations 
specified problem positive definite 
way fix adding term fl ai ai gamma ai objective function 
ai ai equivalent adding self amplification term rangarajan von der malsburg gamma fl ai ai qap objective function 
adding self amplification term equivalent defining new benefit matrix aibj def aibj fl ffi ab ffi ij exists lower bound self amplification parameter fl convergence properties softassign quadratic assignment algorithm newly defined benefit matrix positive definite 
henceforth refer qap benefit matrix understanding eigenvalues easily shifted changing value fl 
softassign quadratic assignment algorithm discrete time synchronous updating dynamical system rangarajan 
combines deterministic annealing self amplification softassign minimizing objective function rangarajan yuille kosowsky gamma aibj ai bj ai bj fi ai ai log ai ai gamma ai gamma form energy function lagrange parameters constraint satisfaction log entropy barrier function luenberger ensures positivity fm ai deterministic annealing inverse temperature parameter fi 
annealing schedule typically prescribed fi 
log entropy term somewhat different traditional barrier functions nonlinear optimization tend approach boundary 
shall see prevent useful barrier function 
qap benefit matrix preset chosen problem example graph matching tsp graph partitioning subsequently modified restricted manner indicated earlier 
handling graph partitioning multiple membership constraint requires slight modification objective function 
problems assume fc ai bj symmetric ai bj bj ai notation summarized table 
derive softassign quadratic assignment algorithm apply algebraic transformation mjolsness garrett quadratic term gamma aibj ai bj ai bj min oe gamma aibj ai bj ai oe bj aibj ai bj oe ai oe bj positive definite right side unique minimum oe oe ai ai algebraic transformation place modified energy function oe gamma aibj ai bj ai oe bj aibj ai bj oe ai oe bj ai ai ai ai gamma ai gamma fi ai ai log ai application algebraic transformation results energy function linear quadratic oe 
minimum oe known oe 
alternately minimizing oe keeping lagrange parameter vectors fixed get oe ai ai ai exp fi bj ai bj oe bj gamma ai gamma gamma gamma convergence properties softassign quadratic assignment algorithm table variables constants softassign qap dynamical system qap problem size fc ai bj qap quadratic benefit matrix fa ai qap linear benefit matrix fi inverse temperature fm ai match matrix nth step fb ai effective assignment matrix defined bj ai bj gamma bj gamma ai row constraint lagrange parameter nth step column constraint lagrange parameter nth step ffl row constraint convergence criterion ai gamma ffl ng deltam ai match matrix difference defined fm ai gamma ai ffi convergence criterion temperature ai deltam ai ffi ng delta row constraint lagrange parameter difference defined gamma delta column constraint lagrange parameter difference defined gamma fmax upper bound absolute value deltal discrete time lyapunov energy difference smallest eigenvalue fc ai bj subspace column constraint identify oe value previous step get discrete time updating dynamical system ai exp fi ai gamma gamma gamma ai def bj ai bj bj gamma ai main intuition alternating sequence updates lowers energy analogy expectation maximization em algorithm dempster jordan jacobs 
rigorously demonstrate existence lyapunov function sequence updates 
see log barrier function leads exponential neuronal activation function 
specified lagrange parameters 
iteration discrete time dynamical system satisfy row column constraints exactly approximately satisfied solving lagrange parameters possible specify separate constraint satisfaction energy function lagrange parameters 
juncture point constraint satisfaction undertaken matrix time step held fixed 
substituting definition reversing sign solving lagrange parameters maximization minimization problem get new energy function convergence properties softassign quadratic assignment algorithm minimized lagrange parameters lag fi ai exp fi ai gamma gamma gamma derivatives obtain fixed point relations gamma exp fi ai gamma gamma gamma ai gamma exp fi ai gamma gamma gamma ai possible simultaneously solve closed form lagrange parameters alternating minimization algorithm obtained 
assume initial condition vectors held fixed moment 
specify alternating sequence updates specification get exp fi exp fi ai gamma gamma exp fi exp fi ai gamma gamma corresponding alternating sequence updates lagrange parameters may define sequence updates matrix odd part fixes rows part fixes columns gamma ai exp fi ai gamma gamma gamma gamma ai exp fi ai gamma gamma gamma see gamma ai ai exp fi gamma gamma ji gamma bi similarly row constraint get ai ai exp fi gamma ji aj may write ai gamma ai gamma ai ai ai ai convergence properties softassign quadratic assignment algorithm alternating sequence normalizations identical sinkhorn theorem called sinkhorn balancing sinkhorn doubly stochastic matrix obtained positive square matrix simple process alternating row column normalizations 
note time indices ai outer time index consequently held fixed apply sinkhorn balancing 
sinkhorn balancing changes order satisfy row column constraints 
note lagrange parameters set suitable initial values time enter sinkhorn balancing loop 
having completely specified softassign qap dynamical system write pseudocode softassign quadratic assignment algorithm initialize fi fi ai ai deterministic annealing 
fi fi relaxation 
ai converge 
ai bj ai bj gamma bj gamma ai softassign initialize lagrange parameters ai exp fi ai gamma gamma gamma sinkhorn 
ai converge update delta normalizing columns ai gamma ai gamma ai update delta normalizing rows ai ai ai softassign fi having specified softassign qap algorithm natural questions arise juncture dynamical system converge fixed point setting fi 
ii convergence criterion sinkhorn balancing procedure affect convergence properties dynamical system temperature 
answers questions 
convergence properties softassign quadratic assignment algorithm convergence properties recall previous section minimizing space doubly stochastic matrices yields permutation matrix provided positive definite 
consequently convergence algorithm permutation matrix crucially depends minimizing energy function 
main issue discrete time dynamical system solving lagrange parameters converges fixed point value barrier function parameter fi 
emphasized motivated choice log entropy barrier function associated exponential neuronal activation function 
entropy barrier function independently motivated statistical physics considerations yuille kosowsky rangarajan turns play central role convergence properties analyzed 
log function privileged barrier function perspective luenberger develop analysis convergence properties general barrier function 
shall see choice barrier function convergence assumptions regarding sinkhorn balancing connected fundamental way 
turns key assumption separating general barrier functions entropy barrier function convergence sinkhorn balancing doubly stochastic matrix 
sinkhorn returns doubly stochastic matrix general analysis terms unspecified barrier function carried 
sinkhorn returns matrix merely close doubly stochastic analysis complex designing algorithm equivalent sinkhorn constraint satisfaction complex 
instance barrier function oe gamma log popular choice interior point methods analytical solution lagrange parameters turns hard derive impossible 
accordingly section assuming sinkhorn returns doubly stochastic matrix 
section assumption relaxed analysis sinkhorn approximately converges carried 
exact convergence sinkhorn examples barrier functions oe log function oe gamma log gamma log gamma log gamma barrier functions barrier function control parameters fi inseparable luenberger barrier function annealing schedule prescribed fi 
rewrite qap energy function general barrier function 
assume exact constraint satisfaction gamma aibj ai bj ai bj ai ai ai ai gamma ai gamma fi ai oe ai oe suitable barrier function 
barrier function oe corresponding neuronal activation function oe gamma provided relevant derivatives inverses exist 
assuming exact constraint satisfaction flagged repeat derivation section discrete time assignment algorithm 
objective function 
introduce algebraic transformation new vari convergence properties softassign quadratic assignment algorithm able oe get new objective function shown 
minimized oe yield alternating update equation 
oe shown value previous time step obtain discrete time synchronous update equation 
replacing entropy barrier function general barrier function oe exactly repeating aforementioned steps obtain discrete time synchronous update dynamical system corresponding general barrier function oe ai gamma oe delta gamma fi bj aibj bj gamma ai fi oe ai bj aibj bj gamma ai choices barrier function oe log oe gamma unpleasant form making difficult solve lagrange parameters assume exact constraint satisfaction 
see barrier function oe lead corresponding oe gamma neuronal activation function 
mentioned solving lagrange parameter vectors sinkhorn theorem ensure row column constraints satisfied 
deeper assumption 
exponential activation update equation entropy barrier function shown trivially satisfies positivity constraint ai true general barrier functions 
positivity constraint separately checked barrier function 
section bypass potential problems assuming exact constraint satisfaction positivity row column constraints 
assumption exact constraint satisfaction relaxed specialize log barrier function 
assumption exact convergence sinkhorn follows lagrange parameter vectors dropped energy function 
tantamount assuming restricted doubly stochastic 
assume positivity constraint satisfied 
dropping terms involving lagrange parameters write new energy function 
new energy function turns suitable discrete time lyapunov energy function modify notation somewhat gamma aibj ai bj ai bj ai ai ai fi ai oe ai energy function discrete time synchronous updating dynamical system equation temperature corresponding exact constraint place state theorem theorem barrier function oe convex constraint satisfaction exact temperature energy function specified discrete time lyapunov function discrete time synchronous update dynamical system specified provided lagrange parameters specified row column constraints satisfied 
proof need show change energy step step greater zero 
convergence properties softassign quadratic assignment algorithm need separate argument 
change energy deltal def gamma gamma aibj ai bj ai bj ai ai ai aibj ai bj ai bj gamma ai ai ai fi ai oe ai gamma fi ai oe ai function oe convex oe gamma oe oe gamma change energy rewritten deltal aibj ai bj deltam ai deltam bj aibj aibj bj deltam ai gamma ai ai deltam ai gamma fi ai deltam ai oe ai substituting get deltal aibj ai bj deltam ai deltam bj ai deltam ai ai deltam ai constraint satisfaction exact reduces deltal aibj ai bj deltam ai deltam bj due positive definiteness examining proof clear global positive definiteness stronger condition required energy function discrete time lyapunov function 
sufficient positive definite linear subspace spanned row column constraints 
summarize shown lyapunov function exists fairly general discrete time dynamical system 
main assumptions convex barrier function oe ii exact constraint satisfaction 
approximate convergence sinkhorn assume sinkhorn balancing yields doubly stochastic matrix 
practice softassign stopped suitable convergence criterion met 
loss generality may consider situation column constraint exactly satisfied ai row constraint merely approximately satisfied ai gamma ffl ffl small positive quantity 
section analyze convergence properties softassign qap algorithm sinkhorn approximately converges 
analysis carried entropy barrier function 
think difficult analyze general case 
demonstrate write energy difference general barrier function case column constraint exactly satisfied row convergence properties softassign quadratic assignment algorithm constraint approximately satisfied 
done substituting energy difference formula column constraint exactly satisfied deltal aibj ai bj deltam ai deltam bj ai deltam ai fi ai oe ai gamma oe ai deltam ai oe ai term third term positive due positive definiteness convexity oe respectively 
analyzing properties lagrange parameter vector general barrier function turn quite intricate involved 
contrast bounds lagrange parameters easily derived entropy barrier function shown appendix may possible repeat analysis specific barrier functions 
point focus exclusively entropy barrier function 
lyapunov function entropy barrier function theorem temperature energy function gamma aibj ai bj ai bj ai ai ai fi ai ai log ai discrete time lyapunov function discrete time synchronous updating dynamical system ai exp fi ai gamma gamma gamma provided conditions hold column constraint ai exactly satisfied 
ii row constraint approximately satisfied ai gamma ffl ffl 
iii qap benefit matrix strictly positive definite smallest eigenvalue 
iv convergence criterion temperature ai deltam ai ffi ffi ffl max ai cj gammac bi cj max ai gammaa bi fi log gamma ffl gammaffl proof change energy deltal def gamma gamma aibj ai bj ai bj ai ai ai aibj ai bj ai bj gamma ai ai ai fi ai ai log ai gamma fi ai ai log ai convergence properties softassign quadratic assignment algorithm rewritten deltal aibj ai bj deltam ai deltam bj aibj aibj bj deltam ai gamma ai ai deltam ai fi ai ai log ai ai gamma fi ai deltam ai log ai may write fi log ai bj ai bj bj gamma ai gamma gamma gamma fi results simplification lyapunov energy difference deltal aibj ai bj deltam ai deltam bj fi ai ai log ai ai gamma ai ai ai deltam ai ai deltam ai column constraint ai kept continuously satisfied sinkhorn iteration simplifications deltal aibj ai bj deltam ai deltam bj fi ai ai log ai ai ai deltam ai relation ai 
convergence require discrete time lyapunov energy difference greater zero 
term strictly positive ai bj positive definite subspace spanned column constraint ai 
second term greater equal zero non negativity measure 
third term positive negative 
controlling degree positive definiteness qap benefit matrix ai bj ensure energy difference positive convergence 
specified section lower bound eigenvalues achieved 
require upper bound absolute value third term 
row constraint convergence criterion ai gamma ffl derive upper bound derivation appendix max def max ai cj gamma bi cj max ai gamma bi fi log gamma ffl gamma ffl assuming convergence criterion ai deltam ai ffi temperature get considering third terms deltal ffi gamma provided ffi convergence properties softassign quadratic assignment algorithm substitute value max get ffi ffl max ai cj gamma bi cj max ai gamma bi fi log gamma ffl gammaffl consequence theorem loss independence constraint satisfaction threshold parameter ffl match matrix convergence temperature threshold parameter ffi 
ffl exists lower bound ffi condition iv 
lower bound approximately constant delta ffl 
smallest eigenvalue easily shifted changing self amplification parameter fl value ffi appropriate problem instance chosen 
section example conditions theorem meaningfully met 
experiments experiments qap benefit matrix set manner ai bj ab ij omega particular decomposition useful permits straightforward manipulation row column subspaces 
linear benefit matrix add insight convergence properties set zero 
demonstrate insight gained theorem 
generated quadratic benefit matrix positive definite 
separately generated matrices gamma normal mean zero variance random numbers 
matrices symmetric completely specifies shifted eigenvalues eigenvalue spectrum roughly centered zero 
ran softassign qap algorithm 
energy difference shown computed lagrange parameter energy term set zero 
transient fluctuations negative positive energy difference settles limit cycle length 
positive definite shifting spectra upward 
refine experiment making positive definite subspaces column row constraints respectively 
recomputing reran softassign qap algorithm 
compute energy difference iteration shown right 
expected energy difference greater zero 
demonstrated positive definite leads convergent algorithm 
carefully set parameters criteria derived theorem 
parameter values ffl gamma 
column constraint satisfied positive definite linear subspace column constraint smallest eigenvalue subspace 
column subspace corresponds eigenvalues shifted spectrum smallest eigenvalue column subspace 
shifted spectrum smallest eigenvalue unrestricted subspace 
eigenvalues product eigenvalues achieve lower bound 
restriction linear subspace affect 
convergence properties softassign quadratic assignment algorithm ffl set may calculate lower bound ffi 
ffi ffl max ai cj gamma bi cj max ai gamma bi fi log gamma ffl gammaffl ai bj ab ij max ai cj gamma bi cj max max ij max max ac gamma min ac min ij min min ac gamma max ac lower bound ffi calculated temperature convergence criterion ai deltam ai ffi 
temperature softassign qap algorithm executed ai deltam ai falls lower bound ffi 
experiments linear temperature schedule fi fi 
convergence criterion gamma ai ai row dominance 
temperature checked see gamma ai ai 
checked see obtained permutation matrix executing winner take rows called row dominance kosowsky yuille 
parameters set manner ffi 
ffi remains function temperature significantly change entire range temperatures particular set chosen parameters 
energy difference shown greater zero 
break conditions imposed theorem 
parameter ffl changed earlier value gamma 
time kept fixed convergence criterion parameter ffi dropped ffi 
theorem recalculate ffi approximately result ffi unacceptable convergence threshold ai deltam ai executed softassign qap algorithm parameters parameters annealing schedule kept exactly previous experiment 
evolution dynamical system monitored energy difference derived corresponding theorem 
second term non negative include energy difference computation 
energy difference straightforward combination quadratic term lagrange parameter energy term 
monitored energy difference corresponding theorem deltae aibj ai bj deltam ai deltam bj deltam ai fact delta 
energy difference deltae plotted 
deltae fluctuates zero due comparatively larger value ffl 
show relative contributions quadratic lagrange parameter terms 
discussion existence discrete time lyapunov function softassign quadratic assignment algorithm fundamental importance 
existence lyapunov function depend period convergence properties softassign quadratic assignment algorithm limit cycles possible show mild assumptions regarding number fixed points koiran softassign qap algorithm converges fixed point 
apply equally cases exact approximate doubly stochastic constraint satisfaction exponential neuronal activation function 
extension constraint satisfaction case outliers graph matching gold rangarajan rangarajan multiple membership constraint graph partitioning peterson problems construction lyapunov function case exact constraint satisfaction trivial minor modifications needed derive bound lagrange parameter case approximate constraint satisfaction 
results derived general quadratic assignment problem specialized individual cases tsp subgraph isomorphism graph matching graph partitioning 
initial effort lines specific exact constraint satisfaction appears rangarajan 
bounds lagrange parameter vector rewriting update equation ai exp fi ai gamma gamma gamma sinkhorn balancing approximately converges assume column constraint ai exactly satisfied row constraint approximately satisfied ai gamma ffl 
column constraint exactly satisfied may eliminate 
ai exp fi ai gamma gamma gamma exp fi exp fi ai gamma gamma ai exp fi ai gamma ji exp fi ai gamma ji identical familiar softmax nonlinearity bridle understanding set row constraint approximately satisfied 
proceeding derivation bound note invariant global shifts transformation ff leaves unchanged 
consequently loss generality assume 
ai exp fi ai gamma ji exp fi bi gamma ji exp fi bi gamma ai gamma ji min exp fi bi gamma ai gamma ji convergence properties softassign quadratic assignment algorithm approximate convergence row constraint implies gamma ffl ai may write gamma ffl exp fi bi gamma ai gamma ji min exp fi bi gamma ai gamma ji rearranged give min exp fi bi gamma ai gamma ji gamma ffl gamma ffl inequality remains true term summation left 
min bi gamma ai gamma fi log gamma ffl gamma ffl get gamma max ai gamma bi fi log gamma ffl gamma ffl assumed loss generality may write max ai gamma bi fi log gamma ffl gamma ffl bound max maximum value follows max max ai gamma bi fi log gamma ffl gamma ffl result reported yuille kosowsky 
sake completion bound lagrange parameter vector far sought bounds respect support matrix qap pre specified constant 
depends current estimate ai bj ai bj bj gamma ai substituting get max max cj ai cj cj gamma cj bi cj cj gamma ai bi fi log gamma ffl gamma ffl max commute entries non negative max cj max ai cj gamma bi cj cj max ai gamma bi fi log gamma ffl gamma ffl define cj def max ai cj gamma bi cj ffi def max ai gamma bi 
max ai ai ai ffi fi log gamma ffl gamma ffl convergence properties softassign quadratic assignment algorithm dependence max time step obviously unsatisfactory 
constraint ai get ai ai ai max ai get max max ai ffi fi log gamma ffl gamma ffl max ai cj gamma bi cj max ai gamma bi fi log gamma ffl gamma ffl blum wang 

stability fixed points periodic orbits bifurcations analog neural networks 
neural networks 
bridle 

training stochastic model recognition algorithms networks lead maximum mutual information estimation parameters 
touretzky editor advances neural information processing systems pages san mateo ca 
morgan kaufmann 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
statist 
soc 
ser 

fogelman soulie martinez 

energy functions neural networks continuous local functions 
complex systems 
gee prager 

analytical framework optimizing neural networks 
neural networks 
gee prager 

polyhedral combinatorics neural networks 
neural computation 
gold rangarajan 

softmax softassign neural network algorithms combinatorial optimization 
journal artificial neural networks 
gold rangarajan 

graduated assignment algorithm graph matching 
ieee trans 
patt 
anal 
mach 
intell 
hopfield 

neural networks physical systems emergent collective computational abilities 
proceedings national academy sciences 
hopfield 

neurons graded response collective computational properties state neurons 
proceedings national academy sciences 
jordan jacobs 

hierarchical mixtures experts em algorithm 
neural computation 
convergence properties softassign quadratic assignment algorithm koiran 

dynamics discrete time continuous state hopfield networks 
neural computation 
kosowsky yuille 

invisible hand algorithm solving assignment problem statistical physics 
neural networks 
luenberger 

linear nonlinear programming 
addison wesley reading ma 
marcus 

dynamics iterated map neural networks 
physical review 
mjolsness garrett 

algebraic transformations objective functions 
neural networks 
peterson 

new method mapping optimization problems neural networks 
intl 
journal neural systems 
rangarajan chui mjolsness goldman duncan 

robust point matching algorithm alignment 
medical image analysis 
press 
rangarajan gold mjolsness 

novel optimizing network architecture applications 
neural computation 
rangarajan yuille gold mjolsness 

convergence proof softassign quadratic assignment algorithm 
advances neural information processing systems nips pages 
mit press 


statistical mechanics underlying theory elastic neural optimisations 
network 
sinkhorn 

relationship arbitrary positive matrices doubly stochastic matrices 
ann 
math 
statist 


mathematical programming formulations neural combinatorial optimization algorithms 
journal artificial neural networks 
van den bout miller iii 

graph partitioning annealed networks 
ieee trans 
neural networks 
von der malsburg 

network self organization 
davis lau editors neural electronic networks pages 
academic press san diego ca 
wang jagota garzon 

absence cycles symmetric neural networks 
touretzky mozer hasselmo editors advances neural information processing systems nips pages 
mit press 
convergence properties softassign quadratic assignment algorithm waugh 

analog neural networks local competition 
dynamics stability 
physical review 
wolfe parry macmillan 

hopfield style neural networks tsp 
ieee intl 
conf 
neural networks volume pages 
ieee press 
yuille kosowsky 

invisible hand algorithm time convergence temperature tracking 
technical report harvard robotics laboratory 
yuille kosowsky 

statistical physics algorithms converge 
neural computation 
list figures iteration number energy difference iteration number energy difference energy difference plot 
left change energy positive positive definite 
right change energy positive positive definite 
energy difference left implies energy function increases positive energy difference right implies energy function increases 
convergence properties softassign quadratic assignment algorithm iteration number energy difference plot change energy positive conditions established theorem imposed 
iteration number energy difference plot deltae 
ffl ffi 
energy difference plot clearly shows carefully setting ffl ffi needed ensure convergence 
convergence properties softassign quadratic assignment algorithm iteration number quadratic energy term iteration number energy difference plot left quadratic term right lagrange parameter energy difference term deltam ai deltae 
