bayesian methods adaptive models thesis david mackay partial fulfillment requirements degree doctor philosophy california institute technology pasadena california fl submitted december ii years benefited greatly discussions ron benson john bridle peter cheeseman sidney fels steve gull andreas herz john hopfield doug allen david mike lewicki tom loredo steve luttrell ronny meir ken miller marcus mitchell radford neal steve nowlan david robinson ken rose john skilling haim sompolinsky nick weir 
comments referees chapter helpful 
especially advisor john hopfield support criticism advice 
am grateful anderson dong brian fox tom expert management hopfield cns computers 
dr goodman dr smyth funding trip maxent learnt final tools needed research 
supported caltech fellowship studentship serc uk 
hope ideas dissertation ends 
current address cavendish laboratory road cambridge cb united kingdom 
mackay cam ac uk second edition second edition thesis 
typographical errors corrected small number clarifications text 
postscript files document may obtained anonymous ftp ra phy cam ac uk pub mackay 
typeset iii bayesian methods adaptive models thesis david john cameron mackay partial fulfillment requirements degree doctor philosophy california institute technology pasadena california submitted december advisor prof hopfield bayesian framework model comparison regularisation demonstrated studying interpolation classification problems modelled linear non linear models 
framework quantitatively embodies occam razor 
complex regularised models automatically inferred probable flexibility allows fit data better 
applied neural networks bayesian framework possible objective comparison solutions alternative network architectures objective stopping rules network pruning growing procedures objective choice type weight decay terms regularisers line techniques optimising weight decay regularisation constant magnitude measure effective number determined parameters model quantified estimates error bars network parameters network output 
case classification models shown careful incorporation error bar information classifier predictions yields improved performance 
comparisons inferences bayesian framework traditional cross validation methods help detect poor underlying assumptions learning models 
relationship bayesian learning framework active learning examined 
objective functions discussed measure expected informativeness candidate data measurements context interpolation classification problems 
concepts methods described thesis quite general applicable data modelling problems involve regression classification density estimation 
iv contents contents summary need occam razor bayesian modelling 
neural networks need occam razor 
bayesian interpolation data modelling occam razor evidence occam factor noisy interpolation problem selection parameters ff fi model comparison demonstration practical bayesian framework backpropagation networks gaps backprop review bayesian regularisation model comparison adapting framework demonstration discussion information objective functions active data selection choice information measure maximising total information gain maximising information interpolant region interest maximising discrimination models demonstration discussion evidence framework applied classification networks classifier sets outputs evaluating evidence active learning discussion inferring input dependent noise level contents postscript closed hypothesis space approximation probabilities relevant 
having explicit alternative interpretation weight decay tasks open problems bibliography vi list figures list figures abstraction data modelling process bayesian inference fits data modelling process bayes embodies occam razor occam factor best interpolant depends ff choosing ff bad parameter measurements evidence data set data set interpolated splines typical samples prior distributions models typical neural network output data error versus number hidden units test error versus number hidden units test error vs data error log evidence solutions regulariser number determined parameters data misfit versus fl log evidence versus test error regulariser comparison test errors classes weights second prior log evidence versus number hidden units second prior log evidence second prior versus test error demonstration total marginal information gain approximation moderated probability comparison probable outputs moderated outputs moderation thing 
test error versus data error test error versus evidence correlation test error evidence amount data varies demonstration expected mean marginal information gain chapter summary need occam razor countless problems science statistics technology require limited data set preferences assigned alternative models differing complexities 
example alternative hypotheses accounting planetary motion geocentric model simpler model solar system 
contentious similar problem fitting curve data alternative models assign different functional forms curve example linear function free parameters quadratic cubic function parameters 
nice just rank models fit data familiar difficulty complex model typically fits data better fit curve data quadratic curve parameters fit data better linear model parameters polynomial terms fits data better preferring best fit model leads choose detailed parameterised models interpolate generalise poorly 
occam razor principle states unnecessarily complex models preferred simpler ones 
quantify intuitive principle objective part modelling method 
bayesian probability theory provides framework inductive inference called common sense reduced calculation poorly known fact bayesian methods embody occam razor automatically quantitatively 
bayesian model comparison central theme thesis 
particular power bayesian occam razor demonstrated neural networks 
neural networks novel modelling tools capable learning examples 
currently popular models notorious lack objective grounding main goal thesis provide objective practical framework neural network techniques applying methods bayesian model comparison 
process enhancements current neural network methods arise 
bayesian modelling 
bayesian methods inductive inference developed detail early century cambridge sir harold jeffreys 
time jeffreys ideas opposed fisher debate persisted orthodox view statistics minority bayesian camp 
dwell details bayesian methods adaptive models gather data create alternative models gamma gamma fit model data gamma gamma assign preferences alternative models choose data gather gather data decide create new models create new models choose actions oe gamma gamma gamma psi abstraction data modelling process 
central boxes inference steps bayesian methods applied 
philosophical argument goes deep meaning probability thesis demonstrate possible bayesian methods solve problems neural networks laborious impossible 
bayesian minority steadily growing especially fields economics pattern processing 
time state art problem speech recognition bayesian technique hidden markov models best image reconstruction algorithms bayesian probability theory maximum entropy bayesian methods viewed mistrust orthodox statistics community framework model comparison especially poorly known people call bayesians 
thesis takes time thoroughly review flavour bayesianism am 
word bayesian denotes decision strategy minimises expectation cost bayesian tries incorporate prior knowledge inference decision process 
fact varieties bayesian 
thesis presents flavour bayesianism decisions involved 
inference decision cleanly separated 
terms bayes risk bayes optimal vocabulary thesis 
genealogy flavour laplace jeffreys cox jaynes gull 
difference approach known bayesian emphasis inverse forward probability 
forward probability uses probabilities priors bayes rule 
forward probability example evaluate typical performance modelling procedure averaged different data sets defined ensemble 
philosophy inverse probability evaluate relative plausibilities alternative models light single data set observe 
unaware bayesian occam razor 
chapter 
summary inference fits data modelling process illustrates abstraction data modelling process summary applies example tasks fitting curve data reconstructing blurred image making automatic pattern recognition system descriptive general scientific method 
start gathering data creating models account data 
levels inference marked double framed boxes 
level fitting model data task infer free parameters model data 
second level inference task model comparison 
wish rank plausible alternative models light data 
having fitted models compared decide gather data invent new models data repeat inference process 
knowledge gained data decisions actions world 
bayesian methods solve inductive inference problems central boxes tasks modelling process directly addressed bayes rule applies inductive inference problems 
level inference fitting model data usually straightforward task differences bayesian non bayesian solutions pronounced level 
thesis especially emphasise second level inference task model comparison 
inference problem straightforward quantitative occam razor needed penalise complex models 
boxes diagram visited thesis 
bayes rule fundamental concept bayesian analysis plausibilities alternative hypotheses represented probabilities inference performed evaluating probabilities 
suppose collection models hl competing account data gather 
initial beliefs relative plausibility models quantified list probabilities hl sum 
model predictions different data sets model true 
predictions described probability distribution djh probability 
observe actual data bayes rule describes update beliefs models light data 
plausibility model observed written jd obtained multiplying quantities plausible thought data arrived second djh model predicted data 
symbols bayes rule written jd djh denominator normalising constant final beliefs jd add 
bayesian addresses inference problem equation 
hard line bayesian position cox axioms prove consistent inference bayesian inference methods pain inconsistency 
develop moderate position bayesian method important tool alongside pragmatic modelling tools 
bayesian methods adaptive models demonstrate simultaneous application bayesian non bayesian methods leads insights obtained tool 
neural networks need occam razor 
research neural networks motivated observation brain connectionist computational architecture brain composed simple devices neurons massively interconnected computational abilities brain emergent phenomenon arising cooperative interactions simple components 
workers field neural networks create novel connectionist devices try understand brain try create new useful tools tasks speech recognition character recognition robotics 
popular neural network algorithm backpropagation capable learning examples 
case neural network viewed black box produces output give input 
output depends input controlled tens thousands knobs black box teacher able twiddle 
object learning process adjust knobs get black box give desired output response input 
inside black box essential discussion usually contains network simple neurons feeding inputs outputs knobs strengths synapses neurons 
imagine feed inputs black box simple encoding piece english text imagine want outputs black box pronunciation piece text simple code defined 
untrained black box piece english text outputs complete garbage compared coded pronunciation wanted produce 
adjust knobs black box little time give piece text input output box little closer 
backpropagation learning algorithm prescription tweak knobs black box achieve precisely goal 
backpropagation performs gradient descent error function 
surprising outcome procedure repeated training dictionary english words black box consisting neurons learn pronounce correctly large fraction words trained perform equally words training set 
device able extract underlying structure examples trained generalise 
backpropagation algorithm applied tasks text pronunciation example earliest successes performance ability human experts obtained 
especially impressive results obtained adaptive optics 
performance algorithms depends considerable number design choices currently rules thumb trial error 
example designing neural network text pronunciation decide neurons architecture black box connected constraints imposed parameters network 
problem occam razor head repeatedly try design choices complex unconstrained neural network nearly chapter 
summary learn examples training set better simpler simpler neural network may better model problem generalise better new examples 
fact performance training set choose different solutions matter plenty data limitless computational resources generate solutions thousands different models different complexities rank evaluating test error reserved test data 
limited resources able data fit models rank 
furthermore find technique automatically optimising choice model design having perform massive computational searches design space 
bayesian framework thesis satisfies desiderata 
overview thesis consists papers 
chapter reviews detail bayesian framework model comparison regularisation due gull skilling studying problem interpolating noisy data set traditional linear models 
chapter demonstrates bayesian methods embody occam razor consistent intuitive quantitative way 
second chapter framework applied neural networks demonstrated toy problem studied bayesian probability theory chooses alternative solutions networks different architectures way succesfully embodies occam razor 
enhancement neural network training methods concerns regularisation 
neural networks perform poorly parameters weights network blow large values order fit details training set 
prevent popular procedure called weight decay training 
objective procedure previously existed setting weight decay rate apart computationally expensive option testing multiple decay rates parallel experiments 
bayesian framework neural network learning yields simple prescription optimising weight decay rate interpreted regularisation constant 
prescription easily approximated implemented line may useful practical tools emerge research 
chapter see combination bayesian non bayesian model assessment techniques draw attention defects hypothesis space helping traverse loop right invent new models 
third chapter information utility functions discussed purpose data selection left hand loop 
evaluation data utility problem relevant scientist data measurements expensive autonomous robot decide explore satisfy pre programmed curiosity environment need evaluate data utility situations data abundant decide data throw away 
information criteria derived chapter promising properties believe final solution data selection problem artefacts may result criteria applied poor models 
fourth chapter applies methods developed papers neural networks solving classification problems regression problems 
simplest important results chapter demonstration careful bayesian methods adaptive models incorporation error bar information outputs classifier give improved predictions 
chapter bayesian occam razor job surprisingly 
chapter short note extending framework chapters allow modelling input dependent noise level 
maximum likelihood solution problem singularities interpolant fits data exactly bayesian solution naturally avoids problems 
final chapter reflect strengths weaknesses bayesian approach adaptive modelling open questions frontiers facing framework 
relevance biology intended shed direct light functioning biological neural networks 
clear biological neural networks solved occam razor problem expert adaptive modelling systems 
believe understand brain prerequisite understand problems solved 
need understand model infer 
course expect brain embodies equations thesis am sure nature far elegant solutions problems 
hope bayesian normative theory learning serve guide trying elucidate learning performed natural systems 
chapter bayesian interpolation bayesian analysis laplace bayesian method model comparison developed depth 
chapter bayesian approach regularisation model comparison demonstrated studying inference problem interpolating noisy data 
concepts methods described quite general applied data modelling problems 
regularising constants set examining posterior probability distribution 
alternative regularisers priors alternative basis sets objectively compared evaluating evidence 
occam razor automatically embodied process 
way bayes infers values regularising constants noise levels elegant interpretation terms effective number parameters determined data set 
framework due gull skilling 
data modelling occam razor science central task develop compare models account data gathered 
particular true problems learning pattern classification interpolation clustering 
levels inference involved task data modelling 
level inference assume models invented true fit model data 
typically model includes free parameters fitting model data involves inferring values parameters probably take data 
results inference summarised probable parameter values error bars parameters 
repeated model 
second level inference task model comparison 
wish compare models light data assign sort preference ranking alternatives 
chapter ph thesis bayesian methods adaptive models david mackay california institute technology submitted december 
note levels inference distinct decision theory 
goal inference defined hypothesis space particular data set assign probabilities hypotheses 
decision theory typically chooses alternative actions basis probabilities minimise expectation loss function 
chapter concerns inference loss functions utilities involved 
misconception concerns relationship model comparison model choice 
emphasising bayesian method model comparison mean imply correct action choose probable model 
right way bayesian predictions integrate model space 
bayesian methods adaptive models gather data create alternative models gamma gamma fit model data gamma gamma assign preferences alternative models choose data gather gather data decide create new models create new models choose actions oe gamma gamma gamma psi bayesian inference fits data modelling process 
illustrates abstraction part scientific process data collected modelled 
particular applies pattern classification learning interpolation 
double framed boxes denote steps involve inference 
steps bayes rule 
bayes tell invent models example 
box fitting model data task inferring model parameters model data 
bayes may find probable parameter values error bars parameters 
result applying bayes problem little different answers orthodox statistics 
second inference task model comparison light data bayes class 
second inference problem requires quantitative occam razor penalise complex models 
bayes assign objective preferences alternative models way automatically embodies occam razor 
example consider task interpolating noisy data set 
data set interpolated splines model radial basis functions polynomials feedforward neural networks 
level inference take model individually find best fit interpolant model 
second level inference want rank alternative models state particular data set example splines probably best interpolation model interpolant modelled polynomial probably cubic 
bayesian methods able consistently quantitatively solve inference tasks 
popular myth states bayesian methods differ orthodox known frequentist sampling theory statistical methods inclusion subjective priors arbitrary difficult assign usually don difference 
true level inference bayesian results differ little outcome orthodox attack 
widely may model choices reasons computational economy models needed give sufficiently accurate approximation ideal bayesian solution 
chapter 
bayesian interpolation evidence bayes embodies occam razor gives basic intuition complex models penalised 
horizontal axis represents space possible data sets bayes rule rewards models proportion predicted data occurred 
predictions quantified normalised probability distribution probability data model djh called evidence simple model limited range predictions shown djh powerful model example free parameters able predict greater variety data sets 
means predict data sets region strongly assume equal prior probabilities assigned models 
data set falls region powerful model probable model 
appreciated bayes performs second level inference 
bayesian methods totally different orthodox sampling theory methods 
regression density estimation discussed statistics texts example task model comparison virtually ignored general orthodox method exists solving problem 
model comparison difficult task possible simply choose model fits data best complex models fit data better maximum likelihood model choice lead inevitably implausible parameterised models generalise poorly 
occam razor principle states unnecessarily complex models preferred simpler ones 
bayesian methods automatically quantitatively embody occam razor ad hoc penalty terms 
complex models automatically self penalising bayes rule 
gives basic intuition expected rest chapter explore property depth 
bayesian methods simultaneously conceived bayes laplace laid depth cambridge sir harold jeffreys 
logical basis bayesian probabilities measures plausibility subsequently established cox proved consistent inference closed hypothesis space mapped probabilities 
general review bayesian philosophy reader encouraged read excellent papers jaynes loredo reprinted text box tiao 
jeffreys emphasis bayesian probability theory formally utilize prior information perform inference way explicit prior knowledge ignorance orthodox methods omit 
jeffreys laid foundation bayesian model comparison involve emphasis prior information emphasises getting maximal information data 
jeffreys applied theory simple model comparison problems geophysics example testing single additional parameter justified data 
jeffreys model comparison methods applied bayesian methods adaptive models extended economics literature small number statisticians 
aspect bayesian analysis developed applied complex problems fields 
chapter review bayesian model comparison regularisation noise estimation studying problem interpolating noisy data 
bayesian framework describe tasks due gull skilling bayesian methods achieve state art image reconstruction 
approach regularisation developed part szeliski 
bayesian model comparison discussed smith spiegelhalter bretthorst bayesian methods push back limits nmr signal detection 
bayesian theory underlies unsupervised classification system autoclass 
fact bayesian model comparison embodies occam razor rediscovered kashyap context modelling time series includes thorough discussion bayesian model comparison different orthodox hypothesis testing 
earliest applications sophisticated bayesian methods model comparison real data patrick wallace fascinating competing models accounting stone circle geometry compared description length framework equivalent bayes 
pleasing note current appearance increasing number publications bayesian model comparison 
quantities data collected science engineering continue increase computational power techniques available model data multiply believe bayesian methods prove important tool refining modelling abilities 
hope review help introduce techniques neural modelling community 
chapter demonstrate techniques fruitfully applied backpropagation neural networks 
chapter show framework relates task selecting gather data gain maximal information models 
evidence occam factor write bayes rule levels inference described see explicitly bayesian model comparison works 
model stands hypothesis assumed vector parameters model defined functional form probability distributions prior distribution wjh states values model parameters plausibly take predictions djw model data parameters particular value note models parameterisation different priors parameters defined different models 

model fitting 
level inference assume model true infer model parameters data bayes rule posterior probability parameters wjd djw wjh djh words posterior likelihood theta prior evidence chapter 
bayesian interpolation normalising constant djh commonly ignored irrelevant level inference choice important second level inference name evidence common gradient methods find maximum posterior defines probable value parameters wmp common summarise posterior distribution value wmp error bars best fit parameters 
error bars obtained curvature posterior writing hessian log wjd taylor expanding log posterior deltaw gamma wmp wjd wmp jd exp gamma deltaw deltaw see posterior locally approximated gaussian covariance matrix error bars gamma 
model comparison 
second level inference wish infer model plausible data 
posterior probability model jd djh notice data dependent term djh evidence appeared normalising constant 
second term subjective prior hypothesis space expresses plausible thought alternative models data arrived 
see subjective part inference typically overwhelmed objective term evidence 
assuming reason assign strongly differing priors alternative models models ranked evaluating evidence 
equation normalised data modelling process may develop new models data arrived inadequacy models detected example 
start completely defined hypothesis space 
inference open ended continually seek probable models account data gather 
new models compared previous models evaluating evidence 
key concept chapter assign preference alternative models bayesian evaluates evidence djh 
concept general evidence evaluated parametric non parametric models alike data modelling task regression problem classification problem density estimation problem evidence bayesian transportable quantity comparing alternative models 
cases evidence naturally embodies occam razor examine works shortly 
course evidence story reason assign unequal priors alternative models 
evidence model comparison equivalent maximum likelihood parameter estimation 
classic example approximation depend problem solving 
interpolation models discussed chapter single maximum posterior distribution gaussian approximation exact 
general statistical models expect posterior dominated locally gaussian peaks account central limit theorem 
multiple maxima arise complex models complicate analysis bayesian methods successfully applied 
bayesian methods adaptive models sure thing hypothesis fl jaynes hypothesis data set precise data set occurred evidence sure thing hypothesis huge 
sure thing belongs immense class similar hypotheses assigned correspondingly tiny prior probabilities posterior probability sure thing negligible alongside sensible model 
models sure thing rarely seriously proposed real life models developed clearly need think precisely priors appropriate 
patrick wallace studying geometry ancient stone circles people proposed extremely elaborate theories discuss practical method assigning relative prior probabilities alternative models evaluating lengths computer programs decode data previously encoded model 
procedure introduces second sort occam razor inference prior bias complex models 
include prior biases address data preference alternative models evidence occam razor embodies 
limit large quantities data objective occam razor important 
modern bayesian approach priors pointed emphasis modern bayesian approach inclusion priors inference 
significant subjective prior entire chapter 
problems significant subjective priors arise see 
emphasis idea consistent degrees preference alternative hypotheses represented probabilities relative preferences models assigned evaluating probabilities 
historically bayesian analysis accompanied methods right prior wjh problem example principles insufficient reason maximum entropy 
modern bayesian take fundamentalist attitude assigning right priors different priors tried allowing data inform appropriate 
particular prior corresponds different hypothesis way world compare alternative hypotheses light data evaluating evidence 
way alternative regularisers compared example 
try model obtain awful predictions learnt 
failure bayesian prediction opportunity learn able come back data set new models new priors example 
evaluating evidence explicitly study evidence gain insight bayesian occam razor works 
evidence normalising constant equation jh djw wjh dw problems including interpolation common posterior wjd djw wjh strong peak probable parameters wmp 
evidence approximated height peak integrand djw wjh times width deltaw word box tiao counted modern bayesians 
chapter 
bayesian interpolation wmp deltaw delta wjh wjd occam factor shows quantities determine occam factor hypothesis having single parameter prior distribution dotted line parameter width delta posterior distribution solid line single peak wmp characteristic width deltaw 
occam factor deltaw delta jh deltaw evidence best fit likelihood occam factor evidence best fit likelihood model achieve multiplying occam factor term magnitude penalises having parameter interpretation occam factor quantity deltaw posterior uncertainty imagine simplicity prior wjh uniform large interval delta representing range values thought possible data arrived 
delta occam factor deltaw delta ratio posterior accessible volume parameter space prior accessible volume factor hypothesis space collapses data arrive 
model viewed composed certain number equivalent submodels survives data arrive 
occam factor inverse number 
log occam factor interpreted amount information gain model data arrive 
typically complex model parameters free vary large range delta penalised larger occam factor simpler model 
occam factor provides penalty models finely tuned fit data occam factor promotes models required precision parameters deltaw coarse 
occam factor measure complexity model dimension algorithmic complexity relates complexity predictions model data space depends number data points properties data set 
model achieves greatest evidence determined trade minimising natural complexity measure minimising data misfit 
bayesian methods adaptive models occam factor parameters dimensional posterior approximated gaussian occam factor obtained determinant gaussian covariance matrix jh det gamma evidence best fit likelihood occam factor log wjd hessian evaluated calculated error bars wmp amount data collected increases gaussian approximation expected increasingly accurate account central limit theorem 
linear interpolation models discussed chapter gaussian expression exact comments ffl bayesian model selection simple extension maximum likelihood model selection evidence obtained multiplying best fit likelihood occam factor 
evaluate occam factor need hessian gaussian approximation 
bayesian method model comparison evaluating evidence computationally demanding task finding model best fit parameters error bars 
ffl common degeneracies models parameters equivalent parameters affecting likelihood 
cases right hand side equation multiplied degeneracy wmp give correct estimate evidence 
ffl minimum description length mdl methods closely related bayesian framework 
log evidence log djh number bits ideal shortest message encodes data model akaike criterion originally derived predictor generalisation error viewed schwartz approximation mdl bayes 
implementation mdl necessitates approximations evaluating length ideal shortest message 
earliest complex model comparison involved mdl framework mdl apparent advantages approximate evidence directly 
ffl emphasised occam factor computationally complex model 
evidence measure plausibility model 
cpu time takes model certainly interesting issue bias decisions simpler models bayes rule address issue 
choosing models basis function calls need exercise decision theory addressed chapter 
probabilities described inferred optimal actions chosen standard decision theory suitable utility function 
chapter 
bayesian interpolation noisy interpolation problem bayesian interpolation noise free data studied skilling 
chapter study problem interpolating data dependent variables assumed noisy task known regression curve fitting signal estimation neural networks community learning 
am examining case independent variables noisy 
different difficult problem studied case straight line fitting gull 
assume data set interpolated set pairs label running pairs 
simplicity treat scalars method generalises multidimensional case 
define linear interpolation model set fixed basis functions foe chosen interpolated function assumed form oe parameters inferred data 
data set modelled deviating mapping additive noise process modelled zero mean gaussian noise standard deviation oe probability data parameters jw fi exp gammafie djw zd fi fi oe ed gamma zd fi jw fi called likelihood 
known finding maximum likelihood parameters ml may ill posed problem 
minimises ed underdetermined depends sensitively details noise data maximum likelihood interpolant cases oscillates wildly fit noise 
clear complete interpolation model need prior expresses sort smoothness expect interpolant 
model may prior form ff exp ff example functional dx regulariser cubic spline interpolation 
parameter ff measure smooth expected 
prior written prior parameters exp zw ff case adaptive basis functions known feedforward neural networks examined chapter 
strictly probability written fi interpolation models predict distribution input variables liberty notation taken thesis 
strictly particular prior may improper form constrained prior 
bayesian methods adaptive models zw exp 
ew commonly referred regularising function 
interpolation model complete consisting choice basis functions noise model parameter fi prior regulariser regularising constant ff 
particular settings hyperparameters ff fi viewed sub models level inference ff fi known posterior probability parameters wjd ff fi djw fi djff fi writing ffe fie posterior wjd ff fi exp gammam zm ff fi zm ff fi exp gammam 
see minimising combined objective function corresponds finding probable interpolant wmp error bars best fit interpolant obtained hessian rrm evaluated wmp known bayesian view regularisation known maximum penalised likelihood ridge regression 
bayesian methods provide far just interpretation regularisation 
described far just levels inference 
second level described sections model comparison splits second third level problem interpolation model continuum sub models different values ff fi 
second level bayes allows objectively assign values ff fi commonly unknown priori 
third bayes enables quantitatively rank alternative basis sets alternative regularisers priors principle alternative noise models furthermore quantitatively compare interpolation model fa rg interpolation learning models neural networks similar bayesian approach applied 
second third level inference successfully executed occam razor 
bayesian theory second third levels inference worked chapter goal review framework 
section describe bayesian method inferring ff fi section describe bayesian model comparison interpolation problem 
inference problems solved evaluation appropriate evidence 
selection parameters ff fi regulariser ff omitted conditioning variables likelihood data distribution depend prior known 
similarly prior depend fi name stands misfit demonstrated natural measure misfit fied error bars represent uncertainty interpolant confused typical scatter noisy data points relative interpolant 
bayesian inference slightly non gaussian distribution performed box tiao 
chapter 
bayesian interpolation interpolant data interpolant data interpolant error bars data best interpolant depends ff figures introduce data set interpolated variety models chapter 
notice density data points uniform axis 
figures data set interpolated radial basis function model basis equally spaced cauchy functions radius 
regulariser ew coefficients basis functions 
shows probable interpolant different value ff gamma note extreme values data overfitted respectively 
assuming flat prior ff probable value ff 
probable interpolant displayed oe error bars represent uncertain interpolant point assumption interpolation model value ff correct 
notice error bars increase magnitude data sparse 
error bars get bigger near datapoint close radial basis function model expect sharp discontinuities error bars obtained assuming model correct point interpreted improbable outlier 
bayesian methods adaptive models typically ff known priori fi unknown 
ff varied properties best fit probable interpolant vary 
assume prior encourages smoothness imagine interpolate large value ff constrain interpolant smooth flat fit data 
ff decreased interpolant starts fit data better 
ff smaller interpolant oscillates wildly overfit noise data 
choice best value ff occam razor problem large values ff correspond simple models constrained precise predictions saying interpolant expected extreme curvature tiny value ff corresponds powerful flexible model says interpolant prior belief smoothness weak 
task find value ff small data fitted small overfitted 
severely ill posed problems deconvolution precise value regularising parameter increasingly important 
orthodox statistics ways assigning values parameters example misfit criteria test data cross validation 
gull demonstrated popular misfit criteria incorrect bayes sets parameters 
test data may unreliable technique large quantities data available 
cross validation orthodox method choice discussed section chapter 
explain bayesian method inferring ff fi reviewing statistics misfit 
misfit effect parameter measurements independent gaussian variables mean standard deviation oe statistic gamma oe measure misfit 
known priori expectation sigma fitted data setting degree freedom expectation gamma 
second case measured parameter 
parameter determined data way unavoidable parameter fits noise data 
expectation reduced 
basis distinction oe oe gamma buttons calculator 
common distinction ignored cases interpolation number free parameters similar number data points essential find analogous distinction 
demonstrated bayesian choices ff fi simply expressed terms effective number measured parameters fl derived 
misfit criteria principles set parameters ff fi requiring particular value 
discrepancy principle requires principle requires gamma number free parameters 
find intuitive misfit criterion arises probable value fi hand bayesian choice ff unrelated value misfit 
bayesian choice ff fi infer data value ff fi bayesians evaluate posterior probability distribution ff djff fi ff djh chapter 
bayesian interpolation data dependent term djff fi appeared earlier normalising constant equation called evidence ff fi 
similarly normalising constant called evidence turn compare alternative models fa rg light data 
ff flat prior corresponds statement don know value ff fi evidence function assign preference alternative values ff fi 
terms normalising constants defined earlier djff fi zm ff fi zw ff fi occam razor implicit formula ff small large freedom prior range possible values automatically penalised consequent large value zw models fit data achieve large value zm optimum value ff achieves compromise fitting data simple model 
assign preference ff fi computational task evaluate integrals zm zw zd come back task moment 
sounds determining prior data arrived 
heard preceding explanation bayesian regularisation prior chosen ensemble possible priors data arrived 
precise described probable value ff selected prior corresponding value ff infer interpolant 
bayes infer interpolant 
combined ensemble priors define prior integrate ensemble inference 
happens follow proper approach 
preceding method probable prior emerge approximation 
true posterior wjd obtained integrating ff fi wjd wjd ff fi ff words posterior probability written linear combination posteriors values ff fi 
posterior density weighted probability ff fi data appeared 
means ff dominant peak ff fi true posterior wjd dominated density wjd ff fi 
long properties posterior wjd ff fi change rapidly ff fi near ff fi peak ff strong justified approximation wjd wjd ff fi approximation valid conditions footnote 
matter ongoing research develop computational methods cases approximation invalid skilling personal communication neal personal communication 
cases including linear models chapter integral performed ff fi scale parameters prior understood flat prior log ff log fi 
remarkable laplace got right inferring mean laplacian distribution inferred posterior probability nuisance parameter fi attempted integrate nuisance parameter equation 
bayesian methods adaptive models alpha log evidence data log volume ratio alpha log evidence test error test error gamma choosing ff evidence function ff radial basis function model graph shows log evidence function ff shows functions log evidence data misfit fie weight penalty term ffe log volume ratio det gamma zw ff 
criteria optimising ff graph shows log evidence function ff functions intersection locates evidence maximum number parameter measurements fl shown test error rescaled test sets finding test error minimum alternative criterion setting ff 
test sets twice large size interpolated data set 
note point fl clear unambiguous said minima test energies 
evidence gives ff oe confidence interval 
test error minima widely distributed finite sample noise 
analytically 
chosen approximations regardless approximations give clearer intuition bayesian methods solve regularisation problems approximations applicable cases analytic solution approximations relate closely alternative regularisation methods seek find optimal values ff fi 
find joint optimum ff fi 
satisfactory simply maximise likelihood posterior probability simultaneously ff fi posterior likelihood skew peaks maximum likelihood value parameters place posterior probability 
get feeling familiar problem examine posterior probability parameters gaussian oe samples maximum likelihood value oe oe probable value oe integrating oe gamma emphasised distinction prior parameters ff fi flat 
process marginalisation corrects bias maximum likelihood maximum posteriori 
chapter 
bayesian interpolation evaluating evidence return train thought equation 
evaluate evidence ff fi want find integrals zm zw zd typically difficult integral evaluate zm zm ff fi exp gammam ff fi regulariser quadratic functional favourites ed ew quadratic functions evaluate zm exactly 
letting ffc fib wmp gamma wmp gamma wmp wmp fia gamma bw ml means zm gaussian integral zm gammam mp det gamma cases regulariser quadratic example entropy gaussian approximation 
write log evidence ff fi log djff fi mp gamma fie mp gamma log det gamma log zw ff gamma log zd fi log term fie mp represents misfit interpolant data 
terms mp gamma log det gamma log zw ff constitute log occam factor penalising small values ff ratio det gamma zw ff ratio posterior accessible volume parameter space prior accessible volume term ffe mp measures far wmp null value 
illustrates behaviour various terms function ff radial basis function model illustrated 
just proceed evaluate evidence numerically function ff fi deep fruitful understanding problem possible 
properties evidence maximum maximum ff fi djff fi zm ff fi ff fi remarkable properties give deeper insight bayesian approach 
results section useful numerically intuitively 
gull transform basis hessian ew identity transformation simple case quadratic ew rotate eigenvector basis stretch axes quadratic form ew homogeneous 
natural basis prior 
continue refer parameter vector basis ew rrm differentiate log evidence respect ff fi find condition satisfied maximum 
log evidence log djff fi mp gamma fie mp gamma log det log ff log fi gamma log differentiating respect ff need evaluate dff log det ffi fib dff log det trace gamma da dff trace gamma gamma bayesian methods adaptive models ml wmp bad parameter measurements components parameter space directions parallel eigenvectors data matrix circle represents characteristic prior distribution ellipse represents characteristic contour likelihood centred maximum likelihood solution wml wmp represents probable parameter vector 
direction small compared ff data strong preference value poorly measured parameter term ff close zero 
direction large determined data term ff close 
result exact ew ed quadratic 
result approximation omitting terms ff 
differentiating setting derivative zero obtain condition probable value ff ffe mp gamma gamma quantity left dimensionless measure amount structure introduced parameters data fitted parameters differ null value 
interpreted parameters equal oe ff oe quantity right called number parameter measurements fl value written terms eigenvalues fib subscript runs eigenvectors 
eigenvalues ff fl gamma ff gamma gamma ff ff ff eigenvalue measures strongly parameter determined data 
constant ff measures strongly parameters determined prior 
ath term fl ff number measures strength data relative prior direction components wmp fl mla direction parameter space small compared ff contribute number parameter measurements 
fl measure effective number parameters determined data 
ff fi fl increases condition probable value ff interpreted estimation variance oe gaussian distribution weights drawn fl effective samples distribution oe fl 
concept important locating optimum value ff fl parameter measurements expected contribute reduction data misfit occurs model fitted noisy data 
process fitting chapter 
bayesian interpolation data unavoidable fitting model noise occur components noise indistinguishable real data 
typically unit noise fitted determined parameter 
poorly determined parameters determined regulariser reduce way 
examine concept enters bayesian choice fi 
recall expectation misfit true interpolant data know true interpolant misfit measure access inferred interpolant data fie discrepancy principle orthodox statistics states model parameters adjusted un regularised squares regression suggests estimate noise level set gamma number free parameters 
find opinion bayes rule matter 
differentiate log evidence respect fi obtain setting derivative zero fie gamma fl probable noise estimate fi satisfy gamma gammafl bayesian estimate noise level naturally takes account fact parameters determined data inevitably suppress noise data poorly measured parameters 
quantity gamma fl may called effective number degrees freedom 
note value enters determination fi misfit criteria role bayesian choice ff 
summary optimum value ff fi fl gamma fl 
notice implies total misfit fied satisfies simple equation interpolant resulting bayesian choice ff illustrated 
illustrates functions involved bayesian choice ff compares test error approach 
demonstration bayesian choice fi omitted straightforward fi fixed true value demonstrations chapter 
inference input dependent noise level fi demonstrated publication 
results generalise case separate regularisers independent regularising constants fff 
case regulariser number parameter measurements fl associated 
multiple regularisers neural networks chapter 
finding evidence maximum head approach involve evaluating det searching ff fi results enable speed search example re estimation formulae ff fl replace evaluation det evaluation gamma large dimensional problems task demanding skilling developed methods estimating gamma statistically time 
model comparison rank alternative basis sets noise models regularisers priors light data examine posterior probabilities alternative models fa rg hjd djh bayesian methods adaptive models data dependent term evidence appeared earlier normalising constant evaluated integrating evidence ff fi djh djff fi ff dff dfi assuming reason assign strongly differing priors alternative models ranked just examining evidence 
evidence compared evidence equivalent bayesian analysis learning interpolation models allow data assign preference alternative models 
notice pointed earlier modern bayesian framework includes emphasis defining right prior ought interpolate 
invent priors regularisers want allow data tell prior probable 
having said experience recommends maximum entropy principle respected guides consulted inventing priors see example 
evaluating evidence ff fi vary single evidence maximum obtained ff fi quadratic ed ew 
evidence maximum approximated separable gaussian differentiating twice obtain gaussian error bars log ff log fi delta log ff fl delta log fi gamma fl putting error bars obtain evidence 
djh djff fi ff ff fi prior ff assigned 
time chapter met infamous subjective priors supposed plague bayesian methods 
answers question 
coherent method assigning preference alternatives implicitly assign priors 
bayesians adopt healthy attitude sweeping carpet 
thought reasonable values usually assigned subjective priors degree reasonable subjectivity assignments quantified sensitivity inferences priors quantified 
example reasonable prior unknown standard deviation states oe unknown range sigma orders magnitude 
prior contributes subjectivity sigma value log evidence 
degree subjectivity negligible compared log evidence differences 
noisy interpolation example models considered include free parameters ff fi 
chapter need assign value ff assume flat prior flat log ff log fi ff fi scale parameters cancels compare alternative interpolation models 
demonstration demonstrations dimensional data sets imitation 
data set discontinuities derivative second smoother approximation valid fl ae spectrum eigenvalues fib number eigenvalues fold ff fl 
analytic methods performing integrals fi 
chapter 
bayesian interpolation data set 
demonstrations fi left free parameter fixed known true value 
error bars model interpolant bayesian method setting ff assuming single model correct demonstrated quantified error bars placed probable interpolant 
method evaluating error bars posterior covariance matrix parameters gamma get variance linear function parameters oe error bars single point var oe gamma oe 
error bars directly related expected generalisation error assuming model true evaluated 
error bars related expected information gain data point chapter 
access full covariance information entire interpolant just pointwise error bars 
possible visualise joint error bars interpolant making typical samples posterior distribution performing random walk posterior bubble parameter space 
shows data set interpolated typical interpolants random sampling posterior distribution 
error bar properties assumption model correct possible true interpolant lie significantly outside error bars poor model 
model comparison section bayesian model comparison demonstrated models differing number free parameters example polynomials different degrees comparisons models disparate splines radial basis functions feedforward neural networks 
characters models illustrated shows typical sample 
individual model value ff optimised evidence evaluated integrating ff gaussian approximation 
logarithms base legendre polynomials occam razor number basis functions shows evidence legendre polynomials different degrees data set basis functions chosen orthonormal interval enclosing data regulariser form ew 
notice evidence maximum obtained certain number terms evidence starts decrease 
bayesian occam razor 
additional terms model powerful able predictions 
flexibility automatically penalised 
notice characteristic shape occam hill 
left hill steep simple models fail fit data penalty data scales number data measurements 
side hill steep log occam factors scale log number parameters 
note table value maximum evidence achieved models move alternative models 
choice orthonormal legendre polynomials described motivated maximum entropy argument 
models polynomial basis sets tried 
motivated basis sets hermite polynomials occam factors far bigger evidence substantially smaller 
size bayesian methods adaptive models number basis functions evidence legendre polynomials log evidence number basis functions cauchy functions gaussians number coefficients test error number coefficients evidence data set see table log evidence legendre polynomials 
notice evidence maximum 
gentle slope right due occam factors penalise increasing complexity model 
log evidence radial basis function models 
notice occam penalty additional coefficients models increased density radial basis functions model powerful 
oscillations evidence due details basis functions relative data points 
log evidence splines 
evidence shown alternative splines regularisers see text 
representation spline model obtained limit infinite number coefficients 
example yields cubic splines model 
test error splines 
number data points test set number data points training set 
axis shows ed value ed true interpolant expectation sigma 
chapter 
bayesian interpolation table evidence models interpolating data sets logs natural 
evidence djh density space absolute value log evidence arbitrary additive constant 
differences values log evidences relevant relating directly probability ratios 
data set data set model best parameter values log evidence best parameter values log evidence legendre polynomials gaussian radial basis functions sigma sigma cauchy radial basis functions sigma sigma splines splines splines splines splines hermite functions neural networks neurons neurons occam factor increases rapidly parameterisation generally sign space alternative models poorly matched problem 
fixed radial basis functions radial basis function kernel model basis functions oe gammax equally spaced range interest 
examine choices gaussian cauchy function quantitatively compare alternative models spatial correlation data set evaluating evidence 
regulariser ew note model includes new free parameter demonstrations parameter set probable value value maximises evidence 
penalise free parameter occam factor included log delta log delta log posterior uncertainty log log prior log subjective small degree log sigma 
radial basis function model intrinsic correlation model charter gull skilling 
shows evidence function number basis functions note models increasing occam penalty large numbers parameters 
reason extra parameters model powerful fixed ff 
increased density basis functions enable model significant new predictions kernel band limits possible interpolants 
bayesian methods adaptive models typical interpolants data data set interpolated splines 
data set shown typical interpolants drawn posterior probability distribution 
contrast probable interpolant shown pointwise error bars 
splines occam razor choice regulariser splines models implemented follows basis functions fourier set cos hx sin hx 
regulariser ew cos sin limit cubic splines regulariser dx regulariser dx notice non parametric splines model easily put explicit parameterised representation 
splines models include knots 
shows evidence data set function number terms 
notice terms occam razor cases discussed occur increases model powerful occam penalty 
increasing gives rise penalty 
case fence 
increases regulariser opposed strong curvature 
reach model probable data demand sharp discontinuities 
evidence choose order splines regulariser 
data set turns probable value multiples passing radial basis function models described transformed fourier representation splines models 
radial basis function kernel regulariser splines representation ew cos sin gamma discrete fourier transform chapter 
bayesian interpolation typical sample splines error bars prior bubble typical sample cauchy rbf model error bars prior bubble typical sample splines error bars prior bubble typical sample legendre model error bars prior bubble typical sample splines error bars prior bubble sample ordinary polynomial model error bars prior bubble typical samples prior distributions models illustrates character models chapter 
model represented basis functions typical sample prior distribution shown 
regularisation constant case set typical magnitude interpolants similar 
splines 
splines cubic splines 
splines 
splines represented fourier set period 
notice typical sample decreases order spline increases 
cauchy radial basis functions 
basis functions equally spaced scale 
legendre polynomials 
polynomials stretched interval corresponds natural interval 
notice characteristic amplitude diverges boundaries characteristic frequency typical sample increases boundaries 
ordinary polynomials 
illustrates bad results obtained prior assigned 
uniform prior coefficients yields highly non uniform typical sample 
bayesian methods adaptive models results smoother data set shows data set comes smoother interpolant data set table summarises evidence alternative models 
confirm evidence behaves reasonable manner noting differences data sets splines family probable value shifted upwards splines gamma intuitively expect 
legendre polynomials reader may noticed data set modelled legendre polynomials probable number coefficients similar number data points 
data set probable number coefficients confirms evidence prefer polynomial data set behaved way poorly modelled polynomials 
hermite function model poor model data set probable long way times probable 
reason data generated hermite function 
bayes systematically reject truth ask sampling theory question models offer bayes true model data generated possible bayes systematically ensemble possible data sets prefer false model 
clearly worst case analysis bayesian posterior may favour false model 
furthermore skilling demonstrated data sets free form maximum entropy model greater evidence truth possible happen typical case skilling claim 
show answer effect skilling demonstrated systematic 
precise expectation possible data sets log evidence true model greater expectation log evidence fixed model 
proof 
suppose truth single data set arrives compare evidences different fixed model 
models may free parameters irrelevant argument 
intuitively expect evidence djh usually greatest 
examine difference log evidence expectation difference true log djh djh ae djh log djh djh note integral implicitly integrates parameters prior distribution known normalised log minimised setting gibbs theorem 
distinct model expected systematically defeat true model just reason wise bet differently true odds 
ffl result important implications 
gives confidence ability skilling result presumably occurred particular parameter values true model generated data typical prior evaluating evidence model 
case log evidence difference show transient bias true model small quantities data biases usually corrected greater quantities data 
chapter 
bayesian interpolation bayesian methods average identify true model 
secondly provides stringent test numerical implementations bayesian model comparison 
imagine written program evaluates evidence models generate mock data sources simulating evaluate evidences 
systematic bias averaged mock data sets estimated evidence favour false model sure numerical implementation evaluating evidence correctly 
issue illustrated data set truth data set generated quadratic hermite function gamma gammax argument evidence ought probably favour model interpolant coefficient hermite function models 
table shows evidence true hermite function model models 
stated truth considerably probable alternatives 
having demonstrated bayes systematically fail models true examine way framework fail models offered bayes 
comparison generalisation error popular intuitive criterion choosing alternative interpolants different models compare errors test set derive interpolants 
cross validation refined computationally expensive version idea 
method relate evaluation evidence described chapter 
displayed evidence family spline interpolants 
shows corresponding test error measured test set size twice big training data set determine interpolant 
similar comparison 
note trends shown evidence matched trends test error flip graph upside 
particular problem ranks alternative spline models evidence similar ranks test error 
evidence maximum ff surrounded test error minima 
suggests evidence reliable predictor generalisation ability 
necessarily case 
reasons evidence test error correlated 
test error noisy quantity 
necessary devote large quantities data test set obtain reasonable signal noise ratio 
twice data test set difference log ff test error minima exceeds size bayesian confidence interval log ff 
second model greatest evidence expected best model time bayesian inferences uncertain 
point bayes quantifies precisely uncertainties relative values evidence alternative models express plausibility models data underlying assumptions 
third evidence generalisation error 
example imagine models probable interpolants happen identical 
case solutions generalisation error evidence general typically model priori complex suffer larger occam factor smaller evidence 
fourth test error measure performance single probable interpolant evidence measure plausibility entire posterior ensemble bayesian methods adaptive models best fit interpolant 
probably stronger correlation evidence test statistic obtained test statistic average test error posterior ensemble solutions 
ensemble test error easy compute 
fifth interesting reason evidence correlated generalisation error flaw underlying assumptions models compared poor models 
poor regulariser example ill matched statistics world bayesian choice ff best terms generalisation error bayesian methods sensitive poor model assumptions say cross validation 
failure occurs chapter 
attitude failure bayesian prediction 
failure evidence mean discard bayes rule generalisation error criterion choosing ff 
failure opportunity learn healthy scientist actively searches failures yield insights defects current model 
detection failure evaluating generalisation error example motivates search new models fail way example alternative regularisers tried model data probable 
uses generalisation error criterion model comparison denied mechanism learning 
development maximum entropy image deconvolution held years bayesian choice ff bayesian choice ff results obtained making clear poor regulariser motivated immediate search alternative priors new probable priors discovered search heart state art image deconvolution 
similarity regularisation early stopping parameterised model fitted data set gradient descent data error noted model generalisation error passes minimum decreasing monotonically 
known learning neural networks community researchers advocate early stopping stopping gradient descent data error minimum reached try obtain solutions smaller generalisation error 
author believes learning viewed symptom model ill matched data set appropriate response patch bad model search models better matched data 
particular models incorporating simple regularisers expected give results qualitatively similar results early stopping 
seen examining 
regulariser moves minimum objective function ml wmp strength regulariser ff increased wmp follows knee shaped trajectory ml origin typical solution wmp shown 
hand gradient descent likelihood data error typical initial condition close origin gradient descent follow similar knee shaped trajectory 
qualitatively similar solutions expected increasingly early stopping increasingly strong regularisation complete minimisation 
regularisation preferred robust repeatable comprehensible procedure 
chapter 
bayesian interpolation admitting neural networks canon bayesian interpolation models chapter discuss apply bayesian framework feedforward neural networks 
preliminary results methods included table 
assuming approximations valid interesting evidence neural nets spiky smooth data sets 
furthermore neural nets spite arbitrariness yield relatively compact model fewer parameters needed specify splines radial basis function solutions 
developed methods bayesian model comparison regularisation 
models ranked evaluating evidence solely data dependent measure intuitively consistently combines model ability fit data complexity 
precise posterior probabilities models depend subjective priors assign terms typically overwhelmed evidence 
regularising constants set maximising evidence 
regularisation problems theory number measured parameters possible perform optimisation line 
interpolation examples discussed evidence set number basis functions polynomial model set characteristic size radial basis function model choose order regulariser spline model rank different models light data 
needed formalise relationship framework pragmatic model comparison technique cross validation 
techniques parallel possible detect flaws underlying assumptions implicit data models 
failures direct search superior models providing powerful tool human learning 
thousands data modelling tasks waiting evidence evaluated 
exciting see learn done 
bayesian methods adaptive models chapter practical bayesian framework backpropagation networks quantitative practical bayesian framework described learning mappings feedforward networks 
framework possible objective comparisons solutions alternative network architectures objective stopping rules network pruning growing procedures objective choice magnitude type weight decay terms additive regularisers penalising large weights measure effective number determined parameters model quantified estimates error bars network parameters network output objective comparisons alternative learning interpolation models splines radial basis functions 
bayesian evidence automatically embodies occam razor penalising flexible complex models 
bayesian approach helps detect poor underlying assumptions learning models 
learning models matched problem correlation generalisation ability bayesian evidence obtained 
gaps backprop knobs black box backprop learning back propagation errors 
generally knobs set rules thumb trial error reserved test data assess generalisation ability sophisticated cross validation 
knobs fall classes parameters change effective learning model example number hidden units weight decay terms parameters concerned function optimisation technique example momentum terms 
chapter concerned making objective choice parameters class ranking alternative solutions learning problem way full available data 
bayesian techniques described theoretically founded practically implementable 
review basic framework learning networks discuss points objective techniques needed 
training set mapping learned set input target pairs fx label running pairs 
chapter ph thesis bayesian methods adaptive models david mackay california institute technology submitted december 
chapter 
bayesian framework backprop networks neural network architecture invented consisting specification number layers number units layer type activation function performed unit available connections units 
set values assigned connections network network defines mapping input activities output activities distance mapping training set measured error function example error entire data set commonly taken ed jw gamma task learning find set connections gives mapping fits training set small error ed hoped learned connections generalise new examples 
plain backpropagation learns performing gradient descent ed space 
modifications include addition momentum term inclusion noise descent process 
efficient optimisation techniques may conjugate gradients variable metric methods 
chapter discuss computational modifications concerned speeding optimisation 
address modifications plain backprop algorithm implicitly explicitly modify objective function decay terms regularisers 
moderately common extra regularising terms ew added ed example terms penalise large weights may introduced hope achieving smoother simpler mapping 
hints fall category additive weight dependent energies 
sample weight energy term ew weight energy may implicit example weight decay subtraction multiple weight change rule corresponds energy 
gradient optimisation minimise combined function ffe fie jw ff fi black box parameters 
constant ff confused momentum parameter introduced backprop context ff decay rate regularising constant 
note ff viewed causing forgetting ed defined error entire data set gradient descent treats data points equally irrespective order acquired 
lacking procedures include host free parameters choice neural network architecture regularising constant ff 
established ways objectively setting parameters rules thumb see examples 
popular way comparing networks trained different parameter values assess performance measuring error unseen test set similar cross validation techniques 
data divided sets training set framework developed chapter apply networks composed neurons regression model compute derivatives outputs respect parameters bayesian methods adaptive models optimise parameters network test set optimise control parameters ff architecture utility techniques determining values parameters ff fi comparing alternative network solutions limited large test set may needed reduce signal noise ratio test error cross validation computationally demanding 
furthermore parameters ff fi question optimise parameters repeating learning possible values parameters test set 
parameters optimised line 
interesting study objective criteria setting free parameters comparing alternative solutions depend data set training 
criteria prove especially important applications total amount data limited doesn want sacrifice data test set 
wish find way data process optimising parameters process optimising control parameters ff chapter describe practical bayesian methods filling holes neural network framework just described 
objective criteria comparing alternative neural network solutions particular different architectures single architecture may minimum objective function large disparity minima plausible choose solution smallest difference great desirable able assign objective preference alternatives 
desirable able assign preferences neural network solutions different numbers hidden units different activation functions 
occam razor problem free parameters model smaller data error ed achieve 
simply choose architecture smallest data error 
lead complex network generalises poorly 
weight decay fully alleviate problem networks hidden units generalise worse weight decay see section 

objective criteria setting decay rate ff 
choice occam razor problem small value ff equation allows weights large overfit noise data 
leads small value data error ed small value base choice ff ed bayesian solution implemented line necessary multiple learning runs different values ff order find best 

objective choice regularising function ew 
objective criteria choosing neural network solution solution different learning interpolation model example splines radial basis functions 
probability connection tishby introduced probabilistic view learning important step solving problems listed 
idea force probabilistic interpretation neural network technique able objective statements 
chapter 
bayesian framework backprop networks interpretation involve addition new arbitrary functions parameters involves assigning meaning functions parameters 
probabilistic framework extends concepts techniques adapted gull skilling bayesian image reconstruction methods 
chapter adopts shift emphasis tishby 
concentrated predicting average generalisation ability network trained task drawn known prior ensemble tasks 
called forward probability 
thesis emphasis quantifying relative plausibilities alternative solutions interpolation classification task task defined single data set produced real world know prior ensemble task comes 
called inverse probability 
thesis avoids language statistical physics partly avoid concepts sound strange language example probability distribution temperature unfamiliar physics probability distribution noise variance innocent counterpart literal terms 
review probabilistic interpretation network learning 
ffl likelihood 
network specified architecture connections viewed making predictions target outputs function input accordance probability distribution jx fi exp gammafie jx zm fi zm fi dt exp gammafie 
error single datum fi measure presumed noise included quadratic error function corresponds assumption includes additive gaussian noise variance oe fi 
symbol denotes implicit noise model 
ffl prior 
prior probability assigned alternative network connection strengths written form exp zw ff zw exp 
ff measure characteristic expected connection magnitude 
ew quadratic specified equation weights expected come gaussian zero mean variance oe ff 
alternative regularisers different energy function ew implicitly correspond alternative hypotheses statistics environment 
ffl posterior probability network connections wjd ff fi exp gamma fie zm ff fi zm ff fi exp gamma fie 
notice exponent expression minus objective function defined 
framework minimisation ffe fie identical finding locally probable parameters wmp minimisation ed identical finding maximum likelihood parameters ml interpretation backpropagation energy functions ed ew parameters ff fi 
bayesian methods adaptive models emphasised probability connections measure plausibility model parameters specified value probability particular algorithm converge framework offers partial enhancements backprop methods levin possible predict average generalisation ability neural networks trained defined class problems 
clear lead practical technique choosing alternative network architectures real data sets 
le cun demonstrated estimate saliency weight change weight deleted 
measure successfully simplify large neural networks 
stopping rule weight deletion offered measuring performance test set 
denker le cun demonstrated hessian assign error bars parameters network outputs 
error bars quantified fi quantified prior knowledge extra data demonstrated 
fact fi estimated training data 
review bayesian regularisation model comparison chapter demonstrated control parameters ff fi assigned bayes alternative interpolation models fa rg compared 
noted satisfactory optimise ff fi finding joint maximum likelihood value ff fi likelihood skew peak maximum located probable values control parameters 
chapter reviewed bayesian choice ff fi neatly expressed terms measure number determined parameters model fl 
chapter assumed significant minimum approximated quadratic 
interpolation models discussed chapter interpreted layer networks fixed non linear layer adaptive linear second layer 
section briefly review bayesian framework retaining assumption 
section discuss framework modified handle neural networks landscape certainly quadratic 
determination ff fi bayes rule posterior probability parameters ff fi jd djff fi ff djh assign uniform prior ff fi quantity interest assigning preferences ff fi term right hand side evidence ff fi written djff fi zm ff fi zw ff fi notation abuses thereof chapter 
chapter 
bayesian framework backprop networks zm zw defined earlier zd gammafi ed simple quadratic energy functions defined equations 
analysis easier complex cases principle handled approach 
number degrees freedom data set number output units times number data pairs number free parameters dimension immediately evaluate gaussian integrals zd zw zd fi zw ff want find zm ff fi exp gammam ff fi 
supposing single minimum function wmp assuming locally approximate quadratic integral zm approximated zm gammam wmp det gamma rrm hessian evaluated wmp maximum djff fi useful properties ffe fl fie gamma fl fl effective number parameters determined data fl ff eigenvalues quadratic form fie natural basis ew comparison different models rank alternative architectures noise models penalty functions ew light data simply evaluate evidence fa rg jh appeared normalising constant 
integrating evidence ff fi djh djff fi ff dff dfi evidence bayesian transportable quantity comparing models light data 
adapting framework neural networks quadratic 
known typically local minima 
network symmetry permutation parameters know share symmetry single minimum belongs family symmetric minima example hidden units single layer non degenerate minimum family size may case significant minima locally quadratic able evaluate zm evaluating significant minimum adding zm number minima unknown approach evaluating zm dubious 
luckily want evaluate zm need evaluate zm order assign posterior probability ff fi entire model evaluate bayesian methods adaptive models evidence alternative entire models 
quite wish neural network perform mapping typically implement neural network time network parameters set particular solution learning problem 
alternatives wish rank different solutions learning problem different minima want evidence function number hidden units able simultaneously implement entire posterior ensemble networks number hidden units 
similarly want posterior ff fi entire posterior ensemble reasonable allow solution minimum choose optimal value parameters 
method chopping complex model space unsupervised classification system autoclass 
having adopted slight shift objective turns set ff fi compare alternative solutions learning problem integral need evaluate local version zm assume posterior probability consists separated islands parameter space centred minimum wish evaluate posterior probability mass islands 
consider minimum located define solution sw ensemble networks neighbourhood symmetric permutations ensemble 
evaluate posterior probability alternative solutions sw parameters ff fi ff fi hjd ff fi zw ff fi ff permutation factor ff fi exp gammam ff fi integral performed neighbourhood minimum refer quantity ff fi zw ff fi evidence ff fi sw parameters ff fi chosen maximise evidence 
quantity want evaluate compare alternative solutions evidence sw sw jh ff fi zw ff fi ff dff dfi thesis uses gaussian approximation gammam det gamma rrm hessian evaluated general ff fi approximation probably unacceptable need accurate small range ff fi close probable value 
regime approximation definitely break number constraints small relative number free parameters large central limit theorem encourages gaussian approximation 
matter research establish large approximation reliable 
obstacles remain prevent evaluating local need evaluate approximate inverse hessian need evaluate approximate determinant trace 
bayesian model comparison performed evaluating comparing evidence alternative models 
gull skilling defined evidence model djh 
existence multiple minima neural network parameter space complicates model comparison 
quantity includes prior sw jh called evidence quantity evaluate compare alternative solutions models 
chapter 
bayesian framework backprop networks typical neural network output 
inset training set output space network 
target outputs displayed small output network oe error bars shown dot surrounded ellipse 
network trained samples regions lower upper half planes inset 
outputs illustrated inputs extending short distance outside training regions bridging gap 
notice error bars get larger perimeter 
increase slightly gap training regions 
pleasing properties obtained diagonal hessian approximation 
solution created layer network hidden units 
denker discussed approximate hessian ed purpose evaluating weight saliency assigning error bars weights network outputs 
hessian evaluated way backpropagation evaluates red see complete algorithm appendix chapter useful approximation 
alternatively evaluated numerical methods example second differences 
third option variable metric methods minimise gradient descent inverse hessian automatically generated search minimum 
important success bayesian method diagonal terms hessian evaluated 
denker method additional complexity 
diagonal approximation strong posterior correlations parameters 
demonstration demonstration examines evidence various neural net solutions small interpolation problem mapping joint robot arm 
cos cos sin sin training set random samples restricted range gaussian noise magnitude added outputs 
bayesian methods adaptive models number hidden units small start large start data error versus number hidden units 
point represents converged neural network trained pair training set 
neural net initialised different random weights different initial value oe ff 
point styles correspond small large initial values oe error shown dimensionless units expectation error relative truth sigma 
solid line gamma number free parameters 
number hidden units small start large start test error versus number hidden units training set test set data points 
test error solutions regulariser shown dimensionless units expectation error relative truth sigma 
chapter 
bayesian framework backprop networks data error alternative solutions test error vs data error 
point represents performance single trained neural network training set test set 
horizontal axis displays data error network performance training data 
small value data error corresponds network learnt training data 
vertical axis displays test error network generalises new examples 
smaller test error better generalisation ability network 
graph illustrates occam problem best generalisation achieved models fit training data best 
number hidden units small start large start log evidence solutions regulariser 
solution evidence evaluated 
notice evidence maximum achieved neural network solutions hidden units 
hidden units quadratic approximations evaluate evidence believed break 
number data points pairs number parameters net hidden units 
bayesian methods adaptive models total number parameters small start large start number determined parameters 
displays fl function network solutions 
gamma small start large start data misfit versus fl 
shows fl line gradient gamma 
right data misfit reduced measured parameter 
model parameters left misfit gets worse greater rate 
chapter 
bayesian framework backprop networks test error small start large start log evidence versus test error regulariser desired correlation evidence test error negative slope 
significant number points lower left violate desired trend failure bayesian prediction 
points violate trend networks significant difference typical weight magnitude layers 
networks learning initialised large value oe regulariser ill matched networks low evidence reflection poor prior hypothesis 
test error small start large start comparison test errors 
illustrates noisy performance measure test error point compares error trained network different test sets 
test sets consist data points distribution training set 
bayesian methods adaptive models neural nets hidden layer sigmoid units linear output units 
optimisation regulariser initially alternative regulariser introduced fi fixed true value enable demonstration properties quantity fl ff allowed adapt locally probable value 
illustrates performance typical neural network trained way 
output accompanied error bars evaluated denker method including diagonal hessian terms 
fi known advance inferred data equation 
solution displayed model estimate fi fact differed negligibly true value displayed error bars fi inferred data 
shows data misfit versus number hidden units 
notice expected data error tends decrease monotonically increasing number parameters 
shows error solutions unseen test set show trend data error 
occam problem illustrated compares test error data error 
data misfit serve criterion choosing solutions 
shows evidence different solutions different numbers hidden units 
notice evidence maximum characteristic shape occam hill steep side parameters shallow side parameters 
quadratic approximations break number parameters big compared number data points 
figures introduce quantity fl discussed chapter number measured parameters 
cases evaluation evidence proves difficult may fl serve useful tool 
example sampling theory predicts addition redundant parameters model reduce unit measured parameter stopping criterion detect point parameters deleted started increase faster gradient decreasing fl 
fl requires prior knowledge noise level fi fi fixed known value demonstrations 
question predictor network quality evidence fact evidence maximum reasonable number hidden units promising 
comparison shows performance solutions unseen test set similar structure evidence 
shows evidence performance test set seen significant number solutions poor evidence perform test set 
wrong 
discuss relationship evidence generalisation ability 
return failure see rectified development new probable regularisers 
relation generalisation error relationship evidence generalisation error close relative cross validation 
correlation certainly expected 
evidence necessarily predictor generalisation error see discussion chapter 
illustrated error test set noisy quantity lot data devoted test set get acceptable signal noise ratio 
furthermore imagine models generated solutions interpolation problem suggestion closely related moody generalised prediction error gpe fl 
chapter 
bayesian framework backprop networks probable interpolants completely identical 
case generalisation error solutions evidence general typically model priori complex suffer larger occam factor smaller evidence 
evidence measure plausibility ensemble networks optimum just optimal network 
evidence generalisation error 
bayesian method fails 
want dismiss utility generalisation error important detecting failures model 
example obtain poor correlation evidence generalisation error bayes fails assign strong preference solutions perform test data able detect attempt correct failures 
failure indicates things case able learn improve numerical inaccuracies evaluation probabilities caused failure alternative models offered bayes poor selection ill matched real world example inappropriate regularisers 
failure detected prompts examine models try discover implicit assumptions model data didn agree alternative models tried data probable 
just met exactly failure 
establish assumption model caused failure learn 
note mechanism human learning available just test error performance criterion 
going test error indication serious mismatch model data 
back demonstration comparing different regularisers demonstrations far regulariser 
equivalent prior expects weights characteristic size 
inconsistent prior input output variables hidden unit activities arbitrarily rescaled mapping performed simple consistency requirement transformations variables imply independent rescaling weights hidden layer output layer 
scales layers weights unrelated inconsistent force characteristic decay rates different classes weights 
inconsistency major cause failure illustrated 
networks deviating substantially desired trend weights output layer far larger weights input layer poor match model implicit regulariser causes evidence solutions small 
failure enables progress insight new regularisers 
alternative prior inconsistent way explained theoretical reasons expect better 
allow data choose evaluating evidence solutions new prior find new prior probable 
second prior independent regularising constants corresponding characteristic magnitudes weights different classes hidden unit weights hidden unit biases output weights biases see 
term ffe replaced ff 
hinton nowlan bayesian methods adaptive models bias bias output layer hidden layer input layer classes weights second prior hidden unit weights 
hidden unit biases 
output unit weights biases 
weights class share decay constant ff number hidden units small start large start derived symmetries detected log evidence versus number hidden units second prior different point styles correspond networks learning initialised small large values oe networks previously trained regulariser subsequently trained second regulariser networks weight symmetry detected cases evidence evaluation possibly reliable 
chapter 
bayesian framework backprop networks test error small start large start derived symmetries detected log evidence second prior versus test error 
correlation evidence test error second prior 
note largest value evidence increased relative smallest test error decreased 
similar prior modelling weights coming gaussian mixture bayesian re estimation techniques update mixture parameters model discovering elegant solutions problems translation invariances 
model achieves better performance task sunspot time series prediction published model 
second prior regularising constant independently adapted probable value evaluating number measured parameters fl associated regularising function finding optimum ff fl increased complexity prior model penalised occam factor new parameter ff see chapter 
preempt questions lines didn weight classes non zero means 
way assigning weight decays just model try evaluating evidence find preference data alternative decay schemes 
new solutions second prior evidence evaluated 
evidence new solutions new prior shown 
notice evidence increased compared evidence prior 
solutions new prior probable factor crunch probable model predictions 
evidence second prior shown test error 
correlation greatly improved 
notice furthermore second prior probable best test error achieved solutions second prior slightly better achieved prior number solutions increased substantially 
bayesian evidence predictor generalisation ability bayesian choice regularisers enabled best solutions 
bayesian methods adaptive models discussion bayesian method founded theoretically works practically remains seen approach scale larger problems 
particular data set evaluation evidence led objectively inconsistent regulariser probable 
evidence maximised networks generalise best showing occam razor successfully embodied ad hoc terms 
furthermore solutions greatest evidence perform better test set solutions 
believe currently technique reliably find identify better solutions training set 
essential success simultaneous bayesian optimisation regularising constants decay terms ff optimisation parameters orthodox search technique cross validation laborious regularising constants easily case larger problems hard imagine search possible 
brings question bayesian calculations scale problem size 
terms number parameters calculation determinant inverse hessian scales note computation needs carried small number times compared immense number derivative calculations involved typical learning session 
large problems may demanding evaluate determinant hessian 
case numerical methods available approximate determinant trace matrix time 
application classification problems chapter far discussed evaluation evidence backprop networks trained interpolation problems 
neural networks trained perform classification tasks 
publication demonstrate bayesian framework model comparison applied problems 
relation dimension papers advocate dimension criterion penalising complex models 
dimension applied classification problems evidence hand evaluated equally easily interpolation classification problems 
dimension worst case measure yields different results bayesian analysis 
example dimension indifferent regularisers value ff regularisers rule absolutely particular network parameters 
dimension assigns complexity model regularised 
set regularising constants ff compare alternative regularisers 
contrast preceding radford neal personal communication pointed possible evaluate gradient validation error respect parameters eval ffc eval wmp delta wmp ffc quantity evaluated backprop second term quadratic approximation gives wmp ffc gamma ic identity matrix weights regularised ff zero 
alternatively radford neal suggested gradients eval ffc efficiently calculated recurrent backpropagation viewing vector activities recurrent network wmp fixed point error eval wish minimise 
levin guyon developed measure effective dimension regularised model 
measure identical fl equation predicted generalisation error vapnik structural risk theory exactly scaling behaviour evidence 
chapter 
bayesian framework backprop networks demonstrations show careful objective choice regulariser ff essential best solutions obtained 
worst case analysis complementary role alongside bayesian methods 
substitute 
tasks needed formalise relationship framework pragmatic model comparison technique cross validation 
moody generalised prediction error gpe interesting contribution direction 
sampling theory approach predicts generalisation error units fl 
evaluated gpe interpolation models chapter demonstration correlation gpe actual test error poor 
needed understand 
gaussian approximation evaluate evidence breaks number data points small compared number parameters 
model problems studied far gaussian approximation break significantly sigma 
matter research characterise failure investigate techniques improving evaluation integral example random walks neighbourhood solution 
expected evaluation evidence provide objective rule deciding network pruning growing procedure stopped careful study idea performed 
interesting see results evaluating evidence networks applied larger real world problems 
appendix numerical methods quick dirty version numerical tasks automatic optimisation ff fi calculation error bars evaluation evidence 
describe cheap approximation solving tasks evaluating hessian 
neglect distinction determined poorly determined parameters obtain update rules ff fi ff fi ed want easy program taste bayesian framework offer try procedure update decay terms 
hessian evaluation hessian needed evaluate fl relates trace gamma evaluate evidence relates det assign error bars network outputs gamma 
methods evaluating approximate analytic method second differences 
approximate analytic method denker backprop bayesian methods adaptive models obtain second derivatives neglecting terms activation function neuron 
hessian built sum outer products gradient vectors mt dy dw denker ignore diagonal terms diagonal approximation 
evaluation fl methods gave similar results approach satisfactory 
evaluation evidence approximate analytic method failed give satisfactory results 
occam factors weak scaling log approximation apparently introduces systematic errors greater 
reason evidence evaluation sensitive errors fl evaluation fl related sum eigenvalues evidence related product errors small eigenvalues product sum 
expect exact analytic evaluation second derivatives resolve 
save programming effort second differences computationally demanding kn analytic approach 
problems errors small eigenvalues possible correct errors detecting eigenvalues smaller theoretically permitted 
demonstrations demonstrations performed follows initial weights random weights drawn gaussian oe 
optimisation algorithm variable metric methods code times sequence values fractional tolerance decreasing gamma gamma loop regularising constants ff allowed adapt accordance re estimation formula ff fl precaution evaluating evidence care taken verify permutation term appropriately set 
may case probably mainly toy problems regulariser hidden units network adopt identical connection values alternatively hidden units switch weights set zero cases permutation term smaller 
cases quadratic approximation perform badly quartic quadratic minima preferable automate deletion redundant units 
chapter information objective functions active data selection learning efficient actively select particularly salient data points 
bayesian learning framework objective functions discussed measure expected informativeness candidate measurements 
alternative specifications want gain information lead different criteria data selection 
criteria depend assumption hypothesis space correct may prove main weakness 
theories data modelling assume data provided source control 
scenarios able actively select training data 
data measurements relatively expensive slow want know look learn possible 
jaynes bayesian reasoning applied problem centuries ago laplace consequence important discoveries celestial mechanics 
second scenario immense amount data wish select subset data points useful purposes 
scenarios benefit ways objectively estimating utility candidate data points 
problem active learning sequential design extensively studied economic theory statistics 
experimental design bayesian framework shannon information objective function studied lindley luttrell 
distinctive feature approach renders optimisation experimental design independent tests applied data loss functions associated decisions 
chapter uses similar information objective functions discusses problem optimal data selection bayesian framework interpolation described chapters 
results chapter direct analogs fedorov quantities involved different chapter ph thesis bayesian methods adaptive models david mackay california institute technology submitted december 
bayesian methods adaptive models interpretations example fedorov dispersion estimator bayesian posterior variance parameter 
directly stimulated presentation john skilling maxent 
neural networks literature active data selection known query learning concentrated slightly different problems baum hwang relates perfectly separable classification problems papers sensible query learning algorithm proposed empirical results algorithm reported baum gives convergence proof 
algorithms human designed clear objective function querying strategy optimises algorithms improved 
contrast chapter discusses noisy interpolation problems derives criteria defined objective functions objective function leads different data selection criterion 
chapter discuss application ideas classification problems 
white study different problem context noise free interpolation assume large amount data gathered principles selecting subset data efficient training entire data set inputs targets consulted iteration decide example add training subset option permitted 
statement problem imagine gathering data form set input output pairs dn fx data modelled interpolant 
interpolation model specifies architecture defines functional dependence interpolant parameters model specifies regulariser prior cost function noise model describing expected relationship may interpolation model may linear non linear chapters described bayesian framework fitting comparing models assuming fixed data set 
chapter discusses framework interpolation relates task selecting data gather 
criterion informative new datum depend interested 
alternatives spring mind 
decided particular interpolation model wish select new data points maximally informative values model parameters take 

alternatively interested getting globally determined interpolant want able predict value interpolant accurately limited region point input space able sample directly 

lastly unsure models best interpolation model want select data give maximal information discriminate models 
chapter study tasks case wish evaluate utility function input location single measurement scalar 
complex task selecting multiple new data points addressed methods generalised solve task discussed 
chapter 
information objective functions similar problem choosing vector outputs measured addressed 
third definitions information gain studied lindley 
cases studied fedorov mainly non bayesian terms 
chapter solutions obtained interpolation problem gaussian approximation cases assuming new datum relatively weak piece information 
common active learning utility evaluated assuming probability distributions defined interpolation model correct 
models assumption may achilles heel approach discussed section 
choice bias inferences 
speculate way choose gather data able bias inferences systematically away truth 
case need inferences way undoes biases account gathered data 
orthodox statistics estimators statistical tests depend sampling strategy 
likelihood principle states inferences depend likelihood actual data received data gathered didn 
bayesian inference consistent principle need undo biases introduced data collecting strategy possible biases introduced long perform inference data gathered 
models concerned estimating distribution output variables input variables allowed look value datum decide include datum data set 
bias inferences distribution tjx 
choice information measure start need select measure information gained unknown variable receive new datum having chosen measure select expected information gain maximal 
measures information suggested shannon entropy properties sensible information measure known 
explore choice task want gain maximal information parameters interpolant probability distributions parameters receive datum 
change entropy distribution deltas sn gamma sn sn log measure argument log dimensionless 
greater deltas information gained case quadratic measure unimportant follows included avoid committing dimensional crimes 
note sign deltas defined information gain corresponds positive deltas 
bayesian methods adaptive models models discussed chapter set measure equal prior quantity sn closely related log occam factor 
alternative information measure cross entropy wp log define gammag obtain positive quantity measure information gain informed true distribution 
information measures equal 
intuitively differ measure flat deltas quantifies probability bubble shrinks new datum arrives incorporates measure bubble moves new datum 
probability distribution shrink certain learnt distribution moves region space 
question information measure appropriate potentially complicated fact consistent additive measure information receive datum datum general ab intriguing complication hinder task base decisions expectations deltas see expectation deltas equal purposes distinction 
result holds independent details models study independent gaussian approximation 
proof deltas evaluate expectation quantities assume probability distribution datum abbreviated comes 
define probability distribution assuming current model complete error bars correct 
means probability distribution total specification model 
conditioning variables right omitted proof 
compare expectations deltas gamma wp wjt log wjt gamma wp wjt log wjt wp wjt log free measure measure 
term gammas gammae dt wp wjt log gammae wp log gammas sn deltas ffl occam factor det gamma exp mp zw ff sn log fl notation chapter 
chapter 
information objective functions candidate information measures equivalent purposes 
proof implicitly demonstrates deltas independent measure 
properties deltas proved 
rest chapter deltas information measure set constant 
maximising total information gain solve task choose expected information gain maximised 
intuitively expect learn interpolant gathering data location error bars interpolant currently greatest 
quadratic approximation confirm intuition 
notation likelihood data defined terms noise level oe fi gamma fi exp gammafie zd ed gamma zd appropriate normalising constant 
likelihood defined dependent noise level fi gamma correlated noise multiple outputs case fi gamma covariance matrix noise 
treated scalar simplicity 
likelihood data combined prior exp zw regularising constant weight decay rate ff corresponds prior expected smoothness interpolant obtain current probability distribution exp gammam zm ffe fie objective function quadratically approximated near probable parameter vector wmp wmp deltaw deltaw deltaw gamma wmp hessian rrm evaluated minimum wmp quadratic approximation 
minima treated distinct models chapter 
need know entropy gaussian distribution easy confirm gammam flat measure log log det gamma aim minimising size joint error bars parameters det gamma small possible 
expanding wmp wmp delta deltaw dependent sensitivity output variable parameter evaluated wmp imagine choose particular input collect new datum 
datum falls region quadratic approximation applies new hessian approximation rr gamma gg expression neglects terms terms exactly zero linear models discussed chapter bayesian methods adaptive models necessarily negligible non linear models neural networks 
notice new hessian independent value datum takes specify information gain deltas datum evaluate just calculating see property datum causes maximally informative 
new entropy sn equal gamma log gamma det delta neglecting additive constants 
determinant analytically evaluated identities gamma gamma gamma fia gamma gg gamma fig gamma det det fig gamma obtain total information gain delta log det log fig gamma product fig gamma term fi tells surprisingly learn information low noise high fi measurement 
second term gamma precisely variance interpolant point datum collected 
result obtain maximal information interpolant take datum point error bars interpolant currently largest assuming noise oe measurements 
rule resulting optimal minimax design criteria 
interpolation models error bars largest extreme points data gathered 
criterion cases lead repeatedly gather data edges input space considered non ideal behaviour necessarily need introduce ad hoc procedure avoid 
reason want repeated sampling edges want know happens 
accordingly derive criteria alternative objective functions value information acquired interpolant defined region interest 
maximising information interpolant region interest come second task 
assume wish gain maximal information value interpolant particular point quadratic approximation uncertainty interpolant gaussian distribution size error bars terms hessian parameters oe gamma evaluated entropy gaussian distribution log oe const 
measurement sensitivity error bars scaled factor gamma ae ae correlation variables ae gamma oe oe oe oe gamma information gain marginal information gain delta log oe gamma log gamma gamma oe oe oe chapter 
information objective functions term gamma maximised sensitivities maximally correlated measured inner product metric defined gamma second task solved case extrapolation single point 
objective function demonstrated criticised section 
generalisation multiple points imagine objective function defined information gained interpolant set points fx points thought representatives region interest example points test set 
case includes generalisation output variable full generalisation optimisation experiment measurements see fedorov luttrell 
preceding objective function information generalised ways lead results 
objective function multiple points obvious objective function joint entropy output variables interested 
set output variables want minimise uncertainty fy runs sequence different input locations set different scalar outputs 
sensitivities outputs parameters covariance matrix values fy gamma matrix disregarding possibility full rank necessitate complex treatment giving similar results joint entropy output variables fy related log det gamma find information gain measurement sensitivity vector identities 
joint information gain delta log det gamma gamma log gamma gamma gamma gamma oe oe row vector gamma measures correlations sensitivities quadratic form vy gamma measures effectively correlations reduce joint uncertainty fy denominator oe oe term favour measurements small uncertainty 
criticism argue joint entropy fy interpolant values appropriate objective function 
simple example illustrate 
imagine number points defining region interest dimensionality parameter space resulting matrix may singular points close typically full rank 
parameter vector values interpolant fy locally linear correspondence 
means change entropy fy identical change entropy 
confirmed substitution gamma gamma ag gamma yields 
bayesian methods adaptive models datum chosen accordance equation maximise expected joint information gain fy exactly choice result obtained maximising criterion expected total information gain section 
clearly choice independent choice fy region interest 
criticism joint entropy restricted case reason objective function achieve want joint entropy decreased measurements introduce correlations predictions fy measurements reduce individual uncertainties predictions 
don want variables fy strongly correlated arbitrary way want small variance subsequently asked predict value able confident predictions 
second objective function multiple points motivates alternative objective function maximise average information gained 
define mean marginal entropy log oe const probability asked predict oe gamma measurement sensitivity vector obtain mean marginal information gain gamma log gamma gamma oe oe oe mean marginal information gain demonstrated criticised section 
simple variations objective function derived 
minimising mean marginal entropy predictions minimise mean marginal entropy predicted noisy variables modelled deviating additive noise variance oe obtain oe replaced oe oe alternative may lead significantly different choices marginal variances oe fall intrinsic variance oe predicted variable 
take approach loss functions require datum choose minimises expectation mean squared error predictions fy oe obtain objective function leading order deltae gamma oe oe increases bias favour reducing variance variables largest oe optimal design 
comment case linear models interesting note linear model oe quadratic penalty functions solutions second tasks depend locations data previously gathered actual data gathered ftg oe independent fi gg independent ftg 
complete data gathering plan drawn start 
non linear model decisions data gather affected previous observations 
chapter 
information objective functions maximising discrimination models quadratic approximation models slightly different gaussian predictions value datum 
measure datum input value tjh normal oe parameters oe obtained interpolation model best fit parameters wmp hessian sensitivity vector wmp oe gamma fi intuitively expect informative measurement value separated possible scale defined oe oe thought confirm expect gain information oe oe differ significantly points occam factor penalising powerful model significant 
define information gain deltas sn gammas gamma log exact calculations deltas analytically possible assume regime small information gain expect measurement give weak likelihood ratio tjh tjh 
regime gamma oe oe assumption take expectation page algebra leads result deltas oe oe gamma oe gamma oe oe oe terms correspond precisely expectations stated 
term favours measurements separated second term favours places oe oe differ 
third task solved 
fedorov similar derivation uses poor approximation loses second term 
demonstration discussion data set consisting points dimensional interpolation problem interpolated hidden unit neural network 
data generated smooth function adding noise standard deviation oe 
neural network adapted data weight decay terms ff controlled methods chapter noise level fi fixed oe data resulting interpolant error bars shown 
expected total information gain change entropy parameters shown function 
just monotonic function size error bars 
shows expected marginal information gain points interest fx gamma 
notice marginal information gain case peaked near point interest expect 
note height peak greatest gamma interpolant oscillates rapidly lower interpolant smoother 
marginal information gain total information gain equal 
bayesian methods adaptive models shows mean marginal information gain points interest fx defined set equally spaced points interval gamma interval training data lie 
mean marginal information gain gradually decreases zero away region interest hoped 
region left characteristic period interpolant similar data spacing expected utility oscillates passes existing data points reasonable 
surprising feature estimated utility region lower data points estimated utility smooth region right 
achilles heel methods approach potential weakness may models defined region interest points fx expected marginal information gain measurement blows 
sigma error bars 
occur information gain estimates utility data point assuming model correct know model approximation tool incorrect possible undesirable behaviour result 
simple example illustrates problem obtained consider modelling data straight line unknown parameter 
imagine want select data obtain model predicts accurately assume model right clearly gain information sample largest possible jxj points give largest signal noise ratio determining assume model correct approximation tool common sense tells sample closer models know incorrect marginal information gain really right answer wrong question 
task research formulate new question answer appropriate approximation model 
mean marginal information gain promising objective function test 
computational complexity computation suggested objective functions moderately cheap inverse hessian gamma obtained models concerned 
nk process number data points number parameters process may performed order evaluate error bars models evaluate evidence evaluate parameter saliencies enable efficient learning 
cost compared cost locating minimum objective function worst case scales nk result quadratic function 
evaluation mean marginal information gain candidate points requires ck cv time number points interest evaluate gamma evaluate dot product vector 
evaluation mean marginal information gain computationally expensive inverse hessian evaluation 
contexts expensive progress exploring possibility reducing calculations smaller time statistical methods 
question efficiently search informative addressed gradient methods constructed shows information gain may locally non convex scale defined inter datum spacing 
chapter 
information objective functions interpolant error bars data total information gain marginal information gains mean marginal information gain demonstration total marginal information gain data set interpolant error bars 
expected total information gain marginal information gains 
mean marginal information gain region interest defined equally spaced points interval gamma 
information gains shown scale nats nat log bits 
bayesian methods adaptive models specifications information maximised solution obtained 
solutions apply linear non linear interpolation models depend validity local gaussian approximation 
solution direct analog non bayesian literature generalisations multiple measurements multiple output variables 
case function derived predicts information gain measurement function search optimal value large dimensional input spaces may trivial task 
function serve way reducing size large data set omitting data points expected informative 
function form basis stopping rule rule deciding gather data desired exchange rate information gain measurement 
possible weakness information approaches estimate utility measurement assuming model correct 
lead undesirable results 
search ideal measures data utility open 
chapter evidence framework applied classification networks bayesian ideas supervised adaptive classifiers 
argued output classifier obtained posterior distribution parameters simple approximation integral proposed demonstrated 
involves moderation probable classifier outputs yields improved performance 
second demonstrated bayesian framework model comparison described regression models chapters applied classification problems 
framework successfully chooses magnitude weight decay terms ranks solutions different numbers hidden units 
third information data selection criterion derived demonstrated framework 
quantitative bayesian framework described learning mappings feedforward networks chapters 
demonstrated evidence framework successfully choose magnitude type weight decay terms choose solutions different numbers hidden units 
framework gives quantified error bars expressing uncertainty network outputs parameters 
chapter information objective functions active learning discussed framework 
chapters concentrated interpolation regression problems 
neural networks trained perform classification tasks 
chapter show bayesian framework model comparison applied problems 
assume set candidate classification models fitted data set standard methods 
aspects classifiers distinguished 
individual classification models predictions new targets 
chapter ph thesis bayesian methods adaptive models david mackay california institute technology submitted december 
regression target variables real numbers assumed include additive errors classification target variables discrete class labels 
bayesian methods adaptive models 
alternative models ranked light data 

expected utility alternative new data points estimated purpose query learning active data selection 
chapter bayesian ideas tasks 
aspects classifiers prediction generalisation ability addressed 
review framework supervised adaptive classification 
derivation objective function ln notation conventions chapters 
data set fx classification problem target binary variable classes handled activity output classifier viewed estimate probability 
assumed classification problem noisy repeated sampling produce different values certain probabilities probabilities function quantities discriminative classifier intended model 
known natural objective function case information distance measure sum squared errors 
classification model consists specification architecture regulariser parameters classification model parameters set particular value model produces output viewed probability jx 
likelihood probability data function jw gamma gammat exp jw jw log gamma log gamma probabilistic motivation cross entropy objective function log assign prior alternative parameter vectors exp gamma ff zw cost function subset parameters ff associated regularisation constant see chapter obtain posterior wjd fff exp gamma ff zm zw zm appropriate normalising constants 
identical framework obtained chapter gammag replacing term fie note contrast framework chapter free parameter fi zd fi 
teacher supply probability estimates binary targets constant strictly probability fx density fxg modelled discriminative classifiers discussed chapter 
chapter 
evidence applied classification equivalent fi appear expressing precision teacher estimates 
constant correspond effective number observations teacher opinion 
calculation gradient hessian easy quadratic ed output unit activation function traditional logistic gammaa generalised softmax case classes 
appropriateness output function classifier known function converts log probability ratio probability 
gradient defined gradient parameters rg gamma wj hessian hessian analytically evaluated useful approximation neglecting terms rrg gamma approximation expected adequate evaluation error bars data selection evaluation number determined parameters fl 
accurate evaluation hessian probably needed estimation evidence 
chapter demonstrations hessian evaluated second differences 
validity approximations account central limit theorem expect posterior distribution converge set locally gaussian peaks increasing quantities data 
quadratic approximation expected converge slowly quadratic approximation ed error function regression models quadratic function linear model model oe term large scale form ramp function inputs fall bend ramp contribute curvature opportunity active data selection improve convergence quadratic approximation selecting inputs expected contribute maximal curvature 
related data selection criterion derived section 
classifier sets outputs consider classifier output 
assume receive data infer posterior probability parameters perform learning 
asked predictions classifier common probable best fit parameter vector wmp sole representative posterior distribution 
strategy unwise may regions input space posterior ensemble uncertain class regions output network assuming equiprobable classes priori typically network parameters wmp give extreme unrepresentative bayesian methods adaptive models output 
error bars parameters taken account predictions 
regression problems important calculate error bars outputs problem acute case classification account non linear output mean output posterior distribution equal probable network output 
obtain output representative posterior ensemble networks wmp need moderate output probable network relation error bars wmp course idea averaging hidden parameters new marginalisation goes back laplace 
context closer message example 
practitioners adaptive classification currently marginalisation 
suggest classifier sets outputs 
set give usual class probabilities corresponding wmp wmp outputs learning calculating error signals optimisation wmp second set moderated outputs wjd wjd outputs applications prediction evaluation test error evaluating utility candidate data points section 
discuss calculate moderated outputs 
demonstrated outputs better predictions 
calculating moderated outputs assume locally gaussian posterior probability distribution wmp deltaw wjd wmp exp gamma deltaw deltaw assume activation locally linear function activation approximately gaussian distributed jd normal mp exp gamma gamma mp mp wmp gamma means moderated output jx mp da normal mp contrasted probable network output wmp mp 
integral sigmoid times gaussian solved analytically suggest simple numerical approximation mp oe mp mp 
approximation globally accurate mp large function tend error function breaks gracefully 
value chosen approximation correct gain mp 
representative approximation compares oe oe numerical evaluations similar approximation terms error function suggested 
conditioning variables omitted section emphasis model comparison 
chapter 
evidence applied classification psi psi phi psi log phi log psi approximation moderated probability function evaluated numerically 
functions oe defined text shown function 
difference oe gamma shown parameter values 
breakdown approximation emphasised showing log oe log derivatives respect 
errors significant 
output immediately decision moderated outputs difference performance classifier costs associated error functions pass mp 
moderated outputs difference sophisticated penalty function involved 
demonstration performance classifier outputs measured value achieved test set 
model classification problem input variables possible classes shown 
illustrates output typical trained network probable parameter values 
shows moderated outputs network 
notice moderated output similar probable output regions data dense 
contrast data sparse moderated output significantly certain probable output seen widening contours 
shows correct posterior probability problem knowledge true class densities 
neural networks having inputs hidden layer sigmoid units sigmoid output unit trained problem 
optimisation second weight decay scheme chapter independent decay rates weight classes hidden weights hidden unit biases output weights biases 
corresponds prior models weights class coming gaussian scale gaussians different classes independent specified regularising constants ff regularising constant optimised line intermittently updating probable value estimated evidence framework 
prediction abilities networks probable outputs moderated outputs suggested compared 
seen bayesian methods adaptive models class class comparison probable outputs moderated outputs toy problem data set 
data generated circular gaussian distributions gaussians class 
training sets demonstrations data points drawn distribution 
upper right probable output hidden unit network trained data points 
contours equally spaced 
lower left moderated output network 
notice output certain compared probable output input moves away regions high training data density 
true posterior probability class densities generated data 
viewpoint upper right corner 
common grey scale linear dark grey light grey 
chapter 
evidence applied classification moderated network test error test error probable parameters alternative solutions moderation thing 
training set networks contained data points 
network test error probable outputs moderated outputs evaluated test set data points 
test error value note solutions moderated outputs better predictions 
predictions moderated outputs nearly cases superior 
improvement substantial underdetermined networks relatively poor performance 
small fraction solutions especially best solutions moderated outputs slightly significantly inferior performance 
evaluating evidence having established particular model fa rg regularising constants fff predictions turn question model comparison 
discussed chapter levels inference distinguished parameter estimation regularisation constant determination model comparison 
second levels inference require occam razor solution best fits data plausible model need way balance goodness fit complexity 
bayesian inference embodies occam razor automatically 
level model regularising constants fff fitted data involves inferring value parameters probably 
bayes rule level inference form wjd fff djw fff thesis posterior approximated locally gaussian wjd fff exp gammam zm exp gammam mp gamma deltaw deltaw specified model predict class datum viewed level inference 
bayesian methods adaptive models moderated test error data error test error versus data error illustrates task ranking solutions classification problem requires occam razor solutions smallest data error generalise best 
deltaw gamma wmp ff gamma rrm second level inference regularising constants optimised fff gjd fff djh data dependent term evidence normalising constant equation 
evaluation quantity optimisation parameters fff accomplished framework due gull skilling discussed detail chapters 
third level inference alternative models compared hjd djh data opinion alternatives evidence previous level case djh 
omitting details second level inference identical methods chapter demonstration presents final inferences evidence alternative solutions 
evidence evaluated gaussian approximation properties probable fit wmp error bars gamma described chapter 
shows test error calculated moderated outputs solutions data error occam razor problem seen solutions smallest data error generalise best 
shows log evidence solutions test error seen moderately correlation obtained 
correlation perfect 
speculated discrepancy mainly due inaccurate evaluation evidence quadratic approximation study needed 
explores dependence correlation evidence generalisation amount data 
seen correlation improves number data points test set increases 
chapter 
evidence applied classification moderated test error log evidence alternative solutions test error versus evidence solution training set data points 
solutions symmetry detected hidden units omitted graph evidence evaluation solutions unreliable 
log evidence alternative solutions log evidence alternative solutions correlation test error evidence amount data varies 
data points 
data points 

comparison number parameters typical hidden unit network 
note data points fall informative decision regions effective number data points smaller case bear mind data point consists bit 
solutions symmetry detected hidden units omitted evidence evaluation solutions unreliable 
bayesian methods adaptive models active learning assume opportunity select input datum gathered query learning 
papers suggested strategies active learning problem example hwang propose samples near current decision boundaries 
strategy baum human designed strategies clear objective function optimise clear strategies improved 
chapter chapter philosophy derive criterion defined sensible objective function measures useful datum expected 
criterion may guide query learning alternative scenario pruning uninformative data points large data set 
desiderata hwang strategy try establish reasonable objective function 
strategy sampling decision boundaries motivated argument gain information sampling region confident correct classification 
similarly sampled great deal particular boundary don gain useful information repeatedly sampling location boundary established 
repeated sampling locations generates data large entropy informative way white noise informative 
utility sample distance decision boundary 
prefer sample near boundaries location determined probably enable precise predictions 
interested measurements convey mutual information unknowns interested 
second criticism strategy samples near existing boundaries new discoveries strategy samples near potential boundaries expected informative 
final criticism efficient strategy take account influential datum data may convey information discriminant larger region 
want objective function measures global expected informativeness datum 
objective function chapter study mean marginal information 
objective function suggested chapter discussion probably desirable joint information 
define objective function define region interest 
objective maximal information gain model parameters region interest lead sample extremes input space 
region interest defined set representative points normalised distribution 
interpreted probability asked prediction 
theory worked case continuous region defined density ae discrete case preferred relates directly practical implementation 
marginal entropy distribution point defined log gamma log gamma chapter 
evidence applied classification average output classifier ensemble 
gaussian approximation moderated output may approximated oe mp 
mean marginal entropy sm sampling strategy studied maximise expected change mean marginal entropy 
note information gain minus change entropy 
estimating marginal entropy changes measurement result measurement 
assuming current model complete gaussian error bars correct probability mp oe mp 
wish estimate average change marginal entropy measurement 
problem solved calculating joint probability distribution finding mutual information variables 
values form da da exp gamma deltaa sigma gamma deltaa deltaa deltaa deltaa activations mp deltaa mp deltaa assumed gaussian distribution covariance matrix sigma gamma gamma gamma gamma normalising constant ss gamma ae expected change entropy deltas jt gamma gamma notice mutual information symmetric approximate deltas jt taylor expanding independence ae 
order perturbation introduced ae written terms single variable gamma gamma taylor expanding find deltas jt gamma taylor expand obtain dependence correlation activations 
derivative respect ae ae ae da da deltaa deltaa ss exp gamma deltaa sigma gamma deltaa mp mp bayesian methods adaptive models moderated probability defined denotes yields ae ae gamma mp mp substituting find deltas jt gamma gamma mp mp assuming approximation oe mp numerically approximate mp mp 
gamma obtain deltas jt gamma mp mp gamma terms expression correspond intuitions sampling near decision boundaries informative able gain information points interest near boundaries 
term gamma modifies tendency accordance desiderata 
expected mean marginal information gain computed adding deltas representative points resulting function plotted grey scale network solving toy problem described 
demonstration points interest defined drawing input points random test set 
striking correlation seen regions moderated output uncertain regions high expected information gain 
addition expected information gain tends increase regions training data sparse 
negative aspect results 
regions greatest expected information gain lie outside region interest right left regions extend long straight ridges hundreds units away data 
estimation utility reveals hyperplanes underlying model unreasonable 
utility points far region interest occurred really high 
plausible explanations 
may taylor approximations evaluate mean marginal information fault particular 
discussed chapter problem arise mean marginal information estimates utility point assuming model true assume classification surface really described terms hyperplanes input space may greatest torque planes obtained sampling away core data 
comparison approximation numerical evaluations deltas indicate approximation factor wrong 
explanation favoured tentatively conclude mean marginal information gain useful models matched real world 
discussion moderated outputs idea outputs classifier accordance uncertainty parameters wide applicability example hidden markov models speech recognition 
moderation especially important approximation inaccurate mp ae ae see 
wise numerical integration implement deltas look tables 
chapter 
evidence applied classification class class demonstration expected mean marginal information gain mean marginal information gain computed network demonstrated figures region interest defined data points test set 
grey level represents utility single observation function 
darkest regions expected yield little information white corresponds large expected information gain 
contours superposed represent moderated output network shown 
mean marginal information gain quantified grey scale linear nats 
bayesian methods adaptive models classifier expected extrapolate points outside training region 
presumably relationship concept seung generalisation non zero temperature 
suggested approximation moderated output derivative simple brute force solution set look table values 
implementation marginalisation scale large problems involve monte carlo methods 
evidence evidence correlated generalisation ability 
depends having sufficiently large amount data 
remain open questions including theoretical relationship evidence generalisation ability large data set correlated calculations scale larger problems quadratic approximation evidence breaks 
mean marginal information gain objective function derived active learning mind 
selection subset large quantity data filter weed fractions data informative 
white approach filter depends input variables candidate data 
strategy selectively omits data basis output values violate likelihood principle risk leading inconsistent inferences 
comparison mean marginal information gain contours probable networks output indicates proposed data selection criterion offers improvements simple strategy just sampling near decision boundaries mean marginal information gain shows plausible preference samples regions decision boundary uncertain 
hand criterion may give artefacts applied models poorly matched real world 
useful mean marginal information gain real applications remains open question 
chapter inferring input dependent noise level assume interpolating data set wish model input dependent noise level 
short chapter shows calculate unbiased gradient 
data set modelled interpolant plus noise model chapter described bayesian framework regularisation model comparison assuming single global noise level fi gamma oe possible invent models fi dependent 
example coupled neural networks predicts second predicts log fi 
networks coupled gradient objective function network calculated consulting output 
intuitively network errors neighbourhood large encourage second network give large value fi gamma oe similarly error signals teach network need scaled second network small value oe errors penalised strongly 
gradients optimisation model 
simple approach maximise likelihood data 
traditional quadratic model log likelihood data gamma fi edm log fi edm gamma xm maximisation function lead biased estimates fi 
discussed earlier maximum likelihood noise estimate probable value noise 
distinction caused priors result marginalisation 
worst symptom maximising likelihood regions data sparse best fit interpolant passes close data maximum likelihood estimate fi blows estimated noise level goes zero 
separating levels inference consider case single noise level oe moment imagine estimating single parameter corresponding mean gaussian cluster 
examined problem chapter 
likelihood function oe skew peak 
maximum located oe oe gamma peak chapter ph thesis bayesian methods adaptive models david mackay california institute technology submitted december 
bayesian methods adaptive models place centre mass likelihood 
marginalise flat prior find probable value oe oe gamma gamma gamma 
subtraction denominator represents fact parameter determined data typically consumes unit noise 
known oe oe gamma respectively biased unbiased estimators variance 
generalisation distinction 
fit regularised interpolation model distinct levels inference 
level assume particular noise level fi regularisation constant ff find probable parameters error bars 
second level inference compare alternative values ff alternative values fi 
separation find probable noise level oe ed gamma fl fl number determined parameters 
quantity significantly total number parameters regulariser prior playing significant role determining interpolant 
separation levels inference marginalisation leads unbiased estimator fi automatic occam razor choice ff 
see case dependent noise level 
level inference gradients respect model parameters calculated obvious way ff fi edm second level inference centres log evidence ff fi written neglecting additive constants log djff fi mp gamma fi mp dm gamma log det gamma log zw ff log fi probable values ff fi flat prior obtained maximising log evidence 
course infer dependent noise level typically imposing prior fi choice model case need gradient evidence level serves likelihood driving learning 
differentiate evidence respect log fi obtain log djff fi log fi gammafi mp dm gamma fi trace gamma bm fi bm contribution mth datum case linear model fi bm fi mg gm xm quantity trace theta gamma bm precisely magnitude error bars interpolant xm gamma gm term fi trace theta gamma bm gamma gm oe ratio error bars interpolant presumed variance measurement xm quantity fl gamma gamma gm oe measure noise measurement datum xm contributed 
gradient written log djff fi log fi gammafi mp dm fl terms fl easy evaluate error bars xm noted case isolated data point contribution gradient behaved edm fl go zero 
singular behaviour involving fi blowing occurs likelihood maximised 
fl interpolant determined locally datum xm measurement gives information noise 
fl interpolant locally chapter 
inferring input dependent noise level determined measurements regulariser error edm convey information noise level 
similarity fl concept number determined parameters fl obvious fact mathematical relationship 
number bad noise measurements identical number determined parameters gamma fl fl 
implementation framework depend issues 
prior course placed parameters network produces log fi prior contain unknown regularisation constants controlled methods chapter 
secondly management different levels optimisation trivial 
suggested procedure start optimising noise model interpolant fitting data quite levels fitting interpolant inferring noise level setting regularisation constants noise model optimised cyclically 
bayesian methods adaptive models chapter postscript common years religion student views religion matured 
postscript intended communicate reservations criticisms bayesian methods described open questions problems remain 
closed hypothesis space bayesian hard cox axioms drum consistent inference bayesian rarely clear cox axioms 
fact cox result assumes things performing inductive inference defined closed hypothesis space 
edged sword 
aspect bayesian inductive inference proceed properties hypothesis space articulated forced explicit assumptions 
furthermore hypothesis space defined bayesian inference mechanical defined process 
bayesianism need consult sampling theory criteria efficiency consistency sufficiency uniform convergence minimum variance desiderata mutually conflicting 
simply write conditional assumptions making example data occurred propositions plausibilities wish infer evaluate sum product rules probability defined hypothesis space 
axioms probabilistic inference guarantee inferences way coherent 
hand deficiencies arise constraint closed hypothesis space 
central problem bayesian inferences obtained assuming hypothesis space right bayesian way assessing hypothesis space right apart coming alternative hypothesis spaces comparisons 
box tiao share view model criticism essential part modelling process addressed bayesian inference 
example error bars discussed thesis evaluated assuming model true 
think non bayesian procedures improve orthodox confidence intervals identical bayesian error bars gaussian case important aware possible true interpolant lie outside error bars assigned model model defective way 
example seen error bars fail include point fact outlier 
bayesian resolution examine models radial chapter 
postscript basis function model replaced probable neural network model table interpolant goes closer data point probably neural network model probable 
second defect closed hypothesis space assumption discussed chapter 
expected information gain provided datum defined error bars assuming model correct 
extreme cases may lead results distant data points may evaluated mutually informative misleading interdependence model 
chapter shown mean marginal expected information gain artifacts arising assumption model correct 
thirdly bayes rule provides mechanism alternative free tests hypothesis space 
reservation bayesian methods expressed lindley 
hard line bayesians thing alternative free test certainly classical alternative free tests implicit alternative 
example classical test parameter zero implicit alternative hypothesis parameter value interval derivable prior width 
believe perform alternative free tests 
dissatisfied theory making unusually poor predictions 
prompts start searching superior theories 
alternative vaguely specified able infer wrong example chapter 
find superior alternative come back bayes rule reject original theory initial decision search new theory alternative free bayes rule 
bayes rule direct new models 
invention hypothesis spaces remains human domain 
having recognised bayes rule perform alternative free inferences part modelling process right hand loop optimistic note 
believe bayesian methods traditional methods cross validation yield powerful tool alternative free hypothesis testing new model formation 
illustrated chapter poor correlation evidence test error highlighted inconsistency model space test error cross validation model comparison opportunity learning lost likewise bayesian evidence evaluated 
think bayesians include similar alternative free warning bells algorithms 
approximation probabilities relevant 
bayesian approach model comparison evaluates probable alternative models data 
fields image reconstruction nmr may precisely right thing 
contrast adaptive modelling real problem estimate model expected generalise 
know perfectly truth data generated neural network parameters wish infer 
know models false bayesian assessment relative probabilities alternative parameterised models irrelevant interested models approximates 
really bayesian solution task model really believe infer truth decision theory select false models expected appropriate cost function 
bayesian methods adaptive models startling fact spite evidence false models correlated generalisation ability model space matched problem figures 
theories attempt directly predict generalisation ability leading akaike fpe criterion moody gpe 
toy problems studied criteria better correlation generalisation error achieved evidence 
theories dimension lead structural risk minimisation criteria better correlated generalisation error 
fact interesting form guyon predicted generalisation error scaling behaviour identical evidence 
called relationship evidence cross validation generalisation ability understand results 
having explicit statistical problems precisely stated sampling theory school vague bayesian 
bayesian adds additional constraints problem comes fire making assumptions may detail justified 
order statistics provide example problem 
imagine wish infer median density samples precise specification type density dealing particular know distribution gaussian 
bayesian analysis assume explicit parameterised form density example free form density maxent prior solve posterior distribution parameters marginalise distribution get posterior median density 
think principle right thing credit bayesians shown done approach involves introducing eliminating large amount irrelevant baggage explicit parameterised form density 
details parameterisation probably hard justify anyway answer obtain depend sensitively 
unfortunate introduce explicit arbitrary detail order answer simple question 
having said clear am advocating orthodox sampling theory approach order statistics sampling theory procedures order statistics incoherent 
interesting bayesian methods developed avoid having explicitly handle detailed parameterisations marginalised away 
monte carlo methods radford neal step direction 
alternative interpretation weight decay geoff hinton personal communication suggested alternative view mixture weight decay 
decay mechanism viewed implementing prior knowledge literal prior says modelled coming mixture gaussians 
real prior assumption fraction weights ought exactly zero 
true width component origin ought zero set non zero value computational 
view weight decay intended switch weights apparently shared workers 
interpretation reason suppose bayesian choice width component appropriate 
width broad compo nowlan hinton applied bayesian procedure networks predicting sunspot chapter 
postscript nent mixture distribution inferred bayesian methods 
interesting see interpretation formalised leading alternative founded procedure setting parameters zero mixture 
necessitate changes evaluation evidence 
tasks open problems expensive evidence calculations cheaper hessian calculations bayesian calculations thesis depend inverse hessian gamma gaussian approximation 
directions 
expensive direction ask bayesian calculations accurate improving gaussian approximation monte carlo methods example 
addition methods integrating regularisation constants need developed fixing constants probable values 
questions skilling personal communication working 
cheaper direction ask hessian calculations approximate efficient reduce cost calculations 
hope investigate statistical methods reducing nk calculation properties gamma time 
noisy input variables tasks interpolation classification noisy input variables integrated evidence framework 
missing data imagine asked interpolate data set fx tg elements incomplete lacking specification components 
interpolation framework described thesis handle case density modelled 
open problem find simple defined way integrate data missing components interpolation models 
problem common task combining unlabelled data discriminative training 
discriminative training adapt classifier models tjx data set fx tg additional unlabelled data fxg available provide useful information assume sort density reason reluctant specify density speech recognition come discriminative training full probabilistic modelling obtains better performance poor models 
ideally able develop superior word models stuck particular model space computational constraints lack creativity 
combining discriminative training unlabelled data current frontiers bayesian methods 
time series obtained better performance published model 
bayesian methods adaptive models applications concepts bayesian data modelling described thesis great generality relevant experimental scientist 
example applications include speech recognition automated control hidden markov model structure speech recognition selection alternative models single word evidence concept moderation incorporation error bar information expected useful fitting model utterances 
point source image reconstruction estimating point sources astronomical image occam razor needed avoid fitting stars image 
neurophysiology multi neuron recording task infer activities multiple neurons piece brain tissue signals array recording electrodes 
require development non parametric bayesian methods 
density estimation evidence evaluated problem choosing number gaussians mixture model problem choosing gaussian models robust clustering models 
problem relevant regression problems non gaussian noise models thought appropriate definitive bayesian attack problem inferring slightly non gaussian distribution box tiao 
applications neural networks remains investigated methods scale real larger problems 
framework applied sophisticated regularisers mixture decay models hinton nowlan 
power unifying perspective bayesian methods widely appreciated 
thesis demonstrated utility adaptive models neural networks 
thousands data modelling tasks waiting evidence evaluated 
exciting see learn done 
bibliography abu mostafa 
vapnik chervonenkis dimension information versus complexity learning neural computation 
abu mostafa 
learning hints neural networks complexity 
akaike 
statistical predictor identification ann 
inst 
statist 
math 

angel lloyd hart sandler 
adaptive optics array telescopes neural network techniques nature 
baum 
neural net algorithms learn polynomial time examples queries ieee trans 
neural networks 
bayes 
essay solving problem doctrine chances philos 
trans 
soc 
london reprinted biometrika 
becker le cun 
improving convergence back propagation learning second order methods proc 
connectionist models summer school ed 
touretzky morgan kaufmann 
berger 
statistical decision theory bayesian analysis springer 
bishop 
exact calculation hessian matrix multilayer perceptron neural computation 
box tiao 
look robustness bayes theorem biometrika 
box tiao 
bayesian approach importance assumptions applied comparison variances biometrika 
box tiao 
bayesian approach outlier problems biometrika 
box tiao 
bayesian inference statistical analysis addison wesley 
bretthorst 
bayesian analysis 
parameter estimation quadrature nmr models 
ii 
signal detection model selection 
iii 
applications nmr magnetic resonance 
bibliography bridle 
probabilistic interpretation feedforward classification network outputs relationships statistical pattern recognition neuro computing algorithms architectures applications soulie editors springer verlag 
charter 
quantifying drug absorption 
cox 
probability frequency reasonable expectation am 
physics 
davies 
optimization regularization ill posed problems austral 
mat 
soc 
ser 

denker le cun 
transforming neural net output levels probability distributions advances neural information processing systems ed 
lippmann morgan kaufmann 
duda hart 
pattern classification scene analysis wiley 
el gamal 
role priors active bayesian learning sequential statistical decision framework 
eubank 
spline smoothing non parametric regression marcel dekker 
fedorov 
theory optimal experiments academic press 
fukunaga 
statistical pattern recognition academic press 
jr eds 

maximum entropy bayesian methods wyoming kluwer 
gull 
bayesian inductive inference maximum entropy maximum entropy bayesian methods science engineering vol 
foundations erickson smith eds kluwer 
gull 
developments maximum entropy data analysis 
gull 
bayesian data analysis straight line fitting 
gull skilling 
quantified maximum entropy 
user manual north sg nr england 
guyon vapnik boser bottou solla 
structural risk minimization character recognition advances neural information processing systems ed 
moody hanson lippmann morgan kaufmann 
hanson stutz cheeseman 
bayesian classification theory nasa ames tr fia 
haussler kearns schapire 
bounds sample complexity bayesian learning information theory dimension preprint 
hinton sejnowski 
learning relearning boltzmann machines parallel distributed processing rumelhart mit press 
bibliography hopfield 
learning algorithms probability distributions feed forward feed back networks proc 
natl 
acad 
sci 
usa 

hwang choi oh marks ii 
query learning applied partially trained multilayer perceptrons ieee trans 
neural networks 
jaynes 
bayesian methods general background maximum entropy bayesian methods applied statistics ed 
justice 
berger 
ockham razor bayesian analysis american scientist 
jeffreys 
theory probability oxford univ press 
ji 
generalizing smoothness constraints discrete samples neural computation 
kashyap 
bayesian comparison different classes dynamic models empirical data ieee transactions automatic control ac 
le cun denker solla 
optimal brain damage advances neural information processing systems ed 
david touretzky morgan kaufmann 
lee 
optimal adaptive classifier design criterion hidden units necessary optimal neural network classifier purdue university tr ee 
levin tishby solla 
statistical approach learning generalization layered neural networks colt nd workshop computational learning theory 
lindley 
measure information provided experiment ann 
math 
statist 

lindley 
bayesian analysis regression problems bayesian statistics meyer collier eds peacock publishers 
lindley 
bayesian statistics review society industrial applied mathematics philadelphia 
loredo 
laplace supernova sn bayesian inference astrophysics maximum entropy bayesian methods ed 
kluwer 
luttrell 
design data sampling schemes inverse problems inverse problems 
mackay 
bayesian interpolation neural computation chapter dissertation 
mackay 
practical bayesian framework backprop networks neural computation chapter dissertation 
bibliography mackay 
information objective functions active data selection neural computation chapter dissertation 
mackay 
evidence framework applied classification networks neural computation chapter dissertation 
mark miller 
bayesian model selection minimum description length estimation auditory nerve discharge rates acoust 
soc 
am 

moody 
note generalization regularization architecture selection nonlinear learning systems ieee sp workshop neural networks signal processing ieee computer society press neal 
bayesian mixture modelling monte carlo simulation technical report crg tr dept computer science university toronto 
neal 
bayesian training backpropagation networks hybrid monte carlo method technical report crg tr dept computer science university toronto 
nowlan 
soft competitive adaptation neural network learning algorithms fitting statistical mixtures carnegie mellon university doctoral thesis cs 
nowlan hinton 
soft weight sharing preprint 

information weight evidence singularity probability measures signal detection springer 
patrick wallace 
stone circle geometries information theory approach old world editor cambridge univ press 
pineda 
recurrent back propagation dynamical approach adaptive neural computation neural computation 
white 
active selection training examples network learning noiseless environments dept computer science ucsd tr 
poggio torre koch 
computational vision regularization theory nature 
press flannery teukolsky vetterling 
numerical recipes cambridge 
rissanen 
modeling shortest data description automatica 
rumelhart hinton williams 
learning representations back propagating errors nature 
rumelhart 
cited 
schwarz 
estimating dimension model ann 
stat 

bibliography seung sompolinsky tishby 
statistical mechanics learning examples preprint 

bayesian interpolation 
skilling editor 
maximum entropy bayesian methods cambridge kluwer 
skilling 
eigenvalues mega dimensional matrices 
skilling 
parameter estimation quantified maxent 
skilling robinson gull 
probabilistic displays 
skilling 
fundamentals maxent data analysis maximum entropy action buck macaulay eds oxford 
skilling 
bayesian solution ordinary differential equations maximum entropy bayesian methods seattle erickson smith eds kluwer 
smith spiegelhalter 
bayes factors choice criteria linear models journal royal statistical society 
solla levin 
accelerated learning layered neural networks complex systems 
spiegelhalter lauritzen 
sequential updating conditional probabilities directed graphical structures networks 

laplace memoir inverse probability stat 
sci 

szeliski 
bayesian modeling uncertainty low level vision kluwer 
tishby levin solla 
consistent inference probabilities layered networks predictions generalization proc 
ijcnn washington 
titterington 
common structure smoothing techniques statistics int 
statist 
rev 
walker 
asymptotic behaviour posterior distributions stat 
soc 

wallace boulton 
information measure classification comput 

wallace freeman 
estimation inference compact coding statist 
soc 

weigend rumelhart huberman 
generalization weight elimination applications forecasting advances neural information processing systems ed 
lippmann morgan kaufmann 
bibliography weir 
applications maximum entropy techniques hst data proceedings st ecf data analysis workshop april 
zellner 
basic issues econometrics chicago 
