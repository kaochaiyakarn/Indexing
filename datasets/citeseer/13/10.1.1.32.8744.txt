sparse kernel principal component analysis michael tipping microsoft research st george house st cambridge cb nh microsoft com kernel principal component analysis pca elegant nonlinear generalisation popular linear data analysis method kernel function implicitly de nes nonlinear transformation feature space standard pca performed 
unfortunately technique sparse components obtained expressed terms kernels associated training vector 
shows approximating covariance matrix feature space reduced number example vectors maximum likelihood approach may obtain highly sparse form kernel pca loss ectiveness 
principal component analysis pca established technique dimensionality reduction examples applications include data compression image processing visualisation exploratory data analysis pattern recognition time series prediction 
set dimensional data vectors xn take zero mean principal components linear projections principal axes de ned leading eigenvectors sample covariance matrix xn conventionally de ned design matrix 
projections interest retain maximum variance minimise error subsequent linear reconstruction 
pca de nes linear projection data scope application necessarily somewhat limited 
naturally motivated various developments nonlinear principal component analysis ort model non trivial data structures faithfully particularly interesting innovation kernel pca 
kernel pca summarised section kernel trick ectively exploited support vector machine kernel function 
may considered represent dot inner product transformed space satis es mercer condition continuous symmetric kernel positive integral operator 
elegant way non linear procedures depend inner products examples 
applications utilising kernel pca emerging practice approach su ers important disadvantage sparse method 
computation principal component projections input requires evaluation kernel function xn respect training examples xn unfortunate limitation practice obtain best model estimate kernel principal components data possible 
tackle problem rst approximating covariance matrix feature space subset outer products feature vectors maximum likelihood criterion probabilistic pca model detailed section 
subsequently applying kernel pca de nes sparse projections 
importantly approximation adopt principled controllable related choice number components discard conventional approach 
demonstrate ecacy section illustrate er similar performance full non sparse kernel pca implementation ering reduced computational overheads 
kernel pca pca conventionally de ned terms covariance outer product matrix established eigenvectors obtained inner product matrix xx orthogonal matrix column eigenvectors xx corresponding eigenvalues diagonal matrix de nition xx 
pre multiplying gives inspection seen eigenvectors eigenvalues 
note column vectors normalised column xx correctly normalised eigenvectors principal axes data pca derivation useful dimensionality greater number examples fundamental implementing kernel pca 
kernel pca data vectors xn implicitly mapped feature space set functions xn xn 
vectors xn feature space generally known explicitly inner products de ned kernel xm xn 
de ning notional design matrix feature space exploiting inner product pca formulation allows eigenvectors covariance matrix feature space speci ed kpca eigenvectors values kernel matrix mn xm xn 
compute kpca don know explicitly compute projections arbitrary test vectors kpca feature space kpca vector inner products data kernel space xn 
compute plot projections gives example synthetic cluster data dimensions 
rest centre data feature space may achieved desired see 
fact argue gaussian kernel necessarily sense 
contour plots rst principal component projections evaluated region input space data gaussian clusters standard deviation axis scales shown comprising vectors 
gaussian kernel exp kx width 
corresponding eigenvalues projection 
note rst components pick individual clusters 
probabilistic feature space pca approach kernel pca priori approximate feature space sample covariance matrix sum weighted outer products reduced number feature vectors 
basis technique general application necessarily limited kernel pca 
achieved probabilistically maximising likelihood feature vectors gaussian density model specify covariance wn adjustable weights matrix weights diagonal isotropic noise component common dimensions feature space 
course naive maximum likelihood model obtained optimise weighting factors nd maximum likelihood estimates zero realising sparse representation covariance matrix 
probabilistic approach motivated fact relax form model de ning terms outer products arbitrary vectors xed training vectors realise form probabilistic pca 
fu set eigenvectors values likelihood model maximised weights zero 
computations feature space wish maximise likelihood gaussian model covariance 
ignoring terms independent weighting parameters log log jcj tr computing requires quantities jcj nite dimensionality feature spaces appear problematic 
judicious re writing terms interest able compute log likelihood constant optimise respect weights 
write log log log jw log jwj potential problem nite dimensionality feature space enters rst term constant xed ect maximisation 
term jwj straightforward remaining term expressed terms inner product kernel matrix kernel matrix mn xm xn 
data dependent term likelihood woodbury matrix inversion identity compute quantities xn xn kn kn xn xn xn optimising weights maximise log likelihood respect di erentiating gives ni ii nw de ned respectively kn setting zero gives re estimation equations weights new ni ii re estimates equivalent expectation maximisation updates obtained adopting factor analytic perspective introducing set hidden gaussian explanatory variables conditional means common covariance feature vectors current values weights respectively notation 
guaranteed increase maximum 
alternative re arrangement motivated leads re estimation update typically converges signi cantly quickly new ni ii note updates de ned terms computable dependent explicit feature space vectors quantities principal component analysis principal axes sparse kernel pca proceeds nding principal axes covariance model 
identical eigenvalues larger 
letting need eigenvectors 
technique section eigenvectors kw corresponding eigenvalues values fu desire computing projections compute eigenvectors explicitly compute projections general feature vector principal axes sparse vector containing non zero weighted elements de ned earlier 
corresponding rows combined single projecting matrix column gives coecients kernel functions evaluation principal component 
computing reconstruction error squared reconstruction error kernel space test vector uu kernel matrix evaluated representing vectors 
examples obtain sparse kernel pca projections rst specify noise variance amount variance ordinate prepared allow explained structure free isotropic noise principal axes choice surrogate deciding principal axes retain conventional kernel pca 
unfortunately measure feature space dicult interpret data space equally course interpretation eigenvalue spectrum non sparse case 
apply sparse kernel pca gaussian data earlier kernel function specifying deliberately chosen give representing kernels facilitate comparison 
shows principal component projections approximated covariance matrix gives qualitatively equivalent results utilising kernels 
shows data highlights examples corresponding kernels nonzero weights 
note consider aspect representing vectors highly informative structure data gaussian kernel example tend represent distinguishable clusters 
contours reconstruction error kernels plotted indicate nonlinear model faithfully captured structure data standard linear pca 
principal component projections obtained sparse kernel pca 
illustrate delity sparse approximation analyse training examples dimensional pima indians diabetes database 
left shows plot reconstruction error number principal components utilised conventional kernel pca sparse counterpart chosen utilise kernels 
expected small reduction accuracy evident sparse case 
right shows error associated test set linear support vector machine classify data numbers principal components 
sparse projections perform marginally better average consequence randomness note interest presumably inherent complexity control implied sparse approximation 
data representing kernels circled contours reconstruction error computed feature space displayed function overlaid 
standard sparse rms reconstruction error left test set misclassi cations right numbers retained principal components ranging 
standard case training examples sparse form subset 
gaussian kernel width utilised gives near optimal results svm classi cation 
ripley 
pattern recognition neural networks 
cambridge university press cambridge 
gong 
multi view nonlinear active shape model kernel pca 
proceedings british machine vision conference pages 
rubin thayer 
em algorithms ml factor analysis 
psychometrika 
sch olkopf smola 
uller 
nonlinear component analysis kernel eigenvalue problem 
neural computation 
technical report max planck institut ur kybernetik 
tipping 
relevance vector machine 
solla leen 
uller editors advances neural information processing systems pages 
cambridge mass mit press 
tipping bishop 
probabilistic principal component analysis 
journal royal statistical society series 
