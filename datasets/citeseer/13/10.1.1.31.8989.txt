tutorial variational approximation methods tommi jaakkola mit arti cial intelligence laboratory technology square cambridge ma october provide theory variational methods inference estimation context graphical models 
variational methods useful ecient approximate methods structure graph model longer admits feasible exact probabilistic calculations 
emphasis tutorial illustrating inference estimation problems transformed variational form describing resulting approximation algorithms properties insofar currently known 
term variational methods refers large collection optimization techniques 
classical context methods involves nding extremum integral depending unknown function derivatives 
classical de nition accompanying calculus variation longer adequately characterizes modern variational methods 
modern variational approaches indispensable tools various elds control theory optimization statistics economics machine learning 
nite element method solving di erential equations example inherently variational approach maximum entropy estimation 
number qualitative features shared variational formulations 
primary component naturally optimization problem 
problem interest transformed optimization problem directly formulated principle maximum entropy estimation emphasis tutorial transforming various inference estimation problems variational problems 
quantity optimized typically unknown function simple cases may reduced vector function values discrete points 
solution variational problems terms xed point equations capture necessary conditions optimality characterizing locally optimal solutions 
analogous setting gradient zero ordinary function optimization 
mean eld equations euler lagrange equations prime examples xed point equations 
method successively enforces individual xed point equations provides common way nding solutions variational problems closed form solution 
years number variational approaches successfully inference estimation large densely connected graphical probability models exact probabilistic calculations longer feasible see 
success derives primarily insights rst probabilistic inference problems lend naturally variational formulations second resulting variational optimization problems admit principled approximate solutions 
inherently approximate variational formulations optimization problems naturally facilitate nding approximate solutions 
example extremum problem involving unknown function solved approximately restricting space admissible functions terms nite number basis functions 
analogous restrictions factorization context probabilistic calculations 
primary goal tutorial illustrate inference estimation problems transformed variational form describing resulting approximation algorithms properties insofar currently known 
tutorial intended exhaustive merely highlight mathematical structure properties number variational approaches inference estimation calculations 
organized follows detailed handling examples variational formulations emphasizing general features 
followed brief graphical models derivation variational mean eld approximation context graphical models 
derive structured mean eld approximation variational factorization methods closely related large deviation techniques 
sections concern variational methods maximum likelihood bayesian estimation 
discussion open problems 
examples variational methods variational methods similar mathematical structure 
illustrate building simple examples variational methods 
basic insights derived variational methods carry mean eld approximation 
speci cally wish clarify transformation problem interest variational form resulting variational formulations admit approximate solutions 
start known variational formulation matrix inversion problem estimation context subsequently derive nite element methods variational solution poisson di erential equation 
matrix inversion estimation methods linear regression gaussian process models involve need invert large matrices 
purpose illustration provide variational formulation problem 
ideas suppose set input vectors fx corresponding scalar output values fy wish nd best linear predictor form vector parameters 
simplicity assume tting criterion squares 
squares optimal parameter setting dimension input vectors increases evaluating burdensome 
formulate variational approach computing see 
variational problem starts transformation optimization problem 
surprising start trivial transformation 
suppose knew solution problem evaluated certainly optimize respect nd distance measure weighted matrix deviations count directions input examples vary considerably 
variational formulation leading important realize couldn evaluate rst computing avoid apparent con ict proceed expand trivial objective function 
fact know form solution resulting expression rst term constant far parameters concerned drop 
constant term minimum attained new objective evaluate consulting easy verify convex function may nd helpful interpret rst linear term energy second quadratic term potential term playing role analogous entropy physics 
important progress 
obtain optimal solution minimizing nd approximate solution simply perform partial minimization 
done example conjugate gradient steps steps recover exact solution 
objective function serves metric guiding choice approximate solution need evaluate 
purpose initial exercise demonstrate basic underlying ideas 
transform original problem optimization problem objective evaluated solution sought 
transformation may require creativity argue cases quite natural 
return point 
second idea seek approximate solution variational objective guide selection simpler approximations 
finite element methods problems physics reduced solving di erential equations 
includes example nding temperature distribution material gauging material deformations 
simplest representative problems dimensional poisson di erential equation second derivative respect scalar argument source 
assume solution deformation satis es homogeneous boundary conditions 
number techniques exist solving problem 
best known nite element method see viewed variational method 
associated variational problem possesses number exemplary properties reason introducing 
context linear regression rst transform problem optimization problem subsequently search approximate solution 
nd optimization problem 
denote desired solution satisfying appropriate boundary conditions 
function forced zero boundary points degrees freedom left constant term function 
appropriate way compare estimate optimal solution done terms norm derivative dx serves valid distance measure 
minimizing objective surely recovers know solution 
turn objective form evaluate 
expanding integrand integrating parts form solution dx dx dx const 
dx dx const 
dx dx fact vanish boundary points 
drop rst constant term depends solution objective readily evaluated dx dx similarly previous example convex di erential operator linear linear transformation argument convex function preserves convexity 
solution course unique minimization respect equivalent minimizing original 
result transformed di erential equation optimization problem involving function derivative 
transformation exact sense minimizing objective recovers solution 
main bene variational formulation comes need nd approximate solution 
choose form approximate solution 
natural choice context nd best function linear subspace spanned set basis functions nite element methods basis functions derived local approximating functions discretization interval element 
words nd best solution form ranking solutions objective 
note basis functions con rm boundary conditions solution attempt admissible 
suces substitute form solution back objective function minimize respect free parameters linear coecients omit straightforward algebra clarity resulting objective looks dx ij dx de ning dx ij dx rewrite optimization problem matrix form conveniently exactly variational form matrix inversion problem discussed earlier course generally true variational methods 
necessary case sucient conditions optimality space functions considering obtained setting partial derivatives respect parameters zero 
case resulting xed point equations implying context nite element methods inverting typically somewhat easier basis functions design local support 
inner product matrix band diagonal 
nal observations concerning example 
nd approximate solution variational approach rst specify form solution 
second substituting desired solution form back objective function obtain variational problem time remaining free parameters 
note nding closed form solution variational parameters atypical variational problems solved iteratively 
brief graphical models provided section intuition derived examples guide derivation understanding mean elds 
brief graphical models feasibility working probability models large number variables depends dependent variables 
graphical model presence absence dependencies variables represented terms graph 
graphical representation nodes graph correspond variables probability model edges connecting nodes signify dependencies 
power graph representation arises rigorous connection separation properties graph independence statements pertaining underlying probability model 
main types graph models undirected directed 
distinction arises type edges graphs implies di erence independence semantics 
key problem graphical representation probability models explicate structure probability distribution consistent independence properties derive graph 
illustrates undirected graph model known markov random eld mrf short 
undirected graph models ordinary graph separation nodes isomorphic conditional independence statements variables associated nodes 
example graph states variables conditionally independent independence properties read graph impose factorization con 

undirected graph model boltzmann chain 
highlighted rst cliques undirected graph dotted lines 
simple directed graph model 
marginally independent dependent 
knowing value ect renders causes dependent 
semantics captured undirected graph 
straints probability distribution consistent graph 
words joint distribution expressed terms product nonnegative potential functions depending speci subset variables 
celebrated hammersley cli ord theorem see speci es form factorization joint distribution expressible product potential function cliques graph collection cliques graph fx set variables corresponding nodes clique notation index set variables 
normalization constant partition function plays important role 
exemplify concepts indicated rst cliques 
joint distribution consistent conditional independence properties derive chain structure factor 
important realize probability distribution may factor 
example distribution variables independent expressible product potentials depending single variable consistent graph 
computational cost exact probabilistic inference calculations undirected graph models depends size cliques 
precisely clique maximal set mutually connected nodes 
cost exponential size largest clique triangulated graph 
cliques triangulated graph arranged tree structure junction tree computations carried eciently 
graph triangulated cliques form tree 
directed graphical models second type graph models bayesian networks directed graphs 
directed graphs edges signify asymmetric relations variables loosely speaking edges follow causal ects 
separation properties graph correspond independence statements underlying probability model 
separation criterion separation criterion bit involved imposes simple structure joint probability distribution 
able write joint distribution product conditional probabilities parents pa variables directed arrows jx pa ensure joint distribution de ned directed graph acyclic directed cycles 
note don need normalization constant design 
interpret probability model corresponding directed graph undirected model set potential functions equal conditional probabilities jx pa pa corresponding undirected graph set nodes fully connected 
transformation undirected graph known moralization hides independence properties previously explicit directed graph 
directed graph models regularly transformed undirected models part exact probabilistic calculations see 
additional structure graphical models approximate inference methods rely additional structure joint distribution explicated graph 
example triangulate graph add edges cycle nodes chord 
probability model corresponding fully connected graph may factor product pairwise potential functions depending variables associated undirected edge collection edges graph absorbed normalization constant potentials 
note easily collect edge potentials larger clique potentials 
mean eld approximate inference algorithms heavily exploit type additional factorization structure 
clique potentials conditional probabilities may possess useful additional parametric structure factorization discussed 
parametric structure logistic regression models directly exploited approximate inference algorithms impose additional factorization breaking conditionals products smaller ones 
discuss variational methods purpose tutorial 
variational mean eld method ready apply intuition examples methods probabilistic inference problem graphical models 
start de ning problem 
graph corresponding probability distribution variables fx variables assumed observed instantiated fx remain hidden unobserved fx shorthand instantiation values variables fx sets variables disjoint fx assume notational simplicity variable takes values nite set 
inference problem fold evaluate marginal probability observed data log log summation possible instantiations hidden variables compute posterior probability jx hidden variables 
goals naturally tied evaluate posterior 
exact computation scales exponentially size largest clique induced triangulated subgraph hidden variables nodes 
tacitly assume graph densely connected exact computation practical 
rst step transform problem optimization problem 
apparently silly way log kl kullback leibler kl divergence kl log jx kl divergence positive zero variational distribution hidden variables equals true posterior probability jx 
maximizing respect recover log probability data log 
conclude silly optimization problem gives desired marginal maximum value posterior 
note non negativity kl divergence ensures variational distribution posterior lower bound desired log marginal probability log readily shown concave convex function variational distribution see 
remains show trivial transformation optimization problem useful 
clear evaluate objective function choice variational distribution explicate issue rewrite posterior probability appearing kl divergence terms joint distribution log log jx log log log log log eq log entropy variational distribution eq 
represents expectation respect observed variables remain xed instantiated values 
note variational distribution tries balance competing goals assign values hidden variables high probability second term time entertain large number distinct assignments entropy term 
feasibility evaluating depends types structure 
graph structure factorization original probability model second structure imposed variational distribution 
start exploiting structure original probability model suppose simplicity factorizes edges graph equation 
case log expectation reduces sum simpler terms eq log eq log log variational marginal probability variables associated edge insofar hidden 
note notational clarity dropped explicit hidden observed variables 
resulting objective simpler started 
merely transformed recover exact solution maximize objective respect variational distribution bene arises constraining solution variational distribution second type structure need 
context nite element methods section approximation terms linear basis functions 
case probability distributions appropriate simpli cation comes independence properties 
note may able evaluate partition function joint 
variational objective constant away desired log marginal 
simplest family variational distributions hidden variables fx independent 
precisely assume simple class distributions jhj degrees freedom adjusting variational marginals fq surely able evaluate :10.1.1.56.6066
fact entropy additive independent variables get log evaluation rst summation scales jhj number hidden variables number distinct values variable take 
analogously evaluating second summation term scales expectation involves variables jej edges 
fully factored distribution marginal probability variables associated edge obtained simply picking right components product 
general distributions obtaining marginals may involve considerable ort 
particular true assumption posterior distribution jx 
updating mean eld distribution having succeeded evaluating objective function restricted variable distribution need optimize marginals 
context nite element methods easily solve optimal linear coecients 
longer true setting resort iterative methods maximizing objective function class factored variational distributions equation 
marginals adjusted independently optimize marginal component time 
need bit notation 
eq 
stand expectation respect variational distribution similarly eq 
jx conditional expectation respect frequent conditional expectations provide explicit illustration eq log jx fx log set hidden nodes note expectation speci cally depend variational marginal 
result function conditioning variable update th variational marginal view function 
keeping remaining marginals xed 
emphasize may treat entropy terms corresponding remaining marginals constants appeal linearity expectation eq 
eq 
jx get const 
eq log dependence marginal explicit 
easy verify straightforward calculation maximizing objective respect marginal gives standard gibbs distribution cf 
eq flog xv jx 
local normalization constant partition function 
eq flog xv jx update equations collectively mean eld equations cf 

successive application updates correspond iteratively enforcing di erent mean eld equations 
note update carried closed form updates monotonically increase objective function 
necessarily nd best factored variational approximation 
unfortunate property follows fact concave jointly concave new restricted parameterization terms marginals fq order iterative updates carried initialization marginals ect locally optimal solution arrive 
brie explicate detail feasibility evaluating conditional expectations updates 
purpose factor edges graph 
similarly equation write eq flog jx log fh kg empty set refers single hidden node associated edge single marginal 
edges pertain node complexity evaluating conditional expectation nr 
quality variational approximation variational mean eld approximation explained arguably rough 
uses completely factored distribution approximate posterior distribution jx may possess strong dependencies hidden variables 
explore brie question approximation reasonable expect fail 
fact measures accuracy 
tightness lower bound marginal probability observed data set compute rst place 
words take di erence log gure merit approximation 
constraints variational distribution di erence vanish factored mean eld distribution 
measure pertains closely variational marginals fq match true posterior marginals jx 
maximizing respect equivalent minimizing kl divergence true posterior reasonable expect marginals aspire close 
example demonstrate measures need strongly coupled 
start discussing broad terms expect variational approximation accurate cf 
:10.1.1.29.2933
clearly posterior distribution hidden variables independent variational approximation nearly perfect closely represent true posterior factored variational distribution 
strong independence assumption longer holds expect accuracy measure degrade rapidly 
consider example mixture identical factored distributions 
components distinct factored variational distribution represent components dependencies arising switching 
particularly important setting factored distributions arise large densely connected graph model pairwise couplings variables relatively weak 
net ect large number fairly weak uences impinging variable converges law large numbers mean ect 
result variables nearly independent 
averaging ect underlies success mean eld methods large physical systems 
important undesirable property naive mean eld approximation exhibits spontaneous symmetry breaking 
happens optimal setting variational marginals asymmetric variables play symmetric role posterior distribution 
symmetry breaking generally selection posterior modes accounts poor correspondence variational true posterior marginals 
example speci cally geared clarifying issue 
example simplicity assume joint distribution binary variables suppose addition variables hidden observed variables variational formalism developed earlier marginal probability trying compute case simply normalization constant log log log reason compute value approximately fact value depend properties joint distribution permits easily evaluate accuracy lower bound function controlled changes joint 
add structure representation introducing single parameter controls dependent binary variables 
probability table table 
particular parameter signi es probability mass assigned con gurations consistent xor operation 
remaining probability mass divided equally leftover con gurations 
note joint distribution uniform captured factored variational distribution 
xor con gurations non zero probability factored distribution fails capture deterministic dependence variables 
varying study variational approximation degrades stronger dependencies 
obtain simply substitute simple distributions table symmetric xor dominated joint distribution binary variables probability mass falling xor con gurations controlled parameter general formulas derived earlier 
gives log factored variational distribution 
similarly exploit update equations xed point equations derived earlier eq flog jx log log right hand side evaluated marginal held xed 
update rule analogous 
obtain mean eld solution iteratively employing update rules 
discussed earlier solution may depend initial conditions 
variational marginals initialized uniform distributions subject slight random perturbations 
tracking mean eld solutions function increasing demonstrates spontaneous symmetry breaking 
critical value variational marginals remain xed 
match true marginals symmetry regardless parameter value critical value mean eld solution undergoes symmetry breaking objective prefers solution unequal marginals symmetry breaking arises entirely approximation true marginals remain xed 
see phase transition adverse ect quality variational marginals variational marginals suddenly rapidly diverge 
ect pronounced degree opposite objective function symmetry breaking rapid degradation lower bound slows see gure 
symmetry breaking forced improve lower bound 
resulting symmetry breaking function parameter dashed line represents alternative solution resulting di erent initialization 
lower bound function example simple arti cial provides insight larger problems 
example note slope lower bound zero joint distribution deviates factored distribution close 
naive mean eld approximation appears insensitive weak dependencies 
larger deviations accuracy lost accelerating pace 
example shows dicult guarantee variational marginals fq re ect true marginals 
simple case took fairly strong dependencies large values induce phase transition realistic problems large number variables associated dependencies er considerably ways initiating symmetry breaking 
ect limited symmetries persists generally posterior involves number competing modes variational marginals typically re ect marginals modes 
structured variational approach discussed section susceptible errors 
structured variational approach simple variational mean eld approach computationally attractive may yield suciently accurate results 
natural approach improving simple mean eld method combine exact probabilistic calculations extensions see 
words may able identify tractable substructures chains trees larger graph model substructures readily handled exact methods 
viable approach impose mean eld approximation substructures resorting exact calculations substructure 
rst problem identify substructures 
non trivial problem serious automated solutions proposed cf 

assume tractable substructures identi ed expert obtained means 
sets nodes corresponding substructures hm substructures induced subgraphs sets 
assume substructures create disjoint partition hidden variables hm second problem ensure apply exact probabilistic calculations subgraph variational framework 
achieved introducing constraints variational distribution substructure 
words variational distribution composed unconstrained components fq wish impose mean eld approximation substructures 
equivalent requiring variational distribution factors substructures 
consequently assume additional constraints 
update equations update equations resulting structured approximation exactly analogously simple mean eld 
intuition interpret structured mean eld method mean eld approach mega variables variational marginal updated eq log xv conditional expectation de ned computed analogously mean eld 
updates carried eciently 
depends joint distribution corresponding graph tractable induced subgraphs sets example illustrates detail 
suppose probability model consist coupled markov boltzmann chains shown see 
mean eld approximation variables chain assumed independent 
markov chain individually perfectly tractable improve mean eld approximation considerably decoupling variables chains 
chains original probability model loosely coupled expect structured mean eld approach quite accurate 

chain chain coupled boltzmann chains 
shaded smaller nodes denote observed variables 
develop fx observation sequence th chain collectively fx vm similarly fx sequence hidden states corresponding th markov chain substructure 
chains coupled probability distribution governing variables chain familiar form potential links successive hidden variables time connects observation time corresponding hidden state variable simplicity refer tractable chain structure single potential function 
general consider portion graph connecting substructures 
assume coupling substructures sparse 
joint distribution chains observations including couplings chains rst term represents independent chains second product term quanti es couplings state variables neighboring chains 
demonstrate structured mean eld approach tractable context remains evaluate conditional expectations eq flog jx equation 
computing expectations safely ignore terms depend conditioning variables terms automatically vanish normalization 
th chain relevant components joint distribution interactions th chain couplings neighboring chains 
eq flog jx const 
log eq flog eq flog const 
log log expectations eq 
eq 
taken respect variational marginals state variables chains respectively 
expression collected contributions neighboring chains ective terms log 
result structured mean eld updates additional terms original chain interactions provide independent evidence individual state variables change structure original distribution simply absorbed 
signi cant loss tractability incurred due uence chains structured mean eld approximation 
emphasize interactions substructures remained una ected updates 
optimal variational marginal substructure maintains original strength dependencies addition interaction structure 
uences substructures mediated ective potentials case pairwise couplings substructures appear additional biases individual variables 
related discussion see 
local variational approach variational mean eld approximation introduced previous sections relies suitable additional structure probability model 
additional structure expressed terms additional factoring joint distribution dictated graph pairwise potential functions 
absence factorization may nd useful structure probability model 
example conditional probabilities directed graph model potential functions undirected models may possess parametric structure exploit approximate inference calculations 
example consider noisy probability model binary variables interactions variables de ned terms probabilistic generalizations function 
conditional probabilities directed graph models jx pa pa ij words pass linear combination parents pa ij appropriate transfer function exp exp 
note input required 
setting increasing ij recover function limit fx pa 
local conditional probabilities potentials jx pa depend jpa variables 
number parents increases potentials eciently mean eld approximation directly stated 
cost dealing component potentials exponential number variables depend jpa 
attempt exploit parametric form conditional probabilities impose additional factorization 
ideally get jx pa pa ij product form parents decoupled 
selective factorization transformations may render remaining approximate joint distribution tractable 
alternatively may exploit resulting factorization part mean eld structured mean eld approximation 
remains show factorization achieved 
introduce class variational methods closely related large deviation methods purpose direct application large deviation theory approximate inference see 
approximate factorization provided terms upper lower bounds uncontrolled approximations 
start example large deviation theory see 
large deviation example suppose wish derive standard large deviation result sum independent identically distributed binary variables tails distribution governing sum vanish exponentially fast 
wish capture probability sum deviates expected value np arbitrary 
generative probability event consider sided probability step expectation taken respect product distribution step zero 
step function inside expectation captures appropriate event 
interpret step function transfer function step analogously noisy model discussed 
large deviation probability viewed marginal probability marginalized parents binary variable 
simple case unable obtain closed form expression expectation 
hand evaluating expected value factored approximation respect product distribution done eciently term term basis product expectations respect individual binary variables 
turn original expectation factored form variational transformation step function step min exp serves variational parameter 
understand transformation note increasing decreases exp exponent negative 
letting results exp desired 
hand exp minimized setting 
gives exp 

note optimal setting variational parameter function function step exp 
transformation exact useful 
similarly variational methods obtain controlled approximation restricting choice variational parameters 
require choice variational parameter function constant values gives simple upper bound step function step exp usefulness bound immediate large deviation context step exp exp exp exp variables independent evaluate expectation right hand side respect product distribution term term basis 
expectations identical identically distributed 
gives exp exp exp exp expression comes expectation respect bernoulli distribution improve result utilizing degree freedom choosing 
optimal choice minimizing resulting bound log min log exp 
max log exp expression pulled negative sign minimization turning maximization 
term obtained maximization precisely large deviation rate function see 
basic information theoretic bounds speci cally cherno bounds result simple factorization transformations 
representation theorem exploit factorization transformations generally probabilistic inference calculations need nd appropriate variational transformation situation 
transformations exist family conditional probabilities 
surprisingly question answered factorization transformation exists 
theorem precise theorem jx pa conditional probability model values nite set 
assume number possible instantiations parents pa nite 
variational parameter values nite nitely dimensional set exists non negative pairwise potentials pa jx pa max pa min pa pa 
emphasize merely existence proof mean nd useful transformations lead ecient accurate approximate inference 
finding suitable transformation speci family conditional probabilities apart log concave class generalized linear models discussed remains open problem 
example log concave models useful variational transformations conditional probabilities leading additional factorization systematically log concave class generalized linear models :10.1.1.53.8556
family conditional probabilities includes noisy logistic regression models 
precisely characterized conditional probabilities form jx pa pa ij transfer function 
log concave log concave function argument values exploit concavity property linear predictive structure 
start noting product decomposition equation equivalent additive decomposition log scale 
words achieve jx pa pa ij suces nd additive approximation context log pa ij pa ij simply choose ij log ij preserve equality 
argument log 
desired additive structure merely need nd linear approximation log 
fact log concave guarantees nd linear upper bound approximation rst order taylor expansion 
illustrates log logistic function 
example expanding log point gives log log log log log log log concave convex di erentiable functions set brackets expressed terms gradient note varying point expansion note example strictly concave di erentiable functions gradient monotonically decreasing function invertible 
point example expressed function gradient evaluated equivalent varying gradient space 
may take variational parameter explicitly referring simple explanation captures general duality property concave convex functions concave function log conjugate dual function concave log min takes values domain 
duality comes fact concave function similarly expressed terms log conjugate conjugate function function 
substituting linear upper bound equation log conditional probability separately gives log pa ij pa ij additive expansion follows identifying ij absorbing remaining terms potentials 
variational transformation comes adjustable parameter optimize approximation appropriate context just large deviation example 
table explicates transformations typical members log concave family 
name log conjugate function domain noisy log exp log log logistic log exp log log table upper bound variational transformations noisy logistic functions 
parameter estimation variational methods explain variational lower bound marginal likelihood discussed earlier maximum likelihood ml parameter estimation 
variational approach leads standard em algorithm maximization step place original step 
variational approach remains applicable step em algorithm longer computed exactly guarantees monotonically increasing sequence lower bounds log likelihood 
concave function log logistic function linear variational upper bound 
ideas fx set observations 
assume notational simplicity set observed variables observations 
words division observed hidden variables fx data points 
goal maximize log likelihood data log denotes adjustable parameters joint distribution 
assume parameter estimation problem carried ef ciently observations complete 
transform loglikelihood objective form involves complete data introduce separate variational transformation log marginal probabilities sum 
gives log recall maximizing respect recovers corresponding log marginal likelihood log 
maximizing respect variational distributions recover ml objective max take advantage variational formulation maximize directly maximize variational objective alternating maximization steps 
rst step maximize variational objective respect distributions keeping parameters xed 
constraints imposed variational distributions obtain jx maximum value variational objective equals 
second step variational distributions remain xed maximize variational objective respect parameters 
step max max algorithm leads monotonically increasing log likelihood data 
see denote maximization step successively priming corresponding parameters 
obtain chain inequalities inequality strict maximization steps improve variational objective 
reached local optimum 
algorithm fact precisely standard 
step em algorithm corresponds rst maximization step respect variational distributions maximization step results setting variational distributions equal posterior probabilities hidden variables 
evaluation variational objective equation jx gives expected complete log likelihood data step 
additional entropy terms variational objective kept xed second maximization step inconsequential 
see 
em algorithm variational formulation remains applicable longer handle posterior probabilities jx 
restrict variational distributions example class completely factored mean eld distributions 
rst maximization step carried incompletely restricted class 
guarantee monotonically increasing lower bound log likelihood 
guarantee suces practice depends accuracy structured mean eld approximation 
variational bayesian methods parameter estimation bayesian framework reduces inference problem evaluating posterior probability parameters observed data 
suspect variational framework developed earlier approximate inference context 
case couple additional diculties 
parameters excluding model structure typically continuous discrete making harder represent posterior probabilities 
second parameter setting needs evaluated observed data merely context single observation 
computing distribution parameters data points treated individually set 
context incomplete observations longer suces infer posterior probabilities hidden variables independently observation posteriors contingent speci parameter setting consider settings 
incomplete observations quite dicult handle exactly bayesian framework 
start simpler setting observation assumed complete value assignment variables probability model 
moment drop denoting set visible variables 
goal evaluate posterior probability parameters observed data jd dj prior probability parameters marginal data likelihood ability evaluate determines estimation problem tractable 
computing type inference problem solved 
relevant joint distribution dj factors data points 
component joint factor smaller components product remain tractable 
observations complete case 
assume addition distinct parameters associated di erent factors parameters priori independent prior distributions conjugate corresponding likelihoods typically evaluate marginal data likelihood closed form 
parameter independence conjugate form priors may re ect prior knowledge 
prior distributions associated independence assumptions may necessitate approximate methods evaluating posteriors 
typical approximate computations involve sampling methods 
important useful various aspects bayesian calculations discuss 
number excellent sources available 
focus alternative degree complementary approach variational methods 
formally application variational approach bayesian parameter estimation problem straightforward introduce variational distribution parameters evaluate lower bound log marginal likelihood data cf 
log log dj log log imposing constraints recover log maximizing lower bound respect variational distribution 
maximum jd desired 
additional factorization simpli es necessary expectations respect variational distribution example xj may factor directed graph permitting write xj jx pa conditional probability depends distinct set parameters long prior distribution factors parameters associated conditional probabilities posterior 
may assume loss generality 
variational lower bound reduces case log log log jx pa course recover true marginal likelihood true posterior maximizing respect variational distributions 
cases component posteriors jd evaluated closed form 
example case logistic regression models jx pa pa ij logistic function 
case apply variational formalism constraining variational posteriors fq simpler parametric forms multivariate gaussian distributions 
variational lower bound evaluated closed form combine restriction additional approximations expectations log jx pa log pa ij eciently lower bounded expectation inside logarithm log 
convex function see re ned lower bound 
may impose additional factorization logistic function alluded earlier tutorial resort transformations speci cally tailored logistic function :10.1.1.29.2933
bayesian estimation parameters hyper parameters may preclude exact computations 
prior distribution parameters case marginal hyper parameters wish infer posterior probability parameters hyper parameters jd 
marginal evaluated closed form may rely variational approach provided restrict factored variational distributions see 
earlier assessment accuracy variational mean eld approach applies case 
expect approach accurate parameters hyper parameters loosely coupled 
discussed earlier may dangerous resulting product variational marginals proxy true posterior jd particularly true posterior contains multiple modes 
incomplete cases situation substantially complex incomplete cases data set 
start making simplifying assumptions 
assume xed division hidden observed variables fx data points 
refrain discussing joint distributions components exponential family non conjugate prior distributions 
aspects discussed previous section 
assume xed setting parameters posterior probabilities hidden variables jx computed feasible manner cf 
:10.1.1.29.2933:10.1.1.36.5675
observed cases dataset complete likelihood term pertaining parameters factors observations dj components may lack factorization fact force infer posterior hidden con gurations variables parameters serious impediment 
worse posteriors hidden variables corresponding observation depend speci setting parameters jx 
apply variational framework long explicitly remove direct dependencies parameters hidden con gurations 
put way impose factored structure variational distribution :10.1.1.29.2933:10.1.1.29.2933:10.1.1.36.5675:10.1.1.36.5675
lower bound marginal data likelihood corresponding variational distribution obtained fairly easily 
variational distribution hidden variable con gurations parameters independent introduce variational lower bounds stages rst parameters marginals log 
words log log log note hidden variables may ect part model marginal probabilities observation may possess useful factorization 
log log rst lower bound comes equation second mean eld 
emphasize maximizing resulting lower bound respect variational distributions longer hope recover true marginal likelihood 
true posterior parameters hidden con gurations represented restricted class variational distributions 
lower bound optimize respect variational distributions 
done successively maximizing bound respect variational marginals keeping marginals xed 
minor modi cations borrow update equations earlier derivations see section 
update flog expectation taken respect current xed estimate 
note exponent update rule function 
removed parameters common correlates hidden variable con gurations variational distributions fq updated independent 
second iterative step update variational parameter distribution keeping fq xed log flog expectations exponent taken respect 
nd true posterior distribution parameters special cases updates monotonically increase lower bound marginal data likelihood 
nal observations accuracy variational bayesian approach 
true posterior parameters case surely contain multiple modes 
modes arise di erent possible con gurations hidden variables corresponding observation 
factored nature posterior approximation previous analysis accuracy variational mean eld applicable 
suspect variational posterior re ect posterior modes 
identity selected mode depends initialization variational distributions order updates carried possible di erences posterior weight modes 
discussion focus tutorial formulation variational methods inference estimation problems graphical models associated algorithms 
topics covered diverse tutorial remains respects complementary 
dispensed discussing number variational approaches inference estimation 
example mean eld approximation higher order extensions viewed recursive propagation algorithms 
may go simple disjoint factorization assumption context structured mean eld approach example directed graphical models variational approximating distributions see 
variational approximations inference mixed graphical models containing continuous discrete variables 
terms bayesian estimation variational methods lend naturally line approximation algorithms remain applicable structured bayesian priors brie mentioned text :10.1.1.29.2933:10.1.1.36.5675
treated variational methods tutorial standalone approximation techniques naturally combined approximation techniques sampling methods 
upper lower bounds rejection sampling setting uses variational distributions proposal distributions context importance sampling method 
number combinations extensions possible 
main open problems variational approximation methods characterizing accuracy 
obtain performance guarantees speci classes graphical models upper lower bounds obtained variational formulations provide guarantees speci instantiations inference problem serve priori guarantees 
open problem concerns focusing inference calculations variational approach 
particularly important context decision making 
note graph structure relevant probability model typically xed priori estimation inference problems 
leaves option simple graph model exact inference algorithms adopting expressive models cost having employ approximate inference methods 
little characterizing conditions approach preferable 
error simpler model class greater error resulting approximate inference 
attias 
inferring parameters structure latent variable models variational bayes 
proceedings fifteenth annual conference uncertainty arti cial intelligence uai pages san francisco ca 
morgan kaufmann publishers 
barber 
tractable variational structures approximating graphical models 
kearns solla cohn editors advances neural information processing systems volume 
mit press 
besag 
spatial interaction statistical analysis lattice systems 
journal royal society 
bishop lawrence jaakkola jordan 
approximating posterior distributions belief networks mixtures 
michael jordan michael kearns sara solla editors advances neural information processing systems volume 
mit press 

large deviation techniques decision simulation estimation 
john wiley sons 
cover thomas 
elements information theory 
john wiley sons 
dayan hinton neal zemel 
helmholtz machine 
neural computation 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society :10.1.1.29.2933:10.1.1.36.5675
ghahramani beal 
variational inference bayesian mixtures factor analysers 
solla leen 
mller editors advances neural information processing systems volume 
mit press 
ghahramani jordan 
supervised learning incomplete data em approach 
jack cowan gerald tesauro joshua alspector editors advances neural information processing systems volume pages 
morgan kaufmann publishers 
ghahramani jordan 
factorial hidden markov models 
machine learning 
gibbs mackay 
ecient implementation gaussian processes interpolation 
unpublished manuscript 
hofmann tresp 
model independent mean eld theory local method approximate propagation information 
network computation neural systems 
david heckerman dan geiger david chickering 
learning bayesian networks combination knowledge statistical data 
machine learning 
hinton dayan frey neal 
wake sleep algorithm unsupervised neural networks 
science 
jaakkola jordan 
computing upper lower bounds likelihoods intractable networks 
proceedings twelfth annual conference uncertainty arti cial intelligence uai pages portland oregon :10.1.1.29.2933:10.1.1.36.5675
jaakkola jordan 
variational probabilistic inference qmr dt database 
journal arti cial intelligence research 
jaakkola jordan 
bayesian parameter estimation variational methods 
statistics computing 
jaakkola 
variational methods inference learning graphical models 
ph thesis mit 
jaakkola jordan 
improving mean eld approximation mixture distributions 
michael jordan editor proceedings nato asi learning graphical models 
kluwer 
jaakkola jordan 
recursive algorithms approximating probabilities graphical models 
michael mozer michael jordan thomas petsche editors advances neural information processing systems volume page 
mit press 
jensen lauritzen olesen 
bayesian updating causal probabilistic networks local computations 
computational statistics quarterly 
jordan ghahramani jaakkola saul 
variational methods graphical models 
machine learning 
jordan ghahramani saul 
hidden markov decision trees 
michael mozer michael jordan thomas petsche editors advances neural information processing systems volume page 
mit press 
kapur 
maximum entropy models science engineering 
john wiley sons 
kearns saul 
large deviation methods approximate probabilistic inference 
proceedings fourteenth annual conference uncertainty arti cial intelligence uai pages san francisco ca 
morgan kaufmann publishers 
kearns saul 
inference multilayer networks large deviation bounds 
kearns solla cohn editors advances neural information processing systems volume 
mit press 
lauritzen 
graphical models 
oxford university press 
lauritzen spiegelhalter 
local computations probabilities graphical structures application expert systems 
journal royal statistical society 
mackay 
ensemble learning hidden markov models 
unpublished manuscript 
mccullagh nelder 
generalized linear models 
chapman hall 
murphy 
variational approximation bayesian networks discrete continuous latent variables 
proceedings fifteenth annual conference uncertainty arti cial intelligence uai pages san francisco ca 
morgan kaufmann publishers 
neal 
connectionist learning belief networks 
arti cial intelligence 
neal hinton 
view em algorithm justi es incremental sparse variants 
michael jordan editor proceedings nato asi learning graphical models 
kluwer 
neal 
probabilistic inference markov chain monte carlo methods 
technical report crg tr dept computer science university toronto 
neal 
bayesian learning neural networks 
number lecture notes statistics 
springer new york 
parisi 
statistical eld theory 
addison wesley 
pearl 
probabilistic reasoning intelligent systems 
morgan kaufmann 
rockafellar 
convex analysis 
princeton university press 
saul jordan 
learning boltzmann trees 
neural computation 
saul jaakkola jordan 
mean eld theory sigmoid belief networks 
journal arti cial intelligence research 
saul jordan 
boltzmann chains hidden markov models 
tesauro touretzky leen editors advances neural information processing systems volume pages 
mit press 
saul jordan 
exploiting tractable substructures intractable networks 
david touretzky michael mozer michael hasselmo editors advances neural information processing systems volume pages :10.1.1.29.2933:10.1.1.36.5675
mit press 
schwarz 
finite element methods 
academic press 

dynamic trees structured variational method giving cient propagation rules 
proceedings sixteenth annual conference uncertainty arti cial intelligence uai san francisco ca 
morgan kaufmann publishers 
whittaker 
graphical models applied multivariate statistics 
john wiley sons 

variational approximations mean eld theory junction tree algorithm 
proceedings sixteenth annual conference uncertainty arti cial intelligence 
williams rasmussen 
gaussian processes regression 
david touretzky michael mozer michael hasselmo editors advances neural information processing systems volume pages :10.1.1.29.2933:10.1.1.36.5675
mit press 

