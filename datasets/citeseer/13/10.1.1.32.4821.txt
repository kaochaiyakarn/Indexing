icml enhancing supervised learning unlabeled data sally goldman sg cs wustl edu yan zhou zy cs wustl edu department computer science washington university st louis mo usa practical learning scenarios small amount labeled data large pool unlabeled data 
supervised learning algorithms developed extensively studied 
new training strategy unlabeled data improve performance standard supervised learning algorithms 
prior training procedure blum mitchell assume redundant views sufficient perfect classification 
requirement training strategy places supervised learning algorithm hypothesis partitions example space set equivalence classes decision tree leaf defines equivalence class 
evaluate training strategy experiments data uci repository 

practical learning scenarios small amount labeled data large pool unlabeled data 
supervised learning algorithms id developed extensively studied 
unlabeled data improve accuracy 
intuitively expect different supervised learning algorithms complement different representations hypotheses provided labeled data different ways 
expect algorithms notice different patterns data able label unlabeled data 
motivation training strategy 
new training strategy unlabeled data improve performance standard supervised learning algorithms 
prior training procedure blum mitchell assume redundant views sufficient perfect classification 
training algorithm different supervised learning algorithms originally trained provided labeled data 
statistical techniques learner select unlabeled data label learner 
repeat process long data selected labeled 
combine resulting hypotheses obtain final hypothesis 
requirement training strategy places supervised learning algorithm hypothesis partitions example space set equivalence classes decision tree leaf defines equivalence class 
key issues resolve designing training algorithm 
simple technique statistical confidence intervals combining hypothesis supervised learning algorithms obtain final hypothesis 
aim hypothesis correctly classify portions instance space correctly classified hypotheses combined 
error rate better hypotheses combined 
second methodology deciding supervised learning algorithm label data 
confident learning algorithm prediction labels data 
learning algorithms sure predictions fact hypotheses may significant error rates additional care taken ensure sufficiently low classification noise rate noise labels training data sets expanded 
evaluate training strategy experimental results data uci repository id quinlan kohavi supervised learning algorithms 
id trained labeled data training strategy provided pool unlabeled data 
results promising 
different runs different data sets training strategy yielded improvement id improvement improvement algorithm picks better id individual run 
remaining organized follows 
section outlines related unlabeled data improve performance supervised learning algorithms 
section cotraining algorithm describe theoretical foundations techniques 
section presents empirical results 
section discuss directions 

related different approaches studied unlabeled data improving performance supervised learning algorithms 
classic methods learning unlabeled data generative model classifier expectation maximization em dempster laird rubin approaches model label estimation parameter estimation general model nigam mccallum thrun mitchell wu huang 
uses distribution unlabeled data define metric kernel function perform sanity check support vector machine trained labeled data schuurmans hofmann zhang 
idea large margin classification transductive inference inspires unlabeled data 
large margin classification algorithms favor decision rules achieve large classification margins creates dependencies unlabeled data parameters function class jaakkola meila jebara 
approaches unlabeled data probability estimation target weight estimated labeled data substituted estimates probabilities query kearns learning algorithm done de comit denis 
related area research different goals active learning dagan engelson liere tadepalli lewis algorithm repeatedly selects unlabeled example asks expert human provide correct label rebuilds hypothesis 
field query learning unlabeled data label querying campbell cristianini smola 
important component active query learning selection unlabeled example 
training technique assume expert available provide labels 
techniques introduce useful active learning 
study built initial performed blum mitchell 
show unlabeled data augment labeled data provided instance space represented different views independent redundant sets attributes 
example learning classify web pages high low quality look links page links page 
strong assumption view example sufficient labeled data 
goal large set unlabeled data augment smaller set labeled examples 
different training strategy situation gave empirical theoretical results showing strategy setting 
see craven additional results training classifying web pages 
idea training assumption natural redundancy data collins singer 
significant studying applications area text classification 
example riloff jones consider task learning classify noun phrase positive negative example location 
redundant sufficient features come looking noun phrase linguistic context noun phrase appears 
yarowsky uses similar training approach disambiguate word sense determine word plant refers manufacturing plant botanical plant 
settings independent sufficiently redundant views settings redundant views available 
training procedure general situations redundant views 

training method describe training algorithm provide theoretical basis design 
assume different supervised learning algorithms output hypothesis defines partition instance space 
example decision tree partitions instance space equivalence class defined leaf 
maintain set unlabeled data original labeled data set la data labeled initially empty set lb data labeled initially empty 
keep estimate wa respectively wb number examples la respectively lb mislabeled 
making estimates bias overestimating errors causing training conservative labeling data 
refer estimates conservative estimates 
training algorithm repeats steps la lb change iteration 
start iteration train algorithm labeled examples la obtain hypothesis ha similarly train lb obtain hb algorithm considers equivalence classes decides ones label data algorithm 
tests satisfied labeling data 
ensures equivalence class label data accuracy accuracy hypothesis 
second test help prevent degradation performance due increased noise labels 
data equivalence class ha passes tests labels data places lb labels data manner 
completes round procedure 
detailed pseudo code combining method table detailed pseudo code cotraining algorithm table 
discuss key aspects training algorithm combining ha hb get hypothesis selecting data algorithm label round 
combining section describe hypotheses ha hb combined 
estimate accuracy ha hb confidence interval binomial parameter see larson marx 
hypothesis equivalence class hypotheses fold cross validation compute correct predictions 
compute confidence interval denote 
prediction example compare confidence intervals equivalence class contains equivalence class contains predict hypothesis corresponds maximum quantities 
example mean confidence interval equivalence class ha containing may lower mean confidence interval equivalence class hb containing predict ha ha highest confidence want cautious predicting better ha hb confidence interval mean equivalence class hb containing higher ha hypothesis predict hb performing cross validation originally labeled data labeled data including labeled algorithm 
earlier tests originally labeled data 
advantage originally labeled data confidence interval accurate data labeling errors 
computing confidence interval estimate equivalence classes just original labeled data equivalence classes data 
equivalence class example data mean confidence interval larger maximum leaf confidence intervals predict regardless confidence leaf 
choosing examples label algorithm take unlabeled example place lb labeled ha intuitively consider placing example lb confidence validity label better confidence validity label amount data labeled sufficient compensate increased classification noise cause training rounds 
address question decide examples sufficient confidence prediction label vice versa 
combining method confidence interval binomial parameter 
hypothesis algorithm hb compute confidence interval denoted hb 
equivalence class defined ha compute confidence interval 
high confidence interval equivalence class ha higher low hb examples equivalence class pass test added lb describe second test designed control classification noise rate labeling examples 
test relationship hypothesis worst case accuracy gamma ffl table 
technique combine ha hb create hypothesis 
combine ha hb hb hypotheses combined fold cross validation compute confidence interval ha ha fold cross validation compute confidence interval hb hb example instance space equivalence class ha containing fold cross validation compute confidence interval exs equivalence class hb containing fold cross validation compute confidence interval exs ha 
hb nn ha accurate hb empty ha gamma maxf hb predict ha predict hb ha 

predict ha predict hb nn hb accurate ha empty hb gamma maxf ha predict hb predict ha hb 

predict hb predict ha sample size classification noise rate assume parameters held constant angluin laird ffl gamma equivalently ffl gamma constant 
relationship decide amount additional data labeled sufficient compensate increase classification noise rate 
simplify computation formula compute square inverse error 
specifically training round algorithm chooses data label algorithm follows 
current hypothesis values number labeled training examples classification noise rate jl lb conservative estimate wb jl lb estimate ffl jl lb gamma wb jl lb test passed compute set examples ha maps equivalence class low confidence interval conservatively estimate number examples mislabeled gamma ju compute estimate square inverse error examples labeled jl lb gamma wb jl lb indicating belief hb improved examples labeled ha added lb update lb wb accordingly 
corresponding method select data label training procedure enables algorithm label significant amount data round tends require iterations 

evaluation describe experimental results 
supervised algorithms referred general technique id quinlan decision tree algorithm kohavi decision graph algorithm 
algorithms selected simply available part mlc 
view arbitrary algorithms satisfy property hypotheses form partition data 
id id quinlan top induction decision tree algorithm 
criteria splitting tree table 
training method 
nn initialization labeled data set unlabeled data set la nn data labeled wa nn conservative estimate mislabeled examples la lb nn data labeled wb nn conservative estimate mislabeled examples lb nn main training loop fold cross validation compute confidence interval ha ha fold cross validation compute confidence interval hb hb repeat la lb change iteration training round run algorithm labeled data la obtain hypothesis ha jl la gamma wa jl la estimate ffl run algorithm labeled data lb obtain hypothesis hb jl lb gamma wb jl lb estimate ffl nn choose data label equivalent class defined ha examples map fold cross validation compute confidence interval test passed labeling examples gamma ju estimate exs mislabeled ha jl lb gamma wb jl lb nn estimated error rate decrease labeled exs examples labeled ha lb lb wb wb nn choose data label equivalent class defined hb examples map fold cross validation compute confidence interval test passed labeling examples gamma ju estimate exs mislabeled hb jl la gamma wa jl la nn estimated error rate decrease labeled exs examples labeled hb la la wa wa nn final hypothesis predict combine ha hb table 
test data characteristics summary results 
data set show number test points misclassified id dt dg training algorithm com runs median performance 
average improvement improvement error rate best dt dg com round 
data number errors round avg 
run set attributes alg 
improv 
jlj ju dt dg run shown com dt breast cancer dg run shown com dt corral dg run shown com dt flare dg run shown com dt monk dg run shown com dt mux dg run shown com dt vote dg run shown com dt xd dg run shown com information gain attributes 
attribute lowest average entropy highest information gain root current subtree 
rest tree built recursively 
kohavi stands hill climbing oblivious decision graph 
mlc inducer building oblivious decision graphs bottom 
id replication duplication subtrees disjunctive concepts fragmentation partitioning data fragments problems 
experimental results experimental results 
data sets uci machine learning database merz murphy 
data sets choose reduce amount labeled data provided rest unlabeled data 
picked size labeled data set supervised learning algorithms mediocre performance setting training algorithm designed 
intended setting labeled data highly accurate predictions labeled data bootstrapping process training procedure 
increase size provided labeled data put placed labels test data form data set performed independent runs different random selection labeled data table shows relevant characteristics data sets empirical tests summary results 
shows results runs flare data graphical form 
data set performed better id just initial labeled data round 
cotraining procedure helped algorithms improve performance 
shows results runs breast cancer data set 
data set id better performance 
generally see hypotheses improved training 
number training rounds error rate combined id 
results flare data set run 
number training rounds error rate combined id 
results breast cancer data set run 
different data sets independent runs showed different characteristics terms id interact runs training method reduced error rate id 
runs obtained result better id 
run performance go small amount case error went 
runs xd training procedure able reduce error rate 
summary reduction error rates obtained training shown table 
average number training rounds round included round 

new training strategy learning labeled data unlabeled data settings redundant views data 
empirical results demonstrate standard supervised learning algorithms successfully label data 
interesting directions research 
currently standard confidence interval binomial parameter 
tests tried confidence intervals 
high confidence interval allow table 
summary results 
algorithm avg 
error rate id better id algorithm com data labeled low confidence label allows data labeled 
tried confidence rated boosting procedure schapire singer 
general trained different data sets different sizes 
technique confidence rating relative hypotheses obtained training data 
exploring ways improve estimation confidence interval important direction 
variation training procedure studied follows 
iteration algorithm label examples equivalence class highest confidence level 
significantly increasing number training rounds error rates approach 
considered increasing number equivalence classes currently breaking decision graph level higher uses leaves internal nodes direct ancestors leaf 
improve performance 
clearly additional variations specific training technique interesting explore believe improvements possible 
plan perform additional empirical studies real data application areas redundant set features 
addition plan running test different supervised learning algorithms id 
interesting research direction explore techniques active learning help decide examples valuable labeled 
performing empirical studies hope develop theory training procedure better understand appropriate improve 
acknowledgments authors gratefully acknowledge support national science foundation ccr dan dan blandford useful discussions 
angluin laird 

learning noisy examples 
machine learning 
blum mitchell 

combining labeled unlabeled data training 
proceedings eleventh annual conference computational learning theory pp 

campbell cristianini smola 

query learning large margin classifiers 
unpublished manuscript 
collins singer 

unsupervised models named entity classification 
proceedings joint sigdat conference empirical methods natural language processing large corpora 
craven dipasquo freitag mccallum mitchell nigam slattery 

learning extract symbolic knowledge world wide web 
proceedings fifteenth national conference artificial intelligence pp 

ido dagan sean engelson committee sampling training probabilistic classifiers 
proceedings twelfth international conference machine learning pp 

san francisco morgan kaufmann 
de comit denis gilleron 

positive unlabeled examples help learning 
proceedings tenth international conference algorithmic learning theory pp 

dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
kohavi 

bottom induction oblivious read decision graphs 
proceedings european conference machine learning hofmann 

text categorization labeled unlabeled data generative model approach 
working notes nips workshop unlabeled data supervised learning jaakkola meila jebara 

discrimination 
technical report mit 
mit ai lab cambridge ma 
kearns 

efficient noise tolerant learning statistical queries 
proceedings annual acm symposium theory computing pp 

new york acm press 
larsen max 

mathematical statistics applications 
prentice hall 
lewis 

evaluating optimizing autonomous text classification systems 
proceedings eighteenth annual international acm special interest group information retrieval pp 

new york acm press 
liere tadepalli 

active learning committees text categorization 
proceedings fourteenth national conference artificial intelligence pp 

merz murphy 

uci repository machine learning databases 
nigam mccallum thrun mitchell 

text classification labeled unlabeled documents em 
machine learning 
quinlan 

induction decision trees 
machine learning 
riloff jones 

learning dictionaries information extraction multi level bootstrapping 
proceedings sixteenth national conference artificial intelligence pp 

schapire singer 

improved boosting algorithms confidence rated predictions 
machine learning 
schuurmans 

new metric approach model selection 
proceedings fourteenth national conference artificial intelligence pp 

shawe taylor 

margin transduction 
working notes nips workshop unlabeled data supervised learning wu huang 

unlabeled data supervised learning discriminant em algorithm 
working notes nips workshop unlabeled data supervised learning yarowsky 

unsupervised word sense disambiguation supervised methods 
proceedings third annual meeting association computational linguistics pp 

zhang 

asymptotic results concerning value unlabeled data 
working notes nips workshop unlabeled data supervised learning 
