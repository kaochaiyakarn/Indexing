scalable distributed data structures internet service construction steven gribble eric brewer joseph hellerstein david culler university california berkeley brewer cs berkeley edu presents new persistent data management layer designed simplify cluster internet service construction 
self managing layer called distributed data structure dds presents conventional single site data structure interface service authors partitions replicates data cluster 
designed implemented distributed hash table dds properties necessary internet services incremental scaling throughput data capacity fault tolerance high availability high concurrency consistency durability 
hash table uses phase commits coherent view data cluster nodes allowing node service task 
show distributed hash table simpli es internet service construction decoupling service speci logic complexities persistent consistent state management allowing services inherit necessary service properties dds having implement properties 
scaled hash table node cluster terabyte storage core read throughput operations write throughput operations internet services successfully bringing infrastructural computing masses 
millions people depend internet services applications searching instant messaging directories maps safeguard provide access personal data email calendar entries 
direct consequence increasing user dependence today internet services possess properties telephony power infrastructures 
service properties include ability scale large rapidly growing user populations high availability face partial failures strictly maintaining consistency users data operational manageability 
challenging service achieve properties especially manage large amounts persistent state state remain available consistent individual disks processes processors crash 
unfortunately consequences failing achieve properties harsh including lost data angry users nancial liability 
worse appear reusable internet service construction platforms data management platforms successfully provide properties 
projects products propose software platforms clusters address challenges simplify internet service construction :10.1.1.1.2034
platforms typically rely commercial databases distributed le systems persistent data management address data management forcing service authors implement service speci data management layer 
argue databases le systems designed internet service workloads service properties cluster environments speci cally mind result fail provide right scaling consistency availability guarantees services require 
bring scalable available consistent data management capabilities cluster platforms designing implementing reusable cluster storage layer called distributed data structure dds speci cally designed needs internet services 
dds presents conventional single site memory data structure interface applications manages data interface distributing replicating cluster 
services inherit aforementioned service properties dds store manage persistent service state shielding service authors complexities scalable available persistent data storage simplifying process implementing new internet services 
believe small set dds types hash table tree administrative log authors able build large class interesting sophisticated servers 
describes design architecture implementation distributed data structure distributed hash table built java 
evaluate performance scalability availability ability simplify service construction 
clusters workstations argued clusters workstations commodity pc high performance network natural platform internet services :10.1.1.1.2034
cluster node independent failure boundary means replicating computation data provide fault tolerance 
cluster permits incremental scalability service runs capacity software architecture allows nodes added cluster linearly increasing service capacity 
cluster natural parallelism appropriately balanced cpus disks network links simultaneously increasing throughput service cluster grows 
clusters high throughput low latency redundant system area networks san achieve gb throughput latency 
internet service workloads popular internet services process hundreds millions tasks day 
task usually small causing small amount data transferred computation performed 
example press releases yahoo www 
yahoo com serves page views day 
randomly sampled pages yahoo directory average kb html data kb image data 
similarly aol web proxy cache www aol com handles web requests day average response size kb 
services take hundreds milliseconds process task responses take seconds ow back clients predominantly low bandwidth hop network links 
high task throughput non negligible latency service may handle thousands tasks simultaneously 
human users typically ultimate source tasks users usually generate small number concurrent tasks parallel get requests typically spawned user requests web page large set tasks handled service largely independent 
distributed data structures distributed data structure dds storage layer designed run cluster workstations handle internet service workloads 
dds previously mentioned service properties high throughput high concurrency availability incrementally scalability strict consistency data 
service authors see interface dds conventional data high level view dds dds self managing cluster data repository 
service instances cluster see consistent image dds result wan client communicate service instance 
structure hash table tree log 
interface dds platform hides mechanisms access partition replicate scale recover data 
complex mechanisms hidden simple dds interface authors need worry logic implementing new service 
dicult issues managing persistent state handled dds platform 
shows high level illustration dds 
cluster nodes access dds see consistent image dds 
long services keep persistent state dds service instance cluster handle requests client expect clients anity particular service instances allow session state accumulate 
idea having storage layer manage durable state new course databases le systems done decades 
novel aspects dds level abstraction presents service authors consistency model supports access behavior concurrency throughput demands presupposes design implementation choices expected runtime environment types failures withstand 
direct comparison databases distributed le systems dds helps show 
relational database management systems rdbms rdbms ers extremely strong durability consistency guarantees acid properties derived transactions acid properties come high cost terms complexity overhead 
result internet services rely rdbms backends typically go great lengths reduce workload rdbms techniques query caching front ends :10.1.1.1.2034
rdbms er high degree data independence powerful abstraction adds addi tional complexity performance overhead 
layers rdbms sql parsing query optimization access path selection permit users decouple logical structure data physical layout 
decoupling allows users dynamically construct issue queries data limited expressed sql language data independence parallelization scaling hard general case 
perspective service properties rdbms chooses consistency availability media processor failures rdbms unavailable failure resolved unacceptable internet services 
distributed le systems le systems strictly de ned consistency models 
nfs weak consistency guarantees frangipani afs guarantee coherent lesystem image clients locking typically done granularity les :10.1.1.130.3029:10.1.1.14.473
scalability distributed le systems similarly varies centralized le servers scale 
xfs completely serverless theory scale arbitrarily large capacities :10.1.1.110.7161
file systems expose relatively low level interface little data independence le system organized hierarchical directory les les variable length arrays bytes 
elements directories les directly exposed le system clients clients responsible logically structuring application data terms directories les bytes inside les 
distributed data structures dds dds strictly de ned consistency model operations elements atomic operation completes entirely 
dds copy equivalence data elements dds replicated clients see single logical data item 
phase commits keep replicas coherent clients see image dds interface 
transactions multiple elements operations currently supported show current protocol design decisions implementation choices exploit lack transactional support greater eciency simplicity 
internet services require transactions ecommerce imagine building transactional dds scope believe atomic single element updates coherence provided current dds strong support interesting services 
dds interface structured higher level le system 
granularity operation complete data structure element arbitrary byte range 
set operations data dds xed small set methods exposed dds api rdbms operations de ned set expressible declarations sql 
query parsing optimization stages rdbms completely dds dds interface exible ers data independence 
summary choosing level abstraction rdbms le system choosing de ned simple consistency model able design implement dds service properties 
experience dds interfaces general sql rich successfully build sophisticated services 
assumptions design principles section design principles guided building distributed hash table dds 
state number key assumptions regarding cluster environment failure modes dds handle workloads receive 
separation concerns clean separation service code storage management simpli es system architecture decoupling complexities state management service construction 
persistent service state kept dds service instances crash gracefully shut restart complex recovery process 
greatly simpli es service construction authors need worry service speci logic complexities data partitioning replication recovery 
appeal properties clusters addition properties listed section require cluster physically secure 
properties cluster represents carefully controlled environment greatest chance able provide service properties 
example low latency san ms wide area internet means phase commits prohibitively expensive 
san high redundancy means probability network partition arbitrarily small need consider partitions protocols 
power supply ups system administration help ensure probability system wide simultaneous hardware failure extremely low rely data available failure boundary physical memory disk node designing recovery protocols 
design high throughput high concurrency workloads section control structure ect concurrency critical 
techniques web servers process task thread task scale needed degree concurrency 
asynchronous event driven style control ow dds similar espoused modern high performance servers harvest web cache flash web server 
convenient side ect style layering inexpensive exible layers constructed chaining event handlers 
chaining facilitates interposition middleman event handler easily dynamically patched existing handlers 
addition server experiences burst trac burst absorbed event queues providing graceful degradation preserving throughput server temporarily increasing latency 
contrast thread task systems degrade throughput latency bursts absorbed additional threads 
assumptions dds node communicate assume node stopped executing due planned shutdown crash assume network partitions occur inside cluster dds software components fail 
need network partitions addressed high redundancy network previously mentioned 
attempted induce fail behavior software having terminate execution encounters unexpected condition attempting gracefully recover condition 
strong assumptions valid practice experienced unplanned network partition cluster software behaved fail manner 
assume software failures cluster independent 
replicate durable data place cluster assume replica active failed times 
assume degree synchrony processes take bounded amount time execute tasks messages take bounded amount time delivered 
assumptions workload distributed hash tables 
table key space set bit integers checkpoint mechanism discussed permits recover case cluster properties fail state changes happen checkpoint lost occur 
assume population density space probability key exists table function number values table particular key 
don assume keys accessed working set hot keys larger number nodes cluster 
assume partitioning strategy maps fractions keyspace cluster nodes nodes relative processing speed induce balanced workload 
current dds design gracefully handle small number extreme hotspots handful keys receive workload 
hotspots partitioning strategy probabilistically balance cluster 
failure workload assumptions result load imbalances cluster leading reduction throughput 
assume tables large long lived 
hash table creations relatively rare events common case hash tables serve read write remove operations 
distributed hash tables architecture implementation section architecture implementation distributed hash table dds 
illustrates hash table architecture consists components client client consists service speci software running client machine communicates wide area service instances running cluster 
mechanism client selects service instance scope typically involves dns round robin service speci protocol level level load balancing switches edge cluster 
example client web browser case service web server 
note clients completely unaware dds part dds system runs client 
service service set cooperating software processes call service instance 
service instances communicate widearea clients perform application level function 
services may soft state state may lost recomputed necessary rely hash table manage persistent state 
hash table api hash table api boundary service instance dds library 
api provides services put get remove create destroy operations hash tables 
operation atomic services see coherent image exist ole ole ole ole ole ole distributed hash table architecture box diagram represents software process 
simplest case process runs physical machine preventing processes sharing machines 
ing hash tables api 
hash table names strings hash table keys bit integers hash table values opaque byte arrays operations ect hash table values entirety 
dds library dds library java class library presents hash table api services 
library accepts hash table operations cooperates bricks realize operations 
library contains soft state including metadata cluster current con guration partitioning data distributed hash tables bricks 
dds library acts phase commit coordinator operations distributed hash tables 
brick bricks system components manage durable data 
brick manages set network accessible single node hash tables 
brick consists bu er cache lock manager persistent chained hash table implementation network stubs skeletons remote communication 
typically run brick cpu cluster way smp house bricks 
bricks may run dedicated nodes may share nodes components 
partitioning replication replica consistency distributed hash table provides incremental scalability throughput data capacity nodes added cluster 
achieve horizontally partition tables spread operations data bricks 
brick stores number partitions table system new nodes added cluster partitioning altered data spread new node 
workload assumptions section horizontal partitioning evenly spreads load data cluster 
data hash table spread multiple nodes nodes fail portion hash table unavailable 
reason partition hash table replicated cluster node 
set replicas partition form replica group replicas group kept strictly coherent 
replica service get replicas updated put remove 
node fails data partitions available surviving members partitions replica groups 
replica group membership dynamic node fails replicas removed replica groups 
node joins cluster may added replica groups partitions case recovery described 
maintain consistency state changing operations put remove issued partition replicas partition synchronously updated 
optimistic twophase commit protocol achieve consistency dds library serving commit coordinator replicas serving participants 
dds library crashes prepare messages sent commit messages sent replicas time abort operation 
dds library crashes sending commits replicas commit 
sake availability rely dds library recover crash issuing pending commits 
replicas store short inmemory logs state changing operations outcomes 
replica times waiting commit replica communicates peers nd received commit operation replica commits replica aborts 
peers replica group time waiting commit communicate peers receives commit commit 
replica may abort rst phase phase commit replica obtain write lock key 
dds library receives abort messages rst phase sends aborts replicas second phase 
replicas commit side ects receive commit message second phase 
replica crashes phase commit dds library simply removes replica group continues onward 
replica groups shrink time rely recovery mech nh dds cs dds cs dds cs dds cs dds cs dds cs dds cs nh lq pds wr lq pds wr ri dds cs dds cs dds cs distributed hash table metadata maps illustration highlights steps taken discover set replica groups serve backing store speci hash table key 
key traverse dp map trie retrieve name key replica group 
replica group name looked rg map nd group current membership 
anism described crashed replicas rejoin replica group 
signi cant optimization image replica consistent brick cache having consistent disk image 
allows purely con ict driven cache eviction policy having force cache elements ensure disk consistency 
implication members replica group crash partition lost 
assume nodes independent failure boundaries section systematic software failure nodes cluster power supply 
phase commit mechanism gives atomic updates hash table 
give transactional updates 
service wishes update element atomically dds provide help 
adding transactional support dds infrastructure topic require signi cant additional complexity distributed deadlock detection undo redo logs recovery 
checkpoint mechanism distributed hash table allows force image partitions consistent disk images backed disaster recovery 
checkpoint mechanism extremely heavyweight checkpointing hash table state changing operations allowed 
currently rely system administrators decide initiate checkpoints 
metadata maps nd partition manages particular hash table key determine list replicas partitions replica groups dds libraries consult metadata maps replicated node cluster 
hash table cluster pair metadata maps 
rst map called data partitioning dp map 
hash table key dp map returns name key partition 
dp map controls horizontal partitioning data bricks 
shown gure dp map trie hash table keys nd key partition key bits walk trie starting signi cant key bit leaf node 
cluster grows dp trie subdivides split operation 
example partition dp trie gure split partitions happens keys old partition shu ed new partitions 
opposite split merge cluster shrunk partitions common parent trie merged parent 
example partitions gure merged single partition 
second map called replica group rg membership map 
partition name rg map returns list bricks currently serving replicas partition replica group 
rg maps dynamic brick fails removed rg maps contain 
brick joins replica group nishing recovery 
invariant preserved replica group membership maps partitions hash table member 
maps replicated cluster node dds libraries bricks 
maps kept consistent operations may applied wrong bricks 
enforcing consistency synchronously allow libraries maps drift date lazily update perform operations 
dds library piggybacks hashes maps operations sent bricks brick detects map date brick fails operation returns repair library 
maps eventually consistent 
mechanism libraries restarted date maps library gets maps consistent 
put key value hash table dds library servicing operation consults dp map determine correct partition key 
looks partition name rg map nd current set bricks serving replicas nally performs phase commit replicas 
get key similar process dds library important large hash probability collision negligible currently bits 
select replicas listed rg map service read 
locality aware request distribution lard technique select read replica lard partitions keys replicas ect aggregating physical caches 
recovery brick fails replicas unavailable 
making partitions unavailable remove failed brick replica groups allow operations continue surviving replicas 
failed brick recovers alternative brick selected replace catch operations missed 
rdbms le systems recovery complex process involves replaying logs system properties clusters dds design vast simpli cations 
firstly allow hash table say bricks may return failure operation phase commit obtain locks bricks puts key simultaneously issued replica group memberships change operation 
freedom say greatly simpli es system logic don worry correctly handling operations rare situations 
rely dds library ultimately service wan client retry operation 
secondly don allow operation nish participating components agree metadata maps 
component date map operations fail maps reconciled 
partitions relatively small mb means transfer entire partition fast system area network typically mb gb seconds 
recovery incrementally copy entire partitions recovering node obviating need undo redo logs typically maintained databases recovery 
node initiates recovery grabs write lease replica group member partition joining write lease means operations partition start fail 
recovering node copies entire replica network 
sends updates rg map replicas group means dds libraries start lazily receive update 
releases write lock means previously failed operations succeed retry 
recovery partition complete recovering node recovery partitions necessary 
interesting choice rate partitions transferred network recovery 
rate fast involved bricks su er loss read throughput recovery 
rate slow bricks won lose throughput partition mean time recovery increase 
chose recover quickly possible large cluster small fraction total throughput cluster ected recovery 
similar technique dp map split merge operations replicas modi ed rg dp maps updated operation 
convergence recovery challenge fault tolerant systems remain consistent face repeated failures recovery scheme described property 
steady state operation replicas group kept perfectly consistent 
recovery state changing operations fail recovering partition implying surviving replicas remain consistent recovering nodes stable image recover 
ensure recovering node joins replica group successfully copied entire partition data release write lease 
remaining window vulnerability system recovery takes longer write lease imminent recovering node aggressively renew write lease currently implemented behavior 
recovering node crashes recovery write lease expire system continue normal 
replica lease grabbed crashes recovering node recovery surviving member replica group 
members replica group crash data lost mentioned section 
asynchrony components distributed hash table built asynchronous event driven programming style 
hash table layer designed single thread executes time 
greatly simpli ed implementation eliminating need data locks race conditions due threads 
hash table layers separated fifo queues completion events requests placed 
fifo discipline queues ensures fairness requests queues act natural bu ers absorb bursts exceed system throughput capacity 
interfaces system including dds library apis split phase asynchronous 
means hash table get doesn block immediately returns identi er dds bricks reads writes throughput scalability benchmark shows linear scaling throughput function number bricks serving distributed hash table note axis logarithmic scales 
added bricks dds increased number clients dds throughput saturated 
matched completion event delivered caller speci ed upcall handler 
upcall handler application code queue polled blocked 
performance section performance benchmarks distributed hash table implementation gathered cluster way smps way smps total mhz pentium cpus 
way smp mb ram way smp gb 
connected mb switched ethernet way smps gb switched ethernet way smps 
benchmarks run sun jdk jit compiler green userlevel threads top linux 
running benchmarks evenly spread hash table bricks way way smps running brick node cpu cluster 
way smps brick processes running way smps 
cluster nodes load generators able gather performance numbers maximum brick distributed hash table needed remaining cpus generate load saturate large table 
core benchmarks rst set benchmarks tested core performance distributed hash table 
limiting working set keys requested size ts aggregate physical memory bricks set benchmarks investigates overhead throughput distributed hash table code independently disk performance 
service instances bricks bricks bricks bricks graceful degradation reads graph demonstrates read throughput distributed hash table remains constant offered load exceeds capacity hash table 
throughput scalability benchmark demonstrates hash table throughput scales linearly number bricks 
benchmark consists services maintain pipeline operations gets puts single distributed hash table 
varied number bricks hash table con guration slowly increased number services measured completion throughput owing bricks 
con gurations replicas replica group benchmark iteration consisted reads writes byte values 
benchmark closed loop new operation immediately issued random key completed operation 
shows maximum throughput sustained distributed hash table function number bricks 
throughput scales linearly bricks didn processors scale benchmark 
read throughput achieved bricks reads second day write throughput bricks writes second day performance adequate serve hit rates popular web sites internet 
graceful degradation reads bursts trac common phenomenon internet services 
trac burst exceeds service capacity service property graceful degradation throughput service remain constant excess trac rejected absorbed bu ers served higher latency 
shows throughput distributed hash table function number simultaneous read requests issued service instance closed loop pipeline operations 
line graph represents di erent number bricks serving time ms brick cpu brick cpu throughput pause clients write imbalance leading degradation bottom curve shows throughput brick partition overload top curves show cpu utilization bricks 
brick saturated busy 
hash table 
con guration seen eventually reach maximum throughput bricks saturate 
maximum throughput successfully sustained additional trac ered 
overload trac absorbed fifo event queues bricks tasks processed experience higher latency queues drain burst 
degradation writes unfortunate performance anomaly emerged benchmarking put throughput 
offered load approached maximum capacity hash table bricks total write throughput suddenly began drop 
closer examination discovered bricks hash table unloaded brick hash table completely saturated bottleneck closed loop benchmark 
illustrates imbalance 
generate issued puts hash table single partition replicas replica group 
put operation caused phase commit replicas replica saw set network messages performed computation slightly di erent orders 
expected replicas perform identically replica idle throughput hash table dropped match cpu utilization idle replica 
investigation showed busy replica spending signi cant amount time garbage collection 
live objects populated replica heap time needed spent garbage collecting reclaim xed amount heap space objects examined free object discovered 
random uctuations arrival rates garbage collection caused replica spend time garbage collecting 
replica system bottleneck operations queues amplifying imbalance 
write trac particularly ex hash table value size bytes max reads throughput vs read size axis shows size values read hash table axis shows maximum throughput sustained brick hash table serving values 
situation objects created prepare phase wait network round trip time commit abort command second phase received 
number live objects bricks heap proportional bandwidth delay product hash table put operations 
read trac phase objects garbage collected immediately read requests satis ed 
experimented consistently saw ect 
jdk linux developed imbalance read trac write trac 
sort performance imbalance fundamental system doesn perform admission control task arrival rate temporarily exceeds system ability handle tasks pile system 
systems nite resources inevitably causes performance degradation thrashing 
system degradation rst materialized due garbage collection 
systems happen due virtual memory thrashing pick example 
currently exploring admission control bricks hash table libraries early discard bricks queues keep bricks operational range imbalance 
throughput bottlenecks gure varied size elements read brick hash table 
throughput bytes bytes began degrade 
deduced overhead object creation garbage collection system call overhead saturated bricks cpus elements smaller bytes byte overhead byte array copies tcp stack jvm saturated bricks cpus elements greater bytes 
bytes throughput way smp running bricks mb larger sized hash table values mb switched network throughput bottleneck 
core benchmarks set benchmarks tested performance workloads aggregate physical memory bricks 
benchmarks stress single node hash table disk interaction performance distributed hash table 
terabyte dds test distributed hash table scales terms data capacity populated hash table terabytes kb data elements 
created table partitions dp map replica replica group table withstand node failures 
spread partitions brick nodes ran bricks node cluster 
brick stored data dedicated gb disk cluster nodes disks 
bricks gb worth disk capacity resulting tb data stored table 
populate tb hash table designed bulk loaders generated writes keys order carefully chosen result sequential disk writes 
bulk loaders understood partitioning dp map implementation details single node tables hash functions map keys disk blocks 
loaders took minutes ll table terabytes data achieving total write throughput operations mb disk 
comparatively core throughput benchmark section obtained operations brick table benchmark con gured replicas replica group 
eliminating replication double throughput core benchmark resulting operations bulk loading tb hash table marginally slower terms throughput sustained replica core benchmarks means disk throughput bottleneck 
random write read throughput believe unrealistic undesirable hash table clients knowledge dp map single node tables hash functions 
ran second set throughput benchmarks tb hash table populated random keys 
workload table took minutes populate resulting total write throughput operations mb disk 
similarly sustained read throughput operations mb disk 
throughput substantially lower throughput obtained core benchmarks random workload generated results random read write trac disk 
fact random workload read issued distributed hash table results request random disk block disk 
disk trac seek dominated disk seeks bottleneck system 
expect signi cant locality dds requests generated internet services workloads high locality dds perform nearly core benchmark results 
possible signi cantly improve write performance trac little locality disk layout techniques similar log structured le systems explored possibility 
availability recovery demonstrate availability face node failures ability bricks recover failure repeated read benchmark hash table byte elements 
table con gured single mb partition replicas partition replica group 
shows throughput hash table time induced fault replica bricks initiated recovery 
recovery rate recovered partition copied mb maximum sequential write bandwidth obtain bricks disks 
point bricks operational throughput sustained hash table operations second 
point bricks killed 
performance immediately dropped operations second thirds original capacity 
fault detection immediate client libraries experienced broken transport connections reestablished 
performance overhead replica group map updates observed 
point recovery initiated recovery completed point 
points noticeable performance overhead recovery ample excess bandwidth network cpu overhead transferring partition recovery negligible 
noted points write throughput read throughput hash bucket read written case data stored bucket preserved 
additional read write nearly halving ective throughput dds writes 
time ms availability recovery benchmark shows read throughput brick hash table deliberate single node fault induced recovery performed 
ering partition available writes write lease grabbed recovery 
partition available reads 
recovery completed performance brie dropped point 
degradation due bu er cache warming recovered node 
cache warm performance resumed original operations point 
interesting anomaly point presence noticeable oscillations throughput traced garbage collection triggered extra activity recovery 
repeated measurements occasionally see oscillation times immediately post recovery 
sort performance unpredictability due garbage collection pervasive problem better garbage collector admission control ameliorate haven explored 
example services implemented number interesting services distributed hash table 
services implementation greatly simpli ed dds trivially scaled adding service instances 
aspect scalability covered hash table routing load balancing wan client requests service instances scope 
instant messaging gateway provides protocol translation popular instant messaging protocols icq aol aim conventional email voice messaging cellular telephones 
middleman protocols routing translating messages networks 
addition protocol translation transform message content 
built web allows compose altavista natural language translation service 
perform language translation english french protocol translation spanish speaking icq user send message english speaking aim user providing language protocol translation 
user may reached number di erent addresses networks communicate 
service keep large table bindings users current transport addresses networks distributed hash table purpose 
expected workload dds includes signi cant write trac generated users change networks log network 
data table kept consistent messages routed wrong address 
took person month develop spent authoring protocol translation code 
code interacts distributed hash table took day write 
web server implemented scalable web server distributed hash table 
server speaks web clients hashes requested urls bit keys requests keys hash table 
server takes advantage event driven queue centric programming style introduce cgi behavior interposing url resolution path 
web server written lines java deals parsing url resolution deals interacting hash table dds 
built services part ninja project service recommends related sites user speci ed urls looking ontological entries inversion yahoo web directory 
built collaborative ltering engine digital music jukebox service engine stores users music preferences distributed hash table 
implemented private key store composable user preference service distributed hash table persistent state management 
discussion experience distributed hash table implementation taught lessons storage platform scalable services 
hash table success simplifying construction interesting services services inherited scalability availability data consistency hash table 
exploiting properties clusters proved remarkably useful 
experience assumptions regarding properties clusters component failures speci cally fail behav ninja cs berkeley edu ior software probabilistic lack network partitions cluster valid practice 
assumptions initially problematic observed case systematic failure replica group members inside single replica group 
failure caused software bug enabled service instances deterministically crash remote bricks inducing null pointer exception jvm 
xing associated bug brick situation arose 
serves reminder systematic software bugs practice bring entire cluster 
careful software engineering quality assurance cycle help ameliorate failure mode believe issue fundamental systems promise availability consistency 
scaled distributed hash table noticed scaling bottlenecks weren associated software 
bricks approached point mb ethernet switches saturate upgrading gb switches cluster delay saturation 
noticed combination jvm user level threads linux kernel began induced poor scaling behavior node cluster opened reliable tcp connection nodes cluster 
brick processes began saturate due ood signals kernel user level thread scheduler associated tcp connections data waiting read 
java service platform java adequate platform build scalable high performance subsystem 
ran number serious issues java language runtime 
garbage collector jvms experimented inevitably performance bottleneck bricks source throughput latency variation 
garbage collector active serious impact system activity unfortunately current jvms provide adequate interfaces allow systems control garbage collection behavior 
type safety array bounds checking features java vastly accelerated software engineering process helped write stable clean code 
features got way code eciency especially dealing multiple layers system wraps array data layer speci metadata 
performing copies regions byte arrays order maintain clean interfaces data regions implementation natural exploit pointers malloc ed memory regions ect needing copies 
java lacks asynchronous primitives necessitated thread pool system 
ecient thread task system number threads system equal number outstanding requests number tasks 
introduced performance overhead scaling problems number tcp connections brick increases cluster size 
working introducing asynchronous completion mechanisms jvm jni native interface 
plan investigating interesting dataparallel operations dds iterator lisp maplist operator 
plan building distributed data structures including tree administrative log 
doing hope reuse components hash table brick storage layer rg map infrastructure phase commit code 
explore caching dds libraries currently rely services build application level caches 
exploring adding single element operations hash table order provide locks leases services may service instances competing write hash table element 
related litwin scalable distributed data structures rp helped motivate 
rp focuses algorithmic properties focused systems issues implementing satis es concurrency availability incremental scalability needs internet services 
great deal common database research 
problems partitioning replicating data shared multicomputers studied extensively distributed parallel database communities 
mechanisms horizontal partitioning phase commits need sql parser query optimization layer general purpose queries system 
common distributed parallel le systems :10.1.1.130.3029
dds presents higher level interface typical le system dds operations data structure speci atomically ect entire elements 
research focused scalability availability consistency high throughput highly concurrent trac di erent focus le systems 
similar petal petal distributed virtual disk thought simple hash table xed sized elements 
hash tables variable sized elements additional name space set hash tables focus internet service workloads properties opposed le system workloads properties 
cmu network attached secure disk nasd architecture explores variable sized object interfaces abstraction allow storage subsystems optimize disk layout 
similar data structure interface deliberately higher level block le interfaces petal parallel distributed le systems 
distributed object stores attempt transparently adding persistence distributed object systems 
persistence typed objects typically determined reachability transitive closure object removal objects handled garbage collection 
dds notion pointers object typing applications explicitly api operations store retrieve elements dds 
distributed object stores built wide area mind focus scalability availability high throughput requirements cluster internet services 
projects explored clusters workstations general purpose platform building internet services 
date platforms rely le systems databases persistent state management dds meant augment platforms state management platform better suited needs internet services 
porcupine project includes storage platform built speci cally needs cluster scalable mail server attempting generalize storage platform arbitrary service construction 
projects wide area replicated distributed services 
clusters wide area systems deal heterogeneity network partitions untrusted peers high latency low throughput networks multiple administrative domains 
differences wide area distributed systems tend relaxed consistency semantics low update rates 
designed correctly scale enormously 
presents new persistent data management layer enhances ability clusters support internet services 
self managing layer called distributed data structure dds lls important gap current cluster platforms providing data storage platform speci cally tuned services workloads cluster environment 
focused design implementation distributed hash table dds empirically demonstrating properties necessary internet services incremental scaling throughput data capacity fault tolerance high availability high concurrency consistency durability data 
properties achieved carefully designing partitioning replication recovery techniques hash table implementation exploit features cluster environments low latency network lack network partitions doing right sized dds problem persistent data management internet services 
hash table dds simpli es internet service construction decoupling service speci logic complexities persistent state management allowing services inherit necessary service properties dds having implement properties 
grateful eric anderson rob von behren mike chen armando fox jim gray gummadi drew roselli geo voelker anonymous referees shepherd bill weihl helpful suggestions greatly improved quality 
eric fraser phil brent chun help giving access berkeley millennium cluster performance benchmarks 
amir mccanne katz 
active service framework application real time multimedia transcoding 
proceedings acm sigcomm pages oct 
anderson culler patterson 
case networks workstations 
ieee micro feb 
anderson dahlin neefe patterson roselli wang :10.1.1.110.7161
serverless network file systems 
proceedings th acm symposium operating systems principles dec 
yang ibarra smith 
scalability issues high performance digital libraries world wide web 
proceedings ieee adl washington may 
banga mogul druschel 
scalable explicit event delivery mechanism unix 
proceedings usenix annual technical conference monterey ca jun 
bea systems 
bea weblogic application servers 
www bea com products weblogic 

rfc dns support load balancing apr 
chankhunthod danzig neerdaels schwartz worrell 
hierarchical internet object cache 
proceedings usenix annual technical conference jan 
birrell grapevine exercise distributed computing 
communications acm feb 
dewitt gamma database machine project 
ieee transactions knowledge data engineering mar 
gibson cost ective highbandwidth storage architecture 
asplos viii san jose california 
howard scale performance distributed file system 
acm transactions computer systems feb 
ferreira design implementation persistent distributed store 
advances distributed systems volume lecture notes computer science chapter pages 
springer verlag feb 
pai locality aware request distribution cluster network servers 
asplos viii san jose ca oct 
fox gribble chawathe brewer gauthier :10.1.1.1.2034
cluster scalable network services 
proceedings th acm symposium operating systems principles st malo france oct 
goldberg gribble wagner brewer 
ninja jukebox 
nd usenix symposium internet technologies systems boulder oct 
graefe 
encapsulation parallelism volcano query processing system 
acm sigmod conference management data atlantic city nj may 
jim gray 
transaction concept virtues limitations 
proceedings vldb cannes france september 
gribble brewer 
system design issues internet middleware services deductions large client trace 
proceedings usenix symposium internet technologies systems usits monterey ca dec 
hu pyarali schmidt 
applying proactor pattern high performance web servers 
proceedings th international conference parallel distributed computing systems oct 
iyengar challenger dias dantzig 
high performance web site design techniques 
ieee internet computing mar 
karlsson litwin risch 
lh lh scalable high performance data structure switched multicomputers 
proceedings th international conference extending database technology pages avignon france mar 
krieger stumm 
hfs flexible file system large scale multiprocessors 
proceedings dags pc symposium pages hanover nh jun 
lee thekkath 
petal distributed virtual disks 
asplos vii cambridge ma 
lindsay 
retrospective distributed database management system 
proceedings ieee may 
litwin neimat schneider 
rp family order preserving scalable distributed data structures 
proceedings twentieth international conference large databases pages santiago chile 
mockapetris dunlap 
development domain name system 
acm sigcomm computer communication review 
pai druschel zwaenepoel 
flash ecient portable web server 
proceedings annual usenix technical conference jun 
rosenblum ousterhout 
design implementation log structured file system 
proceedings th acm symposium operating systems principles 
saito bershad levy 
manageability availability performance porcupine highly scalable cluster mail service 
proceedings th symposium operating system principles kiawah island sc dec 
sandberg goldberg kleiman walsh lyon :10.1.1.14.473
design implementation sun network filesystem 
proceedings usenix summer conference el ca jun 
song levy iyengar dias 
design alternatives scalable web server accelerators 
proceedings ieee international symposium performance analysis systems software austin tx apr 
thekkath mann lee :10.1.1.130.3029
frangipani scalable distributed file system 
proceedings th acm symposium operating systems principles st malo france oct 
