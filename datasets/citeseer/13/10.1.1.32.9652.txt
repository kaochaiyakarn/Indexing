graphical models discovering knowledge wray buntine research institute advanced computing sciences computational sciences division nasa ames research center different ways representing knowledge ways different discovery algorithms 
compare different representations 
mix match merge representations algorithms new problems unique requirements 
chapter introduces probabilistic modeling philosophy addressing questions presents graphical models representing probabilistic models 
probabilistic graphical models unified qualitative quantitative framework representing reasoning probabilities independencies 
common element discovery systems described previous books knowledge discovery different 
class discovery problems challenging write single program address knowledge discovery 
discovery system applied health care matheus piatetsky shapiro mcneill instance carefully tailored particular class situations easily application fayyad weir 
know universal learning discovery algorithm buntine universal problem description discovery arguably broad program specification 
consequence power perform application lies way knowledge application obtained represented modified 
unfortunately buntine today technology possible dump data discovery system read dollar savings 
closely experts involved instance selecting customizing tools 
see chapter brachman anand book account interactive aspects knowledge discovery 
important knowledge discovery techniques allow flexibility way knowledge encoded represented discovered 
probabilistic graphical models offer technique 
probabilistic graphical models framework structuring representing decomposing problem notion conditional independence 
special cases variations including bayesian networks influence diagrams markov networks causal probabilistic networks 
models useful reason constraint satisfaction graphs scheduling data flow diagrams scientific modeling fault trees systems health management 
allow access structure problem getting bogged mathematical detail 
probabilistic graphical models representing variables problem relationships 
associated graphical models mathematical details equations linking variables model algorithms performing exact approximate inference model 
probabilistic graphical models attractive modeling tool knowledge discovery ffl lucid representation variety problems allowing key dependencies problem expressed ignored 
flexible represent supervised unsupervised learning systems neural networks hybrids 
ffl come understood techniques key tasks discovery process problem formulation decomposition designing learning algorithm buntine identification valuable knowledge decision theory generation explanations madigan 
simple form graphical model considered chapter bayesian network 
reasoning value knowledge bayesian networks done adding value nodes tools influence diagrams utility theory shachter part modern decision theory 
covered chapter 
bayesian networks introduced section problem decomposition discussed section knowledge refinement discussed section relationships variety learning graphical models discovering knowledge representations discussed section 
implications discovery 
graphical models graphs represent models 
model general proposed representation problem hand showing different variables involved data parameters probabilistic deterministic relationships 
basic model consider consists nodes representing variables arcs indicate dependencies variables arcs indicating independencies 
variables represented may real valued discrete may ffl variables values data ffl hidden variables believed exist medical syndromes hypothesized classes data base stars ffl parameters specify model weights neural network standard deviation gaussian radius diffusion instrument error rate transmission channel 
variables considered different data 
difference values currently known revealed reasonably measure indirectly hypothesize exist calculus probability estimate 
introduce basic kind graphical model bayesian network give brief insight interpretation 
brief tour necessary applying graphical models discovery learning 
bayesian network graphical model uses directed arcs exclusively form directed acyclic graph directed graph directed cycles 
adapted shachter heckerman shows occupation climate age disease symptoms simplified medical problem 
simple bayesian network simplified medical problem 
graph represents buntine domain model problem 
organizes variables way medical specialist usually understand problem arcs graph intuitively correspond notion cause influence 
instance may thought disease causes symptoms age occupation climate causes disease 
stretch imagination disease said caused symptoms graph called causal model 
graphical models represent variables different ordering depending graph represent domain model computational model program particular view representative user 
graphical models manipulated represent different views probabilistic knowledge base 
graphical models language expressing problem decomposition 
show decompose problem simpler subproblems 
directed acyclic graph done conditional decomposition joint probability see instance lauritzen pearl detail including interpretations 
follows full variable names abbreviated 
represents context 
probability statements relative context context dropped discussions brevity 
age occ clim dis occ clim variable written conditioned parents parents set variables directed arc general form set variables xjm compare equation way writing complete joint probability age occ clim dis occ occ clim occ clim dis complete joint identity probability theory independence assumptions problem 
probability models primarily performing inference new problems 
graphical models useful kinds inference performed 
basic inference involves calculating probabilities arbitrary sets variables shachter andersen szolovits 
graphical models domains diagnosis probabilistic expert systems planning control dean kind time delay feedback involved 
graphical models discovering knowledge wellman chan shachter statistical analysis data gilks thomas spiegelhalter goal directed typical knowledge discovery 
graphical models generalize aspects kalman filters poland control hidden markov models basic tool speech recognition rabiner juang fault diagnosis smyth 
graphical models dynamic systems forecasting dagum 
various methods learning simple kinds graphical models data exist heckerman 
extensive introductions probabilistic graphical models henrion breese horvitz whittaker pearl spiegelhalter learning graphical models spiegelhalter buntine heckerman 
problem decomposition learning discovery problems rarely come neatly packaged labeled type 
common practitioner spend time analyzing problem data analysis applied 
analysis decomposition problem routinely done knowledge acquisition software development attracted attention data analysis discovery learning literature 
section introduces technique problem decomposition graphical models 
reasons doing decomposition fold 
clearly simplifying problem 
second importantly simpler model easier learn data parameters 
discovery feasible reliable 
graphical models convenient way making structure decomposition apparent going precise mathematical detail 
section illustrates process problem decomposition working example topic spotting 
examples equally illustrated process 
topic spotting example addresses common problems supervised learning large input space multi class decision problem 
associated press produces short newswires rate tens thousands year 
come approximately broad topics contain different words 
single newswire may words long 
typical newswire 
precious metals climate improving says london april climate precious metals improving prices benefiting renewed inflation fears switching funds dollar stock markets 
silver prices march gained pct dollar buntine terms due weak dollar silver felt fairly cheap relative gold 
report said oil prices continue short term 
reuter topics newswire gold silver precious metals 
topics newswire subject line written author newswire 
ignore purposes illustration 
suppose wish predict topics text newswire ignoring subject line 
naive approach attempt predict topics words monolithic classifier inputs 
problem readily decomposed topics broken sub topics topic space rich structure 
space input words structure suppose newswire known topic precious metals 
presence word beef irrelevant trying determine sub topic gold silver 
word beef relevant topic known relevant agriculture 
partial decomposition problem 
bayesian agriculture commodities exchange precious metals gold tourism chicago board skiing banking hotel dollar dm yen weather silver gold gold platinum precious metals true silver dairy true dairy true milk beef components topics subtopics model shaded nodes known values 
networks different previous nodes shaded 
convention shaded nodes values known time inference unshaded nodes 
partial decomposition goes follows break topics groups 
boolean variables agriculture precious metals tourism forth 
topic variables recognized unshaded nodes graph 
graph model topics conditioned presence various graphical models discovering knowledge words newswire text 
variables consisting quoted words indicate word appears text 
instance variable gold true word gold appears text false 
note different topic text gold 
practice word frequency counts hundreds words 
ignore complication purposes illustration 
quoted variables appear shaded nodes 
indicates text know value word variables know topics 
topic graph predict subtopics sub subtopics 
instance shows sample subtopic graph precious metals 
notice graph top boolean variable precious metals value known true 
notation indicate subgraph contingent precious metals true topic 
likewise shows graph contingent diary true 
graph assumes true predict true 
probability unifying framework combine different graphical models global model predict complete set topics 
done follows adapting equation graphs yield formulae probabilities ffl precious metals banking exchange commodities agriculture gold weather ffl gold silver metals true gold ffl dairy true true beef mcdonald likewise corresponding formulae obtained graphs depicted 
probabilities manipulated combined yield individual probabilities 
instance suppose wish evaluate probability newswire indicates contents newswire words beef 
computed probability identities metals true newswire precious metals metals true newswire gold ft fg platinum ft fg gold silver metals true newswire precious metals computed similarly summing topic variables 
methods combining probabilities multiple buntine networks involve complex schemes 
method developed medical diagnosis suitable topic spotting problem considered similarity networks heckerman 
graphs form distinguish pairs topics 
number interesting questions decomposition approach 
develop decomposition 
diagnosis domains medicine kind decomposition done manually development probabilistic expert systems 
experts able explain decompositions problem 
second decomposition done automatically 
open research question standard techniques learning adapt task 
knowledge refinement unsupervised learning standard tool statistics pattern recognition 
known example discovery autoclass application iras star database cheeseman stutz 
applications unsupervised learning proceed routinely case discovery iterative process 
initial exploration reveals details discovery algorithm modified result 
discovery process parallels iterative refinement strategies popular software engineering 
strategies possible rapid prototyping software tcl tk developing interfaces ousterhout 
aspect discovery discussed brachman anand 
application iterative refinement knowledge discovery knowledge acquisition way viewing knowledge refinement ginsberg weiss towell shavlik noordewier 
application kind refinement required analysis aviation safety data kraft buntine 
task discover classes aircraft incidents 
case standard unsupervised learning revealed incident classes domain expert believed confounded basic relationships expected data 
graphical model illustrating simplifying standard unsupervised learning 
algorithm initial investigation algorithm called wallace boulton related autoclass 
algorithm builds classification model represented 
aircraft incident details recorded pilot controller kind aircraft mission information 
indicates set aircraft hidden incident class details recorded rendered independent 
joint probability recorded details hidden incident class read graph incident class class class graphical models discovering knowledge pilot incident class aircraft phase position environment anomaly resolution consequence reporter airspace controller facility simple unsupervised model aircraft incident domain 
class class 
probabilities evaluated parameters set learning algorithm 
instance particular hidden incident class predominantly wide body aircraft experienced pilots equipment failure details similar general population incidents 
occurrence wide body aircraft experienced pilots equipment failure occur independently class indicated 
aviation psychologists experienced domain expected relationships instance pilot qualifications type aircraft type aircraft phase flight instance wide body aircraft go joy rides 
cases relationships encoded requirements federal aviation authority cases understood causal relationships 
discovered classes aircraft incidents tended confounded known relationships 
way problem construct hybrid model 
expected relationships encoded model 
instance pilot qualifications influenced aircraft facility tracking aircraft depends type aircraft airspace commercial private military aircraft different behaviors encoded 
leaves hidden incident class explain remaining regularity domain 
probability tables elicited aviation psychologists understood probability relations aircraft fixed model 
learning system needs refine model filling remaining parts model left unspecified knowledge elicitation 
number interesting questions refinement approach 
refinement algorithm proceed parts model fixed 
difficult problem sense standard algorithm schemes expectation max buntine controller aircraft facility pilot environment anomaly resolution consequence reporter incident class phase position airspace hybrid unsupervised model aircraft incident domain 
em algorithm autoclass known handle learning context buntine 
software suited exact task currently available 
problem iterative refinement process knowledge discovery stops iteration due lack available software 
models learning discovery section outlines various learning discovery representations modeled probabilistic graphical models 
characteristic problem graphical model 
intention illustrate rich variety discovery tasks represented graphical models 
generality language clear hybrid models represented hybrid unsupervised model 
graphical models model parameters problem inputs marked known 
course practice data analysis model parameters unknown need learned data training set sample usually problem inputs outputs known case set 
represents subsequent inference task underlying problem learning problem 
cases functional form probabilistic model implied graphical model 
linear regression linear regression classic method statistics doing curve fitting predicting real valued variable input variables real discrete 
see casella berger graphical models discovering knowledge instance standard undergraduate 
linear regression general form fits non linear curves term linear implies mean prediction variable linear function parameters model non linear function input variables 
standard model gaussian error function constant standard deviation 
shown 
instance generalized linear model mccullagh nelder basis basis linear gaussian linear regression gaussian error 
linear node core 
basis functions basis basis known deterministic functions input variables xn variables deterministic functions inputs represented deterministic nodes double ellipses 
deterministic functions typically non linear orthogonal functions legendre polynomials 
linear node combines linearly parameters produce mean gaussian 
basis graphical model implies equation deterministic node implies equality holds conditional probability yjx xn oe oe gamma gammam oe standard normal density mean standard deviation oe 
graph shows inputs xn particular distribution 
buntine weighted rule systems weighted rule systems interesting representation independently suggested artificial intelligence neural networks statistics community notation 
system discrete version linear regression network 
linear regression rule rule linear logistic weighted rule network 
instance generalized linear model linear construction core 
deterministic nodes variables rule rule represents rule indicator function value rule fires value 
rules fire cause weights added consequently prediction 
binary classification case multiple rules fire probability class transformation jx xn logistic gamma rule functional type logistic node function ju gamma sigmoid logistic gamma maps real value probability binary variable function inverse logistic logit function generalized linear models related sigmoid function feed forward neural networks 
graphical models discovering knowledge weighting scheme rule rule fires isolation probability class logistic gamma 
interpreted log odds logistic probability single rule fires 
multiple rules fire formula corresponds combining probabilities original combining formula duda hart nilsson iv combine delta delta gamma delta gamma combining formula associative commutative order combination irrelevant 
approach implements weighted rule system classification combining formula 
model interpreted neural network output node corresponds sigmoid intermediate deterministic nodes interpreted hidden nodes 
combination rules different effects achieved instance fuzzy style combinations 
hierarchical mixtures experts jordan jacobs developed classification approach notion mixture experts 
weighted rule system model predicts class vector inputs combining number linear models form complex classifier 
decision tree representation dag mixture model left right respectively 
decision tree case discrete variables 
general inputs outputs real valued discrete 
traversing tree left leaf node leads leaf represents expert 
experts combine prediction class prediction done log linear model parameters gates suppose class valued cg 
class prediction ijx log linear delta delta similar weighted rule system described section rules correspond vector matrix dimension theta dim convention 
binary equivalent logistic node section 
decision tree variables denoting gates node second level nodes data 
values buntine gating gating log linear log linear log linear level mixture experts 
gates predicted data parameters ae ae jg respectively 
level discrete valued gate tree represented binary ary general 
value chosen probabilistic fashion log linear model parameters ae ijx ae log linear ae ae matrix dimension theta dim convention ae 
second gate chosen probabilistic fashion log linear model time gate input final probabilities generated log linear model formula 
graphical model goes follows 
log linear models gates final class probability 
gating nodes matrix lookup select parameters log linear nodes values variables 
graphical model yields conditional probability cjx ae ae log lin ae log lin ae jg log lin mixture model titterington smith makov sense sums hidden variables basic joint probability jx ae ae standard form 
layer associated gates deleted model corresponds supervised version unsupervised autoclass system described section 
graphical models discovering knowledge unsupervised learning range unsupervised learning systems statistics neural networks artificial intelligence 
represented graphical models hidden nodes represent hidden classes 
sense learning bayesian networks data called unsupervised learning accurately termed model discovery 
described heckerman 
aviation safety model hybrid different kinds models 
consider autoclass iii probabilistic unsupervised learning systems 
instance simple autoclass iii classification boolean variables var var var parameterization oe 
var var var class explicit parameters simple autoclass model 
class unobserved hidden 
class assignment known variables var var var rendered statistically independent explained sense 
complex models allow correlations variables autoclass iii introduce 
parameters oe vector class probabilities gives proportions hidden classes parameters give variables distributed hidden class 
instance classes oe vector class probabilities prior probability case class oe var binary variable probabilities class case known class probability var true probability var false gamma models unsupervised learning similarly represented probabilistic graphs 
includes undirected graphs mixtures directed undirected graphs buntine 
includes stochastic networks hopfield models neural networks hertz krogh palmer buntine complex unsupervised learning systems autoclass iv variety covariances hanson stutz cheeseman systems multiple classes 
learning algorithms methods developed learning simple discrete gaussian bayesian networks data learning simple unsupervised models mentioned section 
previous models linear regression weighted rule systems represented bayesian networks learning algorithms apply 
unfortunately 
general categories algorithm schemes learning mixed matched various problems 
categories considered represented models address 
section briefly explains categories 
algorithms learning described exp family mixture model exp family exp family partial exponential model exponential model generic categories models 
buntine 
simplest category learning models exact closed form solutions learning problem 
category exponential family distributions includes gaussian multinomial basic distributions bernardo smith decision tree gaussian bayesian network known fixed structure linear regression gaussian error described section 
exponential family distributions closed form solutions learning problem linear graphical models discovering knowledge sample size bernardo smith buntine 
instance univariate gaussian distribution estimate unknown mean standard deviation sample mean sample standard deviation usually adjustment estimate unbiased 
search numerical optimization involved 
exponential family category represented exponential model 
probability model data parameters xj shown exponential family 
important categories learning models exponential family category 
second category learning models useful subset model fall exponential family 
represented partial exponential model 
part problem exponential family solved closed form mentioned 
remaining part problem typically handled approximately 
decision trees bayesian networks multinomial gaussian variables fall second category buntine buntine spiegelhalter structure tree network known linear regression subset selection relevant variables 
represented follows 
know structure model exponential family parameters probability model xj exponential family hold fixed 
third category learning models hidden variables introduced data problem exponential family hidden values known 
represented mixture model 
general family models xjc exponential family hidden variable variables model parameters 
occur data yields probability model xj xjc cj examples category mixture experts model section unsupervised learning models mentioned section 
category models model unsupervised learning incomplete data classification problems robust regression general density estimation titterington 
mixture model category learned em algorithm 
em algorithm inner loop closed form solution underlying exponential family model 
final category problems catch represented generic model 
case data unconstrained probability model xj assume form 
includes feed forward neural networks weighted rule model section 
models learned algorithms buntine posteriori map algorithm general error minimization schemes 
notice general categories learning models cast form ignoring structural detail model 
algorithms map algorithm applied categories learning models 
graphical component probabilistic models relevant visual aid describing models 
graphs provide structural view probability model getting lost mathematical detail 
invaluable way qualitative physical model invaluable explaining behavior recourse numeric detail 
probabilistic modeling 
buy 
probabilistic models provide language performing problem decomposition recomposition illustrated section knowledge refinement illustrated section 
inference probabilistic models developed performed variety probabilistic inference schemes listed section 
second flexibility probabilistic graphical models suitable language represent wide variety learning models 
course said 
probabilistic models allow probability theory applied directly derive inference algorithms principles maximum likelihood maximum posterior probabilistic schemes 
relevant algorithms discussed section 
offers unifying conceptual framework developer instance smooth transitions modes probabilistic reasoning diagnosis explanation information gathering 
third probabilistic framework offers computational approach developing learning discovery algorithms 
conceptual framework 
probability decision theory decompose problem computational prescription search optimization techniques fill prescription 
software tool exists implements special case conceptual framework gibbs sampling computational scheme gilks 
gibbs sampler family algorithms fit general framework 
explained buntine framework categories learning models described section basis 
real gain scheme arise potential reimplementation existing software understanding gained putting different models learning discovery common language ability create novel hybrid graphical models discovering knowledge optimizing search methods statistical decision methods basis basis linear gaussian glm software generator 
algorithms ability tailor special purpose algorithms specific problems 
instance recognizing connection logistic regression neural networks rules done section able borrow algorithms fields address task 
scheme supports problem decomposition iterative knowledge refinement processes described sections 
bibliography iv 
automated knowledge acquisition expert systems 
proceedings european conference machine learning 
bernardo smith 
bayesian theory 
chichester john wiley 
boulton wallace 
program numerical classification 
computer journal 
brachman anand 
process knowledge discovery databases sketch 
advances knowledge discovery data mining eds 
fayyad piatetsky shapiro smyth 
mit press 
buntine 
operations learning graphical models 
journal artificial intelligence research 
buntine 
learning classification trees 
artificial intelligence frontiers statistics ed 
hand 
london chapman hall 
buntine buntine 
theory refinement bayesian networks 
uncertainty artificial intelligence proceedings seventh conference eds 
ambrosio smets bonissone 
san mateo california morgan kaufmann 
buntine 
myths legends learning classification rules 
eighth national conference artificial intelligence 
boston massachusetts aaai press 
casella berger 
statistical inference 
belmont california wadsworth brooks cole 
chan shachter 
structural controllability observability influence diagrams 
uncertainty artificial intelligence proceedings conference eds 
dubois wellman ambrosio smets 
stanford california morgan kaufmann 
cheeseman stutz 
bayesian clustering 
advances knowledge discovery data mining eds 
fayyad piatetsky shapiro smyth 
mit press 
dagum horvitz 
uncertain reasoning forecasting 
international journal forecasting 
forthcoming 
dean wellman 
planning control 
san mateo california morgan kaufmann 
duda hart nilsson 
subjective bayesian methods rulebased inference systems 
national computer conference afips conference proceedings vol 

fayyad weir 
system sky survey cataloging analysis 
advances knowledge discovery data mining eds 
fayyad piatetsky shapiro smyth 
mit press 
gilks thomas spiegelhalter 
language program complex bayesian modelling 
statistician 
ginsberg weiss 
automatic knowledge base refinement classification systems 
artificial intelligence 
hanson stutz cheeseman bayesian classification correlation inheritance 
international joint conference artificial intelligence 
san mateo california morgan kaufmann 
graphical models discovering knowledge heckerman 
bayesian networks knowledge representation learning 
advances knowledge discovery data mining eds 
fayyad piatetsky shapiro smyth 
mit press 
heckerman 
probabilistic similarity networks 
networks 
henrion breese horvitz 
decision analysis expert systems 
ai magazine 
hertz krogh palmer 
theory neural computation 
addison wesley 
jordan jacobs 
supervised learning divide conquer statistical approach 
machine learning proc 
tenth international conference 
san mateo california morgan kaufmann 

computational scheme reasoning dynamic probabilistic networks 
uncertainty artificial intelligence proceedings conference eds 
dubois wellman ambrosio smets 
san mateo california morgan kaufmann 
kraft buntine 
initial exploration database 
seventh international symposium aviation psychology columbus ohio 
lauritzen dawid larsen 

independence properties directed markov fields 
networks 
mccullagh nelder 
generalized linear models 
london chapman hall 
second edition 
madigan 
explanation belief networks 
research report seattle washington 
submitted publication 
matheus piatetsky shapiro mcneill 
key findings reporter analysis healthcare information 
advances knowledge discovery data mining eds 
fayyad piatetsky shapiro smyth 
mit press 
ousterhout 
tcl tk toolkit 
addison wesley 
pearl 
probabilistic reasoning intelligent systems 
morgan kaufmann 
poland 
decision analysis continuous discrete variables mixture distribution approach 
ph diss dept engineering economic systems stanford univ buntine rabiner juang 
hidden markov models 
ieee assp magazine january 
shachter andersen szolovits 
global conditioning probabilistic inference belief networks 
uncertainty artificial intelligence proceedings tenth conference eds lopez de mantaras poole 
san mateo california morgan kaufmann 
shachter heckerman 
thinking backwards knowledge acquisition 
ai magazine fall 
shachter 
evaluating influence diagrams 
operations research 
smyth 
detecting novel classes applications fault diagnosis 
ninth international conference machine learning 
san mateo california morgan kaufmann 
spiegelhalter dawid lauritzen cowell 
bayesian analysis expert systems 
statistical science 
titterington smith makov 
statistical analysis finite mixture distributions 
chichester john wiley sons 
towell shavlik noordewier 
refinement approximate domain theories knowledge neural networks 
eighth national conference artificial intelligence 
boston massachusetts aaai press 
whittaker 
graphical models applied multivariate statistics 
wiley 
