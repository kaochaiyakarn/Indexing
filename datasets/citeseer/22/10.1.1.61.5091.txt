log linear interpolation language models alexander university cambridge th november thesis submitted university cambridge partial fulfilment requirements degree master philosophy computer speech language processing building probabilistic models language central task natural language speech processing allowing integrate syntactic semantic pragmatic constraints language systems 
probabilistic language models attractive alternative traditional rule systems context free grammars availability massive amount text corpora efficiently train models binary grammaticality judgement offered rule systems likelihood sequence lexical units obtained crucial factor tasks speech recognition 
probabilistic language models find application part speech tagging machine translation semantic disambiguation numerous fields 
widely language models estimation probability observing lexical unit conditioned observations preceding lexical units known gram models 
gram estimates poor reason may technique called smoothing applied adjust estimates hopefully produce accurate model 
smoothing techniques may roughly divided backing interpolation 
case best gram model current context selected second case gram models different specificities combined form better predictor 
thesis proposed novel interpolation scheme investigated log linear interpolation 
original publication dealt combining models unrelated nature aim thesis formulate theoretical framework smoothing gram probability estimates obtained similar language models different levels specificity corpus called log linear gram smoothing compare established linear interpolation back methods 
framework proposed includes probability combination parameter optimisation dealing data sparsity parameter clustering 
resulting technique shown outperform conventional linear interpolation back techniques applied gram smoothing tasks 
ii foremost supervisor dr thomas niesler encouraged supported guided language modelling endeavours step way sound friendly advice completion dissertation possible 
deepest professional gratitude goes 
owe gratitude william lee department earth sciences proofreading thesis making insightful comments mathematical aspects presentation thesis appear appears valerie department english applied linguistics supplied interesting insights things doing theoretical linguistics perspective 
completion experiments described possible patrick gosling continues maintain excellent facilities speech group fallside laboratory 
people introduced language modelling speech natural language processing dr ted briscoe excellent course lectures stochastic context free unification grammars phil woodland expert professional coverage language modelling speech recognition people members staff alike dr mark gales lien max paolo dan nathan smith matt friendly attitude high expertise proved deeply rewarding experience want dr stephen pulman explaining intricate differences mathematical computational linguistics pointing inherent difficulties existing approaches 
unfair mention simon sch del numerous coffee tobacco breaks regardless time weather conditions excellent helped enjoy life cambridge ka 
iii declaration dissertation result original draws acknowledged appropriate points text 
dissertation submitted part degree institution 
length thesis including footnotes approximately words 
implementation theoretical frameworks algorithms derived appendix dissertation submitted separately 
iv family 
contents scope thesis 
thesis organisation 
background language modelling equivalence classification history 
statistical estimation 
maximum likelihood estimation 
sparse data problem 
quality assessment language models 
overview smoothing techniques discounting methods 
basic discounting 
turing estimate 
cross validation deleted estimation 
unconstrained discounting model 
leaving estimate joint probabilities 
combination estimators 
katz backing 
linear discounting 
absolute discounting 
kneser ney smoothing 
linear interpolation 
unified view backing linear interpolation 
parameter tying 
interpolation language models linear smoothing 
parameter tying 
parameter optimisation 
log linear smoothing 
formal framework 
parameter tying 
multidimensional optimisation 
similarity maximum entropy models 
significance log linear interpolation weights 
vi contents vii performance evaluation language modelling corpora 
transcriptions conversational telephone speech 
wall street journal wsj corpus 
baseline models 
linear interpolation 
log linear interpolation 
performance hub evaluation sets 
performance wsj evaluation sets 
discussion 
summary bibliography list tables hub language modelling corpus details sizes corpora shown total number words 
wsj language modelling corpus details sizes corpora shown total number words 
total number words kept held portions training data hub train wsj train wordlist sizes 
perplexities baseline back models trained hub language model training data wsj training data tested respective evaluation sets 
performance linear interpolation models maximum likelihood back estimates clustering constraint nmin back language model test sets hub 
perplexity log linear smoothing model built hub language modelling data nmin versus linear counterpart 
optimal linear log linear interpolation weights trained hub language model training corpus nmin pair interpolation weights corresponding bigram model tuple trigram model 
total number optimal log linear smoothing weights calculated level log linear trigram language models trained experiments total number optimal weights absolute values exceeded unity hub language model evaluation sets 
optimal linear log linear interpolation weights trained wsj language model training corpus nmin pair interpolation weights corresponding bigram model tuple trigram model 
performance log linear smoothing model built wsj language modelling data versus back linear counterparts best models terms clustering criterion selected 
value clustering parameter nmin optimal perplexities obtained wsj evaluation sets linear log linear interpolation models 
viii list figures rank frequency plot words unigrams doubly logarithmic axes small evaluation big training corpora showing pattern underlying zipfian distribution 
plot unnormalised log linear interpolation term function probability weight values taken different ranges 
unnormalised log linear interpolation term surface function probability log linear weight 
influence cluster size nmin performance linear interpolation maximum likelihood estimates 
influence cluster size nmin performance linear interpolation katz back models 
influence cluster size nmin performance log linear interpolation katz back estimates hub language modelling evaluation sets 
influence size kept portion hub training set performance linearly log linearly smoothed katz back estimates 
performance trigram log linear linear interpolation models wsj evaluation sets 
ix chapter language modelling attempt characterise capture exploit syntactic semantic pragmatic regularities exhibited natural language 
widely domains including speech recognition optical character recognition handwriting recognition machine translation part speech tagging dialog modelling spelling correction 
simplest form language model may representation list sentences belonging language complex models may try describe structure meaning underlying sentences natural language 
techniques modelling language historically fall categories 
type models traditional grammars context free unification grammars rigorously defined linguistic perspective suffer typical deficiencies rule systems 
difficult maintain adapt new domains languages computation complexity high efficiently employed time critical applications large vocabulary continuous speech recognition 
linguistically appealing models scope thesis 
years second language model category corpus shallow probabilistic models statistical representation natural language gained common usage 
statistical language model describes probabilistically constraints word order language typical word sequences assigned high probabilities atypical ones assigned low probabilities 
statistical models language may evaluated measuring predicted probability unseen test utterances models generate high average word probability equivalent low novelty low entropy low perplexity considered superior 
perplexity measure commonly measure goodness model 
widely statistical model language gram model estimate likelihood word wn solely identity preceding words utterance 
strengths gram model come success capturing local constraints ease may constructed text corpora computational efficiency 
problems exhibited statistical prediction natural language problem data sparseness arising uneven distribution lexical units language 
gram models instance possible tuple events encountered text corpus train model regardless size corpus order language model reliable ensured probabilities model assigns word strings 
scope thesis nonzero unseen word sequences question rendered improbable hypothesised 
technique language modelling obtaining accurate probability estimates insufficient amount training data called smoothing 
smoothing overcomes shortcomings conventional probability estimates account considerations word combinations possible single word sequence zero probability 
nonlinear distribution words natural language amount training data assumed small size millions words going events evaluation corpora unobserved training corpus 
simplest smoothing techniques achieve effect pretending ngram occurs complicated ones define complex discounting frameworks 
thesis discussion restricted smoothing gram models structure model unchanged method estimate probabilities model modified 
numerous types language models smoothing applied class language models maximum entropy models decision tree models stochastic grammars remains seen improved smoothing techniques gram language modelling lead improved performance models 
language model smoothing frameworks usually fall basic categories selecting best model current context available ones predictor probably best known model back model suggested katz second ones usually combine language models obtain probability estimate linear interpolation jelinek typical representative 
thesis concerned special case smoothing belonging second basic category combination language models 
novel interpolation technique called log linear interpolation investigated 
scope thesis problems selected investigate log linear interpolation gram language model smoothing different levels framework accurate probability estimation efficient parameter optimisation parameter tying 
accurate probability estimation framework important task language model supply reliable estimates exceptional conditions manifestations data sparsity problem 
particular data available log linear interpolation attempts remedy combination lower order models 
additional issues addressed concern choice calculation normalisation factors model 
parameters log linear interpolation model optimal respect training data yield satisfactory performance test corpora 

thesis organisation different optimisation algorithms proposed efficient parameter estimation possible 
order log linear interpolation parameters controlling performance model estimated reliably training data available model 
amount training data sufficient problem solved constraining certain groups parameters value tying 
context possible parameter tying algorithm log linear interpolation proposed 
widespread aforementioned linear interpolation back language models selected baseline models theoretical experimental results obtained log linear interpolation compared 
thesis organisation dissertation organised follows chapter provides necessary background language modelling chapter discusses conventional smoothing techniques prevalent language modelling chapter provides theoretical framework linear log linear interpolation context smoothing techniques parameter clustering optimisation efficient probability estimation chapter describes experiments carried interpolation models developed dissertation presents interesting results obtained novel log linear smoothing framework language modelling 
chapter presents summary 
chapter background language modelling task language modelling assign probability value possible word text stream likelihood occurrence context finds 
task fundamental speech optical character recognition areas natural language processing word sense disambiguation probabilistic parsing 
speech recognition need calculate probabilities word strings wn word wi belongs fixed known vocabulary 
definition conditional probabilities decomposition obtained wi wi wi wi probability word wi spoken words wi uttered previously 
task statistical language model provide decoder adequate estimates probabilities wi wi 
word string wi usually referred history word wi 
noted vocabulary size possible distinct histories values needed complete specification probabilities wi wi 
practical vocabulary sizes astronomical number estimates stored accessed efficiently 
equivalence classification history order avoid problem mentioned possible conditioning histories wi distinguished belonging manageable number nh equivalence classes 
desirable define applications mapping operator classify history wi word wi belonging nh subsets hk wi hk nh set equivalence classes 
statistical estimation mapping conditional probabilities may estimated wi wi 
problem define appropriate mapping operator 
popular approach assume dependence conditional probability observing word wi position restricted prior local context immediate predecessor words wi wi 
essentially markov chain assumption leads directly notion gram language models wi wi wi 
widely gram models obtained bigram trigram 
number alternative equivalence classifiers lie outside scope discussion developed past decade application decision trees clustering word histories 
statistical estimation training corpus size representing language interest history equivalence classification divides training corpus nh subsets second goal find way derive reliable probability estimates words corpus histories 
sections describe various specialised statistical techniques obtain estimates 
commencing notions defined 
chapter counts describe training data wi wn 
example trigram counts obtained counting particular word trigram occurs training data count definitions wi wi wi uvw number observations joint event number observations word number observations history total number observations 
addition count counts frequencies frequencies nr nr defined certain count occurred nr number distinct words seen history exactly times nr total number distinct joint events occurred exactly times 
events called unseen observed training data events called singleton events observed exactly 
shall see play crucial role estimation sparse data 
counts referred relative frequencies 

statistical estimation maximum likelihood estimation word wi position text corpus wi wn conditioning history hi known 
arrive maximum likelihood estimate set conditional probabilities consider logarithm likelihood optimised set log wi hi log wi hi log line summation index changed count definitions 
addition normalisation constraint observed optimising log likelihood function 
function includes constraints optimised method lagrangian multipliers log 
partial derivatives respect probabilities lagrangian multipliers equating zero obtain set equations 
seen second equation expresses exactly normalisation constraint history straightforward manipulations maximum likelihood estimate word history pml 
seen estimate assigns highest probability training corpus waste probability mass events observed training 
follows estimator form assign zero probability event seen training corpus 
problem unseen events linked directly notion data sparseness 

statistical estimation sparse data problem fundamental problem language modelling problem data sparseness 
words language interest common vast majority words uncommon longer grams involving rarer 
grams assigned zero probabilities maximum likelihood estimator grams seen training 
zero probabilities propagated result wrong estimates sentences 
experiments training corpus words described shown trigram tokens test corpus contained words previously unseen 
assuming size corpus big hope collecting data possible avoid problem data sparseness 
may initially plausible increasing coverage training corpus possible refine existing probability estimates obtain additional ones general solution problem 
limited number frequent events language investigation seemingly tail probability distribution rarer rarer events simply collecting data reach tail 
phenomenon observed zipf uncovered pattern statistical distribution language obtaining counts word type corpus sorting word types order frequency occurrence relationship counts frequency word wi position list known rank order frequency wi roughly reciprocal curve wi wi equivalently exists constant different word types wi corpus wi wi 
rough approximation attempt find closer fit empirical distribution words consult mandelbrot useful description frequency distribution words human languages common words number medium frequency words low frequency words 
shows rank frequency plots words different corpora corresponds small evaluation corpus consisting approximately word tokens word types corresponds bigger training corpus consisting word tokens word types 
specific text corpora described detail section 
seen significant increase size corpus form statistical distribution underlying language obtained rendering approaches maximum likelihood impractical 
need devise better estimators allow possibility observing unseen events making word combinations possible assigning non zero probabilities amount training data insufficient 
ongoing debate nature processes described zipf law best summarised renowned linguist george miller wrote 
statistical estimation frequency frequency rank hub eval evaluation corpus rank hub train training corpus rank frequency plot words unigrams doubly logarithmic axes small evaluation big training corpora showing pattern underlying zipfian distribution 

quality assessment language models faced massive statistical regularity alternatives 
assume reflects universal property human mind assume represents necessary consequence laws probabilities 
zipf chose synthetic hypothesis searched principle effort explain apparent equilibrium uniformity diversity words 
subsequently attracted problems chose analytic hypothesis searched probabilistic explanation 
findings supporting synthetic hypothesis arguments supporting analytic hypothesis 
contains interesting overview research efforts non linguistic areas zipf law applies 
quality assessment language models common evaluation techniques language modelling information theory 
assuming language information source emitting sequence symbols finite vocabulary viewing stochastic process characterised inherent entropy defined amount non redundant information conveyed word average certain language question 
entropy discrete random variable defined log ep log log base expressing entropy bits ep expectation random variable log 
probability mass functions relative entropy kullback leibler distance log ep log 
quantity non negative equals zero probability distributions identical 
cross entropy random variable true probability distribution probability distribution defined log possible introduce cross entropy language described pl respect certain model pl pm pl log pm 
shannon mcmillan breiman theorem entropy equation defined lim log xn known asymptotic equipartition property 

quality assessment language models allows rewrite cross entropy pl pm lim pl xn log pm xn xn lim log pm xn assuming pl ergodic simplifies pl pm lim log pm xn sufficiently large sample size simplified pl pm log pm xn 
quantity useful specifies upper bound unknown true entropy pl language model pl pl pm difference pl pm pl measure inaccuracy model respect true model cross entropy estimate commonly metric evaluating performance language models 
test corpus composed nt sentences disjoint data train model equation calculate probability sentence wi model allowing calculate cross entropy test corpus pt pm nt log wi st st total number word tokens simple trigram models smoothed linear interpolation trained slightly half words drawn various corpora assumed reasonable representative sample english tested brown corpus words brown give upper bound bits character entropy english higher original shannon estimate bits character human subjects obtaining gambling estimate 
alternative metric directly related cross entropy perplexity defined reciprocal geometric average probability assigned model word test corpus ppm pt pm log assumed base need necessarily probability estimate sentence size defined cross entropy estimate perplexity expressed ppm log qn pm wi wi pn log pm wi wi pm wn consequent experiments log taken base 

quality assessment language models thought measure complexity task recognising text ppm equally words language model 
minimising perplexity analogous minimising cross entropy model respect test set 
goal statistical language modelling viewed minimising perplexity cross entropy bring close possible true entropy language 
chapter overview smoothing techniques chapter described popular techniques language modelling obtaining reliable probability estimates applying technique called smoothing 
section presents various discounting techniques basis forming statistical estimators section presents models comprised combination statistical language model estimators various methods discounting usually yield reliable robust predictors 
discounting methods section presents overview discounting methods prevalent language modelling try remedy data sparseness problem manifested natural languages 
basic discounting alternative form maximum likelihood estimate gram pml 
probability estimate gram seen times pml 
basic idea approaches remove probability mass observed events assign events unseen training 
oldest solution employ laplace law succession referred adding adds phantom observation frequency count required obtain mle model 
set distinct grams considered number phantom observations 
expressed notation modified frequency count may written 

discounting methods zipfian distributions long tails infrequent events tends assign probability mass unseen events 
variant laplace law defined usually involves adding positive value smaller 
technique known law 
effect assuming uniform uninformative prior events applying bayes estimator viewed linear interpolation mle estimate uniform prior 
law help avoid problem manifested laplace approach probability mass observed events choosing small value major objections prove ineffectiveness way guessing appropriate value advance ii simple discounting schemes yield probability estimates linear mle frequency match empirical distribution low frequency 
techniques mentioned shown perform poorly 
turing estimate turing estimator central smoothing techniques 
initial development derivation important formula field biology widely 
result stated theorem formulation theorem rigorous proof may place subsequent developments proper context 
section implications important result discussed 
sv finite collection types words bigrams species animals 
type tokens examples words bigrams sampled ps denote sample size drawn types type sv having binomial distribution probability pv 
nr number types frequency sample rv denote frequency vth type 
theorem independent marginally binomial samples ps ps drawn expected frequency sample types occurring times nr ps nr ps practical sizes samples immediately follows nr nr 
unigram case essentially equal size vocabulary bigram case text corpus 
discounting methods assumption introduces relative error practical computation expectations estimated smoothed values yielding nr nr 
unsmoothed count counts takes form turing estimator nr nr referred turing estimator may alternatively derived cross validation approaches described discussed chapter 
substitution empirical estimates nr expectations nr done uniformly nr unreliable high values particular frequent type estimated probability zero number types frequency greater zero 
prevents directly 
original solution proposed fit function observed values nr smoothed values expectations leading 
different turing estimators possible depending smoothing performed 
note calculation rests knowing number types observed training calculated vocabulary size 
example consider bigrams total universe types estimate nr nr practical size vocabulary approximation safe assumption 
interesting theoretical empirical comparison turing estimator zipf law 
cross validation deleted estimation result sect derived assumption distribution type binomial essentially means events occur independently empirical realisation result held estimator available training corpus divided retained held parts general name methods held retained sets cross validation 
assumption methods weaker binomial assumption simply states parts text generated process 
basic held estimation done follows denoting counts retained held sets respectively letting nr denote number grams frequency retained set occurrences grams frequency held set counted cr hw necessarily assumption 

discounting methods adjusted frequency calculated cr nr 
training data frequency counts smoothing probability estimates efficient schemes possible part training data initial training retained data held data 
method efficient training data basic held estimation deleted estimation 
denoting parts training data number types occurring times th part total number occurrences types st part 
likewise defined number types occurring times st part training data number occurrences types th part 
basic held estimates combined yield equation deleted estimator 
experiments described shown deleted estimator outperform basic held estimator large training corpora methods inferior turing estimate :10.1.1.131.5458
alternative extension cross validation leaving method training corpus size split training part consisting tokens held part consisting token sort simulated testing 
process repeated times tokens held part 
advantage training approach tokens training held part efficiently exploiting training corpus 
particular method explores effect model changes particular token observed 
deleted estimation leaving approaches shown lead turing estimates 
derivation turing estimate leaving method discussed sect 
unconstrained discounting model 
unconstrained discounting model shown sect applying turing estimate feasible solution problem high frequency types 
alternative original smoothing solution turing re estimation frequencies low frequency words quite numerous substitution observed expectations quite accurate approximation regular mle estimates high frequencies accurate need discounting 
unconstrained model discounting constructed heuristically need resort turing analysis 
letting maximum count count dependent discounting factors normalisation constraint gained probability mass redistributed unseen events 
model joint probability events proposed 
discounting methods estimated number unseen events defined 
examining previous definition somewhat simplifying count notation equations obtained hw rnr rnr rnr 
equations useful deriving closed form solution optimal discounting factors section 
leaving estimate joint probabilities events joint events obtained training corpus wi wn isolating word wi history hi text positions 
consider process removing certain observation hi wi observations held part 
original count 
removing observation observations type left training observations 
held part data count discounting factor 
follows rnr observations held data parameter 
summing counts full log likelihood function leaving method defined log hw hw hw log hw log rnr log log decomposition log likelihood function parts part events events essential 
discounting methods holding count form probability function original singleton events form defined unseen events second part decomposition reflects probability rest counts stay nonzero 
partial log likelihood find optimal values constructed considering dependent parts log hw hw hw log log log hw log log rnr log nr log definitions 
partial derivatives respect equating zero system equations unknown parameters obtained rnr ssns nr 
exploiting fact sum depend index closed form solution obtained sns sns nr rnr nr 
rnr plugging events unconstrained discounting model probability estimates obtained nr 
probability mass unseen events hw total probability mass events seen training nr hw 
combination estimators leaving probability estimate ignoring probability mass counts assuming obtain turing estimate nr nr turing discounting factor obtained nr rnr turing estimate probability mass unseen events hw useful equation checking coverage vocabulary 
combination estimators techniques described far raw count gram base prediction 
methods assign probability grams appeared appeared rarely desirable 
theory estimate different probabilities grams frequency order account better occurrences different word patterns natural language 
example cases hope produce better estimates looking frequency grams gram 
supply additional information refine estimates 
initial developments area described estimates unseen bigrams shown refined terms probabilities unigrams compose bins describing disjoint groups bin treated separate distribution turing estimation performed giving corrected counts normalised yield probabilities 
section problem combining probability estimates different models popular solutions described 
katz backing katz smoothing extends intuitions turing estimate adding combination different models consulted order depending specificity 
detailed model able provide sufficiently reliable information current context models defined recursively terms lower order models 
katz unconstrained model count dependent discounting factors applied integer constant katz suggests 
satisfy normalisation constraint gained probability mass computed separately history distributed unseen events general lower order distribution process backing 

combination estimators generalised history gram defined gram example bigram history trigram unigram generalised history 
general distribution conditioned generalised history katz model estimate conditional probability defined subject usual normalisation constraint 
large counts taken reliable discounted 
count counts nr nr nr rnr leaving method constructs log likelihood function way similar section order obtain optimal values discounting coefficients arriving closed form solution sns sns nr rnr virtually identical leaving discounting case joint probabilities 
original solution requires total probability mass unseen events equal turing estimate unseen events rnr defined re normalising corresponding turing factors factor nr rnr 
combination estimators obtaining final estimates nr rnr nk nk 
note developments leading derivations katz model far assumed generalised distribution known 
obvious needed way estimating generalised distribution 
standard model relative frequency estimates lower order events advocated 
move justified estimate conventional maximum likelihood estimate 
linear discounting linear discounting non zero maximum likelihood estimates scaled constant slightly remaining probability mass redistributed novel events general leaving method obtain closed form solution identical turing probability estimate unseen events 
model shown perform poorly high low frequencies discounted constant factor known higher frequency accurate raw maximum likelihood estimate property reflected linear discounting 
absolute discounting absolute discounting method attempts leave non zero counts virtually unchanged 
heuristic justification may argued count certain event change significantly replacement training corpus size corpus size variation expected range 
leads average non integer count offset independent count 
subject normalisation constraint model takes form 
combination estimators constraint size universe events 
leaving estimation closed form solution obtained approximation upper bound approach taken turing probability mass unseen events leads possible estimate bgt nr enhancements absolute discounting possible 
usage discounting parameters singleton events advocated leads improved performance 
additional refinement suggested apply absolute discounting low counts similar katz model mle estimates high frequencies 
kneser ney smoothing approaches far lower order distribution taken smoothed version lower order maximum likelihood distribution 
lower order distribution important factor counts higher order distribution 
method section tries simulate condition 
kneser ney note word bigrams bona fide sri collocations proper names second word strongly coupled unigram count high predecessor word occurs corpus 
backing case know exactly predecessor word occurred 
result relative frequencies word unigrams estimate true probabilities 
solve problem authors propose generalised singleton distribution computing lower order distribution word bigrams seen smoothing applied backing methods discussed derivation described detail 
experiments language models improved incorporating singleton backing distribution described 
experiments kneser ney smoothing variants interpolation consistently outperform approaches :10.1.1.131.5458
linear interpolation important alternative back models described far linear interpolation technique higher order models mixed lower order models 
combination estimators suffer data sparseness 
basic form linear interpolation obtained floor method related law adding constant floor value value proportional specific distribution added hq dependent history 
introducing new constant equation linear interpolation formula proposed jelinek pli hq conditional probabilities may rewritten pli hq 
alternative way arrive result replace concept linear discounting described sect interpolation 
reason making interpolation parameters history dependent apparent considering fact higher counts higher distribution reliable smaller value appropriate low counts setting larger desirable 
smoothing parameters chosen maximise probability estimate pli 
solution advocated baum welsh em algorithm guaranteed converge local optimum 
method approaches language model smoothing problem rigorous hidden markov model hmm point view exploits training corpus process deleted interpolation similar deleted estimation method sect 
alternative derivation estimation equations linear interpolation authors leaving formalism arrive formulae happen produce correct iteration equations convergence guarantee baum welsh algorithm 
case re estimation equation interpolation parameter hq hq influence singleton distribution evident 
unified view backing linear interpolation noted existing smoothing methods described back equation 
combination estimators reliable estimate probability ml specific distribution scaling factor determined completely 
group smoothing methods expressed linear interpolation higher lower order models rewritten pli equation placed back form 
common approach place interpolated model back framework take adjust normalisation factor probabilities sum 
parameter tying order reduce number free history dependent parameters context linear interpolation need pooled different histories 
placing interpolation model back context described previous section obtain leaving formalism possible estimates obtained ney different types tying tying history history count tying assumption parameters depend history history count tying process carried dividing history counts moderate number partitions bins separate parameter bin approach taken experiments described 
extension approach average count method proposed chen parameters tied average number counts non zero element history reported yield better performance :10.1.1.131.5458

combination estimators history dependence results linear discounting model sect full tuple dependence results katz model sect 
seen different types tying result different smoothing models 
general having interpolation desirable defeats purpose cross validation case differ conventional learning 
recommended reasonable amount tying improve robustness model respect new data 
chapter interpolation language models chapter presents theoretical framework linear log linear interpolation different ways combining gram probability estimates 
particular scope thesis interested developing frameworks linear log linear interpolation gram language models nature built concept text corpus varying degree specificity 
efficient ways probability estimation parameter optimisation tying theoretical comparative analysis techniques 
section presents linear interpolation section presents log linear interpolation 
linear smoothing linear smoothing probability estimates known statistical nlp linear interpolation mixture models probably widely technique combining language models 
linear interpolation briefly introduced previous chapter general framework language modelling smoothing 
section discussed detail 
basic way linearly combine probability estimates take pli kpk linear interpolation hurt 
optimally interpolated model guaranteed worse components 
components viewed special case interpolation weight component 
guaranteed held data new data 
held data large representative result carry test data 
general technique frequently applied combining stochastic models different nature 
applications include combination parser models topic dependent models back maximum entropy models interpolation cache kneser ney smoothed high order gram skipping sentence models 

linear smoothing context concerned linearly combining probability estimates gram models obtained corpus 
expressed terms grams estimate expressed pli wi wi wi 
np wi constraints imposed interpolation weights assuming history dependence parameters introduced sect alternative functional form define linear interpolation recursively way pli wi wi pli wi :10.1.1.131.5458
allows define smoothed gram model recursively linear interpolation nth order actual probability estimate th order smoothed model 
recursion terminated st order smoothed model unigram probability estimate done pli wi wi wi wi wi wi wi th order smoothed model uniform distribution follows pli wi wi vocabulary size 
approach taken brown chen :10.1.1.131.5458
remains shown approaches advantageous 
parameter tying briefly mentioned intuitive point view parameters different different histories context seen sufficient number times reliably estimate parameter reflected appropriately low value making contribution particular probability distribution significant analogously times context appeared training higher giving probability mass lower order smoothed model 
generally feasible accurately train interpolation parameter independently enormous amount data needed estimation 
jelinek suggested divide parameters moderate number bins constraining parameters bin value 
mapping history bin bk number times history appeared training corpus bk bk bk collection possible bins 
number interpolation parameters equal total number bins 
intuitively bin bk small possible cluster similar grams remaining large accurately estimate corresponding parameter bk 
bins built clustering method referred wall bricks bins created bin contains nmin gram history 
linear smoothing counts 
starting minimal possible value assigning increasing values bin updating bin boundaries appropriately process continues minimum count nmin reached point process repeated remaining history counts possible values clustered assigned corresponding bins 
clustering done separately gram models yielding different bin spaces typically histories low counts bins contain large number histories higher history count values bin contain small number histories 
parameter optimisation method adopted purpose describing training optimisation framework linear interpolation held estimation briefly described sect reserves section training data called kept retained data obtaining gram probability estimates determining clustering parameters remaining usually smaller part training data held data optimising interpolation weights general simulate unseen test corpora 
alternative method called deleted interpolation deleted estimation different parts training data rotate simulating retained held part results combined kt denote kept ht held parts training corpus respectively true sizes corpora bn kt ht ht kt may defined intuitively corresponding notion kept held corpora size kept part big reliably estimate probabilities clustering parameters held part reasonably representative possible unseen test corpora size sufficient accurate parameter optimisation experimentally dividing total training data kept held sets way achieve smallest perplexity model respect second held set disjoint training data instance chen goodman divide training data kept set held sets held set parameter optimisation second held set solely cross entropy calculation :10.1.1.131.5458
recursive formalism defined noted sufficient estimate weight corresponding history independently bin bk history count falls 
bins bk estimated kept part training corpus kt interpolation parameter optimisation carried held corpus ht shown alg 
bin loglikelihood function optimised defined bn log pli leaving seen special case deleted estimation choice proved reasonable 

linear smoothing gram history orders starting bigrams bins bn collection bn find bn maximising log likelihood bn defined 
algorithm log likelihood optimisation clusters bins 
denotes counts obtained held set generalised th order history 
notation simplified slightly denoting weight bn 
summation carried events contexts held set ht history counts fall bin bn defined kept set kt log likelihood function bn optimised bottom fashion terms gram history orders 
initial bigram case pli equivalent unigram probability estimate higher order histories equal lower order linearly smoothed estimate 
shown special case probability distributions smoothed maximum likelihood estimates optimising log likelihood function respect kept set fact equivalent assigning maximum weight bn higher order maximum likelihood distribution case procedure essentially similar leading maximum likelihood estimation shown sect maximising loglikelihood model respect 
log likelihood function bn proven represent convex function range property immediately follows unique local maximum range 
consequently log likelihood monotonic function likelihood function unique local maximum range 
derivative respect expression obtained bn bn pli pml pli 
property convex functions log likelihood function attain maximum value boundary points case monotonically decreasing function range case function monotonically increases range interior point case function monotonically increases opt monotonically decreases bahl fast algorithm finding optimal value opt searching root derivative log likelihood function 
second derivative bn bn pli pml pli non positive solution guaranteed local maximum 
algorithm employs technique dividing search interval authors call recall refer counts kept set 

log linear smoothing require nmax log nmax required tolerance 
opt 

opt 

nmax nmax opt 
algorithm interval search optimal 
binary chopping search shown alg reported orders magnitude faster standard forward backward em algorithm employed instance peto rosenfeld thought faster performing variant bisection method root finding difference initial bracketing constraints root imposed behaviour function initially known 
additional improvements performance obtained newton raphson method second derivatives attempted performance algorithm satisfactory 
log linear smoothing alternative way combining language models may derived framework exploits constrained conditional relative entropy approach 
probability distributions pi combined conditional relative entropy unknown target model respect models defined non symmetric kullback leibler distance measure pi log pi di relative entropy conditional probability distributions pi wh di constraints system 
target probability distribution minimised terms conditional relative entropy respect additional model 
lagrangian multipliers constrained notation simplified denoting bn 

log linear smoothing system function minimised expressed pi di 
partial derivatives respect equating zero equation obtained pi di similarly respect target model log log pi rearranging terms yields log assuming log pi 
equating inner sum zero yields log log log pi 
assuming uniform distribution exponential making trivial rearrangements yields log exp pi 
denote interpolation weights 
term product dependent weights denoted allows express target model pi 

order define proper probability distribution properly normalised 
introducing history dependent normalisation factor zh equation obtained zh size vocabulary 
pi 
log linear smoothing absorbed normalisation factor zh true zh pi obtaining pi pi equation introduced viewed linear interpolation log domain contrast regular linear interpolation described preceding section explicit constraints appear log linear interpolation weights 
seen log linear interpolation estimate reliable probability estimates smoothed maximum likelihoods 
case events observed certain context corresponding maximum likelihood estimate zero total log linear estimate assigned zero value undesirable constitutes badly smoothed estimate 
additional problem may arise optimisation interpolation parameters interpolation parameters corresponding estimate equal zero case quantity equation undefined 
desirable nonzero probability estimates smoothing back estimates 
formal framework factors need considered order define efficient framework log linear interpolation interpolation parameters need history dependent order better account data sparsity sparse back estimates history bigger corresponding parameter interpolation parameters history dependent normalisation factors history dependent addition depend choice interpolation weights 
definition normalisation factor summation probability factors events vocabulary specific history immediate way clustering normalisation factors available normalisation factors strictly history dependent interpolation parameters may cluster dependent 
special handling needed cases requested history training set case normalisation factor interpolation weights available case lower order linear interpolation model 
aforementioned considerations result proposing framework obtaining reliable gram log linear probability estimates wi wi wi wi 
log linear smoothing probability estimates combined normalisation factor depends interpolation weights corresponding probability estimates 
case context training data probability estimates taken lower order model normalisation factor interpolation weights 
log linear interpolation estimate unigram taken just single unigram probability estimate wi wi need normalisation factor 
parameter tying noted clustering scheme estimating log linear interpolation parameters similar corresponding linear interpolation scheme described sect 
interpolation weight taken depend particular history observed histories clustered manageable number bins separate interpolation parameter bin 
difference clustering methods way interpolation weights 
mapping history gram wi bin bk number times history appeared kept part training corpus similarly linear smoothing kt bk ch bk bk collection possible bins bk certain bin history count falls ch defined collection parameters associated bin bk 
linear case collection ch consists interpolation weight bk 
log linear framework certain gram history length log linear interpolation weights associated 
lower order history 
addition normalised parameter associated history strictly dependent history interpolation parameters associated history zh bk log linear parameter collection certain bk defined follows kt bk ch bk bk zh bk bk normalisation factors strictly history dependent number normalisation factors equal number distinct histories kept part training corpus 
process building clusters estimate carried wall bricks clustering described sect respect similar process linear interpolation 

log linear smoothing multidimensional optimisation function optimised chosen log likelihood log linear probability distribution expressed follows bn log zh log hi bn bk bk 
bk hi histories different specificity denote counts obtained held set 
conventional held estimation performed divided training data kept set estimating probabilities held set interpolation weights optimisation 
optimisation framework defined complicated linear interpolation due presence normalisation score ensures log likelihood optimised log likelihood proper probability distribution expressed log zh log kt summation words vocabulary kept part training corpus 
non linear nature expression closed form solution derivative exists 
linear smoothing fast algorithm exploiting functional form log linear log likelihood function employed 
expression describes convex function second derivative may shown negative hill climbing unconstrained optimisation 
experiments described multidimensional optimisation algorithms family direct search methods requiring derivatives 
reported nelder mead multidimensional minimisation known simplex probably widely method nonlinear unconstrained optimisation maintains step non simplex geometrical dimensions nonzero volume convex hull vertices 
computationally attractive method typically requires function evaluations construct new simplex 
rigorous theoretical analysis treating nelder mead method published lagarias 
despite attractiveness certain families strictly convex functions nelder mead method converges non stationary point shown considered certain functions variables 
remedies detection non optimality proposed kelley 
due evidence potential problems nelder mead method applied experiments bigger sparse corpora smaller corpora generally slower robust powell method employed direction set method choice successive directions require calculation gradient employs brent dimensional search direction 
clear log likelihood family functions produce functions may cause problems unconstrained direct search optimisation 
zero length history corresponding unigram 
confused dantzig simplex algorithm linear programming 

log linear smoothing convergence problems whatsoever techniques observed experiments 
log likelihood optimisation carried top fashion case trigram smoothing starting trigrams bigrams shown alg 
important points worth mentioning 
firstly linear smoothing histories held set obviously kept set optimisation weight corresponding certain history gram length case log linear smoothing certain gram history length weights associated sub histories gram 
optimising trigrams instance specific weight trigram optimised weights lower order gram models bigram unigram 
optimising bigrams desirable exclude bigram histories corresponding weights optimised trigram optimisation stage order avoid duplication data log likelihood optimisation 
trigram log linear interpolation model information included optimisation trigram optimisation carried trigram histories consequently corresponding lower order bigram histories held set kept set 
bigram optimisation done bigram histories common kept held set subset held set trigrams kept set 
way collecting histories ensures sets data trigram bigram level optimisation disjoint 
addition seen equation actual optimisation process histories belonging kept set heldout set influence optimisation process associated counts zero 
actual optimisation normalisation factors calculated histories common held kept set 
optimisation certain bin finished normalisation factors calculated rest histories belonging bin optimal weights 
dramatically reduces computational complexity part algorithm recalculates normalisation factors histories certain bin 
sole computational bottleneck log linear algorithm orders magnitude slower linear smoothing calculation normalisation scores vocabulary 
interesting alternative suggested chen applicable forms exponential language models requiring normalisation 
proposed method ignore normalisation factors completely consider scores special processing required prevent scores rising probabilities model fast conventional language models easily calculate word error rate wer expensive speech recognition tasks lattice rescoring 
major disadvantage suggested technique possible anymore calculate perplexity model 

log linear smoothing gram history orders starting grams collect cluster histories held set ht corresponding grams history length histories length held set ht subset higher order history observed previous iteration optimisation include history optimisation 
include optimisation 
perform bin optimisation bins bn collection bn current bin bn run optimisation guess interpolation weights bn histories ht previously excluded optimisation fall bin bn calculate normalisation scores log zh bn kept set kt vocabulary 
obtain guess bn maximising log likelihood bn defined 
final estimates bn calculate normalisation factors histories kept set kt belonging bin bn algorithm top log likelihood optimisation gram log linear interpolation model 
similarity maximum entropy models goal maximum entropy language modelling construct model process generated training data 
expected value binary indicator function known feature function respect empirical distribution baseline maximum entropy models usually nested trigram bigram unigram features trigram feature instance defined statistic considered useful discovered importance acknowledged requiring model accord constraining expected value model assigns corresponding feature function expected 
log linear smoothing value respect model empirical distribution histories training data 
expected value constrained expected value training data requiring combining equations yields constraint excludes consideration models agree training data output process exhibit feature feature functions fi determine statistics feels important modelling process idea model accord statistics 
conditional entropy distribution log principle maximum entropy model maximising entropy selected set allowed probability distributions 
problem treated constrained optimisation problem feature fi assigned lagrangian multiplier optimisation function fi fi 
holding fixed unconstrained maximum exp ifi normalisation defined exp ifi value known dual function maximum shown log fi maximum entropy language model defined model subject constraints parametric form optimal parameters opt determined maximising dual function opt arg max seen obvious parametric similarities log linear probability estimate maximum entropy equation 
addition techniques employ functional form normalisation 

log linear smoothing significance log linear interpolation weights noted log linear interpolation weights value essence redefined lagrangian multipliers 
interesting check log linear interpolation weights influence exponential term product 
different possible ranges fall possible scenarios shown table 
particular context possible cases 
case shown absolute value case values graph 
case strictly positive weights contribution small probabilities strongly attenuated negative weights small probabilities strongly boosted 
asymptotic case occurs higher probabilities contribution tends 
simplest case log linear interpolation weight zero essentially means term question equal equivalent ignoring particular probability product equation 
case weight equal probability estimate accurate boosted attenuated 
second interesting case shown occurs absolute value bigger equal minus 
case negative weight small probabilities strongly boosted weak boosting bigger probabilities 
boosting stronger case big probabilities 
positive weight seen graph weight effectively cancels small probabilities contribution probabilities small boosting bigger probabilities 
order investigate behaviour expression related counterpart log linear domain log function variables investigated 
behaviour function logarithmic counterpart log function probability log linear interpolation weights summarised 
noted singularities function avoided ensuring probability weight equal zero time 
boosting attenuation effects different weight probability ranges evident graphs 
stage concluded log linear interpolation weights different interpretation regular linear interpolation weights 
constraining log linear interpolation weights certain range possible cases consider analysing specific log linear interpolation model 

log linear smoothing pow lambda pow lambda lambda lambda lambda lambda probability lambda lambda lambda probability plot unnormalised log linear interpolation term function probability weight values taken different ranges 

log linear smoothing pow lambda lambda log lambda lambda log unnormalised log linear interpolation term surface function probability log linear weight 
chapter performance evaluation chapter presents experimental results obtained linear loglinear interpolation trigram models 
section describes text corpora experiments section describes baseline back models resulting linear log linear interpolation models compared section describes experiments trigram linear interpolation model compares performance trigram katz back model section describes results obtained log linear interpolation experiments compares results linear interpolation back models 
language modelling corpora section language modelling corpora training testing language models described 
text corpora selected wide range corpora available language modelling transcriptions conversational telephone speech known switchboard archive wall street journal articles 
corpus sample real conversational speech second wide coverage representation newswire text 
noted sizes corpora different switchboard data smaller important testing behaviour language models corpora different sizes 
transcriptions conversational telephone speech current experiments conversational telephone speech conducted corpora distributed linguistic data consortium ldc switchboard switchboard ii callhome english 
switchboard corpora consist telephone conversations usa strangers 
corpora subject yearly hub evaluation large vocabulary continuous speech recognition hub lvcsr conducted national institute standards technology nist 
switchboard telephone speech corpus originally collected texas instruments darpa sponsorship 
release corpus published nist distributed ldc 
release number corrections data files 
switchboard collection sided telephone conversations speakers male female areas united states 
computer driven robot 
language modelling corpora table hub language modelling corpus details sizes corpora shown total number words 
set size words vocabulary size hub train hub eval hub eval operator system handled calls giving caller appropriate recorded prompts selecting dialling person callee take part conversation introducing topic discussion recording speech subjects separate channels conversation finished 
topics provided frequently 
selection topics callees constrained speakers converse spoke topic 
switchboard ii consists minute telephone conversations involving participants 
corpus collected linguistic data consortium ldc support project speaker recognition sponsored department defence 
recruit asked participate minute phone calls 
ideally participant receive calls designated number calls phones different telephone numbers ani codes 
suggested topic discussion read automated operator participants chat preferred 
aforementioned hub switchboard transcriptions approximately words available language model training transcriptions evaluation sets hub hub testing 
table shows details training portion hub language modelling corpus denoted hub train evaluation sets denoted hub eval hub eval 
wall street journal wsj corpus wsj corpus contains newspaper text collected wall street journal period inclusive 
corpus considerably bigger hub switchboard language modelling corpus findings expected hold larger corpora north american broadcast news nab state art speech recognition systems 
portion corpus selected training consists wall street journal archive denoted wsj train evaluation sets years denoted eval wsj eval eval details training evaluation sets shown table 
experiments wsj corpora settings standard vocabulary words significant reduction oov rate reported comparison cmu vocabulary words north american broadcast news nab corpus training models 

baseline models table wsj language modelling corpus details sizes corpora shown total number words 
set size words vocabulary size wsj train wsj eval wsj eval wsj eval table total number words kept held portions training data hub train wsj train wordlist sizes 
set kept set kt held set ht vocabulary hub train wsj train singleton events discarded 
evaluation sets obtained wsj language model development data corresponding year constrained approximately words 
symbol marking sentence predicted language model testing 
baseline models division training data necessary cross validation done hub language modelling training data hub train wsj training data wsj train sizes kept parts kt held parts ht shown table corresponding word list sizes experiments 
seen table held portion larger wsj training corpus smaller held portion switchboard language model training corpus 
particular reason division consideration held set significantly smaller kept set 
baseline model hub evaluation chosen trigram backoff model turing discounting frequency frequencies discounting range event context cutoffs 
baseline model wsj evaluations chosen trigram back model range singleton events cutoffs applied trigrams bigrams 
back models built training corpus ensure back models linear log linear interpolation models amount training data 
perplexities back models respect evaluation sets described previous section shown table 

linear interpolation table perplexities baseline back models trained hub language model training data wsj training data tested respective evaluation sets 
perplexity model test set perplexity hub train hub eval hub eval wsj train wsj eval wsj eval wsj eval number grams fall cluster hub eval hub eval influence cluster size nmin performance linear interpolation maximum likelihood estimates 
linear interpolation section experiments trigram linear interpolation models carried hub switchboard wall street journal language modelling corpora described 
results different cases concerning linear smoothing maximum likelihood estimates second linear smoothing katz back models 
maximum likelihood estimates optimal clustering scheme linear interpolation maximum likelihood estimates selected experimenting different values clustering constraint nmin defined sect essentially defines bins interpolation weights 
influence parameter performance linear interpolation model abovementioned data sets shown 
worst result obtained clustering corresponding nmin keeping separate interpolation weight gram history 

linear interpolation perplexity hub eval hub eval number gram histories fall cluster nmin influence cluster size nmin performance linear interpolation katz back models 
case going data accurately estimate enormous number parameters resulting perplexities high 
optimal perplexities obtained test sets correspond nmin shown table perplexity hub eval test set equal perplexity hub eval test set equal slightly higher corresponding baseline back model perplexities 
back estimates expectations linear interpolation back probability estimates perform back model 
order check claim recursive model consisting linear combination back scores recursion terminated back probability estimate unigram defined similarly pli wi wi pli wi built tested 
back estimate obtained conventional katz smoothing model introduced sect basic turing discounting applied small counts 
similarly maximum likelihood case discussed probability estimates obtained kept part training set interpolation parameters trained held set 
shows results optimal clustering experiment conducted hub test sets 
comparing performance conventional back model trained training corpus held kept parts linear combination back estimators superiority discounting frequency context event cut offs applied 

log linear interpolation table performance linear interpolation models maximum likelihood backoff estimates clustering constraint nmin back language model test sets hub 
models perplexity hub eval hub eval back linear interp 
ml linear interp 
back model evident regardless clustering criteria 
optimal clustering parameter nmin biggest perplexity reduction achieved nmin value optimal clustering parameter depends size training set ratio held set kept set sizes 
may expected value optimal parameter corresponding different layout training data different 
optimal perplexities best models corresponding clustering criterion nmin test sets shown table 
seen back model outperforms linear interpolation maximum likelihood estimates test sets differences perplexities small 
linear interpolation back estimates consistently outperforms back model reduction perplexity achieved baseline back model hub eval test corpus hub eval test corpus 
log linear interpolation experiments probability estimates smoothed taken conventional katz back estimates turing discounting 
experiments conducted hub switchboard wsj corpora 
performance hub evaluation sets trigram log linear interpolation model built hub language modelling data hub train divided kept held portions described sect similarly linear smoothing kept part corpus obtaining trigram bigram unigram katz back probability estimates held portion optimising log linear smoothing weights 
powell multidimensional direct search parameter optimisation 
influence clustering constraint nmin performance model test sets baseline linear smoothing models described previous section shown 
noted log linear smoothing better behaved high values clustering parameters nmin smaller rate increase perplexity linear smoothing case 
smaller values clustering parameters linear smoothing performs stable manner 
linear log linear smoothing models outperform baseline back models log linear interpolation model performing slightly worse linear counterpart hub eval test set 
log linear interpolation perplexity hub eval li hub eval li hub eval lli hub eval lli number gram histories fall cluster influence cluster size nmin performance log linear interpolation katz back estimates hub language modelling evaluation sets 
table perplexity log linear smoothing model built hub language modelling data nmin versus linear counterpart 
models perplexity hub eval hub eval linear back log linear back slightly better hub eval test set 
perplexities best loglinear interpolation model selected interpolation models built hub language modelling corpus corresponding best linear interpolation baseline model interpolating katz back estimates best clustering parameter nmin summarised table 
seen table reduction perplexity best log linear model hub eval test set respect best linear model tiny 
experiment investigated influence size kept portion hub training data performance log linear interpolation model involved changing kept set size keeping held set size fixed 
performance resulting linear log linear interpolated language models respect baseline back model shown 
noted increasing size kept set depicted range corpus sizes improves performance models respect baseline back model trained maximum amount training data available 
interesting compare optimal weights obtained linear loglinear smoothing methods 
optimal weights case histories fall cluster shown table 
recursive definition linear smoothing framework sect weights lower order model 
log linear interpolation perplexity perplexity hub eval li hub eval lli hub eval bo kept set size number words hub eval baseline back perplexity hub eval li hub eval lli hub eval bo kept set size number words hub eval baseline back perplexity influence size kept portion hub training set performance linearly log linearly smoothed katz back estimates 

log linear interpolation table optimal linear log linear interpolation weights trained hub language model training corpus nmin pair interpolation weights corresponding bigram model tuple trigram model 
weights smoothing bigram trigram bi bi linear log linear tri tri reused higher order model 
trigram case instance bigrams tri tri tri tri bi tri tri bi bi bi bi bi weights bi tri uniquely define trigram linear interpolation model 
case log linear smoothing linear interpolation apparent relation weights necessarily sum negative explained specific way log linear framework defined weights essential defining model 
examining table noted absolute value log linear weights necessarily sum different corresponding linear smoothing weights 
additional interesting observation experiments concerns values log linear weights summarised table total number optimal weights absolute value bigger shown total number optimal weights calculated level trigram log linear model 
statistic hub corpus shows absolute values approximately total number weights calculated exhibited property 
particular upper bound absolute values approximately equal 
performance wsj evaluation sets experiments wall street journal archives trigram log linear interpolation models built wsj train katz back estimates smoothed obtained kept portion training data vocabulary event cutoffs applied singleton bigram unigram distributions 
nelder mead multidimensional direct search optimising smoothing weights 
order compare performance log linear smoothing corpus baseline linear interpolation models katz back models built settings 
tri 
log linear interpolation table total number optimal log linear smoothing weights calculated level log linear trigram language models trained experiments total number optimal weights absolute values exceeded unity hub language model evaluation sets 
model level bigram trigram table optimal linear log linear interpolation weights trained wsj language model training corpus nmin pair interpolation weights corresponding bigram model tuple trigram model 
weights smoothing bigram trigram bi bi linear log linear optimal weights obtained case histories fall bin corresponding nmin linear non linear smoothing shown table 
similar hub case absolute values vast majority weights obtained 
displays performance trigram log linear model evaluation sets wsj corpus 
performance corresponding baseline linear interpolation models displayed plot convenience 
performance log linear model compared baseline katz back linear interpolation test sets summarised table 
seen log linear interpolation compares favourably baseline techniques perplexity reduction wsj eval test set wsj eval test set wsj eval test set respect katz back baseline model 
log linear interpolation outperforms linear interpolation reduction perplexity wsj eval test set table performance log linear smoothing model built wsj language modelling data versus back linear counterparts best models terms clustering criterion selected 
tri tri models perplexity wsj eval wsj eval wsj eval back linear back log linear back tri 
log linear interpolation perplexity lli perplexity lli perplexity lli wsj eval lli number histories fall bin wsj eval li wsj eval baseline back perplexity wsj eval lli number histories fall bin wsj eval li wsj eval baseline back perplexity wsj eval lli number histories fall bin wsj eval li perplexity li perplexity li wsj eval baseline back perplexity performance trigram log linear linear interpolation models wsj evaluation sets 
perplexity li 
discussion table value clustering parameter nmin optimal perplexities obtained wsj evaluation sets linear log linear interpolation models 
models opt min wsj eval wsj eval wsj eval linear back log linear back wsj eval test set wsj eval test set 
graph seen experiments carried different test corpora values clustering parameters optimal perplexities linear log linear interpolation models obtained necessarily coinciding 
values optimal clustering parameters opt min yielding optimal perplexities test sets linear log linear interpolation models shown table 
difference values optimal clustering parameters explained fact histories clustered criterion way histories pooled held set different interpolation schemes eventually different number histories bin characteristics kept set counts identical linear log linear interpolation models 
discussion chapter performance log linear interpolation language models smoothing technique gram probability estimates measured perplexity test corpora models trained tested hub switchboard language modelling corpora wall street journal archives 
baseline models chosen conventional katz back linear interpolation models 
performance log linear interpolation model compared particular baseline model care taken discriminate model 
comparing linear interpolation ensured models exactly amount training data available sizes kept held sets 
case back baseline model trained training set model require cross validation training 
linear log linear interpolation models smooth katz back language model estimates 
smaller corpus hub switchboard corpus multiple runs performed attempt completely characterise relative performance log linear interpolation respect baseline models terms log linear interpolation model parameters clustering criteria training set sizes 
discovered corpus log linear interpolation performs linear interpolation cases slightly better differences perplexities negligible 
linear log linear interpolation models consistently outperformed baseline katz back model 
bigger wall street journal corpus log linear interpolation model 
discussion outperformed linear interpolation back language model terms perplexity calculated evaluation sets 
findings possible assume log linear smoothing gram language models performs linear interpolation small corpora outperforms bigger corpora interpolation models achieving performance superior unsmoothed back language model 
attempt systematically explore log linear smoothing grams remain directions need explored example interesting see division training data kept held sets ratio held kept set sizes affects performance log linear language model 
size held set small deterioration performance expected due fact data estimate interpolation weights kept set size small deterioration performance explained insufficient amount data obtaining probability estimates smoothed 
chapter summary novel smoothing technique gram language modelling log linear interpolation investigated 
log linear interpolation gram language models technique allows form exponential combination probability estimates obtained grams different specificity trained text corpus 
resulting combination guaranteed perform worse probability estimates comprising model provided gram probability estimates reliable form discounting obtain order ensure probabilities nonzero 
maximum likelihood estimates satisfy condition considered 
log linear interpolation introduced combining different language models 
technique interpolating conventional grams type language models called distance grams successfully applied broadcast news transcription task 
publications mentioned essentially propose general purpose method combining arbitrary language models aim investigate applicability technique special case smoothing gram probability estimates attempt rigorous derivation theoretical framework log linear interpolation placed possible smoothing method 
result framework obtaining log linear interpolation probability estimates including techniques parameter optimisation clustering proposed 
consequent experiments carried popular language modelling corpora log linear interpolation shown perform linear interpolation produced small corpus outperform linear interpolation big corpus models outperforming baseline back language model 
log linear interpolation language models shown promising technique successfully employed tasks smoothing language model probability estimates 
bibliography bahl brown de souza mercer 
treebased language model natural language speech recognition 
ieee transactions acoustics speech signal processing assp july 
bahl brown de souza mercer nahamoo 
fast algorithm deleted interpolation 
proceedings nd european conference speech communication technology volume pages genova september 
bahl jelinek mercer 
maximum likelihood approach continuous speech recognition 
waibel lee editors readings speech recognition chapter pages 
morgan kaufmann san mateo ca 
reprinted ieee transactions pattern analysis machine intelligence 
berger della pietra della pietra 
maximum entropy approach natural language processing 
computational linguistics march 
aubert harris 
philips rwth system transcription broadcast news 
proceedings th european conference speech communication technology volume pages budapest september 
black jelinek lafferty mercer roukos 
decision tree models applied labelling texts parts speech 
darpa speech natural language workshop pages 
box tao 
bayesian inference statistical analysis 
addison wesley reading ma 
section 
briscoe 
generalized probabilistic lr parsing natural language corpora unification grammars 
computational linguistics 
brown della pietra de souza lai mercer 
class gram models natural language 
computational linguistics 
brown della pietra mercer della pietra lai 
estimate upper bound entropy english 
computational linguistics march 
bibliography charniak 
statistical language learning 
language speech communication 
mit press cambridge ma london 
chelba jelinek 
exploiting syntactic structure language modeling 
proceedings association computational linguistics volume acl montreal 
chen goodman :10.1.1.131.5458
empirical study smoothing techniques language modeling 
proceedings association computational linguistics volume acl pages santa cruz ca june 
chen goodman :10.1.1.131.5458
empirical study smoothing techniques language modeling 
technical report tr computer science group harvard university august 
chen goodman 
empirical study smoothing techniques language modeling 
computer speech language october 
chen seymore rosenfeld 
topic adaptation language modeling unnormalized exponential models 
proceedings ieee international conference acoustics speech signal processing volume pages seattle washington may 
church gale 
comparison enhanced turing deleted estimation methods estimating probabilities english bigrams 
computer speech language january 
church hanks 
word association norms mutual information lexicography 
proceedings association computational linguistics volume acl pages 
cover thomas 
elements information theory 
wiley series telecommunications 
john wiley sons 
duda hart 
pattern classification scene analysis 
john wiley sons new york 
francis ku era 
frequency analysis english usage lexicon grammar 
houghton mifflin boston 
gale church 
estimation procedures language context poor estimates worse 
proceedings computational statistics pages yugoslavia september 
gale sampson 
turing frequency estimation tears 
journal quantitative linguistics 
gao chen 
probabilistic word classification contextsensitive binary tree method 
computer speech language october 
ney wessel 
extensions absolute discounting language modeling 
proceedings th european conference speech communication technology volume pages madrid september 
bibliography godfrey mcdaniel 
switchboard telephone speech corpus research development 
proceedings ieee international conference acoustics speech signal processing volume pages san francisco march 

population frequencies species estimation population parameters 
biometrika december 
goodman 
putting language model combination 
proceedings ieee international conference acoustics speech signal processing volume pages istanbul june 

class bigram model large corpus 
proceedings international conference spoken language processing volume pages yokohama september 
jeffreys 
theory probability 
oxford science publications 
clarendon press oxford rd edition 
jelinek 
markov source modelling text generation 
editor impact processing techniques communication 
nijhoff publishers dordrecht 
jelinek 
self organised language modeling speech recognition 
waibel lee editors readings speech recognition chapter pages 
morgan kaufmann san mateo ca 
jelinek 
statistical methods speech recognition 
language speech communication 
mit press cambridge ma london march 
jelinek mercer 
interpolated estimation markov source parameters sparse data 
proceedings workshop pattern recognition practice 
north holland amsterdam may 
jelinek mercer 
probability distribution estimation sparse data 
ibm technical disclosure bulletin 
kessler 
markov processes linguistics zipf law 
physical review letters may 
katz 
estimation probabilities sparse data language model component speech recognizer 
ieee transactions acoustics speech signal processing assp march 
kelley 
detection remediation stagnation nelder mead algorithm sufficient decrease condition 
society industrial applied mathematics journal optimization 

log linear interpolation language models 
proceedings th international conference spoken language processing volume pages sydney december 
kneser ney 
improved backing gram language modeling 
proceedings ieee international conference acoustics speech signal processing volume pages detroit may 
bibliography ku era francis 
computational analysis day american english 
brown university press providence ri 
lagarias reeds wright wright 
convergence properties nelder mead simplex method low dimensions 
society industrial applied mathematics journal optimization 
laplace 
philosophical essay probabilities volume sources history mathematics physical sciences 
springer verlag new york 
translated th french edition 
lehmann 
theory point estimation 
probability mathematical statistics 
john wiley sons new york 
li 
random texts exhibit zipf law word frequency distribution 
ieee transactions information theory 
li 
comments zipf law structures evolution natural languages 
complexity 
letters editor 
magerman 
natural language parsing statistical pattern recognition 
ph dissertation stanford university february 
section 
mandelbrot 
fractal geometry nature 
freeman new york 
manning sch tze 
foundations statistical natural language processing 
language speech communication 
mit press cambridge ma london august 
section 
martin wessel ney 
assessment smoothing methods complex stochastic language modeling 
proceedings th european conference speech communication technology volume pages budapest september 
martin ney 
smoothing methods maximum entropy language modeling 
proceedings ieee international conference acoustics speech signal processing volume pages phoenix march 
mcdonald 
internal external evidence identification semantic categorization proper names 
boguraev pustejovsky editors corpus processing lexical acquisition language speech communication pages 
mit press cambridge ma london 

convergence nelder mead simplex method nonstationary point 
society industrial applied mathematics journal optimization 
nadas 
turing formula word probabilities 
ieee transactions acoustics speech signal processing assp december 
bibliography ney essen 
smoothing techniques bigram natural language modelling 
proceedings ieee international conference acoustics speech signal processing volume pages toronto may 
ney essen 
estimating small probabilities leaving 
proceedings rd european conference speech communication technology volume pages berlin september 
ney essen kneser 
structuring probabilistic dependencies stochastic language modeling 
computer speech language january 
ney martin wessel 
statistical language modeling leaving 
young editors corpus methods language speech processing chapter pages 
kluwer academic publishers dordrecht 
niesler 
category statistical language models 
ph dissertation cambridge university june 
paul baker 
design wall street journal csr corpus 
proceedings international conference spoken language processing pages 
peto 
comparison smoothing methods word bigram models 
sc 
dissertation graduate department computer science university toronto october 
press flannery teukolsky vetterling 
numerical recipes art scientific computing 
cambridge university press 

natural law succession 
research report cs tr department computer science princeton university july 
rosenfeld 
adaptive statistical language modeling maximum entropy approach 
ph dissertation carnegie mellon university april 
published research report cmu cs 
appendix samuelsson 
relating turing formula zipf law 
research report claus computational linguistics department universit des saarlandes saarbr cken june 
shannon 
prediction entropy printed english 
bell system technical journal january 

distribution law word frequencies 
journal american statistical association september 

citations zipf mandelbrot law 
complex systems 
simmons yu 
acquisition context dependent grammars english 
computational linguistics december 
bibliography schultz 
zipf law structure evolution languages 
complexity 
wessel baader 
robust dialogue state dependent language modeling leaving 
proceedings ieee international conference acoustics speech signal processing volume ii pages phoenix march 
woodland gales valtchev 
htk large vocabulary recognition system arpa task 
proceedings arpa speech recognition workshop new york 
house 
woodland odell valtchev young 
htk large vocabulary speech recognition system 
proceedings ieee international conference acoustics speech signal processing volume pages detroit may 
zipf 
human behaviour principle effort 
addison wesley reading ma 
zipf 
psycho biology language dynamic 
mit press cambridge ma 
reprint houghton mifflin edition 
