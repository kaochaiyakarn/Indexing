weakly supervised learning methods improving quality gene name normalization data mitre burlington rd bedford ma pervasive problem facing biomedical text mining applications correctly associating mentions entities literature corresponding concepts database ontology 
attempts build systems automating process shown promise demonstrated task evaluation 
significant obstacle improved performance task lack high quality training data 
explore methods improving quality noisy task training data variants weakly supervised learning methods 
positive results demonstrating methods result improvement training data quality measured improved system performance system originally labeled data 
primary set tasks facing biomedical text processing systems categorizing identifying classifying entities literature 
key step process involves grouping mentions entities equivalence classes denote underlying entity 
biomedical domain fortunate structured data resources databases ontologies entries denoting equivalence ben wellner wellner mitre org computer science department brandeis university waltham ma classes 
biomedical text mining process involves associating mentions entities known existing unique identifiers entities databases ontologies process referred normalization 
ability required text processing systems associate descriptions concepts free text grounded organized system knowledge readily amenable machine processing 
task evaluation challenged number systems identify genes associated abstracts different organisms mouse fly yeast 
participants provided large set noisy training data smaller set higher quality development test data 
provided lexicon containing potential gene identifiers occur list known incomplete names synonyms refer 
prepare training data list unique gene identifiers associated full text article obtained appropriate model organism database 
list pruned correspond genes mentioned 
done searching gene list synonyms exact string matching 
process potential genes referred phrase appear synonym list 
additionally list may incomplete genes mentioned article curated mentions genes corresponding identifier gene list 
explores series methods attempting recover missing gene proceedings acl ismb workshop linking biological literature ontologies databases mining biological semantics pages detroit june 
association computational linguistics identifiers task training data abstracts 
start robust machine learning baseline system reimplementation system 
briefly system utilizes classifier select filter matches synonym list loose matching criterion 
baseline explore various methods relabeling noisy training data resulting improved scores task development test evaluation data 
methods weakly supervised learning techniques cotraining self training learning labeled unlabeled data :10.1.1.14.4388
setting different typical setting weakly supervised learning large amount labeled data opposed completely unlabeled data 
main contribution framework applying weakly supervised methods problem re labeling noisy training data 
approach partitioning training data sets viewing problem mutually supporting weakly supervised learning problems 
experimental results demonstrate methods carefully tuned improve performance gene name normalization task previously reported machine learning techniques 
background related gene name normalization extraction task normalizing identifying biological entities genes particular received considerable attention biological text mining community 
task challenged systems identify unique gene identifiers associated abstracts literature organisms mouse fly yeast 
task workshop focused identifying tagging mentions genes biomedical journal abstracts 
nlp noisy un labeled training data biomedical text processing number approaches identification normalization entities attempted available structured biological resources bootstrap systems deriving noisy training data task hand 
novel method noisy weakly labeled training data biological databases learn identify relations biomedical texts 
noisy training data created identify gene name mentions text 
similarly employed essentially approach database identify normalized genes articles 
weakly supervised learning weakly supervised learning remains active area research machine learning 
methods appealing offer way learning system provided small amount labeled training data large amount un labeled data perform better labeled data 
certain situations see improvement substantial 
situations small amounts labeled data large amounts unlabeled data common real world applications labeling large quantities data prohibitively expensive 
weakly supervised learning approaches broken multi view single view methods 
multi view methods incrementally label unlabeled data follows 
classifiers trained training data different views data 
different views realized splitting set features way features classifier conditionally independent features class label 
classifier selects confidently classified instances unlabeled data random subset thereof adds training set 
process repeated data labeled stopping criterion met 
intuition approach classifiers different views data new training instance classified high confidence classifier redundant classifier point view serve informative novel new training instance classifier viceversa 
single view methods avoid problem finding appropriate feature split possible appropriate domains 
common approach involves learning ensemble classifiers bagging 
bagging training data randomly sampled replacement separate classifier trained sample 
un labeled instances labeled separate classifiers agree label instance 
approaches expectation maximization algorithm em 
system description baseline version system essentially reproduction system described modifications 
great appeal system machine learning organism specific aspects hard coded moving new organism involves re training assuming training data setting parameters held data set crossvalidation 
system set abstracts associated gene identifiers training time lexicon 
system proposes candidate phrases possible phrases words length constraints part ofspeech matches lexicon carried performing exact matching ignoring case removing punctuation lexical entries candidate mentions 
maximal matching strings sub strings matching strings match id removed 
resulting set matches candidate mentions matched identifiers results set instances 
instances provided label depending match correct gene identifier associated match annotated 
instances train binary maximum entropy classifier ultimately decides match valid 
maximum entropy classifiers model conditional probability class setting observed data conditional probability form binary case equivalent logistic regression specifically excluded phrases began verbs prepositions adverbs determiners constraint affect recall reducing number candidate mentions 
exp fi normalization function real valued model parameters arbitrary realvalued feature functions 
advantage maximum entropy classifiers freedom large numbers statistically non independent features 
number different feature types classifier matching phrase matched gene identifier previous subsequent words phrase number words matching phrase total number genes matched phrase character prefixes suffixes length words phrase example shown 
excerpt new receptor tor thymus orphan receptor feature class specific feature phrase tor mgi previous previous receptor subsequent subsequent thymus number matches number words prefix prefix prefix tor suffix suffix suffix tor 
excerpt matching phrase tor 
resulting features match detailed table 
addition features created additional features constituting conjunctions atomic features 
example conjoined feature phrase tor ge mgi conjuncts true instance 
assign identifiers new set features extracted matching phrase gene id pair just training constitutes instance classifier classification 
classifier returns probability instance gene id associated instance highest probability returned gene id associated case probability threshold case gene id returned phrase 
training model involves finding parameters maximize log likelihood training data 
standard maximum entropy models employ gaussian prior parameters bias zero reduce overfitting 
model just parameters need tuned different datasets different organisms gaussian prior threshold tuning parameters done held set task development data cross validation weakly supervised methods relabeling noisy normalization data primary contribution novel method re labeling noisy training instances task training data sets 
recall task training data constructed matching phrases synonym lists gene ids curated full text article written 
cases mentions gene appear exactly synonym list result missed association gene id 
cases database curators simply gene id mentioned relevant particular line interest 
method re labeling potentially mislabeled instances draws existing methods weakly supervised learning 
describe generic algorithm include specific variations experimental setup 
step partition training data disjoint sets 
create instances weakly supervised learning note instances derived form disjoint sets abstracts 
helps ensure similar instances appear different partitions 
problem instance viewed labeled training data viewed unlabeled data instance roles reversed 
re labeling instances carried classifier ensemble classifiers trained 
similarly instances relabeled trained 
instances classifier assigns high confidence high existing label disagrees classifier candidates re labeling 
diagrams process 
original training data re labeling classifiers modified training data 
diagram illustrating method relabeling instances 
solid arrows indicate training classifier set data block arrows describe data flow re labeling instances 
assumption approach errors training data labels correlated 
expect particular mislabeled instance may similar positive instances provide evidence re labeling mislabeled 
initial experiments approach met failure negligible gains performance 
initially attributed correlated errors 
detailed error analysis revealed significant portion training instances re labeled derived matches lexicon fact genes common english words happened appear synonym lists classifier mistakenly assigned high probability 
final classifier solution problem impose constraint instances re labeled phrase associated instance required tagged gene name gene name tagger addition instance receiving high probability re labeling classifier 
gene name tagger introduces check classifier trained noisy training data helps reduce chance introducing false positives labeled data 
trained entity tagger genia corpus task gene name training corpus 
entity types annotated genia corpus genes 
appropriate subset entity types corpus 
conditional random fields crfs task employed similar set features crf described 
experiments results main goal experiments demonstrate benefits re labeling potentially noisy training instances task training data 
focus weakly supervised relabeling experiments mouse data set 
mouse data strong bias false negatives training data training instances negative label positive 
reasons focusing data twofold believe situation common practice organism may impoverished synonym lists gaps curated databases experiments resulting analyses clearer focusing re labeling instances direction negative positive 
section describe initial experiment comparing baseline system described original training data version trained augmented data set labels changed simple heuristic 
describe main body experiments various weakly supervised learning methods relabeling data 
report scores evaluation data organisms best system configurations derived development test data 
data methodology task data experiments 
data sets abstracts training data abstracts development test data mouse fly yeast respectively 
final evaluation data consisted abstracts organism 
training data ratios positive negative instances mouse fly yeast 
number features trained model range mouse fly yeast 
classifier able rank test instances case ranks derive probabilities output maximum entropy classifier return top gene identifiers number correct identifiers development test data results balanced measure score 
metric experiments development test data allows better comparison systems factoring need tune threshold 
evaluation data know system returns number identifiers threshold experiments set development test data choose appropriate values different evaluation submissions 
experiment set effect match re labeling set experiments uses baseline system described earlier 
compare results system task training data provided results obtained re labeling negative instances provided classifier positive instances 
re labeled instances positive matched gene identifier associated regardless potentially incorrect label associated identifier 
task dataset creators marked identifier exact lexicon match wasn 
system matching phase bit different remove punctuation ignore case amounts re labeling training data looser criterion 
results match re labeling shown table 
baseline re labeled mouse fly yeast table balanced measure scores comparing baseline vs system trained match re labeled instances development test data 
experiment set effect weakly supervised re labeling set experiments tested number different weakly supervised learning configurations 
different methods simply amount different rankings instances re label confidence gene name tags 
basic algorithm outlined remains cases 
specifically investigated methods ranking instances re label na self training self training bagging training 
na self training consisted training single maximum entropy classifier full feature set partition re label instances partition confidence 
self training bagging followed idea bagging 
partition trained separate classifiers random subsets training data full feature set 
confidence assigned test instance defined product confidences individual classifiers 
training involved training classifiers partition feature split 
split features context features surrounding words number gene ids matching current phrase lexically features included phrase affixes number tokens phrase computed aggregated confidences instance product confidences assigned resulting context lexically classifiers 
ran experiments options gene tagger gene tagger 
systems included gene tagger ranked instances derived tagged phrases instances derived phrases tagged regardless classifier confidence 
final experimental condition explored comparing batch re labeling vs incremental relabeling 
batch re labeling involved training classifiers re labeling instances classifier 
incremental re labeling consisted iteratively re labeling instances epochs classifiers re trained epoch newly re labeled training data 
interestingly incremental re labeling perform better batch re labeling experiments 
results reported batch re labeling 
training data re labeled single maximum entropy classifier trained entire re labeled training set 
resulting classifier applied development set manner described section 
max tagger tagger self na self bagging training avg tagger tagger self na self bagging training table 
maximum average balanced measure scores mouse data set system configurations values number instances re labeled 
numbers parentheses indicate value maximum value achieved 
tested configurations different values total number instances re labeled table highlights maximum average balanced measure scores values different system configurations 
maximum averaged scores appear noticeably higher constraining instances re label tagger 
weakly supervised methods perform comparably bagging performing slightly better 
values considered 

top graph shows balanced measure scores number instances re labeled tagger constraint 
bottom graph compares re labeling instances gene tagger constraint 
order gain insight re labeling instances plotted balanced measure performance development test various values upper graph indicates different methods correlate strongly 
bottom graph apparent benefits tagging constraint 
points weakness tagger 
system tends perform worse tags constraint 
indicates tagger recall errors potential filter candidates re labeling 
observation graphs performance drops small values imply instances classifiers confident re labeling fact spurious 
support hypothesis trained baseline system entire training set computed calibration error development test data 
calibration error measures realistic probabilities output classifier 
see details 

classifier calibration error development test data 
illustrates estimated calibration error different thresholds 
seen error greatest high confidence values indicating classifier confidently predicting instance positive negative 
extrapolating calibration error re classifiers trained half training data offers explanation re labeling starts poorly 
error mass exactly want highest confidence values 
offers explanation incremental re labeling help 
fortunately introducing gene tagger constraint mitigates problem 
experiment set final evaluation report results best system configurations task evaluation data 
submitted runs different mouse configurations fly yeast 
highest scores runs reported table 
best weakly supervised method determined development test data bagging 
match re labeling described section 
gaussian prior set runs submissions configuration varied threshold value measure precision recall table 
final evaluation results 
results competitive compared task results highest measures mouse fly yeast medians respectively 
results mouse fly improve previous best reported results organism invariant automatic system 
quality training data paramount success fully automatic organism invariant approaches normalization problem 
demonstrated utility weakly supervised learning methods conjunction gene name tagger re labeling noisy training data gene name normalization 
result higher quality data corresponding higher performance task gene name normalization task 
includes applying method outlined correcting noisy data classification problems 
doing generally requires independent filter restrict re labeling equivalent gene tagger 
plans improve classifier calibration 
integrating confidence estimates produced gene name tagger avenue investigation 
alex morgan lynette hirschman marc jose castano james pustejovsky helpful comments encouragement 
supported mitre sponsored research msr 

crim mcdonald pereira 
automatically annotating documents normalized gene lists 
workshop critical assessment text mining methods molecular biology 

granada spain 

blum mitchell 
combining labeled unlabeled data training 

proceedings workshop computational learning theory morgan kaufmann 

banko brill 
scaling large corpora natural language disambiguation 
acl eacl 


ng cardie 
weakly supervised natural language learning redundant views 
human language technology conference north american chapter association computational linguistics hlt naacl 


hirschman overview task normalized gene lists 
biomed central bioinformatics special issue 

craven constructing biological knowledge bases extracting information text sources 


morgan gene name extraction resources 
acl workshop natural language processing biomedicine 

morgan gene name identification normalization model organism database 
biomed inform 


nigam ghani 
analyzing effectiveness applicability training 
information knowledge management 


kim genia corpus semantically annotated corpus bio text mining 
bioinformatics 


lafferty mccallum pereira 
conditional random fields probabilistic models segmenting labeling sequence data 
th international conf 
machine learning 

san francisco ca morgan kaufmann 

mcdonald pereira 
identifying gene protein mentions text conditional random fields 
critical assessment text mining methods molecular biology 

spain 

cohen goldszmidt 
properties benefits calibrated classifiers 
pkdd 

pisa italy 

mccallum 
confidence estimation information extraction 
proceedings human language technology conference north american chapter association computational linguistics hlt naacl 

boston ma 
