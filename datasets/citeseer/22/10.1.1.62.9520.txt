learning gaussian process kernels hierarchical bayes anton fraunhofer intelligent data analysis ida berlin anton fhg de volker tresp kai yu siemens corporate technology information communications munich germany volker tresp kai yu siemens com novel method learning gaussian process regression hierarchical bayesian framework 
step kernel matrices fixed set input points learned data simple efficient em algorithm 
step nonparametric require parametric form covariance function 
second step kernel functions fitted approximate learned covariance matrix generalized nystr method results complex data driven kernel 
evaluate approach recommendation engine art images proposed hierarchical bayesian method leads excellent prediction performance 
real world application domains available training data sets quite small learning model selection difficult 
example user preference modelling problem consider learning preference model amount fitting model samples user preference data 
fortunately situations individual data sets small data similar scenarios obtained 
returning example preference modelling data different users typically available 
data stems clearly separate individuals expect models borrow strength data users similar tastes 
typically problems handled mixed effects models hierarchical bayesian modelling 
novel approach hierarchical bayesian modelling context gaussian process regression application recommender systems 
hierarchical bayesian modelling essentially means learn mean covariance function gaussian process 
step common collaborative kernel matrix learned data simple efficient em algorithm 
circumvents problem kernel design parametric form kernel function required 
form learning covariance matrix suited problems complex covariance structure nonstationarity 
portion learned covariance matrix explained input features generalized new objects content kernel smoother 
second step generalize covariance matrix learned em algorithm new items generalized nystr method 
result complex content kernel weighted superposition simple smoothing kernels 
second part applied situations needs extrapolate covariance matrix finite set graph continuous input space example required induction semi supervised learning 
organized follows 
sec 
casts gaussian process regression hierarchical bayesian framework shows em updates learn covariance matrix step 
extrapolating covariance matrix shown sec 

illustrate function em learning toy example sec 
applying proposed methods recommender system images sec 

previous statistics modelling data related scenarios typically done mixed effects models hierarchical bayesian hb modelling 
hb parameters models individual scenarios users recommender systems assumed drawn common hyper prior distribution allowing individual models interact regularize 
examples hb modelling machine learning include 
contexts learning framework called multi task learning 
multi task learning gaussian processes suggested stringent assumption observations set points individual scenario 
sparse approximations gps general gp multi task learner parametric covariance functions 
contrast approach considers covariance matrices non parametric step 
second extrapolation step kernel smoothing leads predictions covariance function data driven combination simple kernel functions 
learning gp kernel matrices em learning task concerned stated follows data observations different scenarios 
th scenario observations 
total points 

order analyze data hierarchical bayesian way assume data scenario noisy sample gaussian process gp unknown mean covariance function 
assume mean covariance function shared different scenarios 
modelling step section consider transductive learning labelling partially labelled data set interested model behavior points xi cardinality 
situation relevant collaborative filtering applications 
test points unlabelled points scenario 
reduces infinite dimensional gaussian process finite dimensional projection points variate gaussian distribution covariance matrix mean vector em algorithm require overlap scenarios xi xj coming back user modelling problem mentioned means items rated user 
modelling step focusses directly learning covariance matrix alternative hb approaches collaborative filtering discussed assume model weights drawn shared gaussian distribution 
data efficient em algorithm 
may particular help problems need specify complex nonstationary covariance function 
hierarchical bayesian assumption data observed scenario partial sample denotes unit matrix 
joint model simply denotes prior distribution mean covariance 
assume gaussian likelihood diagonal covariance matrix 
em learning hierarchical bayesian model eq 
marginal likelihood df 
obtain simple stable solutions estimating data consider point estimates parameters penalized likelihood approach conjugate priors 
conjugate prior mean covariance multivariate gaussian called normal wishart distribution decomposes product inverse wishart distribution normal distribution wi 
prior gram matrix inverse wishart distribution scalar parameter symmetric positive definite matrix 
covariance matrix gaussian distributed mean covariance positive scalar 
parameters interpreted terms equivalent data set mean data set size mean data set covariance size covariance order write em algorithm compact way denote set indices data points observed th scenario 
xj 
keep mind applications interest targets missing training 
denotes square submatrix corresponds points covariance matrix points th scenario 
denote covariance matrix points versus th scenario 
step step computes expected value functional values points scenario expected value standard equations predictive mean gaussian process models covariance functions replaced corresponding sub matrices current estimate 

covariances pairs points estimated predictive covariance gp models denotes matrix transpose ki 

efficient em solution case 
step step vector mean values covariance matrix noise variance updated 
denoting updated quantities get bs trace intuitive explanation step follows new mean weighted combination prior mean weighted equivalent sample size predictive mean 
covariance update sum terms 
term typically irrelevant result coupling gaussian inverse wishart prior distributions second term contains prior covariance matrix weighted equivalent sample size 
third term get empirical covariance estimated measured functional values fourth term gives correction term compensate fact functional values estimates empirical covariance small 
learning covariance function generalized nystr em algorithm described sec 
easily efficiently learn covariance matrix mean vector data obtained different related scenarios 
predictions set easily appealing equations em algorithm eq 
predictive mean eq 
covariance 
example interest collaborative filtering application fixed set items 
section describe covariance generalized new inputs note em algorithm content features xi contribute 
order generalize learned covariance matrix employ kernel smoother auxiliary kernel function takes pair content features input 
constraint need guarantee derived kernel positive definite straightforward interpolation schemes readily applied 
strategy interpolate eigenvectors subsequently derive positive definite kernel 
approach related nystr method primarily method extrapolating eigenfunctions known discrete set points 
contrast nystr extrapolating smoothing kernel known setting employ generic smoothing kernel 
eigendecomposition covariance matrix diagonal matrix eigenvalues orthonormal eigenvectors columns scaled eigenvectors 
approximate th scaled eigenvector vi gaussian process covariance function obtain approximation scaled eigenfunction xj bi weights bi bi 
bi vi 
denotes gram matrix smoothing kernel points 
additional regularization term introduced stabilize inverse 
approximate scaled eigenfunctions resulting kernel function simply 

xn 
resp 
gram matrices training data points kernel function resp 
tuning parameter determines proportion explained content kernel 
reproduced means explained content kernel 
portion explained content kernel 
note eigenvectors required derivation need calculated evaluating kernel 
similarly build kernel smoother extrapolate mean vector approximate mean function 
prediction new object scenario xj weights 
important note richer structure auxiliary kernel expanding expression see amounts data dependent covariance function written superposition kernels input dependent weights rw 
experiments xi illustrate process covariance matrix learning small toy example data generated sampling gaussian process nonstationary neural network covariance function 
independent gaussian noise variance added 
input points randomly placed points interval 
consider scenarios scenario observations random subset 
fig 
scenario corresponds noisy line points 
em covariance matrix learning sec 
data nonstationarity data longer pose problems fig 
illustrates 
stationary covariance matrix shown fig 
initial value prior covariance eq 

learned covariance matrix fig 
fully match true covariance clearly captures nonstationary effects 
recommendation engine testbed proposed methods consider information filtering task 
goal predict individual users preferences large collection art images note true interpolating kernel known obtain approximate kernel obtained nystr honolulu dbs informatik uni muenchen de paintings index jsp training data true covariance matrix initial covariance matrix covariance matrix learned em example illustrate covariance matrix learning em 
data shown drawn gaussian process nonstationary neural network covariance function 
initialized stationary matrix shown em learning resulted covariance matrix shown 
comparing learned matrix true matrix shows nonstationary structure captured user rated random subset total paintings ratings dislike sure 
total ratings users collected user rated paintings average 
image described dimensional feature vector containing correlogram color moments wavelet texture 
fig 
shows roc curves collaborative filtering preferences unrated items set images predicted 
transductive approach eq 
gp em covariance compared collaborative approach pearson correlation collaborative filtering alternative nonparametric hierarchical bayesian approach hybrid filter 
algorithms evaluated fold cross validation scheme repeated times assume ratings items known test user 
known ratings predictions unrated items 
obtain roc curve computing sensitivity specificity proportion truly liked paintings top ranked paintings averaged shows approach considerably better collaborative filtering pearson correlation gains small advantage hybrid filtering technique 
note em algorithm converged quickly requiring em steps learn covariance matrix performance insensitive respect hyperparameters choice equivalent sample sizes fig 
shows roc curves inductive setting predictions items outside set learning standard parametric gpr model preference data randomly chosen user setting kernel parameters marginal likelihood model generate full covariance matrix points 
transductive methods inductive methods roc curves different methods predicting user preferences art images training set referred new item problem 
shown performance obtained generalized nystr method eq 
gp generalized nystr predicting user preferences image features svm squared exponential kernel svm content filtering 
apparent new approach learned kernel superior standard svm approach 
performance inductive approach quite limited 
low level content features poor indicators high level concept liking art image inductive approaches general need rely content dependent collaborative filtering 
purely content independent collaborative effect exploited transductive setting generalized new items 
purely content independent collaborative effect viewed correlated noise model 
summary article introduced novel method learning gaussian process covariance functions multi task learning problems hierarchical bayesian framework 
hierarchical framework gp models individual scenarios borrow strength common prior mean covariance 
learning task solved steps em algorithm learn shared mean vector covariance matrix fixed set points 
second step learned covariance matrix generalized new points generalized form nystr method 
initial experiments method recommender system art images showed promising results 
approach clear distinction content dependent content independent collaborative filtering 
expect approach effective applications content features powerful recommender systems textual items news articles allow better prediction user preferences 
supported part ist programme european union pascal network excellence eu 
obtain kernel fitted gp user preference models randomly chosen users individual ard weights input dimension squared exponential kernel 
ard weights taken medians fitted ard weights 
bakker heskes task clustering gating bayesian multitask learning 
journal machine learning research 
blei ng jordan latent dirichlet allocation 
journal machine learning research 
breese heckerman kadie empirical analysis predictive algorithms collaborative filtering 
tech 
rep msr tr microsoft 
caruana multitask learning 
machine learning 
chapelle machine learning approach analysis 
saul weiss bottou eds neural information processing systems 
mit press 
gelman carlin stern rubin bayesian data analysis 
texts statistical science 
chapman hall 
crc press reprint 
lawrence platt learning learn informative vector machine 
greiner schuurmans eds proceedings icml 
morgan kaufmann 
minka picard learning learn learning point sets 
unpublished manuscript 
revised 
schafer analysis incomplete multivariate data 
chapman hall 
williams computation infinite neural networks 
neural computation 
williams seeger nystr method speed kernel machines 
leen dietterich tresp eds advances neural information processing systems pp 

mit press 
yu tresp ma zhang collaborative ensemble learning combining collaborative content information filtering hierarchical bayes 
meek kj eds proceedings uai pp 

morgan kaufmann 
zhu ghahramani lafferty semi supervised learning gaussian fields harmonic functions 
proceedings icml 
morgan kaufmann 
appendix derive em algorithm eq 
treat functional values scenario unknown variables 
em iteration parameters estimated 
step sufficient statistics computed defined eq 

step parameters re estimated arg max lp lp stands penalized log likelihood complete data lp log wi log log log updated parameters obtained setting partial derivatives zero 

