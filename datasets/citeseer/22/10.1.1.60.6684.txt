data mining metric space empirical analysis supervised learning performance criteria rich caruana computer science cornell university caruana cs cornell edu criteria evaluate performance supervised learning 
different criteria appropriate different settings clear criteria 
complication learning methods perform criterion may perform criteria 
example svms boosting designed optimize accuracy neural nets typically optimize squared error cross entropy 
conducted empirical study variety learning methods svms neural nets nearest neighbor bagged boosted trees boosted stumps compare boolean classification performance metrics accuracy lift score area roc curve average precision precision recall break point squared error cross entropy probability calibration 
multidimensional scaling mds shows metrics span low dimensional manifold 
metrics appropriate predictions interpreted probabilities squared error cross entropy calibration lay part metric space far away metrics depend relative order predicted values roc area average precision break point lift 
fall metrics depend comparing predictions threshold accuracy score 
expected maximum margin methods svms boosted trees excellent performance metrics accuracy perform poorly probability metrics squared error 
expected margin methods excellent performance ordering metrics roc area average precision 
introduce new metric sar combines squared error accuracy roc area metric 
mds correlation analysis shows sar centrally located correlates metrics suggesting general purpose metric specific criteria known 
categories subject descriptors pattern recognition design methodology classifier design evaluation 
general terms algorithms measurement performance experimentation 
permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
kdd august seattle washington usa 
copyright acm 
alexandru niculescu computer science cornell university cs cornell edu keywords supervised learning performance evaluation metrics roc precision recall lift cross entropy 

supervised learning finding model predict true underlying probability test case optimal 
refer ideal model true model 
reasonable performance metric optimized expectation true model model yield performance better 
unfortunately usually know train models predict true underlying probabilities 
true model easy learn 
correct parametric model type domain known training sample small model parameters estimated accurately noise data 
typically problems occur varying degrees 
magically true model difficulty selecting true models 
performance metrics reliably assign best performance probabilistically true model finite validation data 
practice train models minimize loss measured specific performance metric 
don metrics reliably select true model accept fact model select necessarily suboptimal 
may true model suboptimal models 
different ways suboptimal models differ true model tradeoffs different kinds deviation true model 
different performance metrics reflect different tradeoffs 
example ordering metrics area roc curve average precision care predicted values near true probabilities depend relative size values 
dividing predictions change roc curve metrics roc curve insensitive kind deviation truth 
metrics squared error cross entropy greatly affected scaling predicted values affected small changes predicted values alter relative ordering significantly change deviation target values 
squared error cross entropy reflect different tradeoffs metrics roc curve 
similarly metrics accuracy depend predicted values fall relative threshold 
predicted values rescaled accuracy unaffected threshold rescaled 
small changes max acc max auc min rms min mxe min cal max sar level curves error metrics acc auc rms mxe cal sar simple problem 
predicted values cases near threshold large impact accuracy 
accuracy reflects tradeoff deviation truth measured 
true model available expectation best accuracy best roc curve best cross entropy different tradeoffs metrics important 
accept able find true model accept suboptimal models different tradeoffs different performance metrics interesting important 
unfortunately little known different performance metrics compare 
results empirical analysis widely performance metrics 
perform empirical comparison models trained learning algorithms svms neural nets nearest neighbor bagged boosted trees boosted stumps 
multidimensional scaling mds correlation analysis interpret results 
examine learning methods perform best different metrics 
introduce new metric sar combines squared error accuracy roc area single robust metric 

performance metrics experiment performance metrics boolean classification accuracy acc lift lft score fsc area roc curve auc average precision apr precision recall break point bep root mean squared error rms mean cross entropy mxe probability calibration cal 
definitions metrics appendix shows level curves performance metrics model parameters trained simple synthetic binary problem 
peak performance plots occurs ridge weight space 
plots peak performance indicated solid dots 
peak performance metrics nearly coincide rms mxe peak nearly model weights 
metrics peak different places cal local optimum near optima rms mxe global optimum different place 
ridges optimal acc optimal auc align ridges cross optima metrics 
optimizing metrics yields different models representing different tradeoffs kinds errors models 
tradeoffs best depends problem learning algorithm model predictions ultimately 
originally divided metrics groups threshold metrics ordering rank metrics probability metrics 
threshold metrics accuracy acc score fsc lift lft 
score harmonic mean precision recall threshold 
lift measures true positive rate fraction cases fall threshold 
see appendix definition lift description lift curves 
lift precision threshold scaled larger 
usually acc fsc fixed threshold 

lift threshold adjusted fixed percent cases predicted positive rest falling threshold 
usually depends problem 
example marketing want send customers 
somewhat arbitrarily set problems 
note threshold metrics important close prediction threshold predicted value threshold 
ordering rank metrics look predictions differently threshold metrics 
cases ordered predicted value ordering rank metrics measure ordering ranks positive cases negative cases 
rank metrics viewed summary performance model possible thresholds 
rank metrics area roc curve auc average precision apr precision recall break point bep 
see discussion roc curves machine learning perspective 
rank metrics depend ordering predictions actual predicted values 
ordering preserved difference predicted values range 
group lift threshold metrics bep ordering metrics bep lift similar respects 
lift directly proportional bep lift calculated equal proportion positives data set 
threshold break point precision equals recall 
bep lift similar ordering metrics threshold depends implicitly ordering similar threshold metrics sensitive orderings side threshold threshold defined 
results suggest lift bep similar ordering metrics threshold metrics 
probability metrics depend predicted values values fall relative threshold relative 
probability metrics uniquely minimized expectation predicted value case coincides true probability case positive 
probability metrics consider squared error rms cross entropy mxe calibration cal 
cal measures calibration model model predicts large number cases cases prove positive model calibrated 
see appendix details cal calculated 
experiment new performance metric sar combines squared error accuracy roc area measure sar acc auc rms 
sar behaves somewhat differently acc auc rms robust metric correct metric unknown 
sar discussed section 
normalizing scores performance metrics accuracy squared error range lift cross entropy range depends data set 
metrics lower values indicate better performance 
higher values better 
metrics roc area baseline rates independent data accuracy baseline rates depend data 
baseline accuracy accuracy probably performance problem bayes optimal rate achieving accuracy excellent performance 
order compare performance metrics meaningful way metrics need placed similar scale 
way scale performances problem metric poor performance performance 
example place baseline performance bayes optimal performance 
unfortunately estimate bayes optimal rate real problems 
performance best observed model proxy bayes optimal performance 
calculate baseline rate follows predict case percent positives test set 
normalize performances range baseline represents best performance 
model performs worse baseline normalized score negative 
see table example normalized scores 
disadvantage normalized scores recovering raw performances requires knowing performances define top bottom scale new best models top scale changes 
cal metric measure probability calibration unusual baseline model predicts cases percent positives test set excellent calibration 
measures cal typically conjunction measures auc insure models discrimination calibration selected 
see picture unusual cal error surface compared metrics 
creates problem normalizing cal scores baseline model bayes optimal model similar cal scores 
mean cal poor metric effective distinguishing poorly calibrated models calibrated models 
address problem 

experimental design goal analyze metrics compare 
train different kinds models test problems calculate test problem performance model metrics 
train models learning algorithms neural nets ann svms bagged decision trees bag dt boosted decision trees bst dt boosted decision stumps table accuracy adult problem model acc norm score bst stmp bag dt dt svm bst dt ann knn baseline bst stmp single decision trees dt memory learning knn 
algorithm train variants parameter settings 
example train styles decision trees neural nets different sizes svms different kernels total models trained tested problem 
see appendix description parameter settings learning method 
strategy won create possible model won create uniform sample space possible models feel adequate sample models trained practice 
problem models trained train set points 
performance model measured large test set performance metrics 
order put performances scale different metrics different problems transform raw performance normalized scores explained section 
total problems models model score performances metrics 

data sets compare algorithms binary classification problems 
adult cover type letter uci repository 
adult problem nominal attributes 
anns svms transform nominal attributes boolean 
dt bag dt bst dt bst stmp model trained twice transformed attributes original attributes 
cover type converted binary problem treating largest class positive rest negative 
converted letter boolean ways 
letter treats letter positive remaining letters negative yielding unbalanced binary problem 
letter uses letters positives rest negatives yielding balanced problem 
hyper spect data set difficult class soybean positive class 
slac problem collaborators stanford linear accelerator medis medical data set 
characteristics data sets summarized table 
table description problems problem attr train size test size pos 
adult cover type letter letter medis slac hyper spect 
mds metric space training models problem learning algorithms gives models evaluated performance metrics 
gives sample points compare performance metric 
build table lines represent performance metrics columns represent models entry table score model metric 
mds treat row table coordinate point dimension space 
distance metrics calculated euclidean distance corresponding points space 
coordinates strongly correlated curse dimensionality problem euclidean distance dimensional space 
interested metrics compare models performance models poor performance 
delete columns representing poorer performing models order focus interesting part space models performance lie 
analyses reported delete models perform baseline metric cal 
metrics permits pairwise comparisons 
calculate euclidean distance pair metrics sample space perform multidimensional scaling pairwise distances metrics 
mds sensitive performance metrics scaled 
normalized scores described section yield performances suitable mds analysis metrics 
unfortunately discussed section normalized scores cal perform mds ways 
normalized scores exclude cal metric 
second include cal scale performances mean standard deviation normalized scores 
scaling standard deviation resolves problem cal mds somewhat intuitive scores scaled standard deviation depend full distribution models just performances fall top bottom scale 
shows mds stress function number dimensions mds cal included 
metrics appear span mds space dimensions 
section examine mds plots detail 
shows mds plots metrics result dimensionality reduced dimensions 
plot left mds normalized scores cal excluded 
plot right mds standard deviation scaled scores cal included 
mds plots show similar pattern 
metrics appear form somewhat distinct groups 
upper right hand corner group includes auc apr bep lft sar 
groups rms mxe acc possibly fsc fsc possibly acc cal 
surprising squared error cross entropy form cluster 
presumably squared error tends better behaved cross entropy rms closer measures mxe 
somewhat surprised rms centrally located mds plots 
partially explains squared error proved useful applications 
mds stress number mds dimensions mds stress vs number dimensions somewhat surprising accuracy appear correlate strongly metrics possibly fsc 
acc fall close metrics thresholds lift score score uses threshold accuracy experiments 
threshold lift adjusted dynamically cases predicted positive 
accuracy surprisingly close rms closer rms mxe suggesting part reason rms useful close relationship metric acc widely 
surprising pattern mds plot includes cal cal distant metrics 
appears axis running cal ordering metrics auc apr forms largest dimension space 
surprising way achieve excellent ordering accurately predict true probabilities measured calibration metric 
achieve excellent auc apr predicted values extremely poor calibration accurately predict relative ordering cases 
mds plot suggests models achieve excellent ordering achieving probabilistic calibration 
closer examination shows models boosted decision trees yield remarkably ordering extremely poor calibration 
believe maximum margin methods boosting tradeoff reduced calibration better margin 
see section discussion issue 
achieve calibration poor auc apr example decision trees leaves may calibrated coarse set values predict provide basis ordering 
shows mds plots test problems 
seventh plot similar omitted save space 
omitted plot letter problems 
variations plots mds plots problems remarkably consistent different test problems 
consistency mds plots suggests adequate sample size models reliably detect relationships metrics 
metrics acc fsc lft move respect plots 
may different acc fsc rms mxe apr bep sar lft auc cal fsc acc rms mxe bep sar apr auc mds plot normalized scores left standard deviation scaling right 
ratio positives negatives data sets 
example bep proprtional lft behaves similarly percentage positives dataset equals fraction predicted threshold 
able correlate differences see individual plots characteristics problems explain differences currently believe mds plots combine problems represents accurate summary relationships metrics 
note mean performance different learning algorithms exhibits pattern test problems fact different relationships metrics appear similar test problems learning algorithms considered time 

correlation analysis mds analysis previous section performance metrics measure performance models trained different learning methods test problems 
section correlation analysis models compare metrics mds 
correlation analysis easier interpret scale performances range best performance observed metric problem learning methods performance baseline performance metric data set performance 
eliminates inverse correlation measures accuracy squared error normalizes scale metric 
metrics permits pairwise correlations 
comparisons linear correlation excluding cal rank correlation 
results linear rank correlation analyses qualitatively sim ilar 
results non parametric rank correlation rank correlation fewer assumptions relationships metrics rank correlation insensitive cal scaled 
table shows rank correlation pairs metrics 
entry table average rank correlation test problems 
table symmetric contains unique pairwise comparisons 
full matrix easier scan comparisons 
final column mean rank correlations metric 
gives rough idea correlated metric average metrics 
metrics pairwise rank correlations near behave similarly smaller rank correlations 
ignoring sar metric discussed section metric pairs rank correlations lift roc area roc area average precision accuracy break point rms cross entropy break point roc area break point average precision average precision lift expected auc average precision behave similarly high rank correlation 
surprised see lift high correlation auc 
note lift high correlation auc auc high correlation average precision surprising lift high correlation average precision 
expected break point highly correlated ordering metrics auc average precision 
high correlation accuracy break lft cal fsc rms acc cover type mxe sar bep cal fsc rms mxe acc bep sar apr lft auc fsc acc cal letter rms mxe bep apr auc medis sar apr lft lft auc cal fsc acc rms adult mxe sar apr lft bep auc cal fsc acc rms hyper spect sar mxe bep lft apr auc cal mxe rms slac acc sar auc lft bep apr mds plots test problems 
seventh problem yields similar plot omitted save space 
missing plot letter problems 
table average rank correlations metrics acc fsc lft auc apr bep rms mxe cal sar mean acc fsc lft auc apr bep rms mxe cal sar point somewhat surprising currently know explain 
weakest correlations calibration metric cal metrics 
average cal correlates metrics 
surprised low correlation probability calibration metrics currently looking measures calibration see true 
cal fsc bep lft acc sar rms auc apr mxe mds rank correlation shows mds plot metrics distance metrics calculated rank correlation making mds insensitive metrics scaled 
distances rank correlation respect triangle inequality proper metric space 
pattern similar observed mds plots 
cal space far metrics 
cross entropy closest rms close plots 
cross entropy rms high rank correlation cross entropy lower rank correlation metrics rms pushed far rms close metrics mds plot 
apr auc space farthest cal fsc upper left side space 
acc rms near center space 

sar general purpose metric applying supervised learning data decision metric train metric model selection 
learning algorithm dictates metrics training difficult train neural net metrics rms mxe 
usually freedom selecting metric model selection metric pick best learning algorithm best parameters algorithm 
correct metric problem known model selection probably done metric learning algorithm trained 
done correct metric known 
mds plots correlation analysis suggest rms remarkably correlated measures serve general purpose metric specific optimization criterion known 
wondered devise new metric centrally located rms better correlation metrics 
devise completely new metric tried averaging behaved metrics new metric robust individually 
sar combines squared error accuracy roc area measure sar acc auc rms 
chose metrics sar reasons 
wanted select metric metric group threshold metrics ordering metrics probability metrics 
acc auc rms popular metric groups respectively 
metrics correlated metrics groups mds plots lie closest metrics groups seen mds plots tables sar behaves differently acc auc rms 
table sar higher mean rank correlation metrics metric 
mds plots sar tends consistently centrally located metrics 
table metric best reflects ordering mean performance learning methods 
results suggest metrics examined sar metric average correlated metrics separately groups 
sar representative rms rms table normalized scores learning algorithm metric average problems model acc fsc lft auc apr bep rms mxe cal mean sar ann svm bag dt knn bst dt dt bst stmp 
experiment sar model selection sar outperformed metrics selecting models best tied rms 
believe results suggest sar robust combination popular metrics may bey appropriate correct metric known benefit sar rms modest best 
attempts sar better optimizing weights acc auc rms sar average significantly improve sar compared equal weights metrics 
impressed behaved rms currently working devise better sar metric yields improvement rms 

performances metric table shows normalized performance learning algorithm metrics 
cal scaled mean observed cal score maximum observed cal score test problem find best parameter settings learning algorithm compute normalized score 
entry table averages scores problems 
columns mean normalized scores metrics sar performance 
higher scores indicate better performance 
models table ordered mean performance 
written separate compare performance learning methods metrics interesting relationships learning algorithms metrics worth discussing context 
best performing models neural nets svms bagged trees 
surprisingly neural nets outperform model types averages metrics 
anns appear excellent general purpose learning methods 
say anns best learning algorithm win rms cal rarely perform poorly problem metric excellent performance 
svms perform anns 
note svm predictions suitable measures cross entropy calibration squared error 
svms metrics platt method transform svm predictions calibrated probabilities 
neural nets svms appear safe general purpose high performance learning method predictions calibrated method platt scaling 
single decision trees perform poorly bagged trees perform nearly neural nets svms 
bagging improves decision tree performance metrics yields particularly large improvements probability metrics 
neural nets svms bagged trees appear safe general purpose high performance learning method 
boosted trees outperform learning methods acc lft roc apr bep 
boosting wins threshold metrics rank metrics performs poorly probability metrics squared error cross entropy calibration 
maximum margin methods boosted trees yield poorly calibrated probabilities 
svms perform platt scaling undoes maximum margin 
boosting wins metrics suited easily top performing learning method consider threshold ordering metrics 
knn methods competitive better algorithms done better larger train sets 
single decision trees perform methods probably recursive partitioning runs data quickly train sets small trees predicting probabilities 
tested different kinds decision trees including smoothed unpruned trees picked best poor performance trees due tree type inferior tree types tested perform methods 
interestingly boosting stump models perform boosting full decision trees 
boosted stumps outperform single trees threshold rank metrics 
place ranking decision trees due extremely poor performance probability measures 

related large literature comparing performance metrics 
closest flach 
flach uses roc space understand compare different metrics 
analyzes accuracy precision weighted relative accuracy decision tree splitting criteria 
statlog project performed large scale empirical evaluation number learning algorithms 
statlog compared performance different algorithms analysis predictions algorithms compared 
statlog compare performance different metrics 

discussion analysis allows draw variety summarize 
goal maximize accuracy model needs continuous performance metric backpropagation train neural net probably better train model squared error cross entropy squared error sits closer accuracy metric space 
result surprising cross entropy theoretically preferred loss function binary classification 
suspect cross entropy robust squared error real data sets real data contains class noise cross entropy sensitive 
squared error remarkably robust performance metric higher average correlation metrics metric sar 
squared error appears excellent general purpose metric 
models achieve excellent performance ordering metrics auc apr bep making predictions yield probabilities 
example neighbor models best roc performance values large predictions close fraction positives data 
yields predictions poor viewed probabilities small differences predicted values sufficient provide ordering 
expected maximum margin methods boosting svms yield excellent performance metrics accuracy designed 
surprisingly maximum margin methods yield excellent performance ordering metrics 
expected maximizing distances decision boundaries provide basis ordering cases fall far boundaries 
boosted trees perform accuracy roc perform poorly probability metrics squared error cross entropy 
poor performance probability metrics consequence boosting maximum margin method 
svms exhibit problem scale svm predictions platt method linearly scaling svm predictions 
neural nets trained backpropagation excellent performance boosting perform metrics including probability metrics rms mxe cal believe part reason neural nets perform trained backpropagation squared error seen squared error excellent metric 
ordering metrics auc apr bep cluster close metric space exhibit strong pairwise correlations 
metrics clearly similar somewhat interchangeable 
originally grouped lft threshold metrics acc fsc results suggest lft behaves bep ordering metric 
group lft bep ordering metrics auc apr metric space metrics significant dimensions 
metrics measure thing 
different performance metrics yield different tradeoffs appropriate different settings 
metric metric optimized model selection matter 
sar metric combines accuracy roc area squared error appears general purpose metric rms sar may provide benefit rms 
hope additional research area enable design better metrics shed light metrics appropriate different settings 

acknowledgments geoff crew alex help running experiments 
creators xgobi interactive mds software generate mds plots 
collaborators stanford linear accelerator slac data tony nasa goddard help indian pines data 

blake merz 
uci repository machine learning databases 
degroot fienberg 
comparison evaluation forecasters 
statistician 

applied data mining 
john wiley sons new york 
johnson 
support vector machine classifiers applied data 
proc 
eighth jpl airborne geoscience workshop 
joachims 
making large scale svm learning practical 
advances kernel methods 
king feng 
statlog comparison classification algorithms large real world problems 
applied artificial intelligence may june 
flach 
geometry roc space understanding machine learning metrics roc 
proc 
th international conference machine learning icml pages 
aaai press january 
platt 
probabilistic outputs support vector machines comparison regularized likelihood methods 
smola bartlett schuurmans editors advances large margin classifiers pages 
provost domingos 
tree induction probability rankings 
machine learning 
provost fawcett 
analysis visualization classifier performance comparison imprecise class cost distributions 
knowledge discovery data mining pages 
appendix performance metrics accuracy probably widely performance metric machine learning 
defined proportion correct predictions classifier relative size dataset 
classifier continuous outputs neural nets threshold set threshold predicted positive 
root mean squared error rmse widely regression measures predictions deviate true targets 
rmse defined rmse pred true mean cross entropy mxe probabilistic setting interested predicting probability root mean squared error applicable binary classification settings classifier outputs predictions compared true target labels 
example positive 
proven setting minimizing cross entropy gives maximum likelihood hypothesis 
mean cross entropy defined mxe true ln pred true ln pred assumptions pred true receiver operating characteristic roc roots early days radar difficult distinguish true positives false positives 
roc plot sensitivity vs 
specificity possible thresholds 
sensitivity defined pred positive true positive approximated fraction true positives predicted positive recall 
specificity pred negative true negative 
approximated fraction true negatives predicted negatives 
auc area roc curve summary statistic 
roc number nice properties principled similar measures average precision 
auc widely fields medicine popular machine learning community 
lift marketing analysis lift measures better classifier predicting positives baseline classifier randomly predicts positives rate observed positives data 
definition lift true positives threshold dataset threshold usually threshold set fixed percentage dataset classified positive 
example suppose marketing agent wants send advertising potential clients afford send ads population 
classifier trained predict client respond advertisement ads sent population predicted respond 
classifier optimal lift get clients possible respond advertisement set 
precision recall measures widely information retrieval 
precision fraction examples predicted positive positive 
recall fraction true positives predicted positives 
measures trivially maximized predicting predicting respectively positive 
measures 
different ways combine measures described metrics 
precision recall score threshold score harmonic mean precision recall threshold 
precision recall level name suggests set threshold recall precision threshold computed 
precision recall break point defined precision point threshold value precision recall equal 
average precision usually computed average precisions eleven evenly spaced recall levels 
cal reliability diagrams 
calculated follows order cases predicted value put cases bin 
calculate percentage cases true positives 
approximates true probability cases positive 
calculate mean prediction cases 
absolute value difference observed frequency mean prediction calibration error bin 
take cases compute errors way bins 
cal mean binned calibration errors 
parameter settings parameter settings algorithm variations learning methods knn values ranging 
knn euclidean distance euclidean distance weighted gain ratio 
distance weighted knn locally weighted averaging 
kernel widths locally weighted averaging vary times minimum distance points train set 
ann train nets gradient descent backprop vary number hidden units momentum 
don validation sets weight decay early stopping 
performance metric examine nets different epochs 
dt vary splitting criterion pruning options smoothing laplacian bayesian smoothing 
tree models buntine ind package bayes id cart cart mml 
generate trees type pruning bs bayesian smoothing mml laplacian smoothing 
see description 
bag dt bag trees type 
bst dt boost tree type 
boosting overfit consider boosted dts steps boosting 
bst stmp stumps single level decision trees different splitting criteria boosted steps 
svms kernels svmlight linear polynomial degree radial width vary regularization parameter factors output range svms 
svm predictions compatible models platt method convert svm outputs probabilities fitting sigmoid 
scaling svms poor rms possible calculate mxe cal 
