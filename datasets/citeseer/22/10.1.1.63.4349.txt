tuning performance mmas heuristic ridge daniel kudenko department computer science university york england ridge com kudenko cs york ac uk 
presents depth design experiments doe methodology performance analysis stochastic heuristic 
heuristic investigation max min ant system mmas travelling salesperson problem tsp 
specifically response surface methodology model tune mmas performance regard tuning parameters problem characteristics performance metrics solution quality solution time 
accuracy predictions methodically verified separate series confirmation experiments 
conflicting responses simultaneously optimised desirability functions 
recommendations optimal parameter settings 
optimal parameters methodically verified 
large number degrees freedom mmas design overcome minimum run resolution design 
publicly available algorithm problem generator implementations 
serve illustrative case study principled engineering stochastic heuristic 
motivation mmas heuristic member ant colony optimisation aco metaheuristic family 
general purpose approximate algorithm solving variety related problems 
generality comes price 
typical heuristics aco algorithms stochastic possible tuning parameters influence performance 
performance influenced problem instance characteristics 
presents problems heuristic engineer 
heuristic practical engineer able determine tuning parameters problem characteristics affecting performance 
relate parameters characteristics performance best parameter settings chosen problem instance characteristics 
allow fact stochastic heuristics vary performance identical repetitions run 
perform simultaneous analysis conflicting responses 
parameter tuning problem 
despite parameter tuning problem central understanding heuristic performance treatment st tzle birattari hoos eds sls lncs pp 

springer verlag berlin heidelberg tuning performance mmas heuristic literature lacking 
existing provided useful insights importance recommended settings tuning parameters 
research improved addressing concerns 
research rarely reports parameters methodically chosen choice 
leaves reader knowledge repeat procedure recognised methodology 
research uses single performance measure 
stochastic approximate nature heuristics necessitates simultaneously consider conflicting performance measures 
researchers attempt methodically tune parameters factor time approach 
approach examines influence single tuning parameter value holding parameters constant 
demonstrates established methodical approach called design experiments doe successfully brought bear parameter tuning problem 
advantages design experiments approach recognised fields designed experiments require fewer resources terms experiments time material amount information obtained 
designed experiments estimate interactions factors case experiments 
experimental information larger region experiment design space 
process optimisation efficient factor space searched 
begins background mmas algorithm tsp problem domain introducing explaining tuning parameters examined 
doe approach discussed 
section presents methodology 
sections describe experiment results 
summarises related presents 
background mmas heuristic tsp max min ant system mmas member ant colony optimization aco metaheuristic family 
research mmas applied travelling salesperson problem tsp 
mmas shown effective aco algorithms tsp 
factor independent variable manipulated experiment 
factor describes tuning parameters problem characteristics manipulate experiments 
ridge kudenko tsp problem visiting set cities shortest distance possible city visited 
discrete combinatorial optimisation problem 
tsp represented graph data structure nodes graph represent cities edges represent cost moving cities 
broadly aco metaphor natural ants forage food 
artificial ants create tours tsp graph depositing pheromone graph edges composing tours 
ant movement decisions probabilistic pheromone levels graph edges heuristic incorporating cost moving edge 
repeated iteration artificial ants tour building pheromone deposition decay allow aco algorithms converge solutions optimisation problem 
summarise mmas stages reader identify tuning parameters intended affect performance 
detailed descriptions literature 
mmas implementation java port publicly available code port informally verified produce output original code sets inputs instances 
stage initialise pheromone starting pheromone value assigned graph edges 
case graph edges set tnn parameter discussed tnn length nearest neighbour tour problem 
stage ant placement involves ants starting node graph 
explore possibilities 

scatter ants scattered randomly chosen nodes graph 

node ants single randomly chosen node 
stage tour construction ant movement decisions explorative exploitative 
number uniformly randomly chosen 
number exploration threshold ant simply chooses city best combination pheromone edge cost 
number greater equal exploration threshold ant probabilistic decision city visit 
probability city chosen ant city pij ij ij fi il fi il ij inverse cost edge ij fi set unvisited feasible cites visit city parameters 
higher iridia ulb ac aco aco code public software html tuning performance mmas heuristic greater influence associated terms choice city 
maximum minimum limits trail pheromone levels updated max far min max problem size 
stage local search common practice improve ants tours local search procedure 
potentially large number local search procedures plugged mmas chose experiment local search 
methodology analysis incorporate investigating local search 
stage evaporate pheromone pheromone edges ij ij tuning parameter 
stage trail update pheromone deposited trail single ant ij ij length tour single ant updates choice best ant current iteration iteration best ant best ant iterations far best far 
encountered choice incorporated mmas logic different ways 

fixed frequency 
iteration best ant occasional best far fixed frequency 

schedule 
iteration best ant increasingly frequent best far schedule 
research uses fixed frequency 
firstly simpler form schedule understood experimenting complicated schedule 
furthermore local search decision possible schedules applied 
experimenting scope 
stage pheromone pheromone triggered algorithm 
algorithm judges conditions met firstly variation pheromone levels called branching factor edges drops threshold 
secondly number iterations improvement exceeded 
original source code check frequency iterations 
literature cites condition met 
research remains compatible original source code uses condition 
ridge kudenko concerned mask iteration threshold condition implementation checks iteration 
due trails reset max 
comments 
parameters implicit mmas implementation 
calculation branching factor number edges pheromone levels exceed cutoff level min max min max min maximum minimum pheromone values edges 
obviously contains parameter research original source code fixed 
ant decisions tour construction limited candidate list edges due computational expense evaluating edges 
candidate list ordered list nearest cities city 
computationally expensive evaluation branching factor limited way 
experiment designs response surface models goal research investigate changes algorithm tuning parameters problem characteristics affect algorithm performance 
tuning parameters characteristics vary called factors performance measures gather called responses 
investigating effect factors response general approach vary factors number levels 
experimental region covered chosen ranges factors called design space 
measure responses interpolate measurements response surface design space 
response surface methodology rsm 
popular families designs building response surfaces known central composite designs ccd 
ccd contains embedded factorial design augmented design points centre design space centre points called axial points located distance design centre 
popular ccd designs circumscribed central composite face centred central composite 
circumscribed central composite ccc sets value axial points create circle square defined embedded factorial 
face centered composite fcc sets axial points lie faces square defined embedded factorial research uses face centred composite design practical limits parameters exploration threshold section example 
detailed descriptions ccc literature 
full factorial embedded part ccd expensive 
crossing say factors needed research minimum levels require design points embedded factorial 
state art design called minimum run resolution design introduced 
provides vast saving experiment runs allowing main second order interactions estimated 
full factorial design crosses levels factors 
methodology tuning performance mmas heuristic stagnation stopping criterion research takes practical view algorithm longer producing improved results regularly preferable halt execution better employ computational resources 
leads stagnation stopping criterion stagnation number iterations solution improvement observed 
stagnation offers reproducibility combinatorial count incorporating problem difficulty algorithm halting 
avoids cpu time stopping criterion 
criticised acceptable scientific grounds reproducibility 
research uses iterations 
course results dependent criterion chosen 
nature experiments heuristics 
responses performance measures reflect conflicting heuristic goals high solution quality low time solution 
johnson advocates reporting running times 
key reason approximation algorithms trade quality solution reduced running time readers may legitimately want know tradeoff works algorithms question 
research specifically addressed trade 
response reflects time solution cpu time algorithm run time algorithm halted time best solution 
careful include time calculate write output essential algorithm functioning 
open question solution quality response appropriate 
research uses relative error defined difference algorithm solution optimal solution expressed percentage optimal solution 
alternatives adjusted differential approximation exist 
optimal calculated concorde solver 
problem instances research includes problem size standard deviation problem edge cost factors need method produce instances controllable levels characteristics 
publicly available benchmark libraries breadth provide instances 
reproducibility java implementation th dimacs implementation challenge problem generator 
informally verified produce instances original code 
generator modified draw edge costs log normal distribution standard deviation resulting edge costs controlled mean fixed 
inspired previous investigations problem difficulty exact tsp algorithm verified mmas heuristic 
ridge kudenko experiment design reported research minimum run resolution face centred composite design replicates factorial axial points centre points 
requires total runs 
statistical significance level 
table lists experiment factors high low factorial levels 
factor levels chosen criteria 
factor bounds values example bounds chosen high low factor levels 
alternatively factor bounds example chose high low values incorporate values typically seen literature 
table 
experiment factors high low factor levels 
denotes numeric factor factor 
factor meaning type alpha pheromone term 
beta distance heuristic term 
ants number ants expressed problem size 
length candidate list problem size 
exploration exploitation threshold 
rho evaporation parameter 
branch branching factor threshold trail occurs 
iters iteration threshold trail occurs 
stdev standard deviation edges tsp size number cities tsp instance 
best iteration frequency best far freq ant pheromone updates 
ant placement ants scattered randomly graph single randomly chosen city 
random rsm combinations factors required problem instances including centre points 
experiments conducted similar identical machines necessary randomize run order deal unknown uncontrollable nuisance factors 
factors include hardware differences operating system differences impact cpu time response 
model fitting tuning performance mmas heuristic outliers data removed data amenable statistical analysis 
outliers chosen usual diagnostics responses transformed log transformation passed tests satisfactorily 
normal plots solution quality responses exhibited deviation line previously encountered ant colony algorithm 
may due restart nature mmas 
pursued analysis assumption anova tests robust small deviation verified model detailed section 
fit analysis conducted response determine highest order statistically significant model fit responses 
fit analysis begins fitting lowest order model linear model response 
higher order model fitted response 
additional terms higher order model statistically significant necessary higher order model 
procedure continues highest required order 
minimum run resolution design aliased cubic models 
leaves linear model factor interaction model quadratic model considered 
results quadratic models built response combination factor ant placement table 
space restrictions prevent reproducing models 
drawing resulting response surface models response confirm accuracy models 
model verification common approach doe randomly sample points design space run actual process case algorithm points compare model predictions measurements randomly generated algorithm runs 
criteria satisfaction model confidence predictions judged 
conservative prefer models provide consistently higher predictions relative error higher solution time observed 
matching trend prefer models match trends algorithm performance 
prediction parameter combinations give best acknowledge authors question deletion outliers outliers represent real data may reveal process studied 
normal plot internally residuals plot internally residuals predicted value plot internally residuals run number plot predicted versus actual values plot externally residuals run order plots leverage cook distance 
please see nist handbook statistical methods www itl nist gov div handbook meanings tests interpretation 
ridge kudenko worst performance match actual parameter combinations give observed best worst performance 
randomly generated new experiment runs combinations factors design space 
randomly chosen combinations problem characteristics completely new instances generated 
experiment run replicated times 
measured responses compared models prediction intervals 
prediction interval simply range runs fall 
illustrates results time relative error responses new instances 
relative error relative error pi low pi high treatment relative error time time pi low pi high treatment time fig 
verification rsm predictions model better predictor time relative error small violations normality assumption relative error rsm 
relative error rsm satisfy matching trend criterion 
response optimisation express multiple responses terms single desirability function response yi desirability function di yi maps values possible values yi 
di yi completely undesirable di yi ideal response 
desirability responses geometric mean individual contributions response desirability weighted reflect user preferences 
equal weighting quality solution di www itl nist gov div handbook pri section pri htm tuning performance mmas heuristic time 
goal minimize response research desirability functions take form li ui ti lower bound upper bound target values desired response 
di yi yi yi ui ti ti ui yi ui yi ui note fitted response value yi place yi 
nelder mead downhill simplex maximise desirability responses 
important points note optimisation problem characteristics factors model 
included optimisation optimisation select small problem size low standard deviation 
perform optimizations characteristics fixed level factorial combinations 
recall forcing alpha beta take integer values expensive cost non integer exponentiation 
results optimizations table ranking relative size contribution factor rsm models table 
important consider optimisations rankings factor relative small effect responses take value optimisation 
points noted 
alpha beta generally kept low large instances high cost standard deviation 
ants candidate list small number ants short candidate list preferred 
exploration exploitation threshold maximum 
means exploration rarely ant movement decisions best heuristic product value 
pheromone evaporation higher value preferred increasing size standard deviation 
high threshold iterations preferred smaller instances 
low branching factor threshold preferred 
different value usually fixed literature 
iteration threshold smallest contributors responses suggesting branching factor threshold important determining trail 
far smaller instances lower standard deviation preferred high frequency best far ant 
larger instances higher standard deviations preferred best far frequently 
factor lower ranking contributors responses table 
contradicts result literature 
ant placement smallest contributors responses suggesting placement little difference performance 
ridge kudenko table 
desirability optimisation results relative error time combinations problem size problem standard deviation size stdev alpha beta ants rho branch iters best placement time rel error desirability random random random random table 
ranks contribution size factors relative error time responses 
columns represent ranks main effects 
third fourth columns represent ranks model effects 
time rel error time rel error alpha beta ants candidate list rho branch threshold iters threshold best far freq ant placement optimisation verification tuning performance mmas heuristic predictions created important verify optimal parameter settings optimal improve parameter settings 
randomly generate set tsp instances factorial instance characteristic combinations section 
run mmas algorithm optimal parameter settings sets instances 
instance run mmas combinations randomly chosen parameter settings 
runs replicated times 
expect instance optimal parameter settings produce solutions lower solution time lower relative error randomly generated parameter settings 
illustrates plots relative error time respectively instances 
relative error size st dev instance relative error time size st dev time time instance time fig 
comparison optimal shaded non optimal unshaded parameter results non optimal results close optimal results relative error lower relative error time 
difficulty emerged instances high standard deviation cost matrix 
illustrates relative error non optimal parameters occasionally optimal parameters 
time results optimal parameters remained non optimal parameters 
related factorial designs combined local search procedure systematically find best parameter values heuristic 
unfortunately cali bra tune algorithm parameters 
restrictive linear assumption precludes examining interactions parameters 
aco algorithms require parameters fitting analysis shows interactions higher order models important 
ridge kudenko relative error size st dev optimal non optimal instance relative error time size st dev optimal non optimal instance time fig 
comparison optimal shaded non optimal unshaded parameter results systematically find settings tuning parameters set vehicle routing problems vrp 
set problems solve procedure finds high quality parameter settings small number problems problem set analysis set combines settings achieve set parameters complete problem set 
fractional factorial design produce response surface 
optimal parameter settings analysis set averaged obtain final parameter settings problems set 
method perform poorly representative test problems chosen correctly problem class broad requires different parameter settings 
park kim non linear response surface method find parameter settings simulated annealing algorithm 
parsons johnson central composite design embedded fractional factorial build response surface improve performance genetic algorithm test data set 
parameters modeled 
birattari uses algorithms derived machine learning technique known racing incrementally tune parameters metaheuristics including max min ant system tsp author pursue idea bi objective optimisation time quality 
achieved stagnation stopping criterion desirability functions 
attempt tune aco algorithm addressed parameters 
authors partitioned parameter ranges values respectively 
reasoning granularity partitioning number partitions varied parameters 
treatment run times iteration optimum stopping criterion single city instance 
single city problem prevents examination problem characteristics size small trivial algorithm 
resulted experiments 
approach inefficient requiring times runs tune quarter parameters 
contributions tuning performance mmas heuristic significance doe techniques follows 
able efficiently model verify performance stochastic heuristic terms performance measures minimum run resolution face centred composite design 
important contributions field ant colony optimization broader field heuristics 
efficient experiment designs 
minimum run resolution designs offer huge savings experimental runs full factorials fractional factorials 
saving overcomes challenge designing heuristics large number degrees freedom 
simultaneous analysis conflicting responses 
solution quality solution time critical heuristic performance analyses 
desirability functions permit simultaneously analysing optimising conflicting responses assigning relative weights 
optimal parameter recommendations 
nelder mead numerical optimisation desirability allowed find parameter settings optimise mmas performance different combinations problem characteristics 
accuracy optimisations demonstrated independent set experiments 
results particularly noteworthy 
higher values alpha beta appropriate occasions 
maximum exploration exploitation threshold recommended effectively ruling exploration algorithm 
unexpected result merits investigation 
smaller instances lower standard deviation preferred high frequency best far ant 
larger instances higher standard deviations preferred best far frequently 
contradicts related result literature 
low threshold branching factor trail recommended 
different fixed value literature 
conclude branching factor threshold considered tuning parameter 
detailed description doe methodology adaptation performance analysis heuristics available literature 

dorigo st tzle ant colony optimization 
mit press cambridge ma 
factor time versus designed experiments 
american statistician 
st tzle hoos max min ant system 
generation computer systems ridge kudenko 
myers montgomery response surface methodology 
process product optimization designed experiments 
john wiley sons chichester 
small efficient resolution fractions designs application central composite designs 
proceedings th fall technical conference 
american statistical association 
johnson theoretician guide experimental analysis algorithms 
proceedings fifth sixth dimacs implementation challenges 
dorigo model search combinatorial optimization comparative study 
beyer fern ndez schwefel 
eds 
parallel problem solving nature ppsn vii 
lncs vol 
springer heidelberg 
applegate bixby cook implementing dantzig fulkerson johnson algorithm large traveling salesman problems 
mathematical programming series 
johnson mcgeoch experimental analysis heuristics stsp 
traveling salesman problem variations kluwer academic publishers dordrecht 
cheeseman kanefsky taylor really hard problems 
proceedings twelfth international joint conference artificial intelligence vol 
pp 

morgan kaufman usa 
ridge kudenko analysis problem difficulty class optimisation heuristics 
proceedings 
lncs vol 
pp 

springer heidelberg 
statistics research nd edn 
iowa state university press 
ridge kudenko analyzing heuristic performance response surface models prediction optimization robustness 
proceedings genetic evolutionary computation conference acm press new york 
montgomery design analysis experiments th edn 
wiley 
simultaneous optimization response variables 
journal quality technology 
press flannery teukolsky vetterling numerical recipes pascal art scientific computing 
cambridge university press 
az laguna fine tuning algorithms fractional experimental designs local search 
operations research 
golden experimental design find effective parameter settings heuristics 
journal heuristics 
park kim systematic procedure setting parameters simulated annealing algorithms 
computers operations research 
parsons johnson case study experimental design applied genetic algorithms applications dna sequence assembly 
american journal mathematical management sciences 
birattari problem tuning metaheuristics 
phd universit bruxelles 
clark optimal parameters ant colony optimization algorithms 
proceedings international conference artificial intelligence vol 
pp 

csrea press 
ridge kudenko sequential experiment designs screening tuning parameters stochastic heuristics 
workshop empirical methods analysis algorithms iceland 
pp 

