appears advances neural information processing systems nips vol 
mit press spectral bounds sparse pca exact greedy algorithms moghaddam merl cambridge ma usa merl com yair weiss hebrew university jerusalem israel cs huji ac il shai avidan merl cambridge ma usa avidan merl com sparse pca seeks approximate sparse eigenvectors projections capture maximal variance data 
cardinality constrained non convex optimization problem np hard encountered wide range applied fields bio informatics finance 
progress focused mainly continuous approximation convex relaxation hard cardinality constraint 
contrast consider alternative discrete spectral formulation variational eigenvalue bounds provide effective greedy strategy provably optimal solutions branch bound search 
exact methodology reveals simple renormalization step improves approximate solutions obtained continuous method 
resulting performance gain discrete algorithms demonstrated real world benchmark data extensive monte carlo evaluation trials 
pca indispensable basic tool factor analysis modeling data 
despite power popularity key drawback lack sparseness factor loadings linear combinations input variables 
sparse representations generally desirable aid human understanding gene expression data reduce computational costs promote better generalization learning algorithms 
machine learning input sparseness closely related feature selection automatic relevance determination problems enduring interest learning community 
earliest attempts pca statistics literature consisted simple axis rotations component thresholding underlying goal essentially subset selection identification principal variables 
true computational technique called jolliffe provided proper optimization framework lasso proved computationally impractical 
zou proposed elegant algorithm spca elastic net framework penalized regression regular pcs solved efficiently angle regression lars :10.1.1.10.2712
subsequently relaxed hard cardinality constraint solved convex approximation semi definite programming sdp 
direct formulation sparse pca called yielded promising results comparable better zou lasso method demonstrated standard pit props benchmark dataset known statistics community lack sparseness subsequent difficulty interpretation 
pursued alternative approach spectral formulation variational principle courant fischer min max theorem solving maximal eigenvalue problems dimensionality constrained subspaces 
nature discrete view leads simple post processing renormalization step improves approximate solution provides bounds sub optimality 
importantly points way exact provably optimal solutions branch bound search 
exact computational strategy parallels ko solved different optimization problem maximizing entropy bounds determinants 
experiments demonstrate power greedy exact algorithms solving optimal sparse factors real world pit props data de facto benchmark summary findings large comparative study extensive monte carlo evaluation leading algorithms 
sparse pca formulation sparse pca cast cardinality constrained quadratic program qp symmetric positive definite covariance matrix maximize quadratic form ax variance sparse vector having non zero elements max subject card card denotes norm 
optimization problem non convex np hard intractable 
assuming solve optimal vector subsequent sparse factors obtained recursive deflation standard numerical routines 
sparseness controlled value different factors viewed design parameter unknown quantity known oracle 
alas currently guidelines setting especially multiple factors orthogonality relaxed ordinary pca decompositions may unique 
contributions providing sound theoretical basis selecting clarifying art crafting sparse pca factors 
note cardinality constraint quadratic form eq rayleigh ritz quotient obeying analytic bounds min ax max corresponding unique eigenvector solutions 
optimal objective value variance simply maximum eigenvalue principal eigenvector un note rank ui increasing order magnitude min max nonlinear cardinality constraint optimal objective value strictly max principal eigenvectors longer instrumental solution 
show eigenvalues continue play key role analysis design exact algorithms 
optimality conditions consider conditions true oracle revealed optimal solution unit norm vector cardinality yielding maximum objective value necessarily imply contains non zero elements ak principal submatrix obtained deleting rows columns corresponding zero indices equivalently extracting rows columns non zero indices 
vector unit norm equivalent standard unconstrained rayleigh ritz quotient 
subproblem maximum variance max ak optimal objective summarize important observation proposition 
note multi factor version eq ill posed additional constraints basis orthogonality cardinality variable redundancy ordinal rank allocation variance 
proposition 
optimal value sparse pca optimization problem eq equal max principal submatrix largest maximal eigenvalue 
particular non zero elements optimal sparse factor exactly equal elements principal eigenvector underscores inherent combinatorial nature sparse pca equivalent class cardinality constrained optimization problems 
despite providing exact formulation revealing necessary conditions optimality simple matrix terms proposition suggest efficient method finding principal submatrix short enumerative exhaustive search impractical due exponential growth possible submatrices 
exhaustive search viable method small guarantees optimality toy problems small real world datasets calibrating quality approximations optimality gap 
variational renormalization proposition immediately suggests simple turns quite effective computational fix improving candidate sparse pc factors obtained continuous algorithm various solutions 
proposition 
unit norm candidate factor cardinality approximation technique 
non zero subvector uk principal maximum eigenvector submatrix ak defined non zero indices uk ak optimal solution 
replacing nonzero elements uk guarantee increase variance ak 
variational renormalization suggests somewhat ironically continuous approximate solution certainly better discard loadings keep sparsity pattern solve smaller unconstrained subproblem indicated submatrix ak 
simple procedure fix referred decrease variance surely improve continuous algorithm performance 
particular expedient ad hoc technique simple thresholding st setting smallest absolute value loadings un zero normalizing unit norm recommended sparse pca 
section illustrate straw man algorithm enhanced proper renormalization 
consequently past performance benchmarks simple technique may need revision previous results pit props dataset section 
sparse pca factors published literature readily improved inspection proper renormalization mere cost single eigen decomposition 
eigenvalue bounds recall objective value eq bounded spectral radius max rayleigh ritz theorem 
furthermore spectrum principal submatrices shown play key role defining optimal solution 
surprisingly eigenvalue spectra related inequality known inclusion principle 
theorem inclusion principle 
symmetric matrix spectrum ak principal submatrix eigenvalues ak 
integer ak proof 
proof omit straightforward consequence imposing sparsity pattern cardinality additional orthogonality constraint variational inequality courant fischer min max theorem see example 
words eigenvalues symmetric matrix form upper lower bounds eigenvalues principal submatrices 
special case eq leads known eigenvalue interlacing property symmetric matrices 
spectra interleave eigenvalues larger matrix bracketing smaller 
note positive definite symmetric matrices covariances augmenting am am adding new variable expand spectral range reducing min increasing max 
eigenvalue maximization inequality constraint card eq tight equality optimum 
maximum variance achieved preset upper limit cardinality 
function optimal variance cardinality monotone increasing range max max max largest diagonal element variance concise informative way quantify performance algorithm plot variance curve compare optimal 
seek maximize variance relevant inclusion bound obtained setting eq yields lower upper bounds ak max ak max ak max shows th smallest eigenvalue lower bound maximum variance possible cardinality utility lower bound doing away guesswork oracle setting interestingly see spectrum traditionally guided selection eigenvectors dimensionality reduction classical pca consulted sparse pca help pick cardinality required capture desired minimum variance 
lower bound useful speeding branch bound search see section 
note close max practically principal submatrix ak yield near optimal solution 
right hand inequality eq fixed loose upper bound max branch bound search intermediate subproblem am yields new tighter bound max am objective 
bound computations efficient relatively inexpensive power method 
inclusion principle leads interesting constraints nested submatrices 
example possible principal submatrices am obtained deleting th row column submatrix am maximal eigenvalue major fraction parent see am implication inequality search algorithms simply possible spectral radius submatrix arbitrarily small especially large large matrices large cardinality nearly variance captured 
combinatorial optimization propositions inclusion principle interlacing property especially monotonic nature variance curves general class binary integer programming ip optimization techniques ideally suited sparse pca 
greedy technique backward elimination suggested bound eq start full index set 
sequentially delete variable yields maximum max elements remain 
small cardinalities computational cost backward search grow near maximum complexity 
counterpart forward selection preferred start null index set sequentially add variable yields maximum max elements selected 
forward greedy search worstcase complexity 
best strategy problem empirically bi directional greedy search run forward pass plus second independent backward pass pick better solution proved remarkably effective extensive monte carlo evaluation realworld datasets 
refer discrete algorithm greedy sparse pca 
despite near optimal greedy search worthwhile invest optimal solution strategies especially sparse pca problem application domain finance engineering small optimality gap accrue substantial losses time 
ko branch bound relies computationally efficient bounds case upper bound eq active subproblems fifo queue depth search 
lower bound eq sort queue efficient best search 
exact algorithm referred guaranteed terminate optimal solution 
naturally search time depends quality variance initial candidates 
solutions dual pass greedy search ideal initializing quality typically quite high 
note initializations branch bound search take long time hours 
practice early termination set thresholds eigenvalue bounds 
general cost effective strategy recommend run forward pass settle near optimal variance initialize finding optimal solution 
full run added benefit giving near optimal solutions cardinalities run times typically faster single approximation continuous method 
experiments evaluated performance validated various synthetic covariance matrices real world datasets uci ml repository excellent results 
typical examples order illustrate advantages power discrete algorithms 
particular compared performance continuous techniques simple thresholding st spca elastic net regression dspca semidefinite programming :10.1.1.10.2712
revisited pit props dataset standard benchmark classic example difficulty interpreting fully loaded factors standard pca 
ordinary pcs capture total variance methodology compared explanatory power exact method sparse pcs 
table shows pcs loadings 
spca captures variance cardinality pattern pcs totaling non zero loadings dspca captures sparser cardinality pattern totaling non zero loadings :10.1.1.10.2712
aimed sparser pattern non zero loadings captured nearly variance spca loadings slightly dspca loadings 
evaluation protocol compared cumulative variance cumulative cardinality published results spca dspca 
goal match explained variance sparser representation 
loadings table optimal definition section 
run time including initialization bi directional pass negligible dataset 
computing factor took msec matlab ghz 
spca pc pc pc dspca pc pc pc pc pc pc table loadings sparse pcs pit props data 
see plots corresponding cumulative variances 
original spca dspca loadings taken 
cumulative variance spca dspca pcs cumulative cardinality spca dspca pcs pit props cumulative variance cumulative cardinality sparse pcs 
sparsity patterns cardinality ki pci 
spca magenta dspca green optimal red 
factor loadings sparse pcs shown table 
original spca dspca results taken 
specifically demonstrate benefits variational renormalization section consider spca sparse factor table st row spca block iterative penalized optimization unit norm scaling 
captures total data variance variational renormalization variance increases 
sparse factor dspca table st row dspca block captures total variance variational renormalization captures gain mere additional cost eigen decomposition 
variational renormalization results maximum variance possible indicated sparsity pattern omitting simple post processing step counter productive approximations sense doubly sub optimal globally locally subspace subset sparsity pattern 
give representative summary extensive monte carlo mc evaluation continuous algorithms 
show typical average case performance results random covariance matrices synthetic stochastic brownian processes various degrees smoothness ranging sub gaussian super gaussian 
mc run consisted covariance matrices normalized variance curves 
matrix find optimal solution ground truth subsequent calibration analysis performance evaluation 
spca lars elastic net spca matlab toolbox sj strand equivalent zou spca source code freely available dspca authors matlab source code uses sdp toolbox 
main dspca routine called recommended calibration see documentation 
mc evaluations continuous methods st spca dspca variational renormalization post processing applied declared solution 
note comparing raw output algorithms pointless variance dspca original dspca fix optimal cardinality log frequency st spca dspca optimality ratio typical variance curve continuous algorithm post processing original dash green variational renormalization fix solid green 
optimal variance black 
optimality ratio increases gain 
monte carlo study log likelihood optimality ratio max complexity st blue dspca green spca magenta red 
continuous methods fixed 
mean optimality ratio st spca dspca cardinality frequency st spca dspca cardinality monte carlo summary statistics means distributions optimality ratio estimated probability finding optimal solution cardinality 
fix variance curves markedly diminished 
shows histogram optimality ratio ratio captured optimal variance shown half sparsity typical mc run different covariances matrices 
order view sided tails distributions plotted log histogram values 
shows corresponding mean values optimality ratio continuous algorithms sdp dspca generally effective comparable 
smaller matrices lars spca matched dspca terms complexity speed spca times faster dspca 
times faster spca 
note simple thresholding st enhanced variational renormalization performs quite adequately despite simplicity captures optimal variance seen 
shows alternative revealing performance summary fraction trials optimal solution essentially likelihood success 
performance measure elicits important differences algorithms 
practical terms capable finding optimal factor time vs dspca 
naturally variational fix shown continuous algorithms rarely optimal solution 
discussion contributions summarized exact variational formulation sparse pca requisite eigenvalue bounds principled choice simple renormalization fix continuous method fast effective greedy search efficient optimal method 
surprisingly simple thresholding principal eigenvector st shown effective especially perceived straw man considered 
naturally performance vary effective rank eigen gap covariance matrix 
fact hard show exactly rank st optimal strategy special cases continuous methods ultimately competitive discrete algorithms variational renormalization fix section 
note somewhat remarkable effectiveness entirely unexpected supported empirical observations combinatorial optimization literature greedy search sub modular cost functions having monotonicity property variance curves known produce results 
terms quality solutions consistently performed continuous algorithms runtimes typically faster lars spca roughly faster sdp dspca matlab cpu times averaged 
view discrete algorithms complementary tools especially leading continuous algorithms distinct advantages 
example highdimensional datasets zou lars method currently viable option rely computing storing huge covariance matrix 
mention possibility solving larger systems faster nesterov st order method require full matrix memory discrete algorithms 
sdp formulation elegant robustness interpretation applied non square matrices sparse svd 
acknowledgments authors karl sj strand dtu customized code helpful advice lars spca toolbox gert lanckriet berkeley providing pit props data 
jolliffe 
loadings correlations interpretation principal components 
applied statistics 

dspca toolbox 
www princeton edu dspca htm 
el ghaoui jordan lanckriet 
direct formulation sparse pca semidefinite programming 
advances neural information processing systems nips 
vancouver bc december 
horn johnson 
matrix analysis 
cambridge press cambridge england 

cases studies application principal components 
applied statistics 
jolliffe 
modified principal component technique lasso 
journal computational graphical statistics 
ko lee queyranne 
exact algorithm maximum entropy sampling 
operations research july august 
mccabe 
principal variables 
technometrics 
nemhauser wolsey 
integer combinatorial optimization 
john wiley new york 
sj strand 
matlab implementation lasso lars elastic net spca 
informatics mathematical modelling technical university denmark dtu 
sturm 
matlab toolbox optimization symmetric cones 
optimization methods software 
tibshirani 
regression shrinkage selection lasso 
journal royal statistical society 
wilkinson 
algebraic eigenvalue problem 
clarendon press oxford england 
zou hastie tibshirani :10.1.1.10.2712
sparse principal component analysis 
technical report statistics department stanford university 
