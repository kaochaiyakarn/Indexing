meta learning model selection example selection machine learning domains concept drift ralf klinkenberg university dortmund computer science department artificial intelligence unit dortmund germany ralf klinkenberg cs uni dortmund de www ai cs uni dortmund de tasks data collected extended period time underlying distribution change 
typical example information filtering adaptive classification documents respect particular user interest 
interest user may change time 
machine learning approaches handling concept drift shown outperform static approaches ignoring experiments different types simulated concept drifts real word text data experiments real world data task classifying phases business cycles exhibiting real concept drift 
previous concept drift handling approaches single base learning algorithm employ base learner step time proposes metalearning approach allowing alternative learners automatically selecting promising base learner step time 
progress investigates contextdependent selection base learner leads better adaptation drifting concept lower classification error rates approaches single base learner 
furthermore investigates proposed metalearning approach allows speed selection process gained reduction error rate may lost speed 
approaches base learner selection meta learning compared experiments real world data mentioned domains simulated real world concept drifts respectively 
machine learning methods applied problems data collected extended period time 
real world applications introduces problem distribution underlying data change time 
example companies collect increasing amount data sales figures customer data find patterns customer behavior predict sales 
customer behavior tends change time model underlying successful predictions adapted accordingly 
experimental results covered klinkenberg 
problem occurs information filtering adaptive classification documents respect particular user interest 
amount online information communication growing rapidly increasing need automatic information filtering 
information filtering techniques example build personalized news filters learn news reading preferences user lang 
interest user concept underlying classification texts document content change time 
filtering system able adapt concept changes 
formalization concept drift problem section shortly describing previous approaches handling concept drift sections section describes machine learning approaches handling type concept drift shown outperform static approaches ignoring concept drift experiments different types simulated concept drifts real word text data see section klinkenberg joachims klinkenberg ping klinkenberg real world data real concept drift see section klinkenberg 
methods maintain adaptive time window training data klinkenberg joachims select representative training examples weight training examples klinkenberg ping klinkenberg 
key idea automatically adjust window size example selection example weighting respectively estimated generalization error minimized 
approaches theoretically wellfounded effective efficient practice 
require complicated parameterization simpler robust comparable heuristics 
section proposes meta learning approach adding additional degree freedom concept drift handling techniques selecting examples best suited learning certain point time selecting base learner parameterization set base learners parameterizations achieving smallest expected classification error point time 
furthermore section outlines approach extended meta data current situation experience previous time points predict selection meta data reduce search space selection 
experiments data sets mentioned analysed choosing base learner point time separately meta learning approach reduces classification error second meta learning approach allows reduce model selection search space computation time sacrificing accuracy 
section summarizes results provides outlook 
concept drift problem definition study problem concept drift pattern recognition problem framework klinkenberg joachims klinkenberg 
example consists feature vector label indicating classification 
data arrives time batches 
loss generality batches assumed equal size containing examples 
batch batch batch batch denotes th example batch batch data independently identically distributed respect distribution pri 
depending amount type concept drift example distribution pri pri batches differ 
goal learner sequentially predict labels batch 
example batch learner subset training examples batches predict labels batch 
learner aims minimize cumulated number prediction errors 
heuristic approaches concept drift machine learning changing concepts handled time windows fixed adaptive size training data mitchell widmer kubat klinkenberg renz weighting data parts hypothesis age utility classification task taylor 
approach weighting examples information filtering incremental relevance feedback approaches allan balabanovic 
earlier approach maintaining window adaptive size explored 
detailed descriptions methods described approaches klinkenberg 
windows fixed size choice window size compromise fast adaptivity small window generalization phases concept change large window 
basic idea adaptive window management adjust window size current extent concept drift 
theoretical approaches concept drift task learning drifting time varying concepts studied computational learning theory 
learning changing concept infeasible restrictions imposed type admissible concept changes drifting concepts provably efficiently learnable certain concept classes rate extent drift limited particular ways 
function randomly jumping values zero predicted learner accuracy 
helmbold long helmbold long assume possibly permanent slow concept drift define extent drift probability subsequent concepts disagree randomly drawn example 
results include upper bound extend drift maximally tolerable learner algorithms learn concepts drift certain constant extent drift 
furthermore show sufficient learner see fixed number examples 
window certain minimal fixed size allows learn concepts extent drift appropriately limited 
helmbold long restrict extend drift kuh petsche rivest kuh determine maximal rate drift acceptable learner maximally acceptable frequency concept changes implies lower bound size fixed window time varying concept learnable similar lower bound helmbold long 
practice usually guaranteed application hand obeys restrictions reader electronic news may change interests arbitrarily radically 
furthermore large time window sizes theoretical results hold impractical 
application oriented approaches rely far smaller windows fixed size window adjustment heuristics allow far smaller window sizes usually perform better fixed larger windows widmer kubat klinkenberg renz 
heuristics intuitive particular application domain usually require tuning parameters transferable domains lack proper theoretical foundation 
meta learning approaches concept drift domains concept changes dependence hidden context variables 
kind domains meta learning schemes developed identifying context variables variables indicating current context learning separate models context see widmer kubat widmer 
approaches assume existence context indicating variables keep base learners fixed meta learning frameworks proposed assume context indicator variables allow select different base learner point time 
handling concept drift learning problem drifting concepts introduced section face problem decide information past examples may find hypothesis adequate predict class information data 
know concept drift happens opposing effects hand older data probability distribution differs current distribution underlies process data may misleading 
hand data learning process better results concept drift occurred data arrived 
section different approaches learning drifting concepts 
differ way previous examples construct new hypothesis 
approaches share assumption concept drifts reverse newer examples important older ones 
assumption implemented common scheme estimating performance learner experiments performance calculated batch data regardless batches training 
base learners offer effective efficient error estimators 
support vector machines svms called estimator joachims estimates leave error svm solely svm solution learned examples get estimation performance efficient 
adaptive time windows simplest scenarios detecting concept drift concept drifts happen quickly relatively stable single concepts 
example imagine user information filtering system wants buy new car interested information sorts cars decision bought car interested information special type car 
may accurately called concept change concept shift concept drift 
scenario problem learning drifting concepts approached problem finding time point concept change happened 
standard learning algorithm fixed concepts learn data similarly concept drift scenarios handled time window training data assuming amount drift increases time focusing training examples 
shortcomings previous windowing approaches fix window size mitchell involve complicated heuristics widmer kubat klinkenberg renz 
fixed window size strong assumptions quickly concept changes 
heuristics adapt different speed amount drift involve parameters difficult tune 
klinkenberg joachims klinkenberg joachims approach automatically selecting appropriate window size involve complicated parameterization 
key idea select window size estimated generalization error new examples minimized 
svms get estimate generalization error special form estimates joachims 
adaptive window approach employs estimates way 
batch essentially tries various window sizes training base learner svm resulting training set 
batch batch batch batch batch batch window size computes estimate result training considering batch estimation batch training examples reflects assumption examples similar new examples batch 
window size minimizing estimate error rate selected algorithm train classifier current batch 
window adaptation algorithm summarized follows input training sample strain consisting batches containing labeled examples train base learner svm examples compute estimate examples output window size minimizes estimate example selection klinkenberg ping proposed extension time window adjustment approach klinkenberg joachims allowing select batches individually training set allowing uninterrupted sequence batches time window training set 
step classifier learned batch data 
course cases classifier sure classifier date drifting concept 
classifier estimate batches data generated model users interest batch comparing estimated leave error classifier batch test error batches 
higher error data model 
note point important leave estimation training error avoid errors fitting data 
second step information error classifier build training set actual classifier 
exclude batches new training set significantly higher classification error batch batch selection 
hope train final classifier data generated current model way similar example selection scheme section adaptive time window 
meta learning selecting best base learner model selection error reduction previously described concept drift approaches select examples best suited learning fixed base learner svms 
alternative base learners decision tree rule instance learners approaches long provide reliable error estimates base learner fixed chosen applying approaches 
idea concept drift handling frame select appropriate learner time step 
svms usually perform large data sets phases little concept drift 
may problems small data sets 
shortly concept drift representative examples available training base learner may appropriate 
selecting example set learner parameterization add additional degree freedom model selection process 
call type framework selecting promising combination example set base learner parameterization meta learner type 
meta learner selects combination example selection base learner selection minimizes expected error 
meta learning predicting best model selection speed learning meta learner type expands search space best expected model selection significantly hope reduce classification error 
section introduce second meta learning framework meta learner type reduces expanded search space order speed search 
meta learner type learn metalevel complete search search space level want meta learning guide search 
may expect choice appropriate learner certain point time depend expected best size training set 
attributes training examples meta level include training example constructed previously seen batch previously chosen base learner evaluated batch number old batches training previous batch number non interrupted batches training number batches detected concept drift succesful learner previous batch succesful learner batches seen far possibly meta attributes 
goal attribute learned meta learner prediction best base learner description time point meta attributes 
second goal attribute learned meta learner prediction best example set selection description time point meta attributes 
meta learner trained new meta example available batch 
prediction meta model predict best expected example set selections best expected base learners test test combinations possible combinations base learners example set selections 
meta learner framework learner providing ranking target class attribute values new meta example 
choice allows trade learning time search space size amount time required learning versus classification error 
larger complete search better result expected 
furthermore set different ways choice example selection choice base learner really critical 
best choice base learner expected depend expected best size training set learners better small data sets larger ones different learners different learning curves 
indicator locally successful learner 
choice learner certainly depends data set unknown characteristics previous success data set relevant indicator 
evaluation simulated concept drift scenarios experimental setup evaluation scheme simulated scenarios business news order evaluate learning approaches drifting concepts proposed simple non adaptive data management approaches compared adaptive time window approach batch selection strategy svms core learning algorithm full memory learner generates classification model previously seen examples forget old examples 
memory learner induces hypothesis batch 
corresponds window fixed size batch 
window fixed size time window fixed size batches training data 
adaptive window window adjustment algorithm described section klinkenberg joachims adapts window size current concept drift situation 
batch selection batches producing error twice estimated error newest batch applied model learned newest batch receive weight selected final training set 
weight examples set zero see section klinkenberg ping 
experiments performed information filtering domain typical application area learning drifting concept 
text documents represented attribute value vectors bag words model distinct word corresponds feature value ltc tf salton buckley word document 
words occurring times training data occurring list words considered 
document feature vector normalized unit length different document lengths 
performance classifiers measured prediction error 
reported results estimates averaged runs 
detailed results including precision recall results graphical plots performance selected window size time see klinkenberg joachims klinkenberg ping klinkenberg 
evaluation methods machine learning cross validation assume examples independent identically distributed assumption clearly unrealistic presence concept drift 
concept drift approaches proposed estimates joachims cross validation estimate optimize performance particular parameterization learning step estimate performance currently batch see section 
appropriate concept drift situation hand efficient estimator computed single training run 
fixed window size batches outperformed smaller larger fixed window sizes experiments chosen 
evaluation problem occurs time step concept drift scenario handled concept drift frameworks internal evaluation parameter optimization occurs frameworks compared external evaluation performance comparison 
just earlier case evaluation methods cross validation inappropriate case 
repeated runs simulated concept drift scenarios obtain averaged statistically reliable results 
experiments subset documents data set text retrieval conference trec consisting english business news texts 
text assigned categories 
categories considered cases pending joint ventures debt rescheduling dumping charges third world debt relief 
experiments concept change scenarios simulated 
texts randomly split batches equal size containing documents 
texts category distributed equally possible batches 
scenarios document considered relevant certain point time matches interest simulated user time 
trec topic batch scenario probability document topic relevant user interest time batch specified 
scenarios simulated user interest changes topics 
documents classes relevant scenarios 
shows probability relevant document category batch scenarios 
documents category specified inverse relevance probability documents category relevance category 
scenario scenario documents category considered relevant user interest documents irrelevant 
changes abruptly concept shift batch documents category relevant irrelevant 
second scenario scenario documents category considered relevant user interest documents irrelevant 
changes slowly concept drift batch batch documents category relevant irrelevant 
third scenario scenario simulates abrupt concept shift user interest category category batch back category batch 
experiments conducted machine learning environment yale fischer 
time window example selection approaches support vector machines core learning algorithm 
chose svm implementation ping 
linear kernels standard kernel type text classification problems complex kernel types usually perform better text classification tasks linear kernel types tried 
preliminary experiments determine value capacity constant svm allows trade model complexity versus gener trial documents randomly selected texts considered 
yale cs uni dortmund de yale sf net www ai cs uni dortmund de software relevance topic scenario scenario scenario batch relevance trec topic concept change scenarios relevance trec topic relevance topic 
relevance topics zero 
table error time window example selection methods scenarios averaged trials batches 
full fixed adaptive batch memory memory size size selection 


alization testing values value chosen experiments described 
experimental results table shows results static adaptive time window batch selection approaches scenarios terms prediction error 
adaptive time window approach batch selection strategy clearly outperform trivial non adaptive approaches 
example selection strategies batch selection performs better adaptive time window approach especially scenario table shows initial concept reflecting user interest topic shortly interrupted concept shift topic returns topic 
batches second concept shift scenario adaptive time window capture data second concept shift exclude longer representative data concept shifts batch selection strategy earlier data time concept shift selectively exclude longer relevant batches concept shifts 
allows batch selection maintain larger consistent training set better generalize resulting lower error rate 
results non adaptive approaches full memory memory fixed size see error lowest time window contains time points current model 
example scenario depicted long concept drift occurs full memory approach lowest error 
immediately concept drift memory approach error memory full memory fixed size batch classification errors trivial approaches batch scenario averaged runs 
error adaptive window batch selection batch classification errors batch selection approaches batch scenario averaged runs 
quickly returns previous error level fixed size memory approach takes longer reaches lower error memory approach 
full memory approach time points concept shift error rate times higher error strategies 
course findings surprising 
practical non adaptive approaches useful determined concept shift occur optimal static time window set 
longer time window lower error classifier achieve concept drift occurs shorter time window faster adjust new concept 
general balancing extremes full memory fixed size approach best 
adaptive window batch selection approach adjust concept drift 
scenarios error rate quickly reaches prior level concept drift occurred 
scenario batch selection approach outperforms adaptive window method flexible way selecting final training set allows exclude outlier batches data adaptive window method information concept shift includes outliers middle 
summing batch selection strategy achieves lowest error tested approaches 
explanation may outliers relatively seriously hurt performance svm classification 
special properties text data high dimensionality linear separability easy identify large groups outliers batch selection method reliably choose largest possible set training examples useful construct final hypothesis 
detailed description results including plots showing performance different approaches selected window sizes time klinkenberg joachims klinkenberg ping klinkenberg 
concept drift handling meta learning results allowing concept drift framework choose different base learners support vector machines svms decision tree learners weka nearest neighbor learning comparing performance approaches base learner selection meta learning klinkenberg 
evaluation real world concept drift problem economics predicting phases business cycles second evaluation domain task economics real world data 
quarterly data describes west german business cycles nch nch 
examples described indicator variables 
task predict current phase business cycle 
nch model phases business cycle cycle consists lower turning point upper turning point swing turning points cover month theis shown clustering analysis west german macro economic data clusters identified theis 
clusters roughly correspond cycle phases eventual third cluster corresponds period oil crisis 
suggests phases may suitable description business data 
linear discriminant analysis baseline model achieves accuracy uni variate rules morik ping phase model data set sophisticated statistical models achieve accuracy 
incorporate economic background knowledge business cycle analysis advanced markov switching models express knowledge past transition probability phase 
morik ping applied inductive logic programming approach domain knowledge data set achieved accuracy phase model phase model morik comparison results reported section accuracy obviously correspond classification error rate 
result obtained leave cycle validation data cycles violating timely ordering data 
experiments described preserve timely ordering learners obviously data learn 
table error time window example selection methods splits business cycle data batches respectively 
full fixed adaptive batch memory memory size size selection batches batches ping morik ping 
experiments phase model mapping time points classified upper turning point quarters year classified lower turning point 
background domain knowledge 
timely order examples quarters preserved artificial concept drift simulated 
questions interest domain exhibits concept drift behavior approaches able handle 
experimental results experiments performed machine learning environment yale underlying learner 
evaluations performed splitting data batches equal size splitting batches equal size 
splits change timely order example impose artificial concept drift 
results evaluations shown table 
results fixed time window approach correspond results fixed size performed best result offline parameter optimization considering performance batches 
practice optimal fixed size known advance particular point time batch 
real application guess size advance experience adaptive time window approach batch selection approach able adapt automatically online 
optimal results fixed size approach provided comparison order show performance approach maximally reach theoretically 
results tell domain 
simple fixed window size approach significantly outperforms full memory approach may conclude domain exhibits real concept drift behavior 
learning available training data allow better generalization lead better results 
closer inspection results adaptively chosen window sizes suggests data early years business cycles follow slightly rules years 
automatically chosen window sizes tend exclude batches 
results tell concept drift handling approaches 
obviously adaptive time window approach batch selection approach handle concept drift 
fixed size time window approach theoretically compete optimal fixed time window size surprising generalization performance 
table error fixed time window method fixed window sizes batches splits business cycle data batches respectively 
fixed fixed fixed fixed memory size size size size batch batches batches batches batches batches batches known advance usually possible realworld application 
shown table fixed window sizes leads significant drops performance error levels adaptive approaches 
fact fixed size approach competitive domain may due cyclic nature domain 
chosen time window length business cycles sufficient generalization result sufficiently early drop data years significantly reduce performance larger time windows full memory approach obey somewhat rules years 
domain fixed window size optimized offline possible online application fixed window size guessed domain expert simple time window approach quite competitive online setting real world task adaptive concept drift adjustment techniques appropriate rely offline optimization expert guess adapt current concept drift automatically 
concept drift handling meta learning results allowing concept drift framework choose different base learners support vector machines svms decision tree learners weka nearest neighbor learning comparing performance approaches base learner selection meta learning klinkenberg 
summary proposed meta learning framework handling concept drift automatically selecting promising base learner promising parametrization best set examples learn step time estimated generalization error minimized 
framework expected improve accuracy learning system selecting appropriate learner globally times step time meta level optionally speed meta level process reducing search space step time meta learning 
restricting meta level search space may course lead decrease accuracy 
experiments real world data mentioned domains simulated real world concept drifts respectively comparing performance approaches base learner selection meta learning technical report author appear november klinkenberg 
allan 

incremental relevance feedback information filtering 
proceedings nineteenth acm conference research development information retrieval pp 

new york ny usa acm press 
balabanovic 

adaptive web page recommendation service 
proceedings international conference autonomous agents pp 

new york ny usa acm press 
fischer klinkenberg 

yale learning environment tutorial technical report ci nd edition 
collaborative research center computational intelligence sfb university dortmund dortmund germany 
issn yale cs uni dortmund de 
nch 

forecasting stage business cycle technical report 
institut rwi essen germany gerhard mercator university duisburg duisburg germany 
project link meeting rio de janeiro september 
nch 

classification west german business cycles technical report 
collaborative research center reduction complexity multivariate data sfb university dortmund germany 
helmbold long 

tracking drifting concepts minimizing disagreements 
machine learning 
joachims 

estimating generalization performance svm efficiently 
proceedings seventeenth international conference machine learning icml pp 

san francisco ca usa morgan kaufmann 
klinkenberg 

zum bei sich ver 
masters thesis computer science department university dortmund germany 
klinkenberg 

predicting phases business cycles concept drift 
der gi workshop lernen wissen pp 

karlsruhe germany 
klinkenberg 

learning drifting concepts example selection vs example weighting 
intelligent data analysis ida special issue incremental learning systems capable dealing concept drift 
volume number 
klinkenberg 

meta learning concept drift handling technical report 
computer science department university dortmund dortmund germany 
technical report appear november 
klinkenberg joachims 

detecting concept drift support vector machines 
proceedings seventeenth international conference machine learning icml pp 

san francisco ca usa morgan kaufmann 
klinkenberg renz 

adaptive information filtering learning presence concept drifts 
workshop notes icml aaai workshop learning text categorization held fifteenth international conference machine learning icml pp 

menlo park ca usa aaai press 
klinkenberg ping 

concept drift importance examples 
franke renz eds text mining theoretical aspects applications 
heidelberg germany physica verlag 
kuh petsche rivest 

learning timevarying concepts 
advances neural information processing systems pp 

san mateo ca usa morgan kaufmann 


und zur data mining 
masters thesis computer science department university ulm germany 
lang 

newsweeder learning filter netnews 
proceedings twelfth international conference machine learning icml pp 

san francisco ca usa morgan kaufmann 


dynamic neural classification 
masters thesis computer science department university braunschweig germany 
klinkenberg fischer 

flexible platform knowledge discovery experiments yale learning environment 
lernen wissen proceedings workshop special interest groups machine learning knowledge discovery data mining intelligent tutoring systems adaptivity user modeling interactive systems german computer science society gi 
university karlsruhe karlsruhe germany 
short version 
klinkenberg fischer 

flexible platform knowledge discovery experiments yale learning environment technical report 
collaborative research center computational intelligence sfb university dortmund dortmund germany 
issn 
long version 
yale cs uni dortmund de 
mitchell caruana freitag mcdermott zabowski 

experience learning personal assistant 
communications acm 
morik ping 

inductive logic programming approach classification phases business cycles technical report 
collaborative research center reduction complexity multivariate data sfb university dortmund dortmund germany 
morik ping 

multistrategy approach classification phases business cycles 
machine learning ecml pp 

berlin germany springer 
klinkenberg fischer 

yale machine learning environment 
der gi workshop lernen wissen pp 

university dortmund dortmund germany 
technical report issn 
yale cs uni dortmund de 
ping 

manual 
artificial intelligence unit computer science department university dortmund germany 
www ai cs uni dortmund de software 
salton buckley 

term weighting approaches automatic text retrieval 
information processing management 


incorporating background knowledge better prediction cycle phases 
workshop notes ijcai workshop learning temporal spatial data pp 

menlo park ca usa aaai press 
held conjunction international joint conference artificial intelligence ijcai 
taylor 

structural change classification 
workshop notes ecml workshop dynamically changing domains theory revision context dependence issues held ninth european conference machine learning pp 

theis 

clustering techniques detection business cycles technical report 
collaborative research center reduction complexity multivariate data sfb university dortmund dortmund germany 


einsatz eines zur eines pers 
master thesis computer science department university dortmund germany 
widmer 

tracking context changes metalearning 
machine learning 
widmer kubat 

learning presence concept drift hidden contexts 
machine learning 
