auc maximizing support vector learning ulf informatik hu berlin de tobias scheffer scheffer informatik hu berlin de humboldt universit zu berlin department computer science unter den linden berlin germany area roc curve auc natural performance measure goal find discriminative decision function 
rigorous derivation auc maximizing support vector machine optimization criterion composed convex bound auc margin term 
number constraints optimization problem grows quadratically number examples 
discuss approximation large data sets clusters constraints 
experiments show auc maximizing support vector machine fact lead higher auc values 

receiver operating characteristics roc analysis recognized useful tool machine learning allows assess uncalibrated decision functions prior distribution classes known provost bradley 
decision function compared threshold yield classifier 
roc curve details rate true positives false positives range possible threshold values 
area curve probability randomly drawn positive example higher decision function value random negative example called auc area roc curve 
goal learning problem find decision function high auc value natural learning algorithm directly maximizes criterion 
years auc maximizing versions learning algorithms developed 
contribute field characterizing appearing proceedings icml workshop roc analysis machine learning bonn germany 
copyright author owner 
auc maximizing support vector machine svm 
construct appropriate convex optimization criterion derive corresponding dual quadratic programs solved standard qp solvers 
study empirically auc maximizing svm optimizes auc effectively regular svm 
experiments cover linear polynomial kernels 
number constraints parameters problem grows quadratically sample size 
increases relative contribution slack terms optimization criterion study influences optimal value regularization parameter 
large samples optimization problem hard 
approximate optimization problem clustering resulting constraints 
rest structured follows 
introduce problem setting section derive auc maximizing svm section 
section apply clustering methods reduce number constraints optimization problem 
experiments section discuss related section 
section concludes 

preliminaries roc curve decision function characterizes possible trade false positives true positives achieved comparing threshold 
roc curve plots number true positives axis number false positives axis 
area roc curve auc equal probability decision function assigns higher value randomly drawn positive example randomly drawn negative example auc 
auc refers true distribution positive negative instances estimated sample 
normalized wilcoxon mann whitney statistic yan gives maximum hood estimate true auc equation positive negative examples auc 
sums equation iterate pairs positive negative examples 
pair satisfies contributes auc performance 
maximizing auc equivalent maximizing number pairs satisfying 
auc invariant respect priori class distribution independent decision thresholds remainder analyze auc maximization model 
transformation denotes implicit preprocessing input data equivalent mapping input space feature space explicit representation required 
kernel learning algorithms direct computation inner product kernel function suffices learn feature space refer matrix elements kij xi xj 

auc support vector learning section auc maximizing svm auc svm 
derive optimization problem objective function consists upper bound number examples violate constraint margin complexity term 
second step complete derivation auc maximizing svm primal dual 
characterize relationship linear auc svm vanilla svm 
key concept auc support vector learning observation auc performance depends directly number pairs 

satisfy equivalently 
introduce margin confidence measure actual solution equation obtain hard margin auc optimization problem 
optimization problem positive negative examples maximize subject constraints 
geometrical margin maximized fixing const maximizing functional margin fixing const minimizing norm analogy kernel methods apply set 
solution hard margin optimization problem solution regular hard margin svm optimization problem satisfies auc training data equation true mini maxj 
general allow pairwise relaxations margin nonnegative slack variables ij leading soft margin optimization problem 
optimization problem positive negative examples minimize ij subject constraints ij ij 
optimization problem convex quadratic quadratic objective linear constraints 
regularization constant determines trade complexity term sum slacks 
theorem may interpreted upper bound number pairs satisfy equation 
theorem optimization problem sum ij upper bounds number pairs sample violate 
proof 
set constraints optimization problem guarantees equation 
pair violates equation follows pair contributes sum ij ij ij ij ij ij nonnegative guaranteed nonnegativity constraints ij ij greater equal number pairs vio 
lation constraints optimization problem integrated objective function introducing lagrange multiplier constraint 
describe derivation norm norm auc maximizing svm respectively 
norm auc svm norm auc maximizing svm transform optimization problem dual representation resolve primal lagrangian ij ij ij ij ij ij xi xj ij ij optimization problem convex karush kuhn tucker kkt conditions necessary sufficient solution 
kkt conditions ij xi xj ij ij ij ij xi xj ij ij ij ij ij xi xj ij ij ij 
substitution equation primal lagrangian removes dependence primal variables resolve corresponding dual ij uv xi xj xu xv ij uv ij ij ij ij uv kiv 
ij ij uv equations enforce ij derivative ij occur dual bounds ij implicitly box ij define kernel ij uv ij uv kiv derive norm auc maximizing support vector optimization problem 
optimization problem positive negative examples ij maximize subject constraints ij ij ij uv ij uv ij uv ij optimization problem paves way implementation auc maximizing norm svm 
solved standard qp solver 
kkt complementarity conditions equation guarantee sparsity optimal ij xi xj ij enforces ij 
example pairs corresponding ij holds support vectors primal solution written linear combination ij xi xj 
ij ij solution geometric margin norm auc svm may drop nonnegativity constraints ij optimization problem ij satisfies xi xj ij ij guarantees objective positive 
primal lagrangian ij ij ij xi xj ij ij omit complete presentation kkt conditions similar norm case proceed directly optimal primal variables ij xi xj ij ij ij 
derive corresponding dual primal variables 
dual ij uv kiv ij ij uv ij ij ij 
summand equation may integrated kernel denotes identity 
norm auc maximizing support vector optimization problem stated optimization problem 
optimization problem negative examples ij maximize subject constraints ij ij ij uv ij 
ij uv ij uv optimization problem shows auc maximizing norm svm implemented standard qp solver 
resulting decision function written dual coordinates ij xi xj ij ij analogy regular svm theorem characterizes relation vanilla svm auc maximizing support vector learning case linear kernel 
intuitively linear auc maximizing svm emulated regular svm threshold new training set constructed consists positive exam ples zij analogy hold nonlinear kernels 
theorem case linear kernel optimization problem class svm training examples 
zn zij equivalent optimization problem 
proof 
soft margin optimization problem svm min ij yij zij ij ij substitution zij optimization problem min yij leads ij ij ij choice implies exactly optimization problem 
linear auc svm simple geometric interpretation 
primal weight vector normal hyperplane passes origin 
training procedure adjusts hyperplane margin vectors zij maximized 
implementation execution time optimization problems solved standard qp solver 
theorem reveals main disadvantage auc maximizing svm methods criterion wilcoxon mann whitney statistic 
number constraints parameters optimization problem grows quadratically number examples 
execution time svm training estimated 
number constraints parameters grows quadratically number examples table 
approximate linear auc maximizing svm 
input positive negative examples tradeoff parameter 
generate zij results vectors 

clustering employ linear clustering procedure find clusters 
cu cluster centers 

define kernel matrix ku cu cv 
solve optimization problem norm svm norm svm learn class svm cu positive examples 
return decision function 
training auc svm incurs execution time roughly 
execution time feasible small samples unsatisfactory large databases 
discuss number constraints reduced clustering section 
approximate auc maximization approximate execution time acceptable small samples unsatisfactory large databases 
study method reduces number constraints clustering 
method requires linear kernel 
order achieve complexity reduce number constraints parameters primal coordinates constraint pair instances equation 
linear case simplifies equation 
ij ij strategy approximating problem represent pairs cluster centers reduce number constraints parameters 
table details method 
execution time generation pairs clustering method consecutive optimization procedure auc svm 
execution time back 
method feasible linear kernels calculate differences instances requires explicit representation 
means algorithm meets requirements 
implemented converges usually scans data set 
fast exact means algorithm estimates initial cluster centers small sample 
starting approximate clusters algorithm computes exact cluster centers passes entire data 
exact refers cluster centers result regular means algorithm 

empirical studies study regular auc maximizing support vector machine maximizes auc effectively 
investigate execution time explore benefit means auc svm 
focus problem domains svm known solution auc frequently evaluation metric 
select text classification reuters corpus linear svm hand written character recognition mnist benchmark data set polynomial kernel 
experimental setting follows 
variation bootstrapping allows vary training sample size 
separate frequent classes reuters corpus complementary class 
draw specified number training examples distinct holdout examples replacement random iteration 
average performance holdout set iterations 
discriminate hand written digits mnist data set similar pair characters 
draw examples training part holdout examples testing part written distinct authors 
average iterations 
error bars indicate standard error 
qp solver loqo implementation alex smola nonlinear svm light linear case 
auc svm maximize auc better regular svm adjusted 
explore space values parameter training examples 
shows results reuters examples mnist training instances 
summarizes parameter value maximizes average auc studied reuters problems studied sample sizes 
auc svm requires regular svm roughly 
problem studied sample size fix apparently optimal methods re trade average optimal trade parameter means auc svm auc svm svm sample size 
optimal trade parameter averaged reuters problems 
evaluate auc parameter settings averaging iterations distinct resampled training holdout samples 
compare equally parameter values regular auc maximizing svm 
table compares resulting auc values svm regular svm examples 
cases reject null hypothesis methods perform equally confidence level favor auc svm 
details auc regular auc maximizing svm optimized trade parameter various sample sizes 
problems sample sizes conduct sided test confidence level 
single case regular svm significantly better comparisons auc svm performs significantly better svm 
table 
auc reuters examples optimal auc svm svm crude earn grain interest money fx compares auc mnist polynomial kernel degree training examples 
table details observed auc values 
auc svm appears perform better results significant 
experiments reuters mnist problems conclude average auc svm achieves higher auc regular svm 
addition auc svm requires trade parameter adjusted roughly optimal value svm 
auc auc auc acq auc svm svm trade earn auc svm svm trade polynomial kernel degree trade auc auc crude trade interest auc svm svm trade auc svm svm 
exploration trade values reuters examples 
auc svm svm auc polynomial kernel degree trade auc svm svm 
exploration trade values mnist vs examples 
table 
auc mnist examples optimal auc svm svm effective approximate optimization clustering 
compares regular svm auc svm approximate auc svm means clustering 
conduct sided tests level compare means auc svm regular svm 
means svm beats regular svm significantly cases regular svm significantly better case 
conclude average means auc svm achieves higher auc regular svm 
addition suggests optimal regularization parameter lies regular auc svm 
auc auc auc grain trade money fx auc svm svm trade auc svm svm polynomial kernel degree trade auc svm svm execution time auc svm means auc svm compare regular svm 
compares execution time regular svm auc svm approximate auc svm fast kmeans pass data 
shows averaged execution time iterations examples fitted polynomials 
auc svm slow large samples fitted curve term 
means approximation substantially faster fitted curve quadratic sample size regular svm quadratic sample size computing kernel matrix quadratic linear term dominates observed curve 
regular svm faster means auc svm single pass data 

related roc analysis widely radar technology egan psychology swets medicine auc auc time seconds acq auc svm svm means auc svm sample size earn auc svm svm means auc svm sample size auc auc crude auc svm svm means auc svm sample size interest auc svm svm means auc svm sample size 
auc svm vs regular svm reuters optimal trade value execution time earn auc svm svm means auc svm sample size 
execution time 
pattern recognition bradley machine learning provost 
provost fawcett argue roc analysis useful instance class specific costs misclassification known class distribution unknown skewed 
roc analysis helpful visualize understand relationship various possible evaluation metrics flach rnkranz flach 
learning goal maximize auc performance appears appropriate employ algorithms directly maximize criterion instance error rate 
auc maximizing variants learning methods developed including decision trees ferri rules fawcett boosting freund cortes mohri logistic regression raskutti subgroup discovery kav sek 
auc auc grain auc svm svm means auc svm sample size money auc svm svm means auc svm sample size light previous auc maximizing kernel classifier overdue 
rigorous derivation auc svm fact maximizes auc effectively svm 
previous efforts implement auc maximizing kernel classifier far lead methods worse maximizing auc regular svm 
classification goal auc maximization seen special case ordinal regression herbrich quadratic optimization problem encounters ordinal regression correspondingly resembles optimization problem 
auc maximization problems quadratically terms contribute optimization function 
random heuristic sampling proposed solution strategy raskutti 
large databases execution time regular svm unpleasant clustering proposed strategy reducing number optimization constraints parameters yu 

developed auc maximizing kernel machine optimizes bound auc margin term 
starting optimization criterion derived corresponding convex quadratic optimization problems norm norm machines handled standard qp solvers 
experiments different types kernels show auc svm generally incurs higher auc performance regular svm 
optimal setting trade parameter smaller regular svm 
execution time auc svm approximately 
direct auc maximization feasible small samples large databases approximated 
approximate means auc svm effective maximizing auc svm linear kernels 
execution time quadratic sample size empirically observe regular svm substantially faster 
roc analysis extended multi class problems volume multi class roc surface approximated averaging multiple versus hand till rest curves provost domingos 
directly maximize criteria auc svm applied corresponding binary versus versus rest problems 
acknowledgment supported german science foundation dfg 
bradley 

area roc curve evaluation machine learning algorithms 
pattern recognition 
cortes mohri 

auc optimization vs error rate 
advances neural information processing systems 
egan 

signal detection theory roc analysis 
academic press 
fawcett 

rule sets maximize roc performance 
proceedings international conference data mining 
ferri flach hernandez 

learning decision trees area roc curve 
proceedings international conference machine learning 
flach 

geometry roc space roc understand machine learning metrics 
proceedings international conference machine learning 
freund iyer schapire singer 

efficient boosting algorithm combining preferences 
proceedings international conference machine learning 
rnkranz flach 

roc rule learning better understanding covering algorithms 
machine learning press 
jin agrawal 

fast exact core means clustering 
proceedings ieee international conference data mining 
hand till 

simple generalization area roc curve multiple class classification problems 
machine learning 
herbrich graepel obermayer 

support vector learning ordinal regression 
proceedings international conference artificial neural networks 
raskutti 

optimizing area curve 
proceedings international conference machine learning 
kav sek lavra todorovski 

roc analysis example weighting subgroup discovery 
proceedings workshop roc curves ai 


signal detectability medical decision making 
science 


way 
medical decision making 
provost domingos 

tree induction probability rankings 
machine learning 
provost fawcett 

robust classification imprecise environments 
machine learning 
provost fawcett kohavi 

case accuracy estimation comparing inductive algorithms 
proceedings international conference machine learning 


optimizing auc svms 
proceedings workshop roc curves ai 
swets 

signal detection theory roc analysis psychological diagnostics collected papers 
lawrence erlbaum publishers 
yan mozer 

optimizing classifier performance wilcoxon mann whitney statistics 
proceedings international conference machine learning 
yu yang han 

classifying large data sets svms hierarchical clusters 
proceedings acm sigkdd international conference knowledge discovery data mining 
