simple method estimating conditional probabilities svms support vector machines svms popular learning algorithm particular large high dimensional classification problems 
svms shown give accurate classification results variety applications 
methods proposed obtain classification estimate svms confidence correctness predicted label 
algorithms compared scale svm decision function obtain estimate conditional class probability 
new simple fast method derived theoretical arguments empirically compared existing approaches 
support vector machines svms popular learning algorithm particular large highdimensional classification problems 
svms shown give accurate classification results variety applications 
methods proposed obtain classification estimate svms confidence correctness predicted label 
usually performance classifier measured terms accuracy performance measure comparison classifiers prediction true class 
cases give sufficient information 
example credit card fraud detection usually negative positive examples optimal classifier may default negative classifier 
find transactions probably fraudulent probability small 
situations information retrieval interested ranking examples respect interestingness simple decision 
third may interested integrate classifier bigger system example multi classifier learner 
combine compare svm prognosis learners comparable defined confidence estimate 
best method achieve confidence estimate allows rank examples gives defined interpretable values estimate conditional class probability 
obviously complex problem finding classification possible get classification function comparing threshold vice versa 
stefan ping cs department ai unit dortmund university dortmund germany ls cs uni dortmund de numerical classifiers classifiers type numerical decision function usually tries estimation conditional class probability decision function 
reduces probability estimation multi variate dimensional problem find scaling function 
idea approach classification examples lie close decision boundary easily change examples randomly perturbed small amount 
hard examples high low argument requires sort continuity differentiability constraints function 
probability classifier correct higher larger absolute values 
noted platt means strong prior selecting monotonic scaling function 
rest organized follows section shortly support vector machine kernel logistic regression algorithm far necessary 
section existing methods probabilistic scaling svm outputs discussed new simple scaling method 
effectiveness method empirically evaluated section 
algorithms support vector machines support vector machines classification method statistical learning theory 
goal find reg term function minimizes expected risk learner minimizing regularized risk reg weighted sum empirical risk respect data complexity optimization problem efficiently solved dual formulation kernel trick inner product equation replaced kernel function corresponds inner product space called feature space 
exists mapping 
allows construction non linear classifiers essentially linear algorithm 
resulting decision function actual svm classification 
shown svm solution depends support vectors sv 
see detailed svms :10.1.1.117.3731
kernel logistic regression kernel logistic regression kernelized version known logistic regression technique :10.1.1.28.8322:10.1.1.20.400
optimization problem similar svm problem equation exponential loss function loss svm problem solved dual formulation contrast svm kernel logistic regression directly models conditional class probability estimated drawback klr typically nonzero examples play role estimating conditional class probability svm small number support vectors needed classify examples 
klr computationally expensive svm 
probabilistic scaling support vector machines easily see svm decision function gives feature space distance transformed example hyperplane defined 
assuming continuous reasonable examples lying closer hyperplane larger probability misclassified examples lying far away closer example hyperplane smaller changes produce different classification 
suitable model conditional class probability function value svm decision function appropriate scaling function 
ad hoc scaling functions softmax scaler softmax monotonously maps decision functions value interval 
scaler assumes decision function type classifiers class decision smallest mapped conditional class probability 
allows view softmax probability 
mapping founded scaled values justified data 
justify interpretation better data calibrate scaling 
subset data training cross validation approach optimize scaling function minimize error predicted class probability empirical class probability defined class values new data 
error measures usually cross entropy mean squared error 
cross entropy defined kullback leibler distance predicted empirical class probability 
comparison different data sets better divide cross entropy number examples mean cross entropy 
mean squared error defined appropriate error measure binary random variable expected value minimized 
task estimating conditional class probability regression task 
open question types scaling functions fitted data 
motivated empirical analysis platt uses scaling functions form obtain monotonically increasing function 
parameters minimization cross entropy error test set 
set efficient implementation see 
proposes method scales classification values beta distribution function parameters 
parameters selected test 
average value class identical classification performance classifier class error 
mean square minimized 
originally algorithm designed multiclass problems computes individual scaler predicted class 
binary problems better modify approach scaler generated 
avoids svm klr dimensional comparison svm klr predictions 
negatives examples drawn dots positive examples dots 
methods find class border svm prediction essentially constant outside 
klr correctly estimates higher confidences points nearer class centers 
discontinuities prediction changes class 
binning applied problem 
decision values discretized bins estimate conditional class probability counting class distribution single bins 
complicated approaches exists see ch 

theoretical limitations bartlett tewari show tradeoff sparseness classifier ability estimate conditional probabilities 
result says contains short able estimate interval sparseness lost region 
question arises far decision function svm generally produces sparse classifiers approximate true conditional density estimate non sparse klr respectively 
problem seen equation 
obtain maximally accurate classifier svm objective function classifier punished support vector 
case forces ordering values value higher similar example rest examples class feature space 
consequently estimation constructed 
example classified correctly sufficient margin example generates loss specific order enforced examples 
svm examples right side margin probability 
behavior seen 
said support vectors 
previous section saw minimizing mean squared error estimation function gives proper estimate fixed mse minimized 
error criterion svm absolute error squared error show fixed absolute error minimized iff 
comes rescue determined independently 
overfitting occurs value indicator plausible contains useful information 
simple estimation method previous discussion know decision function value unreliable estimating conditional class probability 
iff values directly optimize order examples respect 
question arises possible estimate trivial procedure iff iff fraction positive examples fraction positive examples svm function simply linearly scaled 
similarly define clipping advantage method compared existing approaches requires training applying classifier counting probabilities gives reasonable empirically founded probability estimates 
experiments experiments conducted data sets including data sets uci repository covtype diabetes digits digits ionosphere liver mushroom promoters real world data sets business cycle analysis problem business analysis direct mailing application data set life insurance insurance intensive care patient monitoring data medicine 
prior learning nominal attributes attributes scaled expectancy variance 
multi class problems converted problems arbitrarily selecting classes covtype digits combining smaller classes single class business medicine 
covtype data set sample drawn 
table sums description data sets name size dimension covtype diabetes digits ionosphere liver mushroom promoters business insurance medicine experiments support vector machines kernel logistic regression linear radial basis kernel 
parameters algorithms selected prior step optimize accuracy 
algorithms compared experiments klr kernel logistic regression baseline 
svm platt svm platt scaling 
svm beta svm beta scaling 
svm beta svm binary beta scaling 
svm bin svm binning 
svm softmax svm softmax scaling 
svm svm output clipped 
svm pp svm output clipped 
reported results fold cross validated 
linear svm klr results obtained method mse klr svm platt svm beta svm beta svm bin bins svm bin bins svm softmax svm svm pp svm svm softmax klr svm bin respect mean squared error get ranking svm platt svm beta svm pp svm bin svm beta 
sorting mean svm beta svm pp change places svm softmax bin 
rbf kernel gave results method mse klr svm platt svm beta svm beta svm bin bins svm bin bins svm softmax svm svm pp gives ranking mse klr svm platt svm beta svm pp svm svm bin svm softmax svm bin svm beta 
close inspection reveals results give full picture error measures reach different values individual data sets 
mse kernel logistic regression radial basis kernel runs mushroom liver 
allow better comparison methods ranked performance data set 
table gives average rank methods linear kernel avg 
rank method mse klr svm platt svm beta svm beta svm bin bins svm bin bins svm softmax svm svm pp corresponding table radial basis kernel avg 
rank method mse klr svm platt svm beta svm beta svm bin bins svm bin bins svm softmax svm svm pp validate significance results paired test run cross validation runs 
table shows comparison cross entropy linear kernel best scaling algorithms 
row table shows hypothesis estimation row better estimation corresponding column rejected 
row column shows hypothesis softmax scaling better klr rejected data sets 
contrary hypothesis rejected data sets row column 
klr platt beta pp bin soft klr platt beta pp bin soft results cross entropy radial basis kernel klr platt beta pp bin soft klr platt beta pp bin soft corresponding tables mse show similar results 
summing see kernel logistic regression give best estimation conditional class probability outliers linear case 
best scaling svm obtained platt method binary beta scaling 
trivial pp scaling performs comparable complicated techniques 
multiclass beta scaling gives far worst results expected non method scaling predicted class 
summary experiments showed trivial method estimating conditional class probability output svm classifier performs comparably complicated estimation techniques 
acknowledgments financial support deutsche forschungsgemeinschaft sfb reduction complexity multivariate data structures gratefully acknowledged 
peter bartlett tewari 
sparseness vs estimating conditional probabilities asymptotic results 
submitted 
burges 
tutorial support vector machines pattern recognition 
data mining knowledge discovery 
joseph 
obtaining calibrated probability estimates support vector machines 
technical report university california san diego june 

classification rules standardized partition spaces 
phd thesis universit dortmund 
jaakkola haussler 
probabilistic kernel regression models 
proceedings conference ai statistics 
keerthi duan poo 
fast dual algorithm kernel logistic regression 
submitted publication machine learning 
james tin yau kwok 
outputs support vector machine classifiers 
ieee transactions neural networks september 

lin 
lin weng 
note platt probabilistic outputs support vector machines may 
murphy aha 
uci repository machine learning databases 
john platt 
advances large margin classifiers chapter probabilistic outputs support vector machines comparisons regularized likelihood methods 
mit press 
volker roth 
probabilistic discriminative kernel classifiers multi class problems 
editors pattern recognition dagm number lncs pages 
springer 
vapnik 
statistical learning theory 
wiley chichester gb 
grace wahba 
advances kernel methods support vector learning chapter support vector machines reproducing kernel hilbert spaces randomized pages 
mit press 
ji zhu trevor hastie 
kernel logistic regression import vector machine 
neural information processing systems volume 
