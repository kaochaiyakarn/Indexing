case study building layered dht applications chawathe chawathe intel com anthony lamarca anthony lamarca intel com intel research seattle research shown distributed hash tables dhts build scalable robust efficient applications 
question left unanswered simplicity implementation deployment 
explore case study building application ease deployment dominated need high performance 
application focus place lab user positioning system 
evaluate feasible dhts application independent building block implement key component place lab mapping infrastructure prefix hash trees data structure place lab geographic range queries built entire top standard dht 
strictly layering place lab data structures top generic dht service able decouple deployment management place lab underlying dht 
identify characteristics place lab amenable deploying layered manner comment effect performance 
categories distributed systems general terms design algorithms experimentation keywords dhts layering range queries 
distributed internet scale applications typically designed scalability availability robustness mind 
issue frequently overlooked simplicity implementation deployment 
practice equally important difficult challenge 
particularly true peer peer systems highly distributed location administration 
describes design evaluation application concerns ease operation dominated need high performance 
application place lab user positioning service location enhanced applications 
place lab clients estimate physical location listening nearby radio beacons access points gsm cell towers conjunction database known beacon locations 
beacon database initially designed centralized mapping service permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
sigcomm august philadelphia pennsylvania usa 
copyright acm 
sriram sriram cs ucsd edu scott shenker shenker icsi berkeley edu uc san diego icsi uc berkeley sylvia ratnasamy sylvia ratnasamy intel com joseph hellerstein joseph hellerstein intel com intel research berkeley system gained popularity march users downloaded place lab software unique locations concerns privacy ownership beacon database required transitioning decentralized architecture composed mapping servers distributed organizational domains 
place lab mapping service interesting case study place lab operators small group ubiquitous computing researchers wished limit implementation deployment overhead involved providing fully decentralized infrastructure 
ask question possible isolate place lab developers distributed application deployment management robustness concerns 
powerful design principle commonly simplify construction complex systems layering 
example strict layering ip tcp allows network handle complex operations packet delivery hosts deal reliability congestion control 
higher layer distributed hash tables dhts cited playing similar role building decentralized applications 
building application top dht frees designers having address issues scalability robustness directly 
approach viable greatly simplify building distributed applications 
cfs past examples applications straightforward dht simple rendezvous storage easy implement layered fashion 
hand systems mercury coralcdn sophisticated requirements achieve altering underlying dht mechanisms :10.1.1.102.473
place lab mapping service closer second category applications complex requirements simply storage rendezvous 
application interface geographic range queries exact match lookups 
place lab clients download relevant segments beacon database needed 
example user arrives new city device query mapping service beacon data region 
spite requirements easily layer place lab existing dht go long way simplifying implementation 
simplify operation service place lab operators deploy manage full fledged dht arguably easy task 
decided push notion layering step outsourced operation dht altogether third party dht service 
building top third party dht service restricts interaction application dht narrow defined api 
precisely conflicting needs building complex data structure having live narrow dht interface believe place lab admittedly harsh stress test claim dhts composable building block 
important question lot value dhts lie validation flexibility re usable programming platform large scale distributed applications 
describe design implementation place lab mapping service opendht service 
experience answer questions feasible simple dht service building block larger complex application 
application leverage purported simplicity deployability advantages dhts 
performance impact dhts application 
recognize single case study sufficient answer general question just broad class applications supported top strictly layered dht 
results provide initial insight requirements applications simple rendezvous storage impose dht infrastructures 
requirements arise real application actively members research community early adopters 
primary challenge address place lab need range queries modifying underlying dht 
solution called prefix hash trees phts distributed trie data structure built top generic dht 
simple pht perform single dimensional range queries extension linearization techniques allows perform multi dimensional queries specifically geographic range queries 
experience building place lab mixed results 
simple dht interface goes long ways supporting place lab non traditional varied dht 
building place lab dht relatively easy lines glue code system effortlessly inherited scalability robustness self configuration properties dht 
validate hope narrow waist networked systems 
simple dht put get remove interface quite 
opendht support atomicity primitives crucial correctness face concurrent updates 
simple atomicity primitive implemented application independent extension basic dht api possible third party dht implementation support primitives 
remain hopeful sophisticated applications layered top dht service think dht services slightly broaden interface 
return ease implementation deployment sacrificed performance 
opendht implementation pht query operation took median seconds 
layering entirely top dht service inherently implies applications perform sequence get operations implement higher level semantics limited opportunity optimization dht 
loss performance worthy tradeoff ease deployment individual application developers assess 
rest organized follows 
discuss related section 
section describes place lab requirements dht framework section presents details prefix hash tree data structure 
section discuss experimental results highlight lessons learned section conclude section 
related variety related dht applications techniques distributed range queries trie schemes networking 
place lab means application built dhts 
existing applications uses dhts traditional key lookup building block implementing data structure richer functionality pht retaining simple application independent api dht 
dht systems early significant class dht applications storage rendezvous systems including past oceanstore chord dhash layer example cfs ivy 
applications straightforward dht implementations decomposable underlying dht 
scribe dht topology construct trees multicast anycast aggregation 
pier uses dhts relational database file sharing queries extending dht basic put get semantics support query dissemination join aggregation operations 
lastly systems coralcdn post support large scale applications building custom dhts underneath 
distinguishes place lab applications strict layered approach building entirely top opendht service 
peer peer range queries years flurry providing peer peer range query functionality 
believe pht scheme describe stands built modifying internal routing topology dht 
clean layering easy implement third party dht infrastructures allows dhts support multiple functionalities tuned specifically range search 
comparison mercury system sword karger ruhl item balancing ganesan online balancing explicitly load balance distribution items including specific modifications behavior underlying dht :10.1.1.102.473
typically evolving applications data sets induce churn dht 
phts hand built entirely top existing dht rely spatial distribution data achieve load balancing 
aspnes shah proposed skip graphs distributed data structure implements range search 
awerbuch scheideler build skip graphs dht dht mechanism implement pointers skiplist structure 
maintaining load balance mapping items peers network requires non trivial extensions skip graphs 
contrast pht trie data structure simplicity allows simple realization network peers demonstrated 
related includes dht caching scheme tree special purpose range search structure technique specifically dht space filling curves 
trie peer peer systems cone dht inspired trie distributed data structure evaluate aggregation operators min max sum keys dht 
cone similar phts trie designed aggregation operations range queries 
comparison pht perform range queries easily capable evaluating aggregation operators elements satisfying range predicate 
grid dht peer peer lookup system uses trie approach core network randomized links 
quite different design spirit pht data structure layered top dht 
independent proposed trie scheme similar pht proposal 
explored basic concept pht built deployed real application 

place lab overview place lab radio beacon device positioning system runs commodity laptops pdas cell phones 
client devices listen broadcasts beacon messages nearby access points gsm cell towers 
estimate position looking beacons beacon database maps beacon identifiers location 
locally cached segments database clients position median accuracy meters depending beacon density 
input beacon database come organizations know locations access points war drivers drive neighborhood mobile computer equipped gps device card gathering traces beacon availability 
centralized beacon databases exist www net 
place lab infrastructure grows popular central authority beacon database raise numerous concerns privacy ownership access 
database critical clients compute location centralizing ways analogous single centralized dns server clients resolve dns names 
place lab researchers proposed decentralized architecture place lab mapper service number organizations mapping servers host portion beacon database 
divide database geographically raises concerns responsible high profile areas manhattan chose distribute data randomly servers making server responsible random portion beacon identifier space 
method eliminates need assign servers geography ensures robustness spreading data single region multiple random servers 
organization suited implementation top dht 
place lab requirements ease deployment built service top opendht third party dht service 
opendht provides simple put get remove interface applications 
applications put data dht time dht stores data ttl expires 
war drivers dht route data mapping servers 
mapping servers responsible aggregating information individual radio beacons generating single estimate beacon location 
addition build maintain geographical index access points simplify retrieval 
dht takes care robustness availability data 
rest section details separation concerns dht mapping servers achieved 
content routing processing war driving records single radio beacon independent beacons 
accordingly distribute mapping data data single beacon hosted deterministic mapping server war driving records beacon forwarded mapping server 
dhts provide natural mechanism achieving distribution 
map beacon identifiers sha keys 
mapping server responsible defined portion key space 
allow mapping servers register dht clients route war driving records appropriate mapping servers opendht mechanism 
maintains hierarchy rendezvous points allows clients look appropriate server records 
implemented entirely mapping servers clients simpler put get interface dht 
mapping server coalesces war driving records single radio beacon computes running estimates positions radio beacons manages 
periodically stores estimates dht keyed beacon identifier ensure availability 
effectively dht provides routing primitives clients locate mapping servers stores estimates beacons locations generated mapping servers 
indexing retrieval place lab client enters new area download beacon data new region 
involves performing geographical range query data 
allow arbitrarily complex query regions restrict queries rectangular bounding boxes 
underlying dht routing algorithm spreads beacon data uniformly mapping servers spatial locality locality important perform query efficiently 
prefix hash trees phts solution efficiently coalesce estimated positions nearby radio beacons 
mapping server updates estimate beacon location new war driving readings updates pht 
efficiency updates batched performed lazily 
discuss data structure implementation top opendht detail section 
deployability robustness availability opendht provides routing storage robustness substrate place lab 
individual mapping servers connect details described 
directly dht 
rely dht provide robustness availability 
servers store current estimates radio beacon location dht dht handles replication recovery 
mapping server fails dht routing mechanisms automatically ensure failed server successor routing overlay takes responsibility failed server key space 
mapping server administrator handle restarting failed server dht provides automatic graceful fail 
mapping servers periodically refresh data dht 
ensures event catastrophic failure dht replicas beacon data lost mapping servers eventually recover 
additionally temporary loss data affect application performance 
resilience due temporal spatial redundancy data 
effect lost information beacon reduced likelihood new war driver submit fresh information beacon eventually 
spatially impact lost beacons reduced readings nearby beacons map different servers 
show section loss beacon data results noticeable reduction positioning accuracy 

prefix hash trees look pht data structure detail 
various proposals incorporating range query support dhts prefix hash trees built entirely top simple put get remove interface run dht specifically third party dht service opendht 
range queries get key operation assume knowledge require changes dht topology routing behavior 
phts efficient updates doubly logarithmic size domain indexed 
selforganizing load balanced 
tolerate failures protect data loss nodes go failure node prefix hash tree affect availability data stored nodes 
phts take advantage replication data preserving technique employed dht 
data structure prefix hash tree assumes keys data domain expressed binary strings length fairly straightforward extend alphabets multiway indexing encoding binary 
pht essentially binary trie node corresponds distinct prefix data domain indexed 
node trie labeled prefix defined recursively node label left right child nodes labeled respectively 
root labeled attribute indexed downstream nodes labeled described 
node pht zero children 
keys stored leaf nodes 
binary search tree keys stored leaf node share leaf node label common prefix 
pht imposes limit number keys single leaf node store 
leaf node fills capacity split descendants 
similarly keys deleted pht sibling leaf nodes may merge single parent node 
result shape pht depends distribution keys deep regions longitude latitude recursive shape curve linearization map dimensional coordinate space onedimensional sequence 
shaded region represents dimensional range query data points space 
bold line represents corresponding dimensional range curve lowest highest linearization points original query 
domain densely populated conversely shallow regions domain sparsely populated 
described far pht structure fairly routine binary trie 
pht interesting lies logical trie distributed servers form underlying dht 
achieved hashing prefix labels pht nodes dht identifier space 
node label assigned dht server mapped dht hashing algorithm 
assignment implies pht node label possible locate dht single get 
direct access property successive link traversal associated typical tree data structures results pht having desirable features discussed section 
adapting phts place lab queries place lab performed dimensional latitude longitude coordinate domain longitude latitude 
index domain phts rely technique known linearization space filling curves map multi dimensional data single dimension 
known examples include hilbert gray code order curves 
normalize latitudes unsigned bit integer values turn represented simple binary format 
curve linearization technique map dimensional data point bit onedimensional key space 
curve linearization performed interleaving bits binary representation latitude longitude 
example normalized point represented curve bit key 
shows zig zag shape mapping takes dimensional coordinate space 
chose curves simple understand easy implement 
section compare performance various linearization techniques 
pht dimensional queries uses curve keys prefixes node labels 
due interleaving latitude longitude bits curve key successive level pht represents splitting geo pht node label lat lon lat lon binary curve key portion sample dimensional pht place lab 
table shows data items curve keys stored leaf nodes pht 
shown data item stored unique leaf node label prefix item curve key 
graphic space alternately latitude axis longitude axis 
data items tuples form latitude longitude beacon id inserted leaf node label prefix curve key associated latitude longitude coordinate 
shows sample pht example assignment data items pht leaf nodes assuming bit normalized latitude longitude values 
pht operations described pht data structure looks focus various operations needed build query data structure dht 
lookup lookup primitive implement pht operations 
key returns unique leaf node leaf label prefix lookup implemented efficiently performing binary search possible prefixes corresponding bit key 
important feature lookup traditional tree lookups require operation originate root reducing load root nodes close root 
minor modifications algorithm perform lookup prefix full length key binary search requires log log dht gets doubly logarithmic size data domain indexed 
ensures lookup operation extremely efficient 
binary search drawback fail result failure internal pht node 
search may able distinguish failed internal node case search proceed downwards absence pht node case search proceed upwards 
situation pht client restart binary search hope refresh operation repaired data structure see section perform parallel gets prefixes key parallel search guaranteed succeed long leaf node alive dht able route 
suggests alternative modes operation low overhead lookups binary search low latency fail lookups parallel search 
range query dimensional pht keys range query returns keys contained pht satisfying range query performed locating pht node corresponding longest common prefix performing parallel traversal subtree retrieve desired items 
multi dimensional range queries required place lab slightly complicated 
query matching data rectangular region defined lat min performed follows 
determine linearized prefix minimally encompasses entire query region 
done computing curve keys zmin zmax points query longest common prefix keys 
look pht node corresponding perform parallel traversal sub tree 
simpler case dimensional queries nodes leaf minimum key leaf maximum key contribute query result 
illustrated figures show query rectangular region 
shown linearized range points shown bold line passes points correspondingly pht nodes rectangular region search 
depicted pht representation 
leaves points query 
entire subtree rooted contain data items fall query range 
query algorithm works follows starting pht node corresponding determine node leaf node 
apply range query items node report result 
node interior node evaluate left subtree prefix contribute results query 
done determining overlap rectangular region defined subtree prefix range original query 
check performed additional gets incurs penalty fails 
overlap exists query propagated recursively left subtree 
parallel perform similar test right subtree prefix test succeeds propagate query sub tree 
query algorithm requires sequential steps depth tree 
insert delete insertion deletion key require pht lookup locate leaf node leaf 
insertion leaf node full limit values contains items latitude coordinates form xx form xx items range 
range overlap query range entire subtree discounted 
split children 
cases keys distributed children stores possible keys distributed child necessitating split 
avoid split operation determines longest common prefix keys creates new leaf nodes level deeper common prefix ensuring new leaves keys 
keys distributed new leaves nodes original node split new leaves marked interior nodes 
operations parallelized efficiency 
similarly key deleted pht may possible coalesce sibling leaf nodes single parent node 
merge operation essentially reverse splits performed lazily background 
refreshing recovering failure phts inherit resilience failure recovery properties underlying dht 
event catastrophic failure replicas dht pht lose data 
algorithms fairly resilient face loss interior pht nodes eventually restore lost data 
achieve rely soft state updates 
pht entry leaf node keys interior node markers associated time live ttl 
ttl expires entry automatically deleted dht 
mapping server periodically refreshes values inserted pht 
keys inserted dht ttl seconds 
seconds mapping server refreshes keys resetting ttl time checks parent leaf node 
parent ttl dropped seconds refreshes parent 
continues recursively reaches root parent ttl greater 
interior nodes refreshed needed 
interior node lost due failure eventually refreshed consequence refresh value descendant leaf nodes 
dealing concurrency pht described potential race conditions result temporary loss data duplication 
example mapping servers attempt insert keys leaf node leaf node full servers attempt split leaf node resulting duplicate 
worse race condition cause server insert operation get lost different server begun process splitting leaf node 
temporary problem refresh mechanisms described previous section eventually recover lost data 
inefficiencies occur pht implemented entirely outside dht independent mapping servers 
absence concurrency primitives dht eliminated 
added localized atomic test set mechanism opendht api 
note extension pht place lab specific potentially benefit distributed applications 
test works follows get key returns generation number key 
generation number updated dht key modified 
modification timestamp generation number 
addition implemented put conditional key value gen put succeeds key modified generation number gen implement concurrency primitive correctly presence replication failures dht provide strong guarantees atomic writes 
consensus protocol paxos provide guarantees 
protocol fairly involved significantly complicate dht implementation 
extension uses simpler mechanism works practice common case serialize put conditional operations master replica key 
event churn multiple dht nodes think master replica key mechanism fail 
events hopefully rare dht service mentioned earlier result inefficiency pht loss correctness 
primitives insert operation modified follows 
inserting key leaf put conditional primitive ensure leaf modified split performed lookup 
leaf node needs split mark transition put conditional primitive 
multiple servers attempt split node succeed 
pht nodes involved split operation marked transition 
split operation performed 
transition markers removed split operation completed 
caching improve performance lookup primitive central pht operations 
optimized client side hint cache keeps track shape pht previous lookup operations 
lookup key returns leaf node prefix cache records leaf node entries root parent interior nodes 
new lookup different key checked cached information 
cache returns leaf node client performs get verify pht reconfigured node leaf node 
cache hit generates single dht operation 
cache lookup revert binary search algorithm 
query operations similar caching scheme finds relevant leaf nodes directly querying cache 
number heuristics optimize performance phts 
example certain queries small range containing midpoint key space may desirable break search query treat sub queries independently 
ensure searches start level pht appropriate query smaller queries start lower pht 
optimization ary tree binary trees reduce number internal nodes queried 
phts versus linked data structures section compares merits pht indexes tree particular emphasis implementation distributed setting 
tree indexes may better traditional indexing applications databases argue reverse true implementation dht 
efficiency balanced tree height log number elements tree key lookup requires log dht lookups 
phts binary search lookup algorithm requires log dht operations number bits pht key 
load balancing lookup tree index go root creating potential bottleneck 
case phts binary search allows load spread nodes case uniform lookups eliminating bottleneck 
fault resilience typical tree structure loss internal node results loss entire subtree rooted failed node 
phts require topdown traversals directly jump node pht 
failure node pht affect availability data stored nodes 

evaluation measure performance dht implementation place lab mapping service 
main operations place lab performs routing beacon records war drivers mapping servers updating beacon position estimates routing beacon position estimates pht 
straightforward dht 
records hashed beacon identifier hash redirect dht mapping server 
accordingly focus measurement effort prefix hash tree mechanism way behaves insert loads mappings servers query loads downloading clients 
setup implemented phts rest place lab infrastructure top opendht 
implementation effort required build glue place lab application code underlying dht build robust pht implementation small 
code consists lines java 
comparison underlying opendht codebase lines code 
deployed run place lab mapping service pht top public planet lab opendht deployment 
experimental evaluation chose independent opendht deployment 
reasons understand effects concurrent operations needed enhanced apis put conditional evaluate effect churn wished kill restart opendht nodes needed 
deployment consisted nodes spread machines west coast east coast england 
conducted experiments larger deployment planetlab 
due vagaries load planet lab results experiments erratic left discussion 
input data set composed known locations access points gathered war driving community web service www net 
data set consists estimated ap positions war drives submitted users united states service 
shows distribution input data 
conducted experiments different data set sizes picked uniformly random larger set 
constructed query workload composed queries represent set typical place lab queries workload proportional distribution access points input data 
choice assumption distribution input data set 
intensity dots map corresponds density data points region 
structure pht input data set block size 
high access point density corresponds higher population density higher likelihood queries regions 
query generated picking access point random input data set building rectangular region location access point size picked uniformly random latitude longitude units approximately km 
query corresponds requests form hear access point find aps distance ap structural properties set experiments constructed phts progressively larger data sets measured structure resulting trees 
shows depiction pht entire data set block size overlaid top map 
rectangle map represents leaf node pht 
comparing input data set shown note areas high ap density get sub divided smaller rectangular blocks sparse areas 
constant block size pht 
organization ensures queries dense areas spread larger number dht nodes reducing bottleneck popular queries may cause 
measured tree characteristics metrics depth tree block utilization number elements pht leaf node percentage block size 
tree depth shows cdf depth leaf nodes pht elements block size 
th th percentiles tree depth varies 
nodes densest part data set higher depth deep small fraction nodes sparse parts country shallower 
shows variation average depth pht varying block sizes different input data set sizes 
see tree depth decreases cumulative leaf nodes node depth pht cumulative distribution function cdf leaf node depth pht input data set block size 
avg 
depth leaf node logarithmically block size larger block sizes result shallower trees 
larger blocks fewer accesses needed retrieve portion data space greater contention nodes pht 
shows expect tree depth increases increasing data set sizes 
obvious increase logarithmic 
block utilization experiment looks full leaf nodes percentage block size 
shows utilization function block size varying input data sizes 
plots input data size show block utilization high small block sizes 
drops block size increased eventually begins grow block size begins approach total input data size 
non uniformity input data results skewed distribution data leaf nodes causes average leaf utilization lower data uniformly distributed 
non uniform data small block sizes blocks fill capacity utilization cases high 
large block sizes comparable input data set size tree shallow non uniformity data averaged resulting better block utilization 
performance pht critical advantage offered pht simpler data structures traditional pointer binary tree structured key space layout pht lookups bypass root looking data lower levels tree 
offers phts potential avoid having upper levels tree hotspots limit throughput 
figures show spread dht accesses pht levels pht insert items query operations entire query workload respectively 
graphs show levels tree close root accessed seldom bulk activity depth range 
sparse regions queries large sized areas query starts higher tree 
dominant accesses leaf nodes deep tree 
previous charts show distribution dht operations pht nodes 
critical test viability phts actual latencies required perform insert query operations 
set experiments evaluate performance 
block size variation tree depth function block size different input data sizes block utilization block size block utilization number items leaf node percentage block size versus block size varying input data set sizes 
insert operations experiment pre loaded pht elements 
started insert workload composed new randomly chosen elements measured performance insert operations 
shows cdf insert operations function insert latency pht block size 
graph shows effect lookup cache section 
pht pre loaded started client empty cache 
gradually client inserted elements pht discovered shape tree able skip binary lookups hitting directly cache 
notice median insert operation takes seconds 
cache inserts take median seconds cache hit median ms 
part performance deficiency due lack optimization opendht implementation 
dht get operation key matches number values current opendht implementation returns kbytes values requires clients perform additional get operations retrieve remaining values 
fetches large leaf nodes result cascade number dht level operations 
communicated issue opendht developers version expected fix allowing bulk gets 
minor optimizations dht implementation expect median insertion latency reduced factor 
insertion latency negligible 
large extent result decision build range query data structure entirely top general purpose dht service 
typical insert operation composed binary search median dht gets experiment followed put 
small number insertions result splits higher latency 
operations invoked outside dht service take advantage specialized routing dht efficiency 
see techniques aggressive caching help reduce latency substantially 
practice workload place lab anticipate pht structure stay static result modifications direct comparison phts performance customized system mercury ideal unfortunately mercury implementation available distribution :10.1.1.102.473
dht accesses dht gets dht puts node depth pht plot accesses dht puts gets pht tree level inserting items pht 
total items pht block size 
data query size time sec accesses node depth pht plot accesses dht gets pht tree level query workload 
pht contains access points block size 
block query size time sec table variation average query processing time different input data set sizes block size varying block sizes input data set size 
consequently potential cache invalidations new war drive submitted system 
typically expected war drives local neighborhood affect portion pht 
typical place lab usage expect lookup cache provide significant improvement insert latencies 
query performance look performance query workload 
pre loaded pht input data sets varying sizes varying pht block sizes 
table shows average query latencies functions input data set sizes block sizes 
expect larger data sets queries take longer resolve 
jump factor data set size causes query latency increase factor 
due parallelism logarithmic performance afforded pht 
vary block size query latencies initially drop larger blocks implies fewer operations fewer pht nodes need touched 
keep increasing block size query latency starts go 
large block sizes get operations pht leaf nodes potentially return items query matches 
note direct result decision implement phts entirely third party dht service 
run pht specific code directly dht nodes reduced overhead filtering values query returning dht 
shows scatter plot query times queries function query response size run experiments input data set elements block size 
graph shows total time query time taken time seconds cdf percentage inserts inserts cache hits cache misses insert latency seconds cumulative distribution function cdf plot percentage insert operations function insert latency pht items block size 
total query time time response query response size plot query response time total time time data item function response set size pht items block size 
item results reach client 
general queries larger responses take longer 
queries return result second issuing query 
experiment median query response time seconds median time set responses seconds 
query experiments lookup cache provide benefit inserts 
cache perform initial binary search query latencies dominated sub tree traversal 
said easy extend lookup cache apply subtree traversal help improving performance queries 
effect linearization pht implementation uses curve linearization convert multi dimensional indexes single dimension 
exercise compared linearization technique compares techniques hilbert curves curves 
results experiments input data set items block size summarized table 
hilbert curves theoretically shown better clustering properties curves dimensional queries benefits limited 
advantage hilbert curves producing linearizations fewer non contiguous segments resolving query issue phts entire query processed parallel starting top sub tree linearization avg block average depth occupancy gets query curve hilbert curve gray code table variation pht characteristics different linearization types 
pht case disk indexes discontinuity implies additional disk seeks 
experimented phts dimensions extended arbitrary number dimensions 
high dimensional data complex linearizations pyramid technique known perform better 
effort possible adapt linearization conjunction phts 
handling concurrency mentioned section concurrent pht operations result sub optimal performance absence concurrency primitives dht 
particular notice behaviors multiple clients simultaneously split full leaf node 
pht leaf nodes fill larger block size multiple clients attempt insert item node time 
insertions lost instance clients simultaneously attempt fill available slot leaf node client succeeds third client splits leaf node progress second client insert lost 
measured frequency behaviors occur concurrent operations 
ran experiment concurrent clients inserting data pht starting empty pht 
shows plot average number duplicate splits occur node depth pht 
note contention happens closer root tree 
tree grows number unique leaf nodes increases consequently race conditions leaf decrease 
saw similar behavior cases 
upgraded dht deployment include support atomic test set operation re ran experiments 
simple addition dht apis pht able operate correctly longer exhibited behaviors described 
note absence concurrency primitives underlying dht problems cause inefficiencies operation pht repaired refresh mechanisms 
dealing churn evaluate efficacy dht handling issues robustness availability replication performed set experiments introduced churn dht 
minute intervals randomly killed existing dht node started new node 
dht service opposed client system admittedly high churn rate 
measured effect churn query workload respect percentage expected query responses lost due churn 
results indicate negligible loss query responses 
queries reported fewer results expected 
queries reported expected results 
cases loss greater total expected number results quite small fewer items 
data loss temporary recovered soon dht replication recovery algorithms kicked 
measured latency overhead introduced result churn 
define churn overhead ratio query response time churn versus response time churn 
plots cdf percentage queries function churn overhead 
spite churn system queries show negligible overhead small number queries affected significantly take longer respond 
overhead largely due momentary increases dht routing latency replication overhead 
queries reported fewer expected items exactly ones highest overhead 
hand queries performed faster churn largely effect vagaries internet latencies 
true evaluation effect churn affects user application 
reproduce data experiment published previous 
experiment demonstrates effect data loss pht due large amounts churn accuracy client device location 
bayesian positioning algorithm described estimate user position 
catastrophic failure causes significant loss place lab mapping data application resilient able handle loss 
shows drop availability low see negligible effect positioning error 
ignoring fact dht hides effects churn place lab data get lost place lab capable absorbing effects loss minimal observable effects user 

lessons learned experience building place lab top open dht demonstrates feasible build applications complex semantics simply put get entirely top third party dht 
summarize lessons drawn experience 
simplicity implementation code required hook place lab underlying opendht service including entire pht implementation consists lines java compared lines opendht 
customized non layered implementation required place lab implement scratch scalable routing robustness management properties got opendht free 
number features place lab suited strictly layered implementation 
data structures link free 
node largely independent easily distributed top dht 
similarly information place lab beacon independent beacons making easy decompose data servers 
place lab mapping data significant redundancy allowing mask transient avg 
duplicate splits node depth average number duplicate splits function node depth concurrent pht writers 
failures effectively 
data structures capable refreshing recovering failures 
suited deployment infrastructure place lab developers control 
ease deployment started discussion asking question building place lab mapping service top opendht simplifies deployment 
question facets longterm service deployment experimental deployment performance testing 
long term deployment strategy implementation place lab able hand management overhead running maintaining distributed system opendht 
mapping server place lab essentially independent servers 
outsourcing opendht deployment third party participant place lab infrastructure worry management individual mapping servers connection dht 
hand experimenting application performance ended having install opendht infrastructure separate existing deployed version 
opendht shared service maintainers service unwilling kill machines random allow experiment effects churn 
similarly discuss extended opendht apis resulted disruption shared opendht deployment maintainers upgrade support new apis 
large extent different experimenting say internet protocols expect tinker directly deployed shared infrastructure 
flexibility apis able build place lab entirely top narrow set application independent apis 
experience demonstrated put get remove primary interfaces place lab relied needed additional auxiliary apis correctly efficiently support distributed data structures application 
typically dhts designed best effort performance 
provide concurrency primitives provide atomicity guarantees reads writes 
may sufficient simple rendezvous storage applications difficult build complex data structures 
principle simplicity strong cdf queries churn overhead cdf percentage queries function churn overhead 
churn overhead defined ratio query response time churn versus response time churn 
median error meters unavailable data median positioning error place lab function availability beacon data 
central tenet design internet 
obviously served internet clear sufficient large applications internet 
phts overcome ttls periodic refresh recover concurrency problems simple test set operation put conditional extension provides best effort guarantees goes long way improving pht performance 
performance dhts implement place lab distributed infrastructure significantly simplified implementation deployment expense performance 
queries take average seconds depending size input data set 
contrast single centralized implementation eliminate round trips account performance overhead 
similarly implementation allowed modifying underlying dht routing example mercury provide opportunities optimization 
tradeoff inherent layered versus monolithic implementation 
aggressive caching significantly improves place lab performance 
example pht data structure modified infrequently eliminate round trips caching amounts representation current shape tree 
applications forms caching suited provide reasonable performance 
course performance tradeoff worth ease implementation deployment depends entirely requirements application users 

explored viability dht service general purpose building block place lab user positioning system 
particular investigated suitability layering place lab entirely top dht minimize deployment management overhead 
place lab differs traditional dht applications requires stronger semantics simply put get operations specifically needs dimensional geographic range queries 
designed evaluated prefix hash trees multi dimensional range query data structure layered top opendht service 
phts provide elegant solution gathering radio beacon data location place lab mapping servers 
layered approach building place lab allowed automatically inherit robustness availability scalable routing properties dht 
able significantly reduce implementation overhead layering simplification price performance 
place lab phts unable optimizations possible customized dht underneath 
certainly word feasibility general purpose dhts building block large scale applications 
place lab demonstrates ease deployment primary criterion maximal efficiency simple dht apis minor extensions provide necessary primitives build richer complex systems top 

aberer grid self organizing access structure information systems 
proc 
coopis 
anonymous opendht public dht service uses 
submission sigcomm 
aspnes kirsch krishnamurthy load balancing locality range data structures 
proceedings third acm symposium principles distributed computing july 
aspnes shah skip graphs 
proc 
acm siam symposium discrete algorithms soda 
awerbuch scheideler peer peer systems prefix search 
proc 
acm symposium principles distributed computing podc 
berchtold ohm kriegel 
pyramid technique breaking curse dimensionality 
proceedings international conference management data sigmod june 
bhagwan varghese voelker cone augmenting dhts support distributed resource discovery 
tech 
rep cs uc jul 
bharambe agrawal seshan mercury supporting scalable multi attribute range queries :10.1.1.102.473
proc 
sigcomm 
cheng chawathe lamarca krumm accuracy characterization metropolitan scale wi fi localization 
proceedings mobisys seattle wa june 
gehrke shanmugasundaram querying peer peer networks trees 
proc 
webdb workshop 
dabek kaashoek karger morris stoica wide area cooperative storage cfs 
proceedings th acm symposium operating systems principles sosp lake louise ab canada october 
druschel rowstron storage management caching past large scale persistent peer peer storage utility 
proceedings th acm symposium operating systems principles sosp lake louise ab canada october 
fips pub 
secure hash standard april 
freedman mazi res content publication coral 
proceedings st symposium networked systems design implementation nsdi san francisco mar 
ganesan bawa garcia molina online balancing range partitioned data applications peer peer systems 
proc 
vldb 
gupta agrawal approximate range selection queries peer peer systems 
proc conference innovative data systems research cidr 
huebsch chun hellerstein loo maniatis roscoe shenker stoica architecture pier internet scale query processor 
proc 
conference innovative data systems research cidr jan 
jagadish linear clustering objects multiple attributes 
proceedings acm sigmod international conference management data sigmod may pp 

karger ruhl simple efficient load balancing algorithms peer peer systems 
proc 
spaa 
kubiatowicz oceanstore architecture global scalable persistent storage 
proceedings asplos cambridge ma usa november 
lamarca place lab device positioning radio beacons wild 
proceedings international conference pervasive computing pervasive june 
lamport part time parliament 
acm transactions computer systems 
mislove post reis druschel wallach sens post secure resilient cooperative messaging system 
proceedings th workshop hot topics operating systems hi may 
muthitacharoen gilbert morris fault tolerant algorithm atomic mutable dht data 
technical report massachussetts institute technology june 
muthitacharoen morris gil chen ivy read write peer peer file system 
proc 
osdi 
oppenheimer albrecht vahdat patterson design implementation tradeoffs wide area resource discovery 
proceedings th ieee symposium high performance distributed computing hpdc research triangle park nc july 
peterson anderson culler roscoe blueprint introducing disruptive technology internet 
proceedings acm hotnets workshop princeton nj oct 
see www planet lab org 
rowstron kermarrec castro druschel scribe design large scale event notification infrastructure 
networked group communication third international cost workshop ngc nov crowcroft hofmann eds vol 
lecture notes computer science pp 

sit dabek robertson low overhead usenet server 
proc 
rd iptps feb 
stoica zhuang shenker surana internet indirection infrastructure 
proceedings acm sigcomm pittsburgh pa usa august 
stoica morris karger kaashoek balakrishnan chord scalable peer peer lookup service internet applications 
proceedings acm sigcomm san diego ca usa august 
tang xu mahalingam psearch information retrieval structured overlays 
sigcomm comput 
commun 
rev 
browne solving range queries distributed system 
tech 
rep tr ut cs 
dahlin scalable distributed information management system 
proc 
sigcomm 
