bayesian methods frequent terms text models contagion statistic william cohen stephen fienberg school computer science department statistics carnegie mellon university pittsburgh pa usa statistical approaches modeling text implicitly assume informative words rare 
assumption usually appropriate topical retrieval classification tasks non topical classification soft clustering problems classes latent variables relate sentiment author informative words frequent 
comprehensive set statistical learning tools treat words higher frequencies occurrence sensible manner 
introduce probabilistic models contagion classification soft clustering poisson negative binomial distributions share multinomial desirable properties simplicity analytic tractability 
introduce statistic select features avoid fitting 
years ago herbert simon argued text progresses creates meaningful context words appear simple important notion context particular word appeared previously 
unfortunately type context captured usual multivariate bernoulli multinomial models captured contagious distributions poisson negative binomial model word frequencies documents 
contagious distributions language modeling new 
mosteller wallace model frequency function words indicators personal writing styles authors federalist papers church gale showed poisson mixtures fit observed word frequency data better standard :10.1.1.38.3957
modern language models sophisticated ones multinomial models frequency largely mathematical convenience simple dirichlet conjugate prior tendency complex models overfit performance multinomial distribution classical problems topic classification 
derive practically useful contagious distributions naturally fit modern language models 
introduce new hierarchical bayesian model naturally extends mosteller wallace re parameterization poisson negative binomial distributions order take advantage dirichlet natural non informative prior new parameters maintaining analytical tractability 
second order avoid overfitting novel statistic selecting features importance statistic helps avoid fitting sound assumptions particular contagious distribution occurrence words false discovery rate arguments order control probability selecting irrelevant words 
third demonstrate distributions improve cross validated classification accuracy achieved multivariate bernoulli multinomial models 
order boost speed feature selection strategies statistic derive asymptotic distributions different degrees precision assuming poisson negative binomial word counts allows compute values sampling 
importantly analytic tractability contagious distributions enables fast inference mechanisms complex language models latent dirichlet allocation blei ng jordan author topic models fienberg lafferty simply plugging realistic distributions updating formulas necessary approximations 
example simple approximations obtain lower bound variational inference closed form soft clustering version models :10.1.1.110.4050
background related na bayes approach usually associated multivariate bernoulli multinomial models 
consists simple application bayes theorem solve classification problem 
na fact different words considered pairwise independent model specific position words text 
domingos pazzani give complete characterization na bayes models study conditions optimality decision theoretic perspective 
works focused analysis limitations na bayes bernoulli multinomial models particular assumption occurrences word happen independently seriously challenged strong evidence theoretical empirical produced extensive studies textual data 
ad hoc models proposed go independence assumption :10.1.1.38.3957
investigates principled approach relaxing independent occurence assumption 
number extensions proposed na bayes approach prescribe hierarchical graphical models observed hidden variables order describe cluster classify documents 
order perform inferences models constants underlying distributions variables top layer hierarchy fixed 
empirical bayes approach combination methods approximate certain intractable marginal distributions example mcmc variational methods expectation propagation 
main ingredients extensions multinomial model conjugate dirichlet prior adapted handle frequency models tractable conjugate priors 
discuss extent models combined sophisticated language models section 
notation data consists number times words appear texts 
category collection dc documents represent random vector xv dc bag word counts words indexed belong pre specified vocabulary 
denote observed word counts instances corresponding random numbers lowercase contagious distributions words context contagious distributions provide better fit frequent terms relaxing assumption independence successive occurrences word text 
intuitively contagion means occurrence word subsequent occurrences 
argue notion contagion introduces natural notion context word occur defining writing style author prevalent sentiment sentence 
experiments distributions led lower cross validated classification errors 
section widely contagious distributions poisson negative binomial word frequency 
goal connection explicit introduce quantities help correcting estimates relevant parameters account different lengths texts 
poisson model revisited text data poisson model implicitly assumes words terms occur randomly independently mean frequency 
stated differently suppose usage word word modeled random variable denoting expected time till usage poisson distribution gives particular form density may interpret poisson distribution pa rameter probability times time interval length 
encodes number times appears document poisson 

rewrite observable size document thousands words rate occurrence word words poisson mw 

maximum likelihood estimator mw takes account variable length texts note 
negative binomial model revisited negative binomial distribution obtained expansion positive real 
note need 
encodes number times appears document neg bin pw qw 
parameterization mean equals pw variance equals pw pw 
standard parameterization obtained introducing single parameter pw 
qw poisson model negative binomial model pool words reagan texts reagan texts reagan texts reagan texts highest frequency words semantic features words information gain table goodness fit poisson negative binomial models various pools words 
pools selected positive written reagan negative written examples reagan radio addresses 
counts predicted number words brackets give actual number words 
predictions values sample kolmogorov smirnov test 
source 
intuitively negative binomial distribution poisson distribution extra variability order connection explicit obtain parameters easy interpret specify model word counts introduce extra variability parameter dw set pw qw pw get neg bin mw dw redundant parameter 
poisson case observable size document thousands words rate occurrence word words 
parameter non parameter parameter controls far negative binomial distribution corresponding poisson limit 
formally negative binomial converges distribution poisson limit fixed rate 
negative binomial parameters mean equal dm variance equal dm dd mean corresponding poisson limit extra variability factor dd 
extra parameter allows model heavy tails extra variability relative poisson 
example prior studies authorship attribution 
parameterization negative binomial terms observed estimates relatively stable words authors 
words 
negative binomial model captures variability observed word frequency data better poisson model 
table data demonstrating flexibility gained introducing thorough treatment distributions johnson 

dataset classes selection na bayes poisson neg bin newsgroups info 
gain reuters info 
gain spam info 
gain fraud detection info 
gain table prediction errors popular data sets 
errors refer words selected information gain internal fold cross validation select 
best accuracy obtained words selection involved 
baseline na bayes estimates corrected different lengths documents 
provide stronger baseline give best accuracy tfidf scaled marked unscaled na bayes 
numeric parameters word allows better capture way frequent semantically important words 
mosteller wallace gave non bayesian method moment estimators negative binomial parameters 
estimators account different word length documents 
optimal poisson limit mw dw max vw mw mw vw conditioning word length document mw dp word count models multinomial distribution fail account variability length documents various ways 
sampling document models guarantees desired word length average exactly accounting variability 
specifically parameter condition size documents parameterizations poisson negative binomial respectively 
consider poisson case rate dm assume observations number times certain word occurs set documents possibly different word lengths 
new parameterization dm breaks rate parts rate occurrence word study say consecutive words text consecutive words general rate measured document length terms number words length document expressed pure number multiple word length document document word long text words 
allows express rate rate occurrence word document word long conditionally desired observed length text expressed multiple word length text 
similar considerations binomial case 
bayesian models contagion words context models sections depend large number parameters word vocabulary 
section introduce parsimonious models assuming population parameters described compactly terms distributions simple functional forms ultimately depend set underlying constants 
bayesian models introduce hierarchical generative models 
class models focus hierarchy probabilistic assumptions parameters data 
classification soft clustering tasks sides coin differing amount labeled documents available initializing inference training parameters initializing latent categories 
parameterizations terms account natural variability length texts hard posit set natural prior distributions cases little information parameters 
introduce novel idea contagious distributions discuss properties entails 
sum ratio parameterizations idea term sum ratio parameterizations intuitive map parameter vector new parameter vector introducing sum parameter sum components original vector additional ratio parameters obtained dividing components original vector sum parameter models contagion introduced word wc wc log wc wc wc pc wc wc pc wc note ratio parameters sum need 
log transformation log serves purpose dampening heavy tails distribution explored separate studies 
transformation possibility generally log depends document length 
parameters redundant 
assumptions vectors independent words vector vector independent word class parameterizations major advantage separating rate occurrence way allocated various categories independently observed latent 
simplifies times inference calculations complex language models 
naturally leads simple analytic forms non informative priors 
cases possible derive expression maximum likelihood estimators sum parameter condition inference process classification soft clustering versions models order improve fit data 
natural non informative priors full specification parameter vector wc wc support assume values follow symmetric dirichlet distribution entails expected rate occurrence parameters wc 
residual parameter greater zero 
rely prior studies order pick functional form non informative priors parameter 
summary frequent terms propose symmetric dirichlet parameter improper constant density symmetric dirichlet parameter briefly model dirichlet distribution 
argue natural choice alternative sampling schemes equivalent exactly asymptotically 
example model components vector rates independent gamma distributions sum parameter gamma ratio parameter vector follows dirichlet distribution 
see kotz 
details similar results 
improper constant density constant density infinite support improper integrate 
gamma parameters 
model encodes notion words occur priori useful discriminating categories 
higher occurrence word higher higher dirichlet parameter lower priori variability elements inference parameter estimation bayesian models contagion frequent words context classification soft clustering tasks calculations relate classification task 
extent assume categories predict category new document evaluating log odds class xnew log xnew xnew log odds function parameters wc need learned training documents 
models posit hierarchy probabilistic assumptions parameters bayesian inference required learn values 
note posit separate model category new index appears runs map estimation evaluated log odds mode posterior distribution parameters data 
derived closed form expressions second derivatives quantity proportional posterior hierarchical bayesian models 
newton raphson perform maximization 
note maximization may fail fairly rare words matrix second derivatives corresponding negative binomial model may vanish 
mcmc alternative evaluated log odds mean posterior distribution parameters data theoretically sound computationally expensive 
models metropolis gibbs sampler gaussian proposals 
briefly outer loop iteratively samples onedimensional full conditionals gibbs inner loop called sample conditionals known proportionality constant metropolis 
dirichlet poisson model example posterior distributions entails full conditionals 
log log 
log log log log similar derivations give set full conditionals perform inference dirichlet negative binomial model 
full bayes sets constants 
underlying poisson negative binomial models respectively need fixed 
fully bayesian approach estimate underlying constants data 
relied results prior study separate data set selected sets constants lead reasonable tails priors 
evaluated cross validated error rates corresponding set underlying constants order get sense sensitive predictions may 
errors reported table obtained note soft clustering reparameterization section partially maps parameters poisson negative binomial models simplex allowing combine hierarchy probabilistic assumptions dirichlet density natural non informative prior frequent terms powerful contagious distributions introduce notion context 
interest understanding mathematical connections models example latent dirichlet allocation blei 
improves analytical tractability 
specifically separating sum rates split classes allows estimate directly data condition estimates classification allows carry variational inference conditionally soft clustering leading closed formula variational em updates 
posit fully generative models soft clustering share hierarchy probabilistic assumptions parameters models classification 
focus bayesian paradigm set probabilistic assumptions enables fit practically useful contagious distributions complex language models 
briefly soft clustering version models allow variational lower bound closed form 
devise step updates variational em algorithm conditionally parameters reliably estimated data hinted 
ultimately models extract richer set categories competing latent allocation models 
feature selection expressive classes distributions represent word frequency cause overfitting 
propose distribution feature selection strategy tests feature relevance specific word frequency model poisson 
test produces defined value feature selection features performed principled way standard methods combining multiple statistical tests false discovery rate 
denote number times wth word dictionary appears dth document belonging cth class denote observed counts texts 
define word follows 

statistic test null hypothesis word irrelevant extent discriminating documents categories specifically assume contagious frequency model word xw test 
value provide probabilistic assessments word occurred categories differently discard hypothesis differences outcome pure chance word irrelevant discrimination 
order perform test irrelevance word compute observed value statistic obs ii estimators sections estimate parameters underlying word frequency model iii compute value evaluate integral obs 
na solution sampling distribution step iii 
may expensive especially rare words 
alternatively approximate analytically distribution poisson negative binomial estimate set parameters corresponding collection documents 
possible document labels weight parameter estimates corresponding different classes 
models compute value approximate density 
tedious calculations lead normal approximations corresponding expansions different orders nd case independent 
similar approximations negative binomial available 
extend statistics word multiple categories iteratively computing values class versus keeping smallest value 
experiments compared cross validated accuracies na poisson negative binomial bayesian dirichlet poisson baselines multinomial multivariate bernoulli eleven data sets 
data sets newsgroups problem want classify newsgroups posts topic 
reuters problem abandon typical breakdown narrow categories scenario low frequency keywords drive classification create high level categories money crops natural resources order medium frequency weakly topical words drive classification 
fraud detection problem want find messages contain fraudulent intent 
opinion extraction problems want categorize opinion expressed online news articles courtesy com positive neutral negative 
spam problem want classify emails easy ham hard ham spam ham term indicates legitimate emails web master problem task classify web site update requests add change delete 
reagan data problem attributing authorship text ronald reagan radio addresses broadcasted years 
movie reviews problem want associate positive negative sentiment movie review 
medical data task classify patient certain disease outcomes different tests 
rd corpus available online www org 
dataset class selection na bayes poisson neg bin dir reagan data ig movie reviews ig medical data ig opinions finance ig opinions mixed opinions web master ig ig ig table prediction errors refer words selected information gain internal fold cross validation select 
best accuracy obtained words selection involved 
baseline na bayes estimates corrected different lengths documents 
na bayes improved scaling counts tfidf weights 
provide stronger baseline give best accuracy unscaled na bayes 
accuracies scaled na bayes marked 
errors central columns refer parameterizations poisson negative binomial models sections 
errors dip model obtained 
results allow fair comparison corrected parameter estimates baseline models account different length documents transformed word counts tfidf 
fact na bayes improved scaling counts tfidf weights 
tables report best accuracy tfidf scaled unscaled na bayes compared accuracies sets words selected information gain different values different number words results comparable 
experiments suggest poisson negative binomial models fit textual data better lead log odds consistently extreme multinomial 
need lead better accuracy case email fraud data set 
statistic favors words occur leads higher accuracies accuracies scaled na bayes marked 
information gain classification problems 
advantage choosing words occur small set may sufficient represent collection documents promoting insights problem interpretability results 
described simple principled extension widely multinomial model text 
extension allows better modeling frequent words replacing widely multinomial distribution simple contagious distributions relaxing assumption independence different occurrences word text 
eleven data sets show model generally leads better classification accuracy substantially better 
experiments simple na hierachical bayes models classification important advantage proposed extension easy combine complex models text mixtures hierarchical mixture models 
current developed tractable non informative priors models settings fully bayesian empirical bayesian approach appropriate 
successfully exploited proposed hierarchy probabilistic assumptions parameters build soft clustering counterparts models 
anderson fienberg skinner 
wrote ronald reagan radio addresses 
bayesian analysis 
appear 
bai padman 
sentiment extraction unstructured texts markov meta heuristic search 
lecture notes computer science 
springer verlag 
appear 
fienberg 
serial analysis gene expression data dirichlet poisson model 
manuscript september 
malin 
data mining challenges electronic safety case fraudulent intent detection mails 
proceedings workshop privacy security aspects data mining pages 
ieee computer society 

hierarchical mixture models theory practice 
manuscript september 
beeferman berger lafferty 
model lexical attraction repulsion 
proceedings th annual meeting acl pages 
blei ng jordan 
latent dirichlet allocation 
journal machine learning research 
buntine 
applying discrete pca data analysis 
uncertainty artificial intelligence 
canny 
gap factor model discrete data 
proceedings th annual international acm sigir conference research development information retrieval 
carlin louis 
bayes empirical bayes methods data analysis 
chapman hall second edition 
church 
term 
fox ingwersen fidel editors annual international acm sigir conference research development information retrieval 
acm press 
church gale 
poisson mixtures 
natural language engineering 
cohen tomasic 
learning understand web site update requests 
proceedings international joint conference artificial intelligence 
domingos pazzani 
optimality simple bayesian classifier zero loss 
machine learning 
efron morris 
limiting risk bayes empirical bayes estimators part ii empirical bayes case 
journal american statistical association 
fienberg lafferty 
mixed membership models scientific publications 
proceedings national academy sciences 
lewis madigan 
naive bayes model text categorization 
proceedings workshop artificial intelligence statistics 
griffiths steyvers 
finding scientific topics 
proceedings national academy sciences suppl 

johnson khudanpur ostendorf rosenfeld editors 
mathematical foundations speech language processing 
springer 
johnson kotz kemp 
univariate discrete distributions 
john wiley 
jordan ghahramani jaakkola saul 
variational methods graphical models 
machine learning 
katz 
distribution content words phrases text language modeling 
natural language engineering 
kotz balakrishnan johnson 
continuous multivariate distributions volume models applications 
john wiley 
lewis yang rose li 
rcv new benchmark collection text categorization research 
journal machine learning research 
mccallum nigam 
comparison event models na bayes text classification 
aaai workshop learning text categorization 
miller newman friedman 
length frequency statistics written english 
information control 
minka 
expectation propagation approximate bayesian inference 
uncertainty artificial intelligence uai pages 
minka lafferty 
expectation propagation generative aspect model 
uncertainty artificial intelligence 
tom mitchel 
machine learning 
mcgraw hill 
mosteller wallace 
inference disputed authorship federalist 
addison wesley 
mosteller wallace 
applied bayesian classical inference case federalist papers 
springer verlag 
pang lee vaithyanathan 
thumbs 
sentiment classification machine learning techniques 
proceedings empirical methods natural language processing 
rennie jaakkola 
term informativeness named entity detection 
proceedings th annual international acm sigir conference research development information retrieval 
rennie shih karger 
tackling poor assumptions na bayes text classifiers 
international conference machine learning 
robert casella 
monte carlo statistical methods 
springer texts statistics 
springer verlag new york ny corrected second edition 
simon 
class skew distribution functions 
biometrika 
storey taylor 
strong control conservative point estimation simultaneous conservative consistency false discovery rates unified approach 
journal royal statistical society series 
karger 
empirical development exponential probabilistic model text retrieval textual analysis build better model 
proceedings th annual acm sigir international conference research development retrieval 
yang liu 
re examination text categorization methods 
proceedings th annual acm sigir international conference research development retrieval 
yule 
statistical study literary vocabulary 
cambridge university press 
zipf 
selected studies principle relative frequency language 
harvard university press 

