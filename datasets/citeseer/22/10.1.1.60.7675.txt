competent committees 
aw duch ski department informatics nicholas university toru poland 
www phys uni pl kmk 
committees models frequently employed improve accuracy decrease variance individual models 
model equal right vote democratic procedure despite obvious differences model competence different regions feature space 
adding competence factors different models calculation committee decision procedure improves quality committee 
method creation committee competent models described empirical tests 
combining information different classifiers important quite popular subject machine learning 
conferences special issues journals devoted subject see know ensemble learning mixture experts voting classification algorithms committees models :10.1.1.33.353
ortega argamon point problems predicting glucose levels patients large number different learning algorithms applied 
mixture models may improve accuracy single model may decrease variance stabilizing improving generalization system 
sources contribute variability models created committee different samples taken data example crossvalidation training methods boosting bagging arcing different bias models due change complexity number neurons parameters :10.1.1.33.353
developed framework similarity methods sbm methods belong framework create voting committees obtaining significant improvements results decrease variance committee errors 
typical voting techniques follow democratic majority decision linear combination selecting confident models 
mixture experts neural architecture jacobs introduced gating network select competent model 
ortega similar idea referee meta model deciding model contribute final decision 
procedures exploit fact different models may different areas competence 
idea competent voting mentioned developed 
global selection competent models introduced 
training meta model area input space model number errors identified penalty factor decrease influence model voting 
section methods model combination briefly discussed algorithms creating committees competent models described 
third section results numerical experiment 
plans 
combining models 
individual models frequently unstable quite different models created result repeated training learning algorithms stochastic training set slightly perturbed 
mixture models allows approximate complicated probability distributions quite accurately 
models providing estimation probabilities ci ml classes majority voting average results models select model highest confidence gives largest probability set threshold select subset models highest confidence majority voting models 
empirical comparison voting algorithms including bagging boosting published bauer kohavi 
tests decision trees naive bayes method 
bagging algorithm uses classifiers trained bootstrap samples created randomly drawing fixed number training data vectors pool contains training vectors drawing remove pool 
results aggregated voting 
adaboost adaptive boosting creates sequence training sets determines weights training instances higher weights incorrectly classified 
arcing method uses simplified procedure weighting training vectors 
bauer kohavi provided interesting decomposition bias variance components errors algorithms 
renormalized product different predictors advocated hinton context unsupervised probability density estimation 
linear meta model ci wi lp ci ml provides additional mk linear parameters model combination determined standard mean squares lms procedure 
committees competent models ccm far models selected ensemble allowed vote final result 
krogh vedelsby showed ensemble generalization error small highly accurate classifiers disagreeing 
contrary idea xin yao averaging results negative correlation individual models diversify pool 
model need accurate data account different overlapping subset data 
similarity models vectors selected training set relatively easy determine areas input space model competent errors fails 
vectors correctly classified show errors model vectors erroneously classified model may correctly handled 
information may ways 
simple algorithm includes information competence different models 

optimize parameters models ml training set cross validation procedure 

model training vectors ri generate predicted classes cl ri cl ri ri model ml error vector ri determine area incompetence model finding distance nearest vector ml correctly classified set parameters incompetence factor ml way value decreases significantly ri di 
incompetence function model product factors ri ml training vectors incorrectly handled 
incompetence function ml areas model worked ml near training vectors errors 
number functions may purpose gaussian function ri ml ri coefficient flatten function simpler ri ml ri function sum logistic functions ri di ri di 
number factors enters incompetence function model factor quickly reach outside incompetence area 
achieved large values high slopes sigmoids defining cut values value taken 
committee competent models may ways 
voting phase nearest neighbor vectors determined classifiers competent included voting procedure 
competent models vector classification probably outlier left rejected impossible classify 
helps vectors removed training set achieved automatically competent classifiers 
simpler way creating competent committee introduced linear combinations majority voting 
class ci coefficients linear combination determined mean square solution ci wi lf ml ci ml incompetence factors simply modify probabilities ci ml set linear equations training vectors solution done way 
renormalization ci cj give final probability classification 
contrast adaboost similar procedures explicit information competence quality classifier performance different feature space areas :10.1.1.33.353
numerical experiments numerical experiments vowel data containing intensities formants vowels 
classes overlap strongly samples dataset 
results different tests collected table 
methods discussed ref 
gave worse results 
table 
comparison results vowel data 
xcv means fold stratified crossvalidation test 
system accuracy remarks committee xcv calculation knn euclidean xcv calculation mlp xcv neurons fuzzy mlp xcv neurons bayes classifier xcv fuzzy kohonen xcv calculations nearest neighbor classifier gave quite promising results divided dataset randomly parts trained committee knn models part treating part test data 
committee included models euclidean euclidean 
table 
accuracy models class 
class average accuracy model table 
accuracy may similar models significantly differ accuracy different classes 
select best model class example model class accuracy grow course class selected predicted 
vectors assigned classes incorrectly models giving chance account correctly remaining vectors vectors 
table contains results obtained types committees created majority voting selecting model highest confidence linear combination linear combination competence factors 
results obtained committees usually better results single model majority voting worst highest confidence linear combination level slightly better significant improvement linear combination competent models 
table 
results committees created ways majority voting highest confidence linear combination linear combination competence factors 
class majority confidence combination competence average empirical tests needed assigning incompetence factors various voting procedures including linear combination models attractive idea may significantly improve analysis difficult problems 
need create single model handles data correctly learning may modular model specializing different subproblems 
constructive approach committee growth may creating initial committee combining competent models created far new models searched classify correctly just vectors committee problems 
ideas may developed number directions 
far tried aggregate models generated different parameters 
procedure may applied models generated adaptive boosting similar algorithms :10.1.1.33.353
interesting possibility train neural network providing input vectors predicting competent models 
combination classifiers gives receiver operator characteristic roc curves cover convex combination individual roc curves allowing reach better operating points detection rates false alarm rate 
models small effective coefficients training data may pruned 
diversification models adding explicit negative correlation worth considering 
lot options remains investigated 
acknowledgments support polish committee scientific research gratefully acknowledged 

ortega argamon 
arbitrating competing classifiers learned referees 
knowledge information systems 
bauer kohavi 
empirical comparison voting classification algorithms bagging boosting variants 
machine learning 
breiman 
bias variance regularization instability stabilization 
bishop 
ed 
neural networks machine learning 
springer berlin heidelberg new york 
duch 
similarity methods general framework classification approximation association control cybernetics 
duch ski 
ensembles similarity models 
information systems advances soft computing physica verlag springer pp 

jacobs 
bias variance analyses mixtures experts architectures 
neural computation 
duch 
classification association pattern completion neural similarity methods 
applied mathematics computer science 
roli dynamic classifier selection multiple classifier behaviour 
pattern recognition 
intrator 
boosted mixture experts ensemble learning scheme 
neural computation 
bauer kohavi 
empirical comparison voting classification algorithms bagging boosting variants 
machine learning 
hinton 
training products experts minimizing contrastive divergence 
gatsby computational neuroscience unit technical report 
krogh vedelsby 
neural network ensembles cross validation active learning 
advances neural information processing systems mit press 

yao liu 
new evolutionary system evolving artificial neural networks 
ieee transaction neural networks 
pal mitra 
neuro fuzzy pattern recognition 
wiley new york 
duch new methodology extraction optimization application crisp fuzzy logical rules 
ieee transactions neural networks 
michie spiegelhalter taylor machine learning neural statistical classification 
horwood london 
swets 
measuring accuracy diagnostic systems 
science 
