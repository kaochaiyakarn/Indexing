gatsby computational neuroscience unit queen square london university college london wc ar united kingdom www gatsby ucl ac uk funded part gatsby charitable foundation 
may tr infinite latent feature models indian process thomas griffiths cognitive linguistic sciences brown university zoubin ghahramani gatsby unit define probability distribution equivalence classes binary matrices finite number rows unbounded number columns 
distribution suitable prior probabilistic models represent objects potentially infinite array features 
derive distribution limit distribution binary matrices strategy inspired derivation chinese restaurant process aldous pitman limit dirichlet multinomial model 
strategy preserves exchangeability rows matrices 
define simple generative processes result distribution equivalence classes binary matrices call indian process 
illustrate distribution prior infinite latent feature model deriving markov chain monte carlo algorithm inference model applying algorithm artificial dataset 
infinite latent feature models indian process thomas griffiths cognitive linguistic sciences brown university zoubin ghahramani gatsby unit unsupervised learning aims recover latent structure responsible generating observed properties set objects 
statistical models typically unsupervised learning draw relatively small repertoire representations latent structure 
simplest representation mixture models associates object single latent class 
approach appropriate objects partitioned relatively homogeneous subsets 
properties objects better captured representing object possessing multiple latent features 
example describing friend characterize married democrat red sox fan 
features may useful explaining aspects behavior necessarily directly observable 
methods exist representing objects terms latent features 
approach associate object probability distribution features 
approach proven successful modeling content documents feature indicates topics appears document blei ng jordan 
probability distribution features introduces conservation constraint object expresses feature express 
constraint inappropriate settings example imply friend red sox married imposed feature representation schemes 
instance choose represent object binary vector entries indicating presence absence feature ueda saito allow feature take continuous value representing objects points latent space jolliffe define factorial model feature takes discrete set values zemel hinton ghahramani 
regardless form representation takes critical question approaches dimensionality representation classes features needed express latent structure responsible observed data 
treated model selection problem choosing model dimensionality results best performance 
treatment problem assumes single finite dimensional representation correctly characterizes properties observed objects 
alternative assume number classes features potentially unbounded observed objects manifest sparse subset classes features rasmussen ghahramani :10.1.1.32.5075
assumption appropriate describing friend red sox fan possible imagine arbitrarily large set features describe people subset features depend properties want explain 
assumption observed objects manifest sparse subset unbounded number latent classes nonparametric bayesian statistics 
particular assumption dirichlet process mixture models nonparametric density estimation antoniak escobar west ferguson neal :10.1.1.51.1747
interpretation dirichlet process mixture model datapoint assigned latent class class associated distribution observable properties 
prior distribution assignments datapoints classes specified way number classes model bounded number objects making dirichlet process mixture models infinite mixture models rasmussen 
extended methods models object represented distribution features blei griffiths jordan tenenbaum teh jordan beal blei 
equivalent methods dealing feature representations binary vectors factorial structures vectors continuous feature values 
take idea defining priors infinite combinatorial structures nonparametric bayesian statistics develop methods unsupervised learning object represented sparse subset unbounded number features 
features binary take multiple discrete values continuous weights 
representations difficult problem deciding features object possess 
set features possessed set objects expressed form binary matrix row object column feature entry indicates particular objects possesses particular feature 
focus problem defining distribution infinite sparse binary matrices 
distribution define probabilistic models represent objects infinitely binary features combined priors feature values produce factorial continuous representations 
plan follows 
section reviews principles infinite mixture models focusing prior class assignments assumed models defined terms simple stochastic process chinese restaurant process 
section discusses role prior infinite binary matrices defining infinite latent feature models 
section describes prior corresponding stochastic process call indian process 
section illustrates prior defining infinite dimensional linear gaussian model deriving sampling algorithm inference model applying simple dataset 
section discusses 
latent class models assume objects ith object having observable properties represented row vector xi 
latent class model mixture model object assumed belong single class ci properties xi generated distribution determined class 
matrix xt xt xt indicate properties objects vector cn indicate class assignments model specified prior assignment vectors distribution property matrices conditioned assignments 
distributions dealt separately specifies number classes relative probability determines classes relate properties objects 
section focus prior assignment vectors showing prior defined placing upper bound number classes 
indicate probability mass functions indicate probability density functions 
assume xi density 
finite mixture models mixture models assume assignment object class independent assignments objects 
classes ci ci multinomial distribution classes probability class distribution 
assumption probability properties objects written xi ci 
distribution xi generated mixture class distributions xi ci determining weight class mixture weights treated parameter estimated variable prior distribution 
bayesian approaches mixture modeling standard choice symmetric dirichlet distribution 
dirichlet distribution multinomials classes parameters conjugate multinomial bernardo smith 
probability multinomial distribution dirichlet normalizing constant simplex multinomials classes generalized factorial function 
non negative integer symmetric dirichlet distribution equal 
example take case equation mean multinomial uniform classes 
probability model defined dirichlet ci discrete discrete multiple outcome analogue bernoulli event probabilities outcomes specified ci multinomial 
dependencies variables model shown 
having defined prior simplify model integrating values representing explicitly 
ci zik graphical models different priors 
nodes variables arrows indicate dependencies plates buntine indicate replicated structures 
model defining chinese restaurant process 
beta binomial model defining indian process 
marginal probability assignment vector integrating values ci mk mk mk mk ci number objects assigned class tractability integral result fact dirichlet conjugate multinomial 
equation defines probability distribution class assignments ensemble 
individual class assignments longer independent 
exchangeable bernardo smith probability assignment vector remaining indices objects permuted 
exchangeability desirable property distribution class assignments indices labelling objects typically arbitrary 
distribution assignment vectors defined equation assumes upper bound number classes objects allows assignments objects classes 
infinite mixture models intuitively defining infinite mixture model means want specify probability terms infinitely classes modifying equation xi ci infinite dimensional multinomial distribution 
order repeat argument need define prior infinite dimensional multinomials com pute probability integrating 
essentially strategy taken deriving infinite mixture models dirichlet process antoniak ferguson james sethuraman 
directly distribution assignment vectors equation considering limit number classes approaches infinity green richardson neal 
expanding gamma functions equation recursion cancelling terms produces expression probability assignment vector mk number classes mk re ordered indices mk 
kn possible values diverges 
happens probability single set class assignments goes 
finite clear 
consequently define distribution equivalence classes assignment vectors vectors 
specifically define distribution partitions objects 
setting partition division set objects subsets object belongs single subset ordering subsets matter 
assignment vectors result division objects correspond partition 
example objects class assignments correspond partition differs cases labels classes 
partition defines equivalence class assignment vectors denote assignment vectors belonging equivalence class correspond partition 
distribution partitions sufficient allow define infinite mixture model equivalence classes class assignments induced identifiability assignment vectors correspond partition apply statistical inference level partitions level assignment vectors 
assume partition objects subsets class labels applied subsets 


assignment vectors belong equivalence class defined partition 
define probability distribution partitions summing class assignments belong equivalence class defined partition 
probability class assignments equal distribution specified equation obtain 

mk 
rearranging terms compute limit probability partition lim 
mk 
mk 

partition induced chinese restaurant process 
numbers indicate customers objects circles indicate tables classes 
details steps taken computing limit appendix 
limiting probabilities define valid distribution partitions equivalence classes class assignments providing prior class assignments infinite mixture model 
objects exchangeable distribution just finite case probability partition affected ordering objects depends counts mk 
noted distribution partitions specified equation derived variety ways limits green richardson neal dirichlet process blackwell mcqueen equivalent stochastic processes james sethuraman 
briefly discuss simple process produces distribution partitions chinese restaurant process 
chinese restaurant process chinese restaurant process crp named jim pitman lester metaphor objects customers restaurant classes tables sit process appears aldous attributed pitman 
imagine restaurant infinite number tables infinite number seats 
customers enter restaurant choose table random 
crp parameter customer chooses occupied table probability proportional number occupants chooses vacant table probability proportional 
example shows state restaurant customers chosen tables procedure 
customer chooses table probability 
second customer chooses table probability second table probability second customer chooses second table third customer chooses table probability second table probability third table process continues customers seats defining distribution allocations people tables generally objects classes 
extensions crp connections stochastic processes pursued depth pitman 
distribution partitions induced crp equation 
assume ordering objects assign classes sequentially method specified crp letting objects play role customers classes play role tables 
ith object assigned kth class probability ci ci mk mk number objects currently assigned class number classes mk 
objects assigned classes process probability pitman statisticians uc berkeley inspired apparently infinite capacity chinese restaurants san francisco named process 

partition objects equation 
crp provides intuitive means specifying prior infinite mixture models revealing simple sequential process exchangeable class assignments generated 
inference gibbs sampling inference infinite mixture model slightly complicated inference mixture model finite fixed number classes 
standard algorithm inference infinite mixture models gibbs sampling escobar west neal :10.1.1.51.1747
gibbs sampling markov chain monte carlo mcmc method variables successively sampled distributions conditioned current values variables geman geman 
process defines markov chain ultimately converges distribution interest see gilks richardson spiegelhalter 
implementing gibbs sampler requires deriving full conditional distribution variables sampled 
mixture model variables class assignments relevant full conditional distribution ci probability distribution ci conditioned class assignments objects data applying bayes rule distribution expressed ci ci second term right hand side depends distribution class assignments 
finite mixture model defined equation compute ci integrating obtaining ci ci number objects assigned class including object posterior predictive distribution multinomial distribution dirichlet prior 
infinite mixture model distribution class assignments defined equation exchangeability find full conditional distribution 
exchangeable unaffected ordering objects 
choose ordering ith object assigned class 
follows directly definition chinese restaurant process ci number classes 
result limit full conditional distribution finite model equation neal 
combined choice equations sufficient define gibbs samplers finite infinite mixture models respectively 
demonstrations gibbs sampling infinite mixture models provided neal rasmussen 
similar mcmc algorithms bush maceachern west muller escobar escobar west james 
algorithms go local changes class assignments allowed gibbs sampler jain neal dahl 
features objects objects features objects features feature matrices 
binary matrix shown basis sparse infinite latent feature models indicating features take non zero values 
elementwise multiplication matrix continuous values gives representation shown 
contains discrete values obtain representation shown 
summary review infinite mixture models serves purposes shows infinite statistical models defined specifying priors infinite combinatorial objects illustrates priors derived limit priors finite models demonstrates inference models remain possible despite large hypothesis spaces imply 
infinite mixture models fundamentally limited representation objects assuming object belong single class 
remainder insights underlying infinite mixture models derive methods representing objects terms infinitely latent features 
latent feature models latent feature model object represented vector latent feature values fi properties xi generated distribution determined latent feature values 
latent feature values continuous principal component analysis pca jolliffe discrete cooperative vector quantization zemel hinton ghahramani 
remainder section assume feature values continuous 
matrix ft ft indicate latent feature values objects model specified prior features distribution observed property matrices conditioned features 
latent class models distributions dealt separately specifies number features probability distribution values associated feature determines features relate properties objects 
focus showing prior defined placing upper bound number features 
break matrix components binary matrix indicating features possessed object zik object feature second matrix indicating value feature object 
expressed elementwise hadamard product illustrated 
latent feature models pca objects non zero values feature entry 
sparse latent feature models sparse pca ghaoui jordan lanckriet jolliffe zou hastie tibshirani press subset features take non zero values object picks subsets 
prior defined specifying priors separately 
focus defining prior effective dimensionality latent feature model determined assuming sparse define prior infinite latent feature models defining distribution infinite binary matrices 
analysis latent class models provides desiderata distribution objects exchangeable inference tractable 
suggests method desiderata satisfied start model assumes finite number features consider limit number features approaches infinity 
distribution infinite binary matrices section derive distribution infinite binary matrices starting simple model assumes features limit 
resulting distribution corresponds simple generative process term indian process 
finite feature model objects features possession feature object indicated binary variable zik 
object possess multiple features 
zik form binary feature matrix assume object possesses feature probability features generated independently 
contrast class models discussed probabilities take value 
model probability matrix zik mk mk mk zik number objects possessing feature define prior assuming follows beta distribution 
beta distribution parameters conjugate binomial 
probability beta distribution beta function take equation 
exploiting recursive definition gamma function 
probability model defined beta zik bernoulli zik independent assignments conditioned generated independently 
graphical model illustrating dependencies variables shown 
having defined prior simplify model integrating values representing explicitly 
marginal probability binary matrix zik mk mk mk mk 
result follows conjugacy time binomial beta distributions 
distribution exchangeable depending counts mk 
model important property expectation number non zero entries matrix ik zik upper bound column independent expectation times expectation sum single column zk expectation easily computed zk zik kp result follows fact expectation beta random variable consequently tz ke zk finite expectation number entries bounded 
equivalence classes order find limit distribution specified equation need define equivalence classes binary matrices analogue partitions assignment vectors 
equivalence classes defined respect function binary matrices lof 
function maps binary matrices left ordered binary matrices 
lof obtained ordering columns binary matrix left right magnitude binary number expressed column row significant bit 
binary matrix shown 
row left ordered matrix columns grouped left 
second row columns grouped left sets 
grouping structure persists matrix 
history feature object defined 
object specified history refer full history feature znk 
histories features decimal equivalent binary numbers corresponding column entries 
example object features histories corresponding feature previous assignments feature feature feature possessed previous objects assigned 
kh denote number features possessing history number features mk kh number features mk 
method denoting histories facilitates process placing binary matrix left ordered form definition lof 
lof function binary matrices reduce left ordered form unique left ordered form binary matrix 
lof lof binary matrices left ordered form 
binary matrix left transformed left ordered binary matrix right function lof 
left ordered matrix generated exchangeable indian process 
empty columns omitted matrices 
define set equivalence classes 
binary matrices lof equivalent lof lof map left ordered form 
lof equivalence class binary matrix denoted set binary matrices lof equivalent lof equivalence classes preserved permutation rows columns matrix provided permutations applied members equivalence class 
performing inference level lof equivalence classes appropriate models feature order identifiable unaffected order columns model probability specified terms linear function pca property 
need evaluate cardinality number matrices map left ordered form 
columns binary matrix guaranteed unique object possess multiple features possible features possessed exactly set objects 
number matrices reduced contains identical columns re orderings columns result exactly matrix 
account cardinality 
kh 
kh count number columns full history lof equivalence classes play role binary matrices partitions assignment vectors collapse binary matrices assignment vectors differ column ordering class labels 
relationship precise examining classes binary matrices constructed assignment vectors 
define class matrix generated assignment vector binary matrix zik ci straightforward show class matrices generated assignment vectors correspond partition belong lof equivalence class vice versa 
infinite limit distribution defined equation probability particular lof equivalence class binary matrices 
kh 
mk mk 
order take limit expression divide columns subsets corresponding features mk features mk 
re ordering columns mk mk break product equation parts corresponding subsets 
product mk mk mk mk 
mk 
mk 
mk mk 
substituting equation equation rearranging terms compute limit lim 
kh 


kh 
exp hn mk 
mk 
mk mk 

hn nth harmonic number hn details steps taken computing limit appendix 
distribution exchangeable number identical columns column sums affected ordering objects 
indian process probability distribution defined equation derived simple stochastic process 
crp process assumes ordering objects generating matrix sequentially ordering 
metaphor defining stochastic process appropriately adjusted geography 
indian restaurants london offer apparently infinite number dishes 
define distribution infinite binary matrices specifying procedure customers objects choose dishes features 
indian process ibp customers enter restaurant 
customer encounters consisting infinitely dishes arranged line 
customer starts left takes serving dish stopping poisson number dishes plate 
ith customer moves sampling dishes proportion popularity serving probability mk mk number previous customers sampled dish 
having reached previous sampled dishes ith customer tries poisson number new dishes 
indicate customers chose dishes binary matrix rows infinitely columns zik ith customer sampled kth dish 
shows matrix generated ibp 
customer tried dishes 
second customer tried dishes tried new dishes 
third customer tried dishes tried previous customers dishes tried customer customers dishes binary matrix generated indian process 
new dishes 
vertically concatenating choices customers produces binary matrix shown 
indicate number new dishes sampled ith customer probability particular matrix produced process exp hn mk mk 

seen matrices produced process generally form 
matrices ordered arbitrarily poisson draws result choices new dishes right previously sampled dishes 
customers exchangeable distribution number dishes counted depends order customers choices 
pay attention lof equivalence classes matrices generated process obtain exchangeable distribution equation qn kh 
ma generated process map left ordered form obtained multiplying equation quantity 
possible define similar sequential process directly produces distribution left ordered binary matrices customers exchangeable requires effort part customers 
exchangeable indian process customer samples poisson number dishes moving left right 
ith customer moves single decision set dishes history 
kh dishes history mh previous customers sampled dishes customer samples binomial mh kh number dishes starting left 
having reached previous sampled dishes ith customer tries poisson number new dishes 
attending history dishes sampling left guarantees resulting matrix left ordered form easy show matrices produced process probability corresponding lof equivalence classes equation 
distribution collections histories section noted lof equivalence classes binary matrices generated assignment vectors correspond partitions 
likewise lof equivalence classes general binary matrices correspond simple combinatorial structures vectors non negative integers 
fixing ordering objects collection feature histories objects represented frequency vector indicating number times history appears collection 
collection feature histories translated left ordered binary matrix horizontally concatenating appropriate number copies binary vector representing history matrix 
left ordered binary matrix translated collection feature histories counting number times history appears matrix 
partitions subset collections histories collections object appears history process strictly general crp 
connection lof equivalence classes feature matrices collections feature histories suggests means deriving distribution specified equation operating directly frequencies histories 
define distribution vectors non negative integers assuming kh generated independently poisson distribution parameter mh mh mh mh 
mh number non zero elements history gives kh mh mh 
kh 
kh 
exp hn kh exp mh mh 
mh mh 
kh easily seen equation 
harmonic number histories nj exponential term obtained summing mh 
histories mh mh mh 


properties distribution hn 
different views distribution specified equation straightforward derive properties 
effective dimension model follows poisson hn distribution 
easily shown generative process described section kh process sum set poisson distributions 
sum set poisson distributions poisson distribution parameter equal sum parameters components 
equation hn 
second property distribution number features possessed object follows poisson distribution 
follows definition exchangeable ibp 
customer chooses poisson number dishes 
exchangeability technically vector non negative integers particularly generic example vector entries 
customers choose poisson number dishes specify ordering customers begins particular customer 
possible show remains sparse 
simplest way exploit previous result number features possessed object follows poisson distribution expected number entries 
consistent quantity obtained limit expectation finite model 
generally equation property sums poisson random variables described show follow poisson distribution 
consequently probability values higher mean decreases exponentially 
inference gibbs sampling defined distribution infinite binary matrices satisfies desiderata objects rows matrix exchangeable distribution 
remains shown inference infinite latent feature models tractable case infinite mixture models 
derive gibbs sampler latent feature models exchangeable ibp prior 
critical quantity needed define sampling algorithm full conditional distribution zik ik zik ik ik denotes entries zik leaving aside issue feature values moment 
prior contributes probability specifying zik ik 
finite model equation straightforward compute full conditional distribution zik 
integrating gives zik zik set assignments objects including feature number objects possessing feature including need condition ik columns matrix generated independently prior 
infinite case derive conditional distribution exchangeable ibp 
choosing ordering objects ith object corresponds customer visit obtain zik 
result obtained limit equation 
similarly number new features associated object drawn poisson distribution 
derived equation kind limiting argument obtain terms poisson 
latent feature model binary features derived prior infinite sparse binary matrices indicated statistical inference done models defined prior 
section show prior put models unsupervised learning illustrating issues graphical model linear gaussian model binary features 
arise process 
describe simple linear gaussian latent feature model features binary 
start finite model consider infinite limit 
finite linear gaussian model finite model dimensional vector properties object xi generated gaussian distribution mean covariance matrix zi dimensional binary vector matrix weights 
matrix notation za 
feature matrix form binary factor analysis 
distribution matrix gaussian nd exp tr za za tr trace matrix 
easy integrate model parameters need define prior take matrix gaussian exp kd tr parameter setting prior 
dependencies variables model shown 
combining equations results exponentiated expression involving trace za za za mz xm mz identity matrix line obtained completing square quadratic term second line 
integrate obtain da nd kd xm nd nd kd exp nd kd exp tr exp tr xm mz da exp tr tr 
result intuitive exponentiated term difference inner product matrix raw values projections space spanned regularized extent determined ratio variance noise variance prior derivation infer set observations provided prior finite feature model discussed prelude ibp prior 
full conditional distribution zik zik zik 
evaluating involves matrix multiplication need involve matrix inverse 
rewritten zt zi allowing rank updates efficiently compute inverse zi modified 
defining zt zj zi zim zt zi zim zim 
iteratively applying updates allows computed equation different values zik requiring excessive number inverses full rank update occasionally avoid accumulating numerical errors 
second part equation zik evaluated equation 
infinite limit sure define infinite version model need check remains defined unbounded number columns 
appears places equation behave 
zt examine left ordered form write consists columns sums mk consists columns sums mk 
follows expressions concerned reduces zt ik ik 
appearance expression problem see shortly 
abundance zeros leads direct reduction second expression ik uses finite portion combining results yields likelihood infinite model nd zt exp ik tr ik 
exponents appears result introducing multiples factor equation 
likelihood infinite model just likelihood finite model defined columns gibbs sampler model straightforward 
assignments classes drawn way finite model equation equation obtain equation zik 
finite case equations compute inverses efficiently 
distribution number new features approximated truncation computing probabilities range values reasonable upper bound 
value computed equation prior number new classes poisson 
demonstration applied gibbs sampler infinite binary linear gaussian model simulated dataset consisting images 
image xi represented dimensional vector pixel intensity values 
images generated representation latent features corresponding image elements shown 
image elements correspond rows matrix model introduced section specifying pixel intensity values associated binary feature 
non zero elements set indicated white pixels 
feature vector zi image sampled distribution feature probability 
image generated gaussian distribution mean covariance xi 
images shown feature vectors zi generate 
gibbs sampler initialized choosing feature assignments column setting zi probability 
initially set sampled adding metropolis steps mcmc algorithm see gilks 
shows trace plots iterations mcmc log joint probability log iteration stimuli results demonstration infinite binary linear gaussian model 
image elements corresponding latent features generate data 
sample images dataset 
image elements corresponding features possessed objects th iteration mcmc 
reconstructions images output algorithm 
lower portion shows trace plots mcmc simulation described detail text 
data latent features log number features object model parameters 
algorithm reached relatively stable values quantities approximately iterations remaining analyses samples taken point forward 
latent feature representation discovered model extremely consistent generate data 
shows distribution computed samples 
mode distribution samples tended include features large number objects features objects 
shows mean frequency objects tended possess different features ordering features frequencies sample 
features averaged objects remainder averaged 
shows distribution number features possessed object 
objects features objects 
model tended latent feature representation dominated features consistent representation generate data 
show quantities feature matrix generate data illustrating close correspondence posterior distribution true representation 
posterior mean feature weights 
shows posterior mean ak frequent features th sample produced algorithm ordered match features shown 
features pick image elements generating data 
shows feature vectors zi sample images posterior means reconstructions images sample 
similar reconstructions obtained averaging values produced markov chain 
reconstructions provided model clearly pick relevant features despite high level noise original images 
shown methods define infinite latent class models extended models objects represented terms set latent features deriving distribution infinite binary matrices prior models 
derived prior infinite limit simple distribution finite binary matrices shown distribution specified terms simple stochastic process indian process 
distribution satisfies desiderata prior infinite latent feature models objects exchangeable inference remains tractable 
number directions extended 
focussed distribution binary matrix indicating features possessed different objects intent combined prior feature values define richer infinite latent feature models discussed section 
anticipate mcmc algorithms similar described section applied models developed algorithm simple model discrete feature values infinite version cooperative vector quantization zemel hinton 
introducing feature values model raises significant technical issues models feature values represented explicitly structure model permit conjugate priors care taken ensure posterior distributions remain proper inference algorithms defined 
similar issues arise infinite mixture models discussed neal 
probability frequency frequency feature ordered frequency feature ordered frequency probability probability number features number features statistics derived mcmc simulation compared representation generate data 
posterior distribution number features possessed object 
mean frequencies objects assigned features ordered highest lowest sample 
distribution number features possessed object 
show statistics computed representation generate data 
second direction extended considering models priors 
particular infinite latent feature models relationship data features non linear may useful manifold learning problems 
number applications infinite latent feature models distributions binary matrices rows infinitely columns useful 
example matrices represent relations hold classes entities class contains known number entities class contains unknown number 
cases arise causal learning dependencies fixed set observable variables explained relationships variables unknown number hidden causes 
distribution defined prior graph structures causal learning problems kind 
large scale applications models require developing sophisticated inference algorithms 
gibbs sampling algorithms discussed rely local changes class feature assignments move space representations 
methods slow converge large problems tend get stuck local maxima posterior distribution sampler demonstration section stabilized rapidly explored modes posterior switching order features inference infinite mixture models improved supplementing local changes produced gibbs sampler metropolis hastings step occasionally produces global changes dahl jain neal 
similar algorithms may beneficial inference infinite latent feature models 
question methods lead priors infinite combinatorial structures 
obvious extension current explore distributions infinite binary matrices produced making different assumptions generation parameter model generated beta distribution 
range possibilities 
success transferring strategy limit finite model latent classes latent features suggests strategy fruitfully applied representations broadening kinds latent structure recovered unsupervised learning 
aldous 

exchangeability related topics 
cole de probabilit de saint flour xiii pp 

berlin springer 
antoniak 

mixtures dirichlet processes applications bayesian nonparametric problems 
annals statistics 
bernardo smith 

bayesian theory 
new york wiley 
blackwell macqueen 

ferguson distributions polya urn schemes 
annals statistics 
blei griffiths jordan tenenbaum 

hierarchical topic models nested chinese restaurant process 
advances neural information processing systems 
cambridge ma mit press 
blei ng jordan 

latent dirichlet allocation 
journal machine learning research 
bush maceachern 

semi parametric bayesian model randomized block designs 
biometrika 
dahl 

improved merge split sampler conjugate dirichlet process mixture models tech 
rep 
department statistics university wisconsin 
ghaoui jordan lanckriet 

direct formulation sparse pca semidefinite programming tech 
rep 
ucb csd 
computer science division university california berkeley 
escobar west 

bayesian density estimation inference mixtures 
journal american statistical association 
ferguson 

bayesian density estimation mixtures normal distributions 
rizvi eds advances statistics 
new york academic press 
geman geman 

stochastic relaxation gibbs distributions bayesian restoration images 
ieee transactions pattern analysis machine intelligence 
ghahramani 

factorial learning em algorithm 
advances neural information processing systems 
san francisco ca morgan kaufmann 
gilks richardson spiegelhalter 
eds 

markov chain monte carlo practice 
chapman hall 
green richardson 

modelling heterogeneity dirichlet process 
scandinavian journal statistics 
james 

gibbs sampling methods stick breaking priors 
journal american statistical association 
jain neal 

split merge markov chain monte carlo procedure dirichlet process mixture model 
journal computational graphical statistics 
jolliffe 

principal component analysis 
new york springer 
jolliffe 

modified principal component technique lasso 
journal computational graphical statistics 
neal 

bayesian mixture modeling 
maximum entropy bayesian methods proceedings th international workshop maximum entropy bayesian methods statistical analysis 
dordrecht kluwer 
neal 

markov chain sampling methods dirichlet process mixture models 
journal computational graphical statistics 
pitman 

combinatorial stochastic processes 
notes saint flour summer school rasmussen 

infinite gaussian mixture model 
advances neural information processing systems 
cambridge ma mit press 
rasmussen ghahramani 

occam razor 
advances neural information processing systems 
cambridge ma mit press 
sethuraman 

constructive definition dirichlet priors 
statistica sinica 
teh jordan beal blei 

hierarchical dirichlet processes 
advances neural information processing systems 
cambridge ma mit press 
ueda saito 

parametric mixture models multi labeled text 
advances neural information processing systems 
cambridge mit press 
west muller escobar 

hierarchical priors mixture models application regression density estimation 
freeman smith eds aspects uncertainty 
new york wiley 
zemel hinton 

developing population codes minimizing description length 
advances neural information processing systems 
san francisco ca morgan kaufmann 
zou hastie tibshirani 
press 
sparse principal component analysis 
journal computational graphical statistics 
appendix details limits appendix contains details limits expressions appear equations 
expression 

kk kk kk kk finite terms go zero 
second expression mk mk 


mk mk 
finite mk terms go zero 
third expression fact 
compute limit equation obtaining desired 
lim mk 

lim exp exp exp exp hn 
