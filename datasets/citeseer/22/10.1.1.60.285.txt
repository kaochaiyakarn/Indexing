multi level boundary classification information extraction finn nicholas kushmerick smart media institute computer science department university college dublin ireland finn nick ucd 
investigate application classification techniques problem information extraction 
particular support vector machines different feature sets build set classifiers 
show approach competitive current state art algorithms specialized learning algorithms 
introduce new technique improving recall algorithm 
approach uses level ensemble classifiers improve recall extracted fragments maintaining high precision 
show approach outperforms current state art algorithms benchmark tasks 
information extraction process identifying set pre defined relevant items text documents 
numerous algorithms machine learning techniques proposed 
algorithms monolithic sense clean separation learning algorithm features learning 
furthermore proposed algorithms effectively reinvent aspects machine learning specialized learning algorithms exploit existing machine learning algorithms 
investigate relatively standard machine learning techniques applied information extraction 
adopt standard classification formalization task classifying document position start field extract field :10.1.1.23.4524:10.1.1.23.4524
investigate different feature sets contribute performance algorithm 
show approach support vector machine classification competitive cases superior current state art approaches algorithms crafted specifically 
initial results describe improvements basic approach give superior performance variety benchmark tasks 
enhancements call multi level boundary classification consist combining predictions sets classifiers set high precision high recall 
intuition approach follows 
system consists sets classifiers 
classifiers adopt standard classification approach 
uses second level biased classifiers 
extract fragment need identify start 
classifier predicts fragment start assume correct 
prediction guide classifier identify complete fragment see fig 

fig 

example contributions 
show shelf support vector machine implementation competitive current algorithms specialized learning algorithms 
second significant introduce novel multi level boundary classification approach demonstrate new approach outperforms current algorithms variety benchmark tasks 
prior research discussion comparison prominent adaptive algorithms 
rapier uses inductive logic programming techniques discover rules extracting fields documents :10.1.1.10.6389
try identify start tags separately learns identify relevant strings entirety 
rapier performs specific bottom search starting specific rule positive training example repeatedly trying generalize rules cover positive examples 
rapier uses features tokens part speech information semantic class information 
bwi learns large number simple wrapper patterns combines boosting :10.1.1.23.4524
bwi learns separate models identifying start tags uses histogram training fragment lengths estimate accuracy pairing start tag 
bwi learns identify start tags form specific search 
bwi features consist actual tokens supplemented number orthographic generalizations alphabetic capitalized alphanumeric lower case numeric punctuation modest amount lexical knowledge list names 
lp learns symbolic rules identifying start tags 
bwi identifies starts ends fields separately 
addition token orthographic features lp uses shallow linguistic information morphological partof speech information 
uses user defined dictionary gazetteer 
learning algorithm covering algorithm starts specific rules tries generalize cover positive examples possible 
process supplemented learning correction rules shift predicted tags order correct errors learner 
snow snow relational learning algorithm specifically tailored large scale learning tasks 
snow identifies fragments extracted separately identifying start tags 
uses token orthographic pos semantic features 
learns stages 
stage involves filtering candidate fragments second involves picking correct fragments remaining 
bwi uses fewest features uses just tokens orthographic information 
lp rapier snow supplement features part speech semantic information 
elie algorithm information extraction classification 
treat tasks multiple fields multiple independent single field extraction tasks extract field time 
treat identification field start positions distinct token classification tasks :10.1.1.23.4524
tokens labeled field positive instances start classifier tokens negative instances classifier 
similarly positive examples classifier tokens labeled field instances negative examples 
features encoding 
instance set features describe token 
features include specific token part speech pos chunking orthographic gazetteer information 
token 
actual token 
pos 
part speech token 
token tagged corresponding pos brill pos tagger :10.1.1.38.8357
represent chunking information tokens 
pos tags grouped noun phrases verb phrases 

values associated token gazetteer 
gazetteer user defined dictionary 
contains lists names names taken census bureau list countries cities time identifiers am pm titles jr list location identifiers postal service street boulevard 
orthographic 
features give various orthographic information token 
examples features include token upper case lower case capitalized alphabetic numeric punctuation 
encoding tokens dataset manner gives large number attributes 
filter attributes information gain order discard irrelevant features reduce learning time 
relational information encoded additional features 
represent instance encode features particular token 
addition fixed window size add features previous tokens tokens 
example window size instance feature represent token instance token preceding instance token instance 
similarly features represent pos orthographic information current instance previous instances 
learning elie 
elie algorithm distinct phases 
phase elie simply learns detect start fragments extracted 
experiments demonstrate phase generally high precision low recall 
second phase designed increase recall 
find false negatives extracted start correctly identified start 
second phase elie trained detect fragment fragment 
level learning 
learner treats standard classification task augmented simple mechanism attach predicted start tags 
fig 
shows learning process 
set training examples converted set instances start tags described 
token training document single instance positive negative example start tag 
instances encoded features particular token question tokens surrounding 
attributes filtered information gain 
instances passed learning algorithm uses learn model 
training phase models start tags start pairs 
start pairs passed tag matcher charged matching start tags 
experiments involve tag matcher derives histogram number tokens start tag training data 
matching predictions probability start tag paired tag estimated proportion field length occurred training data 
approach performs adequately don focus tag matching 
intelligent tag matcher may improve performance 
example tag matcher incorporate learning component learns shift tags correct errors output predictions 
current experiments weka 
weka smo algorithm learner algorithms substituted 
fig 

elie architecture 
level learning 
learner builds model large number negative instances small number positive instances 
prior probability arbitrary instance boundary small 
low prior probability means model produce false negatives false positives 
learner learned training data prior probability instance boundary higher learner number irrelevant instances vastly reduced 
focused training data constructed follows 
building start model take instances occur fixed distance tag 
similarly model instances occur fixed distance start tag 
fig 
shows example instances lookahead lookback 
example token bill start field token field 
building classifiers available instances 
building start model token tokens preceding 
building model start token tokens 
note instances encoded way difference simply learner allowed look small subset available training data 
extracting classifier applied tokens token tagged start token 
similarly start classifier applied instances tagged preceding tokens 
technique selecting training data means models higher recall lower precision models 
blindly apply model entire document generate lot false positives 
shown fig 
fig 
reason model improve performance apply regions documents model prediction 
specifically extraction classifiers predictions models identify parts document predicted contain fields 
classifiers generally high precision low recall intent procedure enable elie converge correct boundary classifications 
level approach takes advantage fact highly dependent learners high precision 
prediction indicates high probability second prediction 
training classifier drastically alter prior probabilities training data instances fixed distance annotated start 
classifier predictions trained smaller set negative instances 
identify starts ends classifier missed 
experiments evaluated elie algorithm benchmark datasets compared results achieved algorithms 
evaluation method 
truly comprehensive comparison compare algorithm dataset splits exact scoring method 
unfortunately conclusive comparison different algorithms impossible published results 
algorithms evaluated slightly different methodologies simply report results corpus 
orthogonal issues regarding evaluation give credit partial matches occurrences field extracted 
evaluation conservative results comparison competitors adopted liberal evaluation strategy 
experimental setup 
evaluate algorithm standard benchmark datasets seminar announcements sa dataset job postings jobs dataset reuters corporate acquisitions reuters dataset fields see fig :10.1.1.10.6389
sa consists seminar announcements annotated fields detailing upcoming seminars 
jobs consists newsgroup messages detailing jobs available austin area 
dataset annotated fields 
reuters consists reuters articles describing corporate acquisitions 
dataset annotated fields 
difficult task fields related 
example separate annotations name abbreviated versions name 
split dataset repeated times 
experiments window length tokens lookahead lookback tokens 
sa dataset typically gives set approximately training instances positive approximately attributes 
experiments features enabled initially top features ranked information gain learning model 
compare system bwi rapier lp snow available published results system 
systems published results fields 
experimental results 
fig 
compares performance 
results measured passing predictions classifiers directly tag matcher bypassing classifiers see fig 

fields shown 
sa jobs 
field equal higher precision 
field higher recall fields especially lower performance increase recall large 
fields higher measure 
causes precision drop recall rise 
cases increase recall greater drop precision giving corresponding increase measure 
fig 
compares precision recall measure bwi rapier lp snow 
graph horizontal axis shows performance vertical axis shows performance systems 
points diagonal indicate outperformed particular system particular field 
points diagonal indicate competitor system outperformed 
recall measure outperforms algorithms fields 
systems perform better precision considered 
improved recall expense precision task requires high precision 
learning algorithms features test learning algorithm different feature sets contribute performance evaluated elie various learning algorithms reduced sets features sa dataset 
compared smo known learning algorithms naive bayes winnow ripper 
sa naive bayes performs quite poorly fields 
winnow performs etime field poorly fields 
ripper performs fields competitive smo 
experiments different feature sets showed location stime etime fields performance comes token features 
fields adding pos orthographic features gives small increase performance 
speaker field get token features compared features 
pos orthographic features addition token features gives increase performance token features gives increase token features 
indicates gazetteer significantly enhances performance field 
unsurprising gazetteer contains list names 
addition pos orthographic provide significant benefit 
fields gazetteer provides little benefit 
fields gazetteer contain information relevant field 
appropriate gazetteer list improve performance fields general may involve significant additional effort 
fig 

comparison performance versus fig 

comparison elie systems dataset field fp fn fp fn sa speaker sa location sa stime sa etime jobs id jobs title jobs jobs salary jobs jobs state jobs city jobs country jobs language jobs platform jobs application jobs area jobs req exp jobs des exp jobs req degree jobs des degree jobs post date reuters acquired reuters purchaser reuters seller reuters reuters reuters reuters reuters reuters reuters status fig 

elie error analysis 
elie error analysis 
table shows details errors elie 
fields benchmark datasets show ratio false positives false negatives fp fn 
shows percentage false positives partially correct percentage false negatives partially predicted 
false positive partially correct means elie extracted fragment correct start predicted exactly 
kinds predictions useful practical setting conservative method evaluation give credit kinds errors 
fields large proportions errors form 
false negative partially predicted means fragment failed extract predicted start correctly may predicted 
kinds errors facilitate improvement shown 
general gives large reduction partial errors 
ratio fp fn shows errors false negatives generally see increase false positives reduction false negatives 
discussion summary system outperformed compared fields terms recall measure 
high precision required 
evaluated system conservatively performance may relation competitors 
learner consistently improves recall keeping precision high 
difficult fields improvements generally larger 
classifier improves recall usually keeps precision high improve 
investigation errors elie produces reveals errors false negatives 
false positives kinds 
result exact matching evaluation tagged field correctly 
second occur result labeling errors data extract labeled 
accuracy elie main sources 
firstly classifier gives better performance algorithms conclude support vector machines learning algorithm gives rise substantial improvement compared specialized learning algorithms algorithms 
secondly level classification described give significant increases performance 
increases recall maintaining precision 
cases improves elie performance substantially 
described approach treats information extraction token classification task 
smo fast support vector machine implementation elie algorithm learns set classifiers information extraction competitive cases outperform current algorithms specialized learning algorithms 
described multi level boundary classification new way combining classifiers information extraction yields significant performance improvements 
approach exploits high precision token classifiers increase recall algorithm 
algorithm outperformed current algorithms benchmark datasets 
fields especially difficult gave large improvements performance 
scope improvement elie 
plan analyze detail approach give dramatic improvements recall specify precisely properties algorithm documents facilitate 
learning components may improve elie performance component learns recognize correct prediction errors similar lp correction rules 
modification add third level classifier takes predictions classifies extracted fragment correct 
performance may improved changing elie combines predictions 
currently elie uses predictions 
feasible predictions identify incorrect predictions remove 
sophisticated tag matcher improve performance 
acknowledgments 
research funded science foundation ireland office naval research 
fabio ciravegna access lp 
eric brill 
advances transformation parts speech tagging 
aaai 

mary elaine califf raymond mooney 
relational learning pattern match rules information extraction 
proc 
th nat 
conf 
artifical intelligence 

fabio ciravegna 
adaptive information extraction text rule induction generalisation 
proc 
th int 
joint conf 
artificial intelligence 

william cohen 
fast effective rule induction 
icml 

dayne freitag 
machine learning information extraction informal domains 
phd thesis carnegie mellon university 

dayne freitag nicholas kushmerick 
boosted wrapper induction 
proc 
th nat 
conf 
artificial intelligence 

alberto mary elaine califf fabio ciravegna dayne freitag claudio nick kushmerick romano 
critical survey methodology evaluation 
th international conference language resources evaluation 

nick littlestone 
learning quickly irrelevant attributes abound new linear threshold algorithm 
machine learning 

leonid peshkin avi pfeffer 
bayesian information extraction network 
proc th int 
joint conf 
artifical intelligence 

john platt 
fast training support vector machines sequential minimal optimization 
sch lkopf burges smola editors advances kernel methods support vector learning 
mit press 

ross quinlan 
programs machine learning 
morgan kaufman 

dan roth 
learning resolve natural language ambiguities unified approach 
national conference artificial intelligence 

dan roth wen tau yih 
relational learning propositional algorithms information extraction case study 
th international joint conference artificial intelligence 

ian witten eibe frank 
data mining practical machine learning tools techniques java implementations 
morgan kaufmann 
