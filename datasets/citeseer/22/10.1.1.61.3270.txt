fast particle smoothing particles mike cs ubc ca department computer science university british columbia canada mark mb cam ac uk department information engineering cambridge university uk de freitas cs ubc ca arnaud doucet arnaud cs ubc ca department computer science university british columbia canada simon signal com advanced signal information processing group uk lang cs toronto edu department computer science university toronto canada propose efficient particle smoothing methods generalized state spaces models 
particle smoothing expensive algorithm number particles 
overcome problem integrating dual tree recursions fast multipole techniques forward backward smoothers new generalized filter smoother maximum posteriori map smoother 
experiments show improvements substantially increase practicality particle smoothing 

belief inference known smoothing signal processing control literature heart learning paradigms 
sequence observations 
yt initial distribution hidden states transition model xt xt observation model yt xt bayesian theory provides sound framework estimating filtering distribution xt smoothing distribution xt map sequence arg maxx bayesian solution obtained closed form probabilistic graphical model defined transition observation models gaussian tree model discrete tree model 
numerical approxi appearing proceedings rd international conference machine learning pittsburgh pa 
copyright author owner 
mation techniques employed 
algorithms smoothing tree structured graphical models model distributions non gaussian nonlinear non stationary 
ideas extended junction trees simplify presentation focusing chain structured graphs 
historically particle filtering pf proved efficient method approximating filtering distribution see example doucet hundreds scientific publications topic 
contrast particle smoothing hardly received attention 
reasons 
smoothing algorithms filter smoother tfs forward backward smoother fbs map smoother incur cost typically large number particles 
note contrast pf 

existing particle kitagawa isard blake assumptions hard verify satisfy practice 
practitioners filtering proxy smoothing 
example pf postprocessing video sequences sports 
terribly limiting smoothed estimates access data entire video sequence far accurate 
addition historic focus inference hardly learning parameter estimation particle methods 
surprising 
order obtain map maximum likelihood parameter estimates needs smoothed estimates states done expectation maximization algorithm linear gaussian models discrete hidden markov models 
markov chain monte carlo mcmc techniques perform poorly model definition states strongly correlated time 
order particle smoothing viable proposes fast particle smoothing particles unifying treatment comparison particle smoothing methods general state spaces including fbs tfs map smoothing 
fast implementation methods dual metric tree recursions gray moore fast multipole algorithms greengard rokhlin 
anecdotally show models likelihood easy evaluate carry smoothing chain length particles minute 
generalized filter particle smoother circumvents previous unrealistic assumptions 
state space formulation models nonstationary nonlinear multi modal assumption transition prior xt xt defined metric space 
assumption restrictive applies domains interest including image tracking beat tracking robotics econometric models 
interest fast methods discrete state spaces refer reader 
mention maximum posteriori map estimation deterministic grids distance transform algorithm applicable type multivariate monte carlo grids arise particle smoothing 

bayesian filtering general bayesian filtering step procedure 
estimate xt obtain xt follows forward prediction xt xt xt xt dxt forward update xt yt xt xt yt xt xt dxt integrals procedure intractable approximated standard pf methods doucet 
particular pf obtains monte carlo approximation filtering distribution dxt dxt 
denotes normalized importance weight th particle dxt dirac distri bution see example introductory chapter doucet detailed derivation 
filtering step carried smoothers 

bayesian smoothing section forward backward smoother map smoother new generalized filter smoother 

forward backward smoother smoothed density xt factored follows xt xt xt dxt xt xt xt dxt xt filtered smoothed xt xt xt dynamics xt xt xt dxt state prediction dxt 
obtain recursive formula smoothed density xt terms filtering density xt state prediction density xt smoothed density time xt 
algorithm proceeds making forward filtering pass compute filtered distribution time step backward smoothing pass determine smoothing distribution see 
approximate distribution defining dxt dxt obtained backward recursion importance weights fast particle smoothing particles note fbs maintains nal particle locations particles obtain approximation smoothed density 
success depends filtered distribution having support smoothed density significant 
equation costs operations evaluate directly easily performed observing denominator fraction equation independent performed independently expensive reduce cost 

filtering 

perform particle fil tering obtain weighted measure 
initialization 

set 
backward recursion 


nx pn forward backward particle smoother 

filter smoother smoothed marginal distribution obtained combining result independent filter procedures running forward time backward 
factorization xt xt yt xt yt xt yt xt yt xt filter filter filter familiar bayesian filter 
filter computed sequentially backward information filter mayne noting yt xt yt xt xt xt yt xt dxt 
problem recursion yt xt probability density function argument xt integral xt finite recursion blows 
existing monte carlo particle methods approximate yt xt unrealistic assumptions 
isard blake kitagawa implicitly assume yt xt dxt develop sequential monte carlo methods context 
assumption violated methods apply 
solution problem relies artificial distribution xt 
distribution enables define recursion terms finite quantities allow monte carlo methods 
new recursion assuming xt known 
subsequently choices xt impact approximation hand see 
key idea new algorithm express backward filter terms finite quantities xt xt yt xt xt yt xt xi xi yi xi 
derivation ratio follows yt xt xt xt yt xt xt dxt xi xi yi xi dxt xt yt dxt xt xt yt xt clear way xt introduced derivation arbitrary distribution long cause obvious divisions zero 
derive recursive algorithm xt yt expansion yt xt yt xt xt xt yt xt dxt xt yt yt xt xt xt xt xt yt xt dxt xt yt xt xt xt dxt 
xt expansion implies step backward filtering procedure backward prediction xt yt backward update xt yt xt yt xt xt xt dxt xt yt xt xt yt yt yt dx backward recursion initialized xt yt yt xt xt yt note ep xt yt may proper probability distribution finiteness sufficient guarantee convergence monte carlo setting 
recursion xt yt remains specify xt 
clear derivation long positive xt positive 
possible define optimal xt respect minimizing variance resulting incremental importance weights normally performed standard particle filtering choice xt dependent specific choice proposal distribution 
choices intuitively better 
prior analytic compute xt fast particle smoothing particles xt xt dx sensible choice xt xt 
xt xk xk xt xk xk xk xk xk xk xk xk fraser potter consider case 
algorithm general require prior analytic 
approximation prior suffice 
example systems produce realizations markov chain prior xt xt obtain samples xt xt obtained fitting samples analytical distribution 
markov chain ergodic run single markov chain obtain approximate samples invariant distribution 
analytical approximation invariant distribution 
readers familiar notion message passing discrete probabilistic graphical models note artificial distribution allows avoid numerical underflow issues associated calculating messages belief propagation algorithm 
exactly pearl suggesting suggests normalizing messages avoid numerical problems pearl 
implicitly choosing xt xt message probability measure 
approach general thought clever choices lead messages numerical properties 
terms monte carlo implementation weighted measure approximation produced filter conventional forward forward filtering 

perform particle filtering obtain weighted measure 
backward filtering 

initialization 
sample candidate proposal distribution ex yt 
compute importance weights ew yt ex ex ex yt 


sample candidate particle proposal distribution ex ex yt 
calculate backward importance weights ew ew yt ex ex ex ex ex ex ex yt resample necessary 
filter smoother 
note order running filters may interchanged 
filter 
filter backward recursion xt yt 
algorithm shown 
particles resampled discrepancy weights high 
combine forward backward approxima tions xt xt yt obtain xt ep xt yt xt yt xt approximation dxt dxt 
ex note computational bottleneck arising expression 

important warning common mistake smoothing literature inverse dynamics obtain backward transition xt xt 
see erroneous consider auto regressive process xt axt admits proper application bayes rule xt xt xt xt xt xt marginalization backward kernel xt xt axt hand inversion dynamics xt xt incorrectly leads xt xt xt 

maximum posteriori map smoother applications interested maximum posteriori sequence map fast particle smoothing particles arg max 
continuous state spaces equation admits analytic form 
godsill sequential monte carlo approximation developed 
standard particle filtering performed 
time set particles viewed discretization state space 
particles survive regions high probability discretization sensible 
approximate finding sequence maximum probability grid map arg max 
approximation requires consideration number paths grows exponentially time 
maximization solved efficiently dynamic programming 
due markov decomposition model equation additive log space viterbi algorithm compute eq 
efficiently shown 
fast body monte carlo key computational bottleneck fbs tfs evaluation appropriate substitutions case 
map smoothing bottleneck max 
shall refer bottlenecks max kernel problems respectively 
assuming transition model similarity kernel defined metric space xt xt xt xt denotes distance problem approximated log steps algorithms body simulation 
algorithms guarantee approximate solution pre specified error tolerance control approximation error priori 
general 
filtering 

perform particle filter ing obtain weighted measure 
initialization 

log log 
recursion 


log yt max log arg max log 
termination 
arg max ex map 
backtracking 
ex map 
aggregation 
ex map ex map ex map 
ex map map smoothing algorithm produces approximation map employing viterbi algorithm discretized state space induced particle fil ter approximation time 
computational expense algorithm due recursion step 
popular examples body algorithms sum kernel problem include fast multipole expansions greengard rokhlin box sum approximations spatial index methods moore 
fast multipole methods tend low dimensions need re engineered time new kernel transition model adopted 
popular multipole method fast gauss transform algorithm greengard sun name implies applies gaussian kernels 
particular case gaussianity possible attack larger dimensions say adopting clustering partitions improved fast gauss transform yang 
computational storage cost fast multipole methods claimed empirically algorithms behave log storage 
spatial index methods kd trees ball trees general easy implement applied high dimensional spaces gray moore gray moore 
particular apply monotonic kernels defined metric space continuous distributions discrete ones 
building trees costs log practice run time cost behaves log 
tree methods applied exactly solve max kernel problem 
provide intuition fast algorithms brief explanation tree methods sum kernel problem 
step methods involves partitioning particles recursively shown 
kd tree partition state space 
fast particle smoothing particles lower upper node bound influence node particles query particle move node particles closest farthest positions node 
compute bound need carry single kernel evaluation 
nodes tree maintain statistics sum weights node 
want evaluate effect particles node query particle metric component transition kernel 
shown sum ap upper lower bounds follows upper lower lower upper lower upper closest farthest distances query particle node er upper lower ror approximation needs recurse tree level pre specified error tolerance guaranteed 
query particles possible improve efficiency tree methods building trees source query points 
comparing nodes individual query particles compares nodes query nodes 
detailed explanations dual tree techniques appear gray moore gray moore 

experiments 
linear gaussian model simple experiment show possible conduct particle smoothing observations minute particles 
particular fbs tested threedimensional linear gaussian state space model populated synthetic data 
keep model sufficiently simple compare particle smoother analytic solution case kalman smoother 
sum kernel problem acceleration method necessarily introduces error 
report cpu time na fast method case error algorithms amount cpu time 
results verify particles smoothed approximately better terms rmse fewer particles smoothed exactly 
rms error standard cpu time cpu time particles standard particle smoothing results synthetic data shown log log scale clarity 
data plots stem experiment 
computation cost achieves rmse orders magnitude lower 
able smooth particles method 

non linear non gaussian model consider synthetic model xt xt xt cos xt yt posterior multi modal non gaussian nonstationary 
ideal setting compare time exact filter dual tree filter exact dual tree particles fast particle smoothing particles time exact filter dual tree filter exact forward backward dual tree forward backward particles rmse fwd filter bkw filter exact dt exact dt particles non linear non gaussian model 
exact dual tree accelerated algorithms 
left middle algorithms provide orders magnitude speedup 
tfs amenable acceleration fbs factor 
right fbs performs filtering tfs significantly performs 
rmse approximate algorithms comparable exact versions 
particle smoothers 
closed form expression natural choice xt simply flat gaussian distribution 
tested forward filtering backward information filtering forward backward smoothing filter smoothing monte carlo runs realizations model 
contains results 
tfs significantly accurate filtering fbs surprisingly faster 
conjecture due resulting kernel matrix sparser providing opportunities node pruning 

parameter learning sv models stochastic volatility sv models extensively analysis financial time series data kim 
common inference problem estimate parameters complex models coordinate ascent algorithms em perform inference 
sv model written follows xt xt yt exp xt independent standard gaussian noise processes 
initial state independent states times distributed invariant distribution process evolution 
standard em algorithm current estimate arg max log log dx application natural invariant distribution artificial prior maximize analytically possible maximize modified function initial state discarded setting xt xt xt xt exp xt 
algorithm proceeds iteratively computing smoothed marginals xt step updating parameters eq 
step 
contains results applying tfs stochastic volatility model 
note expectations eq 
involve smoothed distributions application smoothing merely higher quality version filtering integral problem 
log prob fast particle smoothing particles time exact parameter estimation example log likelihood smoothing step time 
particles runs set tolerance time exact algorithm takes iteration converged 

beat tracking beat tracking process determining time slices raw song file correspond musical beats 
challenging multi modal map problem cemgil kappen 
applied model lang de freitas survive cake map particle smoothing algorithm described section exact dual tree acceleration method 
dual tree version faster particles dominates takes smooth particles na computation requires see 
problem benefits high particle count particles results map sequence probability particles results 
time naive dual tree particles distance computations naive dual tree particles beat tracking results log log scale 
method efficient ms dominates na method 

experiments demonstrate practical particle smoothing part standard statistical machine learning toolbox 
assist software available author website 
nice feature smoothing algorithms working pf adding smoothing step trivial 
noted people working image tracking map smoother improve results 


filter formula discrete time non linear bayesian smoothing 

doucet 

smoothing algorithms state space models technical report cued infeng tr 
cambridge university engineering department 
cemgil kappen 

monte carlo methods tempo tracking rhythm quantization 
jair 
doucet de freitas gordon 
eds 

sequential monte carlo methods practice 
springer verlag 
huttenlocher kleinberg 

fast algorithms large state space hmms application web usage analysis 
nips 
fraser potter 

optimum linear smoother combination optimum linear filters 
ieee transactions automatic control 
godsill doucet west 

maximum posteriori sequence estimation monte carlo particle filters 
ann 
inst 
stat 
math 
gray moore 

body problems statistical learning 
nips pp 

gray moore 

rapid evaluation multiple density models 
artificial intelligence statistics 
greengard rokhlin 

fast algorithm particle simulations 
jcp 
greengard sun 

new version fast gauss transform 
doc 
math icm 
isard blake 

smoothing filter condensation 
eccv pp 

freiburg germany 
kim shephard chib 

stochastic volatility likelihood inference comparison arch models 
review economic studies 
kitagawa 

monte carlo filter smoother non gaussian nonlinear state space models 

lang de freitas 

fast maximum posteriori inference monte carlo state spaces 
artificial intelligence statistics 
lang de freitas 

beat tracking graphical model way 
nips 
cambridge ma mit press 
mayne 

solution smoothing problem linear dynamic systems 
automatica 
moore 

anchors hierarchy triangle inequality survive high dimensional data technical report cmu ri tr 
robotics institute carnegie mellon university pittsburgh pa de freitas little lowe 

boosted particle filter multitarget detection tracking 
eccv 
prague 
pearl 

probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann 
doucet pe rez 

maintaining multi modality mixture tracking 
iccv 
yang davis 

improved fast gauss transform efficient kernel density estimation 
iccv 
nice 
