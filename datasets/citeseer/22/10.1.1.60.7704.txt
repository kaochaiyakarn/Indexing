rank approximation tensors image matrix representation wang narendra ahuja beckman institute university illinois urbana champaign usa ahuja vision ai uiuc edu novel multilinear algebra approach reduced dimensionality representation image ensembles 
treat image matrix vector traditional dimensionality reduction techniques pca higher dimensional data tensor 
helps exploit spatio temporal redundancies information loss image vector methods 
challenges lie computational memory requirements large ensembles 
currently exists rank approximation algorithm applicable number dimensions efficient low rank approximations 
larger dimensionality reductions memory time costs algorithm prohibitive 
propose novel algorithm rank approximations tensors efficient arbitrary important special case image ensembles video 
algorithms reduce redundancies dimensions 
rank tensor approximation yields compact data representation known image matrix methods 
evaluated performance algorithm vs approaches number datasets main results 
fixed compression ratio proposed algorithm yields best representation image ensembles visually squares sense 
second proposed representation gives best performance object classification 

computer vision applications require processing large amounts multidimensional data face object databases recognition retrieval video sequences security surveillance ct mri images medical analysis shape sequences animation 
dimensions mix space time video sequence dimensions spatial third temporal 
data size amount redundancy increase fast dimensionality desirable obtain compact concise representations data identifying suitable basis functions subspace representation 
process finding set compact bases projections data bases representation referred dimensionality reduction 
large number dimensions dimensionality reduction computation memory intensive process 
focus development novel multilinear algebra algorithm reduces redundancies dimensions treating data tensor image case time memory efficient 

related traditional methods reducing dimensionality image ensembles usually transform image vector concatenating rows 
example methods principle component analysis pca widely face representation face recognition applications 
pca find set mutually orthogonal basis functions capture largest variation training data 
features obtained projecting zero mean image dimensional subspace obtained singular value decomposition svd 
terzopoulos proposed higher order orthogonal iteration algorithm de lathauwer 
apply method vector representation 
shown root mean squared error rmse reconstruction compression ratio higher pca 
inherent problem vector representation lies spatial redundancies image matrix fully utilized information local spatial relationships lost 
realizing intrinsic problem matrix vector formulation researchers computer vision machine learning begun treat image matrix 
shashua levin proposed representing collection images set rank matrices 
yang proposed dimension pca constructing image covariance matrix original image matrices 
noted image representation memory efficient pca terms storage requirements requires coefficients pca 
ye proposed method called low rank approximation matrices lram 
contrast pca lram projects original data dimensional space projection largest variance dimensional spaces 
multilinear algebra received broad attention computer vision signal processing 
higher order singular value decomposition hosvd computer vision applications face recognition facial expression decomposition wang ahuja 
tensor rank decomposition algorithm proposed wang ahuja compact representation multidimensional data tensors 
iteratively finding best approximation residual tensor squares sense higher order tensor decomposed collection rank tensors 
method significantly reduces reconstruction rmse image ensembles compared pca compression ratio 
best rank rn approximation higher order tensors theoretically studied lathauwer 
approach projects tensor data rn dimensional space rank constraints 
time space cost approach impractical computer vision applications 
propose novel rank tensor approximation approach multilinear algebra 
projecting tensor data rn dimensional space expect obtain closest approximation original tensor 
advantages method follows exploit redundancies data dimensions obtain compact representation tensors 
rank approximation accurate representing data sum rank approx columns bases rank approximation mutually orthonormal 
time space complexities lower compared existing algorithm generalized rank tensor approximation 
important large dataset applications 
rest organized follows 
give brief overview multilinear algebra formulation rank approximation tensors section 
describe algorithms problem generalized rank approximation tensors section rank approximation third order tensors section 
section report experimental results quality computational complexity representation efficacy object recognition 
section 
rank approximation tensors overview multilinear algebra section introduce relevant preliminary material concerning multilinear algebra 
notation matrices denoted italic capitals tensors calligraphic letters 
high order tensor denoted mode product tensor matrix jn denoted defined tensor entries ai 
scalar product tensors defined ai 
frobenius norm tensor defined rank denoted rn dimension vector space spanned mode vectors 
example nth order tensor rank equals outer product vectors 
un ai 
values indices written unfolding tensor nth mode denoted uf 
sources details multilinear algebra 
svd matrices hosvd developed tensors 
tensor expressed product 
tensor sin properties table 
comparison data representation different methods method image formulation scalars pca principle components vector pi rank rank tensors matrix lram dimension matrix di rank dimension matrix orthogonality ordering sin 
unitary matrix 
problem formulation real nth order tensor find tensor having rank rank rn minimizes squares cost function arg min desired tensor represented nu rn rn orthonormal columns image ensembles third order tensors dimensionality image number images 
assume simplicity 
name approach rank approximation tensors 
data representation major characteristics rank approximation approach treats image matrix opposed simplifying vector ii reduces redundancies dimensions 
representation data pca consists eigenvectors reduced representations tensor rank decomposition consists projection corresponding tensor lram projects image matrices projection rank approximation tensor approximated core tensor subspaces comparison methods table mean data 
image ensemble applications number images usually greater dimension dimensionality reduction assume representation original data rank approximation compact lram 
rank approximation tensors extract features image ensemble 
projecting original tensor data rn axis system obtain new tensor 
projecting combination axes system define feature slice plane defined axes 
projections third order tensor axes defined test image represented tensor size matrix feature image 

generalized rank approximation tensors recall cost function equation matrices obtained solving classical linear squares problem orthonormal columns stated minimization equation equivalent maximization matrices having orthonormal columns algorithm generalized rank approximation tensors data result find initialize ik rk uf kr svds uf kr svds uf kr svds bj bj function apply alternative squares als find local optimal solution equation 
step optimize matrices keep fixed 
example fixed project tensor rn rn rn dimensional space nu columns orthonormal basis dominant subspace projection 
equation expressed matrix format ease implementation uf represents kronecker product denoted kr 
algorithm rank approximation tensors described algorithm 
issue implementation initialization algorithm 
original algorithm proposed values rn initialized truncation hosvd 
columns column wise orthogonal matrices span space dominant left singular vectors matrix unfoldings uf 
computation hosvd expensive uniformly distributed algorithm rank approximation third order tensors data third order tensor result find initialize ik rk ak svds ai svds svds bj bj random numbers columns necessarily orthonormal 
iterative search methods empirically seen major difference results obtained initializations 
initializations simpler compute hosvd 
algorithm advantages disadvantages 
time spent algorithm computation eigenvectors svd 
time complexity svd matrix rc min 
total time cost max inr min means efficient low rank approximation small 
provides generalized framework approximating tensors order 
hand increases time memory requirements increase fast making algorithm inefficient large 
rank approximation third order tensors algorithm designed approximating tensors order 
section consider important special case image ensembles third order tensors 
specific algorithm rank approximation third order tensors called slice projection 
approach inspired ye 
basic idea slice projection rank approxi mation tensors similar algorithm tensor transformed matrices convenience manipulation 
algorithm tensor unfolded different coordinate axes formulate matrices third order tensor represented slices coordinate axes ai aj ak 
slice represented matrix orthogonal direction 
projecting slices direction corresponding coordinate axes rank constraints find best approximation original slices 
need maximize norm best approximation original tensor corresponds maximizing summation norms slice projections directions 
problem formulated follows tensor ai aj ak find solve max orthonormal columns 
theorem describes find locally optimal solution iterative procedure 
theorem slice projection theorem 
optimal solution maximization problem equation 
consists eigen vectors matrix largest eigenvalues 
aj ai corresponding consists eigen vectors matrix largest eigenvalues 
ak corresponding consists eigen vectors matrix largest eigenvalues 
corresponding projection coordinate axes represented proof 
maximizes max term equation rewritten trace trace trace 
aj maximal consists eigenvectors matrix corresponding largest eigenvalues 
second term equation obtain maximizes trace 
case locally optimal maximization equation 
similarly show parts theorem 
theorem provides iterative procedure find updating iteratively procedure converge local maximum equation 
described algorithm 
advantage algorithm time memory efficient rank approximation third order tensors cost finding eigenvectors tradeoff algorithm works tensors 
algorithm general proposed ye consider projection temporal axis algorithm achieves reduction spatial temporal axes 
contrasts different projection schemes approaches 

experimental results section experimentally evaluate performance proposed algorithm respect quality representation computational complexity efficacy object classification 
datasets include toy video frames face databases orl dataset www uk research att com html rank approximation tensor original tensor low rank approximation matrices 
illustration rank approximations tensors vs low rank approximations matrices ye 
dimension 
images yale dataset images 
yale dataset images centered aligned eyes positions cropped 
compact data representation applied pca lram rank decomposition rank approximation tensors methods different datasets excluded hosvd stated reconstruction error larger hosvd pca 
parameters achieve constant compression ratio different algorithms table 
compression ratio defined number scalars required representation data see table 
compute rmse error reconstructed data respect original data rmse measure relative performance algorithms 
shows reconstructions obtained compression ratio different representation methods 
toy sequence temporal redundancy strong scene change frame frame spatial redundancies exist 
interestingly reconstruction lram visually worse pca 
reconstruction lram blurry features eyes missing 
lram captures redundancies spatial temporal dimension pca captures temporal redundancies case 
reconstructions better tensor rank decomposition rank approximation methods methods capture html cvc yale edu projects 
redundancies dimensions 
basis columns rank approximation orthonormal compact rank approximation tensors yields better reconstruction tensor rank decomposition 
face datasets pca worst methods spatial redundancies captured due image vector formulation 
critical features eyes mouth nose appear pretty clear 
compression ratio corresponding principle components pca reconstruction algorithm visually quite close original image 
shows reconstruction error dataset 
plots consistent relative visual quality reconstructions 
algorithm produces best visual reconstructions smallest rmse methods 
compression ratio decreases methods give similar performance 
reasonable increasing number components leads steadily decreasing amount information loss 
computational efficiency experiments performed matlab pentium iv ghz machine ram 
gives convergence rates shows usually takes iterations algorithm converge locally optimal solution takes iterations algorithm 
gives computation times toy sequence 
see comparable representation errors computation time algorithm slightly algorithm algorithm efficient algorithm due high computational complexity large 

root mean squared error principle component analysis rank tensor decomposition low rank approximation matrices rank approximation tensors principle component analysis rank tensor decomposition low rank approximation matrices approximation tensors root mean squared error principle component analysis principle component analysis rank tensor decomposition low rank rank approximation tensor decomposition matrices principle component analysis rank tensor decomposition low approximation rank approximation tensors matrices low rank approximation matrices compression ratio approximation tensors compression ratio approximation tensors compression ratio 



principle component analysis 
low rank approximation matrices 

rank tensor decomposition 

rank approximation tensors 
comparison quality reconstructions achieved rank approach methods 
parameters method chosen achieve compression ratio 
reconstruction results toy sequence 
original image reconstruction results orl reconstruction results orl 
original image reconstruction error toy sequence orl yale 
iteration time dimension generalized rank approximation tensors rank approximation tensors rmse root mean squared error dimension 
comparison generalized rank approximation tensors vs rank approximation third order tensors 

convergence 
execution time 
reconstruction error 
memory requirements algorithm 
see applicable algorithm data machine required memory 
experiments datasets yale orl databases give similar results 
results direct implementation algorithm 
better implementations help improve efficiency 
example exploit fact svd obtain un unfolded tensor reduce computational time 
appearance recognition evaluate quality representation obtained algorithm applied recognition face images orl yale face databases 
yale database containing different images distinct persons fold cross validation randomly selected images person remaining testing 
orl face database containing different images distinct persons applied fold cross validation 
similarly randomly selected images class training set accuracy compression ratio accuracy principle component analysis low rank approximation matrices compression ratio rank approximation tensors 
comparing quality representation different methods face recognition yale orl face datasets 

accuracy vs compression ratio yale 

accuracy vs compression ratio orl 
remaining image testing 
database repeated process times 
features obtained equation 
face correctly classified feature minimum frobenius distance features person 
reported final accuracy average runs 
compared algorithm lram pca 
shown comparing performance different methods fixed compression ratio method yields highest accuracy case 
shows algorithm best reduced dimensionality representation 

introduced novel approximation tensors multilinear algebra 
method designed capture spatial temporal redundancies dimension tensor 
experiments show superior performance rank approximation tensors terms quality data representation object classification accuracy fixed compression ratio 
support office naval research gratefully acknowledged 
go anonymous reviewers informative comments strengthened 
golub loan 
matrix computation 
johns hopkins university press baltimore md usa third edition 

rank approximation higher order supersymmetric tensors 
siam matrix anal 
appl 

mode principal component analysis 
press leiden netherlands 
lathauwer moor vandewalle 
multilinear singular value decomposition 
siam matrix anal 
appl 
lathauwer moor vandewalle 
best rank rank 
rn approximation high order tensors 
siam matrix anal 
appl 
shashua levin 
linear image regression classification tensor rank principle 
cvpr 
sirovich kirby 
low dimensional procedure characterization human faces 
journal optical society america 
turk pentland 
eigen faces recognition 
cognitive neuroscience 
terzopoulos 
multilinear analysis image ensembles 
eccv pages 
terzopoulos 
multilinear subspace analysis image ensembles 
cvpr june 
terzopoulos 
multilinear image rendering 
proc 
acm siggraph pages august 
wang ahuja 
facial expression decomposition 
int 
conf 
computer vision iccv pages 
wang ahuja 
compact representation multidimensional data tensor rank decomposition 
int 
conf 
pattern recognition icpr 
yang zhang yang 
twodimensional pca new approach face representation recognition 
ieee transactions pattern analysis machine intelligence january 
ye 
generalized low rank approximations matrices 
international conference machine learning icml july 
ye li 
efficient dimension reduction scheme image compression retrieval 
acm kdd pages august 
