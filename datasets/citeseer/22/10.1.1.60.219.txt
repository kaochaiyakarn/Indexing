contrastive divergence learning miguel 
carreira geoffrey hinton dept computer science university toronto king college road 
toronto canada email miguel hinton cs toronto edu maximum likelihood ml learning markov random fields challenging requires estimates averages exponential number terms 
markov chain monte carlo methods typically take long time converge unbiased estimates hinton showed markov chain run steps learning approximately minimizes different function called contrastive divergence cd 
cd learning successfully applied various types random fields 
study properties cd learning show provides biased estimates general bias typically small 
fast cd learning get close ml solution slow ml learning fine tune cd solution 
consider probability distribution vector assumed discrete parameters normalisation constant energy function 
class random field distributions practical applications li winkler teh 
maximum likelihood ml learning parameters iid sample xn done gradient ascent current address dept computer science electrical eng ogi school science engineering oregon health science university 
email miguel cse ogi edu 
learning rate need constant 
average log likelihood log xn log log denotes average data distribution xn 
known difficulty arises computation gradient denotes average respect model distribution 
average readily computed sample data average involves normalisation constant generally computed efficiently sum exponential number terms 
standard approach approximate average distribution average sample obtained setting markov chain converges running chain equilibrium reviews see neal gilks 
markov chain monte carlo mcmc approach advantage readily applicable classes distribution 
typically slow running markov chain equilibrium require large number steps foolproof method exists determine equilibrium reached 
disadvantage large variance estimated gradient 
avoid difficulty computing log likelihood gradient hinton proposed contrastive divergence cd method approximately follows gradient different function 
ml learning minimises kullback leibler divergence kl log cd learning approximately follows gradient difference divergences hinton cdn kl kl pn cd learning start markov chain data distribution run chain small number steps 
greatly reduces computation gradient step variance estimated gradient experiments show results parameter estimates hinton 
cd applied effectively various problems chen murray teh gibbs sampling hybrid monte carlo transition operator markov chain 
hard know parameter estimates really comparison done real ml estimates impractical compute 
little theoretical investigation properties contrastive divergence mackay williams yuille important questions remain unanswered converge 
fast convergence points related true ml estimates 
provide theoretical empirical evidence contrastive divergence fact basis effective approach learning random fields 
concentrate boltzmann machines results generally valid 
show cd provides biased estimates general data distributions fixed points cd fixed points ml vice versa section 
show comparing cd ml empirical tests bias small sections effective approach cd perform learning followed short run ml clean solution section 
eliminate sampling noise investigations fairly small models parameters compute exact model distribution exact distribution step markov chain stage learning 
take ml learning mean exact ml learning markov chain cdn learning mean learning exact distribution markov chain steps 
sampling noise real mcmc estimates create additional large advantage favours cd ml learning cd lower variance gradient estimates 
ml cd learning types boltzmann machine concentrate types boltzmann machine particular case model eq 

boltzmann machines visible units 
xv encode data vector hidden units 
yh units binary take values 
fully visible boltzmann machines visible units connected 
en ergy xt wx wij symmetric matrix real valued weights simplicity consider biases 
denote machine 
log likelihood unique optimum hessian negative definite 
wij xixj ml learning takes form ij ij xixj xixj cdn learning takes form ij ij xixj xixj pn pn nth step distribution markov chain transition matrix started data distribution 
restricted boltzmann machines smolensky freund haussler connections hidden visible unit form bipartite graph 
energy wx visible units hidden units wij matrix real valued weights 
denote machine rbm 
making large rbm far representational power log likelihood multiple maxima 
learning simpler general boltzmann machine visible units conditionally independent hidden units hidden units conditionally independent visible units 
step gibbs sampling carried half steps updates hidden units second updates visible units 
equivalently write tx ty 
wij ml learning takes form ij ij cdn learning takes form ij ij analysis fixed points probability distribution units vector real valued components bi omit dependence parameter values simplify notation 
nary notation lives dimensional simplex xi xi 
coordinate axis corresponds state binary vector 
write distribution emphasising function vector emphasising point simplex 
define markov chain transition operator psfrag replacements stochastic matrix gibbs sampler transition operator simplicity wide applicability distributions 
boltzmann machines finite weights gibbs sampler converges stationary distribution model distribution 
initial distribution pn 
vector ones 
define manifold simplex parameterised wij ignore case infinite weights corresponding distributions intersection simplex boundary 
learning ml cd starts point follows approximate gradient loglikelihood tracing trajectory ml gradient learning fixed points zero gradient points maxima minima saddles satisfy step cd fixed points satisfy section address theoretical question fixed points ml fixed points cd vice versa 
show general give brief explanation framework analysing fixed points ml cd full details appear carreira hinton 
idea fix value weights value moments defined determine data distributions moments opposite learning problem determine conditions ml cd agree distributions 
call matrix energy derivatives defined wi consider column vector elements state takes values 
case binary variables 
write moments distribution gp linear function call transition matrix sampling operator stationary distribution tp 
general functions simplex dimensions 
model single parameter 
tetrahedron represents simplex set 
tetrahedron corners correspond pure states distributions assign probability single state 
red vertical segment manifold distributions reachable 
ml estimate data distribution orthogonal projection model manifold 
cd estimate agrees ml data distributions shaded planes inset 
consider fixed value associated model distribution 
moments gp 
define sets depend 
set data distributions moments gp distribution gives fixed point ml 
likewise define set data distributions distribution tp step markov chain cd uses mo ments gtp nonempty 
reformulate problem terms sets 
example distribution satisfies gives fixed point ml cd cd learning rule move away 
general ignoring technical details regarding inequality linear subspaces dimension full rank moments generally full rank 
generally expect points cd bias rule points cd bias exception intersection subspace 
statement precise model 
example gibbs sampling happens independent compute set value 
resulting set contains data distributions ml cd fixed points bias union intersected simplex planes write distribution dimensional vector corresponding probabilities states 
see fig 

set measure zero simplex cd biased data distribution 
reachable distribution fixed point cd ml invariant hinton 
consistent argument 
distributions practical interest typically unreachable real data nearly complicated computationally tractable model 
summary expect data distribution fixed points ml fixed points cd vice versa 
means general cd biased learning algorithm 
argument applied models boltzmann machines transition operators gibbs sampling writing 
determines cd biased hyperplanes defined matrices gt 
nontrivial models defining lower dimensional manifold may exist cd biased example gaussian boltzmann machines williams gaussian distributions carreira hinton 
analysis imply cd learning converges stable fixed point proof 
cd converge appears practice experiments converge fixed point 
naturally ml converge stable fixed points maxima follows exact gradient objective function noisy sampling case practice converges provided learning rate follows robbins monro schedule benveniste rule performs stochastic gradient learning 
experiments fully visible bms cd biased respect ml data distributions investigate empirically magnitude bias 
experiments ml cd tested exactly conditions stated 
ml cd learning initial weight vectors constant learning rate maximum iterations rarely reached stopping xixj xixj gradient vector ml xixj xixj approximate gradient cd 
experiments step gibbs sampling fixed ordering variables cd learning produce greatest bias cd ml 
simu lated models necessarily small empirical results hold range model sizes conditions suggests may generally valid 
section consider fully visible boltzmann machines denoted single ml optimum 
appears cd single convergence point prove 
checked empirically running cd different initial weight vectors converged point small numerical error 
assume cd unique convergence point 
allows characterise bias model class sampling data distributions computing convergence point ml cd 
value sampled number large computationally feasible data distributions uniformly distributed simplex variables see carreira hinton details generate samples 
ran ml cd starting small weights give faster convergence average 
results experiments 
qualitatively similar 
feasible sample data distributions results summarised figures 
histograms figs 
show bias small distributions 
fig 
shows kl error ml cd small data distributions near simplex centre 
vague data distributions variability distributions having low error having higher 
generally speaking distributions having highest kl error ml distributions modelled worst ones highest bias 
lie near boundaries simplex particularly near corners 
corners boundaries far frag replacements kl pml kl pcd histograms kl pml kl pcd learning data distributions pml pcd convergence points ml cd respectively 
performance cd close ml average 
model manifold depends geometry model 
fig 
discern geometry simplex fig 

discontinuity slope euclidean distance just corresponds radius inscribed sphere 
branch low error corresponds direction passing centre simplex corner corresponding delta distribution state manifold 
branch high error data points corresponds directions passing centre corners away manifold 
increases volume simplex concentrates distance intermediate corners centre close radius inscribed hypersphere 
consequently finite uniform sample contains essentially points near boundaries simplex produce highest bias 
large cd small bias nearly randomly chosen data distributions 
rare distributions near simplex boundaries produce significant bias important practice real world distributions near boundaries far corners large parts data space negligible probability 
fig 
shows typical learning curves 
cd ml decrease similar way converging rate order number iterations converge tolerance 
cd yields ml psfrag replacements cd bias kl pml pcd histogram kl divergence model distributions ml cd data distributions 
shows bias cd small kl error obtained ml distribution data shown 
data distributions exist relatively large bias 
kl euclidean distance simplex centre kl error ml kl pml red cd kl pcd black vs euclidean distance data distribution uniform distribution centre simplex 
euclidean distance gives linear ordering data distributions lowest euclidean distance uniform distribution highest corners simplex 
clarity distributions plotted 
higher kl error 
lower example cd curve increases slightly suggesting came close ml optimum moved away 
summary find cd bias small distributions highest small real world distributions near simplex boundary 
bias small relative terms compared kl error ml absolute terms compared simplex dimensions 
cd ml converge rate ml iteration costs kl psfrag replacements kl psfrag replacements number iterations cd ml cd ml learning curves ml cd randomly chosen data distributions 
axes log scale 
cd mcmc implementation 
experiments restricted bms practically interesting higher representational power 
introduce new element complicates study existence multiple local optima ml cd 
prevents characterisation bias large number data distributions 
afford select data distribution try characterise set optima ml cd 
data distribution generate collection random initial weight vectors compute optima ml cd reachable initial weight vectors optima learning method 
requires iterating current set optima ml cd new optima 
result bipartite self consistent convergence graph arrow indicates ml optimum converges cd optimum cd cd optimum converges ml optimum ml 
different initial weight vectors give representative collection optima turing estimator coarse indicator optima missed 
graph depends decide similar optima really 
threshold number parameter updates carefully chosen truly different optima confused discoveries optimum considered different 
kl distance threshold worked parameter updates 
ran experiments various values various data distributions 
fig 
summarises results representative case corresponding 
data distribution generated data set binary vectors adding extra count possible binary vector close simplex boundary 
iterations different initial weight vectors random random random 
ml optima cd optima missed respectively turing estimate 
panel shows visualisation ml optima red cd optima black visible unit distributions pml pcd convergence relations 
blue data distribution visible variables 
avoid cluttering plot pairs arrows drawn single line arrowheads note lines short distinguished 
view obtained sne hinton roweis tries preserve local distances 
perplexity determine local neighborhood size sne gives better visualisation projecting principal components 
panel shows important robust phenomenon ml cd optima typically come pairs converge 
cd optimum greater equal kl error associated ml optimum difference small 
pairs expected cdn large cd ml 
occur shown 
panels show choice initial weights larger effect kl error cd bias 
cd initialise ml previous experiments show cd takes close ml optimum small bias remains 
obvious way eliminate bias increasing values training progresses 
section explore crude version strategy run cd close convergence short run ml psfrag replacements kl pcd psfrag replacements kl pml kl pml kl pcd psfrag replacements empirical study convergence points ml cd rbm single data distribution 
sne visualization points ml red cd black convergence relations ml cd line arrowhead stands arrows avoid clutter 
kl error cd vs ml initial weight vectors random initial weight vectors 
histograms kl error ml cd 
ml cd kl cd 
cpu cost ml 
cd cd ml ml ml 
learning curves cd ml ml ml iteration scaled cost cd iterations 
axes log scale cd ml order magnitude faster final kl 
reduce bias 
call strategy cd ml 
data distribution representative real problem 
located simplex boundary derived statistics patches images handwritten digits usps dataset 
intensity levels thresholded produce dimensional binary vectors 
normalised counts binary vectors patches 
different initial weight vectors random random random 
starting condition types learning ml learning iterations cd learning iterations followed shorter run ml learning 
ran experiments 
unique ml optimum cd optima varying degrees bias 
cd learning followed iterations ml cd optima converged ml optimum 
fig 
shows learning curves error kl function estimated cpu time different methods cd blue line short ml run iterations cd green line ml red line selected starting condition 
assume ml iteration costs times cd iteration reasonable estimate size rbm 
cd ml reaches error ml small fraction cost 
note sharply cd ml curve drops switch ml suggesting performance achieved expensive ml iterations 
result negative types boltzmann machine shown general fixed points cd differ ml cd biased algorithm 
suggest cd competitive method ml estimation random fields 
remaining empirical results show bias generally small gibbs sampling cd typically converges near ml optimum 
small bias eliminated running ml iterations cd cd initialisation strategy ml total computation time smaller full fledged ml slight bias markov chain run forever 
theoretical analysis cd difficult complicated form pn distribution takes moving target changes complicated way depends sampling scheme gibbs sampling 
result theoretical results cd exist 
mackay gave examples cd bias unusual sampling operators 
analysis applies model operator matrices particular generally applicable operators gibbs sampling 
williams showed gaussian boltzmann machines cd unbiased typically decreases variance estimates 
yuille gives condition cd unbiased condition difficult apply practice 
open theoretical problem exact version cd converges believe 
assuming prove convergence exact case right tools prove noisy case probably stochastic approximation benveniste yuille 
research funded nserc cfi 
fellow holds crc chair 
benveniste 
adaptive algorithms stochastic approximations 
springer verlag 

carreira hinton 
contrastive divergence cd learning 
technical report dept computer science university toronto 
preparation 
chen murray 
continuous restricted boltzmann machine implementable train ing algorithm 
iee proceedings vision image signal processing june 
freund haussler 
unsupervised learning distributions binary vectors layer networks 
nips pages 
gilks richardson spiegelhalter editors 
markov chain monte carlo practice 
chapman hall 

population frequencies species estimation population parameters 
biometrika dec 
zemel 
carreira multiscale conditional random fields image labeling 
cvpr pages 
hinton roweis 
stochastic neighbor embedding 
nips pages 
hinton 
training products experts minimizing contrastive divergence 
neural computation aug 
li 
markov random field modeling image analysis 
springer verlag 
mackay 
failures step learning algorithm 
available online www inference phy cam ac uk mackay abstracts gbm html 
neal 
probabilistic inference markov chain monte carlo methods 
technical report crg tr dept computer science university toronto sept 
available online ftp ftp cs toronto edu pub radford review ps smolensky 
information processing dynamical systems foundations harmony theory 
rumelhart editors parallel distributed computing explorations microstructure cognition 
vol 
foundations chapter 
mit press 
teh welling osindero hinton 
energy models sparse overcomplete representations 
journal machine learning research dec 
williams 
analysis contrastive divergence learning gaussian boltzmann machines 
technical report edi inf rr division informatics university edinburgh may 
winkler 
image analysis random fields markov chain monte carlo methods 
springer verlag second edition 
yuille 
convergence contrastive divergences 
appear nips 
