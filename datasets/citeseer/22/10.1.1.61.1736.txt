kernels features kernels margins low dimensional mappings maria avrim blum santosh vempala computer science department carnegie mellon university avrim cs cmu edu department mathematics mit vempala math mit edu 
kernel functions typically viewed providing implicit mapping points high dimensional space ability gain power space incurring high cost result linearly separable large margin 
johnson lindenstrauss lemma suggests presence large margin kernel function viewed mapping low dimensional space dimension 
explore question efficiently produce low dimensional mappings black box access kernel function 
just program computes inputs choosing efficiently construct explicit small set features effectively capture power implicit high dimensional space 
answer question affirmative method allowed black box access underlying data distribution unlabeled examples 
give lower bound showing access distribution possible arbitrary black box kernel function leave open problem done standard kernel functions polynomial kernel 
positive result viewed saying designing kernel function designing feature space 
kernel running black box manner random unlabeled examples efficiently generate explicit set features data linearly separable margin kernel approximately separable new feature space 
kernels functions powerful tool machine learning :10.1.1.103.1189:10.1.1.15.9362
kernel function viewed allowing implicitly map data high dimensional space perform certain operations paying high price computationally 
furthermore data large margin linear separator space avoid paying high price terms sample size :10.1.1.33.8995:10.1.1.48.8200
preliminary version appeared proceedings th international conference algorithmic learning theory 
springer lnai pp 

starting point observation learning problem large margin property kernel johnson lindenstrauss lemma random linear projection space low dimensional space approximately preserves linear separability 
specifically suppose data comes underlying distribution input space labeled target function target function margin space random linear projection space space dimension log probability linear separator error rate see vempala theorem 
means kernel margin principle think mapping input space dimensional space essence serving method representing data new large feature space 
question consider kernel fact produce mapping efficiently 
problem observation requires explicitly computing function 
particular mapping results applying johnson lindenstrauss lemma function 
rd 
rd random vectors space 
kernel dimensionality space quite large efficient 
efficient procedure black box program produces mapping desired properties running time depends polynomially time compute kernel function dependence dimensionality space 
main result positive answer question procedure computing mapping black box access distribution unlabeled data 
specifically black box access kernel function margin value access unlabeled examples distribution parameters polynomial time construct mapping rd set real valued features log property 
target concept margin space probability randomization choice mapping function induced distribution separable error 
fact data separable separable margin 
note logarithmic dependence implies learning problem perfect separator margin space set small high probability set log labeled examples perfectly separable mapped space 
means apply arbitrary zero noise learning algorithm mapped space highly optimized exists linear separator space example correctly classified margin 
see section formal definitions 
section consider general case fraction distribution separated margin 
linear programming package 
dimension logarithmic dependence number unlabeled examples produce mapping 
give feel mapping look suppose willing dimension ln linear logarithmic concerned preserving margins want approximate separability 
show especially simple procedure suffices 
just draw random sample unlabeled points 
xd define 
xd 
think implicit mapping high dimensional space just similarity function examples doing drawing points defining ith feature similarity point show corollary assumption target function margin space high probability data approximately separable mapping 
gives particularly simple way kernel unlabeled data feature generation 
results natural question possible perform mappings type access underlying distribution 
section show general possible black box access polynomially queries arbitrary kernel may possible specific standard kernels polynomial kernel gaussian kernel 
relation support vector machines margin bounds set training examples kernel matrix defined viewed placing dimensional space weight vector svm lie space maximize margin respect training data 
goal define mapping entire distribution guarantees respect distribution 
addition construction mapping requires unlabeled examples performed seeing labeled training data unlabeled examples freely available 
close relation margin bounds svms see section dimension output space lower produced combining svms standard margin bounds :10.1.1.33.8995
goals extent related ben david 
show negative results giving simple classes learning problems construct mapping low dimensional space functions class linearly separable 
restrict situations know mappings exist goal produce efficiently 
interpretation kernel functions viewed providing power implicit high dimensional space having pay 
results suggest alternative view kernels distribution dependent mapping low dimensional space 
view designing kernel function designing feature space 
kernel running black box manner random unlabeled examples efficiently generate explicit set features data linearly separable margin kernel approximately separable new features 
outline giving formal model definitions section 
section show simple mapping described earlier section preserves approximate separability give modification approximately preserves separability margin 
map data dimensional space ln 
section give improved mapping maps data space dimension log 
logarithmic dependence means set small function dimension input error parameter plug generic zero noise linear separator algorithm mapped space assuming target function perfectly separable margin space 
section give lower bound showing black box kernel access underlying distribution wishes produce mapping low dimensional space 
section experimental results mappings synthetic standard datasets short discussion section 
notation definitions assume data drawn distribution instance space labeled unknown target function 
denote combined distribution labeled examples 
kernel pairwise function viewed legal definition inner product 
specifically exist function mapping possibly high dimensional euclidean space 
call range space denote induced distribution space produced choosing random applying 
say set labeled examples vector space margin min 
margin labeled example correctly classified linear separator furthermore cosine angle magnitude 
vector exists say linearly separable margin kernel simplicity margin defined normalizing length examples case term sample complexity bounds maximum technically normalizing produces stronger bound minimum ratio ratio minimum maximum 
considering separators pass origin results adapted general case see section 
similarly talk terms distribution sample say vector space margin respect pr 
vector exists say linearly separable margin just margin space 
weaken notion perfect separability 
say vector space error margin pr 
starting assumption perfectly separable margin weaken assumption existence vector error margin corresponding weakening implications see section 
goal mapping large approximately preserves separability ideally margin 
denote induced distribution produced selecting points applying denote induced distribution labeled examples 
set vectors 
vk euclidean space span vk denote set vectors written linear combination 

vector subspace proj orthogonal projection instance proj span 
vk orthogonal projection space spanned vk 
note set vectors 
vk ability compute dot products projection computed efficiently solving set linear equalities 
simple mappings goal procedure black box access kernel function unlabeled examples distribution margin value produces probability distribution mappings rd property target function margin space high probability mapping approximately preserve linear separability 
section analyze methods produce space dimension ln desired bound error rate best separator mapped space 
second mappings fact satisfies stronger condition output approximately separable margin just approximately separable 
property allow mapping step better mapping section 
lemma key analysis 
lemma 
consider distribution labeled examples euclidean space exists vector margin 
draw ln examples zd distribution probability exists vector span zd error margin 

proving lemma somewhat weaker bound derived machinery margin bounds 
margin bounds tell log log points probability separator margin observed data true error :10.1.1.33.8995
projection target function space spanned observed data true error 
projecting space maintains value zi possibly shrinking vector increase margin observed data 
technical issue want separator low error rate distribution large margin 
obtained double sample argument cover cover :10.1.1.33.8995
margin bounds bit overkill needs asking existential statement existence universal statement separators large empirical margins 
reason able get better bound direct argument principles 
proof lemma 
set points win projection span wout orthogonal portion win wout win wout 
convenience assume examples unit length vectors defined margins terms angles loss generality 
definitions 
say wout large prz wout say wout small 
notice wout small done win wout means win properties want 
probability mass points win differ 
need consider happens wout large 
crux proof wout large means new random point chance significantly improving set specifically consider wout 
projection span portion orthogonal span 
wout wout proj wout span wout wout equality holds wout orthogonal span projection span projection wout orthogonal wout wout wout wout wout wout implies definition wout wout situation long wout large example chance reducing wout wout happen times 
chernoff bounds state coin bias flipped ln times probability heads 
imply probability wout small ln desired 
lemma implies linearly separable margin draw ln random unlabeled examples 
xd probability separator space error rate written 
xd 
notice 
dk xd immediate implication simply think xi ith feature define 
xd high probability vector 
approximate linear separator 
kernel distribution give particularly simple way performing feature generation preserves approximate separability 
formally 
corollary 
margin space probability 
xd drawn ln 
xd mapping produces distribution linearly separable error 
unfortunately mapping may preserve margins bound length vector defining separator new space length examples 
key problem xi similar associated features xi highly correlated 
preserve margin want choose orthonormal basis space spanned xi orthogonal projection space 
specifically xd ln unlabeled examples implement set desired orthogonal projection follows 
run pairs xi xj xi xj resulting kernel matrix 
decompose ut upper triangular matrix 
define mapping rd mapping corollary 
equivalent orthogonal projection span xd 
technically full rank want moore penrose pseudoinverse place claim lemma mapping maintains approximate separability margin 
theorem 
margin space probability mapping rd ln property linearly separable error margin 
proof 
theorem follows directly lemma fact orthogonal projection 
specifically separable margin lemma implies ln probability exists vector written 
xd error margin respect pr 
consider 
df xd 
orthogonal projection xi clearly space spanned xi viewed just written different basis 
particular get error margin respect pr 
choice probability randomization choice exists vector error margin respect 
notice running time compute polynomial time compute kernel function improved mapping describe improved mapping dimension logarithmic linear dependence 
idea perform stage process composing mapping previous section random linear projection range mapping desired space 
mapping thought combining types random projection projection points chosen random projection choosing points uniformly random intermediate space 
stating result :10.1.1.38.249
standard normal distribution mean variance distribution probability probability 
specific form 
theorem neuronal rp 
ua va random matrix entries chosen independently 
pr mapping section error confidence parameters respectively 
random projection theorem 
specifically pick random matrix entries chosen 
set xa 
consider mapping rd 
claim ln log high probability mapping desired properties 
basic argument initial mapping maintains approximate separability margin lemma second mapping approximately preserves property theorem 
theorem 
margin space probability mapping rd values ln log property linearly separable error margin 
proof 
lemma probability exists separator intermediate space rd error margin 
assume fact occurs 
consider point rd theorem implies choice log sufficient random projection probability squared lengths preserved multiplicative factors 
implies cosine angle margin respect preserved additive factor 
specifically implies 
words shown pr 
true clearly true random 
pr implies pr pr 
error margin implies probability error margin 
combining failure probability completes proof 
running time compute mappings polynomial time compute kernel function dimension mapping theorem logarithmic means set small high probability sample size log perfectly separable 
means noise free linear separator learning algorithm learn target concept 
requires unlabeled examples construct mapping 
corollary 
margin space unlabeled examples sufficient probability mapping property linearly separable error log log 
proof 
just plug desired error rate bounds theorem 
extensions far assumed distribution perfectly separable margin space 
suppose separable error margin 
exists vector space correctly classifies probability mass examples margin remaining probability mass may margin incorrectly classified 
case apply previous results portion distribution correctly separated margin remaining probability mass examples may may behave desired 
preceding results lemma corollary theorem theorem hold replaced error rate resulting mapping 
extension case target separator pass origin form value 
normalized results carry directly 
particular results follow arguments showing cosine angle changes due reduction dimension 
normalized results carry replaced upper bound done standard margin bounds :10.1.1.33.8995:10.1.1.48.8200
necessity access algorithms construct mappings black box access kernel function unlabeled examples input distribution natural ask possible remove need access particular notice mapping resulting johnson lindenstrauss lemma input distribution access space matter distribution random projection approximately preserve existence large margin separator high probability 
mapping produced just computing polynomial number uniform random points 
assume nice space unit ball randomly sampled 
section show possible general arbitrary black box kernel 
leaves open case specific natural kernels 
way view result section follows 
define feature space uniform binary rademacher gaussian random points space know johnson lindenstrauss lemma 
define features points image chosen corollary 
define features points chosen method depend exist kernels 
particular demonstrate necessity access follows 
consider random subset elements uniform distribution target function define special function large margin separator space distribution points behave nicely points provide useful information 
specifically consider defined see 
induces kernel kc notice distribution labeled examples margin space 
theorem 
suppose algorithm polynomially calls blackbox kernel function input space produces mapping polynomial random random construction high probability weakly separable margin space 
clear order quantification statement distribution random projection high probability 
projection may exist bad distributions 
define mapping sort desired expect algorithm randomized 
fig 
function lower bound 
proof 
consider algorithm black box access attempting create mapping random exponentially small fraction high probability calls constructing function inputs assume case 
implies calls constructing function return value runtime chosen map training data function may call different points give 
particular means independent target function size polynomial simply counting number possible partitions halfspaces high probability weakly separable random function specifically halfspace probability choice error exponentially small hoeffding bounds doubly exponentially small dn possible partitions halfspaces 
notice kernel argument positive semidefinite 
wish positive definite kernel simply change definition case keep 
corresponds function mapping points exactly map giving example component dimension scale components keep unit vector 
margin 
modifications provide real change algorithm access original kernel simulate arguments apply kernel 
complain kernels argument efficiently computable 
rectified assuming existence way functions defining cryptographically pseudorandom subset pseudorandom function 
case step argument holds polynomial time algorithms 
issue arises step know polynomial time algorithm test weakly separable distinguish truly random function provide needed contradiction 
need change theorem weakly learnable polynomial time algorithm 
course kernels extremely unnatural hidden target function built 
quite conceivable positive results independent distribution achieved standard natural kernels 
experiments consequence analysis provides alternative learning algorithm modifying algorithm kernels construct mapping low dimensional space kernel data distribution run un kernelized algorithm examples new space 
illustrate idea performed experiments synthetic standard datasets standard kernel functions 
experiment unlabeled examples determine new representations data mappings described section 
find linear decision surfaces new feature spaces come classification rules learning problem balanced winnow algorithm see linear svm 
compared accuracies methods produced svm kernel svm implementation available described 
synthetic datasets test methods generated synthetic datasets follows 
started considering dimensional input data separating boundaries form 
generated points various distributions ensured reasonable margin space induced degree polynomial kernel 
specifically generated points satisfy various experiments report considered notice kernel ideal sense data perfectly separable space right choice running svm kernel 
expect data perfectly linearly separable new feature spaces running linear svm mappings sense lower value parameter table 
classification errors methods second degree polynomial kernel various synthetic datasets 
type surface winnow svm winnow svm svm ellipsis unif unif ellipsis gauss unif unif unif gauss unif ellipsis unif unif ellipsis gauss unif unif unif gauss unif table 
classification errors methods various standard datasets 
dataset kernel size data winnow svm winnow svm svm cancer poly ionosphere poly iris vs poly iris vs rbf iris vs rbf parameters constrained xi turn im plied margin space 
picked random unlabeled examples define mappings random labeled training points train classifiers 
ran experiments setting values uniform distribution inside legal regions truncated gaussian different standard deviation parameters 
summarize table results values parameters 
methods mapping winnow mapping linear svm mapping winnow mapping linear svm svm report average errors random test set runs experiment 
note choices experiments summarize table substantially smaller theoretical bounds performance appears quite reasonable especially mapping 
explicit experiments report table consider distribution 
generate random point ellipsis unif unif case flip unbiased coin decide sign pick point uniformly random region specified xi ellipsis gauss unif case similarly flip unbiased coin decide sign pick point uniformly random region specified xi keep generating points xi distributed gaussian mean variance xi similar way obtain random points unif unif gauss unif cases 
standard datasets compared mappings svm standard datasets uci irvine machine learning repository cancer ionosphere iris dataset 
cancer ionosphere datasets binary classification problems 
iris dataset classes iris setosa iris iris virginica constructed binary classification problems associated separating setosa classes call iris vs separating classes call iris vs separating virginica classes call iris vs 
table summarize results obtained follows 
dataset randomly permute examples pick unlabeled points creating mappings remaining pick examples training keep rest testing 
repeat procedure times report methods average error test set 
polynomial kernel degree cancer ionosphere datasets rbf kernel iris vs iris vs linear kernel iris vs iris dataset considered kernels suggested 
notice cases winnow linear svm performed nearly new feature spaces 
interesting point observe mapping performs nearly svm datasets mapping performs slightly worse 
extent expected mapping expect large margin size training set usually quite small 
discussion experiments show data mappings place data low dimensional space run linear separator algorithm winnow linear svm degradation performance 
note experience improvement performance 
ability perform explicit mappings opens door possible learning algorithms especially designed low dimensional data especially designed speed able run original data representation 
particular mappings allow enjoy benefits having large margin space restricting class learning algorithms easily 
open problems show black box access kernel function distribution unlabeled examples efficiently construct new low dimensional feature space place data approximately preserves desired properties kernel 
procedure uses types note discarded dataset examples missing attributes 
random mappings 
mapping random examples drawn construct intermediate space second mapping rademacher binary gaussian random vectors intermediate space johnson lindenstrauss lemma 
analysis suggests designing kernel function designing feature space 
provides alternative learning algorithm modifying algorithm kernels construct mapping low dimensional space kernel data distribution run un kernelized algorithm examples drawn mapped distribution 
interesting aspect simplest method choosing 
xd mapping 
xd applied generic similarity function necessarily legal kernels necessarily interpretation computing dot product implicit space 
interesting prove guarantees general setting 
main concrete open question natural standard kernel functions produce mappings oblivious manner examples data distribution 
johnson lindenstrauss lemma tells mappings exist goal produce explicitly computing function 
barring reduce unlabeled sample complexity approach 
practical side interesting explore alternatives mappings provide widely algorithms svm kernel perceptron 
adam kalai john langford helpful discussions 
anonymous referees useful comments suggestions helped improve presentation results 
supported part nsf ccr nsf itr ccr nsf itr iis 

achlioptas database friendly random projections journal computer system sciences volume issue pp 


vempala algorithmic theory learning robust concepts random projection proceedings th foundations computer science pp 

journal version appear machine learning 

bartlett shawe taylor generalization performance support vector machines pattern classifiers advances kernel methods support vector learning pp 
mit press 

ben david eiron simon limitations learning embeddings euclidean half spaces journal machine learning research volume pp 


ben david priori generalization bounds kernel learning nips workshop kernel learning 

ben israel generalized inverses theory applications wiley new york 

blake merz uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 

boser guyon vapnik training algorithm optimal margin classifiers proceedings fifth annual workshop computational learning theory pp 


cortes vapnik support vector networks machine learning volume pp 


dasgupta gupta elementary proof johnson lindenstrauss lemma tech report uc berkeley 

freund schapire large margin classification perceptron algorithm machine learning volume pp 


gunn support vector machines classification regression technical report image speech intelligent systems research group university southampton 

goldreich goldwasser micali construct random functions journal acm volume pp 


indyk motwani approximate nearest neighbors removing curse dimensionality proceedings th annual acm symposium theory computing pp 


herbrich learning kernel classifiers mit press cambridge 

johnson lindenstrauss extensions lipschitz mappings hilbert space contemporary mathematics volume pp 


littlestone 
learning quickly irrelevant attributes abound new linear threshold algorithm 
machine learning volume issue pp 


muller mika ratsch tsuda scholkopf kernel learning algorithms ieee transactions neural networks volume issue pp 


el yaniv online learning decision lists journal machine learning research volume pp 


scholkopf burges mika advances kernel methods support vector learning mit press 

shawe taylor bartlett williamson anthony structural risk minimization data dependent hierarchies ieee transactions information theory volume pp 


shawe taylor cristianini kernel methods pattern analysis cambridge university press 

scholkopf tsuda 
vert kernel methods computational biology mit press 

smola bartlett scholkopf schuurmans eds advances large margin classifiers mit press 

scholkopf smola learning kernels 
support vector machines regularization optimization mit university press cambridge 

vapnik statistical learning theory john wiley sons new york 

www isis ecs soton ac uk resources 
