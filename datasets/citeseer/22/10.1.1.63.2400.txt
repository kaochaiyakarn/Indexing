dependence language model information retrieval gao jian yun nie wu cao presents new dependence language modeling approach information retrieval 
approach extends basic language modeling approach unigram relaxing independence assumption 
integrate linkage query hidden variable expresses term dependencies query acyclic planar undirected graph 
assume query generated document stages linkage generated term generated turn depending related terms linkage 
smoothing method model parameter estimation approach learning linkage sentence unsupervised manner 
new approach compared classical probabilistic retrieval model previously proposed language models account term dependencies 
results show model achieves substantial significant improvements trec collections 
categories subject descriptors retrieval models general terms algorithms measurement theory keywords language model dependence parser information retrieval 
independence assumption assumptions widely adopted probabilistic retrieval theory 
states terms statistically independent 
assumption development retrieval models easier retrieval operation tractable hold textual data 
issue motivated research term dependencies decades 
dependence models deliv permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
sigir july sheffield south uk 
copyright acm 
microsoft research asia email microsoft com universit de montr email nie iro umontreal ca university china 
ered consistent improvements effectiveness large scale retrieval experiments 
reasons 
practically difficult estimate dependencies large scale 
second theoretically challenging integrate single words dependencies weighting schema 
theoretically motivated integration model documents containing dependencies phrases may scored weighted way single words 
language modeling approaches try incorporate word dependency bigrams 
word dependencies exist adjacent words distant 
bigram model hardly cover dependencies 
hand noisy dependencies truly connected assumed adjacent words bigram model 
bigram language model showed marginally better effectiveness unigram model 
assuming dependency pair adjacent words believe hand distant dependencies taken account hand strongest dependencies recognized order approach tractable 
presents new method capturing word dependencies 
extend state art language modeling approaches information retrieval introducing dependency structure called linkage inspired link grammar 
linkage structure assumes term dependencies sentence form acyclic planar graph related terms linked 
dependency structure limits dependencies important relationships useful retrieval 
reduces processing time limits estimation errors computing dependence scores due sparse data problem 
implementation existing dependency parsing learning techniques expectation maximization em extended detect linkage term sequence necessary grammatical sentence unsupervised manner 
retrieval step linkage detected query expected linkage relevant document 
model incorporates requirements words unigram model linkage words 
comparison bigram model show model generalization 
rest section reviews previous research trying relax independence assumption 
section presents dependence model introduces modeling assumptions model tractable information retrieval tasks 
section presents detail methods model parameter estimation including smoothing method approach unsupervised learning linkage 
series experiments trec collec done wu cao visiting microsoft research asia 
stephen robertson zhai comments early drafts 
go anonymous reviewers alerted closely related allan 
tions section 
comparison approach probabilistic retrieval models previous language models show model achieves substantial significant improvements 
contributions summarized section 
previous large amount dealing term dependencies probabilistic ir framework language modeling framework 
classical probabilistic ir models binary independence retrieval bir model queries documents represented set terms assumed statistically independent 
respect representations research directions taken order relax independence assumption 
produce better model integrates dependencies representation units words 
example attempts improve bir model considering various forms term dependencies 
approaches documents queries represented set words terms 
term dependencies usually defined statistical incidence terms scale documents 
principle formulation relevance probabilities documents single terms elaborated cover probabilities document relevant combinations terms 
dependence models brought significant improvements retrieval effectiveness 
important practical issue term dependencies estimated 
example blind definition term dependencies possible combination terms 
consequence gain improved independence assumption may outweigh loss increased estimation errors 
approach retain probable term dependencies useful information retrieval 
term dependencies derived linkage scale sentence query 
second direction develop models detailed representations queries documents 
addition terms single words stems compound terms phrases representation units 
phrases defined collocations adjacency proximity selected statistical ground possibly syntactic knowledge pos tags component terms 
phrase may treated unit decomposable unit phrase component single words regarded representation features 
unfortunately experiments provide clear indication retrieval effectiveness improved way 
possible reason phrases different nature words may appropriate apply weighting schemes 
phrases systematically scored independent model 
problem referred weight normalization problem 
tend explain unsuccessful trials dependence models fact dependency information implicitly captured classical probabilistic models 
incorporated techniques passage retrieval query expansion local context analysis 
need introducing empirical dependency information important generally thought 
example cooper points case bir model usual independence assumptions replaced weaker linked dependence assumption 
partially explains bir model effective dependence models extended top bring improvement 
language modeling approaches ir usually model relevance explicitly 
documents ranked capability generating query 
language model proposed uses unigrams consider dependency words 
attempts capture term dependencies 
cases unigrams simply replaced bigrams 
approaches adjacent ordered words assumed related 
models capacity representing word dependencies 
genuine dependencies exist adjacent words 
may occur distant words distributed network distributed personal computer network 
distant dependencies correctly covered bigram bi term model 
hand assume pair adjacent words connected huge number parameters estimated 
data sparseness errors generated 
errors may compromise benefit may obtain term dependencies correctly detected 
presents general dependence language model word dependencies restricted adjacent words 
sentence consider strongest word dependencies order reduce estimation errors 
show model generalization bigram model previously proposed 

dependence language model ir language modeling approach information retrieval multinomial model terms estimated document collection searched 
documents ranked probability query observed sample respective document model 
unigram model estimates query generation probability mp 
assumes independence different query terms multiple occurrences query term 
new model dependence modeling approach assume term dependencies query sentence form linkage acyclic planar undirected graph non stopword related query terms connected graph edge 
example query linkage shown 
assuming linkage query query generation formulated stage process linkage firstly generated document distribution plausible assume syntactic relation natural language sentences acyclic planar 
third property linkage undirected discussed section see 
linkage similar tree dependence term dependencies incorporated classical retrieval model different method 
query generated distribution 
second stage generation query term depend terms linked 
affirmative action 
query example linkage words bracketed 
dependence language model principle recovers probability query possible linkages basic dependence model formulated follows introduce assumptions model equation tractable 
common practice statistical language modeling assume sum lp possible ls dominated single term probable linkage query simply represent equation approximated follows arg max 
fact constraint denoted 
assumption model probabilistically illegitimate equation longer true probability 
probability ranking principle suggests transformation probabilities probability document ranking provided transformation order preserving 
equation preserve order equation write equation 
second assume query term dependent exactly related query term generated previously 
represent set related term pairs governor defined acyclic graph term qj exactly governor exception sentential head word governor governs sentence decomposed follows qi qh qi qi qh qi move nominator term qj outside product operator assume generation single term independent get qi qh affected construction industry see equation plays special role arrived result starting term 
indicates direction term dependencies matter computing query probability 
represented undirected graph 
substituting equation equation new document scoring function 
log rewrite final form log log log mi qi mi qi log comparison models discuss similarities differences model previously proposed dependence models 
model covers language model approaches special cases captures useful information document retrieval 
example independence assumption second score term right hand side equation non zero value 
obtain unigram language model 
define deterministic linkage query term dependent proceeding term dropped equals 
mi qh 
obtain bigram language model similar described srikanth srihari suggest language modeling speech recognition language models information retrieval need record occurrence terms 
introduce bi term language models 
models similar bigram model constraint order terms relaxed 
document containing information retrieval document containing retrieval information assigned probability generating query 
improvement bi term model bigram model reported 
proposed extensions unigram model consider dependencies adjacent terms 
term dependencies captured distant dependencies ignored 
model impose link adjacent words allow links distant words 
links supposed important ones 
classical dependence models estimate term dependencies statistically scale documents 
model assumes dependency structure linkage scale sentence account linguistically motivated constraints planar acyclic 
retain important term dependencies allows apply efficient parsing techniques detect term dependencies 
weight normalization problem described section resolved model systematic way 
instance score term equation viewed normalization factor penalize scores probable term dependencies 
recommend comparison approach principal contrast lies method estimating term dependencies sentence parsing score 

parameter estimation equation terms estimated mi 
estimating recall strongest linkage determined determine linkage query sentence document may suggest syntactic semantic parsers designed natural language processing 
difficulty arises existing parsers require syntactic semantic knowledge word usually unavailable especially special words proper nouns queries grammatical sentences 
solution statistical approach incorporates basic linguistic constraints form linkage acyclic planar 
parsing model assigns probability linkage query 
describe statistical dependency parser detects probable parsing model argmax 
create annotated corpus links parsing model training unsupervised manner 
parsing model moment assume available training corpus linkage sentence annotated 
creation corpus described section 
model inspired 
set probabilistic dependencies links assuming dependencies independent parsing model dependency probability estimated linkage annotated training data 
particular count relative frequency link appear sentence qi qi qi qj number times qi qj link sentence training data number times seen sentence 
quantity equation normalized give dependency probability equation 
ignore normalization factor change parsing results ranking results retrieval 
assume qi qj 
parsing model experiments arg max argmax exactly equation approximated quantity un normalized parsing score approximation order preserving language modeling approaches information retrieval parsing model precisely case usually estimated single document 
maximal likelihood estimator equation problematic data sparseness document may low give reliable estimate worse may zero leaving estimate undefined 
experiments smooth estimate stages 
linearly interpolate parsing models trained document entire collection qi fd qi fc qi interpolation weight determined empirically 
second fd qi qj fc qi qj equation backoff schema similar smoothing 
basic idea backoff estimates contextual information 
ignore notation difference estimates document collection general form estimated respect document corpus define estimates counts appropriate context document corpus 
wild card matching word 
final estimate fd qi qj fc qi qj takes form smoothing parameters defined parsing algorithm parsing model standard bottom chart parsing algorithm detect probable equation 
dynamic programming heuristics link crossing cycle detected link lowest dependency probability conflict eliminated 
unsupervised learning section describes create training corpus annotated links parsing model estimated 
corpus available purpose unsupervised learning method discovers sentence 
detailed description 
principle follows 
viterbi iterative training procedure approximation em training joint optimization parsing model linkage training data 
method consists steps step initialize set window size assumed word pair headword gram constitutes initial dependency 
optimal value experiments 
word trigram initial links 
initial links computed initial dependency parsing model equations 
step re parse corpus parsing model parser select probable linkage sentence training data 
parser successively eliminates weakest conflicting link parsing sentence 
results updated set links 
step re estimate parameters parsing model re estimated parsing model parameters updated link set 
steps iterated improvement probability training data threshold 
notice parser uses approximation chart parsing algorithm 
iterative training operating speed 
complexity abovementioned chart parser 
guarantee find optimal sentence demonstrated approximation practice 
estimating qi stage smoothing method proposed estimate unigram probability 
document language model smoothed dirichlet prior 
second interpolated query background model model trained entire collection final estimation cd qi cc qi cc qi qi qi parameter dirichlet distribution constant discount mass stolen turning method redistributed unigram probabilities unseen terms turing assumes number unseen events number events occur 
final estimate word unigram nr nr number times term occurs total number term occurrences nr number terms occur times estimating mi qi qj unigram probability collection information estimating term dependencies mi 
unseen links document simply assign mi qj meaning terms independent words knowing term reduce entropy 
values seen term dependencies estimated qi mi qi log cd qi log cd qi log denotes count link document 

experiments experimental setting evaluated dependence language model approach described previous sections different trec collections 
statistics shown table 
documents processed standard manner terms stemmed porter stemmer words removed 
queries trec topics description field trec disks 
topics natural language queries consisting sentence length words 
different trec collections remove queries relevant document 
coll 
description size mb doc 
query wsj wall street journal disk pat patents disk fr federal register disk sjm san jose mercury news disk ap associated press disks ziff articles computer select disks disks table 
trec collections 
retrieval models comparison language models bir models contain free parameters estimated empirically trial error 
parameters include smoothing parameters language models weights constants bir model 
applied experimental paradigm called fold cross validation 
trec collection divided document set similar halves odd numbered respectively weight computation weight application 
experiments retrieval results reported trec collection shown tables combined sets results halves collection respectively 
set results half obtained parameter settings optimized half 
desirable small collections big collection known retrieval performance varies lot different collections 
topic trec evaluations retrieved relevant document see example 
models wsj pat fr avgp change change avgp change change avgp change change bm ug bm ug bm ug bm ug dm bg bt bt table 
comparison results wsj pat fr collections 
indicate difference statistically significant test indicates value indicates value 
models sjm ap ziff avgp change change avgp change change avgp change change bm ug bm ug bm ug bm ug dm bg bt bt table 
comparison results sjm ap ziff collections 
indicate difference statistically significant test indicates value indicates value 
performance information retrieval measured precision collections 
collections precision recall pair 
main evaluation metric study improvement dm ug statistically significant non interpolated average precision avgp 
significance value test 
indicates additional tests test query query analysis employed 
terms equation parsing score term dependencies results score provide useful term dependency information document retrieval 
pilot study compare versions tables experimental results compare dependence model probabilistic retrieval models including implementation bir model state art language modeling approaches account term dependencies 
bm represents bir model 
performed experiments okapi system considered representative implementation bir model 
retrieval approach models document frequencies means mixture poisson distributions 
hypothesizes occurrences term document stochastic element reflects distinction documents concept elite represented term 
great number term weighting functions provided okapi choose bm achieved performance previous experiments 
ug implementation unigram language model approach information retrieval proposed 
serves baseline language model approach experiments 
trec collections ug achieves performance similar slightly worse bm 
observed general classical probabilistic retrieval model unigram language model approach perform similarly fine tuned 
slightly worse performance ug experiment due tuned okapi system bm weighting parameters tuned empirically 
dm dependence model described equations 
create linkage annotated corpus unsupervised manner described section trec collections collection table 
iterate learning process times 
comparing dm bm ug see dependence model achieves substantial improvements average dependence model equation parsing score term 
find model parsing score consistently outperforms 
may indicate normalization capability dependence model described section parsing score serves normalization factor penalty balance impact single terms term dependencies information retrieval 
bg implementation bigram language modeling approach information retrieval 
query generation probability estimated mp qi qi 
assumes query term depending preceding term 
deal sparse data problem smoothing methods 
linearly interpolated bigram models trained document entire collection respectively 
second bigram models bigram probability linearly interpolated unigram probability 
described section bigram model special case dependence model assuming pre defined linkage 
results show assumption practice bg slightly worse dm trec collections substantially outperforms ug collections 
investigate detail linkages detected parser 
turns links adjacent terms captured bigram model 
see bigram model capture part interesting dependencies 
bt bt implementations bi term language models originally described 
approximations bigram language model relaxing constraint term order 
bt bi term probability term pair qi qi pbt viewed average bigram probability ordered pairs qi qi qi qi smoothed bigram probability bg described 
pbt qi qi qi qi qi qi bt bi term probability computed ratio frequency term pair minimum frequencies terms qi qi 
bt cd qi qi cd qi qi qi qi min handle data sparse problem bi term probability pbt smoothed unigram probability qi turn smoothed collection probability qi 
shown tables bi term models outperform ug substantially outperform bg 
lower effectiveness model dm 
summary drawn experiments 
dependence model outperforms unigram language model classical probabilistic retrieval model substantially significantly 
language model approaches information retrieval models capture term dependencies achieve substantial improvements unigram model 
bigram language model turns approximation proposed dependence model practice simpler achieves slightly worse performance 
bi term language models expected approximations bigram language model delivered substantial improvements effectiveness experiments 
discussions term dependencies linguistic structure approaches incorporating term dependencies language modeling classified scale linguistic structure 
scale term cooccurrence models little linguistic information terms distance document assumed dependency 
models proved applicable information retrieval discussed section 
study generate term occurrence model cm table assuming term pair term trigram sentence link 
shown table size cm dependencies larger improvement limited 
spectrum models sophisticated syntactic structure models constituency models 
syntactic grammars parsing parsing model estimated manually annotated training data bank 
evaluated models information retrieval due complexity 
adopt information retrieval tasks open question 
leave 
dependence model described previous sections falls complexity linguistic structure uses 
particular grammar language modeling 
syntactic constraints linkage acyclic planar considered training data annotation models avgp change bm change ug dependencies bm ug unigram cm dm table 
comparison results collections 
results models cm dm obtained re ranking best lists generated bir model 
indicates difference statistically significant test value 
captured implicitly resulting model 
promising empirical results dependence model achieved raise interesting question syntactic grammars capture exactly term dependencies need information retrieval 
answer empirical results probably 
find experiments generated dependencies sense syntactic point view dependencies set table reduces entropy language model results improvements information retrieval queries table 
experiments queries incorporation term dependencies positive impact queries collections negative impact queries 
sample queries dependencies detected shown table 
new dependence language modeling approach information retrieval 
approach introduce linkage query hidden variable expresses term dependencies sentence forms acyclic planar undirected graph 
approach suggests generating query document stages generate linkage generate term turn depending related terms linkage 
general approach covers state art language model approaches special cases 
discussed proposed dependence model resolves problems classical dependence models term dependency estimation weight normalization 
demonstrated dependence model applicable information retrieval system learning linkage efficiently unsupervised manner smoothing model different smoothing techniques 
experiments standard trec collections indicate effectiveness dependence model outperforms substantially classical probabilistic retrieval model state art unigram bigram language models 

buckley allan salton 
automatic retrieval approaches smart trec 
information processing management 
charniak eugene 

immediate head parsing language models 
acl eacl pp 
chelba frederick jelinek 

structured language modeling 
computer speech language vol 

pp 
query set set impr 
ug status nuclear proliferation nuclear proliferation proliferation status nuclear violations monitoring 
ties status monitoring violation monitoring different techniques different techniques self induced hyp create create self induced 
techniques techniques government provide government government provide provide increased increased support national en provide support national endowment government nation arts 
endow art reports evaluation reports evaluation near death experi near death experience 
ence evaluation experience progress fuel cell progress progress fuel cell technology 
technology support 
drugs 
support drugs drugs affirmative action affected affirmative action action affected construction industry 
construction industry affected industry extent arms exports 
arms arms exports extent exports table 
sample queries words bracketed query column dependencies links impact ir dependencies set syntactic meaningful sentence query dependencies set 
chelba engle jelinek jimenez khudanpur rosenfeld stolcke wu 

structure performance dependency language model 
processing eurospeech vol 
pp 
collins michael john 

new statistical parser bigram lexical dependencies 
acl pp 

cooper 

inconsistencies probabilistic information retrieval 
sigir pp 

croft 
boolean queries term dependencies probabilistic retrieval models 
jasis 
della pietra della pietra lafferty ures 

inference estimation long range trigram model 
technical report cmu cs department computer science cmu 
fuhr 
probabilistic models information retrieval 
computer journal 
harper van rijsbergen 

evaluation feedback document retrieval occurrence data 
journal documentation 
gao jian yun nie chen ming zhou 

resolving query translation ambiguity decaying occurrence model syntactic dependence relations 
acm sigir pp 
gao suzuki 

unsupervised learning dependency structure language modeling 
acl pp 

harman 
overview fourth text retrieval conference trec 
trec pp 
jelinek frederick 

statistical methods speech recognition 
mit press cambridge massachusetts london england 
katz 
estimation probabilities sparse data language component speech recognizer 
ieee transactions acoustics speech signal processing 
lewis 
na bayes independence assumption information retrieval 
pp 

losee 
term dependence truncating bahadur expansion 
information processing management 
jones walker robertson 

probabilistic model information retrieval development status 
technical report tr cambridge university computer laboratory 
katz 
estimation probabilities sparse data language component speech recognizer 
ieee transactions acoustics speech signal processing 
lafferty sleator temperley 
grammatical trigrams probabilistic model link grammar 
proc 
aaai fall symposium probabilistic approaches natural language 
lafferty john zhai 

document language models query models risk minimization information retrieval 
sigir pp 

miller leek schwartz 
hidden markov model information retrieval system 
sigir pp 

allan 

capturing term dependencies language model sentence trees 
cikm pp 

ponte croft 
language modeling approach information retrieval sigir pp 

robertson walker 

simple effective approximations poisson model probabilistic weighted retrieval 
sigir pp 

robertson walker 
microsoft cambridge trec filtering track 
trec pp 

song croft 
general language model information retrieval 
cikm pp 

sparck jones 
role nlp text retrieval 
language information retrieval ed 
strzalkowski dordrecht kluwer 
srikanth srikanth 
language models document retrieval 
sigir pp 

van rijsbergen 
theoretical basis cooccurrence data information retrieval 
journal documentation 
xu croft 
improving effectiveness information retrieval local context analysis 
acm transactions information systems 


discovery linguistic relations lexical attraction 
ph thesis mit 
zhai john lafferty 

stage language models information retrieval 
sigir pp 

