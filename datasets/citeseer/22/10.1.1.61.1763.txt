
niculescu mitchell rao probabilistic models increasingly popular decade ability capture non deterministic relationships variables describing real world domains 
models graphical models received significant attention ability compactly encode conditional independence assumptions random variables development effective algorithms inference learning representations 
bayesian network heckerman particular case graphical model compactly represents joint probability distribution set random variables 
consists components structure set parameters 
structure directed acyclic graph node variable 
structure encodes markov assumption variable conditionally independent non descendants network value parents 
parameters describe variable relates probabilistically parents 
bayesian network encodes unique joint probability distribution easily computed chain rule 
learning bayesian networks correctness learned network course depends amount training data available 
training data scarce useful employ various forms prior knowledge domain improve accuracy learned models 
example domain expert provide prior knowledge specifying conditional independencies variables constraining fully specifying network structure bayesian network 
addition helping specify network structure domain expert provide prior knowledge values certain parameters conditional probability tables cpts network knowledge form prior distributions parameters 
previous research examined number approaches representing utilizing prior knowledge bayesian network parameters type prior knowledge utilized current learning methods remains limited insufficient capture types knowledge may readily available experts 
contribution previous niculescu development general framework perform parameter estimation bayesian networks presence parameter constraints obey certain differentiability assumptions formulating constrained maximization problem 
framework algorithms developed frequentist bayesian point view complete incomplete data 
optimization methods algorithms new general learning approach serious limitations methods especially arbitrary constraints 
approach constitutes basis efficient learning procedures specific classes parameter constraints described 
applying efficient methods allows take advantage parameter constraints provided experts learned program perform accurate learning large bayesian networks thousands variables tens examples see 
main contribution consists efficient algorithms closed form solutions fast iterative algorithms classes parameter constraints current methods accommodate 
show widely models including hidden markov models dynamic bayesian networks module networks context specific independence special cases constraint types described subsection 
framework able represent parameter sharing assumptions level granularity individual parameters 
prior parameter sharing dirichlet priors accommodate simple equality constraints bayesian network learning parameter constraints parameters extends provide closed form solutions classes parameter constraints involve relationships groups parameters sum sharing ratio sharing 
provide closed form maximum likelihood estimators constraints come form types inequality constraints 
estimators come series formal guarantees formally prove benefits advantage parameter constraints reduce variance parameter estimators study performance case domain knowledge represented parameter constraints entirely accurate 
method automatically learning parameter constraints illustrate complex task modelling fmri brain image signal cognitive task 
section describes related research constraining parameter estimates bayesian networks 
section presents problem describes prior framework incorporating parameter constraints perform estimation parameters bayesian networks 
section presents main contribution efficient ways closed form solutions fast iterative algorithms compute parameter estimates important classes parameter constraints 
show learning current models parameter sharing assumptions viewed special case approach 
section experiments real world synthetic data demonstrate benefits advantage parameter constraints compared baseline models 
formal guarantees estimators section 
conclude brief summary research directions 

related main methods represent relationships parameters fall main categories dirichlet priors variants including smoothing techniques parameter sharing kinds 
geiger heckerman shown dirichlet priors possible priors discrete bayes nets provided certain assumptions hold 
think dirichlet prior expert guess parameters discrete bayes net allowing room variance guess 
main problems dirichlet priors related models impossible represent simple equality constraints parameters example constraint jk xi xi parents xi paik priors hyperparameters prior case marginal likelihood longer computed closed form expensive approximate methods required perform parameter estimation 
second problem expert ability specify full dirichlet prior parameters bayesian network 
extensions dirichlet priors include dirichlet tree priors minka dependent dirichlet priors hooper 
priors allow correlation parameters model standard dirichlet priors essentially face issues 
case dependent dirichlet priors parameter estimators computed closed form hooper presents method compute approximate estimators linear rational fractions observed counts dirichlet parameters minimizing certain mean square error measure 
dirichlet priors considered part broader category methods employ parameter domain knowledge called smoothing methods 
comparison smoothing methods zhai lafferty 
widely form parameter constraints employed bayesian networks parameter sharing 
models different types parameter sharing include dynamic bayesian networks niculescu mitchell rao murphy special case hidden markov models rabiner module networks segal context specific independence models boutilier bayesian recursive dynamic geiger heckerman pena bilmes probabilistic relational models friedman object oriented bayes nets koller pfeffer kalman filters welch bishop bilinear models tenenbaum freeman 
parameter sharing methods constrain parameters share value capture complicated constraints parameters inequality constraints constraints sums parameter values 
methods restricted sharing parameters level sharing conditional probability table cpt module networks hmms level sharing conditional probability distribution single cpt context specific independence level sharing state state transition matrix kalman filters level sharing style matrix bilinear models 
prior models allow sharing level granularity individual parameters 
additional type parameter constraints described probabilistic rules 
kind domain knowledge rao 
assign values certain parameters bayesian network 
aware probabilistic rules purpose estimating parameters bayesian network 

problem definition approach define problem describe previous general optimization approach solve 
approach serious limitations constraints arbitrary 
constitutes basis efficient learning procedures classes parameter constraints described section 
optimization methods new applying task allows take advantage expert parameter constraints perform accurate learning large bayesian networks thousands variables tens examples see subsection 
describing problem state assumptions deriving estimators 
problem task perform parameter estimation bayesian network structure known advance 
accomplish task assume data set examples available 
addition set parameter equality inequality constraints provided domain expert 
equality constraints form gi inequality constraints form represents set parameters bayesian network 
initially assume domain knowledge provided expert correct 
investigate happens knowledge completely correct 
enumerate assumptions satisfied methods 
similar common assumptions learning parameters standard bayesian networks 
assume examples training data set drawn independently underlying distribution 
words examples conditionally independent parameters graphical model 
second assume variables bayesian network take different values 
safe assumption uncertainty random variable bayesian network learning parameter constraints possible value 
variables bayesian network deleted arcs nodes corresponding variables 
computing parameter estimators discrete case additionally assume observed counts corresponding parameters bayesian network strictly positive 
enforce condition order avoid potential divisions zero may impact inference negatively 
real world expected observed counts zero 
problem solved priors parameters essentially effect adding positive quantity observed counts essentially create strictly positive virtual counts 
functions gm hk twice differentiable continuous second derivatives 
assumption justifies formulation problem constrained maximization problem solved standard optimization methods 
general approach order solve problem described briefly mention previous approach niculescu existing optimization techniques 
idea formulate problem constrained maximization problem objective function data log likelihood logp maximum likelihood estimation log posterior logp maximum aposteriori estimation constraints gi easy see applying karush kuhn tucker conditions theorem kuhn tucker maximum satisfy system number equations variables 
solve system existing methods example newton raphson method press 
approach niculescu develop methods perform learning frequentist bayesian point view fully partially observable data extended em algorithm 
known finding solution system kkt conditions determine optimum point niculescu discuss estimators meet sufficiency criteria optimum solutions learning problem 
describe constrained conjugate parameter priors map estimation bayesian model averaging 
sampling algorithm devised address challenging issue computing normalization constant priors 
furthermore procedures allow automatic learning useful parameter constraints derived 
unfortunately methods serious shortcoming general case 
large number parameters bayesian network extremely expensive involve potentially multiple runs newton raphson method run requires expensive matrix inversions 
methods finding solutions system equations employed noted press 
methods limitations case constraints arbitrary non linear functions 
worst case happens exists constraint explicitly uses parameters bayesian network 
shortcoming optimization methods derive algorithms new choose go details 
mention show learning presence parameter constraints formulated general constrained maximization problem 
general framework provides starting point efficient learning methods particular classes parameter constraints section 

parameter constraints classes niculescu mitchell rao previous section mentioned existence general methods perform parameter learning bayesian networks set parameter constraints 
methods deal arbitrary parameter constraints obey smoothness assumptions slow involve expensive iterative sampling procedures 
fortunately practice parameter constraints usually involve small fraction total number parameters 
data log likelihood nicely decomposed examples variables values parents variable case discrete variables 
maximum likelihood optimization problem split set independent manageable optimization subproblems solved closed form efficient algorithms derived 
example standard maximum likelihood estimation parameters bayesian network subproblem defined single conditional probability distribution 
general discrete case optimization subproblem span set conditional probability distributions 
set maximum likelihood parameters union solutions subproblems 
section shows classes parameter constraints system equations karush kuhn tucker theorem solved efficient way closed form fast iterative algorithm 
cases able find closed form formula normalization constant corresponding constrained parameter prior 
parameter sharing distribution class parameter constraints allows asserting specific user selected parameters single conditional probability distribution shared 
type constraint allows representing statements combination causes effects equally scope constraint type go level single conditional probability distribution single cpt problem maximizing data likelihood split set independent optimization subproblems conditional probability distribution 
consider subproblems variable specific value pa pa parents 
assume parameter constraint asserts parameters equal asserting parameter appears ki different positions conditional distribution 
denote ni cumulative observed count corresponding cumulative observed count sum observed counts corresponding ki positions appears distribution 
ni sum observed counts conditional probability distribution total number observed cases pa pa may appear develop maximum likelihood estimates network parameters standard methods introducing new variables capture groups shared parameters 
see case consider example 
assume variable values depends assume parameter constraint states 
introduce variable 
variable assumed dependent added parent easy see equal distribution assigns half probability 
takes value task finding maximum likelihood estimators parameter sharing reduced finding standard maximum likelihood estimators 
takes bayesian network learning parameter constraints value safely remove parent take values assume expert states additional assumption 
need introduce new variable depends add parent edge structural assumption conditionally independent obviously true 
assuming parent constraints expert need modelled distribution 
fail encode constraints new structure complicated problem adding nodes network 
similar argument holds discrete types parameter constraints section 
closed form solutions maximum likelihood estimators complete data normalization constant corresponding constrained dirichlet priors perform maximum aposteriori estimation 
priors similar standard dirichlet priors assign zero probability space expert constraints satisfied 
normalization constant constrained dirichlet prior computed scope certain constraint constants multiplied obtain normalization constant prior set parameters bayesian network 
em algorithm approach learning incomplete data type parameter sharing 
maximum likelihood estimation complete data theorem maximum likelihood estimators parameters conditional probability distribution ni ki proof problem maximizing data log likelihood subject parameter sharing constraints broken subproblems conditional probability distribution 
subproblem restated argmax ni log ki counts positive easily proved global maximum achieved interior region determined constraints 
case solution lagrange multipliers 
introduce lagrange multiplier constraint lm 
point maximizes solutions system lm 
solution system 
lm ni ki ki ni summing values obtain lm ki ni equation compute value gives ni ki fact set maximum likelihood estimators follows objective function concave constraint linear equality 
constrained dirichlet priors niculescu mitchell rao bayesian point view choice parameters occur certain probability 
learning easier type parameter constraints employ conjugate constrained dirichlet priors form conditional probability distribution bayesian network ni ki maximum aposteriori estimation performed exactly way maximum likelihood estimation see theorem difference objective function 
normalization constant computed integration depends elimination order 
eliminated obtain zn kn ni normalization thought corresponding 
eliminate different parameter computing integral obtain different normalization constant corresponds different tuple parameters 
note having different constants inconsistency corresponding probability distributions remaining parameters obtained variable substitution constraint ki 
easy see niculescu learning procedures affected way parameter eliminated 
case parameter sharing ki normalization constants equal obtain standard dirichlet prior 
maximum likelihood estimation incomplete data easily proved learning incomplete data achieved modified version standard expectation maximization algorithm train bayesian networks step expected counts estimated step parameters re estimated expected counts theorem 
algorithm expectation maximization discrete bayesian networks randomly initialize network parameters value repeat steps convergence reached step iteration inference algorithm compute expected counts ni distribution network current parameter estimates step re estimate parameters theorem assuming observed counts equal expected counts step 
parameter sharing hidden process models hidden process model probabilistic framework modelling time series data hutchinson predicts value target variable point time bayesian network learning parameter constraints sum values certain hidden processes active 
model inspired interest modelling hidden cognitive processes brain time series observed fmri images brain activation 
think observed image feature value fmri signal small cube inside brain called voxel 
hidden process may thought mental process generates fmri activity various locations brain response external stimulus 
example process may describe fmri signal happens brain starting subject picture 
process may provide characterization situation subject reading sentence 
assume cognitive processes may active point time assume cases observed fmri signal sum corresponding processes translated starting times 
hidden process models viewed subclass dynamic bayesian networks described hutchinson 

formally hidden process model defined collection time series called hidden processes pk 
process pk denote pkt value corresponding time series time process starts 
xt value target variable time process pk starts time tk hidden process model predicts random variable xt follow distribution xt tk considered variance measurement kept constant time 
formula sense consider pkt 
shows example hidden process model fmri activity voxel brain cognitive task involving reading sentence looking picture 
general allow modeling uncertainty timing hidden processes allow uncertainty types processes allow multiple instances process active simultaneously hutchinson 
treatment experiments simplifying assumptions 
assume times hidden processes occur known types processes known instances types process may active simultaneously 
simplifying assumptions lead formulation equivalent analysis approach dale multivariate regression general linear model 
typical fmri experiment subject performs cognitive task multiple times multiple trials providing multiple observation sequences variable framework denote value xt trial starting point process pk trial total number observations 
write entirely necessary method assume tracked length time trial 
length trial observation 
modelling happens assume process length natural constraints domain lead opportunity specify prior knowledge form parameter constraints follows external stimulus typically influence activity multiple voxels brain cognitive task 
example looking picture may activate niculescu mitchell rao hidden process model model human subject asked read sentence look picture 
half observations sentence picture shown 
half observations picture 
activity voxel brain modelled hidden process model processes sentence picture 
observation length fmri snapshots seconds holds processes 
shows observation sentence time picture shown seconds 
time processes overlap fmri signal xt sum corresponding values processes plus measurement variance 
blue dotted line represents fmri activity happen time bayesian network learning parameter constraints voxels visual cortex 
activation voxels may different point time 
intuitively means stimulus may produce different hidden processes different voxels 
certain groups voxels close similarly shaped time series different amplitude 
case believe reasonable assume underlying hidden processes corresponding voxels proportional 
experiments performed section prove assumption help learn better models ones choose ignore 
paragraph explained intuitively sense share base processes time varying random variables allow different scaling factors 
formally say time varying random variables share corresponding hidden process models exist base processes pk constants cv nt nk values different variables independent parameters model 
represents variance measurement shared variables 
consider efficiently perform maximum likelihood estimation parameters variables assuming share corresponding hidden process model parameters described 
parameters estimated base process parameters pkt scaling constants cv variable process common measurement variance pkt set parameters involved base processes cv set scaling constants 
subsequently think sets column vectors 
recall represents number observations 
incorporating parameter sharing constraints log likelihood function optimization problem argmax log log nt pk tv nk easy see value maximizes values 
maximize minimize xv nt cv pk tv nk respect maximize respect minimum point may notice sum squares quantity inside square seen linear function imagine iterative procedure minimizes respect respect squares method 
find min value maximizes nv derived straightforward fashion enforcing 
considerations ready algorithm compute maximum likelihood estimators parameters shared hidden process model algorithm maximum likelihood estimators shared hidden process model column vector values nt 
start random guess repeat steps niculescu mitchell rao converge minimum function 
step 
write kt matrix depends current estimate scaling constants 
specifically row corresponds squares column corresponds kt parameters base processes column number associated parameter coincide position column vector 
minimize respect ordinary squares get new estimate step write kv matrix depends current estimate base processes 
specifically row corresponds squares column corresponds kv scaling constants column number associated constant coincide position column vector 
minimize respect ordinary squares get new estimate step 
convergence reached repeating steps nv expensive algorithm iterative method 
applied fmri data experiments usually converges repetitions steps 
believe main reason happens partial step iteration compute closed form global minimizer potentially expensive gradient descent algorithm 
section experimentally prove benefits algorithm methods take advantage parameter sharing assumptions 
may suspect easy learn parameters model particular case bilinear model 
case 
bilinear model representation tenenbaum freeman style matrices correspond process parameters content vectors correspond scaling constants 
easy see case style matrices common pieces depending processes started example 
svd method tenenbaum freeman assumes independence style matrices appropriate problem 
classes parameter constraints subsections discussed efficient methods perform parameter estimation types parameter constraints discrete variables continuous variables 
methods bypass need potentially expensive methods newton raphson 
number additional types parameter constraints developed closed form maximum likelihood maximum aposteriori estimators equality inequality constraints individual parameters sums ratios parameters discrete continuous variables 
cases able compute normalization constant closed form corresponding constrained priors allows perform parameter learning bayesian point view 
results niculescu 
briefly describe types parameter constraints provide real world examples prior knowledge expressed form constraint 
bayesian network learning parameter constraints constraint type known parameter values discrete 
example patient heart attack disease heart attack probability patient experience chest pain 
constraint type parameter sharing distribution discrete 
example combination risk factors diseases equally 
constraint type proportionality constants distribution discrete 
example combination risk factors disease twice occur disease constraint type sum sharing distribution discrete 
example patient smoker chance having heart disease heart attack congestive heart failure having pulmonary disease lung cancer chronic pulmonary disease 
constraint type ratio sharing distribution discrete 
example bilingual corpus relative frequencies certain groups words aggregate frequencies groups may different 
groups words words computers computer mouse monitor keyboard languages words business countries computer extensive expect aggregate probability words computers different 
natural assume relative proportions words computers different languages 
constraint type general parameter sharing multiple distributions discrete 
example probability person heart attack smoker family history heart attack patient lives polluted area 
constraint type hierarchical parameter sharing multiple distributions discrete 
example frequency international words instance computer may shared latin languages spanish italian languages russian bulgarian 
latin words frequency latin languages holds languages 
words language specific example names country specific objects frequencies shared language 
constraint type sum sharing multiple distributions discrete 
example frequency nouns italian frequency nouns spanish 
constraint type ratio sharing multiple distributions discrete 
example different countries relative frequency heart attack main diagnosis aggregate probability heart disease heart attack may different differences lifestyle countries 
constraint type inequalities sums parameters distribution discrete 
example aggregate probability mass adverbs greater aggregate probability mass verbs language 
niculescu mitchell rao constraint type upper bounds sums parameters distribution discrete 
example aggregate probability nouns english greater 
constraint type parameter sharing distribution continuous 
example stock computer maker dell gaussian mean weighted sum stocks software maker microsoft msft chip maker intel intl 
parameter sharing corresponds statement msft intl importance weight predicting value stock dell 
constraint type proportionality constants distribution continuous 
example suppose throw stock power supply maker linear mix example 
expert may give equal weights intl msft times lower 
constraint type parameter sharing hidden process models 
example neighboring voxels brain exhibit similar activation patterns different amplitudes subject stimulus 
note general parameter sharing constraint type encompasses models including hmms dynamic bayesian networks module networks context specific independence particular cases allows finer grained sharing level individual parameters different variables distributions different lengths 
briefly general parameter sharing allows group conditional probability distributions share parameters distributions group share remaining parameters 
type parameter constraint described detail niculescu 
demonstrate estimators task modelling synthetic emails generated different subpopulations 
important note different types parameter constraints mixed learning parameters bayesian network long scopes constraints overlap 

experiments section experiments synthetic real world data 
experiments demonstrate bayesian network models take advantage prior knowledge form parameter constraints outperform similar models choose ignore kind knowledge 
synthetic data estimating parameters discrete variable section describes experiments involving simplest forms parameter constraint parameter sharing distribution subsection 
purpose experiments purely demonstrative complicated scenario real world data subsection 
experimental setup task estimate set parameters bayesian network consists discrete variable assume prior knowledge available distribution shares certain parameters 
loss generality consider parameter constraint states bayesian network learning parameter constraints parameters estimate appears ki known places distribution synthetic data set created follows randomly generated distribution true distribution exhibits parameter sharing 
distribution described variable values total roughly shared parameters ki ki ki ki 
distinct parameter appeared times 
start empty distribution generate uniformly random parameter 
generate random integer share places distribution 
continue generate shared parameters reach parameters 
generate rest parameters uniformly randomly 
parameters obtained procedure normalize yield valid probability distribution 
distribution generated sampled obtain data set examples subsequently perform parameter estimation 
experiments compare models estimate parameters distribution standard bayesian network learned standard bayesian networks maximum likelihood estimators parameter sharing 
second model bayesian network learned results assuming correct parameter sharing specified oracle 
needs estimate ki parameters needs estimate parameters 
deal potentially zero observed counts priors parameters models performed maximum aposteriori estimation 
introduced dirichlet count parameter constrained dirichlet count ki distinct parameter network 
role priors simply assure strictly positive counts 
results discussion performed parameter estimation models varying number examples training set 
synthetic data able assess performance computing kl kl kl divergence true distribution shows graphical comparison performance models 
seen model takes advantage parameter constraints consistently outperforms standard bayesian network model employ constraints 
difference models greatest training data sparse 
highest observed difference kl kl observed models trained examples 
expected amount training data increases difference performance models decreases dramatically 
training examples kl examples needed table equivalent training set size achieves performance 
kl niculescu mitchell rao training set size kl divergence respect correct model get better idea beneficial prior knowledge parameter constraints case examine far 
model learned data set size measured number examples requires order achieve performance 
table provides numbers training set sizes 
example uses examples achieve kl divergence examples factor maximum observed increase number training samples required 
average needs times examples perform 
mentioned previously subsection intended proof concept 
provide experimental results complex task involving random variables prior knowledge form parameter constraints conditional probability distributions 
real world data fmri experiments noted earlier functional magnetic resonance imaging fmri technique obtaining threedimensional images activity brain time 
typically fifteen voxels dimensional pixels image voxel covers tens millimeters brain tissue 
due nature signal fmri activation observable due neural activity extends approximately seconds neural activity resulting temporally blurred response see mitchell 
brief overview machine learning approaches fmri analysis 
section presents generative model activity brain human subject performs cognitive task hidden process model parameter sharing approach discussed section 
experiment involving real fmri data complex cognitive task domain experts unable provide parameter sharing assumptions advance 
bayesian network learning parameter constraints developed algorithm automatically discover clusters voxels accurately learned shared parameters 
section describes algorithm discovering parameter sharing constraints shows training parameter constraints leads hidden process models far outperform baseline hidden process models learned absence parameter constraints 
experimental setup experiments reported fmri data collected study sentence picture comprehension carpenter 
subjects study sequence trials 
trials subject sentence seconds plus sign star sign blank screen seconds picture seconds 
trial subject required press button indicate sentence correctly described picture 
remaining trials picture sentence second timing 
data set voxels grouped anatomically defined spatial regions interest voxel having resolution millimeters 
image brain taken half second 
trial considered images seconds brain activity 
results reported section data single human subject 
particular subject data set tracked activity different voxels 
model activity voxel hidden process model processes corresponding cognitive processes comprehending sentence picture 
start time processes assumed known advance assume process begins immediately seeing sentence picture stimulus 
assume activity different voxels independent hidden processes corresponding voxels 
true underlying distribution voxel activation known average log likelihood score log likelihood test data divided number test examples assess performance trained 
data scarce afford keep large held test set 
employ leave cross validation approach estimate performance models 
experiments compare models 
model consider baseline consists standard hidden process model learned independently voxel 
second model hidden process model shared voxels roi 
words voxels specific roi share shape hidden processes different amplitudes see section details 
learned algorithm 
third model learns set shared hidden process models assuming priori particular set voxels grouped chooses voxel groupings nested cross validation hierarchical approach come partition voxels clusters form shared hidden process model 
algorithm follows algorithm hierarchical partitioning hidden process models learning niculescu mitchell rao step 
split examples set containing folds fold containing example sentence example picture 
step 
keep fold fk aside learn model remaining folds steps 
step 
start partition voxels brain mark subsets final 
step 
subsets partition final take subset try split equally spaced hyperplanes directions experiments split subset smaller subsets 
cross validation average log score model learned new subsets algorithm folds fk lower cross validation average log score initial subset folds fk mark initial subset final discard subsets 
remove initial subset partition replace subsets mark final 
step 
partition computed steps data points fk learn hidden process model shared voxels inside subset partition 
model compute log score examples trials fk 
step 
steps came partition fold fk 
come single model compute partition steps folds partition learn model step examples 
average log score model estimated averaging numbers obtained step 
results discussion estimated performance models average log score leave cross validation approach fold contains example sentence example picture 
set experiments summarized table compared models performance visual cortex calc 
actively involved cognitive task contains voxels 
training set size varied examples examples multiples 
sharing parameters hidden process models proved beneficial impact observed best training set size smallest 
increase number examples performance starts degrade biased assumption voxels calc described single shared hidden process model 
assumption paid small training set size reduction variance definitely hurt terms bias larger sample size 
bias obvious calc see experiments certain assumption holds cases gains performance may quite large 
expected hierarchical model performed better takes advantage shared hidden process models making restrictive bayesian network learning parameter constraints training sharing shared hierarchical cells trials table effect training set size average log score models visual cortex calc region 
sumption sharing entire 
largest difference performance observed examples case basically fails learn reasonable model highest difference occurs maximum number examples presumably bias harmful 
number training examples increases tend perform better better see marginal improvement performance obtained addition new examples tends shrink models approach convergence 
infinite amount data expect converge true model examples outperforms baseline model difference terms average log score improvement terms data likelihood 
probably measure shows best improvement baseline number examples needed achieve performance 
turns average needs roughly times number examples needed order achieve level performance visual cortex calc 
column table displays number clusters voxels partitioned calc 
seen small sample size draws performance reductions variance cluster voxels 
number examples increases improves finding refined partitions 
number shared voxel sets tends stabilize clusters number examples reaches yields av niculescu mitchell rao erage voxels cluster calc voxels 
training set examples largest cluster voxels clusters consist voxel 
roi voxels sharing shared hierarchical cells hierarchical calc lips lit lt rips rit roper rt sma brain table roi performance average log score models learned examples 
second set experiments see table describes performance models individual brain trained entire brain 
seen biased calc see sense characterize voxels single shared hidden process model 
fact regions finds cluster voxels 
outperforms baseline model outperforms 
may ask possibly outperform roi may represent case sharing 
explanation hierarchical approach get stuck local maximum data log likelihood search space improve splitting bayesian network learning parameter constraints parameter sharing model 
slice brain showed 
shared neighboring voxels color 
specific step greedy process look split finer grained partition 
fortunately problem appears rare experiments 
brain outperforms factor terms log likelihood outperforms factor 
main drawback restrictive sharing assumption suggest recommended approach 
give reader feel learned model looks 
mentioned automatically learns clusters voxels represented shared hidden process model 
shows portions learned clusters slice vertical slices brain image captured fmri scanner 
neighboring voxels assigned cluster pictured color 
note large clusters picture 
may fact sense represent entire roi single shared hidden process model cognitive process activate voxels roi 
large clusters areas calc know directly involved visual processing 
see learned sentence hidden process voxels visual cortex calc 
graphs corresponding voxels belong cluster painted color color 
graphs readable plotted base process disregarding scaling amplitude constants corresponding voxel cluster consult section details shared hidden process models 
niculescu mitchell rao voxel base sentence processes visual cortex calc 
summarize subsection experiments training different generative models fmri signal cognitive task hidden process models 
demonstrated experimentally parameter sharing hidden process models defined section greatly benefit learning possible automatically discover useful parameter sharing constraints domain hierarchical partitioning algorithm 

formal guarantees advantage parameter constraints beneficial learning intuitively effect lowering variance parameter estimators shrinking degrees freedom model 
section provide formal proof fact 
order proof assumption true distribution factors bayesian network structure obeys parameter constraints provided expert 
second interesting result section give theoretical guarantees case constraints provided expert entirely accurate 
investigate issue type constraint parameter sharing distribution introduced subsection believe similar formal guarantees describe types parameter constraints 
variance reduction parameter constraints assume want learn bayesian network case domain expert provides parameter constraints specifying certain parameters appear multiple times shared conditional probability distribution 
conditional probability distribution bayesian network constraints 
case parameters distinct distribution may seen particular case parameter sharing distribution parameter shared exactly 
bayesian network learning parameter constraints ways perform maximum likelihood parameter learning bayesian network 
may choose ignore constraints expert compute standard maximum likelihood estimators 
second option incorporate constraints learning method case results described subsection 
intuitively expect advantage constraints provided expert reduce variance parameter estimates compared approach 
niculescu prove result theorem assuming domain expert specify parameter sharing assumptions take place inside conditional probability distributions bayesian network maximum likelihood estimators domain knowledge computed theorem lower variance standard maximum likelihood estimators computed ignoring domain knowledge 
specifically parameter jk shared times xi pai paik denote ml jk maximum likelihood estimator ignores domain knowledge ps jk maximum likelihood estimator uses parameter sharing assumptions specified expert 
identity var ml jk var ps jk jk nik nik performance potentially inaccurate constraints may happen parameter constraints provided expert completely accurate 
methods far assumed parameter constraints correct errors domain knowledge prove detrimental performance learned models 
section investigate relationship true underlying distribution observed data distribution estimated methods parameter constraints 
particular come upper bound estimated model perform set potentially incorrect parameter constraints 
assume expert provides set potentially incorrect parameter sharing assumptions described subsection 
words conditional probability distribution bayesian network expert stating parameter ic shared positions 
denote nic cumulative observed count corresponding presumably shared parameter ic nc cumulative observed count corresponding conditional distribution essentially follow notations subsection add additional index corresponding conditional probability distribution parameter belongs 
introduce notion true probabilistic counts tpc 
suppose true distribution data sampled 
example expert states ic shared parameter describes set pa pa pa pa xi pa pa 
distribution factorizes structure provided expert parameters theorem observed counts replaced true probabilistic counts 
theorem closest distribution terms kl factorizes structure obeys expert parameter sharing assumptions 
niculescu mitchell rao proof distribution 
minimizing equivalent maximizing logq 
set parameters describe distribution breaking logarithms sums logarithms factorization provided structure optimization problem reduces maximization log ic exactly objective function theorem 
equivalent fact see definition minimizes kl distributions factorize structure obey expert sharing assumptions 
theorem infinite amount data distribution maximum likelihood estimators theorem converges probability 
proof assume number data points data set sampled denoted law large numbers limn nic 
implies converges probability 
corollary true distribution factorizes structure parameter sharing provided expert completely accurate distribution estimators computed theorem converges probability 
mention analyzed formal guarantees section type parameter constraints 
confident results extended types constraints computed closed form solutions 

building accurate models limited training data possible form prior knowledge augment data 
demonstrated theoretically experimentally standard methods parameter estimation bayesian networks naturally extended accommodate parameter constraints capable expressing wide variety prior domain knowledge 
mentioned previous methods incorporating general parameter constraints estimators parameters bayesian network framing task constrained optimization problem 
general case solving resulting optimization problem may difficult 
fortunately practice optimization problem decomposed set smaller independent optimization subproblems 
parameter estimators types constraints including constraints force various types parameter sharing constraints sums relationships groups parameters 
subsection provides comprehensive list parameter constraint types studied brief examples 
considered learning discrete continuous variables presence equality inequality constraints 
types parameter constraints derive closed form maximum likelihood estimators developed efficient iterative algorithm perform task shared hidden process models 
cases discrete variables able compute closed form normalization constants corresponding bayesian network learning parameter constraints constrained parameter priors allowing perform closed form map bayesian estimation data complete 
general parameter sharing domain knowledge type constraint type defined subsection encompasses models including hmms dynamic bayesian networks module networks context specific independence particular cases allows finer grained sharing parameter level different variables distributions different lengths 
important note combine different types parameter constraints learning parameters bayesian network long scopes constraints overlap 
experimental results fmri brain imaging application demonstrate advantage parameter constraints beneficial learning high dimensional domain 
context application developed methods automatically discover parameter sharing constraints 
methods program discovered clusters voxels parameters shared 
results showed impact learned parameter constraints equivalent size training set task 
experiments synthetic data demonstrated beneficial effect incorporating parameter constraints 
basic theoretical result estimators advantage simple form parameter sharing achieve variance lower achieved estimators ignore constraints 
conjecture similar results hold types parameter constraints proof left 
addition proved asserted parameter constraints turn incorrect infinite amount training data maximum likelihood estimators converge best describable distribution distribution closest terms kl distance true distribution distributions obey parameter constraints factor structure 
see useful directions 
considered take advantage deterministic parameter constraints structure bayesian network known advance 
interesting investigate methods incorporate probabilistic constraints learning algorithms bayesian networks 
second direction explore parameter constraints perform structure learning 
achieved specifying initial set parameter constraints step hill climbing structure search performing change variable adapt constraints new parameterization network 
extend results undirected graphical models extent intuitive acquire domain knowledge expert harder interpret parameters models 
acknowledgments people useful comments suggestions development research john lafferty andrew moore russ greiner zoubin ghahramani krishnapuram 
student carnegie mellon university radu stefan niculescu sponsored national science foundation nos 
ccr ccr darpa pal program contract generous gift siemens medical solutions 
niculescu mitchell rao bilmes 
dynamic bayesian multinets 
proceedings uai pages 
boutilier friedman goldszmidt koller 
context specific independence bayesian networks 
proceedings th uai pages 
carpenter just keller eddy 
time course language spatial networks sentence comprehension 
neuroimage 
dale 
optimal experimental design event related fmri 
human brain mapping 
friedman getoor koller pfeffer 
learning probabilistic relational models 
proceedings th ijcai pages 
geiger heckerman 
knowledge representation inference similarity networks bayesian multinets 
artificial intelligence 
geiger heckerman 
characterization dirichlet distribution global local parameter independence 
annals statistics 
heckerman 
tutorial learning bayesian networks 
jordan editor learning graphical models 
mit press cambridge ma 
hooper 
dependent dirichlet priors optimal linear estimators belief net parameters 
press editor proceedings th annual conference uncertainty artificial intelligence uai pages 
hutchinson mitchell 
learning identify overlapping hidden cognitive processes fmri data 
th conference human brain mapping june 
hutchinson mitchell 
hidden process models 
technical report cs carnegie mellon university february 
koller pfeffer 
object oriented bayesian networks 
proceedings th uai pages 
kuhn tucker 
nonlinear programming 
proceedings second berkeley symposium mathematical statistics probability pages 
university california press 
minka 
dirichlet tree distribution 
unpublished available online research microsoft com minka papers dirichlet minka pdf 
mitchell hutchinson just newman niculescu pereira wang 
learning decode cognitive states brain images 
machine learning 
murphy 
dynamic bayesian networks representation inference learning 
phd thesis uc berkeley 
bayesian network learning parameter constraints niculescu 
exploiting parameter domain knowledge learning bayesian networks 
technical report cmu tr carnegie mellon university 
niculescu mitchell rao 
parameter related domain knowledge learning graphical models 
proceedings siam data mining conference 
pena lozano 
learning recursive bayesian multinets data clustering means constructive induction 
machine learning 
press teukolsky vetterling 
numerical recipes art scientific computing 
cambridge university press 
rabiner 
tutorial hidden markov models selected applications speech recognition 
proceedings ieee 
rao niculescu rao 
clinical financial outcomes analysis existing hospital patient records 
proceedings ninth acm sigkdd international conference knowledge discovery data mining pages 
segal pe er regev koller friedman 
learning module networks 
proceedings th uai pages 
tenenbaum freeman 
separating style content bilinear models 
neural computation 
welch bishop 
kalman filter 
technical report tr university north carolina 
zhai lafferty 
study smoothing methods language models applied ad hoc information retrieval 
proceedings sigir pages 

