structural event detection rich transcription speech thesis submitted faculty purdue university yang liu partial fulfillment requirements degree doctor philosophy december parents husband 
ii acknowledgments started research structural event detection purdue university continued icsi past half years 
icsi provided wonderful environment enrich research speech language processing 
gratefully acknowledge major advisor mary harper academic moral support past years 
away campus constant touch email phone supporting research 
benefited insightful guidance discussion encouragement 
intellectual freedom research spoken language pro cessing provided lots advice 
taught researcher years drafts revised 
elizabeth shriberg andreas stolcke giving opportunity continue research icsi 
valuable tions comments encountered difficulties research 
learned look problem scientific engineering point view 
especially elizabeth shriberg teaching linguistics providing academic advice past years 
ph committee members jamieson jack purdue university 
generous time supportive research topic 
benefited discussions jamieson speech processing years study purdue university 
people icsi deserve acknowledgment 
academic front bar bara peskin shared vision entire structural event detection project time willing spend time working details 
nelson iii morgan director icsi created excellent environment research learning 
chuck wooters james fung deserve special generating speaker results 
jeremy ang barry chen dave dan gillick andy hatch yan huang adam janin zhu helpful office mates neighbors icsi 
time icsi enjoyable 
people contributed research 
ferrer sri helped prosodic feature extraction 
mari ostendorf university washington collaboration structural event detection 
wen wang finished ph purdue university sri patient questions regarding language models 
am glad chance lei chen purdue university multimodal corpus sentence boundary detection 
chawla wonderful source answers machine learning questions 
andrew mccallum university massachusetts fernando pereira university pennsylvania support advice crf model 
julia yoav freund columbia university assistance boosting algorithm 
family support education 
able reach journey consistent support encouragement husband 
belief thesis possible 
love parents sister supported difficult times graduate life 
iv table contents page list tables 
list figures 
xvi 
xviii 
motivation 
scope thesis 
structural event detection tasks 
approach problem 
related 
sentence boundary detection 
text processing sentence boundary detection 
combining textual prosodic information sentence boundary detection 
summary past research sentence boundary detection edit disfluency processing 
production properties disfluencies 
past research automatic disfluency detection 
summary past research disfluencies 
filler word processing 
production perception fillers 
past research filler word processing 
chapter summary 
data resources tasks 
structural speech events types 
vi page sentence units sus 
fillers 
edit disfluencies 
structural event detection task description 
task description 
performance measures 
corpora 
hmm approach structural event detection 
overview 
feature types 
prosodic features 
textual features 
models 
prosody model 
language model lm 
model combination 
chapter summary 
hmm baseline performance 
system description 
choice classes 
training procedures 
testing procedures 
baseline system performance 
task su detection 
task filler word detection 
task edit word ip detection 
summary tasks 
chapter summary 
vii page incorporating textual knowledge sources hmm system 
review related language model techniques 
various knowledge sources 
word lm 
automatically induced classes aic 
part speech pos tags 
syntactic chunk tags 
word lms additional corpora 
integration methods lms hmm 
experiments su detection task 
cts su task 
bn su task 
chapter summary 
prosody model 
addressing imbalanced data set problem 
imbalanced class distribution problem 
approaches address problem 
pilot study su detection 
experimental setup 
sampling results 
bagging results 
sampling bagging su ip tasks 
experimental setup 
results su ip tasks 
evaluation full nist su task 
experimental setup 
results nist su task 
viii page chapter summary 
summary 
discussion 
approaches combine knowledge sources 
knowledge sources 
review hmm su detection 
maxent posterior probability model su detection 
description maxent model 
features 
comparisons maxent hmm approaches 
results discussion maxent su model 
conditional random field crf model su detection 
description crf model 
comparisons crf models 
results discussion crf su model 
chapter summary 
system rt 
rt tasks data 
system performance su boundary detection 
su su subtype detection 
edit word detection 
methods 
edit detection results 
chapter summary 
related efforts 
factors impacting performance 
word error rates wer 
speaker label su detection 
ix page word fragment detection 

acoustic prosodic features 
experiments 
chapter summary 
final remarks 
impact research efforts 
structural event information word recognition su detection multi modal corpus 
dialog act detection meeting corpus 
summary experiments 
contributions 

list 
appendices 
appendix adt boosting su ip detection 
adt boosting description 
experimental results 
adt boosting summary 
appendix prosodic features 
vita 
list tables table page symbols structural events example annotated transcriptions 
summary important prior studies sentence boundary detection 
column task chosen investigation boundary means sentence boundary detection task compared subtype punctuation detection column describes model information sources investigation column corpus experiments conducted column represents experiments performed human transcriptions ref recognition results asr 
note cts conversational telephone speech corpus column experiments conducted switchboard corpus 
textual information automatic detection model ref condition study evaluation 
summary important prior studies disfluency detection 
column investigation column describes model information sources investigation column corpus experiments conducted column represents experiments performed human transcriptions ref recognition results asr 
core preliminary repair information provided parser corrects 
structural events annotated ldc investigated thesis 
note subtype edit disfluency annotated ldc correction edit disfluency 
information cts bn corpora including data set sizes percentage different types structural events training set word error rate wer speech recognizer test set 
examples cue words highly representative structural event types 
examples prosodic features su detection problem appear decision tree shown 
table page cts su detection results nist su error rate boundary cer parentheses human transcriptions ref recognition output stt lm prosody model individually combination 
baseline error rate assuming su boundary word boundary nist su error rate cer 
deletion insertion error rates nist su error rate cts ref condition lm prosody combination 
feature usage su detection cts 
bn su detection results nist su error rate cer parentheses prosody model lm combination 
results shown ref stt conditions 
baseline error rate nist su error rate cer 
deletion insertion error rates nist su error rate bn ref condition lm prosody model combination 
feature usage su detection bn 
results cts filler word including fp dm detection fp detection dm boundary detection nist error rate cer parenthesis prosody model lm combination 
results ref stt conditions 
baseline cer filler word detection fp detection dm boundary detection 
feature usage fp dm detection tasks cts 
cts edit word ip detection results nist error rate cer parenthesis prosody model lm combination 
results ref stt conditions 
baseline cer edit word detection edit ip detection 
feature usage ip detection cts corpus 
system performance nist error rate structural event detection tasks cts bn test sets 
results ref stt conditions 
examples automatically induced classes cts su detection task depicting member words word probability class 
xi table page pos chunk tags sentence bn corpus top selling car nineteen announced today winner toyota 
su detection results nist error rate human transcriptions cts data various lms combination prosody model 
deletion del insertion ins total error rate reported 
su detection results nist error rate human transcriptions bn data various lms combination prosody model 
deletion del insertion ins total error rate reported 
description data set pilot study cts su detection task 
su detection results cer measure different sampling approaches pilot study cts corpus prosody model combination lm 
cer lm test set 
recall precision results sampling methods pilot study cts su detection 
lm yields recall precision 
cts su detection results cer measure bagging applied randomly downsampled data set ds ensemble downsampled training sets original training set 
results training conditions bagging shown comparison 
description data sets su ip detection tasks 
data set pilot study shown second column subset data set investigation large set denoted table 
ip su detection results cer 
ds denotes downsampled 
chance performance original test set ip su 
cer lm ip task su task 
su detection results nist error rate cts bn corpora ref stt conditions 
su detection results nist error rate different state configurations trigram lm cts condition 
insertion ins deletion del total error rate shown 
xii table page su detection results nist error rate maxent hmm approaches individually combination bn cts transcriptions ref recognition output stt 
deletion insertion total error rate nist error rate hmm maxent approaches transcriptions bn cts 
su detection results nist error rate different knowledge sources bn cts evaluated transcription 
comparison posterior probabilities prosody model binary features versus continuous valued features maxent approach su detection cts transcription condition 
gram features highest ig weights cts su detection task 
notation contingency table chi square statistics 
su detection results nist error rate different feature selection metrics different pruning thresholds number preserved features cts ref condition 
su detection results nist error rate hmm maxent crf approaches individually combination bn cts transcriptions ref recognition output stt 
combination approaches obtained majority vote 
cts su detection results nist error rate hmm maxent crf individually different knowledge sources 
note features condition uses knowledge sources described section 
bn su detection results nist error rate hmm maxent crf individually different knowledge sources 
data description cts bn rt nist evaluation 
bn training data combined rt rt data 
cts contains rt training data 
su boundary detection results nist su error rate rt evaluation data 
combination majority vote maxent crf improved hmm approaches 
ds denotes downsampled training set 
percentage su subtypes cts bn 
xiii table page su su subtype detection results rt cts evaluation data 
results reported nist su boundary error rate substitution error rate subtype classification error rate cer 
su subtype detection results confusion matrix cts human transcription condition 
cell shows count percentage subtype row hypothesized subtype shown column 
states transitions crf edit word edit ip detection 
class tags edit inside edit possible ip associated ip ip outside edit 
results nist error rate edit word ip detection hmm maxent crf approaches recognition output conditions cts data 
results nist error rate edit word ip detection hmm maxent approaches 
su edit word detection results nist error rate cts bn ref various stt conditions rt data 
su detection results reported su boundary detection error 
stt stt different stt outputs wer shown table 
comparisons different ways derive speaker labels rt test set bn su boundary detection task 
results shown nist error rate hmm transcription condition 
word fragment detection results confusion matrix downsampled data switchboard corpus 
feature usage word fragment detection switchboard data 
wer su information fed back re segment re recognize speech compared baseline acoustic segments evaluating half rt bn data 
su detection results nist error rate wombat data 
note combined result shown textual information order results parallel results chapter table table 
xiv table page da boundary detection results nist error rate icsi meeting data 
results transcriptions ref stt output pause decision tree pause dt model hidden event lm hmm combination 
da subtype classification accuracy da boundaries icsi meeting corpus human transcriptions recognition output 
conditions word features combined word features binned posterior probabilities decision tree dt 
chance performance obtained majority type statement hypothesized da 
su ip detection results classification error rate adt learning algorithm bagging 
training testing conducted downsampled training testing set 
chance performance 
xv list figures page flow diagram automatic structural event detection task 
examples transcriptions cts bn respectively 
su boundaries shown examples 
waveform pitch energy contours word alignment su boundaries utterance um hadn heard 
raw stylized contours utterance um hadn heard 
example decision tree su detection 
line represents node tree associated question regarding particular prosodic feature class distribution class examples going node stands su boundary non su boundary 
indentation represents level decision tree 
features tree described table 
data preparation model training 
system flow diagram testing procedure 
system diagram edit word ip detection 
valid state transitions repetitions words 
axes represent position reparandum repetition regions respectively events denoted orig rep 
orig means position word reparandum rep total number repeated words represents position event repeat region 
optional filler words allowed ip transition 
rule method determining reparandum region ips hypothesized 
su hypotheses rules 
integration methods various lms prosody model 
bagging algorithm 
experiments 
bag class distribution original data 
xvi page xvii roc curves aucs decision trees trained different sampling approaches original training set 
roc curves aucs decision trees bagging downsampled training set bag ds ensemble downsampled training sets bag ensemble original training set bag original 
roc curves ip su detection prosody model cts corpus 
graphical model su detection problem 
word event pair depicted state model grams previous tokens condition transition state 
observations consisting words prosodic features structural events 
graphical model pos tagging problem 
pos tags hidden states problem 
pos tags words 
graphical representation crf sentence boundary detection problem 
represents state tags su boundary word prosodic features respectively 
observations consisting 
graphical model representations hmm cmm crf approaches 
observations events tags 
illustration speaker change obtained cts data 
arrow represents speaker change segment 
pruned decision tree detect word fragments 
decision leaf nodes decision internal node tree shown 
su information re recognition bn 
example alternating decision tree adt 
xviii liu yang 
ph purdue university december 
structural event detection rich transcription speech 
major professor mary harper 
speech recognition technology significantly improved past decades current speech recognition systems output stream words providing useful structural information aid human reader downstream language processing modules 
thesis research focuses automatic detection helpful structural events speech including sentence boundaries type utterance filled pauses discourse markers edit disfluencies 
systems evaluated combine prosodic cues textual information sources variety ways support automatic detection structural events 
exper iments conducted corpora conversational speech broadcast news speech different transcription quality human transcriptions versus recog nition output 
imbalanced data problem investigated training decision tree prosody model component system structural events frequent non events 
variety sampling approaches bagging address imbalance 
significant performance improvements obtained bagging 
sampling methods useful depending performance metrics 
sentence boundary detection disfluency detection tasks impacted differently sampling bagging boosting suggesting inherent differences tasks 
variety methods combining knowledge sources examined hidden markov model hmm maximum entropy maxent model conditional random field crf 
maxent crf approaches discriminatively trained model posterior probabilities correlate performance mea sures 
support correlated features enable combination variety textual information sources 
hmm crf model sequence information maxent explicitly models local infor mation 
model combines approaches superior method 
interactions research efforts suggest methods developed thesis generalize corpora multimodal corpus multiparty meeting corpus similar tasks gestural model dialog act segmentation classification 
xix motivation 
speech recognition technology improved significantly past decades tasks involving read pre planned speech recognition accuracy greater 
word level transcription accuracy spontaneous conversational speech falls far short level generally lower 
acoustic properties spontaneous conversational speech quite challenging model due phenomena coarticulation word fragments filled pauses 
additionally disfluencies ungrammatical utterances pose serious problems language models lms 
factors combine affect performance speech recognizers spontaneous speech 
excerpt transcription spontaneous conversational speech 
human transcription recogni tion output shown example 
presence word fragment example represented partial word 
word recognition errors recognition output corresponding correct words shown bold face inside curly parentheses corresponding deletion substitution errors 
human transcription uh think know mean uh mean lot experiences uh people especially uh extended family kind see know know may need get close family environment get values know mean uh money big issue wi going today recognition output um uh think know mean uh mean lot experiences uh people especially extended family night kind see know know may need re get close family environment get values know mean uh money big issue wi really going today seen recognition output example current automatic speech recognition asr systems simply output stream words 
structural informa tion location punctuation disfluencies speaker turns missing making difficult human read downstream automatic processors deal 
shown example human transcription contains word errors hard read due absence punctuation presence speech disfluencies filler words 
transcriptions marked different types structural information enhance readability ease downstream processing 
thesis types structural events considered sentence boundaries sentence ends statement incomplete statement question marked transcription examples thesis 
filler words include filled pauses uh um discourse marker words know 
tokens mark extent filler words 
edit disfluencies disfluencies highly prevalent conversational speech 
thesis term edit disfluency disfluencies structure see chapter details reparandum editing term correction edited portion disfluency reparandum marked examples parentheses 
example lot human transcription shown reparandum marked parentheses 
interruption point ip inside edit disfluency marked 
editing term follows ip precedes corrections optional 
edit disfluency structure embedded utterances may preceded followed words part edit disfluency 
types structural information described detail chap ter 
annotation human transcription example 
words interrupt fluency speech shown bold face example 
table summarizes meanings symbols annotated transcriptions 
uh think know mean lot experiences uh people especially uh extended family kind see know know may need get close family environment get values know mean uh money big issue wi going today transcriptions containing structural information called rich tran contain richer information simple stream disfluencies called speech repairs literature 
human transcription illustrate importance structural information order factor effect speech recognition errors 
table symbols structural events example annotated transcriptions 
symbol meaning sentence boundaries complete incomplete filler words reparandum edit disfluency interruption point edit disfluency words 
structural information human annotated automatically generated human transcriptions recognition output cleaned im proved readability 
example disfluencies fillers removed previous transcription sentence appropriate ation cleaned transcription follows lot experiences people especially extended family 
kind see may need get close family environment get value 
money big issue going today 
clearly cleaned transcription readable easier understand appropriate subsequent language processing modules 
growing interest study impact structural events 
jones conducted experiments showing cleaned tran improve human readability compared original transcription 
research considered automatically generated sentence informa tion play role parsing 
gregory sentence internal prosodic cues degrades parsing performance method automatically generating sentence internal annotations state art 
hand kahn achieved significant error reductions pars ing performance sentence boundary information state art automatic detection system 
scope thesis structural event detection tasks automatic structural event detection crucial step improving ity speech recognition output making spontaneous speech understanding systems possible 
goal thesis enrich recognition output multiple levels structural information including sentence boundaries filled pause discourse marker words edit disfluencies 
construct evaluate algorithms automatically detect structural event types 
note problem sentence boundary detection differs analog text processing called sentence splitting sentence boundary detection 
goal sentence splitting task identify sentence boundaries written text punctuation available problem effectively reduced deciding symbols potentially denote sentence boundaries 

sentence splitting problem deterministic punctuation symbols occur sentences 
example watch final period denotes sentence 
sentence boundary detection task speech punctuation available availability speech provides additional useful information 
investigate structural event detection corpora broadcast news conversational telephone speech 
broadcast news comprises read speech formal interviews man street interviews spontaneous speech usually conversational 
contrast telephone conversational speech spontaneous quite informal 
broadcast news usually fewer edit disfluencies spontaneous conversational speech may caused reading errors 
algorithms evaluated human transcriptions recognition output investigate effect incorrect words asr output system performance 
approach problem framework current speech recognition systems find word sequence speech signal 
hidden structure utterance sentence boundaries disfluencies explicit acoustic signal hard integrate problem structural event detection word recognition current speech recognition systems 
address problem post processing approach generates structural informa tion recognition results available 
knowledge sources employed involving textual information prosodic cues reduce ambiguity inherent knowledge source 
shows diagram approach final output rich transcription cleaned transcription 
shows prosodic information obtained combination speech signal recognition output provide word phone alignments 
investigations textual information obtained word strings transcriptions generated human transcriber asr system 
type information doubt important 
cases people problem inferring appropriate structural events word transcriptions 
textual cues quite useful automatic identification structural events example words start new sentence repeated revised word string signals disfluencies 
addition syntactic semantic information derived words provides valuable cues structural event detection 
implicit prosodic cues boundary points described chapter 
speech signal extract prosodic features prosodic features asr structural event detection systems structural event output asr transcription extract textual features process transcriptions textural features rich cleaned transcription fig 

flow diagram automatic structural event detection task 
cases textual information may completely disam structural events 
example extracted broadcast news data anne chances ll hear uh substance president prior vote possible step purely textual model able determine second sentence statement question 
rising tone speech signal enable listener determine question intended 
face high word error rates word level information may unreliable possibly misleading 
case lexical syntactic semantic patterns detecting sentence boundaries disfluencies reliable due word errors 
example compares asr output human transcription speech asr output tackle stuff human transcription uh uh tackled stuff difficult impossible word language model identify repetition existing disfluencies asr output 
prosody rhythm melody speech important automating rich transcription 
past research results suggest speakers prosody impose structure spontaneous read speech 
examples prosodic indicators include pause duration change pitch range amplitude global pitch declination melody boundary tone distribution vowel duration lengthening speaking rate variation 
features provide information complementary word sequence provide additional potentially valuable source infor mation structural event detection 
additionally may robust textual features word errors may provide reliable knowledge source 
textual prosodic knowledge sources exploited previous re search combination proven beneficial formance structural event detection 
thesis builds prior combined knowledge sources hidden markov model hmm approach 
focus developing richer feature set knowledge sources building effective models capture information integrating various knowledge sources structural event detection different modeling approaches 
investigations thesis help answer questions respect automatic detection structural events knowledge sources helpful 
best modeling approach combining different knowledge sources 
model performance affected various factors corpora transcriptions event types 

related past decade substantial amount research conducted areas detecting intonational linguistic boundaries conversational speech detecting correcting speech disfluencies 
chapter introduce research related automatic detection different structural events sentence boundaries edit disfluencies filler words 
type related research categorized knowledge sources 
additionally completeness studies linguistics psychology discussed appropriate 
sentence boundary detection speech recognition sentences usually defined acoustic segment boundaries correspond long stretches silence change tional turn 
contrast linguistic segment boundaries mark unit represents complete idea may necessarily represent grammatical sentence long silence turn change 
experiments meteer iyer suggest language model perplexity reduced working linguistic segments acoustic segments 
goal automatically find linguistic sentence units 
previous research focused detecting major sentence bound aries investigated detecting subtypes sentences questions state ments 
prior research related sentence subtype detection divided definition turn varies literature 
thesis turn portion speech uttered single speaker bounded silence speaker 
see secure ldc upenn edu intranet annotation mde guidelines control floor shtml details 
definition sentence varies past research efforts 
term thesis defined chapter 
categories knowledge sources employed text approach approach textual acoustic information 
text approach uses textual information suitable transcribed speech writ text 
text methods may able resolve ambiguities information text example section question type detected rising tone 
combination approach uses acoustic cues textual information 
cases difficult compare results prior research differ corpora training testing information systems 
text processing sentence boundary detection mentioned chapter sentence boundary detection problem written text aims disambiguate punctuation marks goal identifying sentence boundaries 
palmer hirst developed efficient automatic sentence bound ary labeling algorithm uses part speech pos probabilities tokens surrounding punctuation mark input feed forward neural network obtain role punctuation mark 
sentence boundaries available part speech tagger prior probabilities parts speech word 
tested system portion wall street jour nal wsj corpus 
experiments context surrounding tokens hidden layer units yielded best accuracy test set 
training testing conducted texts lower case format net able disambiguate boundaries 
approaches investigate problem example reynar ratnaparkhi maximum entropy algorithm schmid employed unsupervised learning method 
walker compared different methods sentence boundary detection preprocessing step machine translation 
showed maximum entropy method outperforms systems direct model rule system 
argued high recall important application machine translation fragmenting sentences better combining sentences 
insight useful going structural event detection results downstream language processing modules machine translation 
sentence boundary problem text processing different speech processing punctuation information available text deterministic 
knowledge obtained task useful automatic sentence boundary detection speech lexical cues effective determining role punctuation 
automatic punctuation system called lexical information developed beeferman 
counted oc punctuation mark tokens wsj corpus reported tokens corpus punctuation commas periods 
generates commas sentence boundaries provided pre determined 
extended language model account punctuation explicitly including commas gram lm allowing commas occur interword boundaries 
commas added testing word strings finding best hypothesis viterbi algorithm 
evaluated method generating commas sentences penn treebank wsj corpus stripped punctuation marks 
obtained recall rate precision comma generation task 
goal research differs sentence boundary detection speech task find commas assuming major sentence boundaries known 
beeferman claimed punctuation aware language model applied rescore speech recognition lattices general evaluate 
stevenson gaizauskas conducted experiments identifying sen tence boundaries transcriptions wsj corpus memory learn ing mbl algorithm 
word boundary obtained feature vector elements word neighboring words including probability word starting sentence pos tags 
precision recall approach case information word removed 
results improved case information provided sentence boundary detection system 
clearly case information important method suggesting may extend asr outputs capture case information contain incorrect words 
combining textual prosodic information sentence bound ary detection past research conducted combining prosodic information textual information find sentence boundaries subtypes speech 
known strong correspondence discourse structure prosodic information 
comparison syntactic prosodic phrasing 
study syntactic structure generated ab ney chunk parser prosodic structure tobi label files 
showed syntactic boundaries prosodic boundaries read speech 
chen proposed method combining speech recognition punctuation generation acoustic lexical information business letter corpus 
punctuation marks treated words dictionary acoustic baseforms silence breath non speech sounds language model mod ified include punctuation 
chen pauses correspond punctuation marks punctuation marks correspond pauses 
finding suggests pauses closely related punctuation read speech 
chen conducted speech recognition automatic punctuation ex periment business letter words read aloud speakers 
different testing conditions chen reported result accuracy ation placement lower accuracy correct identification punctuation types 
result apply conversational speech larger corpus unknown 
sentence boundary recognizer textual information pause duration developed gotoh renals 
interword boundary decision sentence boundary 
algorithm finds sequence sentence boundary classes speech recognition output combining probabilities language model pause duration model 
conducted sentence boundary experiments hours broadcast news corpus acoustic duration models trained hours acoustic data language model trained words 
word error rate wer test set 
obtained recall rate precision rate sentence boundary detection 
study pause duration model performs accurately gram language model sentence boundary detection 
possibly language model suffers lot word errors recognition output 
result improved combining information sources 
shriberg stolcke colleagues built general hmm framework combining lexical prosodic cues tagging speech various kinds hidden structural information including sentence boundaries disfluencies topic boundaries dialogue acts emotion 
experimental results shown combination prosody model language models generally performs better knowledge source 
shriberg directly compared corpora switchboard broad cast news task sentence segmentation 
experiments conducted human transcriptions speech recognition outputs compare dation prosody model lm face asr errors 
extracted prosodic features pause phone rhyme duration features non prosodic features turn change gender 
features inputs decision tree model predicted appropriate segment boundary type interword boundary 
investigated performance prosody model statistical lm captures lexical correlations segment boundaries combination models 
broadcast news prosodic model performed better word statistical lm human transcriptions recognized words 
prosody model degraded face recognition errors 
furthermore tasks corpora obtained significant improvement word models combining models 
analysis decision trees revealed prosody model captures language independent boundary indicators pre boundary length boundary tones pitch resets 
addition feature usage corpus dependent 
pause features heavily corpora duration cues dominated switchboard conversational speech pitch informative feature broadcast news 
kim woodland combined prosodic lexical information system designed identify full stops question marks commas broadcast news 
approach similar shriberg 
prosodic decision tree tested combination language model improvements reported combined model 
christensen investigated different approaches automatically identify punctuation broadcast news corpus 
finite state approach com linguistic model prosody model significantly reduced detection error rate increased related precision recall measures especially pause duration 
showed prosodic features pause duration increased detection accuracy full stops little impact detecting types punctuation marks 
second approach multi layer ception mlp model prosodic features 
approach provides insight relationship individual prosodic features various tion marks 
results confirmed pause duration features useful features finding full stops 
huang zweig developed maximum entropy method add punc period comma question mark transcriptions switchboard corpus 
features models involve neighboring words tags punc marks associated previous words pause features 
evalu ated approach transcription speech recognition output 
performance measured precision recall measure 
results showed performance varies different punctuation marks adding bigram type features features previous current position current position improves measure unigram information 
noticed adding pause information yields small gain contrast results reported broadcast news speech 
attributed different data sets suboptimal pause information maximum entropy approach 
observed comma hard distinguish punctuation question mark confusable period 
approach provides framework designing additional features 
maximum entropy approach investigated chapter 
nist sentence boundary detection evaluation systems prosodic textual features sentence boundary detection 
ap proaches similar hmm approach 
example system estimated likelihood classes complete sentence incomplete sen tence non sentence 
acoustic prosodic features estimated word boundary including pause speaking rate energy pitch features 
prosodic features train layer neural network 
linguistic subsystem trigram lm sentence tokens inserted words 
com decoder likelihood sentence classes acoustic prosodic subsystem likelihood linguistic system viterbi gorithm find class hypothesis word boundary 
system decision tree predict classes complete sentence incomplete sentence interruption point edit disfluencies non event boundary 
prosodic features provided decision tree similar ones described 
addition posterior probability lms included feature decision tree 
systems combined layer neural network uses minimum square error back propagation algorithm hypothesize binary score word boundary 
systems evaluated tional telephone speech cts broadcast news speech bn human transcriptions speech recognition output 
relies prosodic information finding sentence units 
wang narayanan developed method prosodic features pitch features multi pass approach 
word phone alignment avoid speech recognizer 
fit pitch contour linear folds search major breaks pitch contour 
second pass sentence boundaries detected pre defined rules statistics 
evaluated algorithm subset switchboard corpus obtained false alarm rate rate 
result encouraging pitch information 
conversational speech pitch may effective feature sentence boundary detection 
clearly expect adding additional prosodic textual information may yield improvement 
summary past research sentence boundary detection finding sentence units subtypes transcriptions read able aiding downstream language processing modules typically expect sentence segments 
previous shown lexical cues valuable knowledge source determining punctuation roles detecting sentence boundaries prosody provides additional important information spo ken language processing 
useful prosodic features include pause word lengthening pitch patterns 
past experiments show detecting sentence boundaries relatively easier reliably determining sentence subtypes sentence internal breaks commas 
poor performance sentence internal structure detec tion affects downstream processing parsing 
table summarizes important attributes previous research 
textual information statistical lm employing machine learning strategies 
value adding syntactic information task sentence detection open question 
approaches listed rows simi lar approach taken thesis textual prosodic information combined sentence boundary detection 
edit disfluency processing disfluencies investigated variety approaches 
linguists psychologists considered disfluencies largely production perception standpoint computational linguists concerned recog disfluencies improving machine recognition spontaneous speech 
main focus believe better understanding underlying theory disfluency production effect listeners comprehension help construct better model automatic detection disfluencies briefly discuss studies psychology linguistics 
production properties disfluencies disfluency production disfluencies common spontaneous speech 
speakers formulate entire utterance change minds saying may suspend speech introduce pause filler table summary important prior studies sentence boundary detection 
column task chosen investigation boundary means sentence boundary detection task compared subtype punctuation detection column describes model information sources investigation column corpus experiments conducted column represents experiments performed human transcriptions ref recognition results asr 
note cts conversational telephone speech corpus column experiments conducted switchboard corpus 
textual information automatic detection model ref condition study evaluation 
investigation classification task model corpus ref asr shriberg boundary prosody word lm cts bn ref asr gotoh renal boundary pause word lm bn asr kim woodland punctuation prosody word lm bn ref huang zweig punctuation maxent word pause cts ref asr nist eval systems boundary prosody word lm cts bn ref asr beeferman commas boundary word lm wsj ref stevenson gaizauskas boundary mbl word pos wsj ref chen punctuation punctuation token business letter asr acoustic information wang narayanan boundary pitch cts ref continuing add delete replace words produced 
spontaneous speech systematically shaped problems speakers encounter planning utterance accessing lexical items articulating speech plan 
speech errors disfluencies produced normal speakers studied decades learn linguistic production cognitive processes speech planning 
disfluency evidence cognitive load speech planning 
att shriberg shown different types task oriented conversations long utterances higher disfluency rate short ones 
effect may related planning load utterance speakers difficulty planning longer utterances making task oriented plans time 
observation disfluencies occur frequently utterance utterance early planning stage providing evidence impact utterance planning disfluencies 
clark studied phenomenon repeated words spontaneous speech 
repeats divided stages initial commitment suspension speech restart constituent 
stages cor respond components reparandum interruption editing term correction laid chapter edit disfluencies 
pro posed commit restore model repeated words hypotheses account repeats complexity hypothesis continuity hypoth commitment hypothesis 
hypothesize complex constituent speakers suspend initial commitment complexity hypothesis speakers prefer produce constituents continuous delivery continuity hypothesis speakers preliminary commitment constituents expecting suspend afterward commitment hypothesis 
analyzed repeated articles pronouns large corpora switchboard corpus london lund corpus strong empirical evidence support proposed commit restore model see description corpus 
evidence hypotheses 
noticed speakers premature commitment immediately suspend constituent complex speakers restart constituent suspension disrupts utterance 
example frequent occurrence function words repeats 
long recognized english function words repeated far content words 
speakers want initial commitment constituent word com function word 
clark function words repeated times content words versus switchboard corpus 
frequent occurrence function words repeats explained hypotheses proposed 
knowing types words speakers tend repeat revise helpful building better model spontaneous speech 
example speakers repair content word return major constituent boundary friday mean monday 
observation beneficial defining disfluency patterns aid automatically identifying 
effect listeners valuable understand human listeners cope input 
studies bard shown listeners generally disfluencies incorrectly report occurrence disfluencies suggesting disfluencies may filtered utterance comprehension 
believe disfluencies play specific roles communication sending sig listener things pay attention help speaker find word patient speaker gathers thoughts 
disfluencies provide formation enables people conversation better coordinate interaction manage turn 
brennan investigated comprehension affected listeners hear dis fluent speech 
experiments listeners followed fluent instructions selection object graphical display 
listeners fewer errors hearing misleading information interruption points disfluencies 
observed mid word interruptions better signals word interruptions word produced error speaker intends replace 
supports levelt hypothesis interrupting word speaker signals addressee word error 
word completed speaker intends listeners interpret correctly delivered 
brennan experiments information disfluencies partially compensates disruption listeners meet processing ent speech 
fox tree studied naturally occurring speech disfluencies affect listeners comprehension 
observed disfluencies negative effect comprehension 
example repetitions hinder listeners help listeners recover information missing occurrence words repeated 
take longer identify words false start 
false starts utterances listeners may abort false starts cost comprehension 
false starts middle utterances listeners false start begins abort attach restarted information 
process slows comprehension 
disfluency rates conservative estimate excluding silent rate disfluencies spontaneous speech approximately words words 
variety factors may influence disfluency rate 
speech disfluencies include fillers 
disfluency rates vary different corpora 
oviatt people talking telephone produced disfluencies talking face face disfluencies words 
shriberg disfluency rate lower speech directed machines 
differences disfluency rates conversations conducted different media attributed resources media may offer coordination 
example eye contact visual cues gesture available signal things intention continue speaking difficulty utterance progress 
disfluency rate affected speaker age gender familiarity conversational partners 
shriberg showed men produced relatively fillers women similar disfluency rates disfluency types 
fillers may provide way men maintain floor 
compared speech produced children adults elderly people hungarian observe differences frequency different types disfluencies children generated disfluencies words groups 
males females differ respect rate disfluencies women generating 
examined factors may affect disfluency rates ing corpus task oriented conversations 
factors included speakers ages gender task roles difficulty topic domain relationship speak ers 
older speakers produced slightly higher disfluency rates young middle aged speakers 
disfluency rates higher speakers acted directors discussed figures confirming disfluencies associated increased difficulty planning speech 
fillers dis tributed somewhat differently repeats restarts suggesting fillers may consequence interpersonal coordination 
past research automatic disfluency detection sentence boundary detection research automatic disfluency detection conducted text approach approach combining textual prosodic information 
difficult compare results prior data sets different disfluencies mean different things 
text processing bear proposed stage speech repair processing method 
stage simple pattern matcher uses lexical pattern matching rules retrieve candidate repair utterances 
done finding identical sequences words pre specified simple syntactic anomalies 
sentences atis corpus algorithm hypothesized containing repair algorithm appropriate correction 
repair candidates constitute useful input processing sources information syntactic semantic information 
second step natural language processing system distinguish repairs false positives parsing sentence parsing localized word sequences identified potential repairs avoid effect due factors unrelated portion repair 
bear claimed acoustic information quite effective combined sources information noting acoustic differences true speech repairs false positives 
stage approach bear promotes important idea automatic repair processing robust integrating knowledge multiple sources 
lexical pattern matching approach computationally tractable provides reasonable coverage repair types 
method detect speech repairs predefined patterns 
weakness approach lies conceptualization repair types 
difficult systematically extend pattern definition increase coverage system difficulty listing possible repair patterns 
addition experiments conducted atis corpus template switchboard conversational speech amenable pattern approach 
charniak johnson conducted speech repair detection parsing switch board sentences 
classifier predict edited words dum region edit disfluency features pos tags pre word word word token appears rough copy identifies repeated sequences words repairs 
goal minimizing misclassification greedy boosting algorithm classification 
switchboard corpus words edited words obtained misclassification rate precision recall edited words 
detecting removing edited words statistical parser parses remaining words utterance 
parser achieved precision rate recall cleaned utterances parsing results reported original utterance 
method bear uses parser help detect repairs charniak johnson feed information parser back help detect repairs 
edited word detection assumption edited words relatively shallow phenomena detected repeated words pos tags information provided parser critical 
belief variety knowledge sources critical accurate edited word detection including syntactic information 
johnson charniak proposed new noisy channel model speech repair detection 
tree adjoining grammar tag represent tree structured dependencies reparandum correction regions 
source model language model word gram syntactic parser lm describes clean sentence contain reparandum 
tag channel model defines conditional probability surface sentence distributions estimated probability disfluency word probability distribution editing term probability disfluency type word reparandum correction region probability word mi correction region word inserted substituted word ri reparandum 
distributions estimated alignment reparandum correction regions training set 
experiments conducted transcriptions switchboard corpus 
testing partial words punctuation removed data reflect realistic testing situation speech recognition output assuming word errors 
precision rate recall obtained parser language model 
result significantly better obtained word word classifier 
main reason tag channel model model cross dependency reparandum correction disfluency 
account majority disfluencies conversational speech modeled simpler methods syntactic parsers 
algorithm handle restart disfluencies correction part complex disfluencies correction reparandum disfluency 
core schubert proposed framework handling speech contains repairs 
assuming pre parser repair identification performed parser grammar knowledge syntactic structure input correct errors repair identification results 
trains corpus core schubert obtained increase recall rate speech repairs precision dropped 
approach similar experiment bear focus reducing false alarms 
core schubert argue goal increase recall rate speech repair detection obtained trying alternatives lower parsing probabilities parse drop precision worthwhile tradeoff parser forced accept posited repairs merely option pursuing alternatives include 
argument acceptable rich transcriptions word sequence hypothesized disfluency information input language processing module goal yield cleaned transcriptions false alarms low precision remove fluent utterances reduce important information 
parser core schubert requires speech repair formation input parser syntactic knowledge correct speech repair identification errors input 
asr output contains incorrect words parser may robust 
evaluation approach needed 
zechner word approach disfluency sentence bound ary detection dialogue summarization system 
pos tagger includes addition standard switchboard treebank tag set tags dis fluent regions special purpose words including ordinating conjunctions discourse markers know editing terms speech repairs mean filled pauses 
decision tree determines linguistically motivated sentence boundaries turn turns speaker 
best decision tree yielded error rate sentence boundary detection 
disfluency detection simple repetition detection script find repeated sequences words 
shallow chunk parser support decision tree detection false starts 
zechner evaluated false start detection sentences switchboard corpus human transcriptions obtained score false start detection non false start detection 
memory learning algorithm detect cies dutch 
definition disfluencies fairly broad study including fit tree structure sentence 
man built syntactic tree utterance tag detects sentence boundaries postponed description discuss sentence boundary disfluency detection results 
word inside outside disfluency obtained 
features include lexical features word word context overlap windows pos tags 
infrequent unknown words processed attenuation method 
accuracy measure obtained small corpus dutch spontaneous speech transcriptions 
stolcke shriberg heeman allen extended tradi tional gram language model deal sentences include repairs 
cke shriberg incorporated disfluency resolution word language model assumptions probability estimates words disfluency accurate conditioned intended fluent word sequence dis modeled word events having probability conditioned context 
predicting word summed probability distributions type repair including repair 
hypotheses include repair prediction word cleaned represen tation context account single double word repetition predicted 
switchboard corpus model reduced word perplexity neighborhood disfluency events differences small significant impact recognition accuracy 
goal disfluency lm improve speech recognition accuracy better modeling occurrence disfluencies spontaneous speech 
heeman allen proposed statistical language model includes identification pos tags discourse markers speech repairs intonational phrases 
example tightly coupled lm shown equation 
acoustic signal represent word sequence pos sequence repair annotation sequence editing term sequence intonational phrase sequence respectively 
speech recognition problem redefined goal find sequence words corresponding pos tags intonation editing terms repair tags probable acoustic signal 
arg max arg max deri second term language model probability rewritten follows 
pr nd nr ne ni ii ei ri ii di ii wi ir ii equations extended include information correction repairs 
experimental results show extended lm applied human transcriptions trains corpus able identify turn internal boundaries precision discourse markers precision detecting correcting repairs precision 
attribute results lm accounts interaction tasks identifying intonational phrases discourse markers pos tags detecting correcting speech repairs 
heeman allen pointed prosody model integrated lm approach shown equation 
current speech recognizers rely lms resolving acoustic ambiguity 
dis handled specially captured gram word sequence 
lms ability model disfluencies promising 
stolcke shriberg lm increases perplexity word error rate switchboard corpus 
claimed reason disfluencies inherently local phenomena modeled surprisingly standard grams context cleanup 
attributed results treatment filled pauses utterance medial filled pauses cleaned fore predicting word utterance initial ones left intact 
heeman allen lm allows speech recognizer model aspects speaker utterances addition words generates structural information speaker turn processing 
model able capture interactions exist word prediction variety phenomena dialogue 
joint modeling words tags lm increase sparse data problem 
lm needs trained annotated train ing data size generally smaller text corpus train traditional word lm 
important note heeman allen tightly coupled lm evaluated trains corpus constrained grammar general spontaneous conversational speech relatively eas ier model 
additionally investigation performance tightly coupled lm asr output remains tested 
combining textual prosodic information disfluency processing text approaches largely left open question exist effective acoustic prosodic cues repairs 
different studies conducted attempt answer question 
studies linguistics suggest prosodic emphasis utilized human perception disfluencies 
fox tree listeners reliably detect edits occurred provided speech edited remove false starts repetitions implying prosodic cues perceptible 
prosodic cues may exist comparing reparandum correction regions 
levelt cutler repairs involving erroneous words emphasized 
cutler different corpus speech repairs emphasized 
repairs restructure speech necessarily lexical changes emphasized fact marked 
repairs involving phonetic errors emphasized 
studies linguists focused prosodic emphasis corrections disfluencies 
additional acoustic properties remain investigated 
listeners detect disfluency point interruption point speech stream 
study bard word gating paradigm discover information necessary detecting disfluency 
nearly disfluencies corpus detectable word gate correction lexical access word occurred 
results suggest acoustic cues interruption points insufficient detect disfluencies accessing information interruption point necessary 
hindle originally suggested edit signal serves cue fluent speech interrupted 
evidence single cue corpus studies combinations cues algorithms identify disfluencies reasonable success 
potentially useful prosodic cues include reparandum especially vowel final fragments silence duration lengthening differences interruption point presence similar contours reparandum correction juncture properties offset reparandum onset correction differ fluent boundaries due lack coarticulation 
nakatani hirschberg proposed speech repairs detected speech model acoustic prosodic cues relying word tran scription 
developed repair interval model provide general model stimuli chunks increase word time 
temporal intervals comprise repair explored variety acoustic prosodic signals associated regions intervals 
hand transcribed prosodic acoustic features silence duration energy pitch traditional text cues presence word fragments filled pauses word matches word replacement pos tags position word turn 
decision tree built carry classification 
obtained detection recall rate precision portion atis corpus 
note corpus contained word fragments training testing included turns speech repairs findings seen indicative relative importance various predictors speech repair location true test repair site location 
examined acoustic aspects false starts conducted experiments automatic identification 
disfluencies oc middle utterance accompanied silent pauses ms minority disfluencies occurred syllables utterance variable amount pausing 
acoustic analysis showed word repeated cases virtually prosodic features instances number times repeated word shorter duration lower pitch 
revision sec ond instance greater amount stress 
conducted automatic detection experiments atis corpus 
simple rule pause ms disfluency disfluencies correctly identified false alarms 
spectral analysis exploited repetition detection results described better specific numbers reported 
approach similar sentence boundary detection shriberg stolcke conducted experiments disfluency detection prosody model language model combination switchboard cor false starts edit disfluencies 
pus 
prosody model features pause length performed significantly better chance performance outperformed language model detecting false starts 
experiments evaluated downsampled data research true test set 
non downsampled switchboard data stolcke reported results accuracy event detection including sentence boundaries fillers edit disfluencies 
prosody model significantly better chance combination prosody model lm outperforms model 
system performs better sentence boundary detection disfluencies sentence boundaries frequent disfluencies result represent system performance disfluency detection 
transformation learning tbl detection disfluencies assumption majority disfluencies detected lexical features prosodic cues 
tbl rules learned features including lexeme pos tags word followed pause lexeme average speaker 
simple prosodic features algorithm pause information word 
approach applied transcriptions recognition output cts corpus nist disfluency detection task 
results worse systems evaluation prosodic textual knowledge sources 
methods model textual information different systems hard say poorer performance due limited prosodic features system 
summary past research disfluencies studies linguistics psychology shown disfluencies play important role speaker utterance planning discourse disfluencies repetitions detrimental listeners comprehension 
disfluencies random phenomena measurable patterns exist disfluencies 
understanding possible disfluency patterns help build better automatic disfluency detection model 
prior research shown progress automatic disfluency detection ing prosodic cues statistical lms syntactic structure information 
related research disfluency detection summarized table 
worth pointing prior sentence boundary information available ency detection relatively easier task compared accessing sentence boundary information 
notice research conducted ing transcriptions 
research disfluency detection early stage 
clearly investigations transcriptions provide useful ideas better modeling spontaneous speech testing asr output presence incorrect words create serious problems infor mation extracted words syntactic level reliable 
suggests need develop better understanding interaction disfluency detection speech recognition 
filler word processing filler words treated category structural event detection tasks fore put prior related filler word processing separate section included disfluencies prior studies disfluencies discussed section 
production perception fillers filled pauses fps defined hesitation flow utter ance 
filled pause may occur speaker needs think say 
speaker interrupts speech continuing articulation 
articulation considered word people 
fps table summary important prior studies disfluency detection 
column investigation column describes model information sources investigation column corpus experiments conducted column represents experiments performed human transcriptions ref recognition results asr 
core preliminary repair information provided parser corrects 
investigation tasks model corpus ref asr shriberg stolcke ip detection prosody word lm cts ref asr bear repairs disfluency pattern parser atis ref charniak johnson repairs textual information parser cts ref core repairs parser trains ref nakatani hirschberg repairs prosody textual information atis ref heeman allen repairs lm trains ref structural information repairs ip lexical features tbl cts ref asr serve important purpose helping speaker hold conversational floor 
speaker appears finished idea wishes continue speaking subsequent utterance prepared fp may uttered order keep control conversational floor 
note word lengthening plays similar role filled pause phenomenon categorized filled pause research 
pointed previously chapter filled pauses discourse markers act editing term disfluency 
example uh know 
levelt repairs corpus spontaneous task oriented utterances included type editing expressions uh common 
different perspective fox tree suggestive evidence fillers meaning fps research affect comprehension fillers um uh affect listeners comprehension differently 
help possibly providing information meta communicative process directing listeners attention upcoming phrase 
contrast effect word recognition effects masked pausing effects 
brennan showed experiments interruptions marked filler better error signals interruptions 
follows levelt proposal editing expression uh may warn addressee current mes sage replaced 
showed phonological form filler driving fast comprehension extra time elapsed corrections due presence fillers 
longer editing interval gives listeners time process evidence trouble listeners able better process disfluencies 
past research filler word processing hmm recognizers regard filled pauses vocabulary words deal subword unit decoder processing unknown words 
currently recognition systems include filled pauses vocabulary build acoustic models making filled pause detection subsequent structural event detection system essentially word spotting task 
goto proposed method detects filled pauses word length basis small fundamental frequency transitions small spectral envelope deformations assumption speakers change tor parameters filled pauses 
recall rate precision achieved japanese spoken dialogue corpus 
siu ostendorf created lm account roles filled pause take utterance initial part repair simply filled pause hold floor 
allowing roles distinguished able reduce perplexity lm 
observed reduction lm perplexity discourse markers know 
perplexity reduction lm transfer word error rate reduction speech recognition currently untested 
filled pauses discourse markers strongly dependent word identity 
simple approach filler word detection lexicon lookup deter mine potential words really fillers especially case discourse markers straightforward problem due role ambiguity 
humans difficulty judging role face ambiguity 
knowl edge dialog needed better model discourse markers 
additionally word errors asr output especially filler words seriously affect detection 
chapter summary described psychologists linguists production disfluencies role fillers effect phenomena listeners comprehension 
believe insights efforts help build better automatic detection models 
analog speech recognition standing speech perceived proven extremely beneficial construction better automatic speech recognition systems 
past shown textual prosodic cues provide important information detection sentence boundaries disfluencies 
potential prosodic cues include pauses word syllable lengthening features interrupted words increased stress correction word versus reparandum word 
note focused prior related detecting sentence boundaries disfluencies introduce prior related prosody 
prosody extensively investigated derive information discourse structure 
speech speech translation system verbmobil project prosody including duration guide rescoring best list word hypotheses produced speech recognizer 
previous sentence boundary disfluency detection conducted human transcriptions 
study human tran shed light potential features useful models final goal enrich speech recognition results contain variety errors detection difficult 
language models machine learning techniques capture textual information effective seriously affected word errors asr outputs 
correct word identity fillers disfluency patterns may matched sentence initial word information may corrupted 
important looking impact speech recognition structural information extraction 
previous research divided different categories target event type imply problem addressed sepa 
example fillers start middle sentence different impacts lm prior studies disfluency detection rely availability sentence boundary information 
take account interactions different structural events 
remains done structural event detection rich transcription speech generate readable recognition output help downstream process ing modules 
approach employed thesis builds shriberg stolcke framework sentence boundary disfluency detection 
described chapter build robust prosody model utilize lexical syntactic information optimally combine different knowledge sources 

data resources tasks currently exist corpora different speaking styles annotated ldc structural events annotation guidelines darpa ears program 
ready availability data possible com pare systems corpora chosen investigations structural event detection 
details event types investigated tasks corpora ears program described chapter 
experiments chapters data annotated annotation guideline version chapter data annotated guideline version 
chapter organized follows 
section describes structural event types speech investigated thesis 
section describes struc tural event detection tasks performance measures tasks 
section describes corpora experiments 
structural speech events types types markups transcription readable example addition speaker turn information 
events thesis structure oriented including identification sentence boundaries fillers edit disfluencies 
type described briefly tions 
table structural events annotated ldc investigated thesis 
note subtype edit disfluency annotated ldc correction edit disfluency 
event type subtype annotation mark statement question su backchannel incomplete filled pause filler discourse marker explicit editing term repeat edit disfluency revision original utterance editing term restart correction complex sentence units sus sentences spontaneous conversational speech quite different written text read speech 
section structural event annotation guideline calls sentence units sus 
sus express speaker complete thought idea 
times unit corresponds sentence times unit semantically complete smaller sentence noun phrase response question 
su boundary usually coincides syntactic clause boundary case 
su entire formed sentence phrase single word 
short sus full sentences clauses complete units 
differences sentences sus term indicate sentence units conver speech 
types sus annotated ldc annotation guideline statement complete su functions declarative statement 
example money big issue going today example chapter page 
short phrases grammatical complete statement su example response question 
question complete su functions interrogative including wh questions 
note speaker statements rising final intonation utterance act question example possible step page 
backchannel called acknowledgment nel word phrase encourages dominant speaker continue talking indicating non dominant speaker listening conversation 
backchannels serve function similar gestures head nod ding 
examples backchannel words include hm hmm right huh sure mm hm oh okay really uh huh 
incomplete su occurs speaker interrupted continue old utterance speaker trails 
example get values know mean example chapter page 
fillers indicated section annotation guideline fillers include filled pauses discourse markers explicit editing terms 
filled pauses fps non lexemes non words speakers employ indicate hesitation maintain control conversation thinking say 
fps occur speech stream 
fps current annotation guideline limited words ah eh uh um 
discourse marker dm word phrase functions primarily struc turing unit spoken language 
frequently appears su 
listener signals speaker intention mark boundary discourse including change dominant speaker new topic 
words dms anyway see basically mean see know see 
third filler type explicit editing term 
editing terms defined filled pauses discourse markers 
example today monday sorry tuesday sorry explicit editing term 
filled pauses discourse markers function editing term edit disfluency example inside brackets uh people marked explicit editing terms 
edit disfluencies edit disfluencies happen people need revise saying 
indicated section annotation guideline edit disfluencies follow basic pattern part described 
examples shown thesis extent edit disfluency marked square brackets 
note annotated data ears program correction part annotated subtypes edit disfluencies information provided examples order better illustrate structure edit disfluency 
edit disfluency template appears embedded sentence original utterance editing term correction original utterance called reparandum portion utterance corrected abandoned entirely case restarts 
portion discarded removing disfluencies cleaned transcription 
shown template original utterance indicated parentheses 
convention example chapter page examples provided section 
interruption point point speaker breaks original utterance repeats revises restarts utterance 
interruption points marked examples 
editing term edit disfluencies include overt statement speaker marking existence 
term consist filled pause discourse marker explicit editing term sorry excuse 
editing term optional edit disfluency 
correction consists portion utterance corrects original utterance 
part remain cleanup transcription 
internal structure edit disfluencies divided follow ing subtypes repetitions speakers repeat part utterance 
example may may high standard living 
revisions content replacements speaker modifies original utter ance similar syntactic structure 
example show flights boston uh denver monday 
restarts false starts speaker abandons utterance constituent starts entirely 
restarts correction region disfluency template typically marked empty 
example live georgia 
complex disfluencies speaker produces series disfluencies succession nested structure 
example think think peo ple generally volunteer 
internal structure nested sequential structure inside complex disfluency annotated guideline internal interruption points marked 
annotation different disfluency annotation switchboard penn treebank data indicates internal structural information disfluencies shown think think people generally volunteer square bracket represents disfluency reparandum correction regions split ips 
nested structure represented annotation scheme 
structural event detection task description focus official rich transcription structural metadata extraction mde tasks defined darpa ears program due availability annotated data system training testing availability scoring tools 
believe methodology developed tasks generalize structural event detection tasks similar speech language processing tasks 
task description rich transcription rt structural mde task includes subtasks described 
su boundary detection goal find point su 
note sus may correspond complete incomplete utterances 
filler word detection goal identify words filled pauses fps discourse markers dms explicit editing terms eets edit disfluencies 
edit word detection goal find words reparandum region edit disfluency 
essentially portion utterance deleted results fluent version utterance 
interruption point ip detection goal find interword location point fluent speech 
addition ips edit disfluencies ips defined rt include boundary filler words 
note convenient divide tasks separate subtasks exam ple scoring easier 
subtasks independent 
fact edit word detection ip detection clearly interdependent 
additionally ambiguities different event types incomplete su versus restart edit disfluency affect multiple subtasks 
tasks types boundary detection extent detection tasks 
su boundary detection ip detection belong boundary detection category filler word edit word detection tasks involve extent detection 
boundary detection task equivalent classification task interword boundary decision structural event position 
extent detection task hand needs determine portion utterance filler word phrase reparandum edit disfluency 
structural event detection evaluated different corpora conversational telephone speech cts broadcast news bn speech 
details corpora provided section chapter 
ears mde evaluation different types transcriptions human generated transcription ref speech recognition output stt 
ref su detection task really extent detection task assumption previous su indicates su ignoring possible pauses simplicity su detection task treated boundary detection task 
erence transcriptions provides best case scenario evaluation structural event detection algorithm 
evaluation transcription types allows study structural event detection tasks confounding effect speech recognition errors 
performance measures tasks evaluated separately 
performance mea sures evaluating system performance nist official scoring metric additional metrics convey various types useful information sys tem performance 
describe metrics thesis describe metrics various experiments 
generally nist scoring metric system performance order compare systems performance choose appropriate metrics focusing specific aspect problem 
nist scoring metric 
nist scoring tools align hypothesis words 
straightforward evaluating human tran match exactly 
recognition output words usually align perfectly transcriptions 
case alignment minimizes word error rate 
word alignment hypothesized structural events mapped events word alignment information unmatched structural events counted 
edit filler word detection error rate av erage number misclassified tokens edit filler word token 
su ip detection error rate number misclassified boundaries su ip 
example equations show nist error rate su detection edit word detection su error rate number incorrect boundaries total number su boundaries edit word error rate number misclassified words total number edit words error rate nist metric greater 
example shows system su hypothesis aligned sus system ins del wi word indicates su boundary 
boundaries insertion error deletion error indicated ins del example 
su boundary nist su error rate system output 
detailed description scoring tool provided www nist gov speech tests rt rt fall 
system hypothesizes non event boundary interword boundary nist error rate boundary detection tasks due deletion errors insertion errors 
baseline performance 
classification error rate cer 
structural event detection problem treated classification problem performance easily measured cer 
cer defined number incorrectly classified samples divided total number samples just positive samples 
measurement samples interword boundaries boundary detection tasks total number words extent detection tasks 
example shown word boundaries misclassified cer 
baseline performance called chance performance cer equal event prior 
nist error rate cer tend highly correlated 
transcription errors nist metric correspond directly classification errors 
major difference lies denominator num ber events nist scoring metric total number word word boundaries cer measure 
recognition output cer defined due presence insertion deletion errors recognized word stream 
cor nist error rate cer condition nist error rate converted proportionately cer stt condition follows cer nist error rate event prior measure 
classification detection task measure defined follows precision measure recall precision recall precision recall tp fp denote num ber true positives false positives respectively 
fn represents number false negatives corresponds relative importance precision versus recall 
set false alarms misses considered equally costly 
measure minority class positive class su ip boundaries filler edit words 
receiver operating characteristics roc area curve auc 
roc curves enable visual judgments trade true positives false positives classification detec tion task 
depending application appropriate operating point roc curve selected 
structural event detection tasks threshold needs selected minimize classification error rate nist error rate 
auc tell randomly chosen majority class example higher majority class membership randomly chosen minority class example provide insight ranking pos itive class examples 
measure roc auc measures condition due imperfect alignment problem recognition output 
currently exist standard tests significance test nist scoring method 
problem metric consistent segment 
cer metric sign test utilized test significance word boundary level 
believe findings sign test transfer methods significance test 
corpora conversational telephone speech cts broadcast news bn structural event detection tasks ears 
believe investigations corpora enhance understanding structural information represented human languages 
cts participants paired computer driven robot operator system sets phone call selects topic discussion predefined set topics records speech separate channels conver complete 
conversation minutes average 
bn contains news broadcasts abc cnn television networks npr pri radio networks 
shows examples human generated transcriptions cts bn respectively 
cts transcription person dialog 
cts bn different genres 
differ average sentence length frequency disfluencies 
speech bn fewer disfluencies sentences tend longer grammatical speakers professionals reading text 
speech cts casual conversational containing backchannels filler words edit disfluencies 
cts bn speaker hi um talk dress um normally type normally wear speaker uh corporate control dress kind nice usually wear winter time slacks guess noise summer just dresses speaker speaker re really sup posed wear speaker speaker really doesn vary season season office kind know temperature top selling car nineteen announced today winner toyota toyota sold honda accord ford number past years wall street today dow jones lost just points close oh nasdaq market stocks lost eighteen half points just ahead possibility peace america iran fig 

examples transcriptions cts bn respectively 
su boundaries shown examples 
data training evaluating structural event detection models taken official nist rt data 
training test data annotated structural events ldc guidelines detailed 
cts data set contains roughly hours speech conversations training hours conversations testing 
bn data contains hours speech training hours shows testing 
shows bn training data shows portion annotated structural events 
table shows class distribution different structural event types corpora data size wer speech recognition output test set 
wer determined recognition output sri recognizer nist evaluation 
table information cts bn corpora including data set sizes percentage different types structural events training set word error rate wer speech recognizer test set 
cts bn training size number words test size number words wer su percentage edit word percentage edit ip percentage filler word percentage filled pause percentage discourse marker percentage note corpora described composed speech annotated tran cts bn training testing structural event detection models 
additional speech data correspond ing transcriptions training speech recognition models 
annotations require effort transcribing speech annotated data size generally smaller size data training acoustic language models speech recognition 
additional text corpora utilized language model training structural event detection tasks described chapter 

hmm approach structural event detection chapter hmm introduced baseline method boundary detection tasks 
approach builds prior shriberg stolcke 
chapter presents general approach specific models different event types discussed chapter 
chapter organized follows 
section provides overview components hmm 
section describes textual prosodic features system 
models constructed knowledge source introduced section model combination described section 
summary chapter appears section 
overview structural event boundary detection task represented classification task interword boundary decision exists structural event position 
extent detection boundary detection approach combined additional knowledge processing 
components statistical boundary detection algorithm 
described detail indicated sections 
feature processing section develop inventory input features statistical classifiers including prosodic features temporal intonational energy features lexical features word occurrence part speech keywords 
model construction section evaluate variety model types capture information various knowledge sources including prosody language models 
component model finely tailored data task 
model combination section integrate selected model types knowl edge sources 
addition hmm integration approach briefly describe integration approaches simple interpolation classifier scores 
feature types prosodic features prosodic features reflect information temporal intonational energy contours 
shows example waveform corresponding pitch energy contour word alignment su boundary information ut um hadn heard word alignment information shown phone level alignment prosodic feature com putation 
duration pitch energy features included prosodic feature set 
features associated interword boundary automatically extracted word phonetic alignments word sequence 
description prosodic features investigated computed 
comprehensive listing features prosodic feature set total 
duration features pause duration word boundary extracted alignment human transcriptions recognition output 
included duration pause preceding word boundary reflect speech fig 

waveform pitch energy contours word alignment su boundaries utterance um hadn heard 
right boundary just starting continuation previous speech 
phone durations computed 
capture length typically affects nucleus coda syllables measure vowel rhyme duration 
example normalized vowel duration calculated follows norm duration vowel mean standard deviation vowel training data 
index represent vowel 
extract features duration vowel stressed vowel word tion 
duration word preceding boundary normalization included duration features 
features autocorrelation pitch tracker get function package calculate frame level estimates 
raw values post processed account tracking errors speaker dependent parameters simplify features 
speaker distribution fitted lognormal tied mixture model ltm mixture weights expectation maximization em algorithm 
model returns pitch baseline value speaker represents lowest non halved pitch value pitch normalization 
median filter applied smooth voicing onsets pitch tracker unreliable 
frame level values stylized simplify tonal contours shapes slopes 
piecewise linear fit algorithm create line estimates median filtered values 
particular voiced region algorithm attempts fit lines minimizing mean squared error linearized pitch estimates raw values greedy algorithm 
picking best fit nodes pitch contour represented summation nodes voiced region bk xk xk ak bk best parameters chosen algorithm node indexed total number nodes voiced region 
shows example raw stylized contour utterance um hadn heard 
seen stylized contour captures pitch contour time eliminating effect imperfect estimation raw 
stylized pitch contour different types features computed 
range features features reflect pitch range single word win dow relative speaker specific baseline value computed ltm model 
examples range features minimum maximum mean pitch hz raw pitch values approximation time raw values approximation fig 

raw stylized contours utterance um hadn heard 
values word boundary excluding values un voiced halved doubled 
features normalized baseline values linear difference log difference log ratio 
expect speak ers fall closer bottom pitch range phrase sentence topic boundary 
movement take measurements stylized contours voiced regions word preceding word boundary 
min imum maximum mean values starting stylized values computed compared word 
log difference log ratio normalization values calculated 
slope features stylized pitch values generate pitch slope word predefined length window 
slope boundary compared capture local pitch variation 
continuous trajectory correlate non boundaries broken trajectory tends indicate boundary type 
energy features speakers tend start utterance taper time 
generate frame level root mean square rms energy values obtained get function package compute minimum maximum mean rms values word voiced frames 
similar stylized processing raw energy values fit linear model capture slope change energy difference energy values word boundary computed 
additional features additional automatically extracted features included turn related features gender features 
prosodic features de scribed features automatically extracted speech data gender detection automatic speaker segmentation cluster ing techniques 
features may interact aforementioned prosodic features features features put prosodic feature category model possible interactions 
turn related features include speaker change boundary time elapsed start turn turn count current conversation 
note gender detection speaker segmentation algorithms accurate additional features imperfect 
textual features textual information represented lexical features word occurrence words word part speech tag semantic class 
discussed chapter words highly correlated backchannels filled pauses discourse markers 
cue words associated event types listed table 
provide important lexical cues structural event detection ambiguous right may contexts backchannels verb discourse marker 
baseline system largely uses word occurrence information particular features words tend precede follow structural event type 
investigations incorporating types lexical features syntactic structure information described chapters 
table examples cue words highly representative structural event types 
models prosody model event type example words backchannel okay right filled pause uh um mm discourse marker mean know goal prosody model structural event detection task de termine class membership word boundary prosodic features 
baseline experiments decision tree classifier serves prosody model estimating posterior probability event type interword boundary 
decision tree classifier offers distinct advantage interpretability 
crucial baseline system helpful ob tain better understanding prosodic features signal various event types select design useful features 
second preliminary studies shown decision tree performs classifiers neural networks bayes classifiers mixture models 
third decision tree classifier handle missing feature values continuous categorical features 
fourth decision tree produce posterior probability estimates easily combined language model 
training decision tree learning algorithm selects single feature highest predictive value reduces entropy classification task question 
leaves tree store probabilities class distribution samples falling corresponding region feature space serve predictors unseen test samples 
various smoothing pruning techniques commonly employed avoid overfitting decision tree model training data 
cart algorithm learning decision trees cost complexity pruning approach implemented ind package 
software offers options handling missing feature values capable processing large amounts training data 
test set decision tree generate posterior probabilities sample representing likelihood class prosodic features 
example decision tree shown 
decision tree created su detection task bn corpus 
features tree described table 
prosodic features decision tree associated word boundary system needs event type decision point word 
pau dur wrd diff lr pattern boundary rr ff fr rf rx fx rhyme dur ph nd bin rhyme dur ph nd bin rhyme dur ph nd bin prev pau dur prev pau dur pattern boundary xf xr lr wrd diff pau dur pau dur turn time diff prev pau dur prev pau dur diff prev pau dur rhyme norm dur ph nd bin rhyme norm dur ph nd bin wrd diff wrd diff wrd diff wrd diff prev pau dur turn time pau dur fig 

example decision tree su detection 
line represents node tree associated question regarding particular prosodic feature class distribution class examples going node stands su boundary non su boundary 
indentation represents level decision tree 
features tree described table 
language model lm role lm speech recognition predict word previous word history structural event detection goal lan guage model capture structural information sus disfluencies pau dur pause duration word point dur bin binned normalized duration vowel word word dur word duration prev pau dur pause duration word str rhyme dur ph bin binned normalized duration stressed rhyme word turn turn change word diff log ratio stylized value word table examples prosodic features su detection problem appear decision tree shown 
fillers contained word sequence 
goal hidden event lm model joint distribution boundary types words hmm hidden variable case boundary type 
represent string spoken words represent sequence interword events hidden event language model describes joint distribution words events wn en 
note word level lm explicit features word word occurrence directly incorporated model 
training hidden event lm hand labeled data event represented additional non word token explicitly included gram lm 
example ip event ip additional token dictionary 
bigram parameter ip gives probability ip word 
note represent fluent intra sentence boundary events explicitly considering implied absence events 
believe choice better captures flow word strings including non event explicitly gram lm avoids fragmenting training data 
hidden event lm utilized label word sequence events hmm 
model word event pairs correspond states words observations transition probabilities hidden event gram model 
word sequence forward backward dynamic programming algorithm compute posterior probability ei event ei position boundary detection task event maximizes posterior probability ei individual boundary 
approach minimizes expected boundary classification error rate 
addition statistical hidden event lm approach keyword language model detecting fillers backchannels see table examples keywords 
lexical cues captured hidden event lm example bigram probability ype high hidden event lm choose keyword models locate structural event types 
model combination prosodic lexical cues provide complementary information dif ferent levels granularity expect combination knowledge sources give superior performance model 
approaches described model integration posterior probability interpolation prosody model decision tree classifier denote dt language model yield posterior probabilities event type ei interword boundary better estimation posterior probability event occurring boundary knowledge sources obtained linearly interpolating posterior probabilities models ei plm ei pdt ei optimized held data 
plm ei pdt ei posterior probabilities generated hidden event lm prosody model respectively word sequence represents prosodic features 
addition combining prosody model word lm posterior probability interpolation method applied models described chapter 
integrated hmm approach integrated hmm models joint distribution word sequence prosodic features hidden event types markov model 
goal approach find event sequence maximizes posterior probability arg max arg max position associated prosodic features fi modeled emissions hidden states ei likelihood fi ei 
assumption prosodic observations conditionally independent event type ei word sequence rewritten follows fi ei additionally prosodic observations depend phonetic alignment wt ignoring word identity may prosodic features robust recognition errors 
equation rewritten phonetic alignment information wt second term fi ei wt estimation fi ei wt obtained decision tree class posterior probabilities pdt ei fi wt follows fi ei wt fi wt pdt ei fi wt ei wt fi wt pdt ei fi wt ei ei wt approximated ei assuming structural event dependent word identity independent word alignment information 
substituting equation equation equation obtain expression event sequence hidden event lm decision tree estimation pdt ei fi wt prior probabilities events ei arg max arg max pdt ei fi wt ei term fi wt numerator equation independent ignored argmax formula appear equation 
remains explain ei fi wt calculated testing 
described earlier decision tree prosody model generate posterior probability test sample 
majority class training apply trees non downsampled test data mismatch class distribution training test set posterior probabilities need adjusted accordingly 
classification problem posterior probability class membership sample expressed bayes theorem ck ck ck wt fi computed contextual information wi 
ck class membership sample training testing sets differ significantly class distribution appropriate bayes theorem necessary corrections posterior probabilities test set 
done dividing output posterior probabilities classifier prior probabilities corresponding training set multiply ing new prior probabilities test set normalizing results 
notice formula derived obtain event se quence 
system forward backward algorithm find event interword location viterbi algorithm determine event sequence 
minimizes boundary classification error rate 
alternatives model combination instance scores lm included directly feature decision tree model results showed approach performs worse hmm described section 
method considered thesis 
chapter summary chapter described baseline hmm approach structural event boundary detection task 
word boundary set prosodic features extracted reflect duration pitch energy information 
decision tree implement prosody model estimates event class membership word boundary prosodic features 
hidden event lm model joint word event sequence 
knowledge sources integrated hmm system 
effective model combination important alternative methods investigated chapter 
distribution test set usually unknown estimated original non downsampled training set 

hmm baseline performance chapter describe baseline system performance detection structural events 
research enhancements described chapters 
interword boundary various knowledge sources determine boundary structural events interest su disfluency interruption points filler word boundaries hmm described chapter 
addition rule knowledge event extent detection filler word edit disfluency detection 
chapter organized follows 
section describes choices classes classifiers boundary detection tasks train ing testing procedures 
section provides hmm baseline results structural event detection tasks 
section summarizes findings 
system description choice classes boundary detection tasks general hmm described chap ter 
remaining problem choice target classes system 
recall investigating structural event detection tasks su detection filler word detection edit word ip detection 
su detection su implies su su boundary words represent su start 
ip corresponds interword boundary selection boundary representation ip task obvious 
filler word detection filler word string class boundary detection framework 
filler words limited word list knowing point string allows go backward determine onset filler word sequence 
reason filler word string chosen prosodic information helpful locating final word filler word sequence 
addition filled pauses fps discourse markers dms distinguished quite different phenomena 
structural event types boundary detection tasks su edit ip dm fp 
needs decided train model learns distinguish event type separate models learn distinguish subset event types 
events shown occur example su filler occur sus ips 
approach adopt baseline system train separate prosody model lm event binary classification su versus non su ip versus non ip dm versus non dm fp versus non fp 
distributions events quite different believe combining single model may degrade quality decision trees masking characteristics minority classes 
addition possible occurrence events increase total number unique classes may fragment training data 
expect models better tailored task implementing structural event detection tasks multiple classification tasks 
drawback approach model exploit fact events occur occur post processing step applied presumably able address problems result separate modeling approach 
dm means discourse marker word sequence 
similarly fp means filled pause 
training procedures data preparation described earlier structural event detection tasks evaluated hu man transcriptions recognition output 
note applying models trained human transcriptions recognition output mismatch training testing 
preliminary experiments showed model trained transcriptions yielded better performance trained recognition output evaluating recognition output testing condition 
transcriptions model training apply resulting models transcription recognition output test conditions 
shows diagram training data obtained lm prosody model 
speech needs segmented shorter waveforms processing 
bn segments provided generated transcribers pause syntactic information 
cts segmentation done automatically 
training data word transcription associated time speech data segmented pause adjacent words greater pre defined threshold length greater seconds cts 
forced alignment step called text normalization performed 
purpose procedure normalize words match vocabulary speech recognizer 
example different backchannel words mapped single token compound words split avoid vocabulary words 
normalized words structural event annotation original transcriptions compose joint sequences words structural events hidden event lm training 
fig 

data preparation model training 
normalized transcriptions events force aligned speech data obtain word phone level alignments 
forced alignments obtained sri large vocabulary speech recognizer 
prosodic features computed alignment information described chapter 
word alignment information provided training data phone level alignment needed prosodic feature computation re alignment conducted 
prosodic features extracted word phone alignments word identity information explicitly features 
phone identity obtain normalized phone duration features 
compute features speaker information needed 
cts straightforward obtain information channel corresponds speaker 
bn automatic clustering obtain pseudo speaker label speech segment 
prosodic feature computation data prosody model training ready vector prosodic features associated word boundary plus structural event type corresponding boundary 
model training described chapter decision tree prosody model 
decision tree learning algorithm inductively biased majority class structural event detection case non event boundaries minority class may modeled 
way classification task baseline system training decision tree prosodic features randomly downsampled training data order allow decision trees learn inherent properties event classes 
downsampling evens distributions creating classes number tokens smallest minority class 
ways address skewed class distribution problem investigated chapter 
joint word event sequence gram word hidden event lm trained kneser ney smoothing 
prosody model training lm training word sequence information preserved sampling 
transcriptions segment associated speaker label 
chose information testing information unavailable mismatch training testing speaker information training 
testing procedures testing steps shows steps testing structural event detection models 
steps apply human transcriptions recognition output 
cases word alignment information available 
evaluating human transcriptions test data provided nist contains word alignment information 
evaluating recognition words alignment information available recognizer output 
step testing briefly described 
transcription word alignment information segment speech conversation cts show bn 
done finding pause longer predefined threshold second experiments adjacent words 
segment speech forced alignment performed correspond ing word transcription obtain detailed phone word level alignments prosodic feature computation 
step speaker label added segment speech 
cts straightforward channel corresponds speech speaker 
bn automatic speaker labeling performed 
prosodic features computed forced alignments speaker infor mation 
prosody model lm combined hmm obtain final structural event hypotheses 
evaluation conditions achieve best system performance prosody model lm combined hmm 
model evaluated individually order understand individual contributions 
evaluation conditions investigated different automatic speaker labeling methods discussed chapter 
transcription word alignment speech conversation show segmentation segmented waveform corresponding transcriptions word phone level alignments word sequence hidden event lm prosody model forced alignment prosodic feature extraction classification prosodic textural information system hypotheses structural events speaker information prosodic features speaker labeling fig 

system flow diagram testing procedure 
prosody model decision tree prosody model estimates posterior probability event prosodic features word boundary 
decision trees trained balanced training set test ing prosody model non downsampled test data posterior probabilities generated decision trees adjusted 
assume prior probabilities classes posterior prob abilities decision tree pdt pdt class ci adjusted posterior probability calculated follows ci pdt ci ci pdt pdt lm word boundary hidden event lm compute posterior probability event ei forward backward algorithm 
combination lm prosody model hmm inte grate prosody model word hidden event lm described chapter 
downsampled balanced training set prosody model training non downsampled test set posterior probabil ities adjusted account mismatch decision tree training testing follows ei fi wt ei fi wt ei substituting equation event sequence combined prosody model lm obtained posterior probabilities directly decision tree arg max ei fi wt posterior probabilities generated word boundary condi tions lm prosody model hmm combination approach 
threshold generate final decisions binary structural event detection tasks class posterior probability greater chosen 
goal minimize classification error rate errors associated class event vs non event assumed equally costly 
baseline system performance prosody model lm combination evaluated structural event detection tasks human transcription recognition output cts bn corpora 
experiments subtask described sub sections 
task su detection setup way classification task non su boundaries distinguished su boundaries 
model training testing su subtypes distinguished 
su class includes statements questions backchannels incomplete utterances 
boundaries grouped class non su 
training test data rt evaluation described chapter see table 
decision tree prosody model trained downsampled training set 
evaluation conducted human tions ref recognition outputs stt lm prosody model individually combination 
obtain baseline performance su detection task nist error rate allows comparisons systems data 
results reported classification error rate cer examine boundary detection performance 
su detection results cts table shows su detection results cts 
shown table lm performs better prosody model model combination outperforms model 
word lm generalize unseen cases accurately detect su boundaries word context occurred training data 
hand combining prosody model better generalizing unseen test conditions 
seen table shows deletion insertion error rate lm prosody combination transcription condition 
model combination case fewer missed su boundaries deletion errors increase false alarms insertion errors compared lm 
table cts su detection results nist su error rate boundary cer parentheses human transcriptions ref recognition output stt lm prosody model individually combination 
baseline error rate assuming su boundary word boundary nist su error rate cer 
cts lm prosody lm prosody ref stt table deletion insertion error rates nist su error rate cts ref condition lm prosody combination 
cts del ins total lm prosody lm prosody word errors recognition output negative impact prosody model lm lm severely affected greater relative su detection error increase stt condition lm prosody model 
lm dependent lexical information prosody model follows robust face word errors 
prosody model indirectly impacted prosodic features extracted word alignments transcriptions containing word errors accurate human transcriptions 
addition incorrect phones stt output affect prosodic feature extraction example impacts normalization phone duration uses phone identity 
analysis decision tree created training highlights prosodic features su boundary detection 
table reports feature usage su task 
feature usage reflects percentage times decisions involve certain feature classifying training samples 
features higher decision tree higher usage values 
features feature usage value greater listed table 
descriptions feature abbreviations appendix seen duration extremely important su detection cts data including pause duration phone level vowel duration 
useful feature turn change cases implies speaker finished utterance signaling su boundary 
su detection results bn su detection results bn corpus shown table 
similarly cts results combination lm prosody model yields better formance model performance degradation face speech recognition errors 
contribution prosody model combined performance bn greater cts table feature usage su detection cts 
feature feature usage pau dur word dur prev pau dur max vowel dur nsp turn time turn error rate reduction combining prosody model bn compared cts condition 
table shows deletion insertion errors condition 
similar cts results adding prosody model yields fewer deletion errors increases insertion errors 
table bn su detection results nist su error rate cer parentheses prosody model lm combination 
results shown ref stt conditions 
baseline error rate nist su error rate cer 
bn lm prosody lm prosody ref stt recall table smaller percentage sus bn cts 
number sus denominator nist error rate calculation number misclassified boundaries result higher table deletion insertion error rates nist su error rate bn ref condition lm prosody model combination 
bn del ins total lm prosody lm prosody nist error rate bn cts 
partly explains nist su error rate generally higher bn cts 
boundary cer lower bn cts simple baseline performance lower bn cts 
comparing su detection results bn cts table table notice performance lm worse bn cts 
reasons 
training data size smaller bn cts 
second corpora differ speaking style 
conversational speech person pronouns backchannel words signals su boundaries bn sentence initial final words quite variable data sparse 
characteristics bn su detection harder task reflected higher nist su error rate normalized event priors 
expected performance lm degrades prosody model stt condition 
degradation relative error rate increase lm due stt errors lower bn cts attributed better word recognition accuracies bn cts 
contrast cts observe performance prosody model degrade stt condition 
fact stt condition prosody model yields performance similar lm 
table shows frequently prosodic features decision tree bn su detection task 
differ cts pause duration word boundary frequently feature cases 
bn pitch plays important role cts phone word duration important cts 
pause duration current word useful bn cts 
differences may due fact speakers bn professional reporters reading tele prompted text pitch change reflect sentence structure consistently people engaged conversational speech 
addition cts involves conversational speech speakers pause information speaker affected speech activity speaker 
table feature usage su detection bn 
feature feature usage pau dur turn time rhyme dur ph nd bin diff note rt structural event detection tasks subtypes su evaluated results reported 
preliminary investigation indicates different prosodic features may important identifying su subtypes rising pitch question su 
additionally word exact su error rate prosody model just coincidence errors different 
gram lm effective distinguishing questions statements detecting backchannels 
task filler word detection setup filler words infrequent phenomena bn focus cts task 
detect filled pauses fps discourse markers dms filler word detection 
explicit editing terms considered due lack modeling approach infrequent occurrence 
expect fps dms sufficiently different characteristics separate models built 
explained earlier boundary detection approach determining boundary filler words 
filled pause discourse marker detection tasks involve way classification 
detecting discourse marker phrase search backward words appear pre defined discourse marker list determine onset discourse marker phrase 
filled pauses contain word knowing fp equivalent detecting fp 
evaluation performed human transcription recognition output cts 
su detection task prosody model lm evaluated individually combination 
cts filler word detection results results filler word detection cts corpus shown rows table 
observe performance degradation due errors stt output 
tasks filler word detection strongly depend word identity inaccurate stt output severely affects lm performance 
increase filler error rate stt condition compared human transcriptions dramatic su detection task 
combination lm prosody model substantially better lm filler detection contrast observed su detection cts bn 
table results cts filler word including fp dm detection fp detection dm boundary detection nist error rate cer parenthesis prosody model lm combination 
results ref stt conditions 
baseline cer filler word detection fp detection dm boundary detection 
cts lm prosody lm prosody filler word ref stt fp ref stt dm boundary ref stt detailed look performance fp detection shows interesting trends seen rows fp detection results table 
prosody model yields accuracy fp detection 
quite surprising prosody information utilized accurately detect filler words human transcriptions 
additionally ref stt conditions observe prosody model performs better lm 
believe decision tree learned word specific prosodic features learning specific duration pitch features filled pauses 
dm results rows table dm boundary detection dm extent 
represents better performance models way boundary detection model dm detection 
fp prosody model performs poorly dm detection 
addition stt condition combination lm prosody model better lm 
feature usage prosody model fp dm detection tasks shown table 
features vowel duration rhyme duration play important role filler word detection 
features different chosen su detection task cts bn data 
example feature su detection pause word filler especially fp detection word lengthening informative 
filler detection major focus thesis 
speech recognition output recognizer outputs correct filled pause word largely determines fp detection performance involves simple key word identification 
dm detection ambiguity dis course marker words confusable words situations 
higher level textual information word dependent prosody models needed resolving ambiguity 
task edit word ip detection edit word detection ip detection tasks highly correlated describe tasks section 
focus cts data edit disfluencies infrequent bn 
overview edit detection edit word detection extent detection task ip detection belongs boundary detection category 
speakers fluent starting point ip means ip inside edit disfluency including point filler words editing terms edit disfluency 
table feature usage fp dm detection tasks cts 
task feature usage max phone dur max phone dur energy win diff dur bin rhyme dur ph nd bin fp dur bin avg phone dur str rhyme dur ph bin avg phone dur prev pau dur wrd diff word dur rhyme dur ph bin dm turn time avg vowel dur avg vowel dur edit disfluency acoustic lexical cues location cues point speaker interrupts speech 
baseline approach prosodic lexical features detect interruption point ip knowledge rules locate corresponding reparandum onset hypothesized ips 
shows system diagram method 
final system output ip edit word detection combination hmm ip detection rule reparandum onset detection repetition pattern detection 
box described 
speech human transcription asr output hmm prosody word lm repetition pattern lm ip hypotheses knowledge rules repetitions reparandum reparandum fig 

system diagram edit word ip detection 
hmm ip detection union top left box shown dashed line represents way classification model determine edit ip interword boundary position 
decision tree prosody model trained sampled training set 
hidden event lm trained model joint distribution words ip events ip go school 
models combined hmm 
final output module ip hypothesis word boundary 
modeling repetition patterns word gram lm learn certain frequently occurring cies training data generalize related disfluencies different words 
example hope lots dinner parties repeated regular word hidden event lm fail detect final output ip utterance 
failure affect speech recognition task purpose lm calculate probability word strings 
address issue word lm modified account repetitions 
currently repetitions handled constrained frequently occurring edit disfluencies cts half edit disfluencies repetitions 
train lm deal repetition patterns training corpus processed way 
repetition training data remove reparandum region obtain cleaned utterance record repetition pattern 
example cleaned text hope lots dinner parties repetition mapped follows start orig ip rep pattern sequence example start orig ip rep 
number pattern denotes position event reparandum repeat region 
gram lm trained counts obtained cleaned text counts repetition patterns 
note repetition pattern sequence modeled shown pattern example ip reparandum onset hidden event lm captures hidden event ip information 
testing word boundary repetition events hypothesized valid state transitions word matches previous word 
hidden event repetition pattern properties representing occurs pattern possible valid event inferred 
trellis decoding valid state transitions considered 
shows state transitions repetitions repeated words 
lm interpreted class lm word sequence repeated words reparandum region mapped class tags tokens word lm fail detect repetition utterance observed training data 
repetition patterns 
addition assume lexical context associated repetition patterns gram lm equivalent allowing pattern occur word choice 
probability word sequence calculated way word gram lm fluent words ip point repetition pattern gram probability word probability 
advantage approach detect frequently occurring repetitions training data 
note repetition detection model cue words allowed mapped repetition pattern right repetitions words generally edit disfluencies 
rep st orig orig ip ip ip rep rep rep fp dm fig 

valid state transitions repetitions words 
axes represent position reparandum repetition regions respectively events denoted orig rep 
orig means position word reparandum total number repeated words represents position event repeat region 
optional filler words allowed ip transition 
rule reparandum detection rule knowledge box applies heuristic knowledge de termine extent reparandum region disfluency interruption point detected 
linguistic investigations suggest people tend start constituent repetitions revisions repeating function words 
example revision disfluency may red blue car speaker starts trying correct word red 
example ip correctly hypothesized interword boundary red ip blue car go backward find word word ip example occurred ip determine onset edit disfluency 
word boundary system ip hypothesis hmm component follows word fragment shows rule knowledge applied determine reparandum starting point 
post processing ips system generated su information 
bottom right box looking forward search word matches word ip search stops hits system su hypothesis cue word avoid false alarms 
edit ip detection results table shows results edit word ip detection cts corpus 
testing human transcriptions important type information occurrence word fragments signals disfluency interruption point 
human transcription results shown table information goal essentially recognize fragments speech signal 
currently available speech recognizers hypothesize occurrence word fragments difference ref stt conditions related availability word fragment information 
change incomplete su ip hypothesis word fragment turn change 
restart edit start su hypothesis 
previous su looking backward word match word edit starts matched word ip 
remove ip hypothesis fig 

rule method determining reparandum region ips hypothesized 
su hypotheses rules 
seen table prosody model edit ips edit words detected 
nist error rate deletion errors 
note prosody model access word repetition module uses textual information 
experiments shown useful prosodic cues exist interruption points performance prosody model investigated non downsampled test data study 
prior downsampled test set prosody model yields better accuracy chance performance ip detection non downsampled data table shows prosody model yields chance performance 
may posterior probability generated prosody model table cts edit word ip detection results nist error rate cer parenthesis prosody model lm combination 
results ref stt conditions 
baseline cer edit word detection edit ip detection 
cts lm prosody lm prosody edit word ref stt edit ip stt insufficiently high overtake low prior probability ip event 
shown equation prosody model final posterior estimation word boundary obtained adjusting decision tree output prior probability information 
prior probability ip cts shown table decision tree posterior probability output ip event greater non ip event higher adjusted posterior probability boundary 
contrast fp detection see table non downsampled data prosody model performs fp event lower prior probability ip event 
effectiveness fp prosody model attributed reliability prosodic features task prosody model high posterior probability fp test samples 
contrast prosody model ip detection task reliable current set prosodic features prosody model 
results table indicate lm performs significantly better chance word errors significantly affect robustness lm relies heavily word identities patterns 
analysis results shows ips correctly detected hidden event lm repetitions 
sense repetitions common training set 
additionally difficult capture properties revisions restarts simple word gram model 
prosody model achieves chance performance non downsampled test set ip detection posterior probabilities win high priors non ip event find combination lm prosody model slightly outperforms lm 
downsampled data decision tree learns important prosodic features ip detection 
feature usage prosody model trained training set shown table 
table feature usage ip detection cts corpus 
feature feature usage pau dur avg phone dur turn time word dur slope diff avg phone dur avg phone dur nsp summary tasks table summarizes baseline system performance nist error metric structural event detection tasks human transcription recognition output cts bn corpora 
tasks benchmark tests nist standard metric enables comparison systems 
table system performance nist error rate structural event detection tasks cts bn test sets 
results ref stt conditions 
bn cts ref stt ref stt su edit word filler word ip separate classifiers task conflicts different classifiers decisions interword boundary 
su edit ip conflicts reconciled looking posterior probability su hypothesis value accurate ip posterior probability 
su posterior higher predefined threshold su hypothesis preserved ip edit hypothesis 
example great 
great repetition detection hypothesizes repeat edit disfluency su detection model hypothesizes potential su check posterior probability su result 
case higher edit word ip hypotheses removed su hypothesis preserved 
shown table performance degrades dramatically stt condition tasks 
notice degradation su detection task tasks 
possible reasons greater robustness prosody model su provides help relatively robust recognition errors word model su lm dependent list key words case filler words detection specific patterns case edit word detection su events training data model better trained 
additionally degradation edit disfluency ip detection due fact transcription word fragments provided signal ips recognition output information unavailable 
find greater degradation filler edit detection tasks stt condition bn cts 
probably events larger percentage edit disfluencies contain word fragments bn 
interestingly observe table edit word detection error rate bn worse cts condition percent age edit words smaller bn cts significantly affects denominator performance measure 
suggests extent edit word detection relatively easier task bn cts sense different speaking style corpora 
edit disfluencies bn repeats simple revisions due reading errors 
chapter summary chapter described structural event detection baseline system performance 
hmm incorporating word information prosodic features boundary detection task 
additional rules extent detection 
observed effective acoustic prosodic cues event boundaries features vary different tasks corpora 
example pause word important feature su boundaries word lengthening important filler words pitch effective bn su detection cts 
lm generally outperforms prosody model combination models usually performs better individual model 
comparisons performance ref stt conditions suggest incorrect words contained recognition output detrimental lm prosody model 
sense dependent correct word identity 
summary combination multiple knowledge sources improves perfor mance structural event detection tasks 
different models knowledge sources strengths modeling different components structural event detection tasks 
effectively model knowledge source integrate improved system performance major focus remaining chapters thesis 

incorporating textual knowledge sources hmm system speech recognition role lm predict word context disambiguate word candidates acoustically confusable 
purpose word gram lm performs research shown incorporating knowledge improves performance 
structural event detection tasks syntactic information important determining sus disfluencies word prediction 
chapter investigate textual information words helpful detecting structural events speech 
motivation investigation additional information word level data sparsity issue training hidden event gram lm 
hidden event lm requires annotated training set typically requires effort produce transcriptions 
data set available training hidden event lm generally smaller size corpus train word lm speech recognition 
traditional class lm affected sparse data problem may generalize better groups similar words classes leads decrease vocabulary size 
addition investigate additional annotated text material accurately match type data test 
chapter organized follows 
section reviews related lm techniques 
section describes additional knowledge sources investigate including au induced classes part speech tags syntactic chunks textual class lm contrast word lm tokens class lm classes words 
material imperfectly matches test condition 
section describes textual knowledge sources combined prosody model 
section shows experimental results su detection task hmm system 
section summarizes chapter 
review related language model techniques baseline system word hidden event lm 
lm extended review related efforts aimed improving lms developed context speech recognition 
review word lms parameter estimation word class lms lm adaptation 
techniques applied structural event detection tasks 
word lms goal lm estimate prior probabilities word sequence wn wi wi 
wi word history word wi 
history mapping function wi contains previous words wi gram lm widely lm 
lm captures limited local information powerful applications speech recognition machine translation training corpora reasonably large 
parameter estimation maximum likelihood ml method generally obtain estimates 
ml estimation event occur training data assigned zero probability word sequence zero probability 
prob lem known sparse data problem 
order address problem called class lms 
word class denote word class information lm order distinguish pos chunk lms described word information 
variety smoothing methods proposed 
techniques central improving lm performance speech recognition 
smoothing methods discount probabilities events oc training set probabilities provide probabilities unseen events 
commonly smoothing techniques include linear interpolation combines higher order gram lms lower der ones backoff higher order lm frequency greater threshold back lower order lm 
note sparse data problem limited word lms 
smoothing methods applicable example class lms discussed 
word class lms class lms important reduce number parameters addressing part data sparsity problem poten tial increase generality lms unseen events 
ways obtain classes classes automatically induced statistical information training set existing linguistic classes pos tags semantic tags 
modeling methods jointly utilizing word class information word prediction 
conditional proba model calculates way wi wi wi wi wi wi wi wi mapped class word wi simplified ci 
trigram lm word history mapped classes formula wi wi wi ci ci ci wi ci ci ci ci second method uses joint model calculates probability joint word class sequence follows wi ci ci wi probability word sequence obtained summing class sequences ci cn heeman reported perplexity reduction joint model words classes compared word trigram lm 
conditional model shown equation yielded significant reductions perplexity speech recognition error rate 
joint modeling approach appears provide superior performance conditional model largely due importance lexical information lost con ditional modeling approach 
addition pos tags automatically induced classes lm researches attempting exploit syntactic structure information word prediction 
intuitively people simply employ local word occurrence information word prediction higher level knowledge syntactic semantic pragmatic information face acoustic am 
lms proposed various grammar 
example wang chelba shown significant word error rate reductions large vocabulary speech recognition tasks rating syntactic information 
lm adaptation theme research lms adaptation 
seen large training corpus needed obtain reliable parameter estimation gram tuples 
new application ex corpus different application study terms genre vocabulary case adaptation methods adapt lm parameters general domain new domain usually making limited size training corpus matches application 
approach build topic dependent lms new application detect topic order choose lm matched topic mixture lms weights optimized testing corpus 
reviewed lm research related methods going structural event detection 
lms latent semantic analysis exponential lms skipping lms 
choose elaborate relevant 
various knowledge sources described previous section speech recognition research focuses lms detect structural events speech 
question insights gained word prediction applications transfer detection structural events 
section introduce various knowledge sources expand word hidden event lm 
impact sources evaluated section 
note word prediction speech recognition joint model words classes provides richer information helps resolve ambiguity yields better estimation word word history 
structural event detec tion task unclear joint model words classes events effective goal different word prediction detecting hidden events word related contextual information 
additionally limited training set joint model contains parameters requires training data choose adopt joint modeling approach structural event detection tasks 
class lm separate hid den event lm model joint distribution classes structural events class sequence corresponding word sequence class lms advantage reducing vocabulary size address data sparsity extent potentially capturing semantic syn tactic similarity information 
various lms loosely combined 
subsections describe combined lms 
word lm model hmm baseline system chapter 
hidden event word lm models joint distribution word event sequence wi ei 
wi wi wi wi ei wi wi wi 
note structural events ei su explicitly included training non event token 
equation event ei previous words word history wi gram lm 
kneser ney smoothing method smooth parameter estimates 
automatically induced classes aic word classes induced distributional statistics bigram counts annotated training data criterion minimizing perplexity class gram 
incremental greedy merging performed starting class frequent words adding word time 
class induction algorithm generates mapping word mapped class 
resulting lm applied efficiently right context needed map current word corresponding class 
structural event detection tasks events valid tokens vocab question deal structural events class induction 
mentioned earlier non event explicitly included hidden event lm order avoid fragmenting word context 
inducing classes unclear token preserved bigram statistics 
hypothesis may capture information similar words occur non event token suggesting words similar roles terms struc tural event detection 
preliminary experiments comparing methods automatic class induction adding non event token explicitly word sequence improvement performance including 
non event token omitted class induction 
table depicts examples words resulting classes induced su detection task cts 
observe similar words grouped class class contains words signals su boundaries 
class contains similar words example re re exist words historically dissimilar words class 
criterion class induction reduce perplexity possible mismatch criterion task event detection 
part speech pos tags automatically induced classes derived data driven way 
classes may advantage representing word usage data set necessarily result groupings clearly interpretable linguistic mean ing 
additionally may mismatch training testing corpora bigram statistics induce classes may match test conditions 
contrast part speech tags represent syntactic word class formation related language generated 
example pos tags table examples automatically induced classes cts su detection task depicting member words word probability class 
class word huh class wow bye bye hum re re re somebody class everybody historically peripheral include nn noun vb verb wp wh pronoun 
pos tags investigated structural event detection tasks 
goal model syntactically generalized patterns tendency repeat prepositions type words tend utterances 
hidden event lm model joint distribu tion pos tags structural events pos tag sequence corresponding word sequence pos tags obtained statistical tnt tagger uses trigram hmm approach 
method pos tags represented states hmm transition probabilities maximum likelihood probability es derived training data 
viterbi beam search decoding tagging speed processing 
pos taggers trained cts bn corpora respectively follow way 
cts training 
tnt pos tagger trained switchboard treebank data applied tag training testing data structural event detection task 
similarly identity cue words backchannels filled pauses maintained 
bn training 
bn bn treebank data annotated pos tags 
started wsj treebank initial tagger trained 
tagged bn text corpus train word lm speech recognition re trained final pos tagger tagged bn data 
data set train tagger inaccurate expect large set training data may compensate extent 
retrained tagger tag bn training testing sets structural event detection task 
syntactic chunk tags introduced simple word class hidden event lms au induced classes pos tags directly model syntactic structure 
notice pos lm captures syntactic knowledge mod eling occurrence syntactic pos tags directly represent syntactic structure input utterance 
order investigate modeling syntactic structure helps structural event detection investigate chunk parsing 
chunker chosen full parser reasons 
full parser requires sentence input available situation 
second spontaneous speech contains ungrammatical incom plete utterances full parser may fail ill formed utterances provide misleading structures chunk parser tends robust 
chunk parser requires computational effort full parser 
text chunks represent non overlapping phrases sentence provide rudimentary syntactic structure 
word belongs phrasal constituent 
examples text chunks noun phrase np inside noun phrase np verb phrase vp outside phrase 
example table shows sentence bn corpus associated pos chunk tags 
simple structure sentence shown chunk indicated square bracket chunk types left bracket 
np top selling car pp np nineteen vp announced np today np winner vp np toyota text chunking represented classification problem supervised transformation learning tbl approach 
starting set predefined rule templates tbl learns ordered list rules greedy way 
iteration new rule generated rule templates corrects error training set best rule highest score preserved 
typical objective function tbl optimize evaluation function difference number positive negative examples rule applied 
chose investigate chunk information bn sentences similar written text may constrained syntactic structure 
pos tags input chunker expect higher table pos chunk tags sentence bn corpus top selling car nineteen announced today winner toyota 
word pos tags chunk tags dt np top jj np selling nn np car nn np pp nineteen jj np nn np vbd vp announced vbn vp today nn np cc dt np winner nn np vbz vp toyota nn np nn np tagging accuracy bn cts expect better chunking accuracy bn 
rules provided program trained sections wall street journal part penn treebank 
text chunker obtain chunk tags training test corpus bn su task 
similar pos automatically induced classes hidden event lm built joint chunk event sequence ch ch chunk tag sequence corresponding word sequence word lms additional corpora hidden event word lms trained small corpus annotated ldc structural event detection tasks lms trained data 
important question arises lms corpora annotated structural events help improve performance 
expect additional corpus help address sparse data problem encountered word lm trained annotated corpus ldc 
investigate corpora cts bn su detection task 
cts 
ldc treebank switchboard data contains punctuation disfluency annotations 
annotation guideline differs ears program definition sentences different interpretation incomplete sus versus restart edit disfluencies 
corpus contains words larger rt cts training set words shown table 
percentage su bound aries word boundaries similar rt data 
bn 
bn exists large text corpus train lm bn speech recognition 
punctuation information corpus highly similar sus structural event annotation 
corpus contains words compared words available rt bn data see table 
additional materials merge annotated rt training set train hidden event word lm merged data train separate lms combine models 
additional text corpora differ rt training testing data annotation scheme building separate models combining different weights better option 
note bn cts ldc switchboard treebank data 
presumably cts utilize text corpus train lm speech recognition punctuation approximate su events preliminary experiments shown lm yield gain 
hypothesize probably punctuation large data set carefully annotated sus rt cts data 
additionally sparse data problem serious cts bn cts sentence initial words vary bn 
integration methods lms hmm important issue combining multiple knowledge sources mechanism integration 
depends knowledge sources individual models 
choose train models various knowledge sources separately loosely coupled approach model combination 
mentioned earlier training joint model words various class tags require training data available structural event detection tasks 
annotated transcription straightforward train word hidden event lm described chapter 
word sequence mapped automatically induced class sequence pos tag sequence chunk tag sequence ch bn 
separate hidden event lms trained various tag sequences plus original event sequence aic lm pos lm chunk lm 
word hidden event lm trained additional domain text corpora word lm ood 
testing word sequence question various knowl edge sources best decision structural event 
com various lms integrate combined lms prosody model hmm system 
note descriptions lms hidden event lms 
combining aic lm 
automatically induced classes mapping word class done fly aic lm combined word lm lm level 
example word wi history wi wi wi wi wi wi wi optimized held data set 
probability obtained word lm uses word class information conditional modeling approach shown equation wi wi wi ci ci loosely coupled approach fewer parameters joint model classes words 
combining pos lm chunk lm 
automatically induced classes pos chunk tags obtained fly word sequence mapping entire word sequence needed tagging 
word sequence tagged obtain pos se quence tnt tagger chunks tbl text chunker 
interword boundary event ei computed pos chunk hidden event lms individually 
done forward backward algorithm word lm 
word boundary posterior probabilities pos chunk lm combined linear interpolation lms ei ei ei class 
pp os depending lm pos chunk combination 
note aic lm word information pos lm chunk lm 
combining word lm ood 
lms trained domain ood corpora combined word lms trained domain training data lm level 
aic lm combined final combination wi wi wi wi ood wi shown equation ood means word lm trained domain material 
combination lm level implemented functions srilm toolkit 
describe lms combined prosody model 
shows combination method 
box indicated dotted lines uses hmm approach described chapter 
word hid den event lm state transition probabilities obtained combination word lm aic lm word lm ood equation 
integrated prosody model described chapter 
outputs box posterior probabilities events word boundary 
pos chunk lms generate posterior probabilities individually 
word boundary event probabilities different models interpolated weights chosen held set 
experiments su detection task various knowledge sources described evaluated combina tions su boundary detection task cts bn 
training test word sequence prosodic features pos tagging word lm aic lm word lm ood prosody model pos sequence chunker hmm combination pos lm chunk sequence chunk lm ch posterior probability interpolation final output fig 

integration methods various lms prosody model 
data baseline system 
annotated text corpus train various class hidden event lms aic lm pos lm chunk lm 
additional text material cts bn described section 
cts su task table shows results various lms cts su boundary detection task 
results reported human transcriptions easily understand model considering effect incorrect words 
lms trained rt training data results show word lm yields lowest error rate aic lm second best pos lm coming 
pos lm loses fine grained lexical information word sequence converted pos sequence aic lm performs similarly word lm 
vocabulary size aic lm experiments substantially greater table su detection results nist error rate human transcriptions cts data various lms combination prosody model 
deletion del insertion ins total error rate reported 
cts model del ins total error rate word lm aic lm pos lm word lm ood word lm prosody word lm prosody word lm ood word lm prosody word lm ood aic lm word lm prosody word lm ood aic lm pos lm pos lm maintains fine grained information uses word information equation 
possible reason may poor pos tagging accuracy cts 
looking insertion deletion errors aic lm word lm similar insertion errors 
aic lm slightly deletions possibly effective words signal sentence start grouped correctly 
pos lm yields insertion deletion errors lms 
word lm ood performs worse domain word lm aic lm better pos lm 
mismatch training cor pus test condition word lm ood trained relatively larger training corpus suffers mismatch 
addition find error pattern word lm ood somewhat different word lm trained rt training data fewer insertions deletion errors 
results prosody model combined various lms 
su detection combining various lms baseline word lm prosody model consistent improvement obtained compared word lm 
notice compared results word lm prosody model combining various additional lms decreases insertion errors cost slightly increased deletion errors 
suggests adding prosody model tends generate insertion errors see results word lm versus word lm prosody adding various lms tends decrease insertion errors yield deletion errors 
bn su task experimental results various lms combination prosody model bn su detection task shown table 
contrast observed cts pos lm outperforms aic lm bn 
possible reason training data available train pos tagger result better tagging accuracy 
possible reason may lie difference language styles cts bn 
utterances tend grammatical similar written text bn conversational speech 
chunk lm perform probably coarse granularity preserve sufficient information distinguish events non events accurately 
word lm trained domain corpus perfor mance similar word lm trained rt data 
mismatch tagging accuracy corpora evaluated 
table su detection results nist error rate human transcriptions bn data various lms combination prosody model 
deletion del insertion ins total error rate reported 
bn model del ins total error rate word lm aic lm pos lm chunk lm word lm ood word lm prosody word lm prosody word lm ood word lm prosody word lm ood aic lm word lm prosody word lm ood aic lm pos lm word lm prosody word lm ood pos lm chunk lm training testing potentially affects lm performance avail ability training data helps address sparse data problems compensates part mismatch training testing sets 
contrast observed cts word lm ood yields lower accuracy word lm trained rt data 
cts amount training data compensate mismatch ldc annotated training data additional text corpus 
noted cts adding prosody model decreases deletion error rate cost insertion errors 
combining various lms decreases su detection error rate compared baseline system word lm prosody model 
contribution word lm ood bn significant cts 
chapter summary investigated information word knowledge including automatically induced classes pos tags chunk tags addi tional training data improve accuracy su detection 
experiments confirmed adding class lms improves performance train ing data helpful 
observe impact new sources differs corpora 
pos lm performs relatively better bn cts partly due better tagging accuracy largely different speaking styles corpora 
domain data shown important bn cts possibly relatively larger data size cts data sparsity issue severe bn cts 
aic lm conditional modeling approach shown equa tion combine word class information reduce model param eters compared word lm 
pos chunk lms loosely coupled model class sequence test word string gener ated event probabilities class token sequence computed combined probabilities word lm 
heeman allen proposed tightly coupled approach find best pos sequence disfluency events 
experiments conducted trains corpus differs conversational speech far template 
interaction syntactic information su boundaries disfluency phenomena tightly coupled model model proposed may perform better require larger training set currently available 
maximum entropy approach described chapter provides interesting alternative method tightly integrating knowledge sources 

prosody model described chapter baseline system decision tree classifier implement prosody model combine gram language model hmm find structural events interword boundaries 
structural events frequent non events training data decision tree prosody model designed deal imbalanced class distribution 
initial approach baseline system randomly training set obtain balanced data set decision tree training order model sensitive minority class su boundaries ips followed adjusting posteriors test set 
problem approach potentially important majority class samples model training downsampling may degrade performance test set 
chapter investigate sampling approaches cope imbalanced class distribution bagging scheme attempt build effective structural event prosody model classifiers 
pilot study conducted su boundary detection task uses small training set order extensively evaluate methods 
study human transcriptions factor effect speech recognition errors 
examine effect approaches different classification tasks su ip detection evaluate impact training data size 
findings pilot study successful methods chosen evaluate full nist su boundary task cts bn corpora 
chapter organized follows 
section describes imbalanced class problem approaches investigated address problem 
section different techniques systematically evaluated su boundary detection task pilot study cts corpus 
section sampling bagging approaches investigated su ip tasks 
section shows results nist su task best approaches pilot study 
section summarizes chapter 
addressing imbalanced data set problem imbalanced class distribution problem classification problem training set imbalanced class heavily represented 
clearly problem arises structural event detection tasks mentioned earlier inter word boundaries correspond su boundaries conversational speech broadcast news speech 
data skewed ip detection boundaries ips cts bn 
imbalanced data set problem received attention statisticians machine learning community 
various approaches attempt bal ance class distribution training set oversampling minority class downsampling majority class 
variations approaches sophisticated ways choose representative majority class samples ran choosing majority class samples match size minority class synthetically generate additional samples minority class replicating existing samples combine classifiers trained downsampled oversampled data sets 
important note techniques focus improving minority class prediction due relatively higher importance problem specification 
example application fraud detection tumor detection minority class clearly important 
weiss provost observed empirical study naturally occur ring distribution optimal distribution receiver operating characteristics roc performance criterion balanced distribution preferred choice 
sampling methodologies generally improve prediction minority class tend penalize majority class cases 
structural event detection tasks investigated thesis false positives false negatives considered equally costly 
chang ing distribution relatively minority class samples may produce classifier best performance 
goal evaluate various techniques address imbalanced class distribution training sets structural event detection 
sampling method best greatly depends properties ap plication samples distributed multidimensional space extent different classes mixed 
systematic investi gation different sampling approaches important building better models 
addition sampling methods investigate bagging 
bagging samples training set multiple times shown outperform single classifier trained training set 
sampling bagging techniques described section 
knowledge study imbalanced class problem struc tural event detection speech inputs 
study provide groundwork classification tasks related spoken language processing finding hot spots meeting corpus classes imbalanced 
study properties characteristic machine learning tasks speech involves large amounts data involves inherent ambiguity su boundaries ips matter judgment data noisy mea errors imperfect forced alignments pitch extraction labeling errors human labelers errors class distribution heavily skewed main issue addressed chapter 
addition property majority minority classes equal interest attribute studies needed see missing structural event inserting incorrect event impact human understanding downstream language processing modules 
currently equal penalty insertion deletion errors scoring procedure 
problem interesting 
believe study beneficial machine learning speech language processing communities 
approaches address problem sampling approaches pilot study different sampling approaches investigated sampling original training set random downsampling approach randomly majority class equate number minority majority class samples 
method uses subset majority class samples may result poorer performance majority class :10.1.1.18.5547
oversampling replication sampling approach replicates minor ity class samples equate number majority minority class samples 
majority class samples preserved minority class samples replicated multiple times 
replication poor minority class samples addition training set may lead poorer performance minority class :10.1.1.18.5547
ensemble downsampling ensemble downsampling simple modification random downsampling majority class split subsets roughly number samples minority class classifier trained subsets minority class 
decision trees trained balanced training set 
testing posterior probabilities decision trees averaged obtain final decision 
samples approach oversampling approach majority class samples plus minority class samples replicated times 
approaches differ decision trees trained 
ensemble downsampling approach scalable easily implemented distributed fashion 
oversampling synthetic samples smote smote stands synthetic minority sampling techniques 
oversampling approach mi class samples replicated multiple times 
contrast smote approach generates synthetic minority class samples ing existing samples 
synthetic samples generated neighborhood existing minority class examples 
continuous feature values smote produces new values multiplying random number difference corresponding feature values minority class example nearest neighbors minority class 
nominal cases smote takes majority vote minority class example nearest neighbors 
synthetic samples potentially cause classifier create larger specific decision regions generalize better testing set simple oversampling replication 
original data set sampling method 
original training set utilized bagging technique bagging combines classifiers trained instances sampled replacement training set 
bagging algorithm shown 
maintain fixed class distribution bagging trees class sampled separately 
bag class distribution original data set 
sets samples generated train classifier final classifier built classifiers equally weighted 
classifier generates posterior probability test sample outputs classifiers averaged obtain final probability sample 
input training set number bagging bagging size sizeof class size sizeof class sample class replacement size times sample class replacement size times train decision tree ci output classifiers fig 

bagging algorithm 
experiments 
bag class distribution original data bagging advantages 
different classifiers different errors combining multiple classifiers generally leads superior performance compared single classifier 
combination multiple trees different bags sampled instances final classifier noise tolerant 
second bagging implemented parallel distributed fashion speed training time 
bagging able maintain class distribution training set bagging applied 
important prosody model combined lm described chapter easier fixed class distribution training set 
disadvantage bagging bagging results multiple decision trees difficult understand features contribute final decision 
pilot study su detection experimental setup features data set pilot study small subset rt cts training data order evaluate methods described 
table describes data set pilot study 
training data set contains word boundaries su class remaining non sus 
test set consists word boundaries 
time consuming train decision tree large number features synthetically generate minority samples smote ap proach trained decision tree downsampled training set prosodic features section features selected decision tree features total evaluate various sampling approaches order minimize computational effort pilot 
initial investigations human transcriptions speech recognition output factor impact recognition errors investigation prosody model 
table description data set pilot study cts su detection task 
training size test size class distribution sus non sus features features discrete evaluation conditions sampling bagging approaches evaluated test set conditions prosody model 
decision trees trained balanced data set priors posterior probabilities generated decision trees combined obtain final hypothesis imbalanced test set shown equation 
decision trees trained original data set posterior probabilities need adjusted assumption original training set real test set similar class distributions 
combination lm 
combination prosody model hidden event lm evaluated test set 
decision tree trained balanced data set matter downsampling oversampling approach posterior probability decision tree needs adjusted resulting equation hmm 
decision tree trained original data set posterior probability generated decision tree true posterior probability ei fi wt numerator equation 
priors different classes taken account shown equation 
results reported classification error rate cer measure roc auc metrics described chapter 
metrics order better focus machine learning aspects problem 
test conditions measure computation 
addition threshold set final decision boundary posterior probability su boundary greater hypothesized su non su boundary 
sampling results experimental results different sampling approaches shown table 
generally downsampling methods outperform oversampling method employs replication lead overfitting 
observed table cer oversampling replication higher techniques table indicating decision tree generalize testing set 
downsampling way increase sensitivity decision tree classifier minority class 
slight improvement ensemble multiple decision trees single randomly downsampled data set train prosody model 
gain hold combining prosody model lm 
suggests classifier achieves performance knowledge sources language model may mask gain 
table su detection results cer measure different sampling approaches pilot study cts corpus prosody model combination lm 
cer lm test set 
approaches prosody prosody lm cer measure cer measure chance downsampling oversampling ensemble smote original original training set achieves best performance terms cer 
potentially due equal costs assigned classes 
possible sufficient examples belonging minority class training set learn su boundaries 
cer lower sampling approaches 
training decision tree original training set takes longer downsampled training set 
training set large heavily skewed advantage original training set may diminished 
smote improves results downsampling sampling approaches prosody model 
smote introduces new examples neighborhood minority class cases improving coverage minority class cases 
smote enables entire majority class set single decision tree improve performance majority class non su decision 
smote lead computational bottleneck large data sets distribution imbalanced sufficient examples belonging minority class 
examples added original training set large substantially increase training time 
possibly deploy combination smote pling counter large training set size 
notice gain smote method prosody model hold combined lm 
may due fact synthesized samples extent incompatible normally happens language samples smote helps correct decisions prosody model ones modeled lm 
compares various techniques roc curves auc obtained approaches prosody model 
roc curves span entire continuous region classification thresholds provide visualization trade true positives false positives 
cer measurement original training set achieves best performance shown table advantage sampling techniques pronounced look roc curves corresponding auc value 
auc sampling ensemble techniques significantly larger auc obtained training decision tree original distribution 
weiss provost observe downsampling beats oversampling repli cation smote beats oversampling downsampling 
observed researchers machine learning literature 
shown lower false positive fp rate original distribution competitive sampling techniques higher fp rates sampling schemes significantly dominate original distribution 
minority class greater importance tolerate false positives achieve higher recognition minority class locating appropriate operating point 
obtaining high recall su detection task important roc analysis sampling techniques definitely useful 
non smooth roc original training set largely due imperfect probability estima tion 
example minimum posterior probability test samples decision tree decision threshold greater test samples hypothesized positive class resulting sharp turning point 
sampling approaches posterior probabilities span entire region 
table focus error patterns sampling method show precision recall rate prosody model combination lm 
prosody model oversampling yields best recall result cost lower precision 
may result replicating minority class samples multiple times 
expected balanced training set beneficial recall rate contrary expectations recall performance downsampling ensemble sampling approaches better original training set 
note threshold decisions posterior probabilities false positive true positive downsampling auc oversampling auc ensemble auc smote auc original auc false positive fig 

roc curves aucs decision trees trained different sampling approaches original training set 
false negative errors equally costly 
fewer false positives original distribution learning decision tree 
leads higher value precision compared sampling techniques 
prosody model combined lm observe recall rate substantially improved downsampling ensemble sampling approaches resulting better recall rate original training set 
smote combine lm recall rate worst combining lm smote yields better recall rate downsampling ensemble sampling prosody model 
gain recall rate oversampling approach prosody model diminished combined lm 
table recall precision results sampling methods pilot study cts su detection 
lm yields recall precision 
bagging results approaches prosody prosody lm recall precision recall precision downsampling oversampling ensemble smote original sampling techniques selected bagging applied 
downsampling approach computationally efficient significantly reduce classification accuracy bagging applied downsampled training set construct multiple classifiers 
bagging ensemble ap proach tested 
described ensemble approach majority class samples partitioned sets combined mi class samples obtain balanced training set decision tree training 
final classifier combination base classifiers 
bagging trial number applied balanced sets classifiers generated combination 
bagging applied original training set 
bags bagging experiments 
combine bagging oversampling approaches poorer performance compared downsampling original training set 
bagging results reported table 
table shows bagging reduces classification error rate corresponding method table cts su detection results cer measure bagging applied randomly downsampled data set ds ensemble downsampled training sets original training set 
results training conditions bagging shown comparison 
approaches prosody prosody lm cer measure cer measure downsampling ensemble downsampling original bagging ds bagging ensemble ds bagging original bagging 
bagging downsampled training set uses subset training samples achieves better performance original training set bagging 
difference bagging original training set ensemble bagging significant sign test 
bagging able con struct ensemble diverse classifiers improves generalization decision tree classifiers mitigates overfitting single decision tree classifier 
gain substantial bagging applied downsampled training set original training set ensemble sampling sets compared corresponding conditions bagging respectively 
similarly study sampling techniques roc curves plotted bagging schemes zoomed version curves shown bottom 
auc substantially better bagging employed compared results shown bagging curves similar 
notice auc improved substantially bagging applied original training set 
attributed better posterior probability estimation true positive true positive bag ds auc bag ensemble auc bag original auc false positive bag ds auc bag ensemble auc bag original auc false positive fig 

roc curves aucs decision trees bagging downsampled training set bag ds ensemble downsampled training sets bag ensemble original training set bag original 
obtained average multiple classifiers 
explains roc curve smooth bagging applied original training set compared curve bagging shown 
consistent results bagging applying bagging original training set yields slightly poorer auc downsampled ensemble bagging cases 
sampling bagging su ip tasks investigated variety sampling approaches bagging ensemble methods detecting su boundaries 
considering approaches may affected characteristics task example data skewed structural events prosodic features quite informative evaluate techniques different task ip detection 
additionally effect data size examined su detection task 
experimental setup data experiment full rt cts training set pilot data due relative ip event see table 
con split training testing sets 
table shows experimental setup including training testing set sizes number inter word boundaries percentage minority class data set task ip su detection 
data set larger pilot su study allows examine effect data size sampling bagging techniques su task 
comparison include description data set pilot study su task table 
methods table description data sets su ip detection tasks 
data set pilot study shown second column subset data set investigation large set denoted table 
su ip pilot data large set large set training set test set percentage minority event similarly pilot study decision tree choose impor tant features sampling bagging techniques 
minimizes computational effort 
su ip tasks evaluation conducted transcription condition performance measured cer 
sampling bagging approaches evaluated pilot study smote due computational complexity lack performance gain 
leaves approaches study su ip tasks original training set downsampling oversampling ensemble sampling bagging set bagging ensemble sampling 
results su ip tasks table shows experimental results sampling bagging approaches ip su detection tasks 
addition evaluating original test set results downsampled test set prosody model ip detection due fact prosody model better chance performance non downsampled test set 
table ip su detection results cer 
ds denotes downsampled 
chance performance original test set ip su 
cer lm ip task su task 
ip su method ds test set original test set original test set prosody prosody prosody lm prosody prosody lm original sampling downsampled oversampled ensemble bagging ds ensemble effect data size su task impact data size su task examined comparing results table table 
data set size increases expected gain original training set lost benefit en sampling decrease downsampled training set representative data set 
table shows contrary expectation original training set yields best results greater cost training time 
expected gain ensemble sampling diminished data set size increases 
data set small ensemble sampling advantage making full data set ensemble 
data set increases inherently representative benefit ensemble decreases 
similar smaller data set pilot study oversampling computation ally expensive yield performance improvement 
downsampling training set performs reasonably advantage saving computation 
important training set size large hundreds thousands data samples 
data sets differ data size bagging outperforms single classifier trained bagging 
sampling bagging results ip task observe table prosody model original test set approaches original training set downsampling bagging ensemble able win bias majority class achieve performance better baseline chance performance 
original training set ip samples extremely small portion training set decision tree split classifier able learn characteristics minority class 
classifier performs chance downsampled test set 
sampling methods balanced training set improve classification performance downsampled test set 
bagging ensemble bagging perform significantly better corresponding approaches bagging downsampled test set prosody model 
prosody model trained original data set provide infor mation combined lm original test set techniques sampling bagging despite achieving chance performance prosody model provides added information combined lm 
comparisons ip su tasks ips frequent su boundaries vs sampling appears different impact tasks 
su detection task prosody model best performance achieved original training set different sampling approaches ip task sampling approaches able yield better performance chance 
downsampled test set ip task prosody model sampling techniques help improve classification performance 
prosody model combined lm relative error rate reduction compared lm smaller ip detection task su detection task versus respectively prosody model trained bagging downsampled data set 
prosody model bagging improves classification accuracy downsampled test set ip task non downsampled ip test set chance performance achieved 
contrast su task bagging yields substantial gain non downsampled test 
combined lm gain bagging su ip tasks 
shows roc curves ip su detection tasks orig inal test set downsampled training set bagging downsampled training set ensemble bagging prosody model 
curves sug gest bagging improves performance single randomly downsampled training set 
roc curve ensemble bagging similar bagging downsampled set 
notice relative improvement bagging ip detection task larger su task looking improvement roc curves suggesting bagging improves generality decision tree classifiers ip task su task 
evaluation full nist su task experimental setup best techniques identified pilot study evaluated full nist su detection task cts bn corpora true positive rate su task ip task downsampled downsampled bagging ensemble bagging false positive rate fig 

roc curves ip su detection prosody model cts corpus 
stt output conditions 
data full set previous chapters see table 
results reported official nist su error rate metric order compare baseline systems chapter 
addition evaluation conducted transcriptions stt output classification error rate straightforward 
results pilot study suggest bagging beneficial generating robust classifier best approaches ensemble bagging bagging original training set 
evaluate approaches combination lms 
downsampled training set trained baseline system chapter included comparison 
contrast pilot study preserve prosodic features total features expecting bagging generate different trees different features 
results nist su task table shows results prosody models combination lms cts bn su tasks transcription ref recognition output stt 
lms word hidden event lms trained ldc annotated training data plus domain extra text corpora described chapter 
performance degradation speech recognition output observed chapter 
recognition errors affect lms prosody model impact 
gain bagging sampling techniques transcription condition transfer stt condition 
conditions find applying bagging technique yields substantial win compared non bagging conditions 
prosody model applying bagging original training set achieves significantly better results ensemble bagging corpora prosody model combined lms difference bagging original training set bagging ensemble balanced training set diminished gain significant 
pilot study conducted cts corpus results table shows similar trend bn corpus 
differences corpora different class distributions different speaking styles gain bagging original training set observed bn fact greater cts 
prosody model contributes relatively bn corpus cts better prosody model relatively beneficial bn 
table su detection results nist error rate cts bn corpora ref stt conditions 
approaches bn cts ref stt ref stt lms prosody downsampling prosody ensemble bagging prosody bagging original lms prosody downsampling lms prosody ensemble bagging chapter summary summary lms bagging original attempted build robust prosody model chapter ad dressing imbalanced data set problem arises structural event detec tion tasks 
sampling bagging approaches investigated training decision tree prosody model 
empirical evaluations pilot study su detection task show downsampling data set generates reasonably classifier requiring training time 
computational advantage important processing large training set 
oversampling replication increases training time gain classification performance 
ensemble multiple classifiers trained different downsampled sets yields performance improvements prosody model su task 
performance prosody model may ways correlated results obtained prosody model combined language model example smote outperforms downsampling approach prosody model prosody model combined language model 
original training set achieves best classifi cation error rate sampling methods 
roc auc balanced training set yields better results original training set especially minority class interest 
bagging investigated randomly downsampled training set ensemble multiple downsampled training sets original training set 
bagging com multiple classifiers reduces variance caused single classifier improves generality classifiers 
bagging results better formance samples comparing bagging downsampled training set versus original training set bagging prosody model pilot study su task 
bagging run parallel training computationally efficient 
investigation ip task highlights differences ip su tasks probably due differences magnitude skew inherent differences cues different phenomena 
prosody model combined lm sampling techniques important case ip task severe problem data skew 
bagging generates robust classifiers su ip detection 
prelim experiment boosting highlights additional differences su ip detection tasks see appendix 
best methods pilot study evaluated nist su detection task cts bn corpora transcriptions 
bagging yielded substantial gain compared baseline system 
additionally find prosody model significantly better performance observed bagging original training set ensemble bagging gain eliminated prosody model combined lm su detection 
discussion experimental results confirm multiple classifiers reduces variance improves robustness model 
combination multiple learned models research topic machine learning community goal forming improved estimate 
issues involved model combination 
issue model generation 
important generate set models diverse sense errors different ways 
different approaches developed generating multiple models 
approach particular learning algorithm data resampling technique create set learned models 
bagging technique obtain multiple classifiers different resampling training data 
approach variety learning algorithms training data 
additionally multiple models learned different feature sets 
techniques attempt achieve diversity errors learned models varying training data learning algorithms features 
multiple models typically combined variants weighted majority strategy 
combining multiple classifiers interesting direction investigation effective prosody model 
experiments far yielded performance gain combination multiple decision trees trained different features sets combinations various sampling approaches chapter 
second issue model combination decide models rely decision weight give model 
errors different models uncorrelated majority vote reasonably approach combination 
patterns exist errors different models elaborate strategy far better 
example combining method needs identify unique contribution model inherent redundancy 
currently simple average posterior probabilities variety decision trees prosody model 
training super classifier learns combine multiple classifiers clearly important goal area 

approaches combine knowledge sources described previous chapters baseline structural event detection system hmm approach 
hmm computationally efficient provides convenient way knowledge sources main drawbacks 
standard training methods hmms maximize joint probability observations hidden events opposed posterior probability correct hidden variable assignment observations 
criterion closely related classification error 
second gram lm underlying hmm transition model difficult features highly correlated word pos labels greatly increasing number model parameters turn robust estimation difficult 
chapter non optimal approach interpolation lm level posterior probability level combining different textual sources hmm framework 
chapter describe efforts overcome shortcomings hmm maximum entropy maxent classifier conditional random field crf sequence decoding method su detection task 
approaches estimate conditional posterior probabilities directly contrast generative hmm 
provide principled way combine large number overlapping features 
maxent crf approaches differ directly model sequence information 
techniques previously traditional nlp tasks widely applied task prosodic textual information su detection task 
describe techniques developed incorporate knowledge sources compare su detection performance hmm maxent crf approaches different genres speech cts bn 
word recognition error different knowledge sources affect comparison investigated 
show simple combination approaches improves best results approach 
chapter organized follows 
section describes knowledge sources models evaluated 
section reviews hmm approach previous chapters 
section introduces maxent modeling approach describes experimental results su detection task 
section compares crf model hmm maxent models experimental conditions 
section summarizes chapter 
knowledge sources section briefly summarize knowledge sources previously hmm approach different modeling ap proaches compared chapter 
words su boundaries mutually con strained syntactic structure 
word identities au recognition human transcriptions constitute primary knowledge source su boundary detection task 
various automatic tag gers map word sequence representations 
tnt tagger obtain pos tags 
tbl chunker maps word associated chunk tag encoding chunk type relative word position noun phrase inside verb phrase 
tagged versions word stream provided allow generalizations syntactic structure smooth possibly word probability estimates 
reasons generate word class labels automatically induced bigram word distributions 
hidden event lms various tags described chapter 
model prosodic structure su boundaries prosodic features ex word boundary described chapter 
acoustic alignments produced speech recognizer forced alignments transcriptions 
features capture duration pitch energy patterns associated word boundaries 
crucial aspect features highly correlated derived raw measurements different normalizations real valued discrete pos undefined unvoiced speech regions pitch 
properties prosodic features difficult model directly textual information hmm approach 
modular approach adopted information prosodic features modeled separately decision tree classifier puts posterior probability estimates ei fi ei boundary event wi fi prosodic feature vector associated word boundary 
con approach permits include non prosodic features highly relevant task represented generative hmm speaker turn change occurs location question 
review hmm su detection baseline model described chapter hidden markov model hmm 
model forms basis prior sentence boundary detection speech 
briefly review hmm structural event detection section 
system evaluated chapters 
hmm states model correspond word wi event label ei associated word boundary 
observations associated states words prosodic features fi 
shows graphical model representation variables involved 
note words appear states observations word stream constrains fi fig 

graphical model su detection problem 
word event pair depicted state model grams previous tokens condition transition state 
observations consisting words prosodic features structural events 
possible hidden states matching words ambiguity task stems entirely choice events 
hmm approach standard algorithms available extract probable state event sequence set observations 
goal minimize boundary error rate nist su error rate finding highest probability sequence events identify events highest posterior probability individually boundary arg max ei ei words features entire test sequence respectively 
individual event posteriors obtained applying forward backward gorithm hmms 
training hmm supervised event labeled data available 
state transition probabilities estimated hidden event gram lm 
lm obtained standard gram estimation methods data contains word event pair tags sequence 
en wn 
resulting lm compute required hmm transition probabilities 
wi ei wi 
wi ei ei 
wi ei wi gram estimator maximizes joint word event pair sequence likelihood training data modulo smoothing guarantee correct event posteriors needed classification equation maximized 
second set hmm parameters observation likelihoods fi ei wi 
training likelihood model prosodic classifier described chapter 
observation likelihoods may obtained follows fi wi ei ei fi ei fi ei fi obtained decision tree estimation 
chapter single decision tree ensemble bagging reduce variance clas generate reliable posterior probability estimation 
chapters described deal mismatch class distributions training testing decision tree learning 
hmm modeling representation adopt su detection different hmm pos tagging problem sequence labeling tasks nlp 
comparison shows graphical model pos tagging hidden states consist pos tags observations words 
su detection task classes su hypothesize states state sequence contain discriminative information effectively decode sequence 
words added states constrain event sequence 
table shows su detection results utilize word event contexts length greater hmms order greater equivalently entire word event pair gram state 
fig 

graphical model pos tagging problem 
pos tags hidden states problem 
pos tags words 
comparing models words appear states cts human transcription condition 
approaches trigram models word information sequence decoding prosodic information 
clearly substantial performance degradation word information removed hidden states state configuration remaining experiments 
table su detection results nist error rate different state configurations trigram lm cts condition 
insertion ins deletion del total error rate shown 
state membership su error rate ins del total event word event hmm structure strong independence assumptions features depend current state practice event label word event pair label depends previous tokens 
return get computationally efficient structure allows information entire sequence words prosodic features inform posterior probabilities needed classification forward backward algorithm 
problematic practice integration multiple word level features pos tags chunker output 
theoretically tags simply hidden state representation allow joint modeling words tags events 
drastically increase size state space making robust model estimation standard gram techniques difficult 
method works practice linear interpolation conditional probability es various models simply averaged 
improvement su boundary detection performance obtained combining word hidden event lm class lms discussed chapter 
similarly interpolate lms trained different corpora 
usually effective pooling training data allows control contributions different sources 
example lms obtained extra larger corpora chapter 
larger training corpus get larger weight imprecise labeling su boundaries get lower weight 
tuning interpolation weight lms empirically held data compromise extremes 
maxent posterior probability model su detection maxent model successfully applied variety nlp tasks pos tagging text categorization chunking machine translation language modeling named entity detection word sense disambiguation :10.1.1.12.2996
perform similarly state art approaches nlp tasks 
model taken classification approach designed features accordingly 
features involve word context lexical information word word related information 
focus feature design led superior performance example pos tagging 
show feature design important factor maxent performance structural event detection 
description maxent model discussed chapter su detection problem represented classifica tion task 
word boundary observation consists context word corresponding prosodic features task classify bound ary su 
assumption observations oi fi wi independent 
note pre encoded dependency feature set associated sample word context wi 
contextual prosodic features encoded observation oi just features fi associated word boundary 
classification problem different ways set model param eters maximize joint likelihood conditional likelihood entire training set denotes class labels 
posterior probability likelihood correlates classification error rate metric maximum joint likelihood conditional likelihood objective function cl training data set consisting labeled examples cl con ditional likelihood denotes model 
conditional likelihood closely related individual event posterior probability classification allowing type model explicitly optimize discrimination correct incorrect labels 
problem estimate conditional probability vector features representing different knowledge sources class label su 
maxent model models things known assumptions unknown events provides possible solution 
con straints maxent model obtained training set empirical distribution equal expected value feature functions respect model gi ep gi feature set associated sample class label 
functions gi indicator functions corresponding complex features defined events words prosodic features 
example feature function su detection task wi su confusion call predicate part indicator function features 
generally features machine learning approaches decision trees 
maxent model finds probability distribution satisfies constraints maximum conditional entropy log solution constrained optimization problem exponential form normalization term exp igi exp igi find parameters log likelihood ei training data maximized 
experiments bfgs parameter estimation method gaussian prior smoothing avoid overfitting 
implementations methods maxent toolkit 
training data structural event detection limited smoothing essential 
intuition gaussian priors force parameters distributed gaussian distribution mean variance prior expectation penalizes parameters drift away mean prior value generally 
gaussian smoothing penalty term added equation maximum likelihood estimation features cl cl log exp maxent gives freedom features overlapping dependent choose subset features informative parsimonious order obtain generality robust parameter estimates 
included features correspond information available hmm ap proach summarized 
features triggered training set eliminated improve robustness avoid overfitting model 
discussion feature selection appear section 
word grams 
combinations preceding words encode word context event wi wi wi wi wi wi wi wi wi wi wi wi wi refers word boundary interest 
pos grams 
pos tags see section hmm 
features capturing pos context similar word tokens 
chunker tags 
similarly pos word features tags encoding chunk type np vp word position chunk versus inside 
chunker features bn data 
syntactic chunk tags generated tbl chunker described section 
word classes 
similar gram patterns automatically induced classes obtained way described section 
turn flags 
speaker change marks su boundary binary feature 
note hmm feature grouped prosodic features handled decision tree easy capture information hidden event lm hmm maxent approach separately feature 
prosody 
possible include prosodic features maxent framework designing method encode various prosodic features decision trees straightforward 
decided decision tree classifiers generate posterior probabilities ei fi 
convenient binary features maxent classifier prosodic posteriors encoded binary features thresholding 
equation allows feature maxent model monotonic effect final probability raising lowering constant factor kgk 
suggests encoding decision tree posteriors cumulative fashion series binary features example 
thresholds heuristically chosen 
representation advantage robust possible mismatch posterior probability training test sets small changes posterior value affect feature 
order obtain posterior probability prosody model training samples cross validation approach 
train decision trees training set generate posterior probability set trees probabilities probably biased 
fold cross validation training set split subsets trees trained sets generate posterior probabilities left 
way sample training set assigned probability binned maxent model 
testing trees trained entire training set estimate probabilities 
maxent framework allow real valued feature functions preliminary experiments shown gain compared binary features constructed described 
results continuous features maxent model shown section 
auxiliary lm 
mentioned earlier additional text language model train ing data available 
hmm model incorporated auxiliary lms interpolation possible lm se gram features 
trick prosodic features 
word hmm estimate posterior event probabilities auxiliary lm posteriors thresh yield binary features 
auxiliary lm applied training test set rt training test data generate posterior probability estimations cts bn respectively 
combined features 
date fully investigated compound features combine different knowledge sources order model interaction explicitly 
limited set features included example com bination decision tree hypothesis pos contexts 
reason choose pos tags decision trees hypotheses limit number parameters limited number pos tags attempting combine grammatical constraints prosodic formation 
comparisons maxent hmm approaches hmm training directly maximize posterior probabilities cor rect labels resulting mismatch training model classifier 
second problem hmms underlying gram sequence model cope multiple representations features word se quence words pos short building joint model involving variables 
problems addressed maxent model 
maxent model di rectly estimates posterior boundary label probabilities ei maximizing joint likelihood observation state sequence 
max ent model allows correlated features apply simultaneously gives greater freedom combining knowledge 
desirable characteristic maxent models split data recursively condition prob ability estimates robust decision trees training data limited 
possible include prosodic features directly maxent framework modeling separately decision trees 
hmm maxent differ regarding training objective function joint likelihood versus conditional likelihood respect handling depen dent word features model interpolation versus integrated modeling maxent 
counts maxent classifier superior hmm 
maxent approach theoretical disadvantages compared hmm design 
obvious shortcoming maxent approach information lost thresholding converts posterior probabilities prosodic model auxiliary lm binary features 
qualitative limitation maxent model uses local evidence surrounding word context local prosodic features 
maxent model resembles conditional probability model individual hmm states 
hmm drawback maxent classifier design su detection task 
forward backward procedure propagates evidence parts observation sequence decision point 
results discussion maxent su model experiments comparing maxent hmm approaches conducted su detection task bn cts corpora 
training test data experiments chapters 
system performance evaluated official nist evaluation tools 
experiments compare approaches perform individually combination 
combined classifier obtained simply averaging posterior estimates models picking event type highest probability position 
experimental factors investigated impact speech recognition errors impact genre contribution textual versus prosodic information model 
experimental results table shows su detection results bn cts transcriptions speech recognition output conditions hmm maxent approach individually combination 
maxent approach slightly outperforms hmm approach evaluating transcriptions combination approaches achieves best performance tasks significant sign test ref condition mixed results stt condition 
observe table large increase error rate evaluating speech recognition output 
replicates findings previous chapters 
maxent system degrades hmm recognition output 
seen table maxent outperforms hmm transcription stt condition maxent yields comparable table su detection results nist error rate maxent hmm approaches individually combination bn cts transcriptions ref recognition output stt 
hmm maxent combined bn ref stt cts ref stt slightly worse performance hmm 
sense improvement maxent model comes better lexical feature modeling 
exactly features deteriorate recognizer errors 
hand prosodic information robust face recognition errors fully utilized maxent approach 
table shows deletion insertion error rates hmm maxent approaches condition 
due reduced dependence prosody model errors maxent approach different hmm approach 
deletion errors fewer insertion errors prosody model tends su hypotheses 
consistently observe decrease deletion rate increase insertion error rate lm combined prosody model compared lm 
different error patterns suggest effectively combine system output approaches confirmed table combination consistently yields best performance significantly better hmm sign test 
table shows su detection results approaches textual formation combination prosody model 
focus table deletion insertion total error rate nist error rate hmm maxent approaches transcriptions bn cts 
del ins total bn hmm maxent cts hmm maxent results transcription condition 
maxent achieves lower error rate hmm textual features prosodic information incorporated gain diminished 
superior results text classification consistent maxent model ability combine overlapping word level features principled way 
contrast hmm approach linearly interpolates various lms see chapter lm level posterior probability level 
hmm largely catches prosodic information added 
attributed direct prosodic posterior probabilities hmm fact hmm boundary decision affected prosodic information sequence maxent model uses prosodic features boundary classified 
notice table maxent approach yields gain hmm cts bn ref condition corpora 
possible reason training data sparse data problem cts 
possible reason prosody model contributes bn cts pattern observed chapter role component lower maxent approach 
seen table textual information gain relative error rate reduction maxent hmm slightly bn cts 
table su detection results nist error rate different knowledge sources bn cts evaluated transcription 
additional investigations bn cts hmm textual textual prosody maxent textual textual prosody maxent model better lexical features prosodic information effectively partly due binning posterior probabilities prosody model 
framework maxent restricted binary features features continuous value value weighting features 
investigate preserving posterior probabilities prosody model improves sensitivity maxent model prosodic information 
table shows results maxent model features earlier experiment features listed section exception posterior probabilities decision trees real valued features 
experiment performed transcriptions cts data 
consider posterior probability prosody model log value maxent framework 
clearly improvement posterior probability represented real valued feature 
degradation substantial real valued posterior probabilities log value probabilities significant degradation performance 
potential advantage continuous features result confidence scores generated table comparison posterior probabilities prosody model binary features versus continuous valued features maxent approach su detection cts transcription condition 
posterior probabilities su error rate binary cumulative binned probability continuous posteriors log posteriors speech recognizer textual feature weighted confidence measures stt test condition 
study feature selection feature pruning investigated maxent approach 
feature selection important features noisy hurt performance 
addition overfitting may result features generalize test set 
removing irrelevant features generate accurate predictions compact model 
various feature selection algorithms maxent models investigated 
consider feature selection metrics previously studied text classification 
information gain ig widely machine learning data mining measure association 
definition entropy conditional entropy entropy ig log conditional entropy log feature random variable representing class membership 
information gain measures number bits saved transferring cause presence provide information ig value positive 
training set information gain feature provides way su classification task calculated follows su log su su log su su log su su log su table shows gram features highest ig weights su detection task 
clearly selected words signals su boundaries sentence initial final words 
mutual information mi mutual information feature class defined mi log logp logp logp average mi classes mutual information feature class variable mi ci mi ci ci log log ig mi defined interchangeably literature indicated terms 
table gram features highest ig weights cts su detection task 
feature ig weight current word word current word word word word word oh current word oh word current word right current words word current word current word mutual information represents reduction uncertainty due knowledge mi easily computed training set 
note mutual information equal log similar different information gain log chi square statistics chi square measures independence fea ture class chi distribution 
contingency table shown table chi statistic defined chi ad cb feature find chi statistic results chi distribution degree freedom 
chi square statistic may reliable event frequency low 
table notation contingency table chi square statistics 
feature occurs feature occur class occurs class occur compute weight feature training set metrics described sort features weights 
features highest weights preserved pruned 
table shows results different numbers features preserved weights metrics 
experiments conducted transcriptions cts data 
experiments show preserving features yields consistent gain 
features outperforms pruned feature sets difference significant 
surprising gaussian prior smoothing model parameter estimates robust mitigating issues noisy features overfitting 
feature selection metric difference features significant 
mutual information outperforms feature selection algorithms 
significant difference information gain chi square metrics feature selection 
table su detection results nist error rate different feature selection metrics different pruning thresholds number preserved features cts ref condition 
number preserved features su error rate ig mi chi features conditional random field crf model su detection simple combination maxent hmm improve performance model shown table 
due complementary strengths weaknesses models 
hmm generative model able model sequence forward backward algorithm 
maxent approach discriminative model attempts decisions locally sequential information 
conditional random field crf model combines benefits approaches 
maxent crf accommodate correlated features trained discriminative way 
hmm crf uses sequence decoding globally optimal 
compare performance crf model hmm maxent approaches su detection task 
fig 

graphical representation crf sentence boundary detection problem 
represents state tags su boundary word prosodic features respectively 
observations consisting description crf model crf random field globally conditioned observation sequence crfs successfully variety text processing tasks parsing named entity recognition information extraction 
depicts graphical representation modeling approach sequence labeling task 
crf undirected graph states model correspond event labels ei observations xi associated states words wi prosodic features fi 
sequence input sequence observations arg max exp function potential function events observations normalization term exp crf model trained maximize conditional log likelihood training set 
maxent model closely related performance metric 
sequence viterbi algorithm 
mallet package implement crf model 
avoid fitting gaussian prior employed mean zero parameters similar training maxent models 
features crf su detection model maxent approach 
crf takes longer train hmm maxent models especially number features large 
hmm requires time training models 
comparisons crf models crf maxent differs hmm respect training ob function joint versus conditional likelihood handling dependent word features 
hmm training maximize posterior probabilities correct labels crf directly estimates posterior boundary label prob abilities 
underlying gram sequence model hmm cope overlapping representations word sequence crf model supports simultaneous correlated features allows easily incorporate variety knowledge sources 
crf hmm differs maxent method respect ability model sequence information 
primary advantage crf maxent approach model optimized globally entire sequence maxent model uses local evidence 
crf essentially maxent model sequence level entire sequence treated sample posterior probability estimation maxent framework implemented efficient viterbi algorithm 
forward backward algorithm better implemented current software 
addition crf differs maximum entropy conditional markov model cmm 
cmm single function represents probability current state previous state current observation approach known problem called label bias problem due state normalization transition scores 
causes problems fewer outgoing states 
example extreme case single state observations equivalently ignored 
crf approach addresses label bias problem cmm employing sequence modeling 
compares graphical models hmm cmm crf ap proaches 
hmm generative model models joint distribution state dependent previous state limited set previous states higher orders hmms 
cmm state depends previous state observation 
crf sequence modeling approach single exponential form probability state sequence entire observation state exponential form cmm approach 
results discussion crf su model features crf described section maxent model happen knowledge sources hmm different representations 
keeping knowledge sources consistent models enables focus comparison effectiveness modeling approaches 
worth noting possible different modeling approaches crf new features explored 
investigation compare crf hmm maxent models voting combination 
su detection results crf hmm maxent approaches individually transcriptions recognition output cts bn shown hmm approach cmm approach crf approach fig 

graphical model representations hmm cmm crf approaches 
observations events tags 
tables combination modeling approaches majority vote 
results reported nist su error rate 
seen table crf superior hmm maxent conditions differences significant bn table su detection results nist error rate hmm maxent crf approaches individually combination bn cts transcriptions ref recognition output stt 
combination approaches obtained majority vote 
hmm maxent crf majority vote bn ref stt cts ref stt stt condition 
combination approaches superior model 
previously maxent hmm posteriors combine see table 
toolkit implementation crf provide posterior probability sequence unable combine system output posterior probability interpolation expect yield stronger performance gain 
observe table crf larger increase error rate evaluating speech recognition output compared hmm suggesting crf suffers recognition errors 
crf yields relatively gain hmm bn cts 
possible reason difference training data cts task crf maxent approaches require relatively larger training set hmm 
possible effective sequence information cts due different speaking styles cts bn 
crf model order markov model better modeling shorter sus cts longer sus bn 
similarly comparisons hmm maxent approaches investigate impact different knowledge sources 
table table show su table cts su detection results nist error rate hmm maxent crf individually different knowledge sources 
note features condition uses knowledge sources described section 
cts hmm maxent crf word gram ref word gram prosody features word gram stt word gram prosody features detection results cts bn respectively different knowledge sources word gram word gram prosodic information features listed section 
observe tables word gram information gain crf hmm maxent greatest differences models diminishing features added 
may due impact sparse data problem crf simply due fact differences modeling approaches features stronger strong features compensate weaknesses models 
notice cts fewer knowledge sources word gram prosodic information crf able achieve performance similar better methods sources 
may useful feature extraction computationally expensive 
looking results word gram information observe effect word errors models 
su detection error rate increases stt condition crf model table bn su detection results nist error rate hmm maxent crf individually different knowledge sources 
bn hmm maxent crf word gram ref word gram prosody features word gram stt word gram prosody features models suggesting discriminative crf model suffers mismatch training uses transcription test condition features obtained potentially erroneous words 
chapter summary different approaches described modeling integrating diverse knowledge sources su detection state art approach hmms alternative approach posterior probability estimation maxent method crf sequence decoding approach 
achieve competitive performance maxent crf models devised evaluated binary coding scheme map posterior estimates auxiliary submodels prosody model auxiliary lm features 
hmm maxent approaches complementary strengths weak reflected results consistent findings text nlp tasks 
maxent model discriminative approach yields better accuracy hmm lexical information smaller win com bination prosodic features possibly poorer prosodic feature modeling currently approach 
hmm generative approach modeling currently effective prosodic information degrades erroneous word recognition 
interpolation posterior prob abilities systems achieves relative error reduction compared baseline significant transcription condition 
results consistent different genres speech 
additionally observe maxent affected recognition errors hmm approach due heavy reliance textual information lighter prosodic information 
results show feature selection important issue maxent approach pruning features degrades performance 
investigations shown discriminatively trained crf model competitive approach su detection task 
crf combines advantages generative hmm approach conditional maxent approach discrim trained able model entire sequence 
outperforms hmm maxent approaches consistently various testing conditions 
find knowledge sources differences modeling ap proaches decrease 
simple combination modeling approaches majority vote proven superior single model 
useful area include developing features combine multiple knowledge sources incorporating prosodic features directly maxent crf approaches generating posterior probabilities event crf approach investigating approaches model recognition uncer tainty order mitigate effects word errors 

system rt chapter describe new methods latest nist rt evaluation verify approaches investigated earlier chapters hold new data set 
chapter organized follows 
section describes task data rt evaluation 
section discusses system performance su task 
section introduces method su subtype detection 
section describes new approaches edit word detection compares previous approach described section 
section summarizes chapter 
rt tasks data structural event detection tasks rt rt eval uation described chapter su detection positions su boundaries generated subtype su 
rt data annotated ldc annotation guideline differs guideline annotate rt data table 
bn minor changes conventions introduced identifying sign offs subordinating conjunctions resulting significant dif ferences annotations data rt rt 
fact data sparsity problem severe bn combine rt rt bn data order increase training data size 
contrast differences guidelines cts significant rt training data 
annotation guideline uses open classes filler words backchannels pre defined word list guideline 
additionally conventions rules introduced example order distinguish incomplete sus restart edit disfluencies 
differences straightforward automatically map rt training data annotation guideline 
rt cts data 
table describes training testing data rt nist evaluation 
wer stt output structural event detection tasks reported table 
cts stt output combined ibm sri system bn stt output combination rt ears systems 
table data description cts bn rt nist evaluation 
bn training data combined rt rt data 
cts contains rt training data 
bn cts merged rt rt data rt data training hours hours test hours shows hours conversations stt wer system performance su boundary detection section discuss results su boundary detection task new rt data 
table shows su boundary detection results 
compare baseline system described chapter systems incorporating improvements investigated chapters 
baseline system uses hmm combines word hidden event gram lm trained data shown table prosody model trained downsampled ds training set 
table observe significant improvements hmm baseline sys tem applying bagging prosody model incorporating additional textual knowledge sources hmm 
maxent crf generally outperform table su boundary detection results nist su error rate rt evaluation data 
combination majority vote maxent crf improved hmm approaches 
ds denotes downsampled training set 
su boundary error rate models cts bn baseline hmm ref stt ref stt word lm prosody ds improved hmm lms prosody bagging ds maxent crf combination improved hmm gain greater rt data set chapter bn data set 
notice table maxent crf approaches degrade stt condition compared results chapter 
may due better recognition accuracy larger training set especially bn 
combination results table majority vote improved hmm maxent crf approaches 
explained chapter currently access posterior probabilities crf tool voting scheme system combination 
expect combination methods improve result 
su su subtype detection su su subtype detection task su needs detected subtype su 
task step approach adopted 
su boundary detected hmm maxent crf combination system hypothesized su boundary classifier determine subtype 
reason utilize pass approach boundary detection approach way classification su subtypes plus non su easily incorporate knowledge boundary locations su su initial words subtype decisions 
kind information difficult directly incorporate pass way classification approach 
su boundary detected second pass determine boundary type 
maxent classifier su subtype detection ease incor various features sentence initial cue words 
features hard model current generative hmm 
features include su initial words optional filler words su final words turn change current previous su boundaries length su binned posterior probabilities prosody model way su subtype classification su boundary 
table shows percentage su subtypes cts bn data 
bn statement frequent subtypes cts types balanced statement remains majority class 
highly skewed distribution subtypes bn suggests reasonably performance achieved hypothesizing statement su su boundary su subtype detection task investigated cts 
su subtype detection results shown cts table 
step boundary detection uses majority vote hmm maxent crf ap proaches row table 
report boundary detection table percentage su subtypes cts bn 
statement backchannel question incomplete bn cts table su su subtype detection results rt cts evaluation data 
results reported nist su boundary error rate substitution error rate subtype classification error rate cer 
cts boundary error substitution error subtype cer ref stt error substitution error nist su scoring tools 
errors measured total number sus 
shown table su subtype classification error rate cer defined percentage incorrectly labeled su boundaries 
denominator cer metric su subtype detection total number correct system hypothesized sus sus 
metric better represents classifier maxent classifier performance factoring boundaries missed sys tem hypothesized sus 
interestingly substitution errors subtype classification error rates generally affected stt errors su hypothesis errors 
table shows su subtype classification performance confusion matrix transcription condition 
observed relatively larger percentage misclassified boundaries incomplete question subtypes frequent subtypes see table may difficult discern types 
table su subtype detection results confusion matrix cts human transcription condition 
cell shows count percentage subtype row hypothesized subtype shown column 
system hypothesis backchannel incomplete question statement backchannel incomplete question statement note testing features extracted system hypothesized su boundaries starting point su may wrong su detection insertion deletion error affect features related su initial words 
recall prosody model built features extracted word boundary account longer span prosodic features useful subtype detection 
new task research remains done investigate effective features modeling approaches 
edit word detection methods edit word ip detection hmm described chapter baseline approach 
section additional methods maxent crf approaches examined edit word ip detection 
hmm edit word detection edit word detection hmm detecting ips edit disfluencies 
hidden event word lm trained joint word edit ip sequence 
additional textual information pos tags automatically induced classes ip task 
prosody model trained downsampled training set 
word lm generally able detect repetitions occurred training set repetition detection model described section finds repeated word sequences possible filler words allowed edit ip 
rule approach find edit disfluency ip detected 
maxent edit word detection maxent approach maxent classifier way classification su ip null 
similarly hmm heuristic rules determine onset reparandum 
advantage approach jointly models su ip events 
example great 
great occurred training set model learn sus edit disfluency word sequence repeated 
repetition detection module hmm predefine cue words sus considered edit disfluencies probabilistic maxent model able learn kinds cue words training set models elegantly 
note heuristic rules system su hypotheses determining onset reparandum ip hypotheses 
maxent approach su information generated directly maxent classifier su detection tightly integrated system 
features maxent model su ip null detection task follows features su detection described section 
repetition information 
word boundary feature represents repeated word sequence words ends point optional filler words allowed starting point 
fragment information 
feature represents word fragment 
transcription condition feature triggered 
speech recognition output condition word fragment information provided 
filler words 
feature represents pre defined filler phrase word boundary 
prosody posterior probabilities 
decision tree trained binary classi fication task ip null section 
posterior probabilities represented cumulative binning way 
crf edit word detection crf approach edit word detection finds entire region reparandum similarly named entity recognition task 
approach word associated tag representing edit word 
classes crf edit word detection approach edit inside edit possible ip associated ip ip outside edit 
total states model shown table 
example transcription excerpt class tags crf edit word detection model uh analyst ip ip filler word detection results list cue words 
got got real rough ip goal task find reparandum extent ips including internal ips inside complex edit disfluency 
notation guideline structural event detection task definition ips annotated inside complex edit disfluencies scored ip detection task 
ips included target class crf edit detection order identify internal ips inside complex edit disfluencies 
example example reparandum complex edit disfluency internal ip 
table states transitions crf edit word edit ip detection 
class tags edit inside edit possible ip associated ip ip outside edit 
state number notation meaning possible state destinations outside edit ip ip edit ip ip edit ip ip inside edit ip ip ip inside edit ip crf model able learn valid state transitions training data 
possible states state go shown table 
valid state transitions guaranteed state edit transition state inside reparandum state 
advantage crf method probabilistic model provides principled way represent information heuristic rules 
features crf method grams words pos tags turn change su detection task features maxent ip detection model su detection 
edit detection results different models edit word ip detection compared table cts data 
results shown tasks nist error rate 
condition crf better finding edit words poorer ip detection compared hmm maxent methods 
ties models trained hmm maxent trained detect ips heuristic rules may find correct onset reparandum crf trained jointly detect edit words ips may trained ip detection 
stt condition observe crf approach outperforms maxent hmm methods edit word edit ip tasks suggesting crf degrades edit ip detection task stt condition 
probably due fact edit word ip detection mutually beneficial joint detection approach 
table results nist error rate edit word ip detection hmm maxent crf approaches recognition output conditions cts data 
cts approaches edit word edit ip ref stt ref stt hmm maxent crf table shows results bn edit word detection transcription recognition output conditions hmm maxent approaches 
crf bn data due computational re crf 
addition edit disfluencies infrequent bn 
maxent approach yields better results edit word ip detection hmm 
similar previous findings performance degrades severely stt con dition compared transcriptions error rate increase observed cts edit word ip detection tasks 
table results nist error rate edit word ip detection hmm maxent approaches 
chapter summary bn approaches edit word edit ip ref stt ref stt hmm maxent described investigation new data new tasks new approaches structural event detection rt evaluation 
experiments new rt data su boundary detection consistent previous findings 
addition applied approaches su detection maxent crf tasks su subtype detection edit word detection 
results shown maxent crf outperform prior hmm edit word detection 
crf approach edit word detection avoids ad hoc rules hmm maxent approaches allows features easily incorporated 
step method su su subtype detection yields reasonable baseline sys tem performance additional research needed develop effective features task including textual utterance level prosodic features 

related efforts previous chapters described research related component structural event detection system prosody model language model combination 
chapter investigate factors affect system performance including word error rate recognition output different meth ods automatically derive speaker change information 
addition conduct preliminary experiment investigate acoustic prosodic features word fragment detection 
task currently defined ears program 
accurate identification word fragments helpful edit word detection possibly improve speech recognition performance 
chapter organized follows 
section investigate factors impact system performance structural event detection 
section describe preliminary experiments word fragment detection 
summary chapter appears section 
factors impacting performance word error rates wer observed decreased accuracy structural event detection system testing stt output compared human transcriptions largely due recognition errors 
understand just wer affects performance consider stt outputs recognition systems 
ta ble shows su edit word detection results different stt systems cts bn corpora 
wer stt system indicated 
comparison show results transcription essentially wer 
structural event detection models trained tested rt data set see table 
su detection system major ity vote hmm maxent crf approaches cts combination hmm maxent approaches posterior probability interpolation 
edit word detection system crf approach cts maxent model bn described section 
table su edit word detection results nist error rate cts bn ref various stt conditions rt data 
su detection results reported su boundary detection error 
stt stt different stt outputs wer shown table 
conditions wer su boundary edit word ref cts stt stt ref bn stt stt seen table system performance degrades accurate recognition output 
experimental results show word errors negative impact edit word detection su detection 
relationship wer structural event detection performance appears non linear especially edit word detection 
edit word detection better stt accuracies slightly improve performance large gap best stt output condition 
suggests stt output errors occur region edit disfluencies word errors bigger impact edit word detection task 
recall main dif ference transcription stt output edit word detection word fragment information available 
lack word fragment knowledge greatly impacts edit word detection stt condition 
su detec tion system performance clearly impacted different 
sense intuitively sentence initial final words greater effect system performance su detection 
additionally deletion errors stt output occur short sus backchannels severe impact su detection deletion errors occur middle utterance 
speaker label su detection pointed earlier speaker change useful information detecting su boundaries 
speaker change affects prosody model prosodic feature set single feature derive features related turn 
looking feature usage decision trees su detection table table notice features related speaker change trees 
hmm speaker change affects lm chunk word string speaker sequences continuous speech speaker concatenated sequence hidden event lm applied 
reasonable lm hypothesize su word sequence speaker change 
cts bn processed differently derive speaker turn change information 
cts speech recorded separate channels channel corresponds speaker channel needs considered find speaker change 
speech channel segmented places long pause segments channels sorted time 
segment speech segment comes channel speaker change tag recorded speech segment 
shows speaker change information obtained cts 
segmentations speaker segmentations speaker segmentations sorted starting time speaker change added places marked arrow segment speaker segment speaker 
note lot overlapping conversational speech channel simply approximation speaker change 
speaker speaker fig 

illustration speaker change obtained cts data 
arrow represents speaker change segment 
bn channel speaker information unavailable automatic speaker labeling needed identify speaker change 
investigated different approaches generate speaker labels pause segments described section 
note investigation conducted test set 
automatic speaker clustering method speech recognition 
automatic clustering approach groups similar speech segments fea ture normalization speaker adaptation 
grouped segments cluster id speaker label structural event detection tasks 
speaker important task supported ears mde program 
goal add labels regions speech signal rep sources particular speaker music noise 
icsi speaker subsystem generates associated speaker la chunks speech 
algorithm splits speech clusters generally greater expected number speakers automatically clusters chunks speech metric similar bayes information met ric likelihood increase 
features system mfcc plp features depending different broadcast shows 
speaker labels segments event detection tasks obtained speaker system output 
note pause segments align chunks speech corresponding different speakers system output 
segment may contain multiple speakers speaker results 
case speaker majority speech segment chosen speaker label event detection tasks 
table compares different methods derive speaker labels bn su detection task rt test set 
results reported condition improved hmm system shown table 
observe significant improvements speaker labels derived speaker output suggesting automatic speaker clustering appropriate structural event detection 
goal automatic clustering cluster similar speakers acoustic similarity purpose recognizing words concerned providing correct speaker label 
table comparisons different ways derive speaker labels rt test set bn su boundary detection task 
results shown nist error rate hmm transcription condition 
bn speaking labeling methods su error rate automatic clustering stt far speaker results derive speaker labels pause segments speech 
conducted experiment uses speaker results segment speech pause segmentation 
yields worse results pause seg ments speaker label assigned speaker results 
speaker algorithm uses acoustic information hypothesizes speaker change middle continuous phrase speaker increase su detection errors 
joint model speaker recognition speech recognition structural event detection important direction 
word fragment detection section describe preliminary experiments related detection word fragments evaluated structural event detection task ears program 
word fragment called partial word occurs speaker cuts speaking middle word 
word fragments indicate presence disfluencies current speech recognizers detect important information lost disfluency detection 
accurate word fragment detection important speech recognition 
word fragments occur frequently spontaneous speech indicators speech disfluencies 
levelt percentage disfluencies contain word fragment pattern description task dutch reported casual conversations british english bear atis corpus 
examined conversations switchboard data disfluencies contain word fragments 
accurate identification word fragments speech recognizer unsolved problem 
cases simply treated vocabulary words incorrectly recognized words vocabulary 
affects accurate recognition neighboring words fails provide important information word fragment important detecting interruption point disfluency 
example human transcription speech recogni tion output switchboard corpus human transcription just know just eating sort eat recognizer output just see just eating sort need bird see recognition output word fragment incorrectly recognized words vocabulary 
additionally due failure identify word fragment extremely difficult identify disfluency recognition results 
study word fragments conducted disciplines 
psy linguists suggest speakers rarely interrupt word correct 
word complete speakers committing correctness moment 
linguists considered problem pro duction point view computational linguistics investigated problem goal better speech recognition disfluency detection 
noted beat knowledge location word fragments able cue detection correction disfluencies 
heeman allen proposed integrated model detection speech repairs incorporates word fragments important feature 
nakatani hirschberg proposed speech model detection speech repairs acoustic prosodic cues 
presence word fragments important indicator speech repairs prosodic acoustic features silence duration en ergy pitch 
analyzed properties word fragments example distribution fragments syllable length distribution initial phonemes fragments acoustic cues coarticulation fragments 
role word fragments indicator disfluencies em address problem detect occurrence word fragments suggest word language model word fragment detection effective 
observed atis corpus speaker stops middle word resumes speaking changed inserted words repetition pause lasts ms examples remaining examples having pause second duration 
interrupted words completion vowel intended word syllable speaker stops uttering consonant 
word fragments play important role disfluency processing spontaneous speech identification word fragments tackled speech community 
infeasible treat word fragments regular words including partial words dictionary 
furthermore may quite difficult train model cover word fragments due variability possible partial words 
rose riccardi modeled word fragments single word fragment symbol frag system may help 
system improved explicitly modeling word fragments filled pauses non speech events directly report impact modeling word fragments 
investigate problem word fragment detection speech analysis 
goal study identify reliable acoustic prosodic features word fragment detection 
acoustic prosodic features hypothesis indicative prosodic cues voice quality char boundary word fragments approach extract variety acoustic prosodic features build classifier features automatic identification word fragments 
prosodic features structural event detection chapter 
addition speaker suddenly stops mid word voice quality change new set voice quality related features investigated word fragment detection 
human speech sounds commonly considered result combination sound energy source modulated transfer filter function determined shape vocal tract 
vocal cords open close air flow glottal opening 
frequency pulses determines fundamental frequency source contributes perceived pitch produced sound 
voice source important factor affecting voice quality investigation focuses voice source characteristics 
analysis voice source done inverse filtering speech waveform analyzing spectrum directly measuring mouth non pathological speech 
widely model voice source fant lf model 
research shown intensity produced acoustic waveform depends derivative glottal flow signal amplitude flow 
important representation glottal flow open quotient oq 
oq defined ratio time vocal folds open total length glottal cycle 
spectral domain empirically formulated oq log amplitudes second harmonics spectrum 
different types modal voicing voicing voicing differ amount time vocal folds open glottal cycle 
modal voicing vocal folds closed half glottal cycle 
voicing vocal folds held loosely resulting short open quotient 
voicing vocal folds contact open relatively long portion glottal cycle 
think possible voicing word fragment occurs 
word fragment detection task voice quality related features investigated 
jitter measure perturbation pitch period speech identify pathological speech value represents jitter percent lower bound abnormal speech 
value jitter obtained speech analysis tool 
pitch analysis sound converted point process represents sequence time points case times associated pitch pulses 
periodic jitter value defined relative mean absolute third order difference point process second order difference interval process 
jitter ti ti ti ti ti ith interval number intervals point process 
sequence intervals durations tween shortest period longest period result undefined 
spectral tilt slope spectrum speech instrument signal 
speech responsible prosodic features accent speaker modifies tilt raising slope spectrum vowel put stress syllable 
voice amplitude harmonics spectrum drops quickly frequency increases modal spectra voice greater slope voice 
spectral tilt measured decibels octave 
linear approximation spectral envelope measure spectral tilt 
oq defined equation 
derived difference am second harmonics spectral envelope speech data 
studies shown difference har oq reliable way measure relative 
voice larger oq voice 
approximation second harmonics spectrum 
experiments experimental setup goal identify reliable acoustic prosodic features word fragments 
similar event detection tasks chapter task word fragment identi fication viewed statistical classification problem word boundary classifier determines word boundary word fragment acoustic prosodic features 
part switchboard corpus experiments 
human transcriptions word fragments identified words word fragments 
data training data remaining testing 
boundary location prosodic features voice quality measures extracted described previous section 
decision tree classifier trained downsampled training set contains samples tested downsampled test set samples samples training test set word fragments 
see www icsi berkeley edu thesis fragment html information conver study 
experimental results table shows confusion matrix results classification task word fragment versus complete words downsampled data 
precision recall fragment detection task respectively 
accuracy test samples significantly better chance performance 
results suggest acoustic prosodic features effective word fragment detection 
table word fragment detection results confusion matrix downsampled data switchboard corpus 
hypothesis complete fragment complete fragment inspection decision tree feature usage results reveals effective features distinguishing word fragments complete words 
table reports feature usage word fragment detection task 
shows pruned decision tree task 
voice quality features jitter queried decision tree 
due fact speaker suddenly cuts middle word abnormality vocal fold pitch period captured jitter 
average oq avg oq chosen useful feature suggesting mid word interruption generates voice 
questions produced decision tree show word fragments hypothesized answer positive questions jitter average oq average oq 
speech attributes abnormal voice quality 
conducted classification experiments features jitter average oq classification accuracy obtained 
table feature usage word fragment detection switchboard data 
feature usage jitter energy pattern boundary win diff avg oq turn cnt pau dur table observe energy feature feature queried frequently 
may need careful interpreting prosodic features word fragments missing undefined value stylized energy features due short duration unvoiced frames 
example leaf decision tree word fragment hypothesized energy slope boundary undefined value shown question energy pattern boundary xr xf means undefined value 
notice usage pause feature low pause expected sudden closure speaker 
reason recognizer generate pause phonetic alignments pause mid word interruption short 
example thirds word fragments training test set followed pause alignments 
additionally places sentence boundaries filled pauses followed pause 
jitter energy pattern boundary rf fr fx rr ff rx win diff avg oq turn cnt fragment energy pattern boundary rf fx xr ff xf jitter jitter fragment energy pattern boundary fr rr rx fragment turn cnt avg oq fragment avg oq avg oq fragment win diff energy pattern boundary xr xf fragment jitter fragment win diff fragment win diff fragment energy pattern boundary rf fx rr ff rx pau dur pau dur fragment energy pattern boundary fr xr xf fragment fig 

pruned decision tree detect word fragments 
decision leaf nodes decision internal node tree shown 
followed pause accurately distinguish word fragment complete words 
chapter summary investigated factors impact structural event detection system performance including wer speech recognition results speaker labeling approaches 
better recognition output greater impact su detection edit word detection suggesting word errors may occur edit disfluency region 
difference bn cts suggests edit word detection relatively easier bn human transcription condition cts different styles corpora 
comparison different speaking labeling methods bn su detection task shown speaker labels derived speaker system appropriate automatic speaker clustering speech recognition 
accurate identification word fragments helpful disfluency detection algorithm occurrence word fragments indicator speech disfluencies 
investigated problem word fragment detection acoustic prosodic features 
preliminary experimental results show acoustic prosodic features provide useful information word fragment detection 
approximation characterization word fragments acoustic prosodic cues find results encouraging 
offer alternative approach build acoustic models suggest speech analysis quite relevant building speech recognition systems capable recognizing fragments 
experiments word fragment detection preliminary 
investigations large corpora sophisticated versions measurements especially voice quality measurements 
additionally experiments conducted downsampled data set due highly skewed data distribution 
current word fragment detection method generate false alarms real test situation non downsampled data 
investigate performance algorithm applying directly speech recognition results 

final remarks impact research efforts research thesis impact related research efforts section discuss research efforts benefited 
uses structural event detection system output improve speech recognition modifying speech segments 
involve applying tech niques developed thesis corpora multimodal corpus multiparty meeting corpus 
structural event information word recognition theoretically joint model needed recognize words structural events simultaneously 
due lack general framework purpose utilized step approach investigating impact structural event information speech recognition 
believe linguistic segments better acoustic segments speech recognition 
example lm rescoring intuitively initial word sentence dependent final words previous sentence 
information sentence boundaries particularly beneficial lm 
currently lm treats acoustic segment speech recognition sentence 
error analysis showed errors near sentence boundaries higher places 
motivates investigation su boundary information generating better segments improve recognition accuracy 
speech speech recognizer segment concatenate stt output segments su detection re recognition su hypothesis fig 

su information re recognition bn 
final stt output shows su information fed back recognizer step approach 
su detection system applied recognition output 
speech hypothesized sus word alignments 
see details segmentation step 
new segments pass speech recognition 
evaluations conducted rt development set bn data 
ta ble shows wer re segmenting speech su detection results re recognition 
results shown sus system gen erated sus 
su detection system improved hmm system 
shown table baseline recognition results acoustic segments obtained speech non speech detector 
recognizer experiments simpli fied version sri bn recognizer 
observed table su information chop speech yields better segments better recognition performance 
sus results better recognition accuracies automatically detected sus due su insertion deletion errors 
results suggest linguistic segments provide important alternative acoustic segments recognition 
highlights importance icsi 
result included show impact system 
half rt test set discussed table 
interaction structural event detection speech recognition 
result quite preliminary small testing set additional investigation needed 
additionally loosely coupled approach run recognition detect structural events recognition results re run recognition structural event hypotheses 
tightly coupled approach result better performance 
table wer su information fed back re segment speech compared baseline acoustic segments evaluating half rt bn data 
wer sus automatically generated sus baseline acoustic segments su detection multi modal corpus generalization cts models su detection model applied wombat data set collected investigate multimodality dialog see cs wright edu 
audio digitally recorded unidirectional boom mounted microphone placed fixed distance interlocutor somewhat noisy laboratory environment 
dialog concerns development plan catch family intelligent taken theater town order send back australia 
transcripts force aligned audio signal hand corrected 
speech annotated ldc annotation guideline cts bn corpora 
task domain differs cts bn involves development plan audio far noisier 
dialogs distinct participants 
recording minutes speech 
models rt cts data similar multimodal corpus available modeling training 
domain mismatch training test conditions 
reason chose cts corpora involve conversational speech 
reflected percentage sus wombat data cts data roughly interword boundaries su boundaries 
table shows su boundary detection results task hmm maxent approaches combination 
note conducted transcription 
observe similar patterns cts transcription condition table error greater 
noisier recording conditions new task domain challenges textual prosodic knowledge sources 
table su detection results nist error rate wombat data 
note combined result shown textual information order results parallel results chapter table table 
hmm maxent combination textual prosodic textual combining speech features gesture features thesis recorded speech data structural event detection 
humans mode gesture eye gaze convey information better understand 
hmm applied multimodal wombat corpus prosodic infor mation gestural information lexical cues finding sus 
experiments confirmed knowledge source provides additional information combination achieves best performance 
similarly prosody model sampling bagging techniques building gestural model proven effective 
preliminary experiments shown prosodic features gestural features combine features jointly modeled decision trees loosely combined interpolating posterior probabilities decision trees source 
sug modeling approach developed su detection speech effectively extended knowledge sources 
dialog act detection meeting corpus growing interest automatic processing multiparty meet ings 
common goals addition word recognition include automatic browsing retrieval question answering summarization 
tasks require seg continuous speech functional units dialog acts das 
das similar sense sus investigated conversational speech 
explore da boundary detection subtype detection icsi meeting corpus includes naturally occurring meetings con taining roughly hours speech data associated human generated transcriptions 
corpus hand annotated dialog acts described detail 
grouped various da labels broad categories statements questions backchannels fillers disruptions 
meeting data recognized sri recognizer trained cts data yielded wer entire meeting corpus 
corpus split meetings training development remaining meetings testing 
joint lei chen purdue university 
joint jeremy ang icsi 
hmm described chapter da boundary detection similarly su boundary detection 
experiment single pause feature prosodic feature single decision tree trained prosody model 
hidden event lm trained model joint word da event sequence 
table shows da boundary detection results human transcription stt output 
word boundaries da comparable percentage sus cts 
seen prosody model contributes stt condition combined results condition cts su detection results 
table da boundary detection results nist error rate icsi meeting data 
results transcriptions ref stt output pause decision tree pause dt model hidden event lm hmm combination 
pause dt lm hmm combination ref stt maxent classifier su subtype detection built da sub type detection 
lexical features su subtype detection task 
experiment prosodic features duration pitch energy extracted da unit su subtype de tection task focuses features associated word boundary 
table shows da subtype classification accuracy results da boundaries human transcriptions recognition output 
accu racy measured percentage das labeled correct class 
results shown word features combined da boundaries recognition output generated aligning recognition words transcriptions plus da annotations 
word features binned posterior probabilities decision tree 
seen classification accuracy significantly better chance perfor mance incorporating prosodic information improves classification accuracy 
see details automatic da boundary detection uses similar step approach su su subtype detection described chapter 
table da subtype classification accuracy da boundaries icsi meeting corpus human transcriptions recognition output 
conditions word features combined word features binned posterior probabilities decision tree dt 
chance performance obtained majority type statement hypothesized da 
chance word features word dt posteriors ref stt summary experiments systematic study conducted automatic detection structural events speech su edit disfluency filler detection su disfluencies main focus 
experiments conducted corpora conversational speech broadcast news speech types transcriptions human transcriptions speech recognition output 
experiments show speakers prosodic cues resolve ambiguities speech signal su identify interruption point utterance mark discourse structure 
prosodic features provide valuable knowledge source automatic structural event detection different prosodic features effective different corpora speaking styles 
additionally performance prosody model varies different structural event detection tasks different event distributions inherent characteristics tasks 
textual information important knowledge source detect ing structural events 
hidden event lm performed reasonably tasks 
word gram extended include textual cues pos automatically induced classes syntactic chunks 
lexical fea tures remain investigated parsing information 
repetition pattern detector developed edit disfluency detection 
generally word lm superior prosody model structural event detection tasks 
combination prosody model lm usually outperforms individual model suggesting importance integrating multiple knowledge sources improving system performance 
prosody model lms de grade recognition output due word errors incorrect phone alignments 
lm greater relative error increase recognition output human transcriptions prosody model 
factors impact event detection performance studied including recognition error rate different speaker labeling methods 
investigated imbalanced data set problem encountered training prosody model 
variety sampling bagging methods evaluated su boundary detection task 
classification accuracy performance measure original training set yields best results prosody model performance metric roc minority class deemed interest sampling methods important 
bagging generates multiple classifiers reduces variance significantly improves system performance 
studies su ip tasks highlighted inherent differences respect machine learning methods 
modeling approaches compared combining knowledge sources hmm maxent crf approaches su detection 
hmm generative approach maxent crf discriminative models directly es posterior probabilities events observations textual prosodic features 
maxent approach models local information hmm crf able model entire sequence 
maxent crf better integrating overlapping textual information currently binned posterior probabilities prosody model ignore fine grained knowledge prosody possible reason performance degrades stt conditions 
hmm effective prosody model jointly model various textual features 
model combining approaches generally achieves best performance 
approaches examined edit disfluency detection 
hmm maxent detect interruption points apply heuris tic rules determining onset reparandum 
crf provides principled way incorporate knowledge sources probabilistic way detecting extent reparandum avoiding ad hoc rules hmm maxent methods 
contributions contributions thesis fold 
systematic comprehensive investigation structural event detection speech conducted 
includes finding indicative prosodic fea tures effectively textual information developing better meth ods constructing model combining different models prior 
investigations different corpora human tions recognition output types structural events enabled create better models structural events 
thesis emphasizes importance knowledge disciplines improve spontaneous speech event processing 
machine learning techniques crucial training improved prosody models imbalanced data set combining multiple classifiers 
second methods natural language processing important effectively exploiting textual information 
speech analysis measurements offer additional valuable features extending prosodic feature set 
knowledge disciplines beneficial ultimate goal better modeling speech events 
research done thesis proven helpful related research efforts 
preliminary study shown structural event put improve speech recognition accuracy 
ongoing research attempts tightly integrate event information statistical lm speech recog nition 
methods developed thesis successfully applied corpora prosodic gestural features combined hidden event lm su detection multimodal corpus su su subtype detection approach successfully utilized dialog act detection multiparty meeting corpus 
important directions structural event detection speech 
directions relevant research thesis listed 
additional acoustic prosodic features evaluated effectiveness detecting different structural events 
long span features capture mental information important direction 
speaker dependent modeling interesting important avenue improving models different speakers different speaking styles 
example speakers pause sus specific discourse markers 
word dependent prosody model may helpful discourse marker detection 
direction develop better modeling approach prosody model 
extensively investigated imbalanced date set problem training decision tree prosody model 
directions include machine learning techniques support vector machines shown great success applications implement classifier methods effectively combine multiple classifiers 
textual information direction include syntactic structure knowledge sources 
simple gram lms pos chunk capture higher level information 
error analysis insertion errors phrase boundaries 
parse structure information helpful eliminating insertion errors 
addition maxent crf approaches proven effective modeling correlated textual features currently binned prosody model posterior probabilities 
methods direct incorporation prosodic features maxent crf approaches need investigated 
speech recognition errors significant impact system perfor mance examining confusion networks word lattices leverage multiple recognizer hypotheses may improve performance struc tural event detection speech recognition 
tightly coupled framework structural event detection speech recognition final important direction detection events 
involves creating better models 
believe data driven approach thesis important building automatic structural event detection systems measurement studies important avenue investigating features important detecting events 
structural event detection important bridge links speech recognition downstream language processing modules 
investigating impact structural event detection downstream applications parsing machine translation summarization information extraction important di 
list list jones wolf gibson williams reynolds 
measuring readability automatic speech text transcripts 
proceedings european conference speech communication technology pages 
gregory johnson charniak 
sentence internal prosody help parsing way punctuation 
proceedings human language technology conference north american chapter association computational linguistics annual meeting 
kahn ostendorf chelba 
parsing conversational speech enhanced segmentation 
proceedings human language technology conference north american chapter association computational linguistics annual meeting 
campbell 
durational cues prominence grouping 
proceedings workshop prosody pages lund sweden 
bard 
recognizing disfluencies dialog 
proceedings international conference spoken language processing pages 
de 
perceptual strength prosodic boundaries relation suprasegmental cues 
journal acoustical society america october 
hirst 
peak boundary cohesion characteristics prosodic grouping 
proceedings workshop prosody pages lund sweden 
price ostendorf fong 
prosody syntactic disambiguation 
journal acoustical society america 

prosodic disambiguation automatic speech understanding thai 
phd thesis purdue university 
scott 
duration cue perception phrase boundary 
journal acoustical society america 

prosodic features discourse boundaries different strength 
journal acoustical society america january 
shriberg stolcke tur tur 
prosody automatic segmentation speech sentences topics 
speech communication pages 
nakatani hirschberg 
corpus study repair cues spontaneous speech 
journal acoustical society america pages 

prosody speech understanding system 
springer verlag 
gotoh renals 
sentence boundary detection broadcast speech transcripts 
proceedings isca workshop automatic speech recognition challenges new millennium asr pages 
kim woodland 
prosody combined system punctuation generation speech recognition 
proceedings european conference speech communication technology pages 
christensen gotoh renal 
punctuation annotation statistical prosody models 
isca workshop prosody speech recognition understanding 
shriberg stolcke 
prosody decision tree model disfluency detection 
proceedings european conference speech communication technology pages 
meteer iyer 
modeling conversational speech speech recognition 
proceedings conference empirical methods natural language processing 
palmer hearst 
adaptive sentence boundary disambiguation 
proceedings fourth acl conference applied natural language processing pages 
reynar ratnaparkhi 
maximum entropy approach identifying sentence boundaries 
proceedings fifth conference applied natural language processing washington pages 
schmid 
unsupervised learning period disambiguation tokenization 
university stuttgart internal report 
walker clements darwin 
sentence boundary detection comparison paradigms improving mt quality 
proceedings mt summit viii santiago de 
beeferman berger lafferty 
lightweight punctuation annotation system speech 
proceedings international conference acoustics speech signal processing 
stevenson gaizauskas 
experiments sentence boundary detection 
proceedings north american chapter association computational linguistics annual meeting pages 

comparison syntactic prosodic phrasing 
proceedings european conference speech communication technology 
abney 
chunks dependencies bring processing evidence bear syntax 
computational linguistics foundations linguistic theory pages 
silverman beckman ostendorf price pierrehumbert 
tobi standard labeling english prosody 
proceedings international conference spoken language processing pages 
chen 
speech recognition automatic punctuation 
proceedings european conference speech communication technology pages 
stolcke shriberg bates ostendorf tur lu 
automatic detection sentence boundaries disfluencies recognized words 
proceedings international conference spoken language processing pages 
shriberg stolcke 
prosody modeling automatic speech recognition understanding 
proceedings workshop mathematical foundations natural language modeling 
ang shriberg stolcke 
prosody automatic detection annoyance frustration human computer dialog 
proceedings international conference spoken language processing pages 
stolcke ries shriberg bates jurafsky taylor martin van ess meteer 
dialogue act modeling automatic tagging recognition conversational speech 
computational linguistics 
huang zweig 
maximum entropy model punctuation annotation speech 
proceedings international conference spoken language processing pages 
national institute standards technology 
rt workshop agenda presentations 
www nist gov speech tests rt rt fall presentations november 
wang narayanan 
multi pass linear fold algorithm sentence boundary detection prosodic cues 
proceedings international conference acoustics speech signal processing 
dell 
spreading activation theory retrieval sentence production 
psychological review pages 
levelt 
speaking intention articulation 
cambridge ma mit press 
mackay 
structure words syllables evidence errors speech 
cognitive psychology 
oviatt 
predicting spoken disfluencies human computer interaction 
computer speech language 
shriberg 
preliminaries theory speech disfluencies 
phd thesis university california berkeley 
clark 
repeating words spontaneous speech 
cognitive psychology pages 

missing disfluencies 
proceedings international congress phonetics sciences pages 
brennan 
listeners compensate disfluencies spontaneous speech 
journal memory language 
fox tree 
effects false starts repetitions processing subsequent words spontaneous speech 
journal memory language 
chen harper quek 
gesture patterns speech repairs 
proceedings international conference multimodal interfaces 

age dependent types frequency disfluencies 
proceedings disfluency spontaneous speech workshop pages 
leon bloom brennan 
disfluency rates conversation effects age relationship topic role gender 
language speech 
bear dowding shriberg 
integrating multiple knowledge sources detecting correction repairs human computer dialog 
proceedings annual meeting association computational linguistics pages 
charniak johnson 
edit detection parsing transcribed speech 
proceedings north american chapter association computational linguistics annual meeting pages 
johnson charniak 
tag noisy channel model speech repairs 
proceedings annual meeting association computational linguistics 
core schubert 
syntactic framework speech repairs disruptions 
proceedings annual meeting association computational linguistics pages 
zechner 
automatic summarization spoken dialogues unrestricted domains 
phd thesis carnegie mellon university 
bosch 
memory disfluency chunking 
proceedings disfluency spontaneous speech workshop pages 
stolcke shriberg 
statistical language modeling speech disfluencies 
proceedings international conference acoustics speech signal processing 
heeman allen 
speech repairs intonational phrases discourse markers modeling speakers utterances spoken dialogue 
computational linguistics 
levelt cutler 
prosodic marking speech repair 
journal semantics 
cutler ladd editors 
prosody models measurement chapter speakers 
conceptions function prosody pages 
heidelberg springer 
bard 
disfluencies 
proceedings european conference speech communication technology 
bard 
listeners detect disfluency spontaneous speech 
language speech pages 
hindle 
deterministic parsing syntactic 
proceedings annual meeting association computational linguistics pages 
shriberg 
phonetic consequences speech disfluency 
proceedings international conference phonetics sciences pages 

juncture cues disfluency 
proceedings international conference spoken language processing 

prosodic features types disfluencies 
proceedings disfluency spontaneous speech workshop pages 

analysis automatic recognition false starts spontaneous speech 
proceedings international conference acoustics speech signal processing pages 
dorr schwartz 
lexically driven algorithm disfluency detection 
proceedings human language technology conference north american chapter association computational linguistics annual meeting 
levelt 
monitoring self repair speech 
cognition pages 
kai nakagawa 
investigation unknown word processing strategies spontaneous speech understanding 
proceedings european conference speech communication technology pages 
goto 
real time filled pause detection system spontaneous speech recognition 
proceedings european conference speech communication technology pages 
siu ostendorf 
modeling disfluencies conversational speech 
proceedings international conference acoustics speech signal processing pages 

simple metadata annotation specification 
linguistic data consortium 
darpa information processing technology office 
effective affordable reusable speech text ears 
www darpa mil programs ears 

simple metadata annotation specification 
linguistic data consortium 
hand 
construction assessment classification rules 
john wiley sons chichester 
bradley 
area roc curve evaluation machine learning algorithms 
pattern recognition 
duda hart 
pattern recognition scene analysis 
new york john wiley sons 
ostendorf 
scoring structural mde meaningful error rates 
ears rich transcription workshop 
stolcke bratt franco ramana rao gadde shriberg weng zheng 
sri march hub conversational speech transcription system 
proceedings nist speech transcription workshop college park md may 
ferrer 
prosodic features switchboard database 
technical report sri international 
shriberg weintraub 
modeling dynamic prosodic variation speaker verification 
proceedings international conference spoken language processing pages 
breiman friedman olshen stone 
classification regression trees 
pacific grove ca wadsworth brooks 
caruana 
ind version recursive partitioning 
nasa ames research center moffett field ca 
stolcke shriberg 
automatic linguistic segmentation conversational speech 
proceedings international conference spoken language processing pages 
rabiner juang 
hidden markov models 
ieee assp magazine january 
bishop 
neural networks pattern recognition 
cambridge university press cambridge uk 
kim 
automatic detection sentence boundaries disfluencies conversational fillers spontaneous speech 
master thesis university washington 
sankar stolcke 
acoustic modeling sri hub partitioned evaluation continuous speech recognition system 
proceedings darpa speech recognition workshop pages 
liu 
metadata extraction rich transcription speech 
technical report purdue university electrical computer engineering department 
wang harper 
language model investigating effectiveness tightly integrating multiple knowledge sources language modeling 
proceedings conference empirical methods natural language processing pages 
chelba 
exploiting syntactic structure natural language modeling 
phd thesis john hopkins university 
chen goodman 
empirical study smoothing techniques language modeling 
technical report harvard university computer science group 
ney 
improved backing gram language modeling 
proceedings international conference acoustics speech signal processing pages 
jelinek 
statistical methods speech recognition 
mit press 
bell cleary witten 
text compression 
prentice hall 
jelinek 
self organized language modeling speech recognition 
alex waibel kai fu lee editors readings speech recognition 
morgan kaufman publishers 
heeman 
pos tagging versus classes language modeling 
proceedings th workshop large corpus 
niesler woodland 
variable length category gram language model 
proceedings international conference acoustics speech signal processing pages 
johnson 
joint conditional estimation tagging parsing models 
proceedings annual meeting association computational linguistics 
wang 
statistical parsing language modeling constraint dependency grammar 
phd thesis purdue university 
wang stolcke harper 
linguistically motivated language model conversational speech recognition 
proceedings international conference acoustics speech signal processing 
bellegarda 
multi span language modeling framework large vocabulary speech recognition 
ieee transactions speech audio processing 
peters 
compact maximum entropy language models 
proceedings automatic speech recognition understanding workshop 
rosenfeld 
adaptive statistical language modeling maximum entropy approach 
phd thesis carnegie mellon university 
siu ostendorf 
variable ngrams extensions conversational speech language modeling 
ieee transactions speech audio processing 
brown pietra desouza lai mercer 
class gram models natural language 
computational linguistics pages 
brants 
tnt statistical part speech tagger 
proceedings th applied nlp conference pages 
www ldc upenn edu 
florian 
multidimensional transformational learning 
proceedings conference computational natural language learning pages july 
stolcke 
srilm extensible language modeling toolkit 
proceedings international conference spoken language processing pages 
chawla japkowicz 
workshop learning imbalanced datasets ii th international conference machine learning august 
chawla bowyer hall kegelmeyer 
synthetic minority sampling technique 
journal artificial intelligence research pages 
kubat matwin 
addressing curse imbalanced training sets 
proceedings international conference machine learning pages 

improving identification difficult small classes balancing class distribution 
technical report department computer information science university tampere finland 
kubat holte matwin 
learning negative examples abound 
proceedings european conference machine learning pages 
japkowicz stephen 
class imbalance problem systematic study 
intelligent data analysis 
provost fawcett 
robust classification imprecise environments 
machine learning 
lee 
noisy replication skewed binary classification 
computational statistics data analysis 
chan stolfo 
scalable learning non uniform class cost distributions case study credit card fraud detection 
proceedings fourth international conference knowledge discovery data mining pages 
ling li 
data mining direct marketing problems solutions 
proceedings fourth international conference knowledge discovery data mining pages 
weiss provost 
learning training data costly effect class distribution tree induction 
artificial intelligence research pages 
breiman 
bagging predictors 
machine learning 
shriberg 
spotting hotspots meetings human judgments prosodic cues 
proceedings european conference speech communication technology pages 
dietterich 
experimental comparison methods constructing ensembles decision trees bagging boosting randomization 
machine learning 
chawla moore hall bowyer kegelmeyer springer 
distributed learning bagging performance 
pattern recognition letters vol 
pages 
drummond holte 
class imbalance cost sensitivity sampling beats sampling 
proceedings icml workshop learning imbalanced datasets 
chawla 
imbalanced datasets investigating effect sampling method probabilistic estimate decision tree structure 
proceedings icml workshop class imbalances 
roli kittler editors 
multiple classifier systems 
springer 
roli kittler editors 
multiple classifier systems 
springer 
florian 
transformation learning fast lane 
proceedings north american chapter association computational linguistics annual meeting pages june 
berger della pietra della pietra 
maximum entropy approach natural language processing 
computational linguistics 
ratnaparkhi 
maximum entropy part speech tagger 
proceedings conference empirical methods natural language processing 
toutanova manning 
enriching knowledge sources maximum entropy part speech tagger 
proceedings conference empirical methods natural language processing 
lafferty mccallum 
maximum entropy text classification 
proceedings ijcai workshop machine learning information filtering 

chunking maximum entropy models 
proceedings conference computational natural language learning pages 
och ney 
discriminative training maximum entropy models statistical machine translation 
proceedings annual meeting association computational linguistics 
khudanpur wu 
maximum entropy language model integrating grams topic dependencies conversational speech recognition 
proceedings international conference acoustics speech signal processing 
borthwick 
maximum entropy approach named entity recognition 
phd thesis new york university 
curran clark 
language independent ner maximum entropy tagger 
proceedings conference computational natural language learning 
ng 
named entity recognition maximum entropy approach global information 
proceedings international conference computational linguistics 

maximum entropy word sense disambiguation system 
proceedings international conference computational linguistics 
klein manning 
conditional structure versus conditional estimation nlp models 
proceedings conference empirical methods natural language processing pages 
chen rosenfeld 
gaussian prior smoothing maximum entropy models 
technical report carnegie mellon university 
zhang 
maximum entropy toolkit 
www cn maxent toolkit html 
yang pedersen 
comparative study feature selection text categorization 
proceedings international conference machine learning pages 
dunning 
accurate methods statistics surprise coincidence 
computational linguistics 
lafferty mccallum pereira 
conditional random field probabilistic models segmenting labeling sequence data 
proceedings international conference machine learning pages 
sha pereira 
shallow parsing conditional random fields 
proceedings human language technology conference north american chapter association computational linguistics annual meeting 
li 
early results named entity recognition conditional random fields 
proceedings conference computational natural language learning 
peng mccallum 
accurate information extraction research papers conditional random fields 
proceedings human language technology conference north american chapter association computational linguistics annual meeting pages 
mccallum 
mallet machine learning language toolkit 
mallet cs umass edu 
wooters 
robust speaker clustering algorithm 
proceedings automatic speech recognition understanding workshop 

detecting disfluency spontaneous speech 
phd thesis university edinburgh 
rose riccardi 
modeling disfluency background events asr natural language understanding task 
proceedings international conference acoustics speech signal processing 
fant 
parameter model glottal flow 
stl pages 
fant 
lf model revisited transform frequency domain analysis 
stl pages 
fant 
voice source connected speech 
speech communication 
rosenberg 
effect glottal pulse shape quality natural vowels 
journal acoustical society america 

system doing phonetics computer 
www org 

time course vowels 
phd thesis ucla 
schwartz 
error analysis bn cts results 
ears stt workshop st thomas december 

broadcast news segmentation mde stt information improve speech recognition 
technical report international computer science institute 
stolcke speech text research sri icsi uw 
www nist gov speech tests rt rt spring presentations index htm 
chen liu harper shriberg 
multimodal modal integration sentence unit detection 
proceedings international conference multimodal interfaces 
janin baron edwards ellis morgan peskin shriberg stolcke wooters 
icsi meeting corpus 
proceedings international conference acoustics speech signal processing 
ang liu shriberg 
automatic dialog act segmentation classification multiparty meetings 
proceedings international conference acoustics speech signal processing 
shriberg dhillon ang 
icsi meeting recorder dialog act corpus 
proceedings workshop discourse dialogue 
dhillon meeting recorder project dialog act labeling guide 
technical report international computer science institute 
ostendorf stolcke liu shriberg 
improving automatic sentence boundary detection confusion networks 
proceedings human language technology conference north american chapter association computational linguistics annual meeting pages 
freund 
boosting weak learning algorithm majority 
information computation pages 
freund schapire 
experiments new boosting algorithm 
machine learning proceedings thirteenth national conference pages 
freund mason 
alternating decision tree learning algorithm 
proceedings international conference machine learning pages 
appendices appendices appendix adt boosting su ip detection chapter bagging ensemble techniques explored obtain robust classifiers reliable posterior probability event prosodic features 
bagging experiments largely computational efficiency 
section describe preliminary experi ments boosting method robust classifiers 
experiments placed appendix full structural event detection system 
adt boosting description freund schapire introduced adaboost adaptive boosting im prove classification performance combining multiple weak learning algorithms proven successful classification tasks 
boosting classifier built output classifiers focusing samples incorrect decisions 
boosting algorithm implemented updating weight sample training set 
contrast bagging boosting generates classifiers sequentially implemented par bagging 
generates classifiers different skewed distributions due different weights sample iteration may affect ability combine boosting results combinations lm 
boosting algorithm investigation imbalanced data set problem chapter due reasons 
freund mason proposed alternating decision tree adt learning algorithm boosting produces single tree generalization classical decision tree 
shows example adt tree 
node questions asked different features 
example node contain different decision questions share parent node 
different classical decision tree question asked node 
tree built way similar generated bayes option ind package 
fig 

example alternating decision tree adt 
experimental results adt boosting algorithms runs faster applied algorithm su boundary ip detection tasks 
adt boosting algorithm generate posterior probabilities class membership test sample downsampled training test set report results original test set prosody model combining prosody model lm need posterior probabilities provided trees 
table su ip detection results classification error rate adt learning algorithm bagging 
training testing conducted downsampled training testing set 
chance performance 
bagging boosting adt su ip investigation data section study su ip detection tasks 
model trained downsampled training set tested downsampled test set 
experimental results adt algorithm prosody model shown table 
results show adt boosting algorithm improves performance ip task su detection task compared performance bagging 
highlights difference su ip tasks suggesting metric reducing classification errors adt learning algorithm may better noisy ip task information gain classical decision tree learning investigate methods converting score adt learning algorithm posterior probability 
appropriate su task 
adt learning algorithm robust noisy data better exploiting information small training set 
adt boosting summary chapter shown inherent differences su ip detection tasks hand different class distributions hand differ effectiveness features tasks 
experiments boosting show tasks impacted differently learning algorithms 
preliminary results conducted downsampled data set 
additional investigation generation posterior probabilities adt boosting algorithm evaluation real test set combination lm needed 
appendix prosodic features abbreviated prosodic features appeared thesis ex section 
include prosodic feature set structural event detection 
details 
subset features shown reveals prosodic features correlated derived raw features different binning normalization methods applied 
note feature associated relative word indicated feature descriptions 

duration features pau dur duration pause word wi turn speaker turn change word wi gen gender speaker uttered word wi prev pau dur duration pause word wi rhyme dur ph bin binned value rhyme duration wi number phones rhyme wi rhyme dur ph nd bin binned value rhyme dur ph wi speaker mean str rhyme dur ph bin binned value stressed rhyme duration wi number phones stressed rhyme wi dur bin binned value vowel duration wi variance normalization dur bin binned value vowel duration wi mean normalization turn time time far current turn total duration current turn word dur duration word wi max phone dur avg phone dur max phone dur avg phone dur avg phone dur nsp max phone dur nsp avg vowel dur avg vowel dur duration features average maximum normalized phone duration word wi means variance speakers means mean normalization speakers means variance normalization current speaker nsp means mean normalization current speaker 
features pattern word feature consists sequence uv representing falling slope unvoiced region rising slope word wi pattern boundary pattern word word wi concatenated slope tag wi wrd diff log ratio minimum median filtered value wi wi win diff log ratio minimum median filtered boundary maximum value boundary frame window wi diff fitted value wi relative speaker baseline 
energy features energy win diff log ratio highest stylized energy value wi frame window current boundary wi energy pattern boundary energy slope wi concatenated energy slope wi vita education vita purdue university ph candidate electrical computer engineering department major advisor mary harper tsinghua university electrical engineering department selected publications journal papers 
yang liu chawla mary harper elizabeth shriberg andreas stolcke 
study machine learning imbalanced data sen tence boundary detection speech 
computer speech language submitted 

yang liu mary harper mike johnson jamieson 
effect pruning compression graphical representations output speech recognizer 
computer speech language 
conference workshop papers 
jeremy ang yang liu elizabeth shriberg automatic dialog act segmentation classification multiparty meetings appear proceedings international conference acoustics speech sig nal processing 

yang liu andreas stolcke mary harper elizabeth shriberg com combining generative posterior probability models advances sentence boundary detection speech proceedings conference empirical methods natural language processing 

yang liu elizabeth shriberg andreas stolcke mari os barbara peskin mary harper icsi sri uw metadata extraction system proceedings international conference spo ken language processing 

yang liu elizabeth shriberg andreas stolcke mary harper machine learning cope imbalanced classes natural speech ev sentence boundary disfluency detection proceedings international conference spoken language processing 

lei chen yang liu mary harper elizabeth shriberg multimodal model integration sentence unit detection proceedings ternational conference multimodal interfaces 

yang liu elizabeth shriberg andreas stolcke automatic ency identification conversational speech multiple knowledge sources proceedings european conference speech communi cation technology 

yang liu word fragment identification acoustic prosodic features conversational speech proceedings human language technology conference north american chapter association computa tional linguistics annual meeting student workshop 

wen wang yang liu mary harper rescoring effectiveness lan guage models different levels knowledge integration proceedings international conference acoustics speech signal processing 
professional activities 
student member ieee 
member association computational linguistics 
