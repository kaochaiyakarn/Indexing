automated modeling nonlinear axis scaling wu cmu cs may school computer science carnegie mellon university pittsburgh pa thesis committee christos faloutsos chair ailamaki andrew moore richard caruana cornell university submitted partial fulfillment requirements degree doctor philosophy 
copyright wu research sponsored national science foundation 
iis 
additional material support provided intel 
views contained document author interpreted representing official policies expressed implied intel government 
keywords scaling modeling feature selection dedicated family 
iv thesis examines nonlinear axis scaling impact modeling inter attribute relationships 
automated methods described system identifies possible scaling methods decides attributes serve inputs outputs builds regression trees quantify relationships 
experiments focus accuracy complexity models attempt quantitatively examine results consider applicability inherently qualitative task rule outlier anomaly detection 
results demonstrate nonlinear axis scaling automated system provide significantly accurate models compared unscaled case proportionally higher complexity costs help reveal unusual tuples unusual individual value combination thereof 
vi acknowledgments author owes debt advisor christos faloutsos including ailamaki anthony hal burch richard caruana chakrabarti elena jun gao george andrew moore robert murphy raymond ng wang floating membership cmu database group feedback counsel data helping get 
patience department noted faculty staff students 
author patience support family 
vii viii contents design questions fundamental questions 
related dimensionality reduction 
models 
model complexity 
spartan 

system design axis scaling 
reasons cumulative probability distribution functions choosing probability distributions 
integer variations 
model input selection 
evaluating input output assignment 
fractal dimension 
modeling 
complexity compression 
ix accuracy 
comparison spartan design 
scalability 
numerical methods 
inability predict number retained inputs 
complex tree construction process 
estimates 
experiments implementation 
usage 
impact nonlinear scaling 
abalone 
baseball 
building 
cia world factbook 
dj 
housing 
liver 
page blocks 
wind 
long take 
processing entire sets 
difficulty predicting times 
scalability 
discussion impact attribute selection 
impact resulting models 
model size 
errors 
findings 
doubly truncated normals mixtures truncated normals modified em 
windowing 
estimating fractal dimension box counting 
tug war 
regression trees construction 
pruning 
cross validation 
quick comparison 
linear models experiments 
abalone 
baseball 
building 
cia world factbook 
dj 
housing 
liver 
xi page blocks 
wind 
summary 
classification abalone 
liver 
page blocks 
bibliography xii list figures area population data edition cia world factbook 
area population data edition cia world factbook nonlinear axis scaling 
discarding outliers cropping sparsely populated extremes unscaled cia world factbook data 
round discarding outliers cropping 
ignore complexity risk approving dimensionality reduction modeling connecting dots single parameter plus complex curve approximate high dimensionality data set 
bayes net boolean attributes independent attributes probability tables exist conditioned conditioned conditioned markov blanket consists attributes parents child parent high level diagrams spartan described system 
axis scaling function applied population attribute cia world factbook data 
original distribution indicated marks highly skewed low values scaled range assigned small part unscaled domain 
sierpinski triangle 
log vs log sierpinski triangle 
xiii fractal dimension reduction fdr algorithm 
multiple state hill climbing 
pathological case attribute selector 
projections attributes equally uniform far sense input 
cia world factbook actual versus predicted area original space model generated scaled space 
cia world factbook actual versus predicted area scaled space relevant model built 
quantile error metric 
percentage point curves quantile error unscaled scaled cases weight attribute abalone set 
percentile means median quantile error maximum 
curve scaled case lies unscaled case differences minimal serious errors 
cases worst errors percentiles instance worse errors 
percentage point curves worst quantile errors unscaled scaled cases weight attribute set 
unscaled scaled cases worst errors quite high relative rest 
percentage point curves quantile error unscaled scaled cases rings attribute abalone set 
curves step functions rings distinct values 
abalone height versus weight 
bit general trend number extreme outliers 
percentage point curves quantile error unscaled scaled cases tb attribute baseball set 
large gap unscaled upper scaled lower curves show best worst case errors similar magnitude percentile scaled case lower errors 
instance quantile errors scaled case unscaled case threshold 
xiv percentage point graphs quantile error unscaled scaled cases solar attribute building set 
curve scaled case lower curve unscaled case 
quantile errors scaled case median quantile error unscaled case 
area population data edition cia world factbook 
advisable estimate relationship area population scaling 
area population data edition cia world factbook nonlinear axis scaling 
trend far perfect graph suggest broad positive correlation area population suggests existence extreme exceptions 
axis scaling annotations 
classify named regions outliers traditional cluster definition 
axis scaling annotations 
named points necessarily correspond extrema nations deviate general trend 
hong kong instance extremely densely populated 
percentage point curves quantile error unscaled scaled cases area attribute factbook set 
lower curve scaled quantile errors shows degree improvement approximately quantile errors scaled case fall median quantile error unscaled case 
daily median quantile error dj outputs unscaled scaled cases 
curve case stable usually lower unscaled curve occasional sharp spikes indicating substantial problems half models 
actual modeled prices honeywell hon left axis daily quantile error right axis 
curves diverge considerably times accounting high quantile errors noted taller impulses 
actual modeled prices honeywell hon scaled left axis daily quantile error right axis 
curves track time indicating prediction 
xv scaled model ip jpm 
observations indicated crosses modeled surfaces depicting regression tree 
actual modeled prices eastman kodak ek left daily quantile error right axis 
curves diverge significantly high impulses indicate substantial quantile errors 
actual modeled prices eastman kodak ek scaled left axis daily quantile error right axis 
periods significant divergence high quantile errors scaled model tracks somewhat better unscaled model shown 
ip jpm versus sbc scaled 
observations marked crosses tracked model 
ip versus sbc unscaled 
clearly needs ip model sbc 
ip jpm versus sbc unscaled 
model fairly accurate accurate scaled case shown 
ip versus intc unscaled 
needs ip model intc 
ip jpm versus intc scaled 
model surfaces track observations quite 
ip jpm versus intc unscaled 
model complex scaled case mixed results regarding accuracy median quantile error declined slightly sum squared quantile error increased 
percentage point curves quantile error unscaled scaled cases gm dj 
scaled case reflected lower curve produces lower errors unscaled case 
percentage point curves quantile error unscaled scaled cases dis dj 
lower curve scaled case shows lower quantile errors unscaled case 
percentage point curves quantile error unscaled scaled cases cat dj 
nearly coincident curves occur models approximately equally 
xvi percentage point curves quantile error unscaled scaled cases ek dj 
curves similar cases pronounced rise 
model especially 
percentage point curves quantile error unscaled scaled cases rad housing 
rad discrete values step function 
curves show scaled model lower quantile errors unscaled model 
percentage point curves quantile error unscaled scaled cases liver set 
curves diverge somewhat reflecting moderate improvements allowed scaling 
percentage point curves quantile error unscaled scaled cases blocks set 
curve scaled model nearly coincident percentile axis length shows extreme accuracy curve unscaled model shows opposite 
percentage point curves quantile error unscaled scaled cases blocks set 
lower curve scaled model reflects improved accuracy 
percentage point curves quantile error unscaled scaled cases clo attribute wind set 
coincident curves reflect models nearly identical performance 
percentage point curves quantile error unscaled scaled cases rpt attribute wind set 
improvements rpt appear come errors worse median lower curve corresponding scaled model coincident upper curve 
timing runs different sampling frequencies page blocks set nonlinear axis scaling 
error bars indicate minimum maximum timings 
sampling size bars reflect selection time basically compared modeling time modeling time total time 
total time appears scale linearly sample size 
xvii timing runs different sampling frequencies page blocks set nonlinear axis scaling 
error bars indicate minimum maximum timings 
individual bars reflect scaling selection modeling total time 
total time shows linear correlation sampling size 
timing runs different sampling frequencies dj set nonlinear axis scaling 
error bars indicate minimum maximum timings 
sampling size bars reflect selection time basically compared modeling time modeling time total time 
total time appears scale linearly sample size 
timing runs different sampling frequencies dj set nonlinear axis scaling 
error bars indicate minimum maximum timings 
individual bars reflect scaling selection modeling total time 
total time shows linear correlation sampling size shows probability distribution functions pdfs test estimated distributions 
case test distribution normal truncated percentiles 
cohen estimator gives similar results standard method moments normal levenberg marquardt estimated pdf essentially coincident test distribution 
shows probability distribution functions pdfs test estimated distributions 
case test distribution normal truncated percentiles 
cohen estimator gives similar results standard method moments normal levenberg marquardt estimated pdf matches test distribution 
shows probability distribution functions pdfs test estimated distributions 
case test distribution normal truncated percentiles 
cohen estimator normal estimator select significantly smaller standard deviations test data resulting density assigned mean levenberg marquardt estimator tracks test distribution 
xviii box counting algorithm 
tug war algorithm 
xix xx list tables errors abalone set nonlinear scaling 
bit costs reflect tree cost errors quantiles 
errors set nonlinear scaling 
bit costs reflect tree cost errors quantiles 
comparison results unscaled case show cases models quite general 
statistics baseball data 
errors baseball set nonlinear scaling 
emphasized text indicates better performance attribute modeled unscaled scaled cases 
trees single node small sets regression tree builder needs see substantial accuracy improvements accept split 
bit costs reflect tree cost errors cost inputs measured trees specifically chosen bit cost quantile metric 
errors building set nonlinear scaling 
bit costs reflect tree cost errors quantiles 
errors set nonlinear scaling 
modeled attributes continuous bit costs listed 
compare regarding solar 
errors dj set nonlinear scaling 
modeled attributes continuous bit costs listed 
bs steel acquired period data collection 
models range considerably size accuracy 
xxi errors dj set nonlinear scaling 
modeled attributes continuous bit costs listed 
compare table sum squared quantile errors lower case increase model complexity 
errors housing set nonlinear scaling 
baseball data small size set difficult justify splitting nodes regression tree 
note age 
errors set nonlinear scaling 
attributes modeled cases lower sum squared quantile errors corresponding gain model complexity additional nodes tax 
errors liver set nonlinear scaling 
trees single node expected set size tree pruning algorithm 
errors liver set nonlinear scaling 
trees 
comparing models mcv drinks significantly different results scaling allowed accurate model 
errors page blocks set nonlinear scaling 
class attribute original set broken binary attributes 
errors blocks set nonlinear scaling 
sum squared quantile attributes improved cases extremely instance compare area length wb trans table 
errors wind set nonlinear scaling 
modeled attributes continuous bit costs listed 
single node trees may reflect difficult set tuples pruning algorithm allow splits 
errors wind set nonlinear scaling 
modeled attributes continuous bit costs listed 
model complexity increased cases val bir sum squared quantile errors drop rpt kil dub mal 
timing results unscaled case 
times cpu times expressed seconds 
modeling phase obviously expensive portion implemented 
timing results scaled case 
times cpu times expressed seconds 
modeling phase extremely expensive compared rest 
increased costs versus table reflect greater dimensions resulting multiple versions scaled attributes size unscaled data set 
dimensions relate strictly original data attributes set ignore 
unscaled data number outputs number non ignored attributes number inputs 
size scaled data set 
dimensions relate strictly original data attributes set ignore year attribute wind data 
number possible outputs total number versions attributes scaled version input version trees built tested 
tallies attributes modeled unscaled scaled cases 
columns indicate version nonlinear scaling better approximately difference worse results rows model scaling fewer nodes unscaled case 
results shown totals data set 
cases scaling helps clearer considers case accurate complex models generally models accurate complexity 
quantile errors linear models abalone data set nonlinear axis scaling 
emphasized numbers indicate lower better sum squared quantile errors attributes modeled unscaled scaled cases 
quantile errors linear models baseball data set nonlinear axis scaling 
emphasized text reflects better scores 
quantile errors linear models building data set nonlinear axis scaling 
emphasized scores better unscaled scaled 
xxiii quantile errors linear models dj data set nonlinear axis scaling 
emphasized scores better unscaled scaled 
quantile errors linear models housing data set nonlinear axis scaling 
emphasized scores better unscaled scaled 
quantile errors linear models liver data set nonlinear axis scaling 
emphasized scores better unscaled scaled 
quantile errors linear models page blocks data set nonlinear axis scaling 
emphasized scores better unscaled scaled 
quantile errors linear models wind data set nonlinear axis scaling 
emphasized scores better unscaled scaled 
chapter thesis focuses automatically finding mathematical relationships attributes numerical data sets expressing accurate compact functions 
furthermore described system designed search mutually independent set attributes model inputs require user specify attributes modeled 
addition assume original axis scales appropriate model task uses uniformity heuristics identify suitable nonlinear axis scalings independent input dependent output attributes 
accurate models assist search interesting exceptions model outliers 
functions may directly prediction classification 
obvious example comes insurance business insurance companies need estimate risk incurred covering potential customers histories current behavior 
auto insurance consider factors sex age previous history terms accidents moving violations occupation city residence model automobile deciding coverage extend premium demand 
passenger airlines known charging fares differ significantly flight cabin class prices charge depend expenses costs aviation fuel aircraft maintenance employing air crew prices charged competitors availability seating days remaining flight method purchase ticket 
market coach class tickets competitive routes essentially commodities help know relationship fares demand 
fully understood relationship easier define related optimization problem 
obviously exist benefits predictions building models describe relationships attributes 
instance model accurately predicts data exploiting non obvious relationship worthwhile examine possible causes 
cases collecting data expensive potentially harmful drawbacks examining model may allow narrow scope data collection performs attributes 
addition strong relationships hold data exceptions may keen interest 
example comes abalone data set hosted uci repository murphy aha 
data cover physical attributes individuals particular variety 
documentation suggests number rings varied shell abalone added approximates age 
gender attribute specified male female infant 
expect infant young models reveal number strange tuples including allegedly corresponding year old infant abalone 
age gender strange judging data set knowledge regarding combination suggest data collection entry error developmental abnormalities 
type anomaly expect accurate model detect unusually high prediction error 
ith data tuples consist innocent behavior may adversarial database honest plan rules hold normal behavior adversarial instances extremely useful 
domains opposite situation may hold data fit certain rules re exhibiting interesting properties instance pattern exists system failures adherence pattern may interesting opposite 
situation may useful search rules anomalous cases exclude hold interesting cases 
procedure difficult labeled examples domain expertise 
considers data mining perspective model building unsupervised domain term model specifically refers equation approximates individual values individual attributes functions associated values attributes unsupervised means system operates requiring input output labels able request additional data 
system permitted assume original scales attributes optimal purposes instance may case accurate model acceptable complexity easier build logarithmic transformation applied attribute 
deal possibility consider space possible nonlinear axis scalings attempt determine help model building process 
consider example area population regions defined edition cia world factbook 
prior nonlinear scaling set looks nonlinear order preserving invertible scaling axes selected experiments described chapter get 
conducive demonstrating existence relationship area population addition noting presence regions unusually low high population densities 
population individuals cwf area versus population area km area population data edition cia world factbook 
excerpts shown figures show merely due outliers causing data fall small region suitable examination microscope focusing densely populated region arrives resembles original graph 
repeat discarding reach dense region near origin clarity 
clearly merely problem neat data cluster obscured outliers likewise affine transformations axis rotations suffice 
consider nonlinear axis scaling frequently help case crucial instrument 
current system may divided major components nonlinear population individuals scaled cwf area vs population axes scaled area km scaled area population data edition cia world factbook nonlinear axis scaling 
population individuals cwf unscaled excerpt area km discarding outliers cropping sparsely populated extremes unscaled cia world factbook data 
population individuals cwf unscaled excerpt area km round discarding outliers cropping 
scaling input attribute selection modeling 
stage produces output may interest instance choices scalings division inputs outputs may separate interest 
modeling stage amenable quantitative analysis assess accuracy models easily benefits knowing particular attribute best scaled certain fashion direct means justifying previous stages experiments analysis thereof focus aspect making direct assessments selected axis scalings choice inputs outputs 
chapter design questions noted previous chapter describes system designed data mining unsupervised environment 
tables numerical data rows columns correspond instances attributes necessarily input output labels 
addition may easier build accurate model nonlinearly scaling attributes 
ideally axis scalings determination inputs outputs resulting models built automatically minimum tuning supplement useful scalable expertise attention human domain expert 
want fully automated system identifying relationships inputs outputs evaluating data fits relationships find interesting exceptions complete data set consisting primarily numerical attributes 
fundamental questions light set consider questions 

scaling appropriate axis 

attributes model 

attributes inputs models 

models chosen 
examine second third questions 
closely related queries stem fact fit function requires inputs outputs 
simplicity restrict solution space requiring attribute input output 
addition suppose model builder determine attributes relevant possible inputs 
reduces second third questions problem dividing set attributes inputs outputs 
fourth question directly relates modeling problem 
fixed partition inputs outputs needs decide model 
exact choice model class method searching specific class matters 
instance consider problem modeling data set follows relationship sin 
data model class featuring linear combinations sine waves parameterized amplitude phase shift expected yield solution accuracy fewer parameters model class composed square waves turn better terms accuracy class non periodic functions piecewise linear models 
finite data set classes approximate data excellent accuracy efficiency vary 
noted needs model class method searching class 
complicates design fully automated systems 
instance weights neural network trained methods backpropagation choosing initial design terms nodes activation functions connectivity considerably difficult 
may obvious times re initialize run convergence 
lastly consider questions scaling 
theory cared attaining precise accurate models model classes possess sufficient flexibility scaling matter 
practice finding optimal models sufficiently flexible model classes happens quite difficult scaling affect result model class contains accurate models unscaled data 
addition scaling affect efficiency models 
consider attribute finite data set formula piecewise linear model depending range values require substantial number line segments similarly high number parameters 
addition model generalize tuples outside original range despite simplicity underlying rule 
hand application logarithmic transformation log results simpler relationship 
simplifying rela come cost scaling noise distribution 
possible unscaled case noise obeyed simple gaussian distribution scaling symmetric gaussian 
scaling impact efficacy input selection depending methods 
chapter related attempts handle curse dimensionality desire find patterns models substantial body proves relevant problems hand 
dimensionality reduction problem dividing set attributes disjoint sets model inputs model outputs related explored problem dimensionality reduction 
methods label attributes worth keeping provide binary partition usually notion independence utility 
thesis explore methods attribute may inputs outputs clustering local subspaces subsequent partitioning data set row boundaries provide functionality 
methods perform dimensionality reduction subsets chakrabarti mehrotra performs clustering agrawal hung cheng look clusters subspaces keogh uses contiguous regions temporal domain relevant 
traditional dimensionality reduction methods linear projection principal components analysis bartell calvo jolliffe partridge calvo random linear projection dasgupta gupta independent components analysis hyv rinen hyv rinen generate linear combinations inputs partitioning 
holds nonlinear projection methods principal curves chang ghosh kernel pca sch lkopf neural network methods baldi hornik demers cottrell jones kramer takahashi tokunaga locally linear embedding roweis saul mapping methods fastmap faloutsos lin kruskal multidimensional scaling kruskal sammon maps sammon isomap tenenbaum wang 
restrict search feature selection methods 
maximum entropy discrimination approach jebara jaakkola provides method designed act accordance pre existing goal classification rejection supervised case labels useful necessary 
context people examined merits directly labels influence selection process kohavi john aliferis 
method generalized lasso roth selects relevant attributes domain gene microarrays 
generalized lasso algorithm differs problem domain focuses probabilistic classification continuous prediction 
domain text information retrieval performed identifying insignificant terms requiring labels lang 
vectors consist solely frequency counts means data lie entirely set non negative integers similar meanings 
domains different attributes may wildly different scales semantics 
method traina jr appropriate starting point deals individual attributes attempts capitalize nonlinear dependencies requiring labels 
algorithm alterations thereof shall described detail comprises integral part system 
models addition neural network methods mentioned perform addition possible nonlinear dimensionality reduction principal components methods linear relate features necessary mathematical derivations apply algorithms genetic programming methods assorted regression tree methods cart breiman secret dobra gehrke quinlan rt 
rule regression function prediction methods explored weiss indurkhya 
methods may suitable feature selection remapping techniques provide mathematical grounds representing old features new space 
model complexity general theoretical optimistic superficial sense function fitting consists accepting inputs accurately computing corresponding outputs 
formulation problematic variety reasons 
obvious objections exists possibility noise measurement data entry errors nondeterminism factors captured inputs prevent perfect correspondence inputs outputs 
furthermore aims accurate model possible data cleansing accuracy measured error metric solutions 
ignore complexity risk approving dimensionality reduction modeling connecting dots single parameter plus complex curve approximate high dimensionality data set 
significant body philosophically related incorporate form ockham razor sense quantify trade accuracy complexity 
algorithmic complexity compression fields find theories regarding kolmogorov complexity solomonoff complexity minimum message length minimum description length similar methods hansen yu rissanen wallace dowe 
simpler methods bayesian akaike information criteria bic aic respectively proposed akaike 
methods applied selecting models account model complexity 
instance simplest approaches aic bic suggest selection members model class order minimize aic log bic log log model parameter vector number instances complex methods may applicable considering variety model classes different properties 
instance algorithmic complexity approach frames problem determination part string minimum size provided specific universal turing machine utm returns exact symbol string wish model compress 
utm needs halt immediately properly outputting string varies definition definition 
part framework input string needs composed consecutive non intersecting parts different purposes 
part encodes hypothesis utm second part defines parameters required generate input hypothesis 
specific conditions formalizing partitioning scheme may wallace dowe 
may part schemes explicitly differentiate encoding hypothesis specific inputs 
case objective remains minimizing length complete input string 
brief discussion neglects certain practical issues methods computable general case determining exact kolmogorov complexity designs initial utm 
regards second issue note choice utm affects solution 
fact problem choosing utm resembles dilemma choosing model classes cases just different choices may capable modeling data mean similar efficiency accuracy ease 
approach keogh lonardi keogh replaces utm selection problem involved kolmogorov complexity problem selecting lossless compression methods algorithms gzip bzip uses methods approximate algorithmic complexity 
continuous real valued numbers authors suggest discretization 
dimensionality reduction significant differences appear problem mind problem covered aforementioned methods comparing models 
specifically wish model unordered sets vectors may contain integers floating point numbers single sequence symbols 
addition expect errors imperfect models realistic data problem modeling lossless compression 
furthermore focus values data representations find relationships attributes greater interest relative frequency bit strings particular serialization data encoded specific format memory 
spartan spartan system comprises previous closest problem babu 
spartan builds bayesian networks identify dependencies 
bayesian network simply directed acyclic graph exactly node attribute node contains probability distribution corresponding attribute conditioned attributes nodes edges leading 
bayesian network expresses dependencies requiring explicit instantiation full joint probability table 
algorithm spartan returns usable ones attributes cheng 
taken spartan practice scales approximately linearly number tuples sampled babu 
bayesian nets assist search models attribute suggesting dependencies 
attribute corresponding vertex define markov blanket set containing parents children parents children 
show simple example 
full information attributes markov blanket attribute independent attributes 
considering model reasonable start look markov blanket 
directly modeling bayesian network leads graph 
graph vertices represent attributes 
undirected edges connect vertices spartan consider candidate predictor bayes net boolean attributes independent attributes probability tables exist conditioned conditioned conditioned markov blanket consists attributes parents child parent instance attribute part markov blanket 
bayesian network serves limit connectivity second graph 
limitation proves important initially final phase treats second graph weighted maximum independent set problem weight vertex reflects immediate gain modeling corresponding attribute neighbors 
ordinary spartan edits graph iterates permit models generated inputs originally lacked adjacency attributes modeled 
instance attribute need materialized just spartan chose neighbor predicted long remaining materialized neighbors plus materialized neighbors provide sufficiently accurate efficient model 
underlying model spartan uses piecewise constant cart trees breiman 
classification regression trees provide divide conquer approach allows significant flexibility modeling cost increased complexity 
regression trees single attribute tests internal nodes constant labels leaves 
maintain strict error tolerances spartan stores outliers leaves spartan defines outliers data instances proposed model corresponding inputs produce values error exceeded tolerances 
user specifies tolerances fractions attribute range 
spartan jagadish order compress materialized predictor attributes 
spartan takes care ensure lossy compression cause cascading errors exceed error tolerances 
assessment purposes spartan uses storage cost feasible assumes discrete data enabling prefix free codes 
cost model ignores errors tolerances errors accrue cost due need explicitly store outliers 
cart trees require storage 
node store data identifies type node test data attribute gets tested value labels child nodes corresponding positive negative results test predicted values 
spartan uses class models cart trees system focuses semantic compression complicated assessment schemes may inappropriate 
jagadish jagadish provide alternative semantic compression methods define semantic compression compression focuses values numerical data bit strings represent values particular encoding scheme 
systems produce compressed representation dictionary tuples stand individual rows 
tuples compressed representations meet error tolerances need explicitly stored outliers 
refines storing bitmap tuple specifies attributes values meet error tolerances individual values entire tuples get stored outliers 
methods focus compressing finite static sets attempt derive models useful prediction 
focus limits amount information learn running system instance dictionary representative tuples provide rules governing relationships attributes 
pure compression perspective may perform compared model methods spartan jagadish data mining perspective may useful informative choice 
chapter system design consider system designed implemented 
system consists stages scaling selection modeling 
stages processes data intent eventually returning efficient accurate models demonstrating relationships attributes may return additional information regarding nature data 
name somewhat appropriate system tries numerous possible axis scalings numerical attribute commit particular axis scalings model outputs relative impact assessed model selection 
spartan system separates input output determination modeling 
spartan involves complexity determining inputs outputs allows generated models affect labels thesis project uses simpler workflow splitting attribute set inputs outputs generating models introduces complexity nonlinear scaling complicated model class 
section gives detailed qualitative comparison systems 
axis scaling scale data 
practice question entirely ignored instance spartan system operates data babu reasonable piecewise constant models cart trees 
may case priori knowledge current axes suffice model building 
general case bayesian network dependency graph cart tree nonlinear scaling model input selection model building spartan high level diagrams spartan described system 
assume 
instance recall area population data edition cia world factbook 
unscaled data unclear relationship area population scaled version suggests existence relationship presence non trivial number cases deviate 
described earlier example data set nonlinear axis scaling far help affine transformations axis rotations clustering outlier detection unscaled space 
adding nonlinear scaling modeling problem obviously complicates search space 
example shows nonlinear scaling help perform declare set nonlinear scalings consider describe process selecting methods apply specific attributes specific subsets 
set possible nonlinear scalings attribute infinite similarly innumerable possible criteria preferring subset 
particular choices pragmatic approach opt elegant minimalist approach axis scaling system aims comprehensiveness computationally intensive highly flexible approach 
currently limits realm possible scalings cross product finite sets set box cox transformations set cumulative distribution functions selected estimation data plus identity function 
instance possible scaling function returned composition logarithmic function box cox transformation cumulative distribution function normal distribution log rest section discusses methods justification methods choosing 
box cox transformations represent traditional nonlinear axis scaling methods 
box cox transformations require strictly positive values constraint necessarily obeyed original data uses parameter form follows 
numerical value log permit iterative search limits set values fixed set arbitrarily defaulting chooses minimum xmin xmin sets xmin 
transformations provide range identity function frequently available chosen logarithmic transformation 
reasons cumulative probability distribution functions cumulative distribution functions univariate probability distributions traditional choices axis scaling desirable properties 
monotonicity just box cox transformations univariate cumulative distribution functions cdfs possess monotonicity 
cdf case strict equality restricted cases zero density 
condition happen functions distributions restricted integer outcomes truncated range due implementation issues lack arithmetic approximations 
monotonic axis scaling locally stretches compresses attribute preserves ordering 
likelihood occurrence popular probability distributions usually model common behavior 
instance gaussian normal distribution models real processes summation large number independent identically distributed random variables 
scaling methods take account possible generative process family useful identifying instances addition distribution correctly identified resulting cumulative distribution function transform data considerably uniform 
possible assist modeling cdfs useful tool 
similarly box cox transformations may helpful reasons people study power laws 
known estimation methods distributions exist closed form estimators 
distributions require numerical methods instance known mixture normals may estimated iterative expectation maximization method dempster reasonable closed form estimator exists 
long methods automated 
distributions incremental estimators advantageous considering stream data static sets ordinary multi pass methods require memory 
instance scalable em algorithm incrementally estimates mixture univariate multivariate normals 
may practical original em method large databases summarizes data order avoid repeated iteration original set bradley 
known evaluation methods methods seeking evaluate distribution fits data consider testing log likelihood anderson bayesian information criterion bic 
needs automated methods constantly prompting users judgment calls 
implemented current system considers metrics 
value involved computing anderson scores log xi log xn data xn sorted ascending order estimated cumulative distribution function 
correlation actual data predicted quantiles evenly distributed percentiles inclusive 
keeping range avoids issues related numerical approximations certain distributions percentage point functions functions defined approximations may break vicinity percentages 
distribution parameters estimated data priori hypothesis value lead valid significance level usual tables critical values 
raw values provide basis comparison requiring discretization methods aimed discrete distributions test 
furthermore uses cumulative distribution function density single outlier dramatically alter results 
contrasts likelihood tests bic risk dominated low likelihood single extremely improbable value 
scaling phase iterates pairs permitted box cox transformations probability distributions 
proves necessary automated method filtering unacceptable scalings 
uses value quantile quantile correlation higher worse value higher better minimum acceptable correlation 
estimates summaries choice distribution associated parameters serve summary informs user specific useful properties 
partitioning summary scaling component distribution component means readily interpretable compared combination 
flexibility components mixture normals approximate continuous distribution arbitrary positive error tolerances having known reasonably tractable estimation techniques expectationmaximization bradley johnson kotz xu jordan 
choosing probability distributions ideally choice distributions reflect accurate priors order avoid unnecessary computation produce axis scalings 
purposes evaluation relies fixed list distributions implied equal weighting currently take account relative likelihoods different distribution classes binary weighting distribution implemented enabled 
appropriate assuming absence domain knowledge 
distributions chosen eye flexibility frequency ease estimation 
uses univariate distributions order support axis scaling 
noted parameterizations estimators match johnson kotz 
estimators exist distributions multivariate normals mixtures thereof nearly readily lend attribute scaling selection 

exponential exponential distribution subsumed included gamma distribution parameter exponential distribution happens easier estimate 

gamma system includes gamma distribution default considerable flexibility 
implementation distribution employs chi square distribution turn uses normal distribution 

generalized lambda implementation considered partial due incompleteness tables necessary estimation 
implemented flexibility expected frequency 

normal variations thereof normals doubly truncated normals mixtures normals mixtures truncated normals included 
ordinary normals estimated simply method moments 
normals unknown truncation points possibly side mean underlying non truncated normal estimated integration simultaneous solving nonlinear equations cohen jr robust estimation performance observed levenberg marquardt search order minimize sum squared deviations estimated versus observed quantiles 
described system uses method 
details brief empirical comparison may appendix mixture model families implemented arbitrarily limit number components ensure termination reasonable time 
practice limit reached 
mixtures ordinary normals uses standard expectationmaximization algorithm dempster scalability incremental updates issue methods sem bradley sato algorithm sato thiesson incremental em thiesson may 
accelerate process binned data 
number components starts increases reaches hard limit bayesian information criterion worsens 
number components tried estimator runs multiple passes variety initial states poor start may lead convergence unacceptable local minimum 
mixtures independently doubly truncated normals far harder expensive estimate variety reasons 
primarily curiosity attempt investigator attempt described detail appendix understanding method necessary rest thesis estimator practice proved factor possibly consuming excessive cpu time contrast single doubly truncated normal turned second frequent distribution survived goodness fit criteria mixture normals ahead usual normal 

pareto parameter form chosen happens known inexpensive estimate 

uniform endpoints distribution set just outside range observations 
cumulative distribution function distribution exactly models data density nonlinearly scales data uniform distribution range 
hope uniform distributions uniform ranges helps find dependencies models 
obvious suggestion quantiles achieving uniformity helpful 
considered rejected scaling method reasons 
mapping requires forms estimation interpolation values observations 

extrapolation data outside observed range harder linear extrapolation instance poor job data distribution highly non uniformly distributed lognormal similar 

mappings highly parametric considered summaries intuitive readily interpretable common distributions normals 
integer variations recognition frequent appearance integer data system determines original unscaled attribute contains integers 
cases explicitly including discrete version probability distribution scaling code uses wrappers create new versions probability functions reflect shift density dx scaler uses nelder mead simplex algorithm adjust parameter estimates better reflect discretized probability functions observations maximizing log likelihood nelder mead 
metrics suggest methods 
summary axis scaling system examines attributes individually 
attribute considers full cross product sets set box cox transformations limited possible values set probability distributions estimator generator functions cumulative distribution probability density percentage point functions 
evaluates triple transformation estimated fit attribute anderson score correlation 
surviving triples proceed phase 
addition scaling system preserves original unscaled data regardless scaled version survives filtering criteria 
example small data set containing area population regions world different scaled versions area attribute different scaled versions population attribute survived addition original versions small size set 
logs include specifications attribute attendant scaling population scaled integer original population bc lambda bc shift name gamma discrete params qq chi ll bic declares scaled attribute population scaled integer attribute originally population power box cox transformation shift application cumulative distribution function gamma distribution location shape scale parameters respectively 
quantile quantile correlation anderson log likelihood bayesian information criterion scores reflect specific goodness fit tests uses quantile quantile correlation score leaves rest informational purposes 
shows particular function 
model input selection axis scaling phase results versions original attribute version corresponds identity transformation pairing particular box cox transformation probability scaling 
input selection phase decides versions attributes serve model inputs scalings discards alternative versions input attributes labels remainder possible outputs 
models built input attributes population scaled cwf scaling function population population individuals axis scaling function applied population attribute cia world factbook data 
original distribution indicated marks highly skewed low values scaled range assigned small part unscaled domain 
possible outputs selected criteria described 
exhaustive searches space possible labelings clearly impractical input output assignment space grows exponentially number attributes 
needs apply directed search 
addition selected model class piecewise linear regression trees fairly expensive evaluate phase faster heuristic estimate desirability particular input output mapping 
significant issues 
evaluate set input output labels building evaluating resulting models 

evaluation search solution space input output label assignments 
evaluating input output assignment problem employs fractal dimension 
distance metric value data set log pr log pr strictly holds perfectly self similar infinite sets 
finite sets variety arbitrary criteria determine range radii relevant partial derivative appears sufficiently stable 
system examines contiguous sets half plus log log pr pairs computes slope set lowest square error best fit line 
consider concrete terms convey intuition meaning 
trivial example suppose infinite point set points fall uniformly line dimensional space radius point neighbors 
radius point neighbors 
quadruple radius point qualifying neighbors 
time radius doubles number neighbors doubles 
pr log pr log 
hold true regardless embedding dimensionality depended linear relationship unidimensional nature space 
infinite plane uniform density number points qualify neighbors proportional square radius 
pr log pr log regardless dimensions involved 
matters point set planar 
point sets different properties fractional values possible 
famous sierpinski triangle set fractional dimension log ideally set infinite shows points log series 
shows corresponding graph log pr versus log linear fit points slope 
sierpinski triangle 
log sierpinski triangle log versus log best fit line log log vs log sierpinski triangle 
general point set dimensions necessary define data higher fractal dimension regardless embedding dimensionality 
data defined independent attributes high log pr versus log sets captured independent factors exist lower dimensional manifolds lower fractal dimension 
crucial caveat sans scaling attribute overwhelm distribution spread far higher magnitude 
take square stretch dimension resembles lower dimensional line likewise fractal dimension drop line 
relates attribute selection labeling problem reason assess valuable possibly nonlinearly scaled attribute input 
set selected inputs possibly empty add possible input change reflect attribute acts significantly independent factor may help define space adds little due correlations selected inputs 
hypothesis led methodology described section 
fractal dimension note fractal dimension may estimated computational cost varies linearly data set cardinality 
particular details may appendix data set set attributes algorithm needs choose set materialized attributes permissible attributes contain information due constant extreme happen attributes significant independent content 
fdr parameter provides hard threshold maximum acceptable attribute penalty 
denote fractal dimension projected fdr works follows 

initialize 
current choice attributes retain fractal dimension 

find da da maxa 
attribute removal causes reduction greatest gain 

da terminate return am 

da update da 

terminate return attributes remain 

return step 
fractal dimension reduction fdr algorithm said question remains 
possible approach fractal dimension reduction fdr algorithm previously developed group including investigator fast feature selection traina jr 
fdr algorithm enumerates steps 
fdr performs greedy backwards elimination starting state attributes retained iteratively greedily removes attributes set sees able reduction fractal dimension 
remaining attributes removed projection reduction algorithm concludes set attributes non trivial independence kept dropped attributes hand dropped significant loss correlated remain 
problem dealt fdr algorithm differ significantly specified thesis 
obvious significant differences courtesy axis scaling phase multiple versions original attribute may exist 
embedding dimensionality redundancy may far higher start 
problematic backwards elimination may tractable high sixteen attributes set traina jr clear apply different versions attributes results applying axis scaling dow jones data set described chapter fractal dimension needed computed projections involving attributes 
step require computation fractal dimension dimensions possible projections involving exactly attributes forth 
forwards selection constrain projection size stipulating version original unscaled attribute projection consideration 
fdr algorithm explicitly prohibit choosing keep different versions attribute theoretically possible 
selection approach hand enforce outright allowing selection chosen 
traina jr motivation fdr treated fractal dimension primary metrics 
emphasis lies modeling relationships attributes prefer bias selecting attributes modeling rest possible 
purposes choosing inputs including ensuring selection version original attribute input phase uses hill climbing forwards selection approach shown 
greedy forwards selection iterative search hill climbing tracking just single best state maintains priority queue candidate states limited size original number unscaled attributes order reduce probability greedy selection converging poor local minimum 
case cia world factbook data selector choose model inputs different versions original attributes 

initialize priority queue sorting combinations attributes fractal dimension descending order 
order limit run time hold combination fractal dimension pairs 

initialize 
table tracks combinations explored 

initialize best solution far fractal dimension 
relationship maps attributes unscaled version permit obvious extension sets 

empty highest fractal dimension currently 
select dequeue pair 
decide ties arbitrarily 
set possible states 
filter 
combination compute fractal dimension dc add dc ignore gain lower high threshold lowered wants accurate models fewer dependent inputs 
set contain best entries computed values 
break ties arbitrarily 

best known solution fractal dimension multiple state hill climbing 
initial empty state considers fractal dimensions individual version top choices previously mentioned version population scaled power box cox transformation gamma cumulative distribution function cdf fractal dimension version area scaled logarithmic transformation cdf component normal fractal dimension 
proceeds examining pair scaled version area attribute rejecting pair due fractal dimensions higher 
process occurs successor states selection particular scaled form area yielding pairs meeting threshold 
search terminates just particular scaled form population having selected versions population ignored different versions area possible model outputs 
may noted algorithm restricts final set attributes version attribute prohibits model class uses multiple versions attributes receives 
example algorithm estimating polynomials involve multiple powers attribute 
stated theory explored state table grow excessively large hill climbing explore states issue practice 
original fdr algorithm priority queue driven version nontrivial drawbacks 
obvious issues originate fractal dimension 
categorical data poses problems 
original definition apply need heterogeneous distance function wilson martinez 
furthermore nested grid family fractal dimension computation methods longer relevant barring full symbol hierarchy 
design chosen bypasses problem materializing nominal attributes involving selection phase 
second algorithms may incur substantial computational cost 
iteration requires computations numerous different projections computations expensive attributes involved projection 
third serious lacks clear definition current projection results non self similar data set 
case detected measuring linearity lack thereof point set defined estimated log log values 
unfortunately detection condition imply existence solution 
system currently ignores offending projection notes occurrence 
quick grep logs shows fractal dimension computations involved attribute selection original versions data sets estimates rejected due having low correlation 
different issue arises separation modeling attribute selection 
system attempts select attributes independence incomplete picture attributes may dependent similarly uniform respective projections attribute may far superior choice purposes function fitting 
pathological case seen 
completely avoid relying human interaction may necessary incorporate form model construction stage small sample avoid excessive computational cost 
better output independence provides incomplete picture better input pathological case attribute selector 
projections attributes equally uniform far sense input modeling final stage performs modeling 
previous section may divide set attributes sets specific scaled versions attributes selected materialization non selected versions attributes selected versions attributes materialized form 
set provides possible inputs models second set may ignored third set defines attributes model underlying attribute best model need selected definition best 
modeling phase uses regression trees linear constant models leaves implemented rt system 
little constrains choice modeling system need continuous attributes usable minimal human intervention 
instance reasonable modeling system frequently querying user hidden units neural network connected 
prior versions research version rt due flexibility availability capacity automation limited pruning coupled unavailability source code led investigator implement regression tree system secret dobra gehrke aggressive pruning system considers complexity addition accuracy 
reduce overfitting form fold cross validation performed 
details major differences tree secret cross validation procedure may appendix sets attribute selected input axis scaling 
attribute modeler select model targeted particular axis scaling survived goodness fit checks axis scaling phase 
approach taken consists iterating possibilities 
candidate version set possible inputs consists specific transformed attributes chosen 
main categories metrics traditionally influence sort selection complexity accuracy 
complexity compression described chapter exists substantial body devoted considering complexity models method choosing model classes 
methods assess costs storing model structure parameters addition may operate solely discrete output require perfection eliminating need directly add accuracy storage costs single metric may permit inaccuracy price assessing number bits storing information correct error demanded error tolerance 
methods cover model classes accept input data attributes labeled inputs domain accounted dealing cases cost may change 
instance guarantee number attributes labeled inputs differ scaled unscaled cases 
experimental results section partly describe complexity model terms nodes exist regression trees readily quantified 
numerous different ways may decide assess complexity scaling methods arithmetic operations cycles required lines bytes code language implementation canonical 
likewise method deals bit costs requires greater care floatingpoint numbers 
thesis focuses values data bits represent values 
tree sizes reported comparison purposes terms number nodes suggest particular encoding balance accuracy complexity canonical attempt provide sufficient different methods comprehensive 
relative merit depend application 
said attributes happen completely integers numbers provided correspond bit costs involving standard prefix free codes integers ieee bit doubles floating point numbers model parameters 
numbers stem tree selection influenced metric necessarily relate performance trees selected metric subsection 
curious results significantly analyzed 
accuracy evaluation rests primarily accuracy 
fundamentally problem deciding model scaling function dealing input tuple output datum complicated 
scaling instance error metrics give different rankings model classes depending evaluates original space scaled space due nonlinear nature figures pair describe data model difference presents results scaled space model generated 
intuition suggests cases cover model data differing reversible transformation considered equally 
barring assumption goodness original scaling method evaluation best specifically devise method reasonably scaling invariant 
area km predicted cwf area actual vs predicted area km actual perfect fit cia world factbook actual versus predicted area original space model generated scaled space 
regarding constraint lead rewarding extremely perverse data evaluating scaled space 
instance methods absolute error tweaked scaling axis correct outputs low magnitude values relative error metrics suggest arbitrarily shifting data adding huge constant desired output prediction errors get reduced higher denominators 
area scaled predicted cwf area actual vs predicted scaled area scaled actual perfect fit cia world factbook actual versus predicted area scaled space relevant model built 
reals metric real numbers just integers 
integer focused metrics may usable discretization choosing justifying discretization scheme non trivial absence specific application constraints domain knowledge 
resolution attribute distribution observations highly variable density 
unreasonable propose error tolerances tighter areas high density relaxed areas sparser density error absolute terms may cause confusion dense region sparse region 
deriving quantile metric investigator designed quantile error metric known metrics sum squared error relative error variants thereof 
instance spartan babu relies data inputs outputs discrete penalize accuracy assessing bit cost storing integer corrections 
number ways extend reals involving discretization schemes may give different results addition leaves unanswered questions linear shifts affect costs unevenly different values data range test original scaled spaces 
adds constant times maximum observed value outputs predictions instance relative bit costs closer regardless accuracy 
output attribute yn input matrix xn xi dimensional input vector output yi scaling function modeler quantile error metric computed follows 

define mapping follows map yi value range defined minimum maximum observed rank yi 
instance highest attribute consists value highest value differs yi quantile range yi 
values occur value defining extrema range 
values yi linearly interpolate neighbors side linearly extrapolate relevant extrema second extreme point 
cases map ranges containing single value 

assessing goodness prediction examine yi versus xi 
absolute magnitude closest distance ranges 
quantile ranges intersect distance lesser distances maximum quantile minimum quantile 

assessing combination sum squared errors space 
quantile error metric 
gives procedure computing quantile error 
note procedure transforms data space defined observed quantiles 
effect stretching dense regions compressing sparse regions ranks absolute value 
instance maximum observation increased arbitrarily mapped highest mapped fact mappings change values exceeding second highest observation effect limited value original observed maximum 
likewise linear shifts affect results long shift applies predictions observations 
choice scaling limited impact long order preserving 
values original data set effect unscaled values original data set need interpolate estimate ranks closest data means affects results slightly 
quantile metric computing errors space lets multiple types results 
assess combination input selection axis scalings inputs outputs model sum squared errors 
second give detailed instance median quantile errors frequently provided 
readily interpreted terms rank instance median quantile error means half time errors data set terms rank absolute value 
trivial baseline constant model predicts median observation median quantile error duplicates 
third detail graphs generated show rank versus quantile error single model distribution clearer 
model may expect quantile errors low followed sharply increasing tail encompassing major exceptions rule 
fourth individual quantile errors help flag tuples higher errors 
instance recall applying nonlinear axis scaling input selector world factbook data set yields selection particular scaled form population input thirteen versions area possible outputs 
accurate models single node tree form area km scaled population scaled data domain files providing sufficient information interpret specified scalings 
computes individual quantile errors tuples striking gap emerges highest quantile error magnitude quantile errors 
obvious particular tuple modeled extremely poorly compared rest 
quantile transformation modeling 
quantile transformation evaluation obvious question models built quantile transformation 
consider applying attributes aforementioned nonlinear scaling phase alternately applied outputs 
perspective minimizing eventual quantile error may sense 
noted quantile scaling issues complete lack interpretability difficulty extrapolating working potentially open ended distributions need de facto density estimation interpolation 
interpolation extrapolation issues potential impact building model may applied new data especially outside previously observed ranges 
comparison spartan design point relevant compare contrast spartan babu 
spartan form scaling data working number attributes change issues arise multiple versions original attribute 
second spartan follows wrapper design concept filter concept separate heuristic spartan employs actual modeler cart regression trees constant values leaves evaluating attributes need stored inputs 
reduce computational cost spartan samples data 
spartan discretizes data applies bayes network construction algorithm order identify dependencies initial determination attributes useful modeling attributes 
spartan may alter choice inputs iterations reflect attributes previously chosen modeled bayes net provides start configuration 
theory spartan wrapper design provide superior models compared phase acyclic design 
hand lack scaling combined piecewise constant models versus nonlinear scaling piecewise linear models may models efficient presence sufficiently complex dependencies spartan applies discrete data 
addition cart trees generalize output values original set interpolate extrapolate 
obvious question comes mind scaling phase proposed sim ply prepended version spartan rebuilt piecewise linear regression trees modified take account transformation parameter costs 
substantial reorganization spartan answer spartan relies heavily bayesian networks discrete nature data concerns attributes continuous domains 
addition discretization method specified spartan need modified deal intelligently multiple versions original attribute 
data discretized enable spartan style attribute selection decide retain piecewise constant trees non discretized data build trees knowing bayesian selector consider order values attribute relative magnitude gaps 
domain difference results systems appear suited straightforward combination direct comparison 
systems different metrics success 
spartan focuses compression uses error metrics translate errors bit costs treats modeling accuracy important endeavors metric takes account local data density exchange ignoring compression costs 
spartan scaling enabled favor certain perverse scaling methods provide extremely dense regions quantile error metric designed reward 
scalability complex system interest know just long execution take scale large data sets moderate modification 
want know worst case scenario appears sufficiently complex empirical analysis tenable 
numerical methods scaling methods frequently involve solution problems acceptable closed form approximation exists 
instance cumulative distribution function single univariate gaussian involves integration amenable perfect closed form solution 
iterative numerical integration methods exist known romberg rule simpson method major issues execution time related arbitrary convergence criteria theoretical guarantees correctness convergence algorithms may hold approximate values 
worst case convergence occur existing checks behavior suffice 
theoretical case worst case complexity infinite 
existing checks aimed avoiding cause certain failures instance approximations current implementation failed computing percentage point function gaussian distribution percentile question close extreme 
computation involves numerical integration numerical searching invert integration cumulative distribution function specific values 
holds higher level certain distributions 
expectationmaximization estimate univariate mixtures normals 
barring issues related numerical approximations causing non ideal behavior em converge local minima guaranteed bounds loose 
data set convergence occur rapidly depending initial state actual density data recorded times spent scaling extrapolation sets unreliable 
method replaced time linear number tuples generate histogram followed constant time estimation 
construction secret trees involves estimation binary mixtures multivariate normals dobra gehrke 
current implementation uses sem bradley bootstrapped means clustering algorithm sem scales reasonably initialization due summaries means predictable 
data associated single cluster means may take considerable number iterations discover second cluster 
different initialization methods predictable 
timing experiments section chapter modeling phase includes construction pruning selection final evaluation secret trees completely dominates computational cost current implementation 
inability predict number retained inputs number transformed attributes survive filtering depends number transformations allowed close particular attribute distribution estimation distribution goodness fit criteria 
possible minor significant impact basic algorithms unchanged 
number transformed attributes greatly affects potential worst case complexity scaling system attributes increases space search 
attributes highly correlated search may terminate early incremental fractal dimension changes drop built arbitrary threshold 
needs know number attributes general case number predicted merely knowing dimensions data set 
likewise number attributes selected inputs affects complexity generation model multiple ways 
inputs mean larger sets regression may easier construct sufficiently accurate model fewer pieces inputs means fewer outputs fewer models construct 
complex tree construction process assumes successful termination computation split points including aforementioned scalable expectation maximization sem algorithm bradley binary multivariate normal estimation generate labels likewise estimation linear models singular value decomposition approximation recursive squares tree construction terminate node split requires child nodes relate strictly fewer data parent 
maximum number nodes checks stipulating minimum number data consider split twice number tuples split resulting child single tuple leaf child rest data 
assumptions shed light worst average case stability 
execution single data set holding constant size data number inputs versus outputs observed time generation tree conceivably vary considerably due variations number splits required number computations 
estimates mind suggest guidelines individual stages 
consider nonlinear axis scaling stage 
estimators applied independently attribute nonlinear axis scaling phase scale linearly number attributes attributes similarly expensive process 
supplied distribution estimators enabled far computationally expensive estimators univariate mixture normals univariate mixture truncated normals 
currently relies standard expectation maximization presumably converges rate modified operate binned data essentially performance linear number tuples implemented theory require time cubic number tuples breakpoint divisions proved completely pathological worst case may attainable 
worth considering removal estimator sets significant size refining improved scalability 
second examine attribute scaling phase 
invocation fractal dimension code practice scales linearly number tuples number attributes attributes 
older version pure single state forward selection search axis scaling overestimate computational cost worst case number tuples number attributes number attributes eventually selected inputs mnk number iterations final iteration requires computing fractal dimensions projections dimensionality 
actual algorithm predictable projection needs computed different scalings attribute system tracks best state 
number possible axis scaling functions attribute loosely estimate complexity 
third modeling phase 
note number models need built scales linearly number outputs unscaled case complicated scaled case dimensionality models need built scales linearly number inputs 
reasonably efficient algorithms expect model construction time vary approximately linearly number tuples kn may plausible 
left experiments check conjecture 
may noted regardless difficulty assessing scalability theoretical analysis experiments documented chapter section suggest section incurs computational cost scales linearly number tuples 
judging experiments final kn modeling time dominates input selection scale worse 
chapter experiments attempt theoretical proof regarding performance collection approximations heuristics respect problem assumptions 
experimental testing appropriate 
implementation system exists primarily perl scripts modules code 
including internal documentation excluding interpreters libraries pre existing code consists lines code statistics software fractal dimension package attribute selector modeling system assorted test harnesses 
numerous components parameters tweaked purposes experiments parameters left general purpose defaults set tuning performed 
testing proceeded single machine intel xeon ghz processors gb memory running linux 
relatively small sets processing power far important memory 
attempt exploit parallel computing 
usage noted exist numerous opportunities adjusting parameters best suit data set taken 
data set procedure follows 

write domain file labeled attribute continuous nominal symbolic integer cases ignore 
certain attributes naturally outputs set purpose labeled noted sections come 

run axis scaling system data file 

run model input selection system unscaled scaled versions 

run model building selection system cases 

additional scripts compute quantile error tuple associate line numbers data set 

check logs aggregate error statistics 

check tree files number nodes 

start outlier search examining tuples highest quantile errors especially errors higher general time period applicable 
impact nonlinear scaling earlier sections contend nonlinear scaling capable having significant positive impact accuracy efficiency models 
piecewise linear model fit rt secret nonlinear scaling enable higher accuracy greater efficiency 
regardless expectations relevant examine potential areas observation 

impact nonlinear scaling attribute selection process 
implemented nonlinear axis scaling scheme definite bias increasing uniformity goodness fit tests 
attribute selector fractal dimension prefers uniform independent attributes 
reasonable suspect attributes selected inputs application nonlinear scaling 
happens certainly important selected attributes require bits store may enable better models 
draws line storage cost model accuracy depends application left user 

resulting models 
suggested chapter system design different compression modeling suggest different methods assessing goodness 
integer data traditional mdl mdl approaches combine model complexity model accuracy single metric measured bits 
complexity model assessed terms bits necessary encoding structure parameters specific encoding scheme devise schemes assessing accuracy cost storing correction data 
model described previous chapter results reported cases modeled integer attributes 
values considered remembering problems inherent model implicit assumption correctness original scaling 
addition bit costs apply floating point outputs 
integer floating point data models experiments perform model evaluation selection quantile error method 
quantile method take account model size reasonable report number nodes selected regression trees 

find 
instance sum quantile errors particular model unreasonably high examine distribution 
errors fact quite low errors far exceed rest may identified term semantic outlier necessarily outlier terms traditional metrics distance neighbors tuple represents combination values relationship properly captured may unusual interest 
cases simple scalings tiny accurate regression trees may exist provide useful information data 
experiments run data sets involving factors known experimenter 
variety data sets different domains order reduce likelihood deriving rely unusual traits coincidences 
section describes results 
experiments run data sets restricted strictly linear models piecewise linear models 
experiments described appendix abalone abalone set nash hosted uci repository murphy aha contains dimensional tuples regarding physical characteristics 
dimensions include nominal attribute sex male female infant multiple continuous attributes length diameter height weight weight weight weight integer attribute rings 
task originally associated set predict number rings attribute flagged output 
nonlinear axis scaling nonlinear scaling system chose retain attributes inputs sex length 
median quantile errors attributes ranged rings diameter weight 
mean squared error 
integer attribute attributes lack bit cost results 
attribute bit cost error nodes diameter weight weight weight shell weight rings table errors abalone set nonlinear scaling 
bit costs reflect tree cost errors quantiles 
nonlinear axis scaling nonlinear scaling enabled attributes retained sex weight truncated normal weight scaled truncated normal 
regarding output attribute node tree untransformed ring attribute accrued total squared quantile errors delivering somewhat similar results 
mean sum squared error original unscaled space label attributes inputs accuracy focused balanced resulting node tree mean squared error rings sum squared quantile error median quantile error perfect prediction cases 
table summarizes quantitative scores models case nonlinear scaling 
quantile scores may worse median quantile errors cases quite small 
instance median quantile error weight increases unscaled case scaled case maximum quantile error dropped scaled case maximum substantially second worst 
case diameter worst relative penalty median quantile error remains attribute median quantile error exceed scaled case worst case occurring height median quantile error drop weight 
recall median quantile error means predicted output rank error half time 
noted models general unscaled case require total nodes trees total sum squared quantile error models nonlinear scaling take nodes deliver total sum squared quantile error 
considers models post scaling sufficiently size benefit obvious substantial 
distribution errors prefers look error results detail generating attribute percentage point function curves 
shows errors tend smaller scaled case unscaled case 
reasonably accurate model expect smooth curve errors low sharp rise magnitude errors exceptions rule 
graphs scaled unscaled curves coincide deliver similar performance broad divergence hand indicates model lower curve better distribution errors 
shows worst modeled cases scaled unscaled cases exceptionally badly modeled tuples 
shows corresponding results rings attribute expect discrete attribute percentage point functions step functions 
quantile error percentage point function quantile error percentile unscaled scaled percentage point curves quantile error unscaled scaled cases weight attribute abalone set 
percentile means median quantile error maximum 
curve scaled case lies unscaled case differences minimal serious errors 
cases worst errors percentiles instance worse errors 
quantile error percentage point function highest quantile errors percentile unscaled scaled percentage point curves worst quantile errors unscaled scaled cases weight attribute abalone set 
unscaled scaled cases worst errors quite high relative rest 
quantile error rings percentage point function quantile error percentile unscaled scaled percentage point curves quantile error unscaled scaled cases rings attribute abalone set 
curves step functions rings distinct values 
outliers abalone set contains notable outliers 
instance model scaled version height reports quantile error tuple worst model 
generating tuples claim represent slightly odd female abalone height mm length mm diameter mm weight long wide incredibly flat 
row data set 
female abalone height mm length mm diameter mm weight large lightest bunch 
row data set 
tuples strange top worst errors weight model unscaled case 
shows just extreme tuples considering height weight 
unscaled scaled cases worst result shell weight belongs row infant heights row 
impressively small non zero length diameter weight ring count 
attribute bit cost error nodes length diameter height weight shell weight rings table errors abalone set nonlinear scaling 
bit costs reflect tree cost errors quantiles 
comparison results unscaled case show cases models quite general 
short abalone data set attributes values predicted quite unscaled scaled cases statistics median quantile error tuples deviate extremely mainstream 
weight abalone height vs weight row height mm row abalone height versus weight 
bit general trend number extreme outliers 
baseball baseball data set contains dimensional tuples 
tuple contains set statistics individual player major league baseball season 
statistics listed table 
avg stats continuous attributes rest contain nonnegative integers 
nonlinear axis scaling scaling attributes retained cs table summarizes quantitative results unscaled scaled models single node trees 
attributes contain integers bit costs traditional compression related objectives apply costs listed correspond bit costs trees selected specific objective mind 
complex trees enabled better prediction small amount data regression tree algorithm reluctant split 
ba avg triples slg pct bb bases balls base avg hr home runs games bb bases balls ab bats runs sb stolen bases hits cs caught stealing tb total bases errors doubles table statistics baseball data 
triples easy predict certain results deserve explanation 
number triples instance relatively easy predict due extreme skew 
fully players triples triple player counts fall sharply number triples increases 
median quantile error low sb median quantile error exceeded ba slg exceeded 
recall constant median predictor median quantile error fewer duplicate values 
nonlinear axis scaling case nonlinear scaling attributes selected base average square root scaling derived estimation component mixture normals scaled gamma distribution 
model common unscaled scaled cases sb sum squared quantile errors improved increase slightly 
median quantile errors ba slg approximately respectively 
largest reduction appears bb unscaled model poorly compared bb cs bb unscaled scaled attribute bit cost error bit cost error ba slg input ab tb hr bb input sb cs input input table errors baseball set nonlinear scaling 
emphasized text indicates better performance attribute modeled unscaled scaled cases 
trees single node small sets regression tree builder needs see substantial accuracy improvements accept split 
bit costs reflect tree cost errors cost inputs measured trees specifically chosen bit cost quantile metric 
bb scaled truncated lognormal power box cox transformation component normal gamma distribution 
formula median quantile error approximately versus unscaled case 
distribution errors shows percentage point curves quantile error case tb unscaled scaled versions 
graphs baseball set bit step function due low range curve scaled case shows slow steady increase errors sharper increase worst cases 
unscaled case rate increase great bulk quantile errors lower scaled case despite fact regression trees single node 
quantile error tb percentage point function quantile error percentile unscaled scaled percentage point curves quantile error unscaled scaled cases tb attribute baseball set 
large gap unscaled upper scaled lower curves show best worst case errors similar magnitude percentile scaled case lower errors 
instance quantile errors scaled case unscaled case threshold 
table shows attributes modeled unscaled scaled cases models keep number nodes cases models scaling accurate 
data set supports contention added flexibility improve modeling performance 
may noteworthy attributes bit cost metric quantile score agree nonlinear axis scaling helped despite making fundamentally different judgments evaluate errors 
building building set comes proben benchmark collection prechelt turn cites original great energy predictor 
version includes tuples featuring attributes 
record id ignored hour integer environmental conditions temp solar wind continuous 
objectives wbe building electrical energy building cold water building hot water continuous attributes 
nonlinear axis scaling nonlinear scaling attribute selector keeps temp wind inputs 
attribute bit cost error nodes hour solar wbe table errors building set nonlinear scaling 
bit costs reflect tree cost errors quantiles 
table summarizes quantitative results models generated nonlinear scaling 
median quantile errors ranged wbe 
nonlinear axis scaling scaled case attributes selected hour linearly normalized quarter power box cox transformation scaling mixture normals wind transformation 
table shows tree sizes sum squared quantile errors resulting models 
comparative results mixed aggregate quantile scores worsened scores wbe solar improved 
median quantile errors solar median quantile error medians range temp 
attribute error nodes temp solar wbe table errors building set nonlinear scaling 
modeled attributes continuous bit costs listed 
compare regarding solar 
distribution errors compares percentage point curves quantile errors unscaled scaled cases modeling solar 
scaled error far lower quantile error percentile possibly excepting smallest errors 
cia world factbook extracting area km continuous individuals edition cia world factbook central intelligence agency yielded set tuples specified attributes 
tuples correspond nations belong territories entities 
nonlinear axis scaling unscaled case system chooses retain population input 
model area uses node results sum squared quantile error individual quantile errors range median 
words unscaled area predictor better ideal constant median predictor 
quantile error solar percentage point function quantile error percentile unscaled scaled percentage point graphs quantile error unscaled scaled cases solar attribute building set 
curve scaled case lower curve unscaled case 
quantile errors scaled case median quantile error unscaled case 
nonlinear axis scaling nonlinear scaling enabled system selects population input power box cox transformation gamma transformation 
single node tree modeling area achieves sum squared quantile error individual quantile errors range median expanded range interesting errors declined magnitude highest observed quantile error increased dramatically usefully 
may noteworthy unscaled case progression errors sorted magnitude appears smooth scaled case magnitude worst quantile error greatly exceeds second worst 
examination data suggests worst case belongs territory greenland area km population extremely sparsely populated territory outlier 
unscaled case isn quantile error leaving 
outliers low dimensionality set feasible graph effects scaling 
figures show comparison scaled case axes graphed scaled version selected attribute partitioning scheme scaled version associated best performing model 
figures show annotations unscaled scaled cases respectively 
case points stand unusually high area case label worst modeled points quantile metric 
looking clear greenland extreme outlier receives single highest quantile error applying power box cox gamma distribution area power box cox gamma population 
sparsely populated state area km population accounts second worst score 
round top unusually low population densities 
comparison people republic china modeled reasonably quantile error highly unusual terms area population relationship normal 
holds india quantile error mere unscaled case china india modeled quantile errors acceptable cases push comparing 
hand unscaled case worst modeled area quantile error followed closely macau 
scaled case tiny areas received quantile errors respectively 
tend tiny states population density people km extreme densities greater approximately identical andorra 
unscaled case scaled case andorra considered outlier 
population individuals cwf area versus population area km area population data edition cia world factbook 
advisable estimate relationship area population scaling 
distribution errors shows percentage point functions quantile error modeling area unscaled scaled cases 
unscaled errors usually lower board difference narrows extremes fact worst case worse scaled model 
explained exceptional nature greenland handled separate entry denmark incredibly low population huge area 
population individuals scaled cwf area vs population axes scaled area km scaled area population data edition cia world factbook nonlinear axis scaling 
trend far perfect graph suggest broad positive correlation area population suggests existence extreme exceptions 
population individuals india cwf area vs population unscaled china indonesia usa brazil canada australia russia area km axis scaling annotations 
classify named regions outliers traditional cluster definition 
pop 
individuals scaled hong kong cwf scaled axes top outliers labeled taiwan area km scaled western greenland axis scaling annotations 
named points necessarily correspond extrema nations deviate general trend relating area population 
hong kong instance extremely densely populated 
quantile error area km percentage point function quantile error percentile unscaled scaled percentage point curves quantile error unscaled scaled cases area attribute set 
lower curve scaled quantile errors shows degree improvement approximately quantile errors scaled case fall median quantile error unscaled case 
dj dj set dj submitted statlib 
set contains stock data stock component dow jones october split adjusted period consisting trading days january october 
attribute labeled refer ko suspicion having djia component ko labeling error 
truth matter ramifications fact ko operated different markets presumably different dependencies 
data set description indicates data originally collected yahoo 
set split adjusted closing prices 
sequential nature data allowed experimentation performed exploiting temporal correlations 
instance model closing price american express axp historical prices american express closing prices dow jones components day 
obviously perform considerable experimentation attempting predict concurrent non independent time series 
nonlinear axis scaling nonlinear scaling single stock chosen input ip international 
table lists tree sizes sum squared quantile errors remaining attributes 
consider results various angles 
sum squared quantile errors order notable exceptions 
occurred dupont dd node tree maximum quantile error median quantile error model caterpillar cat accurate median maximum quantile errors respectively 
proctor gamble pg slightly worse score reflects median maximum quantile errors philip morris mo 
time score reaches johnson johnson jnj median quantile error reached twice dupont 
nonlinear axis scaling nonlinear scaling enabled system selects attributes international third root box cox truncated normal jp morgan mixture normals 
stock nodes stock nodes aa jnj axp jpm bs ko cat mcd ci mmm dd mo dis mrk ek msft ge pg gm sbc hd hon ibm intc table errors dj set nonlinear scaling 
modeled attributes continuous bit costs listed 
bs steel acquired period data collection 
models range considerably size accuracy 
table presents model statistics enabling nonlinear scaling 
models common scaled unscaled cases sum squared quantile error dropped 
median median quantile errors models maximum median quantile error dropped maximum medians belonging scaled unscaled cases 
stock nodes stock nodes aa intc axp jnj bs ko cat mcd ci mmm dd mo dis mrk ek msft ge pg gm sbc hd hon ibm table errors dj set nonlinear scaling 
modeled attributes continuous bit costs listed 
compare table sum squared quantile errors lower case increase model complexity 
studying time series sequential nature data dates availability news articles stocks stock market general give opportunity study data detail cases attempt explain observations 
consider 
curves show daily median quantile error output attributes unscaled scaled cases time goes 
scaled case completely lie unscaled case generally lower reaches heights scaled case 
median quantile errors group outputs sudden spike correspond significant event affecting input attribute affecting half outputs 
instance largest spikes unscaled case occur september october similar spike scaled case 
may reflecting news specific ip time dealing acquisitions capacity reduction input scaled space presence input jpm may reduced 
alternately scaling ip may mitigated effects 
temporal locality scaled case errors quite models increase sharply middle september early october 
may related problems jpm time notably fallout publicized involvement possible liability mci 
worst quantile errors instance top errors ibm top mo top pg fell september october 
median daily quantile error daily median quantile error dj unscaled scaled daily median quantile error dj outputs unscaled scaled cases 
curve scaled case stable usually lower unscaled curve occasional sharp spikes indicating substantial problems half models 
examine errors individual models 
example consider node regression tree models dd 
top worst errors occur august september 
errors range greater median quantile error june announced sale pharmaceutical unit bristol myers late july announced job cuts larger expected admitted losing money quarterly report 
model cat worst errors occur trading days april april 
errors exceed quantile space median may noted early april downgraded cat stars avoid stars hold labeling rise share price 
worst quantile errors sbc magnitudes fall th st september vastly exceed third worst error magnitude 
st announced plans purchase outstanding shares prodigy communications may relevance 
honeywell scaled case accurate model sum quantile error belongs honeywell hon unscaled case corresponding model scores relative best 
figures show curves set impulses curves correspond actual estimated split adjusted closing prices left axis impulses track quantile errors daily estimations right axis 
scaled case estimates appear track actual closing prices better unscaled case improved performance interesting examine exceptions 
shows actual model scaled space 
quantile metric scaled model worst mispredictions occurs october score peak series quantile errors exceeding starting october december october 
october announced general electric agreed buy honeywell international beating prior bid united technologies 
rumors bid officially confirmed october johnson 
period interest april april quantile errors scaled model briefly increased peaking april honeywell gain sharply april mentioned articles greenwald searching reveal specific reason 
dupont nonlinear scaling best bargain model dd dupont 
model dd acceptably low sum squared quantile error split close hon actual estimated prices versus quantile error actual modeled quantile error actual modeled prices honeywell hon left axis daily quantile error right axis 
curves diverge considerably times accounting high quantile errors noted taller impulses 
split close hon scaled actual estimated prices versus quantile error actual modeled quantile error actual modeled prices honeywell hon scaled left axis daily quantile error right axis 
curves track time indicating prediction 
quantile error quantile error hon scaled ip vs jpm vs hon scaled actual vs predicted ip scaled jpm scaled scaled model hon ip jpm 
observations indicated crosses modeled surfaces depicting regression tree 
median quantile error maximum quantile error making accurate model unscaled case single node 
ip scaled power box cox transformation truncated normal jpm scaled component mixture normals scaled logarithmic transformation mixture normals tree consisted solely equation dd ip jpm accurate model list scaled models dis gm hon intc mcd msft sbc lower error quite accurate complex 
possible explanation lies fact dupont international count goods nontrivial parts product lines 
eastman kodak corresponding graphs scaling worst case eastman kodak ek figures 
unscaled case model essentially collapsed point places practically constant predictor 
unsurprising result quantile errors high 
scaled case fares better completely fails certain trends modeling steep decline sharp rise circa july greatest discrepancy october model predicts closing price actual price 
relevant sharp climb share price started july ek closed share split adjusted higher share preceding day 
day ek reported positive earnings surprise share second quarter versus expectations actual earnings quarter previous year 
stock peaked week share march fallen share close scaled model estimate share ip jpm 
split close ek actual estimated prices versus quantile error actual modeled quantile error actual modeled prices eastman kodak ek left axis daily quantile error right axis 
curves diverge significantly high impulses indicate substantial quantile errors 
sbc communications sbc communications sbc offers interesting example improvements possible nonlinear scaling 
tables note sum squared quantile error improved significantly sbc median quantile error dropped hand quantile error split close ek scaled actual estimated prices versus quantile error actual modeled quantile error actual modeled prices eastman kodak ek scaled left axis daily quantile error right axis 
periods significant divergence high quantile errors scaled model tracks somewhat better unscaled model shown 
quantile error scaled model shown requires inputs eleven nodes input node 
shows closing prices ip sbc unscaled 
cursory examination shows sbc closing price depend ip unscaled case resulted selecting ip input sbc modeled 
natural ask inclusion jpm suffice 
shows outcome force selection build regression tree inputs output resulting node tree median quantile error sum squared quantile error smaller accurate model derived nonlinear scaling 
sbc scaled ip vs jpm vs sbc scaled actual vs predicted ip scaled jpm scaled ip jpm versus sbc scaled 
observations marked crosses tracked model 
intc intel intc shows similar case 
shows single input chosen unscaled case expected produce reasonable piecewise linear model sum squared quantile error median quantile error reflect nodes regression tree 
compare results scaled case ip jpm inputs nodes model sum squared quantile error median quantile error comparison sbc ip vs sbc unscaled ip ip versus sbc unscaled 
clearly needs ip model sbc 
sbc unscaled ip unscaled ip vs jpm vs sbc unscaled jpm unscaled ip jpm versus sbc unscaled 
model fairly accurate accurate scaled case shown 
shows model resulted unscaled case permitted ip jpm inputs 
node input model slightly complexity scaled case slightly better median quantile error worse sum squared quantile error 
intc closing price ip vs intc closing prices unscaled ip closing price ip versus intc unscaled 
needs ip model intc 
distribution errors number attributes involved nature data companies stock prices related plus may substantial rumors events specific useful multiple graphs comparing percentage point curves 
figures show gm dis cat ek respectively 
gm presents usual example scaled case dominating unscaled terms lower quantile error 
dis gap unusually large cat difference minimal 
ek unusual scaled model unscaled accurate 
housing housing data set uci repository murphy aha originated research described harrison rubinfeld 
sub intc scaled ip vs jpm vs intc scaled actual vs predicted ip scaled jpm scaled ip jpm versus intc scaled 
model surfaces track observations quite 
intc unscaled ip vs jpm vs intc unscaled actual vs predicted ip unscaled jpm unscaled ip versus intc unscaled 
model complex scaled case mixed results regarding accuracy median quantile error declined slightly sum squared quantile error increased 
quantile error gm percentage point function quantile error percentile unscaled scaled percentage point curves quantile error unscaled scaled cases gm dj 
scaled case reflected lower curve produces lower errors unscaled case 
quantile error dis percentage point function quantile error percentile unscaled scaled percentage point curves quantile error unscaled scaled cases dis dj 
lower curve scaled case shows lower quantile errors unscaled case 
quantile error cat percentage point function quantile error percentile unscaled scaled percentage point curves quantile error unscaled scaled cases cat dj 
nearly coincident curves occur models approximately equally 
quantile error ek percentage point function quantile error percentile unscaled scaled percentage point curves quantile error unscaled scaled cases ek dj 
curves similar cases pronounced rise 
model especially 
sequent research originating domain questioned encoding pace 
point testing assess effects nonlinear scaling modeling process draw boston housing market circa ignore disputes regarding accuracy 
set contains tuples fourteen attributes 
original problem consisted predicting median home value variety factors 
twelve attributes continuous including designated output attribute binary integer 
nonlinear axis scaling absent enabling nonlinear scaling system selected attributes binary automatically selected attribute specifying land borders charles river nox continuous attribute specifying concentration parts rm average number rooms dwelling 
table gives basic results models unscaled case 
trees quite small expected due small size data set regression tree builder bias large trees 
examine accurate models 
lowest sum squared quantile error occurs zn 
apparent best case median quantile error quantile errors remaining range 
indus tree single expression indus nox median error higher maximum outstanding outliers 
table suggests slightly accurate lstat nox rm median quantile error maximum 
extremely large errors interesting cases age large sum squared quantile errors high median errors respectively 
median quantile error excess high model predicts true median score worse threshold 
tuples produce quantile errors range 
age tuples producing quantile errors range 
outliers account unusually poor scores models 
attribute bit cost error nodes crim zn indus age dis rad tax lstat table errors housing set nonlinear scaling 
baseball data small size set difficult justify splitting nodes regression tree 
note age 
nonlinear axis scaling system retains input attributes enabling nonlinear scaling 
specifically chose crim mixture identity mandatory binary attribute rm fifth root box cox transformation mixture normals age mixture fifth root box cox transformation mixture normals 
table summarizes quantitative measurements models built assistance nonlinear scaling 
obvious note sum quantile costs suggests existence extreme outliers age unscaled case 
notable attribute modeled scaled unscaled case sum squared quantile errors lower scaled case significantly trees larger tax grew nodes 
consider apparent best worst cases 
zn modeled box cox transformation estimation mixture normals tree yielded tiny median quantile error maximum quantile error 
tuples modeled quantile errors worst case median quantile error maximum quantile error unscaled case scores respectively 
score rad improved significantly corroborated median quantile error unscaled case maximum quantile error 
curious model zn simply encodes zn crim rm output zn scaled power box cox transformation component mixture normals 
attribute bit cost error nodes zn indus nox dis rad tax lstat table errors housing set nonlinear scaling 
attributes modeled cases lower sum squared quantile errors corresponding gain model complexity additional nodes tax 
distribution errors unusual unusual quantile error percentage point function rad index accessibility radial highways 
attribute values data set integers 
shows step function 
liver liver disorders set may uci repository murphy aha credits bupa medical research data set dimensional tuples 
attributes mcv mean volume quantile error rad percentage point function quantile error percentile unscaled scaled percentage point curves quantile error unscaled scaled cases rad housing 
rad discrete values step function 
curves show scaled model lower quantile errors unscaled model 
alkaline concentration gamma contain continuous values 
sixth attribute drinks originally numerical attribute steps multiplied treated integer 
seventh attribute selector attribute possible values predicted coded integer output system currently support unordered nominal outputs 
selector tuples registered remaining value 
nonlinear axis scaling absent scaling system chose retain 
table presents basic results remaining attributes 
quantile results selector misleadingly low quantile metric designed values different values adjacent quantile ranges classification accuracy summarized appendix regards attributes best performance appears drinks median maximum quantile errors respectively worst mcv 
attribute bit cost error mcv drinks selector table errors liver set nonlinear scaling 
trees expected set size tree pruning algorithm 
nonlinear axis scaling nonlinear scaling selector chooses inputs 
mixture normals scaled single lognormal received quarter power box cox transformation scaling cumulative distribution function gamma distribution 
table presents main results remaining attributes 
classification results listed appendix sum squared quantile scores attributes suggest change 
drinks median quantile error dropped slightly maximum increased tuple quantile error 
median maximum quantile errors mcv poor 
attribute bit cost error mcv drinks selector table errors liver set nonlinear scaling 
trees 
comparing models mcv drinks significantly different results scaling allowed accurate model 
distribution errors set see marginal improvement scaling 
borne percentage point graphs quantile error statistic 
curves shown unusual scaled model better cases unscaled model delivers better results percentiles range 
page blocks page blocks data data set uci repository murphy aha attributed donato malerba university bari esposito 
set tuples corresponds block segmented document 
original version tuple values eleven attributes including valued class attribute 
lieu explicitly adding classification capability regression tree system expanded class attribute integer attributes valued depending tuple assigned class 
appropriate classes ordering directly regression single class attribute sense 
version fifteen attributes including integer outputs 
inputs integers include height length area block statistics number black pixels number black pixels smoothing wb trans number white black quantile error percentage point function quantile error percentile unscaled scaled percentage point curves quantile error unscaled scaled cases set 
curves diverge somewhat reflecting moderate improvements allowed scaling 
transitions 
remaining inputs eccentricity block black percentage black pixels percentage black pixels smoothing mean tr mean number white black transitions continuous 
nonlinear axis scaling absent nonlinear scaling attributes retained black percentage black pixels block percentage black pixels application particular smoothing algorithm 
table describes general results modeled attributes unscaled case classification results may appendix unusually large errors regarding attributes logs show best case wb trans median maximum quantile errors high respectively 
similarly scoring height model scores respectively quantile errors close maximum case case quantile errors 
best case length ranges median maximum quantile errors 
model worst sum squared quantile error median quantile error top quantile errors range top 
housing data tuples may interest due extremely bad modeling utter lack annotation fruitless speculate possible explanations 
generated quantile errors ranging higher median error 
highest median quantile error came area model 
attribute bit cost error nodes height length area mean tr wb trans class class class class class table errors page blocks set nonlinear scaling 
class attribute original set broken binary attributes 
nonlinear axis scaling scaled case attribute selector retained severely asymmetric truncated normal mean tr power box cox transformation mixture normals power box cox transformation asymmetric truncated normal 
table summarizes non classification results application nonlinear scaling classification results relegated appendix median quantile errors dropped dramatically area extremely precise wb trans median quantile errors approximately length worse worst median quantile error height 
attribute bit cost error nodes height length area black wb trans class class class class class table errors page blocks set nonlinear scaling 
sum squared quantile attributes improved cases extremely instance compare area length wb trans table 
anomaly area anomaly page blocks set 
data sets attributes regression trees attribute attribute chosen input regardless nonlinear scaling module trees differ greatly size 
tree scaled version area far nodes tree unscaled version 
extra nodes appear benefit modeling accuracy area gone useless extremely precise 
distribution errors notable percentage point curves quantile error area seen figures 
shows dramatic improvement modeling area 
unscaled case black retained inputs derive area regardless model class 
attributes area selected inputs case sufficient information available area piecewise linear nature model class combined lack scaling led larger model necessary scaled case 
shows attributes scaled unscaled model provides completely satisfactory performance scaled model dominates 
quantile error area percentage point function quantile error percentile unscaled scaled percentage point curves quantile error unscaled scaled cases area page blocks set 
curve scaled model nearly coincident percentile axis length shows extreme accuracy curve unscaled model shows opposite 
bit costs sets quite integer attributes serve outputs scaled unscaled cases 
multiple bit cost scores compared definition take account storage costs required models bits required particular error correction scheme permits perfect reconstruction target attribute 
noted differing metrics involve different trees mdl inspired bit cost metric take account skew 
nonlinear scaling affect positively 
quantile error percentage point function quantile error percentile unscaled scaled percentage point curves quantile error unscaled scaled cases page blocks set 
lower curve scaled model reflects improved accuracy 
wind wind data set contains average wind speeds measured twelve meteorological stations ireland raftery 
specifically data set contains tuples fifteen attributes 
attributes encode month integer format 
set metadata ignore day attributes leave month attribute mandatory input account seasonal factors 
remaining twelve attributes continuous encode average wind speeds stations denoted rpt val ros kil sha bir dub cla mul clo bel mal 
nonlinear axis scaling system keeps ros cla mandatory input month 
table provides main quantitative results modeled attributes 
bit costs provided modeled attributes continuous usual prefix free codes apply 
median quantile errors range bir mal 
attribute error nodes rpt val kil sha bir dub mul clo bel mal table errors set nonlinear scaling 
modeled attributes continuous bit costs listed 
single node trees may reflect difficult set tuples pruning algorithm allow splits 
possible outliers examining error logs suggest couple tuples may difficult model quantile errors november november bir approximately remainder top errors model range 
val top quantile errors magnitudes ranging fall march december february highest quantile error val 
nonlinear axis scaling scaling enabled selector chooses inputs month ros sha bel 
table lists main results 
models significantly lower sum squared quantile errors notably rpt val kil smaller improvements 
attributed modeled cases yielded worse total score clo degradation insignificant 
median quantile errors ranged low bir similar cla similar high medians dub mal respectively 
remaining median quantile errors range kil cluster remaining models median errors generally close unscaled case 
attribute error nodes rpt val kil bir dub cla mul clo mal table errors wind set nonlinear scaling 
modeled attributes continuous bit costs listed 
model complexity increased cases val bir sum squared quantile errors drop rpt kil dub mal 
possible outliers examining error logs scaled case occasional tuple may catch eye 
clo top quantile error november stands highest error bit lower 
mal model difficulty particular tuple error model reaches may second largest quantile error 
separation worst ordinary pronounced val model errors exceed december january worst error val 
may interest december poor case val unscaled case 
particular day fairly low average wind speed val lowest significantly lower previous day average wind speeds stations increased 
distribution errors attributes modeled cases nonlinear scaling modest changes sum squared quantile error 
borne percentage point curves 
instance percentage point comparison graphs set range closeness models clo shown slight difference rpt 
quantile error clo percentage point function quantile error percentile unscaled scaled percentage point curves quantile error unscaled scaled cases clo attribute wind set 
coincident curves reflect models nearly identical performance 
long take 
really questions long take set total system scale cardinality data set 
processing entire sets tables provide timing results data sets 
timing results consist cpu time results collected perl benchmark package individual stage total times 
times accurate stated figures collected previously described test platform 
tables specify listed data set number tuples total attributes number selected labeled inputs unscaled scaled cases respectively 
table specifies number possible outputs number scaled versions attributes scaled version selected input 
timings scaling stage included include time spent reading quantile error rpt percentage point function quantile error percentile unscaled scaled percentage point curves quantile error unscaled scaled cases rpt attribute wind set 
improvements rpt appear come errors worse median lower curve corresponding scaled model coincident upper curve 
data applying box cos transformations estimating parameters assorted distributions applying goodness fit tests writing scaled data metadata 
selection timings include time spent reading data iterating assorted candidate combinations attributes computing fractal dimensions writing selected data metadata 
modeling results include time spent reading data splitting attributes segments purposes cross validation building pruning evaluating trees different subsections data evaluating selected trees full data recording trees results 
results class attributes re encoded described exception abalone class attribute page blocks expanded outputs liver set drinks converted counting half drinks drinks rings attribute abalone retains original reasonable value formulation 
input output determination left selector input label month wind output labels rings abalone wbe building class attributes abalone liver page blocks 
difficulty predicting times interesting noted total time set varies quite widely seemingly similar dimensions 
note total time page blocks greatly exceeds abalone building 
noteworthy time scaled unscaled cases modeling phase dominates scaling implemented greatly increases time spent building evaluating models 
part stem capability nonlinear scaling model produce multiple versions attribute 
system builds models version output attribute nonlinear scaling phase implemented substantially increases amount trees get built 
scaling appears encourage slight increase number attributes inputs means tree involves computation involves dimensions 
additional overhead applying inverting scaling need 
effect appears roughly fold increase computation time 
presumably reduced stricter fit tests lessen currently drastic impact dimensionality placing arbitrary limits number versions permitted attribute 
rough rule thumb appears data set nonlinear axis scaling increases absolute amount time spent modeling proportion increase product number inputs outputs 
rough guideline useful prediction sets 
data selection modeling total abalone baseball building cia world factbook dj housing liver page blocks wind table timing results unscaled case 
times cpu times expressed seconds 
modeling phase obviously expensive portion implemented 
scalability fixes data set useful know system scales 
examined timing executions different samples page blocks set 
sample fraction intervals independent samples drawn 
samples processed nonlinear axis scaling 
figures shows results individual selection scaling modeling phase total times nonlinear scaling 
cases cpu time required scales linearly sample size dominated modeling phase 
linearity suggest approach determining feasibility applying system particular data perform scaling selection full data build models samples sizes 
trend just linear page blocks provide estimate amount time required building model full data addition improvements accuracy larger samples lack thereof may informative value doing 
figures show similar tests sampling dj set 
results scaling follow pattern minuscule selection time mod data scaling selection modeling total abalone baseball building cia world factbook dj housing liver page blocks wind table timing results scaled case 
times cpu times expressed seconds 
modeling phase extremely expensive compared rest 
increased costs versus table reflect greater dimensions resulting multiple versions scaled attributes 
data tuples attr input abalone baseball building cia world factbook dj housing liver page blocks wind table size unscaled data set 
dimensions relate strictly original data attributes set ignore 
unscaled data number outputs number non ignored attributes number inputs 
data tuples attr scaled abalone baseball building cia wf dj housing liver page blocks wind table size scaled data set 
dimensions relate strictly original data attributes set ignore year attribute data 
number possible outputs total number versions attributes scaled version input version trees built tested 
cpu time seconds scalability data size versus cpu time unscaled sampling fraction page blocks selection time modeling time total time timing runs different sampling frequencies page blocks set nonlinear axis scaling 
error bars indicate minimum maximum timings 
sampling size bars reflect selection time basically compared modeling time modeling time total time 
total time appears scale linearly sample size 
cpu time seconds scaling time selection time scalability data size versus cpu time scaled sampling fraction page blocks modeling time total time timing runs different sampling frequencies page blocks set nonlinear axis scaling 
error bars indicate minimum maximum timings 
individual bars reflect scaling selection modeling total time 
total time shows linear correlation sampling size 
cpu time seconds selection time scaling modeling time scaling scalability data size versus cpu time sampling fraction dj total time scaling timing runs different sampling frequencies dj set nonlinear axis scaling 
error bars indicate minimum maximum timings 
sampling size bars reflect selection time basically compared modeling time modeling time total time 
total time appears scale linearly sample size 
cpu time seconds scaling time selection time scaling scalability data size versus cpu time sampling fraction dj modeling time scaling total time scaling timing runs different sampling frequencies dj set nonlinear axis scaling 
error bars indicate minimum maximum timings 
individual bars reflect scaling selection modeling total time 
total time shows linear correlation sampling size 
eling time scales linearly sample size results nonlinear axis scaling unstable appears entirely due variance selection time 
may related large number attributes compared relatively small amount data involved 
sampling size grows modeling time continues grows proportionally set larger expect modeling time eventually dominate total time page blocks 
chapter discussion recall major concerns experiments 

impact nonlinear scaling attribute selection process 

resulting models 

find 
points encompass major point thesis quantitatively evaluate impact particular automated nonlinear scaling system heuristics behavior modeling system 
attribute selection came play order enable dealing cases attributes labeled inputs outputs 
third point deals possibility information noted system 
results described previous chapter lead investigator basic 
nonlinear scaling generally led attribute selector select slightly attributes necessarily exact superset 
model performance measured quantile error improved significantly model complexity remained fairly stable cases 
tests sbc intc dj set suggest nonlinear scaling impact model accuracy constrains unscaled input selection unscaled forms inputs selected unscaled case 
third interesting trends dominance mixture normals distribution odd anomaly 
rest chapter goes greater detail regarding 
impact attribute selection continuous integer attribute original set system produces version enabled box cox transformation 
estimation probability distribution followed anderson goodness fit tests purposes filtering poor fits results computation cumulative distribution functions acts scaling function particular transformed version 
theory cumulative distribution functions result reasonable distribution fits reduce skew 
transformed scaled forms plus original version attribute define list attributes attribute selector may choose 
recall attribute selector operates basis fractal dimension 
metric serves method assessing intrinsic dimensionality multidimensional self similar point set 
requirement self similarity may impact applicability allow efficient computation ability handle sets may confound conventional methods rely solely linear components 
attribute selector greedily selects attributes maximize fractal dimensionality 
attempting increase fractal intrinsic dimensionality equivalently ideal number parameters necessary defining relevant manifolds system prefer uniformity attribute independence attributes previously selected 
scaling system described earlier document reduce skew increase uniformity may expect attribute selector chooses attributes 
experiments conducted nonlinear scaling enabled selector module chose attributes nonlinear scaling 
necessarily choose superset choose attributes unscaled case 
furthermore improvements model accuracy credited selection inputs selection ascribed nonlinear scaling 
noted selector provided original version attribute addition version nonlinear scaling passed goodness fit tests 
selected inputs scaled sets just identity transformation strongly suggests data uniformly distributed axes 
impact resulting models scaling selection may provide useful information construction evaluation regression trees allows prospective user evaluate system error metric choice 
addition models may useful right exceptions interest utility model exceptions readily lend quantitative assessment 
material discovery explainable anomalies consists primarily anecdotal evidence relegated section chapter 
noted regarding performance modeling system basic types statistics applicable 
concern size regression trees errors produced trees evaluated data sets classification accuracy appropriate 
table aggregates high level results areas counting attributes modeled unscaled scaled cases models accurate sum squared quantile error efficient terms number nodes regression trees 
models superior accuracy fewer nodes fewer nodes better accuracy improvements modulo computational complexity scaling models inferior accuracy worse efficiency improvements aspect clearly worse 
models efficiency accuracy changed 
remaining models better accuracy worse efficiency vice versa need considered carefully order fairly assess merit 
classification main focus thesis discussed appendix model size class consists number nodes selected binary regression tree may deduce number test nodes leaf nodes 
storage cost regression tree implemented currently dominated leaf node cost 
test node stores single attribute index indicating test attribute single value numerical test tests set symbolic values membership test 
leaf node stores linear equation containing additive constant coefficient input attribute 
cost tree better worse fewer nodes total total total abalone abalone dj building housing page blocks nodes total total total baseball liver abalone cia wf wind baseball dj building housing liver wind nodes total total total building abalone dj housing page blocks wind table tallies attributes modeled unscaled scaled cases 
columns indicate version nonlinear scaling better approximately difference worse results rows model scaling fewer nodes unscaled case 
results shown totals data set 
cases scaling helps clearer considers case accurate complex models generally models accurate complexity 
conditionally independent nonlinear scaling tree structure number attributes selected number attributes available selection 
practice majority trees selected nodes fewer means fewer linear equations fewer test nodes 
storage costs quite small small data sets larger tree risk overfitting 
note scaling applied rarely effect number nodes 
benefit penalty modeling accuracy broadly attributed substantial change model complexity additional metadata defining actual scaling 
exceptions area attribute page blocks set 
attribute received inaccurate node tree nonlinear scaling precise node tree 
directly related tree size hypothetical nonlinear scaling regression tree building algorithm particular splitting pruning criteria 
system implemented uses particular set rules intended fairly compact trees users may different opinions appropriate balancing point 
decisions held constant regardless scaling choice data set introduce possibility differing results systems scope tests nonlinear scaling lack thereof single major experimental variable 
errors explicitly presenting trees data source code tuple errors acceptable description analysis models performed fundamental task regression 
attributes contained integer values cost model applied considers cost model basic accounting assuming ieee bit double precision floating point numbers fields ordinary prefix free codes additional storage necessary identifying tuple index magnitude errors 
methodology comes semantic compression point view allows balance storage cost model accuracy accordance known practice 
sets integer attributes exceptions baseball page blocks 
noting limited basis comparison fair suggest nonlinear scaling generally reduced bit costs particularly page blocks set attributes area 
continuous attributes task fairly measuring model performance considerably complicated 
quantile metric designed look solely model accuracy point view disambiguation task varies expected accuracy local data density 
greater precision necessary ranges tuples fall sparser regions 
previous chapter presents sum squared quantile errors 
results real data included median maximum magnitudes quantile errors occasional note possible cause apparent anomalies 
general reported quantile errors better nonlinear scaling increase tree size usually fairly limited 
occasional exception length attribute page blocks set shows 
findings discuss findings system 
hinted terms investigation anomalies different metrics 
instance models revealed multiple unusual specimens abalone set 
anomalies included page blocks tuple pertaining unusually large block white black transitions possibly errors dj set 
instance dj set provides case anomalies input attribute suspected due sudden jumps errors wide variety models 
likewise models exist substantial blocks time result worse error compared usual nature data lets speculate cause 
world factbook area population data provides examples scaling exists broad simple trend area population model quantile error allows highlight territories nations break trend greenland remarkably low population density separate denmark 
evidence baseball page blocks sets integer attributes common suggests clear method relates quantile errors traditional bit cost system relationship 
scaling aids modeling generation selection traditional integer bit cost scheme general quantile error metric 
system running numerous experiments shown number interesting facts 
instance commonly chosen probability distribution mixture normals multiple components significantly flexible included extremely difficult mixture independently doubly truncated normals 
truncated normals single normals uniform distributions gamma distribution selected frequently 
fractal dimension selector may set strict attributes reduced quantile errors cases rad housing set page blocks set modeling extremely accurate 
separate issue correctness performance 
timing tests suggest current implementation modeling phase requires far cpu time segment system nonlinear scaling greatly increased time spent 
effort spent improving speed best directed phase 
timing results suggest merely knowing dimensions original data set suffice accurately estimate required time note instance drastic difference required time similarly sized abalone page blocks wind sets 
addition cpu time required appears scale linearly number tuples data set samples blocks scaled unscaled cases 
chapter thesis involved design implementation examination system automatic discovery patterns functions relating attributes outliers 
encompasses nonlinear scaling axes labeling attributes inputs outputs modeling mathematical relationships inputs outputs automated fashion tuning 
chapter describes experiments analyze performance system terms accuracy complexity resulting models scalability system chapter summarizes results 
table clear application nonlinear axis scaling helped 
scaling enabled accurate models number nodes regression tree times times scaling resulted accurate larger regression tree 
cases scaling resulted opposite trade accurate smaller model 
attributes resulted ties trees size essentially unchanged accuracy 
cases system worse accuracy complexity improvement 
generation accurate models revealed outliers exceptions accurate rules 
recall instance greenland cia world factbook data 
sparsely populated unusual usual relationship area population obvious nonlinear axis scaling 
scaling model area performs worse worst errors appear concentrated regions regardless population density unusual 
likewise axis scaling certainly helped models dj turn revealed variety periods anomalous behavior individual anomalous behavior armed dates search engine frequently linked events plausibly influence prices question cause individual stock break away usual patterns 
inaccurate models tuples look poor justify searching exceptions nonlinear axis scaling automated domain knowledge tuning improve models point interesting exceptions emerge 
short nonlinear axis scaling help lot 
guaranteed adds significantly computational cost powerful tool worth serious consideration addition simpler methods affine transformations solely normalization 
appendix doubly truncated normals described main body exists long known estimator normal zero independent truncation points number extrema mean second moment standard deviation observations cohen jr 
requires simultaneous solution nonlinear equations involve integration requires great care judging convergence lack thereof 
result implementation specific decisions flaw published algorithm method perform especially relative computational cost 
sample results included 
experimenter designed implemented estimator levenberg marquardt numerical search 
estimator extracts number quantiles number observations whichever corresponding percentiles 
numerical search operates parameter space defined unknown mean standard deviation normal trying minimize sum squared error observed quantiles predicted 
remaining parameters truncation points solved assumption observed extrema data number observations 
truncation points truncate original distribution considered nonexistent distribution truncated direction 
bootstrap estimation search begins mean standard deviation quantiles 
runs reaches specified number iterations achieves particular error tolerance sum squared error fails improve certain amount whichever comes randomly select current best known state possibly alter parameters resume searching total executions 
estimator uses parameter set 
randomness depending arbitrary parameters theory reduce odds returning poor result due converging highly suboptimal local minimum 
comparison generated set points drawn independently percentiles standard unit normal 
truncation points approximately original standard deviations original mean 
density estimating truncated normal pdfs generated data standard normal estimator cohen estimator levenberg marquardt estimator shows probability distribution functions pdfs test estimated distributions 
case test distribution normal truncated percentiles 
cohen estimator gives similar results standard method moments normal levenberg marquardt estimated pdf essentially coincident test distribution 
shows estimation results 
ordinary method moments estimator non truncated distribution yields mean standard deviation approximately seconds 
moments estimator cohen jr consumed seconds estimated truncated normal normal mean standard deviation placed truncation points original standard deviations deviations original mean 
numerical search described consumed seconds estimated original mean standard deviation repeated executions showed number local minimal similar pdfs 
truncation points estimated original standard deviations original mean original generation parameters considerably closer moments method 
test case truncation points side mean generated set 
points time truncation points set percentiles approximately original standard deviations original mean 
density estimating truncated normal pdfs generated data standard normal estimator cohen estimator levenberg marquardt estimator shows probability distribution functions pdfs test estimated distributions 
case test distribution normal truncated percentiles 
cohen estimator gives similar results standard method moments normal levenberg marquardt estimated pdf matches test distribution 
shows estimation results test 
standard estimator normal took approximately seconds estimate mean standard deviation 
moments estimator doubly truncated normal cohen jr required seconds estimate mean standard deviation respectively setting truncation points original standard deviations mean respectively 
search method took seconds yield closest estimate mean standard deviation truncation points original standard deviations mean 
generated point set standard unit normal truncation points standard deviations mean corresponding approximately percentiles 
standard normal estimator took seconds estimate mean standard deviation 
seconds moments estimator suggested original mean standard deviation respectively truncation points original standard deviations mean 
search method took seconds estimate mean standard deviation respectively truncation points original standard deviations mean 
search algorithm lacks certain elegance possessed analytical methods types cases described highly asymmetric truncation truncation points mean middle case truncation points side asymmetrically placed simple case truncation points equally far mean 
respect applicability may noted axis scalings data sets survived goodness fit tests axis scaling phase involved truncated normal 
second common scaling uses mixture normals ahead invocations single non truncated normal 
density estimating truncated normal pdfs generated data standard normal estimator cohen estimator levenberg marquardt estimator shows probability distribution functions pdfs test estimated distributions 
case test distribution normal truncated percentiles 
cohen estimator normal estimator select significantly smaller standard deviations test data resulting density assigned mean levenberg marquardt estimator tracks test distribution 
appendix mixtures truncated normals mixtures univariate normals may estimated variety means primarily expectation maximization em described dempster johnson kotz variation thereof bradley 
expectation maximization multicomponent distribution summarized procedure 

identify number components practice may considered repeating rest algorithm different numbers components determining best suitable metric 
bayesian information criterion may considered way balance complexity additional components improvements log likelihood 

component estimate initial parameters corresponding probability density function fi component mixture probability pi sum mixture probabilities sum 
em frameworks may leave exercise reader helpful adopt policy random initialization multiple executions 

repeat loop convergence metric log likelihood value xj set cardinality components compute non normalized weights wi xj 
compute wi wi wl normalized responsibilities component datum component re estimate corresponding fi weighted data recompute pi wi 
return best known set pi works mixtures ordinary normals 
directly apply normals independent truncation points knows estimate truncation points individual components 
component non zero responsibility wi datum due datum inside truncation points component estimator decide truncate point moving truncation point inwards 
methods explored course satisfactory stage provide basis reasonable method 
modified em obvious approach adjust wi estimation 
instance component compute narrowest range wi max wk responsibility threshold 
words component partly responsible points responsible responsibility 
set wi values set truncation points accordingly re normalize wi weights new pi mixture probabilities wi truncation performed 
choosing provides obvious difficulty undoing truncation point component area responsibility 
windowing second approach sacrifices theoretical flexibility ignoring possibility significant overlap components 
reduction theory permit divide conquer approach allowing distributions ugly model mixture ordinary normals 
priority windowing method identify potential boundaries components call potential boundaries breakpoints 
system starts gaussian kernel estimate density datum 
local minima results regions unusually low density provide set breakpoints may correspond areas components 
estimator applies natural logarithm estimated densities 
logarithms density function normal certain resemblance parabola 
modeled quadratic equations conveniently iteratively fit algorithm called recursive squares rls 
rls parameter controls stability estimation respect updates shifting trends running rls estimators data different stability estimates checking divergence may provide way estimating parabola component begins ends 
long estimate parabola previously models similar values shifting may jump different points 
areas sharp divergence suggest breakpoints rls models reset 
yields set breakpoints defines segmentation data 
windowing estimator starts extrema greedily looks largest consecutive set segments including extrema plausibly estimated single truncated normal goodness fit tests 
longest set satisfactory fit removed estimation continues 
process clearly computationally expensive sacrifices flexibility practice suggests performance applicability far inferior standard mixture normals 
appendix estimating fractal dimension recall fractal dimension defined log pr log pr infinite perfectly self similar set conditions partial derivative constant varying finite sets force techniques looking maximally linear contiguous subset log log pr pairs minimum size 
naive implementation requires checking possible pair values clearly quadratic 
data sets realistic size reasonable approach 
fortunately quite approximations better scalability 
methods practice scale linearly data set size lower constant factor requires far storage may performed incrementally 
box counting known called box counting schroeder traina jr 
basic version algorithm described 
define geometric series distances ri distances ri ideally chosen respect data set 
ri perform steps 
define dimensional cells length ri axes 
simple approach compute coordinate xj index xj ri 
assign attribute tuple cell 
compute sum squared cell occupancies cri 
compute approximation log cr log box counting algorithm 
algorithm described traina jr improves performance hierarchical data structure store compute counts multiple resolutions requiring pass radius simultaneously explicitly storing ri 
likewise database storing cell counts iterate occupied cells radius compute counts coarser radius computational cost basic algorithm variations thereof may expected scale approximately linearly data cardinality potentially high storage cost depending tracks cell occupancies 
tug war investigator involved described wong wong focused developing algorithm approximating fractal dimension minimal amount storage space respect number tuples performed incrementally possessed linear scalability terms computational cost versus number tuples involved 
tug war algorithm relied result alon showed way compute second moment frequencies set incrementally inexpensively probably approximately correct pac bounds 
tug war algorithm uses approach approximate cri second moments computed box counting 
noted relevant theory experimental results see alon wong wong 
algorithm practice return approximations minimal storage costs computational cost linear number tuples constant factor higher due amount calculations 
large data set risk overwhelming box counting due excessive storage costs tug war reasonable choice 

set parameters reflects level approximation related probability 
setting probability estimate second moment greater relative error 

generate random wise independent hash functions ri number radii 
hash function tuple random distinct primes large distinct prime need vary function 
tuple dimensional index gk gj xj hash function returns depending ri parity number value result result gj modq 
associate hash functions counter initialized zero 

tuple increment decrement counter result corresponding function 

estimate cr radius partition counters sets counters 
compute square counter 
compute arithmetic mean squares subset 
compute median means 
estimate 

fractal dimension estimated time estimating cr values performing usual partial derivative computation 
tug war algorithm 
appendix regression trees phase performs actual modeling uses piecewise linear regression tree secret system described dobra gehrke substantial differences 
appendix provides details construction pruning tree 
construction secret modeling phase produces piecewise linear binary regression trees separate construction pruning phases 
current implementation adopts procedure estimating mixture multivariate normals order apply labels purposes split point selection estimator method bradley 
axis aligned splits currently supported 
follow logic secret rely gini index splitting discrete attributes method loh shih continuous attributes 
may differences certain criteria limit growth tree splitting node covers fewer tuples 
linear models computed node recursive squares dealing tuples singular value decomposition 
tree construction similar normal secret method 
processes result binary trees internal nodes contain tests value continuous attribute equal threshold value symbolic attribute certain set leaf nodes contain linear models data reached leaf 
pruning significant differences occur 
published tests secret quinlan resubstitution error pruning quinlan 
addition cross validation techniques described section version written thesis uses pruning metric explicitly converts error magnitudes bits order compare presumably equal lower error splitting node increased cost storing parameters children 
needs profile error distribution node 
expensive recompute error profiles basis individual prune split decisions system summarizes errors approximate quantizer greenwald khanna provide profiles distribution error magnitudes 
split pruned error profile originally computed model parent estimated error magnitudes stored profile relative population sizes children 
convert error magnitudes bits system normalizes dividing range data covered node question 
fractions multiplied rounded resulting non negative integers assessed terms standard prefix free codes 
total cost error profile counted bits compared cost storing parameters relevant subtrees 
setting balance obvious issues current setting prevent pruning excessive benefits reduced errors trivial compared number inputs number parameters required child 
cross validation additional hedge overfitting model builder performs fold cross validation follows 
data set randomly partitioned segments equal size 
system runs independent trials selects segments building regression tree segment pruning final segment evaluating 
tree builder returns tree lowest sum squared error evaluation segment 
reasonable change evaluate data set quantile metric espoused parts 
quick comparison comparison checked data set uci repository murphy aha data attributes rings labeled inputs rings output 
mirrors tests executed dobra gehrke numerous trials achieved mean squared error modeling rings oblique splits complexity resulting tree unfortunately published 
addition pruning method described evaluated traditional pruning method cares accuracy reduces overfitting due holdout set pruning selecting multiple trees 
investigator code balanced storage aware pruning fold cross validation tree selection process yielded single node tree worse prediction error scaling barely reduced single node tree 
accuracy pruning method cross validation harness model builder selected node tree mean squared error sans scaling 
combining accuracy nonlinear scaling node tree obtained mean squared error rings 
boston data set referenced evaluation secret set labeled housing document 
encoding secret original data attributes inputs median house value secret piecewise linear trees oblique splits yielded mean squared error restricted axis orthogonal splits 
single node tree balanced mean squared error accuracy node tree mean squared error orthogonal splits implemented 
combining nonlinear scaling accuracy mean squared error rose nodes 
appendix linear models obvious question discarded nominal attributes built linear models 
linear models obviously far versatile piecewise linear trees built chapter advantages lines fast compute may readily interpreted terms inputs outputs require judgments regarding pruning split criteria 
nonlinear axis scaling improve performance simpler models 
models perform compared versatile complex models discussed 
experiments answer questions build linear models data sets consider results 
choice nonlinear axis scalings inputs remains quantile metric 
cross validation regression performed full data 
abalone table shows results linear models abalone set 
comparing tables striking close accuracy post nonlinear scaling linear models complicated models described chapter 
unscaled linear models significantly accurate piecewise linear counterparts 
error attribute unscaled scaled length input diameter height input weight weight input weight input shell weight rings table quantile errors linear models abalone data set nonlinear axis scaling 
emphasized numbers indicate lower better squared quantile errors attributes modeled unscaled scaled cases 
baseball original models described table linear table table reflects stability model building process 
clearly samples purposes cross validation original model building process generally little impact accuracy compared building model entire data 
trend nonlinear axis scaling improving accuracy quite clear 
building accuracy results linear models table may compared tables 
models linear unconstrained case unscaled hour wbe scaled model performances reported changed 
models suffered varying degrees 
node model scaled version solar instance sum squared quantile error 
node model unscaled version improvement sum squared quantile error error attribute unscaled scaled ba slg input ab tb hr bb input sb cs input input table quantile errors linear models baseball data set nonlinear axis scaling 
emphasized text reflects better scores 
error attribute unscaled scaled hour input temp input input solar wbe table quantile errors linear models building data set nonlinear axis scaling 
emphasized scores better unscaled scaled 
cia world factbook unconstrained case scaling single linear model built 
re running model cross validation serves stability check evaluating powerful model useful 
unscaled scaled cases sole model area 
squared quantile errors unscaled scaled cases respectively 
interestingly score unscaled case quite different original score cross validation lack thereof clearly difference 
scaled score essentially identical original 
dj sum squared quantile errors strictly linear models may table 
piecewise linear models described tables nonlinear axis scaling improves accuracy attribute modeled cases 
unconstrained models linear 
unscaled case gm sbc scaled case dd 
cases sum squared quantile errors quite similar cross validation 
remaining cases piecewise linear models quite frequently significantly better piecewise linear models reducing sum squared quantile error versus linear counterparts 
housing table shows sum squared quantile errors linear models housing data 
table shows unscaled unconstrained models crim zn linear linear versions crim zn produce sum squared quantile errors approximately times sum squared quantile errors piecewise linear models 
remaining models sum quantile errors changed little original model set built sample full data performs extremely poorly large number tuples 
error attribute unscaled scaled aa axp bs cat ci dd dis ek ge gm hd hon ibm intc jnj jpm input ko mcd mmm mo mrk msft pg sbc table quantile errors linear models dj data set nonlinear axis scaling 
emphasized scores better unscaled scaled 
respect scaled models table original unconstrained models tax linear 
linear model scaled tax performs comparably node single split model linear models perform quite similarly original linear models built cross validation deviations approximately sum squared quantile error 
error attribute unscaled scaled crim input zn indus nox input age input dis rad tax input lstat table quantile errors linear models housing data set nonlinear axis scaling 
emphasized scores better unscaled scaled 
liver table shows results strictly linear models liver set 
tables note original models strictly linear serves test sensitivity sampling versus building model entire data 
differences sum squared quantile errors results approximately 
error attribute unscaled scaled mcv input drinks table quantile errors linear models liver data set nonlinear axis scaling 
emphasized scores better unscaled scaled 
page blocks table shows accuracy results strictly linear models built blocks data sans cross validation 
may compared results unrestricted cross validated models tables 
unconstrained models non class attributes length unscaled height scaled linear models rest ranged remarkably high nodes 
sum squared quantile errors deviated somewhat scaled version height model described having approximately lower sum quantile error 
deviation unscaled variant length significantly lower 
examine performance differences multi node trees built cross validation linear models built data 
unscaled outputs large deviations area wb trans area multi node models worse 
scaled outputs linear models worse multi node counterparts extremely area nodes reduces sum squared quantile error factor wb trans node model reduces sum squared quantile error factor approximately 
wind table shows sum squared quantile errors linear models wind set 
may compared tables 
original error attribute unscaled scaled height length area black input mean tr input input wb trans table quantile errors linear models page blocks data set nonlinear axis scaling 
emphasized scores better unscaled scaled 
unconstrained models unscaled outputs mal multi node model scaled outputs multi node models exist val bir mal 
unscaled outputs linear models produce quite similar results regardless built full data set 
node model mal approximately lower sum squared quantile error node model 
considering scaled outputs see linear models built full set produce essentially sum squared quantile errors linear models built cross validation pruning sets rpt kil dub cla mul clo 
node model scaled val barely accurate node model scaled bir worse node model scaled mal roughly lower error linear model 
summary notable drawn constrained tests 
clear model class constrained strictly linear regression nonlinear axis scaling allowed improved model accuracy 
true sets baseball number error attribute unscaled scaled rpt val kil sha input bir dub cla input mul clo bel input mal table quantile errors linear models wind data set nonlinear axis scaling 
emphasized scores better unscaled scaled 
inputs models remained unaffected scaling 
second comparison models built full data set cross validation linear models resulted full cross validation process attributes significant difference accuracy differences quite drastic 
clear readily predicted outside multiple samples presence extreme outliers relative regression results full set presumably circumstance cause gap 
couple cases model built cross validation worse suggests poor choice sample test phase 
sampling allocation tuples building pruning testing data may better choice 
third cases multi node models significantly better strictly linear models linear models built data multi node models built half data pruned quarter selected evaluations quarter 
extreme cases sum squared quantile error linear models times multi node models 
appendix classification important note thesis project design focus classification 
classification differs strongly regression obvious difference regression requires accurately predicting ordered continuum classification stipulates possibly unordered discrete set outcomes 
regression lends regression trees application trees classification complicated 
create multiple trees class declare prediction matches tree highest answer 
addition heuristics commonly build regression trees designed minimize sum squared error sense classification task 
logical tree built heuristic aimed classification gini index shannon information gain 
abalone liver page blocks data sets specific classification tasks obvious question regression trees classify impact nonlinear axis scaling tasks 
results quite mixed instance page blocks combining accuracy axis scaling gives efficient solution accuracy scaling case abalone liver comparison leads opposite 
impact may interest examine better applied 
abalone documentation abalone set notes previous experiments classification aggregated distinct values rings attribute classes 
doing backpropagation neural network hidden units scored approximately correct murphy aha 
set regression testing value aggregation see appendix tests involving secret modeler trees 
selector set regard attributes inputs regression tree set prune accuracy unscaled scaled trees performed par accuracy nodes respectively 
balanced unscaled scaled trees yielded single node trees scoring 
liver liver set binary selector attribute classification performance measured treating binary selector attribute integer attribute 
resulted automatic rounding regression output nearest integer 
treating attributes inputs balanced accuracy achieved scaling 
cases involved single node trees 
accuracy performance increased approximately cases node tree unscaled case node tree scaled case 
documentation specify previous classification accuracy murphy aha constant majority class predictor achieve classification accuracy 
page blocks page blocks set class attribute originally encoded integer corresponding unordered set classes text horizontal line graphic vertical line picture 
class attribute split binary integer attributes cor responding different classes 
models collectively considered classified correctly correct model corresponding class returned models strict approach continuous outputs select class maximal output marking ties incorrect 
distribution classes extremely skewed fully tuples labeled text tuples labeled graphic 
constant majority class predictor achieve accuracy 
setting attributes inputs system achieved classification accuracy nonlinear scaling total nodes accuracy nodes balanced bizarre result skewed distribution classes 
accuracy cases yielded classification accuracy axis scaling allows efficient solution total nodes nodes unscaled scale 
bibliography online april 
www com investor content apr pi pi htm 
agrawal gehrke gunopulos raghavan 
automatic subspace clustering high dimensional data data mining applications 
proc 
acm sigmod international conference management data pages june 
akaike 
new look statistical model identification 
ieee trans 
automatic control ac 
alon matias szegedy 
space complexity approximating frequency moments 
proc 
th acm symposium theory computing pages may 
babu garofalakis rastogi silberschatz 
model semantic compression network data tables 
proc 
may 
baldi hornik 
neural networks principal component analysis learning examples local minima 
neural networks 
bartell cottrell belew 
latent semantic indexing optimal special case multidimensional scaling 
proc 
acm sigir 
bradley fayyad reina 
scaling em expectation maximization clustering large databases 
technical report msr tr microsoft research 
breiman olshen stone 
cart classification regression trees 
chapman hall crc press 
calvo partridge jabri 
comparative study principal component analysis techniques 
proc 
ninth australian conf 
neural networks february 

dj 
lib stat cmu edu 
central intelligence agency 
world factbook 
www cia gov cia publications factbook index html 
chakrabarti mehrotra 
local dimensionality reduction new approach indexing high dimensional spaces 
abbadi brodie chakravarthy dayal kamel 
whang editors proc 
th international conference large data bases pages 
morgan kaufmann september 
isbn 
chang ghosh 
principal curves nonlinear feature extraction classification 
spie applications artificial neural networks image processing iii 
cheng bell liu 
learning belief networks data information theory approach 
proc 
sixth international conference information knowledge management pages 

design implementation genetic algorithm data mining 
abbadi brodie chakravarthy dayal kamel 
whang editors proc 
th international conference large data bases pages 
morgan kaufmann september 
isbn 
cohen jr estimating mean variance normal populations singly truncated doubly truncated samples 
annals mathematical statistics december 
dasgupta gupta 
elementary proof johnson lindenstrauss lemma 
technical report tr uc berkeley 
demers cottrell 
non linear dimensionality reduction 
hanson cowan giles editors advances neural information processing systems volume pages 
morgan kaufmann san mateo ca 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
dobra gehrke 
secret scalable linear regression tree algorithm 
proc 
acm sigkdd pages july 
esposito malerba semeraro 
multistrategy learning document recognition 
applied artificial intelligence 
faloutsos 
lin 
fastmap fast algorithm indexing data mining visualization traditional multimedia datasets 
acm sigmod pages may 
pace 
harrison rubinfeld data 
journal environmental economics management 
greenwald 
sears lead blue collar rally april 
www com bn index cfm story 
greenwald khanna 
space efficient online computation quantile summaries 
proc 
acm sigmod international conference management data pages may 

ge wins honeywell hand welch stays 
www com bn index cfm story 
hansen yu 
model selection principle minimum description length 
journal american statistical association 
harrison rubinfeld 
hedonic prices demand clean air 
journal environmental economics management 
raftery 
space time modeling long memory dependence assessing ireland wind power resource 
applied statistics 
hung cheng chee fu zhang 
entropy subspace clustering mining numerical data 
proc 
th international conference knowledge discovery data mining pages august 
hyv rinen 
survey independent component analysis 
neural computing surveys 
hyv rinen oja 
independent component analysis 
john wiley sons 
jagadish ng 
semantic compression pattern extraction 
proc 
th international conference large data bases pages 
morgan kaufmann september 
isbn 
jagadish ng ooi tung 
iterative semantic compression algorithm 
proc 
international conference data engineering march 
jebara jaakkola 
feature selection dualities maximum entropy discrimination 
proc 
th conference uncertainty artificial intelligence july 
johnson kotz 
continuous univariate distributions 
houghton mifflin 
johnson 
honeywell united tech talks october 
money cnn com deals honeywell 
jolliffe 
principal component analysis 
springer verlag 
jones 
recurrent networks dimensionality reduction 
ai tech report mit 
mcdonald 
extended generalized lambda distribution system fitting distributions data history completion theory tables applications final word moment fits 
communications statistics simulation computation 
keogh chakrabarti mehrotra pazzani 
locally adaptive dimensionality reduction indexing large time series databases 
proc 
acm sigmod international conf 
management data sigmod 
keogh lonardi 
parameter free data mining 
proc 
th acm sigkdd international conference knowledge discovery data mining pages august 
kohavi john 
wrappers feature subset selection 
artificial intelligence 
kramer 
nonlinear principal component analysis autoassociative neural networks 
american institute chemical engineers 

great energy predictor building data analysis prediction competition june 
ashrae meeting 
kruskal 
non metric multidimensional scaling numerical method 
psychometrika 
lang 
newsweeder learning filter netnews 
proc 
th international conference machine learning pages 
loh shih 
split selection methods classification trees 
statistica sinica 
murphy aha 
uci repository machine learning databases 
department information computer science university california irvine ca 
www ics uci edu mlearn mlrepository html 
nash sellers talbot ford 
population biology abalone species 
abalone north coast islands bass 
technical report sea fisheries division 
issn 
nelder mead 
simplex method function minimization 
computer journal 
partridge calvo 
fast dimensionality reduction simple pca 
intelligent data analysis 
prechelt 
proben set neural network benchmark problems benchmarking rules 
technical report 
quinlan 
programs machine learning 
morgan kaufmann 
rissanen 
modeling shortest data description 
automatica 
roth 
generalized lasso wrapper approach gene selection microarray data 
technical report iai tr university bonn 
roweis saul 
nonlinear dimensionality reduction locally linear embedding 
science december 
sammon 
nonlinear mapping data structure analysis 
ieee transactions computers 
sato 
fast learning line em algorithm 
technical report tr atr human information processing research laboratories 
sch lkopf smola 
ller 
nonlinear component analysis kernel eigenvalue problem 
neural computation 
schroeder 
fractals chaos power laws minutes infinite paradise 
freeman new york 
takahashi tokunaga 
nonlinear dimensionality reduction multi layer perceptron superposed energy 
proc 
international symposium nonlinear theory applications volume pages 
tenenbaum de silva langford 
global geometric framework nonlinear dimensionality reduction 
december 
thiesson meek heckerman 
accelerating em large databases 
machine learning december 

inductive learning tree regression models 
phd thesis university porto september 
traina jr traina wu faloutsos 
fast feature selection fractal dimension 
proc 
xv brazilian symposium databases pages october 
aliferis 
principled feature selection relevancy 
proc 
artificial intelligence statistics january 
wallace dowe 
minimum message length kolmogorov complexity 
computer journal 

wang wang 
lin shasha shapiro zhang 
evaluating class distance mapping algorithms data mining clustering 
proc 
th international conference knowledge discovery data mining pages august 
weiss indurkhya 
rule machine learning methods functional prediction 
journal artificial intelligence research december 
wilson martinez 
improved heterogeneous distance functions 
journal artificial intelligence research january 
wong wu gibbons 
fast estimation fractal dimension correlation integral stream data 
proceedings second workshop fractals power laws generation data mining tools august 
wong wu gibbons 
fast estimation fractal dimension correlation integral stream data 
information processing letters 
xu jordan 
convergence properties em algorithm gaussian mixtures 
neural computation 

