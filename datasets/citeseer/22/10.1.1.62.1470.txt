svm knn discriminative nearest neighbor classification visual category recognition hao zhang alexander berg michael jitendra malik computer science division eecs department univ california berkeley ca malik eecs berkeley edu consider visual category recognition framework measuring similarities equivalently perceptual distances prototype examples categories 
approach quite flexible permits recognition color texture particularly shape homogeneous framework 
nearest neighbor classifiers natural setting suffer problem high variance bias variance decomposition case limited sampling 
alternatively support vector machines involve time consuming optimization computation pairwise distances 
propose hybrid methods deals naturally multiclass setting reasonable computational complexity training run time yields excellent results practice 
basic idea find close neighbors query sample train local support vector machine preserves distance function collection neighbors 
method applied large multiclass data sets outperforms nearest neighbor support vector machines remains efficient problem intractable support vector machines 
wide variety distance functions experiments show state art performance number benchmark data sets shape texture classification mnist usps object recognition caltech 
caltech achieved correct classification rate training images class training images 

field visual category recognition seen rapid progress years remains done reach human level performance 
best current approaches deal categories dataset materials caltech dataset objects long way estimate categories humans distinguish 
significant feature human visual recognition trained examples cf 
machine learning approaches digits faces currently require hundreds thousands examples 
thesis scalability dimensions best achieved framework measuring similarities equivalently perceptual distances prototype examples categories 
original motivation comes studies human perception rosch collaborators argued categories defined lists features similarity prototypes 
computer vision perspective important aspect framework emphasis similarity feature spaces gives flexible framework 
example shape differences characterized norms transformations needed deform shape explicitly realizing finite dimensional feature space 
framework scaling large number categories require adding new features perceptual distance function need defined similar objects 
objects compared sufficiently different human observers simply assign entirely different distance measure thompson quotes non 
training examples possible invariance certain transformations typical intra class variation built perceptual distance function 
study human notion shape similarity structural changes suggests characteristics 
readers may may philosophical arguments note argue feature sharing keeps problem man cal evidence studied visual recognition datasets humble nearest neighbor classifier chosen distance function outperformed considerably sophisticated approaches 
examples tangent distance usps zip code dataset simard lecun denker shape context distance mnist digit dataset belongie malik puzicha distances histograms textons data set leung malik varma zisserman geometric blur distances caltech berg berg malik :10.1.1.18.7575
note pleasant aspects nearest neighbor nn classifier techniques decision trees linear discriminants require explicit construction feature space distance functions intractable high infinite dimensional nn classifier deals multiclass nature visual object recognition effortlessly 
theoretical point view remarkable property mild conditions error rate nn classifier tends bayes optimal sample size tends infinity 
despite benefits room improvements nn classifier 
practical setting limited number samples dense sampling required asymptotic guarantee 
cases nn classifier suffers observed jig jag decision boundary 
words suffers high variation caused finite sampling terms bias variance decomposition 
various attempts remedy situation notably dann svm 
hastie tibshirani carries local linear discriminant analysis deform distance metric say nearest neighbors 
gunopulos deforms metric feature weighting weights inferred training svm entire data set 
vincent bengio collection nearest neighbors class span linear subspace class classification done distance prototypes distance linear subspaces intuition linear subspaces effect generate fantasy training examples 
distorting distance metric bypass cumbersome step arrive classification step 
propose train support vector machine svm collection nearest neighbors 
approach supported ingredients practice visual object recognition 

carefully designed distance function nn classifier transformed straightforward way kernel svm kernel trick formula distance function location origin affect svm 
various ways transforming distance function kernel possible 
svms operate kernel matrix underlying feature space bypassing feature space operations previous approaches dann feature vectors rn defined covariances computed classifying query see fig 
translates capability wide variety distance functions previous approaches limited distance 

practice training svm entire data set slow extension svm multiple classes natural nn 
neighborhood small number examples small number classes svms perform better classification methods 

observed psychophysics human perform coarse categorization quite fast image human observers answer coarse queries presence absence animal little ms course tell animal time 
process coarse quick categorization followed successive finer slower discrimination motivated approach model process setting machine learning 
nn initial pruning stage perform svm smaller relevant set examples require careful discrimination 
term method svm knn signifies method dependence choice number neighbors 

difference dann method class problem vs dann deforms metric nearest neighbors denoted dotted circle query positions classifies nn new metric method trains svm nearest neighbors preserving original distance metric directly obtains local decision boundary 
example take exp radial basis kernel fashion 
advantage complex transformation experiments stick simplest transformation retain intuitive interpretation 
philosophy similar local learning bottou vapnik pursued general idea nn followed linear classifier ridge regularizer 
distance driven constraint adapt complex distance function 
rest organized follows section describe method detail view different perspectives section introduces number effective distance functions section shows performance method applied distance functions various benchmark data sets conclude section 
svm knn naive version svm knn query 
compute distances query training examples pick nearest neighbors 
neighbors labels query labeled exit compute pairwise distances neighbors 
convert distance matrix kernel matrix apply multiclass svm 
resulting classifier label query 
implement multiclass svm step variants statistics learning literature tried small number samples data sets 
produce roughly quality classifiers dagsvm chosen better speed 
naive version svm knn slow mainly compute distances query training examples 
borrow insight psychophysics humans perform fast pruning visual object categories 
setting translates practice computing crude distance distance prune list neighbors costly accurate distance computation 
reason simply crude distance big certain accurate distance small 
idea works sense performance classifier unaffected computation orders magnitude faster 
earlier instances idea computer vision simard mori 
term idea 
additional trick speed algorithm cache pairwise distance matrix step 
follows observation training examples participate svm classification lie closely decision boundary invoked repeatedly query time 
preceding ideas incorporated steps svm knn query 
find collection ksl neighbors crude distance function 
compute accurate distance function tangent distance ksl samples pick nearest neighbors 
compute read cache possible pairwise accurate distance union neighbors query 
convert pairwise distance matrix kernel matrix kernel trick 
apply dagsvm kernel matrix label query resulting classifier 
far perspectives look svm knn viewed improvement nn classifier viewed model discriminative process plausible biological vision 
machine learning perspective viewed continuum nn svm small algorithm behaves straightforward nn classifiers 
extreme method reduces svm 
note large data set distance function costly evaluate training dagsvm intractable state art techniques sequential minimal optimization smo platt needs evaluate pairwise accurate distances 
contrast svm knn feasible long evaluate crude distance nearest neighbor search train local svm reasonable time 
comparison time complexity summarized table 
dagsvm svm knn training query sv ksl table 
comparison time complexity number training examples sv number support vectors cost computing accurate crude distances ksl length shortlist length list participating svm classification 

shape texture distances applying svm knn focus efforts classifying major cues visual object recognition shape texture 
introduce distances functions follows 
distance texture leung malik image texture mapped histogram textons captures distribution different types texture elements 
distance defined pearson test statistic texton histograms 

marginal distance texture statistical perspective distance texture viewed measuring difference joint distributions texture responses piece texture passed bank filters joint distribution responses vector quantized textons histogram textons compared 
joint distribution distinguished simply looking difference marginals histogram filter response 
distance function texture sum distances response histograms filter 
experiments real world images may contain types textons reliably quantized 

tangent distance defined pair gray scale images digits tangent distance defined smallest distance linear subspaces pixel domain number pixels derived images including perturbations small affine transformation spatial domain change thickness pen stroke forming dimensional linear space 

shape context distance basic idea shape context follows shape represented point set descriptor control point capture landscape point :10.1.1.18.7575
descriptors iteratively matched deformation model 
distance derived discrepancy left final matched shapes score denotes far deformation affine transformation 

geometric blur distance number shape descriptors defined gray scale image instance shape context descriptor edge map sift descriptor geometric blur descriptor 
experiments focus geometric blur descriptor 
usually defined edge point geometric blur descriptor applies spatially varying blur surrounding patch edge responses 
points center blurred reflect spatial uncertainty deformation 
blurring descriptors normalized norm 
kinds distances section 

distance asymmetry shape context distance geometric blur distance simply define symmetric distance practice discrepancy small 
triangle inequality tangent distance shape context distance geometric blur distance inequality hold times prevents distance translating positive definite kernel 
number solutions suggested issue 
compute smallest eigenvalue kernel matrix negative add absolute value diagonal kernel matrix 
intuitively view kernel matrix kind similarity measure adding positive constant diagonal means strengthening self similarity affect sense expressed similarity examples 

performance benchmark data sets 
mnist mnist data set handwritten digits contains examples training test set contains equal number digits distinct populations census bureau employees high school students 
digit image shape context computation digit resized image 
example digits test set shown fig 

number state art algorithms perform error rate shape context method performs 
distances experiment shape context distance 
shape context error rate may close bayes optimal training examples leave room improvement examples perform fold cross validation 
rely purely shape context image intensities drop appearance term :10.1.1.18.7575
summary results table 
note distance straightforward method number workarounds necessary shape context distance 
cases performance improves significantly 
sc limited training svm knn nn table 
error rate mnist percent parameter algorithm selected best performance range nn svm knn 
svm knn parameter ksl larger ksl doesn improve empirical results 

usps usps data set contains handwritten digits training testing collected mail envelopes buffalo 
digit image 
collection random test samples shown fig 

known usps test set difficult human error rate 
try types distances tangent distance 
shape context tried image small contain details estimating deformation 
tangent distance image smoothed gaussian kernel width obtain reliable tangents 

data sets mnist usps caltech tangent distance svm knn nn dagsvm platt intractable vincent table 
error rate usps percent parameter svm knn nn chosen best performance respectively 
table shows case error rates svm knn dagsvm similar 
svm knn faster train svm involves local neighborhood samples number classes rarely exceeds neighborhood 
experiments cost training examples classes smaller cost usual nearest neighbor search 
contrast dagsvm involves training svm pairs different classes computation pairwise distances training examples 
costly tangent distance function dagsvm intractable train experiment optimal svm knn fast usual nn classifier additional cost training svm examples negligible 
reflects comparison asymptotic complexity section 
case adaptive nearest neighbor technique vincent performs quite 
unfortunately operates input space extended distance function 
tangent distance case quite remarkable svm knn improve performance nn small additional cost performing comparison human performance 
encouraged think svm knn ideal classification method proper invariance structure underlying data set captured distance function 

dana contains images real world textures rabbit fur see fig 
photographed varying illumination viewing angle 
collection images viewing angle oblique picked category half randomly selected training rest test 
take variant texton method achieves best performance substitute step nn classifier method terms error rate case usps svm knn slight advantage dagsvm significantly better state art performance reported 
dagsvm equivalently svm knn slow total pairwise classifiers train cpu secs svm knn faster offers trade time svm knn nn dagsvm table 
error rate percent error rate std dev obtained times fold cross validation 
parameter svm knn nn chosen best performance respectively 
performance fig 

cpu sec error rate svm knn error rate svm knn time cost 
trade speed accuracy svm knn case texture classification 
caltech caltech data set collected fei fei consists images object categories additional background class making total number classes 
significant variation color pose lighting data set quite challenging 
number previously published papers reported results data set berg darrell 
berg constructed correspondence procedure matching geometric blur descriptors define distance function images resulting distance nn classifier 
darrell set local image features matched efficient way pyramid histograms 
resulting matching score forms kernel svm classifier 
fisher scores image obtained generative model object categories 
svm trained fisher kernel scores 
proceedings number groups addition demonstrated results dataset common methodology :10.1.1.81.1372
algorithms data 
difference lies choice distance function 
algorithm previous data sets setting shape texture 
shape part geometric blur fea tures sampled subset edge points section details 
texture part marginal distance texture see section filter bank leung malik 
distance function defined il ir min il ir il ir ir il il ir distance left right images 
computation geometric blur features denoting th feature left image respectively texture histograms hl denoting histogram filter output left image respectively hr 
note texture histograms normalized sum 
large scale geometric blur descriptors radius pixels set experiments small collection images classes 
stay close paradigm previous dataset geometric blur features followed methodology berg randomly picking images class splitting training test 
reverse role training test 
correctness rate average 
table shows results compared corresponding training images class 
compared baseline classifiers nn svm svm knn statistically significant gain 
algo 
svm knn nn dagsvm table 
correctness rate error rate algorithm training images class percentage std dev 
parameter svm nn nn chosen respectively best performance 
algorithm previous caltech berg berg malik sought find shape correspondence deformable template paradigm 
due special character caltech data set objects center image scale vary crude way incorporating spatial correspondence add order geometric distortion term geometric blur features compared position measured center image cf 
general approach second order geometric distortion comparing pairs points 
case distance function il ir min il ir il ir ir il rl denotes pixel coordinates th geometric blur feature left image image center respectively rr 
average image size 
medium scale geometric blur radius pixels algorithm tested benchmark methodology darrell number say images taken class uniformly random training image rest data set test set 
mean recognition rate class populous easier classes favored 
process repeated times average correctness rate reported 
experiments dagsvm classifier 
run svm knn setting performance svm knn better includes dagsvm special case 
performance algorithm plotted fig 
alongside current techniques published press format darrell 
noteworthy algorithm techniques wang lazebnik attained correctness rates neighborhood significant improvement reported result couple years ago 
numbers training images cases table 
confusion matrix training images fig 

buhmann different evaluation methodology correctness rate compared 
table 
correctness rate training images class caltech percentage std dev 
available 
proposed hybrid svm nn deals naturally multiclass problems 
show excellent results variety distance functions benchmark data sets 
experiments virtually difference evaluation methodology vs berg 
mean recognition rate class caltech categories data set zhang berg malik cvpr lazebnik schmid ponce cvpr berg thesis lowe cvpr darrell iccv berg berg malik cvpr wang zhang fei fei cvpr welling perona iccv serre wolf poggio cvpr fei fei fergus perona ssd baseline number training examples class 
correctness rate algorithm plotted format best viewed color results lazebnik schmid ponce berg lowe darrell berg berg malik wang zhang fei fei welling perona serre wolf poggio fei fei fergus perona 
background google faces easy faces airplanes anchor barrel ant beaver bass binocular brain butterfly buddha camera car side cannon ceiling fan chair cougar body cougar face crab head dalmatian cup dollar bill dolphin electric guitar elephant emu ferry head garfield grand piano helicopter inline ibis joshua tree kangaroo laptop lamp lotus octopus okapi pigeon panda platypus pizza pyramid rooster rhino scissors sea horse soccer ball snoopy sign sunflower tick umbrella water watch wheelchair windsor chair wild cat yin yang wrench algorithm confusion matrix train class background faces faces airplanes anchor ant barrel bass beaver binocular brain buddha butterfly camera cannon car ide ceilings chair cougar crab cup dalmatian dollar ill dolphin electric elephant emu ferry garfield grand helicopter inline kate ibis joshua ree kangaroo lamp laptop lotus octopus okapi panda pigeon pizza platypus pyramid rhino rooster scissors sea soccer snoopy ign sunflower tick umbrella water watch wheelchair wild windsor hair yin wrench ang 
confusion matrix algorithm training images class rows denote true label columns denote reported label 
confused pairs lotus platypus octopus 
belongie malik puzicha :10.1.1.18.7575
shape matching object recognition shape contexts 
ieee trans 
pattern anal 
mach 
intell 
berg 
shape matching object recognition 
phd thesis computer science division university california berkeley 
berg berg malik 
shape matching object recognition low distortion correspondence 
cvpr 
berg malik 
geometric blur template matching 
cvpr pages 
bickel 
mathematical statistics volume 
prentice hall nd edition 
bottou vapnik 
local learning algorithms 
neural computation 
bromley 
neural network nearestneighbor classifiers 
technical report tm 
cover 
estimation nearest neighbor rule 
ieee trans 
information theory 
crammer singer 
algorithmic implementation multiclass kernel vector machines 
mach 
learn 
res 
dana van nayar koenderink 
reflectance texture real world surfaces 
acm trans 
graph 
gunopulos 
adaptive nearest neighbor classification support vector machines 
nips pages 
fei fei fergus perona 
learning generative visual models training examples incremental bayesian approach tested object categories 
ieee cvpr workshop generative model vision 

similarity visually perceived forms 
psychological issues 
darrell 
discriminative classification sets image features 
iccv 
darrell 
pyramid match kernels discriminative classification sets image features version 
technical report csail tr mit 
hastie tibshirani 
discriminant adaptive nearest neighbor classification 
ieee trans 
pattern anal 
mach 
intell 
welling perona 
combining generative models fisher kernels object recognition 
iccv 
lazebnik schmid ponce 
bags features spatial pyramid matching recognizing natural scene categories 
cvpr 
lecun boser denker henderson howard hubbard jackel 
backpropagation applied handwritten zip code recognition 
neural computation winter 
lecun bottou bengio haffner 
learning applied document recognition 
proceedings ieee november 
lee lin wahba 
multicategory support vector machines theory application classification microarray data satellite radiance data 
journal american statistical association 
leung malik 
representing recognizing visual appearance materials dimensional textons 
int 
comput 
vision 

statistical issues texture analysis 
phd thesis department statistics university california berkeley 
lowe 
distinctive image features scale invariant keypoints 
int 
comput 
vision 
mori belongie malik 
shape contexts enable efficient retrieval similar shapes 
cvpr volume pages 
mori malik 
estimating human body configurations shape context matching 
european conference computer vision lncs volume pages 
lowe 
multiclass object recognition sparse localized features 
cvpr 
buhmann 
learning compositional categorization models 
eccv 
duin 
generalized kernel approach dissimilarity classification 
mach 
learn 
res 
platt 
analytic qp sparseness speed training support vector machines 
nips pages cambridge ma usa 
mit press 
platt cristianini shawe taylor 
large margin dags multiclass classification 
nips pages 
rosch 
natural categories 
cognitive psychology 
sch lkopf 
kernel trick distances 
nips pages 
serre wolf poggio 
object recognition features inspired visual cortex 
cvpr 
simard lecun denker 
efficient pattern recognition new transformation distance 
nips pages san francisco ca usa 
morgan kaufmann publishers simard lecun denker 
transformation invariance pattern recognition tangent distance tangent propagation 
neural networks tricks trade pages london uk 
springer verlag 
thompson 
growth form 
cambridge university press 
thorpe 
speed processing human visual system 
nature june 
torralba murphy freeman 
sharing features efficient boosting procedures multiclass object detection 
cvpr pages 
varma zisserman 
statistical approach texture classification single images 
international journal computer vision apr 
vincent bengio 
local hyperplane convex distance nearest neighbor algorithms 
nips pages 
wang zhang fei fei 
dependent regions object categorization generative framework 
cvpr 
