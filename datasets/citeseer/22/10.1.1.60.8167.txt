incremental online learning high dimensions vijayakumar vijayakumar ed ac uk school informatics university edinburgh edinburgh eh jz united kingdom aaron souza stefan schaal usc edu department computer science university southern california los angeles ca usa 
june locally weighted projection regression lwpr new algorithm incremental nonlinear function approximation high dimensional spaces redundant irrelevant input dimensions 
core employs nonparametric regression locally linear models 
order stay computationally efficient numerically robust local model performs regression analysis small number univariate regressions selected directions input space spirit partial squares regression 
discuss local learning techniques successfully high dimensional spaces review various techniques local dimensionality reduction deriving lwpr algorithm 
properties lwpr learns rapidly second order learning methods incremental training ii uses statistically sound stochastic leave cross validation learning need memorize training data iii adjusts weighting kernels local information order minimize danger negative interference incremental learning iv computational complexity linear number inputs deal large number possibly redundant inputs shown various empirical evaluations dimensional data sets 
probabilistic interpretation predictive variance confidence intervals derived 
knowledge lwpr truly incremental spatially localized learning method successfully efficiently operate high dimensional spaces 
despite progress statistical learning nonlinear function approximation high dimensional input data remains nontrivial problem especially incremental real time formulations 
increasing number problem domains properties important 
examples include line modeling dynamic processes observed visual surveillance user modeling advanced computer interfaces game playing learning value functions policies models learning control particularly context high dimensional movement systems humans humanoid robots 
ideal algorithm tasks needs avoid potential numerical problems redundancy input data eliminate irrelevant input dimensions keep computational complexity learning updates low remaining data efficient allow online incremental learning course achieve accurate function approximation adequate generalization 
looking learning framework address goals identify broad classes function approximation methods methods fit non linear functions globally typically input space expansions predefined parametrized basis functions subsequent linear combinations expanded inputs ii methods fit non linear functions locally usually spatially localized simple low order polynomial models original input space automatically adjusting complexity number local models locality accurately account nonlinearities distributions target function 
interestingly current trends statistical learning concentrated methods fall primarily class global nonlinear function approximators example gaussian process regression gpr williams rasmussen support vector machine regression smola scholkopf variational bayes mixture models ghahramani beal 
spite solid theoretical foundations approaches possess terms generalization convergence necessarily suitable line learning highdimensional spaces 
require priori determination right modeling biases 
instance case gpr biases involve selecting right function space terms choice basis kernel functions vijayakumar ogawa biases concerned right number latent variables proper initialization second function approximator methods developed primarily batch data analysis easily efficiently adjusted incrementally arriving data 
instance adding new data point drastically change outcome global optimization problem terms data points support vectors carefully selected subset data kept memory re evaluation 
adding new data point computationally expensive property shared gpr 
suffers similar problems due need storing re evaluating data adding new mixture components ueda nakano ghahramani hinton 
general suggested bayesian learning algorithms computationally expensive realtime learning tend represent complete joint distribution data albeit conditionally independent factored representation 
point incremental approximation functions global methods prone lead negative interference input distributions change schaal atkeson changes typical scenario line learning tasks 
contrast global learning methods described function approximation spatially localized models suited incremental real time learning particularly framework locally weighted learning lwl atkeson moore schaal :10.1.1.44.1610
lwl methods useful limited knowledge model complexity model resources increased purely incremental data driven fashion demonstrated previous schaal atkeson 
techniques allocate resources cover input space localized fashion general increasing number input dimensions encounter exponential explosion number local models required accurate approximation referred curse dimensionality scott 
outset high dimensional function approximation computationally infeasible spatially localized learning 
efficient global learning methods automatic resource allocation high dimensional spaces employed successfully techniques projection regression pr 
pr copes high dimensional inputs decomposing multivariate re mixture models global local function approximators local model fitting employ global optimization criterion 
noted scholkopf burges smola started look model selection svms gprs automatic determination number latent models ghahramani beal superposition single variate regressions selected projections input space :10.1.1.54.1561
major difficulty pr lies selection efficient projections achieve best fitting result univariate regressions possible 
best known pr algorithms projection pursuit regression friedman stutzle generalization form generalized additive models hastie tibshirani 
sigmoidal neural networks equally conceived method projection regression particular new projections added sequentially cascade correlation fahlman lebiere 
suggest method extending beneficial properties spatially localized learning high dimensional function approximation problems 
prerequisite approach high dimensional learning problems address locally low dimensional distributions assumption holds large class real world data tenenbaum de silva langford roweis saul vlassis souza vijayakumar schaal 
distributions locally low dimensional allocation local models restricted thin distributions tiny part entire high dimensional space needs filled local models 
curse dimensionality spatially localized model fitting avoided 
circumstances alternative method projection regression derived focusing finding efficient local projections 
local projections accomplish local function approximation neighborhood query point traditional lwl approaches inheriting statistical properties established methods hastie loader atkeson :10.1.1.44.1610
demonstrate resulting learning algorithm combines fast efficient incremental capabilities lwl techniques alleviating problems faced due high dimensional input domains local projections 
sections review approaches find local projections looking various schemes performing dimensionality reduction regression including principal component regression factor analysis partial squares regression 
embed efficient robust projection algorithms incremental nonlinear function approximator vijayakumar schaal capable automatically adjusting model complexity purely data driven fashion 
evaluations synthetic real world data resulting incremental learning system demonstrates high accuracy function fitting high dimensional spaces robustness irrelevant redundant inputs low computational complexity 
comparisons prove competitiveness state art learning systems 
local dimensionality reduction locally weighted learning assuming data characterized locally low dimensional distributions efficient algorithms needed exploit property 
focus locally weighted learning lwl methods atkeson allow adapt variety linear dimensionality reduction techniques purpose nonlinear function approximation see section easily modified incremental learning :10.1.1.44.1610
lwl related methods widespread application mixture models jordan jacobs xu jordan hinton ghahramani beal results section contribute field 
learning problems considered assume standard regression model 
denotes dimensional input vector simplicity scalar output mean zero random noise term 
local subset data vicinity point xc considered locality chosen appropriately low order polynomial employed model local subset 
due favorable compromise computational complexity quality function approximation hastie loader choose linear models 
measure locality data point weight wi computed gaussian kernel wi exp xi xc xi xc diag 
wm positive semi definite distance metric determines size shape neighborhood contributing local model atkeson :10.1.1.44.1610
weights wi enter algorithms assure spatial localization input space 
loss generality assume zero mean inputs outputs algorithms ensured subtracting weighted mean data wi wi denotes number data points 
input data summarized rows matrix xm corresponding outputs coefficients vector corresponding weights determined eq diagonal matrix candidate algorithms local dimensionality reduction consider techniques factor analysis partial squares regression 
factor analysis everitt density estimation technique assumes observed data generated lower dimensional process characterized latent hidden variables independently distributed mean zero unit variance 
observed variables generated form latent variables transformation matrix additive mean zero independent noise diagonal covariance matrix uv denotes expectation operator 
normally distributed parameters obtained iteratively expectation maximization algorithm em rubin thayer 
factor analysis superset dimensionality reduction algorithms 
obtain principal component analysis input space tipping bishop 
purpose regression lower dimensional representation serve new input regression problem algorithm called principal component regression pcr 
documented pcr huge danger eliminating low variance input dimensions crucial regression problem leading inferior function approximation results frank friedman schaal vijayakumar atkeson 
purpose regression useful factor analysis joint space input output data assume obtain pca solution just time performed joint space 
pca algorithms appealing solved efficiently 
alternatively additional constraints diagonal powerful factor analysis algorithm dimensionality reduction obtained requires iterative em solution 
joint space formulations regression parameters cf 
eq recovered computing expectation obtained standard table locally weighted implementation partial squares regression 
initialize 
repeat projections ur diag 
wm matrix locality weights 
zr 
zr 
pr 
manipulations normally distributed joint distribution schaal 
empirical evaluation schaal verified unconstrained non pca version joint space factor analysis performs regression highlighted important problem 
density estimation method factor analysis crucially depends representing complete latent space joint input vector performance degrades severely 
input dimensions irrelevant regression need represented latent variable vector redundant combinations inputs 
property problematic goals expect large number irrelevant inputs high dimensional learning problems 
inferior performance factor analysis latent space underestimated hard apply constructive algorithms algorithms grow latent space data driven way full latent space recovered 
regression results low quality full latent space recovered predictions learning system trusted significant amount data encountered open problem quantify significant 
surprising result empirical comparisons local dimensionality reduction techniques schaal particular algorithm partial squares regression pls wold frank friedman achieved equally robust results factor analysis regression mentioned problems 
pls technique extensively chemometrics recursively computes orthogonal projections input data performs single variable regressions projections residuals previous iteration step 
table provides outline pls algorithm derived implementing locally weighted version 
key ingredient pls direction maximal correlation residual error input data projection direction regression step 
additionally pls regresses inputs previous step projected inputs order ensure orthogonality projections step 
additional regression avoided replacing step similar techniques principal component analysis sanger 
regression step leads better performance algorithm pls chooses effective projections input data spherical distribution spherical case projection pls find direction gradient achieve optimal regression results 
regression step chooses reduced input data resulting data vectors minimal norms pushes distribution spherical 
additional consequence step projections zr uncorrelated zr property important derivations 
due consideration choose pls basis incremental nonlinear function approximator sections demonstrated appealing properties non trivial function fitting problems 
locally weighted projection regression table legend indexes symbols lwpr notation 
training data points input dimensionality dim 
number local models number local projections pls xi yi training data zi lower dimensional projection input data xi pls elements projected input zi zi ur th projection direction zi ur pr regressed input space subtracted maintain orthogonality projection directions batch representations input projected data activation data local model centered weight matrix diag 
wm representing activation due data points sum weights seen local model data points th component slope local linear model var sufficient statistics incremental computation th dimension variable var seeing data points 
nonlinear function approximation core concept learning system locally weighted projection regression lwpr find approximations means piecewise linear models atkeson :10.1.1.44.1610
learning involves automatically determining appropriate number local models parameters hyperplane model region validity called receptive field rf parameterized distance metric dk gaussian kernel cf 
eq wk exp ck dk ck 
query point linear model calculates prediction yk 
total output learning system normalized weighted mean linear models wk yk wk illustrated fig 

centers ck rfs remain fixed order minimize negative interference incremental learning occur due changing input distributions schaal atkeson 
local models created needed basis described section 
table provides list indices symbols consistently description lwpr algorithm 
learning lwpr despite appealing simplicity piecewise linear modeling approach numerically brittle computationally expensive high dimensional input spaces ordinary linear regression determine local model parameters schaal atkeson 
locally weighted partial squares regression local model fit hyperplane 
significant computational advantage expect correlation computation receptive field weighting weighted average output train dk linear unit information processing unit lwpr far fewer projections actual number input dimensions needed accurate learning 
sections describe necessary modifications pls implementation embed local regression lwl framework explain method automatic distance metric adaptation finish complete nonlinear learning scheme called locally weighted projection regression lwpr 
incremental computation projections local regression incremental learning scheme explicitly store training data sufficient statistics learning algorithm need accumulated appropriate variables 
table provides suitable incremental update rules 
variables axz sufficient statistics enable perform univariate regressions step step similar recursive squares fast newton incremental learning technique 
denotes forgetting factor allows exponential forgetting older data sufficient statistics 
forgetting necessary incremental learning change learning parameters affect change sufficient statistics forgetting factors standard technique recursive system identification ljung 
shown prediction error step corresponds leave cross validation error current point regression parameters updated data point denoted 
table number projections input dimensionality entire input space spanned projections ur regression results identical ordinary linear regression wold 
emphasize important properties local projection scheme 
input variables statistically independent equal variance pls find optimal projection direction ur roughly single sweep training data noted insert preprocessing step table independently scales inputs unit variance empirically notice significant improvement algorithm inputs table incremental locally weighted pls rf centered 
initialization data points seen 
new data training point 
compute activation update means 
exp 
wx wy 
compute current prediction error repeat projections 
zr res ru un un 
zr 
zrp mse 
mse 
update local model res projections update local regression compute residuals 
zz zz zr 
zz 
zr 
xz xz update projection directions 

xz zz 
predicting novel data xq mbox initialize yq xq xq repeat yq yq rsr sr xq xq xq srp note subscript referring th local model omitted table referring updates local model rf optimal projection direction corresponds gradient local linearization parameters function approximated 
second choosing projection direction correlating input output data step automatically excludes irrelevant input dimensions 
third danger numerical problems due redundant input dimensions univariate regressions easily prevented singular 
adjusting shape size receptive field distance metric locality receptive fields learned local model individually stochastic gradient descent penalized leave cross validation cost function schaal atkeson wi wi yi yi denotes number data points training set 
term cost function mean leave cross validation error local model indicated subscript ensures proper generalization schaal atkeson 
second term penalty term sure receptive fields shrink indefinitely case large amounts training data shrinkage statistically correct asymptotically unbiased function approximation require maintain increasing number local models learning system computationally expensive 
tradeoff parameter determined empirically assessments maximal local curvature function approximated schaal atkeson general results sensitive parameter schaal atkeson primarily affects resource efficiency input output data preprocessed unit variance kept constant experiments 
noted due local cost function eq learning entirely localized parameters local models needed updates instance competitive learning mixture models 
minimizing eq accomplished incremental way keeping data memory schaal atkeson 
property due reformulation leave cross validation error press residual error kuh 
detailed schaal atkeson bias variance tradeoff resolved local model individually increasing number local models lead overfitting leads better approximation results due model averaging eq sense committee machines perrone cooper 
ordinary weighted linear regression expanding eq press residual error results wi wi yi yi ij 
corresponds inverted weighted covariance matrix input data 
interestingly press residuals eq exactly formulated terms pls projected inputs zi zi 
zi table wi wi yi yi ij ij wi pz corresponds inverse covariance matrix computed projected inputs zi zi spans full rank input space xi eq cf 
proof omit step sake simplicity 
rank deficient input spaces equivalence eqs holds subspace spanned table derivatives distance metric update current data point pls projection activation mkl dij mkl refer table variables mkl stochastic update eq mkl dij dij mkl il jl ij ij 
cv qt zr cv zz zr zz ang appendix 
proved explained appendix pz diagonal greatly contributes computational efficiency update rules 
cost function distance metric lwpr learned gradient descent mt positive definiteness upper triangular matrix resulting cholesky decomposition schaal atkeson stochastic approximation gradient eq derived keeping track sufficient statistics shown table 
noted update laws treated pls projection direction independent distance metric chain rules need taken entire pls recursions 
empirically simplification negative impact reduced update rules significantly 
complete lwpr algorithm update rules combined incremental learning scheme automatically allocates new locally linear models needed 
concept final learning network illustrated fig 
outline final lwpr algorithm shown table 
table pseudocode complete lwpr algorithm initialize lwpr receptive field rf new training sample receptive fields calculate activation eq update projections regression table distance metric table check number projections needs increased cf 
section rf activated create new rf pseudo code threshold determines create new receptive field discussed schaal atkeson computational efficiency parameter complexity parameter mixture models closer set overlap local models beneficial spirit committee machines cf 
schaal atkeson perrone cooper costly compute general overlap permitted better function fitting results danger increase overlap lead overfitting 
initial usually diagonal distance metric eq 
initial number projections set 
algorithm simple mechanism determining increased recursively keeping track mean squared error mse function number projections included local model step table 
mse projection decrease certain percentage previous mse mser algorithm adding new projections mser locally 
mser interpreted approximation leave cross validation error projection threshold criterion avoids problems due overfitting 
due need compare mse successive projections lwpr needs initialized projection dimensions 
comparison mechanisms constructive learning previous algorithms literature platt schaal atkeson 
speed learning trajectories incremental learning training data generated trajectories data temporally correlated possible accelerate lookup training times advantage fact consecutively arriving training points close neighbors input space 
cases added special data structure lwpr allows restricting updates lookups small fraction local models exhaustively sweeping 
purpose local model maintains list local models overlap sufficiently 
sufficient overlap models determined centers distance metrics 
point input space closest centers sense mahalanobis distance di dj 
inserting point eq local models gives activation due point 
local models listed sufficiently overlapping cf 
table 
diagonal distance metrics overlap computation linear number inputs 
new data point added lwpr neighborhood relation checked maximally activated rf 
appropriate counter local model ensures overlap local models checked exhaustively 
nearest neighbor data structure lookup learning confined rfs 
lookup update identification number maximally activated rf returned 
lookup update consider neighbors rf 
shown method performs exhaustive lookup update strategy excludes rfs activated certain threshold pruning local models rfwr algorithm schaal atkeson possible prune local models depending level overlap local models accumulated locally weighted mean squared error pruning strategy virtually identical schaal atkeson section 
due numerical robustness pls noticed need pruning merging non existent lwpr implementation expand possible feature algorithm 
computational complexity diagonal distance metric assumption number projections remains small bounded computational complexity incremental update parameters lwpr linear number input dimensions best knowledge property lwpr computationally efficient algorithms suggested high dimensional function approximation 
low computational complexity sets lwpr apart earlier rfwr algorithm schaal atkeson cubic number input dimensions 
accomplished main goals maintaining appealing function approximation properties rfwr eliminating problems high dimensional learning problems 
confidence intervals classical probabilistic interpretation weighted squares gelman carlin stern rubin local model conditional distribution normal variances wk zk sk wk possible derive predictive variances pred new query point xq local model lwpr derivation measure analogy ordinary linear regression schaal atkeson myers consistent bayesian formulation predictive variances gelman 
individual local model pred estimated refer table table variable definitions pred kqk note wk abbreviated version weight contribution due query point model sake simplicity 
zq projected query point xq th local model sk mse iz wk zk qk wk wk incremental update definition terms sum weights reflects effective number data points entering computation local variance sk schaal atkeson update training points performed 
definition referred local degrees freedom analogous global degrees freedom linear smoothers hastie tibshirani schaal atkeson 
order obtain predictive variance measure averaging formula eq just compute weighed average predictive variance eq 
approach viable ignores important information obtained variance individual predictions yq potentially optimistic 
remedy issue postulate view combining individual yq contributing yq generated process yq yq assume separate noise processes variance independent local model wk accounts differences predictions local models ii noise process pred wk individual local models 
shown appendix eq consistent way combining prediction multiple models noise model just described combined predictive variance models approximated pred wk wk wk pred estimate pred eq 
global variance models approximated wk yq yk wk 
inserting values eq obtain pred wk yq yk wk qk standard deviation confidence interval ic yq pred wk variance estimate eq consistent intuitive requirement local model contributes prediction variance entirely attributed predictive variance single model 
query point receive high weight local model large confidence interval due small squared sum weight value denominator 
illustrates comparisons confidence interval plots toy problem noisy data points 
data range excluded training set 
gaussian process regression lwpr show qualitatively similar confidence interval bounds fitting results 
empirical evaluation sections provide evaluation proposed lwpr learning algorithm range artificial real world data sets 
useful feasible comparisons target approx conf gp confidence bounds target approx conf lwpr confidence bounds function approximation noisy data points plots confidence intervals gaussian process regression lwpr algorithms 
note absence data range state art alternative learning algorithms provided particular support vector regression svm gaussian process regression gp 
gpr chosen due generally acknowledged excellent performance nonlinear regression finite data sets 
noted svm gp batch learning systems lwpr implemented fully incremental algorithm described previous sections 
function approximation redundant irrelevant data implemented lwpr algorithm outlined section 
local model projection regressions performed locally weighted pls distance metric learned stochastic incremental cross validation learning methods employed second order learning techniques incremental pls uses recursive squares gradient descent distance metric accelerated described schaal atkeson 
evaluations initial diagonal distance metric chosen activation threshold adding local models threshold adding new projections cf 
section 
test ran lwpr noisy training data drawn dimensional function cross generated max exp exp exp shown fig 
function mixture areas high low curvature interesting test learning generalization capabilities learning algorithm learning models low complexity find hard capture nonlinearities accurately complex models easily overfit especially linear regions 
second test added constant redundant dimensions inputs rotated new input space random dimensional rotation matrix create dimensional input space high rank deficiency cross 
third test added irrelevant input dimensions inputs second test having target learned nonlinear cross function 
automatically tuned distance metric cross approximation gaussian noise obtaining data set dimensional input space cross 
typical learning curves data sets illustrated fig 
cases lwpr reduced normalized mean squared error thick lines noiseless test set points grid unit square input space rapidly epochs training nmse converged excellent function approximation result nmse data presentations epochs fig shows adapted distance metric fig illustrates reconstruction original function dimensional test data visualized highly accurate approximation 
rising thin lines fig show number local models lwpr allocated learning 
thin lines bottom graph indicate average number projections local models allocated average settled value local projections appropriate originally dimensional data set 
set lwpr incremental algorithm data presentations case refers repeated random order presentations training data noisy data set size nmse test set cross cross cross training data points learning curves data cross approximation tests demonstrate lwpr able recover low dimensional nonlinear function embedded high dimensional space despite irrelevant redundant dimensions data efficiency algorithm degrade higher dimensional input spaces 
computational complexity algorithm increased linearly number input dimensions explained section 
results evaluations directly compared earlier rfwr algorithm schaal atkeson particular figures earlier 
learning speed number allocated local models lwpr essentially rfwr test set 
applying rfwr dimensional data set problematic requires careful selection initial ridge regression parameters stabilize highly rank deficient full covariance matrix input data easy create bias little numerical stabilization initially trap local distance metric adaptation local minima 
lwpr algorithm just computes factor times longer experiment comparison experiment rfwr requires fold increase computation time rendering algorithm unsuitable high dimensional regression 
order compare lwpr results popular regression methods evaluated cross data sets gaussian process regression gp support vector svm regression addition lwpr method 
noted svm gp methods incremental methods considered state art batch regression relatively small number training data reasonable input dimensionality 
computational complexity methods prohibitively high real time applications 
gp algorithm gibbs mackay generic covariance function optimized hyperparameters 
svm regression performed standard available package saunders stitson weston bottou smola optimized kernel choices 
fig 
compares performance lwpr gaussian processes mentioned data sets training data points fig test data set consisted data points corresponding vertices grid plotted results svm regression consistently perform worse receptive fields average projections nmse points points cross dim 
points lwpr points points points cross dim 
points points points cross dim 
normalised mean squared error comparisons lwpr gaussian processes cross data sets unit square corresponding output values exact function values 
approximation error measured normalized weighted mean squared error nmse weighted mse test set normalized variance outputs test set weights chosen pred test point xi 
weighted nmse useful allow algorithms incorporate confidence prediction query point especially useful training data sets data points query points lie far away training data require strong extrapolation form prediction 
multiple runs randomly chosen training data sets performed accumulate statistics 
seen fig 
performance differences lwpr gp largely statistically insignificant training data sizes input dimensionality 
lwpr tendency perform slightly better point data sets due quickly decreasing confidence significant extrapolation required test point 
point data sets gp minor advantage variance predictions point data sets algorithms achieved equivalent results 
gps input dimensions predicting output deduced final converged coefficients covariance matrix lwpr stopped average local projections reflecting exploited low dimensional distribution data 
comparison illustrates lwpr highly competitive learning algorithm terms generalization capabilities accuracy results truly incremental computationally efficient real time implementable algorithm 
comparisons benchmark regression datasets lwpr specifically geared real time incremental learning high dimensions employed traditional batch data analysis 
compare performance natural real world benchmark datasets gaussian processes support vector regression competitors 
data sets boston housing data abalone dataset available uci machine learning repository bay 
boston housing data attributes split randomly random splits disjoint sets training testing data 
abalone dataset attributes downsampled yield disjoint sets training data points testing points gp regression number training data 
gaussian process algorithm problems convergence numerical stability training data sizes table comparison normalized mean squared errors boston abalone data sets gaussian process support vectors lwpr boston abalone gp hyperparameter estimation open parameters covariance matrix svm regression results obtained employing gaussian kernel width boston abalone datasets respectively optimized values suggested scholkopf smola williamson bartlett 
table shows comparisons normalised mean squared error nmse achieved gp svm lwpr datasets 
lwpr highly competitive real world data sets consistently outperforming svm regression achieving similar nmse results gp regression 
sensorimotor learning high dimensional space section look application lwpr real time learning high dimensional spaces data rich environment example learning robot control 
domains lwpr best knowledge viable practical options principled statistical learning 
goal learning evaluation estimate inverse dynamics model referred internal model robotic system component feedforward controller executing fast accurate movements 
inverse dynamics model mapping joint position joint velocity joint acceleration joint torques function times number degrees freedom dof input dimensionality 
implemented lwpr real time operating system vxworks humanoid robot fig 
dof system right hand draw lying pattern 
parallel processors system mhz powerpc processor completely devoted lookup learning lwpr 
order accelerate lookup training times nearest neighbor data lookup described section utilized 
learning inverse dynamics model required learning dimensional input space outputs torque commands dofs 
ideally learn individual lwpr model output dimension 
learning parallel lwpr models exceeded computational power mhz real time processors chose learn single lwpr model dimensional output vector projection pls lwpr regressed outputs vs projected input data 
projection direction chosen mean projection outputs projection stage pls 
approach suboptimal quite output dimensions agree projection direction essentially assumes gradients outputs point roughly direction 
hand souza demonstrated movement data actual physical movement systems lies locally low dimensional distributions hope lwpr multiple outputs successfully simply spanning locally low dimensional input space projections 
lwpr model trained line robot performed pseudo randomly drifting pattern front body 
lookup proceeded hz updating points displacement meters displacement meters traj desired traj traj traj dof humanoid robot results online learning inverse dynamics lwpr humanoid robot learning model achieved hz 
seconds training learning stopped robot attempted draw planar plane robot effector hz frequency entire pattern 
test pattern performed seconds training 
fig 
demonstrates result learning 
traj desired denotes desired pattern traj lwpr learning result seconds training traj result seconds training 
traj trace demonstrates patterns performed inverse dynamics model just low gain negative feedback proportional derivative pd controller 
lwpr rapidly improves control system inverse dynamics controller seconds movement significant inertial gravity perturbation compensated 
convergence low error tracking takes slightly longer seconds traj fig 
reliably achieved 
local models created task 
tracking performance perfect learned inverse dynamics outperformed model estimated rigid body dynamics methods atkeson hollerbach significantly terms average tracking error desired trajectory rigid dynamics model estimated hour data collection minutes line processing data 
results demonstrate actual implementation real time inverse dynamics learning robot complexity 
line learning autonomous airplane control line learning abilities lwpr ideally suited incorporated algorithms provably stable adaptive control 
control theoretic development approach nakanishi 
nakanishi farrell schaal 
essence problem formulation begins specific class equations motion form denotes state control system control inputs nonlinear function approximated 
suitable control law system xc xc xc xc desired trajectory tracked hat notation indicates approximated version unknown function 
applied lwpr control framework learn unknown function problem autonomous airplane control high fidelity simulator 
simplicity considered planar version airplane governed differential equation stevens lewis cos sin mv cm sin cos equations denotes forward speed airplane mass thrust angle attack gravity constant flight path angle horizontal world coordinate system axis pitch rate inertial constant 
complexity equations hidden unknown highly nonlinear aerodynamic lift force drag force pitch moment terms specific airplane 
go detail provably stable adaptive control lwpr control law applied airplane control viewpoint learning main component learn lift drag forces pitch moment 
obtained re arranging cos sin fd mf mf sp sp cos mv sin fl mf mf sp sp fm mf mf sp sp terms denote control surface angles airplane indices flap left right mfr outboard flap left right ofl ofr left right spoilers spl spr 
terms right hand side known cope simultaneous function approximation problems dimensional input space ideal application lwpr 
implemented lwpr function high fidelity simulink simulation autonomous airplane adaptive control approach nakanishi 
airplane started initial knowledge just proportional controller term term multiplied 
task controller fly trajectories essentially sinusoid variations flight path angle 
fig 
demonstrates results experiment 
fig 
shows desired trajectory realization controller 
fig 
illustrate line function approximation seen control achieves perfect tracking just seconds 
function approximation accurate short time 
approximation requires longer time convergence progresses fast 
local models needed learning fd fl local models allocated fm interesting element fig 
happens seconds flight simulated failure airplane mechanics locking mfr degree deflection 
seen function approximators quickly re organize change flight successfully continued tracking error converges back tracking performance 
strong signal changes seconds failure due oscillations control surfaces problem function approximation 
adaptive control airplane crashed 
discussion nonlinear regression spatially localized models remains data efficient computationally efficient methods incremental learning automatic determination model complexity 
order overcome curse dimensionality local learning systems investigated methods linear projection regression employ spatially localized nonlinear function approximation high dimensional input data redundant irrelevant components 
due robustness setting chose partial squares regression core novel function approximator locally weighted projection regression lwpr 
proposed technique evaluated range artificial real world data sets dimensional input spaces 
showing fast robust learning performance due second order learning methods stochastic leave cross validation lwpr low computational complexity updating local model new data point remained linear computational cost number inputs algorithm accomplishes approximation results projections irrespective number input dimensions 
knowledge spatially localized incremental learning system efficiently high dimensional spaces suited line real time applications 
addition lwpr compared favorably generalization performance state art batch regression regression methods gaussian process regression provide qualitatively similar estimates confidence bounds predictive variances 
major drawback lwpr current form need gradient descent optimize local distance metrics local model manual tuning forgetting factor required recursive learning algorithms accumulate sufficient statistics 
derive probabilistic version partial squares regression allow complete bayesian treatment locally weighted regression locally linear models hopefully remaining open parameters manual adjustment 
full bayesian version lwpr achieve computational efficiency current implementation remains seen 
appendix press residuals pls prove assumption lives reduced dimensional subspace press residuals eq replaced residual denoted eq time pred time gamma desired gamma pred time lwpr learning results adaptive learning control simulated autonomous airplane tracking flight angle approximation lift force approximation drag force approximation pitch moment seconds flight failure simulated locks control surface degree angle 
note reasons clearer illustration axis break inserted seconds 
pz corresponds pseudo inverse covariance matrices symbol represents svd pseudoinverse press flannery teukolsky vetterling generates solution inverse embedded lower dimensional manifold minimum norm solution sense mahalanobis distance 
refer table batch notations 
part transformation matrix full rank row space denotes coordinate transformation rank deficient space full rank space corresponding inverse covariance matrix pz show xt xt xi xi linear transformation maintains norm 
part part show recursive pls projections transform inputs written linear transformation matrix completes proof 
clarifying notation xm zm 
zr 
look pls projection directions attempt show zi xi batch sense xt showing individual projections exists tr table table zr 
tr 
xx xt 
know algorithm table represents projection operator 
results eq eq eq xx 
property projection operator xt shown easily writing pseudo inversion exists operator xu eqs write xr xt ut xt xu 
distinguish row vectors column vectors respectively projected data matrix operation carried recursively determine tk showing pls projections written linear transformation 
completes proof validity modified press residual eq pls projections 
note pz diagonal virtue fact pls algorithm projection iteration projected components input space subtracted computing projection table table ensuring component orthogonal previous ones 
property discussed frank friedman 
combined predictive variances noise model combining prediction individual local model yq yq wk pred wk 
mean prediction due multiple local models written average yq wk yq pred wk pred wk wk yq wk assumption pred approximately constant contributing models yq estimate multiple noisy instances yq averaged noise process exactly happens local model 
eq consistent eq proposed dual noise model 
combined predictive variance derived pred yq wk wk wk yq wk wk yq wk wk fact var noting zero mean pred pred wk var wk wk wk wk wk wk pred wk wk var wk gives expression combined predictive variances 
wk pred wk atkeson hollerbach 

model control robot manipulator 
mit press 
atkeson moore schaal 

locally weighted learning 
artificial intelligence review 
bell sejnowski 

independent components natural scenes edge filters 
vision research 
kuh 

regression diagnostics 
john wiley 
bishop 

neural pattern recognition 
oxford press 
souza vijayakumar schaal 

internal models entire body learnable 
society neuroscience abstracts vol 

everitt 

latent variable models 
chapman hall london 
fahlman lebiere 

cascade correlation learning architecture 
touretzky ed advances neural information processing systems pp 

morgan kaufmann los altos ca 
frank friedman 

statistical view tools 
technometrics 
friedman stutzle 

projection pursuit regression 
journal america 
statistical association 
gelman carlin stern rubin 

bayesian data analysis 
chapman hall 
ghahramani beal 

variational inference bayesian mixtures factor analysers 
leen solla ed advances neural information processing systems pp 

mit press 
gibbs mackay 

efficient implementation gaussian processes technical report 
cavendish laboratory cambridge uk 
hastie loader 

local regression automatic kernel carpentry 
statistical science 
hastie tibshirani 

generalized additive models 
chapman hall london 
bay 

uci kdd archive 
univeristy california irvine department information computer science 
horn johnson 

matrix analysis 
press university cambridge 
jaakkola 

mean field methods theory practice chap 
tutorial variational approximation methods 
mit press 
jordan jacobs 

hierarchical mixture experts em algorithm 
neural computation 
kawato 

internal models fot motor control trajectory planning 
current opinions neurobiology 
ljung 

theory recursive identification 
mit press 


principal component regression exploratory statistical research 
american statistical association 
myers 

classical modern regression applications 
pws kent 
nakanishi farrell schaal 

composite adaptive control locally weighted statistical learning 
ieee international conference robotics automation pp 

olshausen field 

emergence simple cell receptive eld properties learning sparse code natural images 
nature 
perrone cooper 

neural networks speech image processing chap 
networks disagree ensemble methods hybrid neural networks 
chapman hall 
platt 

resource allocating network function interpolation 
neural computation 
press flannery teukolsky vetterling 

numerical recipes art scientific computing 
press university cambridge 
roweis saul 

nonlinear dimensionality reduction local linear embedding 
science 
rubin thayer 

em algorithms ml factor analysis 
psychometrika 
sanger 

optimal unsupervised learning single layer feedforward neural network 
neural networks 
saunders stitson weston bottou smola 

support vector machine manual technical report tr csd tr 
dept computer science royal holloway univ london 
schaal atkeson 

assessing quality learned local models 
advances neural information processing systems pp 

morgan kaufmann 
schaal atkeson 

receptive field weighted regression technical report tr 
atr human information processing kyoto japan 
schaal atkeson 

constructive incremental learning local information 
neural computation 
schaal atkeson vijayakumar 

real time robot learning locally weighted statistical learning 
international conference robotics automation icra 
schaal 

origins power law rhythmic movements 
experimental brain research 
schaal vijayakumar atkeson 

local dimensionality reduction 
jordan kearns solla eds advances neural information processing systems vol 

mit press 
scholkopf burges smola 

advances kernel methods support vector learning 
mit press 
scholkopf smola williamson bartlett 

new support vector algorithms 
neural computation 
scott 

multivariate density estimation 
wiley ny 
smola scholkopf 

tutorial support vector regression neurocolt technical report nc tr 
royal holloway college london 
stevens lewis 

aircraft control simulation 
john wiley new jersey 
tenenbaum de silva langford 

global geometric framework nonlinear dimensionality reduction 
science 
tipping bishop 

probabilistic principal component analysis 
journal royal statistical society series 
ueda nakano ghahramani hinton 

algorithm mixture models 
neural computation 
vijayakumar ogawa 

rkhs functional analysis exact incremental learning 
neurocomputing 
vijayakumar schaal 

local adaptive subspace regression 
neural processing letters 
vijayakumar schaal 

algorithm incremental real time learning high dimensional space 
international conference machine learning icml pp 

vlassis 

supervised dimensionality reduction intrinsically low dimensional data 
neural computation 
williams rasmussen 

gaussian processes regression 
touretzky hasselmo eds advances neural information processing systems 
mit press 
williams seeger 

nystrom method speedup kernel machines 
advances neural information processing systems 
mit press 
wold 

perspectives probability statistics chap 
soft modelling latent variables nonlinear iterative partial squares approach 
chapman hall 
xu jordan hinton 

alternative model mixtures experts 
tesauro touretzky leen eds advances neural information processing systems vol 
pp 

mit press 

