annals statistics vol 
doi institute mathematical statistics empirical bayes selection wavelet thresholds iain johnstone bernard silverman stanford university university oxford explores class empirical bayes methods threshold selection wavelet shrinkage 
prior considered wavelet coefficient mixture atom probability zero heavy tailed density 
mixing weight sparsity parameter level transform chosen marginal maximum likelihood 
estimation carried posterior median random thresholding procedure estimation carried thresholding rules threshold 
details calculations needed implementing procedure included 
practice estimates quick compute software available 
simulations standard model functions show excellent performance applications data drawn various fields application explore practical performance approach 
general result risk corresponding marginal maximum likelihood approach single sequence bounds risk method subject membership unknown function wide range besov classes covering case bounded variation 
rates obtained optimal value parameter simultaneously wide range loss functions dominating lq norm th derivative 
attention paid distinction sampling unknown function white noise sampling discrete points placing constraints function discrete wavelet transform sequence values observation points 
results relevant combinations scenarios obtained 
cases key feature theory particular boundary corrected wavelet basis details discussed 
approach described far unique combining properties fast computation theoretical properties performance simulations practice 
key feature appears estimate sparsity adapts different zones estimation signal sparse thresholding benefit second appropriately chosen threshold results substantially improved received september revised september 
supported part nsf dms nih california eb 
supported part engineering physical sciences research council 
ams subject classifications 
primary secondary 
key words phrases 
adaptivity bayesian inference nonparametric regression smoothing sparsity 
empirical bayes selection wavelet thresholds estimation third signal sparse zero estimate gives optimum accuracy rate 



background 
consider nonparametric regression problem observations regularly spaced points ti unknown function subject noise xi ti independent random variables 
standard approaches estimation proceed discrete wavelet transform data xi processing resulting coefficients remove noise usually form thresholding transforming back obtain estimate 
underlying notion wavelet methods unknown function economical wavelet expression approximated function relatively small proportion nonzero wavelet coefficients 
quality estimation quite sensitive choice threshold best choice dependent problem setting 
general terms sparse signals call relatively high thresholds higher dense signals demand choices lower 
typical wavelet coefficients true signal relatively sparse fine resolution scales coarser scales 
desirable develop threshold selection methods adapt threshold level level 
hope methods estimate thresholds stably reflect gradation sparse dense signals scale changes fine coarse 
proven elusive construct threshold selectors combine properties theoretical properties 
principal motivation reported show simple empirical bayesian approach combines computational tractability theoretical practical performance 
software availability see section 
concerned nonparametric regression model wavelet transforms levelwise empirical bayes approach principle directly applicable direct indirect transform shrinkage settings multiscale block structure briefly discussed 

bayesian approaches wavelet regression 
bayesian context notion sparsity naturally modeled suitable prior distribution wavelet coefficients write djk elements discrete wavelet transform dwt vector values ti jk dwt observed data xi 
letn jk djk 
johnstone silverman clyde vidakovic abramovich silverman silverman considered particular mixture prior problem 
prior djk independently distributed djk mixture atom probability zero normal distribution variance parameters distribution depend level coefficient transform 
related prior considered chipman kolaczyk mcculloch survey area see 
see range approaches modeling wavelet coefficients underlying function image 
early version introducing approach 
popular summary posterior distribution model posterior mean abramovich silverman investigated posterior median djk summary posterior distribution 
true thresholding rule jk threshold point estimate djk exactly zero 
wavelet context coefficient wise posterior median corresponds point estimate posterior distribution family loss function equivalent norms function derivatives 
losses case natural wishes allow possibility inhomogeneous functions aims wavelet approach 

choosing parameters prior 
parameters prior chosen 
existing literature parameters chosen directly prior information combination prior information data criteria 
example bayesthresh approach abramovich silverman give results clearly invite possibility systematic approach choice hyperparameters 
take empirical bayes marginal maximum likelihood approach yields completely data method choosing prior parameters 
bayesian formulation set wavelet regression single resolution level special case single sequence bayesian model selection problem considered george foster 
problem considered detail johnstone silverman review basic method give additional implementational details 
suppose zn observations satisfying zi independent random variables 
supposed unknown coefficients zero may nonzero empirical bayes selection wavelet thresholds mind interest estimate basis observed data 
model selection context nonzero correspond parameters enter model 
connection wavelet regression natural zi sample wavelet coefficients suitably renormalized particular level noisy observations sequence population wavelet coefficients zero 
parameters modeled having independent prior distributions mixture 
nonzero part prior assumed fixed unimodal symmetric density 
previous wavelet cited density normal density heavier tailed prior replacing part mixture example double exponential distribution scale parameter may depend level coefficient transform 
possible prior heavier tails introduced section 
apart theoretical advantages approach wainwright simoncelli willsky argue marginal distribution wavelet coefficients images arising practice typically tails heavier gaussian 
bayesian setup noise independent wavelet coefficients 
denotes convolution 
avoid confusion scaling function wavelet family denote standard normal density 
marginal density observations zi wg 
define marginal maximum likelihood estimator maximizer marginal log likelihood log zi wg zi subject constraint threshold satisfies logn 
upper limit threshold universal threshold property asymptotically largest absolute value observations obtained zero signal considered appropriate limiting threshold 
basic approach plug value back prior estimate parameters bayesian procedure value suppose prior observe 
median posterior distribution mean 
posterior median estimated zi corresponding estimate posterior mean zi 
johnstone silverman fixed function monotonic function thresholding property exists 
estimated weight yields estimated threshold say 
simple extension method retain threshold general thresholding rule example hard soft thresholding 
main emphasis choice threshold choice different thresholding rules 
posterior mean rule fails thresholding property produces estimates essentially coefficients nonzero 
shrinkage properties allow give results 
shall see theory simulation studies performance posterior mean quite posterior median 
approach estimate parameters prior 
particular scale parameter incorporated considering prior density wa convolution normal density 
estimated finding maximum parameters log zi zi 
case scale parameter estimated monotonic function root easily numerically provided function tractable 
maximizing package numerical maximization routine uses gradients acceptably efficient way maximizing 
details relevant calculations particular priors section 
calculations implemented authors package johnstone silverman documentation package gives details 

marginal maximum likelihood wavelet context 
wavelet context mml approach applied level wavelet transform separately yield values appropriate depend level transform 
standard deviation noise level assuming original noise independent variance conventional estimated median absolute values coefficients highest level 
generally example case stationary correlated noise may appropriate estimate separately level higher levels transform 
considered effect sampling variability estimation noise variance interesting topic research 
empirical bayes selection wavelet thresholds level define sequence zk jk apply single sequence mml approach sequence obtain wj appropriate estimates parameters prior 
estimated wavelet coefficients discrete wavelet transform sequence ti djk jk wj 
assuming loss generality function defined interval values ti crude estimates wavelet coefficients function jk djk neglecting boundary issues moment 
straightforward generalizations 
natural generalizations include inclusion estimates parameters prior posterior mean posterior median general thresholding rule posterior median posterior median threshold 
addition consider generalizations 
modified thresholds estimation derivatives 
wavelet methods estimate derivatives shown abramovich silverman appropriate universal threshold logn multiple quantity 
develop theory estimation derivatives modified threshold ta appropriately chosen ta logn log 
translation invariant wavelet transform 
recognized translation invariant wavelet transform general gives better results conventional transform applied fixed origin 
level translation invariant transform gives sequence values independent 
subsequence obtained regular selection intervals independent corresponds coefficients level standard wavelet expansion particular choice origin 
way proceeding apply empirical bayes method entirely separately subsequences obtain estimates relevant coefficients translation invariant wavelet transform 
simpler natural estimates mixture hyperparameters position time origin borrowing strength estimation hyperparameters different positions origin 
obtain single estimate level maximize average choice origin marginal log likelihood functions 
average times independent log likelihood function obtained simply summing coefficients level translation invariant transform 
johnstone silverman estimates mixture parameters give individual posterior medians coefficients translation invariant transform estimated function average basis approach 
apart combination log likelihoods involved estimation hyperparameters translation invariant method gives result applying standard method possible choice time origin averaging position time origin 
independent likelihood level choose hyperparameters reminiscent independence estimating equation approach liang zeger parameter fitting marginal distribution sequence identically distributed observations 
concerned observations generalized linear model dependence parameters covariates 
different choices origin prior distributions coefficients general generated single underlying prior model curve translation invariant procedure involves separate modeling prior information origin position modulo coefficients level independence estimating equations method combining separate problems choosing prior single problem level 

theoretical approach results 
classic way study adaptivity wavelet smoothing methods study worst behavior method wavelet coefficients function constrained lie particular besov sequence space corresponding besov function space membership function 
besov spaces flexible family depending parameters allow varying degrees inhomogeneity smoothness functions contain 
relations besov spaces spaces defined lp norms function derivatives reviewed section 
shall show empirical bayes method suitable function automatically achieves best possible minimax rate wide range besov spaces including low values parameter allows inhomogeneity unknown function particular case theory develop follows fuller details assumptions 
suppose observations xi ti function regularly spaced points ti independent random variables 
djk jk coefficients orthogonal discrete wavelet transform sequence ti denote vector elements djk varies 
assume coarsest level wavelet transform carried fixed level 
denote dl vector scaling coefficient level 
periodic boundary conditions power vector dj length isthe finest level sample coefficients defined 
empirical bayes selection wavelet thresholds allow discrete wavelet transforms boundary conditions values suitable multiples powers shall milder assumptions dj defined fixed sum lengths dj equal length dj interval 
length vector dl scaling coefficients assumed lie sothat estimate coefficients djk estimate applying empirical bayes approach level level mixture prior heavytailed nonzero component 
estimator posterior median thresholding rule threshold obeying bounded shrinkage condition set 
scaling coefficients dl estimated observed values obtain estimates ti function values ti apply inverse discrete wavelet transform estimated array djk 
leta define besov sequence space set coefficient arrays jk apj theory shows constant possibly depending sup ti ti log fixed second term bound negligible rate decay mean square error best attained relevant function class 
result shows apart log term estimation method simultaneously attains optimum rate wide range function classes automatically adapting regularity underlying function 
conditions shall discuss besov sequence space norm equivalent besov function space norm parameters 
main theorem goes considerably respects demonstrates optimal rate convergence mean norm errors just mean square error considered 
posterior median thresholding method satisfying certain mild conditions results hold posterior mean 
johnstone silverman appropriate modified threshold method optimality extends estimation derivatives existing statistical wavelet literature concentrates explicitly implicitly white noise model assume independent observations wavelet coefficients function resolution level 
little attention paid errors possibly introduced discretization donoho johnstone discuss form discretization somewhat different simple sampling discrete points 
issue considered detail literature careful treatment statistical context boundary corrected wavelet methods introduced cohen daubechies vial 
current consider effects discretization boundary correction prove theorems white noise model sampled data model 
particular suppose function observed regular grid points subject independent noise 
proceeding appropriate preconditioning data near boundaries treatment boundary wavelet coefficients construct estimate setting lk jk jk jk jk scaling functions wavelets scale letf class functions true wavelet coefficients fall 
appropriate mild conditions special case theory demonstrates sup cc results go far mean integrated square error consider accuracy estimation besov sequence norms wavelet coefficients imply estimation derivatives function allow losses norms 

alternative approaches related bibliography 
finding numerically simple stable adaptive method threshold choice theoretical practical properties proven elusive 
plethora methods choosing thresholds proposed see chapter 
apart empirical bayes methods note methods accompanied theoretical analysis properties software easily written 
cases set zk xk thresholds expressed renormalized scale 
stein unbiased risk estimate sure aims minimize mean squared error soft thresholding method intended adaptive different levels sparsity 
threshold chosen minimizer range logn 
empirical bayes selection wavelet thresholds theoretical properties theoretical analysis combined simulation practical experience shows method unstable choose thresholds sparse cases :10.1.1.161.8697
false discovery rate fdr method derived principle controlling false discovery rate simultaneous hypothesis testing studied detail estimation setting 
order data decreasing magnitudes compare quantile boundary tk false discovery rate parameter crossing index kf max tk set threshold tf kf fdr threshold selection adapts sparse signals dense signals moderate size 
shall see empirical bayes thresholding properties sure fdr thresholding deals transition sparse dense signals stable manner 
detailed discussion theoretical comparisons various estimators provided section 

structure 
section discuss various aspects mixture priors 
priors specified details formulas needed bayesian calculations practice 
take opportunity give additional practical details included 
sections practical performance proposed method investigated simulation section applications data sets arising practice section 
section contains theoretical core estimation coefficient arrays besov sequence norm constraints 
wide ranging result theorem white noise model stated 
explore aspects boundary wavelet construction including ways mapping data scaling function coefficients finest level 
allows definition empirical bayes estimator sampled data problem finite interval 
result state estimator theorem shows essentially attains performance estimator white noise case 
correspondences besov sequence function norms set specifically addressing wavelets functions bounded interval 
relate risk measures expressed terms wavelet coefficients norms appropriate derivatives 
section contains proofs main theorems starting reviewing theoretical results single sequence problem cast form relevant 
results prove white noise case theorem 
proof theorem sampled data case approximation results appropriate boundary corrected wavelets johnstone silverman 
section contains technical details remarks including discussion importance bounded shrinkage assumption results posterior mean estimator 
johnstone silverman 
software 
methods described current implemented contributed package statistical language 
package documentation installed cran archive accessible www project org 
additional description implementational details available 
matlab implementation see 

mixture priors details calculations 
section discuss general aspects priors procedure review theory single sequence case 
denote generic strictly positive constants necessarily single equation 
confusion value prior weight suppressed notation 
write standard normal cumulative set 
assumed model observed data renormalized noise variance 

priors heavy tails 
particular heavy tailed densities shall consider nonzero part prior distribution laplace density scale parameter mixture density exp beta explicitly density tails decay weight cauchy distribution 
reason refer density quasi cauchy density 
shall consider functions satisfy conditions 
function symmetric unimodal density satisfying condition sup log du 

quantity bounded 
du bounded away zero sufficiently large empirical bayes selection wavelet thresholds conditions implies tails exponential heavier second rules tail behavior heavier cauchy 
third condition mild regularity condition 
conditions satisfied laplace quasi cauchy function normal density 
normal laplace quasi cauchy priors posterior distribution observed marginal distribution tractable choice marginal maximum likelihood estimation posterior mean median performed practice outlined paragraphs 
setting generic calculations relevant quantities give specific details particular priors 

generic calculations 
posterior mean 
general posterior probability satisfy define posterior density wg wg 

mean density 
posterior mean equal 
posterior median 
find posterior median du 
find properties 
note median necessarily zero unnecessary evaluate 
ifz antisymmetry property 
johnstone silverman marginal maximum likelihood weight 
explicit expression function facilitates computation maximum marginal likelihood weight single sequence case 
define score function define wg 
decreasing function zi 
letting wn weight satisfies wn logn estimated weight maximizes range wn 
follows zero range 
furthermore smoothness monotonicity possible find binary search faster algorithm 
restriction range implies threshold logn 
shrinkage rules 
posterior median mean examples estimation rules yield estimate general family estimation rules defined called thresholding rule antisymmetric increasing function bounded shrinkage property constant independent immediate consequence weight posterior median thresholding rule threshold denote bounded shrinkage property condition 
general thresholding rules may advantages cases 
example hard thresholding rule suitably estimated threshold may computational advantages may preserve peak heights better investigated aspect detail 
choice shrinkage rule choice threshold somewhat separate issues 
problem dependent devoted 
posterior mean thresholding rule sufficient properties common posterior median allow similar theoretical results obtained restrictions risk functions considered 

calculations specific priors 
calculations set show key quantities marginal density mean function tail conditional probability function 
density density 
function upper tail probability density 
empirical bayes selection wavelet thresholds laplace distribution prior exp az az az az az az weighted sum truncated normal distributions 
shown az az laplace prior az az quasi cauchy distribution manipulation laplace prior equation solved explicitly making function case quasi cauchy prior equation solved numerically 

simulation results 
simulation study carried regression models standard consideration wavelet methods 
simulations models carried noise levels 
high noise ratio standard deviation noise standard deviation signal values inthe low noise case ratio complements simulations single sequence case reported 
plus code carry simulations available authors web sites enabling reader verify results conduct experiments desired 
johnstone silverman 
results translation invariant wavelet transform 
table various wavelet methods making translation invariant wavelet transform compared 
model noise level replications generated 
replication function simulated equally spaced points ti 
normal noise variables models noise levels 
error reported method considered ti ti noise variance case explains results low noise apparently inferior high noise default choices wavelet boundary corrections plus wavelets function 
realization noise variance estimated median absolute deviations wavelet coefficients highest level 
default choice boundary treatment periodic boundary conditions boundary conditions current implementations translation invariant wavelet transform 
detailed consideration idea translation invariant transform combination boundary correction interesting idea research 
laplace prior scale parameter estimated level level marginal maximum likelihood data estimates table average replications summed squared errors points various models methods 
wavelet estimators translation invariant wavelet transform 
standard error entries value reported high noise low noise method bmp blk dop hea bmp blk dop hea laplace median quasi cauchy median gaussian median laplace mean sure levels sure levels univ soft levels fdr fdr fdr fdr spline tukey empirical bayes selection wavelet thresholds constructed posterior median posterior mean 
quasi cauchy prior estimates posterior median calculated 
posterior median mixed gaussian prior calculated laplace prior scale parameter estimated data 
methods translation invariant wavelet transform considered sure applied levels transform universal soft thresholding applied levels transform false discovery rate approach various values parameter false discovery approach wavelet context method applied separately level method derived 
parameter level resulting estimated threshold may course vary 
comparisons included standard methods cubic smoothing splines gcv smooth spline plus tukey rsr method running medians default plus smooth 
standard error entries table value reported values correct significant figures 
standard methods perform badly 
surprisingly specifically designed smooth functions smoothing spline method fails discontinuous spiky signals 
method separating signal noise low noise case 
tukey method extent competitive universal thresholding inhomogeneous signals adapt smoother behavior heavisine signal 
methods wavelet transform performance posterior mean estimator laplace prior consistently slightly worse posterior median 
universal thresholding method compare sure gives noticeably worse performance laplace quasi cauchy empirical bayes methods 
fdr method competitive provided parameter chosen appropriately 
signals sample size give performance performance worse cases considerably worse 
shall see subsequent examples choice parameter crucial performance fdr method situations relative performance fdr method case quite 
translation invariant wavelet transform observed coefficients independent 
benjamini propose modification fdr method take account dependence observations replacing number parameters consideration 
translation invariant wavelet transform number coefficients level equal number original observations simulation example considered correction factor 
results reported correspond benjamini procedure 
choosing parameter arbitrarily johnstone silverman table difference summed square errors methods indicated laplace median method measured terms standard error difference estimated replications high noise low noise method bmp blk dop hea bmp blk dop hea quasi cauchy median laplace mean sure levels univ soft levels fdr fdr fdr fdr case recalibration parameter affect general 
mean precise numerical value translation invariant case necessarily translated directly standard discrete wavelet transform 
mixed gaussian prior model fit theoretical assumptions seen performance heavytailed priors 
clear tail requirements bearing performance empirical bayes approach 
detailed investigation issue interesting topic research 
noise values model correlation various values table 
comparisons methods laplace median method paired sample basis table 
seen empirical bayes method laplace prior posterior median decisively methods heavisine function quasi cauchy prior performs slightly better little choose laplace quasi cauchy priors 
fdr methods inferior performance significant 
results cases fdr method ones significant difference 
comparisons fdr methods empirical bayes methods 

results standard discrete wavelet transform 
order evaluate advantage translation invariant transform simulated data smoothed methods standard transform 
results shown table 
additional comparisons included block thresholding methods considered cai silverman ql method 
block thresholding methods choose thresholds empirical bayes selection wavelet thresholds table average replications summed squared errors points various models methods 
case standard wavelet transform 
methods included give results table 
comparison results laplace prior translation invariant transform repeated table italics high noise low noise method bmp blk dop hea bmp blk dop hea laplace median translation invariant laplace median quasi cauchy median gaussian median laplace mean neighblock neighcoeff ql sure levels sure levels univ soft levels fdr fdr fdr fdr information neighboring coefficients transform 
case neighcoeff neighboring coefficients considering particular coefficient neighblock data processed blocks information drawn neighboring blocks 
coarse scales ql method uses thresholding rule threshold equal standard deviation coefficients finer levels coefficients thresholded threshold increases universal threshold level increases time proportion coefficients allowed nonzero controlled higher level 
interesting drawn table 
case posterior mean generally yields superior estimates posterior median 
neighcoeff method better block thresholding methods generally laplace prior posterior mean method 
ql method performs heavisine signal competitive 
context relative performance fdr method previously importance choosing parameter appropriately remains 
general clear important translation invariant transform 
empirical bayes method gaussian prior tried johnstone silverman context results somewhat inferior heavy tailed priors 
tables give measure performance 
denote value cell table error measure method applied case define performance method mink min 
ratio min quantifies relative performance method case comparing best method case 
minimum efficiency score gives loss efficiency estimator challenging case 
translation invariant transform laplace median case minimum efficiency score fdr method scores 
quasi cauchy method scores fdr scores 
turn standard transform results decisive scores empirical bayes laplace quasi cauchy median methods fdr fdr 
noted scores empirical bayes methods empirical bayes method best varies slightly cases considered 
specific laplace median method consistently outperforms fdr methods 

comparisons illustrative data sets 
section simulations complemented consideration illustrative examples drawn practical applications 
account simulations practical comparisons empirical bayes method laplace prior posterior median estimate fully automatic simulation studies considered practical illustrations performs best nearly best method setting 
fdr method slightly superior simulation study expense substantial cases considered 

inductance data 
practical comparison uses inductance data described 
data collected department bristol university investigation breathing patients general 
details data see help page ipd data package 
plots original data curve estimate obtained laplace prior method shown 
results laplace quasi cauchy priors virtually identical laplace results reported detail 
aim adaptive smoothing data kind preserve features peak heights far possible eliminating spurious rapid variation 
abramovich silverman bayesthresh method performed better regard various wavelet methods best results subjective adjustment parameter empirical bayes selection wavelet thresholds fig 

top panel inductance data 
bottom panel effect smoothing inductance data laplace prior method 
gave preferable results 
mml approach gave virtually results quasi cauchy laplace prior 
efficacy various methods preserving peak heights simply judged maximum various estimates height peak curve 
standard bayesthresh method yields maximum subjectively adjusting gives 
empirical bayes method gives 
empirical bayes method gives results closer adjusted bayesthresh maximum distance empirical bayes curves adjusted bayesthresh curve third original bayesthresh estimate 
efficacy various methods dealing rapid variation near time best quantified range estimated functions small interval near point 
standard bayesthresh method johnstone silverman glitch range adjusted bayesthresh empirical bayes method corresponding substantial dramatic improvement 
fdr method various parameters applied 
simulations fdr approach applied separately level parameter level 
fdr parameters considered maximum estimated curve range estimated curve near time 
fdr competes empirical bayes preserving peak height cost inferior treatment presumably spurious variation 
comparison various methods considering threshold various levels transform 
threshold full description procedure especially bayesthresh laplace prior cases parameters prior threshold useful univariate summary method processing wavelet coefficients 
gives comparison various methods applied data 
seen top levels empirical bayes methods track adjusted bayesthresh method quite closely 
standard bayesthresh uses high thresholds may reason smooths peak height somewhat 
coarser levels empirical bayes methods automatically adjust lower thresholds reflecting way signal sparse levels allowing variation scales go quite closely way observed 
fdr parameter choices gives degree adaptivity threshold level shown empirical bayes methods 
conclude comparison bayesthresh empirical bayes method subjectively adjusted bayesthresh method yielded results data basic message discussion empirical bayes method yields results virtually best bayesthresh method need subjective parameters 
addition maximum likelihood estimate prior parameters ad hoc approach fitting method bayesthresh approach 

ion channel data 
comparison empirical bayes sure provided considering segment ion channel data discussed example johnstone silverman 
constructed data true signal known 
see 
thresholds chosen sure dashed line reasonable coarse scales small fine scales signal sparse show instability way vary lead insufficient noise removal reconstruction 
contrast empirical bayes threshold choices increase monotonically scale reasonable manner 
particular universal thresholds levels automatically 
reconstructions eb thresholds shown panel posterior median shrinkage rule hard thresholding rule 
empirical bayes selection wavelet thresholds fig 

thresholds chosen top levels wavelet transform inductance data various methods 
upper empirical bayes laplace prior empirical bayes quasi cauchy prior bayesthresh bayesthresh subjectively 
lower false discovery rate method parameters 
hard threshold choice tracks true signal better 
choice threshold shrinkage rule problem dependent scope 
somewhat separate issue setting threshold values 
systematic quantitative comparison table 
method considered sequences length drawn original data analyzed 
variances wavelet transform various levels estimated separate consideration imitating effect sequence observations signal calibrate method 
method curve estimated smoothing method rounded nearest zero give final estimate 
figures average percentage error sequences considered 
johnstone silverman fig 

left panel estimated threshold plotted level dashed line sure thresholds solid line eb thresholds 
right panel segment ion channel signal estimates 
solid lines eb thresholds uses hard thresholding rule tracks true signal better uses posterior median shrinkage 
result sure thresholds plotted dashed line dotted line gives true signal 
empirical bayes selection wavelet thresholds table percentage errors estimation ion channel gating signal 
errors average separate sequences length drawn data provided eisenberg levis 
variances wavelet coefficients level estimated separately decimated 
laplace median quasi cauchy median laplace mean sure levels sure levels univ soft levels fdr fdr fdr fdr spline tukey aws special aside note theoretical results course specifically include zero loss estimate rounded nearer zero 
consider lq losses near zero catch flavor discrete losses view fact limit power lq norm zero loss 
comparisons special purpose method developed specifically problem originators data standard smoothing methods including aws method 
specialpurpose method achieves error rate specificity method surprising beaten generalpurpose methods consider translation invariant wavelet methods come close 
case posterior mean slightly outperforms posterior median methods sure fdr 
parameter values appropriate main simulation results inferior underlining need tune fdr parameter problem hand 

image example 
turning briefly images shows effect applying empirical bayes thresholds standard image gaussian noise added 
thresholds estimated separately channel level 
realizations generated signal noise ratio estimates snr log calculated thresholding johnstone silverman fig 

translation invariant hard thresholding applied noisy version peppers image 
original image noisy version see example 
left panel fixed threshold right panel level channel dependent eb thresholds shown table 
image obtained fixed thresholds contains spurious high frequency effects largely obscured printing process 
clearer comparison reader recommended view images online version available authors web sites 
empirical bayes thresholds 
smaller snr corresponds poorer estimation course quantitative measure necessarily correspond visual perception relative quality 
actual images shown correspond median examples ordered increase snr threshold approach empirical bayes approach 
example shown eb thresholds displayed table 
increase monotonically scale finer yield snr 
somewhat smaller vertical channel signal stronger peppers image 
fixing threshold channels leads small noise artifacts fine scales snr fixing threshold logn shown leads marked increase squared error reduced snr 
channel level horizontal vertical diagonal 
theoretical results 
turn theoretical investigation proposed empirical bayes method curve estimation wavelets 
doing empirical bayes selection wavelet thresholds distinguish various different models observed wavelet coefficients theoretical coefficients interest 
suppose level sum lengths coefficient vectors level equal 
models observed data 
white noise model assumed independent observations yjk jk wavelet coefficients jk 
orthonormality properties wavelet decomposition observations kind obtained carrying wavelet decomposition function dw white noise process 
main theory yjk levels setting coefficients higher levels zero 
model practical relevance sampled data model assume data xi independent random variables 
discrete wavelet transform sequence ti sequence sothat yjk jk current statistical literature distinction white noise coefficients yjk sampled data coefficients yjk glossed function coefficients time sampled coefficients 
theoretical framework generally assume jk apj corresponding membership underlying function particular smoothness class 
case shall consider observe estimate 
cases sampled data coefficients retain constraint underlying function show provided wavelet basis chosen appropriately discretization involved construction affect order magnitude accuracy estimates 
case consider estimates coefficients estimates wavelet coefficients function estimated coefficients reconstruct estimate sequence discrete wavelet transform 
impose periodic boundary conditions key prerequisite consideration sampled data model development appropriate boundary corrected bases corresponding preconditioning data near boundaries consider aspect 
final model situation sequence values primary interest place besov array bounds discrete wavelet transform sequence underlying function 
replace constraint jk apj johnstone silverman case require orthonormality discrete wavelet transform condition depends function particular consideration 
asymptotic theorem thought triangular array result limiting result particular function formalism proof identical white noise case need consider terms eliminates error terms result 

array results besov body constraints 
suppose jk coefficient array defined kj kj satisfying kj kl kj integers consider limits 
assume observations yjk jk kj variance assumed fixed known loss generality set 
denote vector jk kj define yj similarly 
vector estimated yl 
vector estimated separately empirical bayes method described 
set yj obtain estimate possibly modified threshold parameter 
threshold modified threshold defined 
threshold corresponding posterior median function provided value threshold estimation carried thresholding rule satisfying bounded shrinkage property 
set 
forj finer scales observations assumed available set 
risk defined rn 
suitable conditions wavelet family norm dominates norm th derivative original function see section 
constant contribution scaling coefficients multiplied somewhat arbitrary may altered affecting method results 
state main result demonstrates empirical bayes method attains optimal rate convergence mean qth power error values 
result yields smoothness properties posterior estimate 
demonstrates values satisfying conditions theorem coefficient array finite norm suitable conditions wavelet th derivative bounded norm 
theorem 
assume suppose coefficient array falls sequence besov ball empirical bayes selection wavelet thresholds aj set 
assume max 
assume sq quantity depend may depend wavelet family norm risk satisfies rn log qn rq ap sq qn log ap sq qn log ap sq 
remarks 
necessarily ap sq cases correspond respectively regular critical logarithmic zones described 
note elementary manipulations ap sq cases equally specified terms relative values andr min 
condition sq satisfied 
particular situation hold standard case 
rates agree lower bounds minimax rates derived theorem term constant multiple minimax dependence risk number observations subject besov body constraints 
fixed terms smaller order 
rates arise demonstrates suitable estimators dependent attain rates 
consider term 
conditions ar min inequality strict term lower polynomial order case 
fixed dominate logarithmic factor 
log term smaller order 
term shows besov space constant allowed johnstone silverman decrease increases zero shown risk reduced term size additional logarithmic term certain cases 
exact definition sq sq sq 
truncating risk fine scales 
consider estimation transform subject discretized constraints 
case need consider levels risk condition equivalent relaxed equivalent define rn 
simpler result rn log 
define sequence obtained inverse discrete wavelet transform applied 
standard case andq orthogonality wavelet transform allows deduce subject constraint implies 
log white noise model fine scale observations available 
assume data yjk levels just relax lower bound 
definiteness estimate data levels set estimate zero higher levels 
result rn exp log log suitable 
second term decays faster polynomial rate fixed proof theorem minor modifications required prove section 
empirical bayes selection wavelet thresholds 
wavelets scaling functions vanishing moments 
turn issue developing theory sampled data case subject retaining constraints function 
crucial theory wavelets constructed scaling function vanishing moments order continuous derivatives integer corresponding mother wavelet orthogonal polynomials degree supported interval integer discussed chapter example wavelets constructed satisfy properties 
zero moments scaling function control discretization error involved mapping observations scaling function coefficients fine scale 
note standard wavelet families scaling functions nearly vanishing moments orders issue investigation tradeoff finite samples relaxing condition exactly vanishing moments wavelets narrower support 
happy restrict attention periodic boundary conditions necessary modify wavelets scaling functions near boundary filters corresponding discrete wavelet transform 
construction section perform modification maintaining orthonormality basis functions 
review application construction fuller details properties see 
remarks 

restriction boundary modified needed theory inferior behavior observed daubechies wavelet families practice 
fact follows daubechies scaling function mean dx second moment necessarily vanishes 
horizontal shift obtains vanishing moments free 
approach sampled data taken donoho johnstone works broad class orthonormal scaling functions direct construction relating white noise sampled data models multiscale interpolation 
construction boundary scaling functions boundary wavelets 
support functions contained 
fix coarse resolution level level scaling functions defined jk jk jk johnstone silverman wavelets jk jk jk 
functions supported 
scaling functions defined gaps definition wavelets 
wavelets boundary wavelets smoothness vanishing moments original wavelets 
interior wavelets affected boundary construction depend interior scaling functions finest scale 
coarsest level scaling coefficients denote kl set indices scaling functions lk defined 
level set jk scaled version boundary wavelet ki set jk scaled version 
function define wavelet expansion jk jk kl lk dt kl jk jk dt need distinguish boundary interior wavelet coefficients write coefficients ki boundary coefficients kb 
constructing wavelet coefficients discrete data 
suppose vector observations values function 
order map scaling function coefficients suitable level necessary construct appropriate preconditioning matrices 
section define matrices set certain properties 
details see 
left boundary define matrix matrix wk dx uj 
empirical bayes selection wavelet thresholds full rank define matrix alu similarly matrix ar constructed satisfy ar wk dx 
sequence xn define preconditioned sequence pj pj pj xk pj 
xi uncorrelated variance variance matrix part pj part indices taken reverse order ar ar freedom choice ar example defined quite original sequence needed evaluate preconditioned sequence 
specifically eliminate dependence values sequence square invertible matrix consisting rows wu ar correspondingly 
hand values sequence freedom choose ar natural choice wu ar superscript denotes moore penrose generalized inverse 
choices minimize traces matrices ar ar sum variances pj suppose xi uncorrelated unit variance 
general assumption maximum eigenvalues ar ar boundary corrected discrete wavelet transform sequence pj eigenvalues variance matrix pj bounded ca 
orthogonality boundary corrected discrete wavelet transform variance elements bounded array interior coefficients yjk depend xi words xi left unchanged preconditioning 
uncorrelated array variables variance johnstone silverman preconditioning possible get approximations wavelet coefficients smooth function sequence gj discrete values 
define vector sj preconditioned sequence pj gj smooth function approximation scaling coefficient jk level precise provided times continuous differentiable jk dt supp jk result depends vanishing moment properties scaling functions construction preconditioning matrices 
full details see proposition 

boundary corrected empirical bayes estimator 
section set detailed definition boundary corrected version empirical bayes estimator prove attractive theoretical properties 
assume boundary corrected basis 
assume sufficient observations xi independent evaluate preconditioned sequence pj denote boundary corrected discrete wavelet transform pj define estimated coefficient array follows estimate coarse scaling coefficients observed values set yl 
estimate interior coefficients applying empirical bayes method level level observed array threshold boundary coefficients separately 
level hard threshold ca log jk yjk 
unobserved levels set jk 
main theoretical discussion measure risk estimate estimate wavelet expansion function estimate discrete wavelet transform array natural measure accuracy risk defined 
noted empirical bayes selection wavelet thresholds preconditioning array specifies uniquely values sequence away boundaries 
main result section demonstrates estimate optimal rate risk behavior zero 
theorem 
assume scaling function mother wavelet continuous derivatives support integer xm dx 
assume wavelets scaling functions modified boundary construction described thresholding carried modified threshold method 
assume available data construction estimator set 
assume sq assume 
assume 

set functions wavelet coefficients fall 
constant independent suitable suitable sup log log defined theorem fixed log smaller order 
general correspondence besov sequence function norms discussed section 
case included space defined membership besov sequence space known include space functions appropriately bounded total variation 
concern accuracy estimation array bound sup rn log 

besov array norms function norms 
theory gives minimax risk bounds functions array wavelet coefficients fall besov sequence balls 
appropriate assumptions wavelet basis besov sequence norms wavelet coefficients equivalent corresponding besov function norms functions 
relevant results specific case boundary corrected wavelets bounded interval considered detail appendix 
equivalence certainly hold wavelets bounded support vanishing moments order provided max 
see literature referenced 
besov function norm easy grasp intuitively integers helpful compare sobolev space norm johnstone silverman minimax results hold balls bm besov spaces standard result embedded space bm shows results hold minimax rates balls sobolev space turn error measure 
suppose function wavelet expansion 
define rq corresponds risk measure relative theoretical bounds obtained 
state prove proposition showing error norm dominates various function 
follows proposition bounds estimation error proved besov body error measure rq hold fortiori error measured integrated qth power derivatives order provided wavelet satisfies appropriate regularity conditions 
note lower bounds zero larger bounds required full besov norm equivalence 
proposition 
suppose integer dt 

sided bound leaves open possibility sobolev norm smaller besov norm right hand side 
reverse inequality terms besov norm space defined 
minimax rate convergence error norms regular logarithmic zones said capturing situation sobolev norm loss 

comparisons 
section compare theoretical results empirical bayes estimators established theorems known existing thresholds 
universal threshold logn leads rates convergence suboptimal logarithmic terms regular case critical cases see detailed discussion section 
example notation bound rate regular case ap sq log rq reason universal threshold empirical bayes selection wavelet thresholds suboptimal way essentially dense cases thresholds set bounded small number standard deviations order logn 
sure threshold recalled section donoho johnstone johnstone establish asymptotic optimality results special conditions squared error loss estimating function besov bodies :10.1.1.161.8697
sure estimate chooses thresholds optimize unbiased estimate mean squared error possible restrictions obtain optimal rates show limiting mse minimax optimal level constants threshold estimators 
token clear expect better optimal rates loss functions say rate issue remains formally investigated 
serious restriction sure reflected constraint 
discussed illustrated sure criterion far smooth asymptotic oracle inequality sure theorem contains error term crude order term prevented optimality drawn :10.1.1.161.8697
instability shown derive derivative discontinuity created threshold zone similar plots obtained applying sure criterion posterior median rule estimate thresholds 
related deficiency sure threshold choice 
sure adapts squared error loss dense signals criterion reliably propose high thresholds sparse signals 
order obtain theoretical results just cited necessary introduce hybrid version sure containing pretest sparsity detected switched logn threshold 
hybrid version creates grossly discontinuous transition thresholds sufficient theoretical result unattractive practice 
simulations hybrid modification counterproductive examples considered 
turn levelwise fdr applied wavelet context 
extensive analysis exact adaptive minimax optimality fdr sparse single sequence model balls losses published analysis rates convergence levelwise fdr estimate wavelet shrinkage setting 
unpublished combined optimality properties fdr sparse signals advantages sure dense signals improved pretest sparsity 
adaptive optimality rates convergence obtained conditions theorem case 
abandoned favor empirical bayes approach due smoother transition threshold choice sparse dense regimes reflected better performance actual examples 
massart investigate complexity penalized model selection approach gaussian estimation give risk bound squared error loss case 
applied setting approach johnstone silverman fig 

instability sure criterion compared stability mml criterion 
replications zk ind zero 
top panels show sure criterion solid expectation dashed 
bottom panels show quasi cauchy score function solid expectation dashed function quasi threshold defined solving 
johnstone silverman quasi threshold 
yields estimators minimax constants 
connection kind log parameter penalties massart abramovich benjamini donoho johnstone fdr estimation discussed 
paul obtains optimal rates convergence certain inverse problems direct estimation case reduce theorem 
optimality bayesian wavelet estimators mixture priors adaptive estimation perspective see example 

proofs main results 
proofs main theorems results johnstone silverman maximum marginal likelihood empirical bayes selection wavelet thresholds procedure single sequence case 
section review results form recast useful multilevel problems raised current 
section giving intuitive overview proof main result 
demonstrates way different kinds error bounds needed different levels array implicit proof way empirical bayes approach automatically adapts 
intuitive discussion formal proof theorem 
proof theorem follows section approximation properties boundary corrected bases preconditioning operators defined sections 

results single sequence problem 
vector suppose observations nn 
estimate applying marginal maximum likelihood approach data multiply result obtain estimate 
assume mixture prior satisfying assumptions set section family thresholding rules bounded shrinkage property 
may modified thresholding method defined 
convention denote unmodified case 
define log log 
making appropriate substitutions theorem obtain result 
rates convergence achieved leading terms various bounds minimax rates various parameter classes considered see details 
theorem 
suppose assumptions hold 
estimate satisfies risk bounds 
robustness 
exists constant cn 
adaptivity moderately sparse signals 
exist constants sufficiently large provided setting log sup 
adaptivity sparse signals 
sufficiently large sup cq qs log johnstone silverman results hold estimation carried posterior mean function weight estimated marginal maximum likelihood procedure 
order get intuitive understanding way result multilevel setting focus attention case pand ignore error term qs 
results theorem allow consider zones behavior underlying signal 
zone large signal noise ratio 
best bound risk order corresponding global risk maximum likelihood estimator mle second zone signal noise ratio smaller substantially reduced thresholding 
hard threshold rule threshold applied typically error individual coordinate 
favorable configuration satisfying constraint occur coordinates size little rest zero 
leads total error order third zone region described signal noise ratio small benefit attempting estimation isthe natural estimator 
incurs risk discussion similar simpler involving zones performance estimator similar mle second zero estimator 
impact theorem empirical bayes estimator adaptively achieves roughly speaking best possible behavior whichever zone signal falls having specify advance 
johnstone silverman assumed subsequently checked results extend broader range 

proof theorem 
heuristic 
formal proof continue intuitive discussion order give heuristic explanation rates convergence phase change proof come 
addition gain understanding kinds estimators bounds needed imitated empirical bayes method levels transform 
discussion inspired modulus continuity point view donoho johnstone kerkyacharian picard adapted setting 
heuristic discussion ignore constants error terms forth 
apply bounds theorem level level noise level multiresolution index structure besov body constraints imply level kj aj heuristic discussion empirical bayes selection wavelet thresholds approximate log simplicity actual proof approximation 
substitutions case zones theorem translate sup sq cn ap sq log cn log log aj aj 
zone corresponds coarsest scales transition middle zone occurs scale defined cn third bound applies scales finer index defined aj log risk bounds increase geometrically rises fall geometrically increases 
key behavior risk ap sq determines way risk behaves zone 
ap sq favorable index geometric decay risks away level rate determined sq rq ap sq favorable index rate qj cq log extra logarithmic terms set values referred logarithmic zone compare 
fig 

schematic risk contributions level logarithmic phase ap sq 
setting nn section set ss inf sup johnstone silverman ap sq level contributes heuristic approximation amount equal maximum case ap sq 
log levels leading extra log factor 
give formal proof strategy 
formal proof division risk 
split sum parts corresponding scaling coefficient large scale fine scale fine scale parts risk 
define satisfied 
smallest integer max log cn able apply bound levels 
second property ap satisfy cn write max rn rlo rhi rlo rhi case term dominate 
assume loss generality point proving result smaller values follows scaling function risk 
bq qth absolute moment standard normal random variable 
kl cn note holds regardless size scaling coefficients proof theorem uses bound empirical bayes selection wavelet thresholds risk coarse scales 
bound rlo result property sq rlo cn sq cn sq sq rlo cn cc rq case ap sq exactly magnitude required 
ap sq bound smaller term 
risk fine scales 
noted previously definition bound applied terms sum terms sum 
set ifa define kj sq cn log case definition 
level aj considering terms yields ap sq log consider cases show case 
combining bound allows conclude cases log 
case case necessarily ap sq exponent defining sum sq aq qj geometrically decreasing 
cc qj rq rq cc cc johnstone silverman case pand ap sq 
sum geometrically decreasing apart log term property cc ap sq log cc ap sq cc rq case pand sq ap 
case necessarily require sq define aj log split bounding sum zones lower zone bound log log log 
upper zone bound 
sums obtained set display 
terms sum geometrically increasing second geometrically decreasing sums dominated multiple value cc log sq ap cc min cc log sq ap cc cc log algebra substituting definition 
case pand ap sq 
argue case sum longer geometric bounded log carrying extra log factor argument yields cc log log 
risk fine scales 
define relative values rhi qj qj cc empirical bayes selection wavelet thresholds proof consideration related results 
combining bounds completes proof theorem 
obtain subject constraints follow exactly argument noting coefficients estimate term corresponding 
prove result observations scales available modify limits summation necessary 
calculations extended appropriate 
bounds affected calculation sq cn cn log hand sum starts leading bound rhi cc qj cc exp log 
incorporating changes main argument leads result 

proof theorem 
remarks preliminaries 
estimation problem considered sequence sj vector expected values preconditioned data pj define boundary corrected discrete wavelet transform sj procedure uses essentially estimate estimate true coefficients 
conditions theorem allow difference arrays bounded proposition aj cc immediate corollary fixed constant cc aj discretized coefficient array obeys constant besov sequence bounds true coefficient array 
precise value constant theorem min min min 
noted remarks theorem lower order fixed true log log term johnstone silverman main component error 
convention refers interior coefficients boundary coefficients 
yjk expected value jk interior coefficients independent normals variance 
bound argue exactly theorem obtain log defined 
equation gives main part risk bound theorem remainder proof consists controlling contributing errors 
coarse scale error 
consider coarse level scaling coefficients 
variance element yl bounded ca cn cn boundary coefficients 
contribution estimates boundary coefficients considered proposition 
proposition 
assumptions theorem uniformly log proof 
sum left hand side 
array number coefficients level elements array normally distributed expected values variances bounded obtain jk individually hard thresholding jk threshold standard properties norms thresholding jk jk jk yjk yjk jk bound give qj qj cj qj jq cn empirical bayes selection wavelet thresholds define aj log split risk parts 
letting rlo risk boundary coefficients levels min arguing rlo cn log cc log rhi contribution levels 
forj proposition account bound ca variance yjk jk jk jk aj jk substituting bound property sq gives rhi jsq cn vector length constant independent cc aj bound 
rhi jsq cn cc qj cn cc log cn complete proof combine bounds 
discretization bias 
risks quantify errors discretized coefficients 
control difference risk norm coefficients true coefficients define proof 
bound property follows cc qj qj qj johnstone silverman expression bounded qj hand sum geometrically increasing expression order qj min cases combined bound log completing proof 
complete proof theorem combine bounds 
gives required result 
additional term proportional log elementary manipulation log log log log follows bound holds case setting max qr 
completes proof theorem 
corresponding theorem periodic boundary conditions functions wavelet decompositions true proved simplified version approach need preconditioning consideration boundary coefficients 
prove result exactly argument need include discretization error error due levels transform 
proofs remarks 

proof proposition 
note values parameters rq shown equivalent corresponding function besov norm result follows embedding besov space sobolev space consequent results section 
deal explicitly parameter values boundary construction give argument embedding 
function jk support length maximum absolute value bounded rj jq rjq sj jk empirical bayes selection wavelet thresholds direct calculation property shows dt kl jk jk sj 
consider interior boundary separately 
define fl lk fi kl jk jk fb jk jk 
dt sup sup dt kl lk consider fi jk indicator function interval 
theorem chapter fact jr jk jk dx qj qj jr jk jk dx johnstone silverman consider contribution boundary wavelets 
sj union supports boundary wavelets level sj sj aj jk jk max kb jk sj sj 
nesting properties sj property obtain applying lder inequality aj sj dt aj aj bound term conclude dt dt bounds conclude dt completing proof 

lost bounded shrinkage 
besov space adaptivity result theorem provides context importance bounded shrinkage property assessed 
construct coefficient array setting jk 
empirical bayes selection wavelet thresholds member besov sequence space 
obviously choice level single nonzero element arbitrary fixed position 

mixture prior setting normal density 
value posterior median function property inequality holds posterior mean 
fixed random weight multiplying sides shows mean square error risk satisfies rn tend zero 
maximum risk besov sequence class diminish increases adaptivity result type theorem proved 
case tails asymptotic exp shown large consideration counterexample demonstrates weight chosen maximum risk besov sequence space bounded multiple large dominate rate theorem assumption tails heavy exponential essentially relaxed restricting removing adaptivity demonstrated theorem 

results posterior mean 
estimation conducted posterior mean strict thresholding rule results theorems hold bounds theorem apply case 
single sequence case johnstone silverman show failure posterior mean strict thresholding rule substantive effect risk 
counterexample unequivocally settle question behavior posterior mean estimator wavelet case 
possible extension modification theorems posterior mean estimator topic investigation 
acknowledgments 
johnstone grateful hospitality university bristol nonparametric semester institut henri poincar paris australian national university parts carried 
similarly silverman grateful department johnstone silverman statistics stanford university center advanced study behavioral sciences stanford 
versions silverman special invited johnstone second wald lecture 
gratefully acknowledge ed george sustained encouragement intellectual generosity period referees detailed helpful comments 
abramovich amato 

optimality bayesian wavelet estimators 
scand 
statist 

abramovich benjamini 

thresholding wavelet coefficients multiple hypotheses testing procedure 
wavelets statistics 
lecture notes statist 

springer berlin 
abramovich benjamini donoho 

adapting unknown sparsity controlling false discovery rate 
ann 
statist 
appear 
available www stat stanford edu 
abramovich 

wavelet thresholding bayesian approach 
stat 
soc 
ser 
stat 
methodol 

abramovich 

wavelet decomposition approaches statistical inverse problems 
biometrika 
antoniadis jansen johnstone silverman 

matlab software empirical bayes thresholding 
available imag fr lmc sms antoniadis 
benjamini 

controlling false discovery rate practical powerful approach multiple testing 
roy 
statist 
soc 
ser 

benjamini 

control false discovery rate multiple testing dependency 
ann 
statist 



gaussian model selection 
eur 
math 
soc 

cai silverman 

incorporating information neighboring coefficients wavelet estimation 
ser 
chipman kolaczyk 

adaptive bayesian wavelet shrinkage 
amer 
statist 
assoc 

clyde 

flexible empirical bayes estimation wavelets 
stat 
soc 
ser 
stat 
methodol 

clyde 

multiple shrinkage subset selection wavelets 
biometrika 
cohen daubechies 

wavelets interval fast wavelet transforms 
appl 
comput 
harmon 
anal 

coifman 

translation invariant de noising 
wavelets statistics 
lecture notes statist 

springer berlin 
daubechies 

lectures wavelets 
siam philadelphia 


minimax wavelet estimators 
appl 
comput 
harmon 
anal 

donoho 

spatial adaptation wavelet shrinkage 
biometrika 
donoho :10.1.1.161.8697

adapting unknown smoothness wavelet shrinkage 
amer 
statist 
assoc 

donoho 

asymptotic wavelet estimators sampled data 
statist 
sinica 
empirical bayes selection wavelet thresholds donoho johnstone kerkyacharian 

wavelet shrinkage 
discussion 
roy 
statist 
soc 
ser 

donoho johnstone kerkyacharian picard 

universal near wavelet shrinkage 
festschrift le cam pollard yang eds 

springer berlin 


quasi linear wavelet estimation 
amer 
statist 
assoc 

george foster 

empirical bayes variable selection 
proc 
workshop model selection 
special issue di ed ed 

bologna 
george 

calibration empirical bayes variable selection 
biometrika 
gopinath 

moments scaling function 
proc 
ieee international symposium circuits systems 
ieee press piscataway nj 
johnstone 

wavelet shrinkage correlated data inverse problems adaptivity results 
statist 
sinica 
johnstone 

threshold selection transform shrinkage 
statistical challenges modern astronomy iii babu eds 

springer new york 
johnstone 

function estimation gaussian sequence models 
draft monograph 
johnstone 

wavelet threshold estimators data correlated noise 
roy 
statist 
soc 
ser 

johnstone 

empirical bayes approaches mixture problems wavelet regression 
technical report dept statistics stanford univ johnstone 

boundary wavelet shrinkage function estimation 
appl 
probab 

johnstone 

needles straw empirical bayes estimates possibly sparse sequences 
ann 
statist 

johnstone 

programs empirical bayes thresholding 
statist 
software 
accompanying software manual 
liang 

longitudinal data analysis generalized linear models 
biometrika 
mallat 

wavelet tour signal processing nd expanded ed 
academic press san diego ca 
meyer 

wavelets operators 
cambridge univ press 
ller eds 

bayesian inference wavelet models 
lecture notes statist 

springer new york 
nason 

wavelet shrinkage cross validation 
roy 
statist 
soc 
ser 

nason 

software 
dept mathematics univ bristol uk 
available cran archive 
paul 

adaptive estimation linear inverse problems penalized model selection 
technical report dept statistics stanford univ 

frequentist optimality bayesian wavelet shrinkage rules gaussian non gaussian noise 
ann 
statist 
appear 


adaptive weights smoothing applications image restoration 
stat 
soc 
ser 
stat 
methodol 

johnstone silverman portilla wainwright 

image denoising scale mixtures gaussians wavelet domain 
ieee trans 
image process 

development core team 
language environment statistical computing 
foundation statistical computing vienna austria 
available www project org 
silverman 

wavelets statistics standard assumptions 
soc 
lond 
philos 
trans 
ser 
math 
phys 
eng 
sci 

triebel 

theory function spaces 
birkh user basel 
vidakovic 

wavelet nonparametric bayes methods 
practical nonparametric semiparametric bayesian statistics 
lecture notes statist 

springer new york 
vidakovic 

statistical modeling wavelets 
wiley new york 
wainwright simoncelli 

random cascades wavelet trees analyzing modeling natural images 
appl 
comput 
harmon 
anal 

zhang 

general empirical bayes wavelet methods exactly adaptive minimax estimation 
ann 
statist 

department statistics stanford university stanford california usa mail stat stanford edu st peter college oxford ox dl united kingdom mail bernard silverman spc ox ac uk 
