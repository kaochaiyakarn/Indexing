random forest approach relational learning van cs kuleuven ac cs kuleuven ac hendrik blockeel hendrik blockeel cs kuleuven ac department computer science katholieke universiteit leuven celestijnenlaan leuven belgium sa zeroski saso dzeroski ijs si department knowledge technologies jozef stefan institute ljubljana slovenia random forest induction ensemble method uses random subset features build node decision tree 
method shown features available 
certainly case relational learning especially aggregate functions combined selection conditions set aggregated included feature space 
presents initial exploration random forests relational context 
experimentally validated approach business domain structurally complex data set 

motivation observations 
random forest induction ensemble method builds decision trees random subset feature set node shown features available breiman 
relational learning typically deals large feature sets worthwhile investigate random forests perform context 
second random forests allows extension feature space including aggregate functions possibly combined selection conditions 
far combining considered difficult task blockeel bruynooghe feature set grows quickly search space behaved due non monotonicity problem 
organized follows 
section discuss random forests 
section illustrates problem combining aggregates selection conditions relational learning 
method random forest approach relational learning section 
section experimentally evaluate method structurally complex domain highly non determinate business domain 
formulate ideas research section 
random forests random forest induction breiman ensemble method 
ensemble learning algorithm constructs set classifiers classifies new data points vote predictions classifier 
necessary sufficient condition ensemble classifiers accurate individual members classifiers accurate diverse hansen salamon 
accurate classifier better random guessing new examples 
classifiers diverse different errors new data points 
different ways constructing ensembles bagging breiman boosting freund schapire instance introduce diversity manipulating training set 
approaches attempt increase variability manipulating input features output targets introducing randomness learning algorithm dietterich 
random forests try increase diversity classifiers resampling data changing feature sets different tree model induction processes 
exact procedure follows build dataset di sampling replacement dataset learn decision tree ti di randomly restricted feature sets predictions majority vote set trees part algorithm random forests differ normal bagging procedure emphasized 
inducing decision tree best feature selected fixed set features node 
bagging set features vary different runs induction procedure 
random forests different random subset size considered node 
best feature subset chosen 
obviously increases variability 
assume instance tests features root tree say best second best feature 
regular bagging approach consistently selected root random forests occur root nodes different trees respectively frequency 
occur slightly lower frequency 
random forests interesting properties breiman 
efficient sample features needs tested node features 
overfit trees added 
furthermore relatively robust outliers noise easily 
efficiency gain random forests especially interesting relational data mining typical large number features expensive compute 
hand relational data mining offers interesting test suite random forests exactly advantage random forests expected clear large feature spaces 
relational data mining data sets abound 
random forests allows enlarge feature set including aggregate functions possibly refined selection conditions 
discuss section 

aggregates selection relational learning considering multi relational learning sets general bags need handled 
represented relationships relations 
blockeel bruynooghe categorize current approaches relational learning respect handle sets 
ilp muggleton biased selection specific elements approaches prm koller approaches wrobel aggregate functions compute feature set summarizes set 
methods optimized highly non determinate business domains geared structurally complex domains molecular biology language learning provost hierarchy relational concepts increasing complexity complexity depends aggregate functions 
argue ilp currently approach explore concepts hierarchy 
combining aggregates selection ilp trivial task 
ilp feature construction aggregation features combined relatively restricted ways allowing complex conditions aggregation operator 
difficulties arise 
approaches ilp suffer large search space integrating aggregation operators increases hypothesis space blockeel bruynooghe 
search space behaved problem non monotonicity renders traditional pruning methods ilp inapplicable 
clearly idea performing systematic search hypothesis search heuristic 
explain reduction pruning opportunities 
suppose refining hypothesis consists adding condition body 
refining decrease coverage unsuitable low coverage refinements unsuitable prune search space conditions added aggregate coverage hypothesis may increase decrease pruning safely applied 
illustrate example count child 
adding literal inside count aggregate gives count child female 
coverage people daughters subset people children 
consider count child 
refinement count child female 
examples covered refinement superset original set people children certainly daughters people daughters may children 

approach combine aggregates reasonably complex selection conditions generalizing selection graph pattern language 
selection graphs graphical description sets objects multi relational database 
explore alternative approach random forests explores feature space 


random forest approach relational learning 
provide definitions useful explanation 
aggregate condition triple aggregate function comparison operator value domain aggregate condition monotone sets records example previous section showed count monotone aggregate condition count 
mentioned section 
describe method including aggregate functions selection graphs 
aggregate condition monotone selection conditions set aggregated allowed 
implemented similar method tilde blockeel de raedt included ace data mining system blockeel 
tilde relational top induction decision trees tdidt instantiation viewed order upgrade quinlan logical queries tree nodes 
expanded feature set considered node tree consist regular features aggregate conditions included augmented refinements aggregate conditions ones discussed example section path root current node branch taken 
built filter allows random subset tests considered node procedure built wrapper order get bagging 
mechanisms result random forest induction method 
want point random forests tackles difficulties arise combining aggregation selection ilp 
size search space limited consider subset possible features node tree 
second decision trees comparison operators equivalent switching branches 
means restrict monotone aggregate conditions 
allow non monotone aggregate conditions refinements aggregates wouldn chosen split node don add extra information tree 
advantage random forests randomly generated features included feature space 
includes construction aggregate conditions aggregate clause consists number conjuncts 
way tests count child female immediately considered root node refinement aggregate occuring tree 
non monotone aggregate functions refined deleting number conjuncts 
lead bottomup search strategy included top approach interesting explore 
implemented system 

experimental results aim experiments investigate benefit random forests relational setting feature set expanded aggregates refinements aggregates business domain structurally complex data set 
sections describe experimental setup results known data sets mutagenesis data set srinivasan financial data set 

experimental setup experiments performed fold crossvalidation carried times folds 
results averaged order obtain reliable estimate differs definition breiman random subset attributes tests chosen 
formance random forests 
different parameters needed set 
number random features node chose consider random subsets square root number tests node trees 
examined influence number trees random forests experimenting trees 
investigate performance random forests context aggregation performed experiments aggregates refinement aggregates conditions set aggregated added 

mutagenesis experiment mutagenesis data set 
ilp benchmark data set introduced ilp community srinivasan 
consists structural descriptions molecules classified mutagenic 
description consists atoms bonds molecule 
data set expect slight gain accuracy aggregates mutagenesis numerical attributes non determinate 
table shows results obtained mutagenesis data 
column best result bold 
experiments refined aggregates see best result random subsets tests 
fact see applying random forests random samples possible tests gives improvement significant decrease accuracy bagging certainly results efficiency gain 
expected clearly outperforms tilde 
results aggregates see random forests don enhance accuracy bagging performing better tilde 
may show random forests profit large feature sets 
results support breiman statement random forests tend overfit 
average difference training test set error random forests randomly tests node tilde features 
expected adding trees forest clearly increases accuracy cases 
results random forest trees making refined aggregates ra aggregates refinement wa aggregates na compared different sizes random subsets nodes table bagging corresponding random forests tests node 

accuracy random forests consisting trees aggregates na aggregates wa refined aggregates ra mutagenesis data 
results shown different numbers randomly chosen features 
trees square root number features node 
see matter number features improvement aggregates added 
allow refinements aggregates queries cases slight improvement observed 
example test frequently different trees aggregate count bid bond bid mol tp 
refinement count bid bond bid mol tp atom mol carbon 
part example represents set molecules bonds 
aggregate test 

refinement aggregate describes molecules bonds connected atom type carbon 

financial data set second experiment deals financial data set discovery challenge organized pkdd pkdd 
data set involves learning classify bank loans bad loans 
data set consists relations contains loan customer information account information includes permanent orders hundreds transactions account 
problem typical business data set non determinate 

accuracy random forests consisting trees aggregates na aggregates wa refined aggregates ra financial data 
results shown different numbers randomly chosen features 
table provides results obtained financial data 
data set want point data distribution quite skewed examples positive 
note case tilde rf trees rf trees rf trees ra wa na ra wa na ra wa na ra wa na sqrt table 
accuracy results mutagenesis data set 
rows describe different proportions random test subsets nodes 
columns compare accuracies experiments refined aggregates ra aggregates refinement wa aggregates na tilde random forests different number trees 
tilde rf trees rf trees rf trees ra wa na ra wa na ra wa na ra wa na sqrt table 
accuracy results financial data set 
rows describe different proportions random test subsets nodes 
columns compare accuracies experiments refined aggregates ra aggregates refinement wa aggregates na tilde random forests different number trees 
square root number tests average larger switched rows table 
observe experiments random part tests select best test certainly tests advisable drop accuracy significant gain profit efficiency gain testing fewer features node 
contrary results mutagenesis data aggregates accuracy increase adding randomness 
due skewness financial data randomness predictive accuracy forests accuracy predicting default value 
adding randomness trees tend predict default value predict small proportion examples negative 
improve accuracy default trees certainly informative 
adding aggregates resulted compact trees seen yielded large improvement accuracy 
allowing refinements aggregates didn improve accuracy 
may due fact theory needs learned represented complex selection aggregates 
trees aggregates rarely refined 

experimental experiments aggregates decreasing number random tests considered node certain threshold slightly increases predictive accuracy 
go threshold accuracy decreases 
quite difficult determine optimal value threshold 
chosen data sets sampling randomly tests node gives improvement significant decrease accuracy 
breiman hand obtained improvements smaller proportions features propositional case 
average improvement higher 
difference due fact approach random subset tests taken breiman takes random subset attributes selects best test attributes 
show relational learning random forests propositional learning 
results aggregates differ lot data sets mutagenesis decreasing random sample tests decreases accuracy financial data set accuracy increases 
effect skewness data 
expected adding trees forest increases accuracy 
course trees mean longer runtimes trade efficiency accuracy 
concerning aggregation selection allowing aggregate functions tests gave improve ment additional refinement aggregates didn consistently increase accuracy 
general performing bagging random forests computationally efficient bagging 
compare random forests consisting trees features bagging trees efficiency gain factor mutagenesis data factor financial data 
factor smaller propositional case need generate tests node set tests remains nodes 
relational case hand constructing new node tree consists steps possible refinement queries need generated secondly queries executed remaining examples query heuristic computed 
heuristics best test placed new node 
random forests time executing queries calculating heuristics reduced sample generated refinements considered 
current implementation time step remains possible refinements generated random sample set taken 

explored random forest approach relational learning 
random forests shown features available 
case relational learning 
investigation random forests context worthwhile 
random forests enlarge feature space including aggregate conditions refining selection conditions set aggregated 
combination aggregation selection conditions trivial task feature space grows quickly search space behaved due non monotonicity problem 
random forests overcomes problems 
experimentally validated strength random forests refined aggregates relational case 
results show randomly decrease feature set node tree certain level classification performance bagging profit gain efficiency 
chosen data sets level turned features threshold classification performance decreased 
benefit including aggregate functions clear results 
refinements aggregates yielded slight performance improvement 
effects unclear data sets explored 
turn possible directions 
mentioned section introduce randomly generated features aggregate functions number selection conditions set aggregated 
moment features refinements aggregate functions 
randomly generated aggregate functions increase diversity features refined deleting selection conditions function non monotone 
lead research benefits bottom search strategies included top approach 
alternative search heuristics explored 
choosing best feature features randomly chosen subset node tree features chosen boltzman distribution 
way features chance chosen chance proportional quality 
increase diversity trees probably improve strength random forest 
interesting investigate effect autocorrelation degree diversity jensen context random forests may influence extent different trees forest independent 
van supported institute promotion innovation science technology flanders 
supported goa inductive knowledge bases 
hendrik blockeel postdoctoral fellow fund scientific research flanders belgium 


guide financial data set 
ecml pkdd discovery challenge 
blockeel bruynooghe 

aggregation versus selection bias relational neural networks 
ijcai workshop learning statistical models relational data srl acapulco mexico august 
blockeel de raedt 

top induction order logical decision trees 
artificial intelligence 
blockeel dehaspe janssens ramon 

improving efficiency inductive logic programming query packs 
journal artificial intelligence research 
breiman 

bagging predictors 
machine learning 
breiman 

random forests 
machine learning 
dietterich 

ensemble methods machine learning 
proceedings th international workshop multiple classifier systems pp 

freund schapire 

experiments new boosting algorithm 
proceedings thirteenth international conference machine learning pp 

morgan kaufmann 
hansen salamon 

neural network ensembles 
ieee transactions patern analysis machine intelligence 
jensen neville hay 

avoiding bias aggregating relational data degree disparity 
proceedings th international conference machine learning 
siebes marseille 

involving aggregate functions multi relational search 
principles data mining knowledge discovery proceedings th european conference pp 

springer verlag 
koller 

probabilistic relational models 
proceedings ninth international workshop inductive logic programming pp 

springer verlag 
wrobel 

learning multi relational aggregation 
proceedings eleventh international conference inductive logic programming pp 

muggleton 
ed 

inductive logic programming 
academic press 
provost 

aggregation feature invention relational concept classes 
proceedings ninth acm sigkdd international conference knowledge discovery data mining pp 

acm press 
quinlan 

programs machine learning 
morgan kaufmann series machine learning 
morgan kaufmann 
srinivasan king bristol 

assessment ilp assisted models toxicology pte experiment 
proceedings ninth international workshop inductive logic programming pp 

springer verlag 
