filter methods law duch department informatics university toru poland department computer science school computer engineering nanyang technological university singapore google duch filter methods feature selection feature ranking feature selection algorithms may roughly divided types 
type encompasses algorithms built adaptive systems data analysis predictors example feature selection part embedded methods neural training algorithms 
algorithms second type wrapped predictors providing subsets features receiving feedback usually accuracy 
wrapper approaches aimed improving results specific predictors 
third type includes feature selection algorithms independent predictors filtering features little chance useful analysis data 
filter methods performance evaluation metric calculated directly data direct feedback predictors data reduced number features 
algorithms usually computationally expensive second group 
chapter devoted filter methods 
feature filter function returning relevance index estimates data relevant feature subset task usually classification approximation data 
data task usually fixed subsets vary relevance index may written 
text classification indices frequently called feature selection metrics may formal properties required call distance metric 
simple function correlation information content algorithmic procedure may estimate relevance index building decision tree finding nearest neighbors vectors 
means wrapper embedded algorithm may provide relevance estimation filter predictor 
relevance indices may computed individual features xi providing indices establish ranking order xi xi law duch xin 
features lowest ranks filtered 
independent features may sufficient features correlated important features may redundant 
best pair features include single best 
ranking guarantee largest subset important features 
methods search best subset features may filters wrappers embedded feature selection algorithms 
search methods independent evaluation feature subsets filters topic chapter 
focus filters ranking remarks calculation relevance indices subsets features sec 

value relevance index positively correlated accuracy reasonable predictor trained task data feature subset may true models theoretical grounds may difficult argue filter methods appropriate data analysis model 
little empirical experience matching filters classification approximation models 
different types filters matched different types predictors far theoretical arguments strong empirical evidence support claim 
case filter methods direct dependence relevance index predictors obviously thresholds feature rejection may set relevance indices evaluation feature contributions final system 
features ranked filter taken may determined predictor wrapper setting 
approach computationally expensive original wrapper approach evaluation predictor performance example cross validation test done pre selected feature sets 
theoretical arguments showing technique prone overfitting pure wrapper methods 
data mining applications example analysis large text corpora noun phrases features relatively inexpensive filter methods costs linear number features may prohibitively slow 
filters feature selection methods may divided local global types 
global evaluation features takes account data context free way 
context dependence may include different relevance different tasks classes different relevance different areas feature space 
local classification methods example nearest neighbor methods similarity may benefit local feature selection filters constructed demand data neighborhood vector 
obviously data samples may lead large errors estimations feature relevance index optimal tradeoff context reliability feature evaluation may difficult achieve 
case filter methods feature selection depends actual predictors data analysis 
filter methods section general issues related filter methods discussed 
section focused correlation filtering sec 
relevance indices distances distributions sec 
information theory 
section decision trees ranking feature selection discussed 
reliability calculation different indices bias respect number classes feature values important treated section 
followed remarks sec 
filters evaluation feature redundancy 
section contains 
general issues related filters mean feature relevant task 
artificial intelligence journal devoted special issue notion relevance vol 

common sense notion relevance rigorously defined axiomatic way see review 
definitions may useful design filter algorithms practical approach followed 
give simple intuitive definition relevance sufficient purpose feature selection feature relevant process distinguishing class values conditional probability different unconditional probability 
feature redundant correlated features selected 
ideas may traced back test theory developed psychological measurements 
main problem calculate strength correlations features classes generally features target output values features 
bayesian point view introduced classification problems approaches estimation relevance indices described subsequent sections 
approaches may directly regression problems may require quantization continuous outputs set pseudo classes 
consider simplest situation binary feature values class problem 
feature joint probability carries full information relevance feature matrix 
summing matrix classes marginalizing statisticians say values probabilities obtained summing feature values gives probabilities 
class probabilities fixed dataset sum elements joint probability matrix independent example 
convenience notation yi xj 
expected accuracy majority classifier mc amc maxy independent feature mc completely ignores informa law duch tion feature values 
bayesian classifier bc optimal decisions maximum posteriori probability selected giving larger fraction correct predictions smaller fraction errors 
equivalent maximum posteriori map rule select class greater posterior probability 
bayes error average accuracy map bayesian classifier bc 
single feature bayes error abc max yi xj max xj yi yi 
precise calculation real joint probabilities yi xj conditional probabilities xj yi observed frequencies require infinite amount training data bayesian formulas strictly true asymptotic sense 
training set large random sample represents distribution data feature space 
amc abc bayesian relevance index scaled convenience interval may taken abc amc amc 
may called purity index indicates pure discretization bins different feature values intervals 
index called misclassifications impurity index evaluate nodes decision trees 
features relevance index ranked equal joint probability distributions yi xj may significantly differ 
suppose feature amc 
distributions accuracy bayesian classifier abc error abc 
long equalities inequalities joint probabilities hold yi xj probabilities may change example influencing abc values 
bayesian relevance index sufficient uniquely rank features simplest binary case 
fact relevance indices additional conditions see sec 

reasoning may extended multi valued features continuous features discretization multi class problems leading probability distributions give identical values 
expected accuracy bayesian classification rule aspects taken account assessment indices 
statistical pattern recognition literature various measures inaccuracy error rates discriminability imprecision validity reliability inseparability resemblance resolution refinement see extended discussion 
knowing joint filter methods probabilities map bayesian classifier rule confusion matrices fij yi yj mij may easily constructed feature representing joint probability predicting sample class yi true class yj true predicted tp fn fp tn number hits true positives tp number hits class true negatives tn number false alarms false positives fp example healthy people predicted sick number misses false negatives fn sick people predicted healthy number samples sum mij 
confusion matrices independent entries row sum yj class probability estimated fraction samples belong class yj 
class accuracies conditional probabilities sample class really classified class usually taken independent variables 
medical informatics called sensitivity true positive rate information retrieval name recall detection rate called specificity 
diagonal elements conditional confusion matrix yi yi reflect type errors predictor 
example sensitivity shows sick people class correctly recognized classification rule feature results medical test specificity shows healthy people class recognized healthy test 
generalization class case obvious 
standard classifier accuracy obtained trace yi yj matrix acc yi yi yi 
arithmetic average class accuracies yi yi called balanced accuracy acc yi yi 
balanced error rate ber acc particularly useful evaluation measure unbalanced datasets 
feature ranking accuracy relevance indices abc indices equivalent comparing true positives minus false positives balanced accuracy equivalent true positives ratio minus false positives ratio terms constant data cancel comparison 
difference may rescaled example bns score standard inverse cumulative probability function normal distribution 
index called bi normal separation law duch index worked particularly information retrieval ir 
simple criterion field called odds ratio odds zero probabilities replaced small positive numbers 
ranking features may combination sensitivity specificity 
cost recognizing sick person low sensitivity may higher cost temporary hospitalization low specificity 
costs misclassification may introduced giving factor specify type errors false positive times important type errors false negative 
just summing number errors total misclassification cost 
binary feature values bc decision rule parameters costs fixed dataset 
probabilities calculated discretization continuous variable binary value calculated step function values sensitivity specificity depend threshold andthe total misclassification cost optimized respect 
popular way optimize thresholds called operating points classifiers receiver operator characteristic roc curves 
curves show points represent tradeoff false alarm rate sensitivity true positives rate 
area roc curve called auc frequently single parameter characterizing quality classifier may relevance index bc classification rules 
single threshold binary features point defined roc curve line segment connecting points 
case auc simply equal balanced accuracy acc ranking identical features difference true positive false positive ratios 
general case comparison aucs may give unique ranking features 
applications example information retrieval classifiers may different operating points depending resources may change time 
optimization roc curves point view feature selection leads filtering methods may appropriate different operating conditions 
number relevance indices modified bayesian rules may constructed facilitating feature selection accuracy cost confidence point view 
confusion matrix class problems may derive various combinations accuracy error terms harmonic mean recall precision called measure jf filter methods justified information retrieval 
selection auc balanced accuracy standard accuracy corresponds selection relative cost factor 
index combining accuracy error term favor type errors may optimize confidence rejection rates logical rules 
leads abc bayesian accuracy index large classification rule maximizes may reduce errors increasing confidence rule expense leaving samples unclassified 
non zero rejection rates introduced significant differences values different classes kept example feature may rejected values bayesian perspective improve result maximum posteriori rule index rarely relevance indices 
numerous theoretical results showing method probability density estimations finite samples convergence may slow bayes error estimate trusted 
reliability estimates rapidly decreases growing number distinct feature values continuous values growing number classes decreasing number training samples class feature value 
features index may different distributions lower entropy may preferred 
methods compare distributions feature class values may advantages 
empirical study simple relevance indices text classification shows accuracy poor choice balanced accuracy equivalent comparison aucs class problems giving higher recall similar precision 
surprising remembering applications text classification number classes high data usually unbalanced small 
distribution similarity may estimated various distance measures information theory correlation dependency coefficients consistency measures discussed sections 
theoretical results relating various measures expected errors bayesian classifier derived theoretical approaches met limited success empirical comparisons missing 
features continuous values discretized estimate probabilities needed compute relevance indices 
alternatively data may fitted combination continuous dimensional kernel functions gaussian functions frequently integration may summation 
relevance indices introduced global context free evaluating average usefulness single feature applications data distributions complex domains features may highly relevant area feature space relevant area 
feature selection algorithms relief described local information calculate global averaged law duch indices 
decision trees classification algorithms divide conquer approach hierarchically partitioning feature space need different subsets features different stages 
restricting calculations neighborhood input vector local context dependent relevance indices computed 
multiclass problems regression problems features important specific target values local output space recognized 
example data strongly unbalanced features important discrimination classes small number samples may missed 
case simplest solution apply filters multiple class problems 
case regression problems filters may applied samples give target values specific range 
correlation filters correlation coefficients simplest approach feature relevance measurements 
contrast information theoretic decision tree approaches avoid problems probability density estimation discretization continuous features treated 
statistics contingency tables defined pairs nominal features frequently analyzed determine correlations variables 
contain numbers times mij yi xj objects feature values yj xi appear database 
feature selection training samples may divided subsets mij specific feature value xj summing rows mij matrix marginal distribution mi samples classes obtained summing columns distribution samples distinct feature values xj obtained 
strength association variables usually measured statistics mij mij mij mij mi ij mij represent expected number observations assuming independence 
terms mij obviously avoided sufficient data non zero counts number samples class feature value replaced small number 
feature target values completely independent mij mij expected large differences show strong dependence 
estimate significance test incomplete gamma function 
number degrees freedom set 
approach justified statistical point view number classes number feature values large 
contrast bayesian indices results depend joint probabilities xi yj xi yj number samples filter methods implicitly including intuition estimation probabilities small samples accurate significance small correlations low 
statistics discretization methods combined feature selection 
linear correlation coefficient pearson popular statistics 
feature values classes values treated random variables defined xy xi xi yi yi xi xi yi yi 
ifx linearly dependent zero completely uncorrelated 
features may correlated positively negatively 
linear coefficient works long relation feature values target values monotonic 
separation means class distributions leads simpler criterion mean value class vectors class 
continuous targets threshold divides vectors groups 
square coefficient similar ratio class variances known fisher criterion 
test uses slightly different denominator number samples class 
ranking absolute values taken 
significant differences index values 
simplest test estimating probability variables correlated erf erf error function 
samples linear correlations coefficients small lead probabilities correlation 
estimation may improved joint probability variables 
feature list ordered decreasing values descending order may serve feature ranking 
similar approach taken problem cases larger values correlation coefficient probability impossible due finite numerical accuracy computations 
initial threshold may ranking determine features worth keeping reliable estimations may law duch done cross validation wrapper approaches 
alternative permutation test computationally expensive improving accuracy small number samples see neal zhang volume 
group features selected correlation coefficients may estimate correlation group class including inter correlations features 
relevance group features grows correlation features classes decreases growing inter correlation 
ideas discussed theory psychological measurements literature decision making aggregating opinions 
denoting average correlation coefficient features output variables xk average different features rkk xk xk group correlation coefficient measuring relevance feature subset may defined xk 
rkk formula obtained pearson correlation coefficient variables standardized 
correlation feature selection cfs algorithm adding forward selection deleting backward selection feature time :10.1.1.53.3935
non parametric spearman rank correlation coefficients may useful ordinal data types 
statistical tests independence define relevance indices kolmogorov smirnov test cumulative distributions statistics 
family algorithms called relief feature weighting estimating value feature helps distinguish instances near 
randomly selected sample nearest neighbors xs class xd different class 
feature weight relief relevance index jr feature increased small amount proportional difference xd relevance grow features separate vectors different classes decreased small amount proportional xs relevance decrease feature values different features nearby vectors class 
jr jr xd xs order large number iterations index captures local correlations feature values ability help discrimination vectors different classes 
variants include ratio average examples distance nearest average distance nearest hit self normalizes results jr ex xd ex xs 
relieff algorithm designed multiclass problems nearest neighbors class number filter methods vectors different classes 
robust presence noise data includes interesting approach estimation missing values 
relief algorithms represent quite original approach feature selection evaluation dimensional probability distributions 
finding nearest neighbors assures feature weights context sensitive global indices see algorithm type 
removing context sensitivity equivalent assuming feature independence possible provide complex formula yj yj gsx sy sy sx xi sy yj xi yj xi yj xi sx term modified gini index sec 

hall symmetrized version index exchanging averaging evaluation correlation pairs features :10.1.1.53.3935
relief combined useful technique successive gram schmidt orthogonalization features subset features created 
connection modified value difference metric mentioned section 
relevance indices distances distributions ways measure dependence features classes evaluating differences probability distributions 
simple measure difference joint product distributions proposed kolmogorov dk yj xi xi yj 
similar statistics results depend number samples 
replacing summation integration formula may easily applied continuous features probability densities known kernel functions fitted data 
may reach zero completely irrelevant features bounded law duch dk xi correlation classes feature values perfect 
index easily rescaled interval 
classes priori probabilities kolmogorov measure reduces dk xi xi 
expectation value squared posteriori probabilities known average euclidean norm conditional distribution called bayesian measure xi yj xi measures concentration conditional probability distribution different xi values way gini index eq 
decision trees sec 

kullback leibler divergence dkl py yi log py yi px xi frequently distance symmetric 
kl divergence may applied relevance estimation way statistics dkl yj xi log yj xi 
xi yj quantity known mutual information mi 
kullback liebler measure additive statistically independent features 
sensitive small differences distribution tails may lead problems especially multiclass applications relevance index taken average value kl divergences pairs classes 
jeffreys distance jm distance provides robust cri djm yj xi xi yj 
gaussian distributions djm related bhattacharya distance 
djm exp dkl exponential transformation jkl exp dkl defined reaching zero irrelevant features growing large divergences highly relevant features 
filter methods evidence distances quite effective remote sensing applications 
entropy defined jv xi yj xi yj xi simply equal jv 
error rate bayesian classifier bounded entropy abc jv 
ways compare distributions may devised may serve better relevance indicators tighter error bounds established 
memory reasoning distance vectors discrete elements nominal discretized class problem computed conditional probabilities vdm yj xi yj formula may evaluate feature similarity redundant features searched 
relevance measures information theory information theory indices frequently feature evaluation 
information negative entropy contained class distribution yi log yi yi mi fraction samples class yi formula calculate information contained discrete distribution feature values xi log xi 
continuous features discretized binned compute information associated single feature kernel functions fitted approximate density values integration performed summation 
information contained joint distribution classes features summed classes gives estimation importance feature 
information contained joint distribution law duch yj xi log yj xi continuous features yj log yj dx yj xi joint probability density continuous features finding feature value xi vectors belong class yj xi probability density finding vectors feature value xi 
indicate vectors single class dominate intervals making feature valuable prediction 
information additive independent random variables 
difference mi may taken mutual information information gain 
mutual information equal expected value ratio joint product probability distribution kullback leibler divergence mi yj xi yj xi log dkl yj xi yj xi 
yj xi feature important mutual information mi target feature distributions larger 
decision trees closely related quantity called information gain ig 
context feature selection gain simply difference ig information contained class distribution information distribution feature values taken account conditional information 
equal mi 
standard formula information gain easily obtained definition conditional information ig yj xi log yj xi ij xi yj xi log yj xi ij term total information class distributions subsets induced feature values xi weighted fractions xi samples feature value xi 
splits induced tests nodes decision trees usually directly attribute values information gain general different mutual information feature selection purposes quantities identical 
filter methods difficult prove bayes error abc bounded half value conditional information fano inequality log abc left side usually negative useful 
minimizing mi maximizing mutual information leads approximation bayes errors optimal predictions 
error bounds known renyi entropy easier estimate line learning shannon entropy 
various modifications information gain considered literature decision trees cf 
aimed avoiding bias multivalued features 
modifications include mi dh dm mi dh mi 
information gain ratio dh entropy distance dm mantaras distance symmetrical uncertainty coefficient 
coefficient particularly useful due simplicity low bias multi valued features :10.1.1.53.3935
measure jj xi yj xi log yj xi yj initially introduced measure information content logical rules applicable feature selection :10.1.1.51.3072
defined index called average weight evidence plausibility alternative entropy information xi log yj xi yj yj xi yj 
minimum description length mdl general idea razor principle kolmogorov algorithmic complexity 
joint complexity theory inferred data length data encoded theory minimal 
mdl applied construction decision trees selection features :10.1.1.51.3072
description test training samples divided subsets mij samples belong class yj specific feature value law duch xi mx 
number bits needed optimal encoding information class distribution training samples estimated number fixed dataset estimation repeated partitioning created feature value interval combinatorics applied information coding leads mdl formula expressed binomial multinomial coefficients 
mk 
way 
mdl log 
mk 
log mx mx log log 

mi marginal distributions calculated mij matrix 
final relevance index obtained dividing value terms representing length class distribution description 
symmetrized version mdl relevance index calculated exchanging features classes averaging values :10.1.1.53.3935
decision trees filtering decision trees select relevant features top hierarchical partitioning schemes 
deeper branches tree small portion data local information preserved 
feature selection global relevance greater importance 
way achieve create tree algorithms allow multiple splits tree single feature algorithms binary splits evaluate accuracy 
additional benefit decision trees continuous features provide optimized split points dividing feature values relatively pure bins 
calculation probabilities xj andp yi xj needed estimation mutual information relevance indices accurate na discretization bins equal width bins equal number samples 
mutual information calculated discretization decision tree may times larger naive discretization 
decision tree algorithm appropriate feature filtering creates single level trees 
features analyzed searching subset values range values vectors single class dominate 
algorithm parameter called bucket size acceptable level impurity range feature values allowing reduction number created intervals 
performance may estimated index optimal bucket size may evaluated filter methods cross validation bootstrap sampling help avoid bias large number intervals increase computational costs 
tree uses information gain determine splits select important features ranks important features close root node 
decision tree algorithm measures association classes feature values values eq 

information gain mentioned relevance indices advantage decision trees automatic discretization continuous features performed 
gini impurity index cart decision trees sums squares class probability distribution tree node yi feature split subsets discrete feature values xj values interval may generated gini indices subsets calculated 
gain proportional average sum squares conditional probabilities xj yi xj giving measure probability concentration useful feature ranking 
index similar entropy class distributions identical bayesian measure eq 

separability split value criterion determine splits decision tree discretize continuous features creating small number intervals subsets high information content 
may feature relevance index 
best split value separate maximum number pairs vectors different classes 
split values satisfy condition separates smallest number pairs vectors belonging class selected 
split value continuous feature real number discrete feature subset possible values feature 
cases left side ls right side rs split value defined test dataset ls rs ls typical test true selected feature xi sor discrete feature xi 
split value defined test ls di rs di min ls di rs di dk subset vectors belong class features separate number pairs training vectors second law duch term ranks higher separates lower number pairs class 
index properties gini easily calculated continuous discrete features 
feature values subsets checked determine simplest groupings larger number unique values feature treated ordered best split intervals searched 
feature selection applications splits calculated applied recursively data subsets dk creating tree 
pure nodes obtained algorithm stops prunes tree 
bayesian classifier rule applied interval subset created algorithm calculate relevance index 
complex tree approaches determine feature relevance pruning techniques 
reliability bias relevance indices different relevance indices 
empirical comparisons influence various indices difficult results depend data classifier 
works document categorization large number classes features samples may best bioinformatics data small number classes large number features samples analysis images 
way characterize relevance indices see features rank identical 
monotonic function transform relevance index indices rank features way 
relations may established indices see sec 
allowing clustering indices highly similar equivalent groups relations may established 
ranking order predicted mutual information information theoretic measures accuracy optimal bayesian classifier information contained single feature identical 
easy find examples binary valued features bc mi predictions re versed 
consider binary features class distributions relevance indices distributions mi values gini indices 
ranking descending order bayesian relevance mutual information gives gini index predicts differences relevance indices apparent contour plots showing lines constant values indices created proba bility distributions contour plots shown fig 
coordinates index linear mi logarithmic nonlinearity gini index stronger quadratic nonlin filter methods 
distributions index give identical values 
unique ranking obtained asking second opinion pairs indices gives identical values 
example bayesian relevance index distinguish mutual information cases give unique ranking fig 

contours constant values bc relevance index left mi index middle gini index right coordinates 
calculation indices information theory discrete features straightforward continuous features accuracy entropy calculations simple discretization algorithms histogram smoothing may low 
literature entropy estimation quite extensive especially physics journals concept entropy wide applications cf 

variance histogram mutual information estimators analyzed 
simple effective way calculate mutual information parzen windows 
calculation mutual information pairs features class distribution difficult interesting approximations conditional mutual information proposed calculate 
filters ranking relevance indices may give similar results 
main differences bias relation number distinct feature values variance respect accuracy estimation small number samples 
issue bias estimating multi valued features initially discussed decision tree literature 
gain ratio mantaras distance introduced precisely avoid favoring attributes larger number values intervals 
biases relevance indices including indices gini measure weight evidence mdl relief experimentally examined informative non informative features :10.1.1.51.3072
class problems biases large number feature values relatively small classes significant 
mutual information gini measure approximately linear increase function number feature values observed steepness proportional number classes 
comparison indices relief sec 
mdl sec 
came biased 
symmetrical uncertainty coefficient law duch similar low bias :10.1.1.53.3935
biases evaluation feature correlations examined hall :10.1.1.53.3935
significant differences observed accuracy stability calculation different indices discretization performed 
fig 
shows convergence plots indices created overlapping gaussian distributions variance means shifted units function number bins constant width partition range feature values 
analytical values probabilities bin simulate infinite amount data renormalized sum 
small number bins errors high observed accuracy bayesian relevance index 
convergence index quite slow oscillatory 
mutual information eq 
converges faster information gain ratio eq 
shows similar behavior gini index eq 
symmetrical uncertainty coefficient eq 
converge quickly reaching correct values bins fig 

convergence low bias coefficient candidate best relevance index 
gini su mi bc fig 

differences gini mi indices exact value vertical axis function number discretization bins horizontal axis 
filters feature selection filter methods relevance indices discussed previous sections treat feature independent exception relief family algorithms sec 
group correlation coefficient eq 
allowing feature ranking 
features relevance index threshold filtered useful 
feature selection algorithms may try include interdependence features 
subset features new candidate feature relevance index index extended set needed 
theory rigorous bayesian approach may evaluate gain accuracy bayesian classifier adding single feature 
features rule abc xk max yi xk sum replaced integral continuous features 
formula converges slowly dimension fig 
main problem reliably estimate joint probabilities yj xk 
density training data goes rapidly zero growing dimensionality feature space 
binary features data samples bins non empty 
various histogram smoothing algorithms may regularize probabilities hashing techniques may help avoiding high computational costs reliable estimation abc possible underlying distributions fully known 
may useful golden standard calculate error bounds done dimensional distributions practical method 
calculating relevance indices subsets selected large number features possible include full interactions features 
note wrappers may evaluate full feature interactions depending classification algorithm 
approximations summing pair wise interactions offer computationally expensive alternative 
cfs algorithm described sec 
eq 
calculating average correlation coefficients features classes different features 
ratio relevance indices may measure correlation dependency features may linear combination terms xs user defined constant introduced balance importance relevance redundancy estimated sum feature feature 
algorithm mutual information relevance measure 
way redundancy features partially taken account search subsets features may proceed filter level 
variant method may maximum pair relevance xs sum features needed fewer features recognized redundant 
law duch idea inconsistency conflict situation vectors subset feature values associated different classes leads search subsets features consistent 
similar relations search reducts rough set theory 
inconsistency count equal number samples identical features minus number samples class largest number samples belong class index zero 
summing inconsistency counts dividing number samples inconsistency rate subset obtained 
rate interesting measure feature subset quality example monotonic contrast relevance indices decreasing increasing feature subsets 
features may ranked inconsistency rates main application index feature selection 
summary comparison various restrictions applications relevance indices discussed previous sections 
example correlation coefficients pearson linear correlation require numerical features applied features nominal values 
indices require probabilities easy estimate continuous features especially number samples small 
usually achieved discretization methods 
relevance indices decision trees may automatically provide discretization methods rely external algorithms 
table information popular filters collected including formulas types inputs binary multivalued integer symbolic continuous values outputs binary class multivalued integer multiclass problems continuous regression 
method bayesian accuracy abc observed probabilities yj xi provides golden standard methods 
relations bayesian accuracy mutual information known relations may inferred information indices general theoretical results sort difficult find indices simply heuristics 
new methods tested bayesian accuracy simple binary features binary classes 
differences ranking features major relevance indices sec 
probably amplified general situations issue systematically investigated far 
methods belong group methods tab 
special 
evaluation confusion matrix elements indirectly dependent probabilities yj xi 
confusion matrix may obtained classifier bayesian approach classification balanced accuracy area curve auc measure bi normal filter methods separation odds ratio best possible approaches assuming specific costs different type errors 
variants simple statistical index separation class means exist 
indices commonly applied problems binary targets extension multiple target values straightforward 
practice pair wise evaluation single target value rest may better finding features important discrimination small classes 
feature values statistical relevance indices numerical target values may symbolic 
pearson linear correlation coefficient applied numerical feature target values averaged maximum version evaluation correlations subset features 
indices applicable symbolic values may computed quite rapidly 
trees may capture importance feature local subset data handled tree nodes lie levels root 
relief family methods especially attractive may applied situations low bias include interaction features may capture local dependencies methods continuous target values especially difficult handle directly distance measures similarity distributions may handle problems 
kolmogorov distance relevance indices group may expressed sum discrete probabilities integral probability density functions 
bayesian measure identical gini index discrete distributions generalizes continuous features continuous targets 
exception group value difference metric specifically designed symbolic data 
indices information theory may continuous features targets probability densities defined 
information gain ratio symmetrical uncertainty coefficient especially worth recommending sharing low bias mdl approach sec 
converging stable quick way correct values 
discussion filters provide cheapest approach evaluation feature relevance 
large number features indispensable filtering features expansive feature selection methods feasible 
approaches filters discussed preceding sections show simple answer question relevance index best construct filter 
sufficient data joint probabilities may estimated reliable way reason bayesian relevance 
relevance indices particular indices theory information approximations bayesian relevance 
unfortunately index difficult law duch method comments name formula bayesian accuracy eq 
theoretically golden standard rescaled bayesian relevance eq 

balanced accuracy eq 
average sensitivity specificity unbalanced dataset auc binary targets 
bi normal separation eq 
information retrieval 
measure eq 
harmonic recall precision popular information retrieval 
odds ratio eq 
popular information retrieval 
means separation eq 
class means related fisher criterion 
statistics eq 
means separation 
pearson correlation eq 
linear correlation significance test eq 
permutation test 
group correlation eq 
pearson coefficient subset features 
eq 
results depend number samples relief eq 
family methods formula simplified version captures local correlations feature interactions 
separability split value eq 
decision tree index 
kolmogorov distance eq 
difference joint product probabilities 
bayesian measure eq 
entropy eq 
gini eq 

kullback leibler divergence eq 
equivalent mutual information 
jeffreys distance eq 
rarely worth trying 
value difference metric eq 
symbolic data similarity methods symbolic feature feature correlations 
mutual information eq 
equivalent information gain eq 

information gain ratio eq 
information gain divided feature entropy stable evaluation 
symmetrical uncertainty eq 
low bias multivalued features 
measure eq 
measures information provided logical rule 
weight evidence eq 
far rarely 
mdl eq 
low bias multivalued features 
table 
summary relevance measures suitable filters 
features targets may binary type symbolic integer symbolic implies integer continuous real numbers 
methods directly continuous values need discretization 
filter methods estimate reliably see fig 
leaving room approaches 
applications including costs different types misclassifications sec 
better choice relevance index leading balanced accuracy eq 
measure optimization roc curves 
evaluation quantities suffer problem evaluation bayesian relevance approximate reliable methods studied 
different approaches relevance evaluation lead large number indices ranking selection 
certainly papers new versions relevance indices information filters published useful 
noted book cart splitting criteria influence quality decision trees cart tree old index known bayesian measure eq 
entropy eq 
employed new name gini 
actual choice feature relevance indices little influence performance filters 
applications simple approach example correlation coefficient may sufficient 
options explored far open questions remain 
similarities equivalence monotonic transformation relevance indices established 
reliability estimation relevance indices exception entropy estimations known 
biases multi valued features indices identified influence ranking clear 
little effort devoted far cost sensitive feature selection 
respect accuracy bayesian classification rules performance metrics related logical rules worth investigating 
attention paid specific class oriented local context dependent filters 
problems especially bioinformatics require simultaneous identification features may individually quite poor relevance 
paradigmatic benchmark problems sort parity problems starting xor 
context dependent local feature selection methods relief filter methods applied vectors localized feature space region able deal cases 
knowledge filter feature selection significantly grown years remains done field 
acknowledgment am grateful drawing attention papers field norbert krzysztof gr critically reading isabelle guyon helpful remarks 
supported polish committee scientific research 
law duch 
almuallim dietterich 
learning irrelevant features 
proceedings th national conference artificial intelligence aaai pages 

devroye 
extensive empirical study feature selection metrics text classification 
ieee transactions pattern analysis machine intelligence 

battiti 
mutual information selecting features supervised neural net learning 
ieee trans 
neural networks 

bell wang 
formalism relevance application feature subset selection 
machine learning 

breiman friedman olshen stone 
classification regression trees 
wadsworth brooks monterey ca 

roli 
extension jeffreys distance multiclass cases feature selection 
ieee transactions geoscience remote sensing 

coetzee glover lawrence giles 
feature selection web applications roc inflections powerset pruning 
proceedings symp 
applications internet saint pages los alamitos ca 
ieee computer society 

cover 
best independent measurements best 
ieee transactions systems man cybernetics 

cox 
theoretical statistics 
chapman hall crc press berlin heidelberg new york 

dash liu 
consistency search feature selection 
artificial intelligence 

de mantaras 
distance attribute selection measure decision tree induction 
machine learning journal 

devroye lugosi 
probabilistic theory pattern recognition 
springer berlin heidelberg new york 

duch andk gr 
new methodology extraction optimization application crisp fuzzy logical rules 
ieee transactions neural networks 

duch 
posteriori corrections classification methods 
editors neural networks soft computing pages 
physica verlag springer berlin heidelberg new york 

duch setiono 
computational intelligence methods understanding data 
proceedings ieee 

duch 
feature ranking selection discretization 
proceedings int 
conf 
artificial neural networks icann pages istanbul 
university press 

duda hart stork 
classification 
john wiley sons new york 

principe 
lower upper bounds misclassification probability information 
journal vlsi signal processing systems 

forman 
extensive empirical study feature selection metrics text classification 
journal machine learning research 
filter methods 

theory psychological measurement 
mcgrawhill new york 

gr duch 
separability split value criterion 
proceedings th conf 
neural networks soft computing pages poland 
polish neural network society 

guyon 
bitter ahmed brown heller 
multivariate nonlinear feature selection kernel multiplicative updates gram schmidt relief 
flint workshop berkeley dec 

hall 
correlation feature subset selection machine learning 
phd thesis department computer science university waikato waikato 

hand 
construction assessment classification rules 
wiley sons chichester 

hanley mcneil 
meaning area receiver operating characteristic roc curve 
radiology 


methods aggregating opinions 
de editors decision making change human affairs 
reidel publishing dordrecht holland 

grosse 
bayes estimators generalized entropies 
physics math 
general 

holte 
simple classification rules perform commonly datasets 
machine learning 

hong 
contextual information feature ranking discretization 
ieee transactions knowledge data engineering 

kass 
exploratory technique investigating large quantities categorical data 
applied statistics 

kohavi john 
wrappers feature subset selection 
artificial intelligence 

kononenko 
biases estimating multivalued attributes 
proceedings ijcai montreal pages san mateo ca 
morgan kaufmann 


choi 
input feature selection mutual information parzen window 
ieee transactions pattern analysis machine intelligence 


choi 
input feature selection classification problems 
ieee transactions neural networks 

li vit nyi 
kolmogorov complexity applications 
text monographs computer science 
springer berlin heidelberg new york 

liu tan dash 
discretization enabling technique 
journal data mining knowledge discovery 

liu setiono 
feature selection discretization 
ieee transactions knowledge data engineering 

michie 
personal models rationality 
statistical planning inference 


statistic estimate variance histogram mutual information estimator dependent pairs observations 
signal processing 
law duch 
ng 
feature selection learning exponentially irrelevant features training examples 
proceedings th international conference machine learning pages san francisco ca 
morgan kaufmann 

press teukolsky vetterling flannery 
numerical recipes art scientific computing 
cambridge university press cambridge uk 

quinlan 
programs machine learning 
morgan kaufman san mateo ca 

kononenko 
theoretical empirical analysis relieff rrelieff 
machine learning 

smyth goodman 
information theoretic approach rule induction databases 
ieee transactions knowledge data engineering 

swets 
measuring accuracy diagnostic systems 
proceedings ieee 


rough set methods feature selection recognition 
pattern recognition letters 

torkkola 
feature extraction non parametric mutual information maximization 
journal machine learning research 

toussaint 
note optimal selection independent binary valued features pattern recognition 
ieee transactions information theory 


theory statistical inference information 
kluwer academic press london 

van rijsbergen 
information retrieval 
butterworths london 


feature evaluation measures probabilistic dependence 
ieee transactions computers 

wilson martinez 
improved heterogeneous distance functions 
journal artificial intelligence research 
