discussion bayesian nonparametric latent feature models zoubin ghahramani david biostatistics branch md national institute environmental health sciences brief comments research triangle park nc usa ghahramani colleagues proposed interesting class infinite latent feature ilf models 
basic premise ilf models infinitely latent predictors represented population particular subject having finite selection 
important advance models allow finite number latent variables 
ilf models useful features rare obtains sparse representation 
realistically hope learn latent feature structure available data 
utility sparse latent factor models illustrated large small problems west carvalho 

performance best number latent features represented sample sample size clear practical advantages ilf formulation finite latent variable models allow uncertainty dimension 
example lopes west allow number latent factors unknown bayesian methods 
said conceptually appealing allow additional features represented data set additional subjects added appealing allow partial clustering subjects 
particular ilf model subjects features common leading degree similarity number shared features values features 
notation ghahramani latent feature vector subject denoted fi 
fik fik zik subject feature zik vik value feature 
important aspects specification infinite latent feature model prior binary matrix zik prior matrix vik 
focus ghahramani prior proposing indian process ibp specification 
ibp follows straightforward elegant manner assumptions elements independent bernoulli distributed probability occurrence kth feature ii beta 
features treated exchangeable specification necessary introduce left ordering function possible base inference finite approximation focusing common features 
discussion briefly consider general problem nonparametric modeling proposing exponentiated gamma dirichlet process prior 
exponentiated gamma alternative ibp advantages dirichlet process dp ferguson nonparametric modeling feature scores subjects possessing feature 
exponentiated gamma dirichlet process provide motivation focus application ilf model clearly warranted 
agricultural health study kamel interest focused studying factors contributing neurological symptom headaches occur rence farm workers 
individual asked questionnaire record frequency symptom occurrence different symptom types resulting response vector yi yi 
yip natural suppose symptom frequencies yi provide mea latent features fi fi 
fik fik zik individual latent risk factor vik denotes severity risk factor individual example feature may represent occurrence mild stroke vik represents severe stroke severe stroke resulting frequent neurological problems 
data characterized typical latent class model requires individuals grouped single set classes 
approach ghahramani ideal case important drawbacks 
assumption feature exchangeability inferences latent features awkward 
posterior samples collected mcmc algorithm feature index changes meaning 
label ambiguity occurs dpm models 
solution setting ilf models choose prior explicitly orders features frequency occurrence feature common 
second potentially characterize data fewer features modeling feature scores vik 
provides realistic characterization data 
assuming parametric model artificially inflates number features needed fit data making latent features characterize true unobserved risk factor 
exponentiated gamma dirichlet process prior address issues 
define exponentiated gamma component prior provides probability model random matrix loss generality features ordered trait tends common population features decrease stochastically population frequency increasing index accomplished letting exp ind 

stochastically decreasing infinite sequence independent gamma random variables stochastic decreasing constraint ensured letting 

marginalizing prior obtain pr exp exp decreasing increasing 

note ibp exponentiated gamma process defined result poisson distribution si number traits sub ject 
si defined convolution independent identically distributed bernoulli random variables 
convenient special case corresponds exp 
results logistic regression model frequency trait occurrence marginalizing 
case hyperparameters characterize pro cess controlling frequency trait controlling rapidly traits de crease frequency index restriction ensures 

assuming straightforward show distribution si ac approximated distribution sit sufficiently large applications sparse representation dominant features expressed ing may preferred 
cases accurate truncation approximation produced replacing ft zt vt denoting element wise product denoting submatrix consisting columns 
finite integer 
expressions provide prior random binary matrix allocating features subjects 
order complete specification gh gh dp 
gh represents random probability measure characterizing distribution hth latent feature score individuals feature 
probability measure drawn dirichlet process dp base measure precision 
nonparametric latent factor models illustrate focus nonparametric extension factor analysis 
subjects 
yi yi 
yip denote multivariate response vector 
typical factor analytic model expressed yi fi np 
mean vector factor loadings matrix fi fi 
fik vector latent factors normal residual nal covariance see example lopes west 
parametric specification typically assumes constraining factor loadings matrix ensure identifiability 
fi denotes unknown distribution fi shorthand notation exponentiated gamma dirichlet process prior hyperparameters 
due constraint higher numbered factors correspond rarer features frequent population avoid need constrain 
remove sign ambiguity restrict strictly positive support ensuring note characterization appealing properties 
distributions factor scores modelled subjects automatically clustered groups separately factor 
groups corresponds cluster subjects having factor formed discreteness property dp 
second formulation automatically allows unknown number factors represented subjects sample 
uncertainty number factors accommodated different manner lopes west 
third chosen truncated normal posterior computation proceed efficiency data augmentation mcmc algorithm 
truncation approximation say algorithm alternately updates conditionally gibbs sampling steps ii elements sampling bernoulli full conditional posterior distributions iii 
data augmentation step relying approach similar stanford holmes held iv standard algorithms maceachern ller 
details excluded space considerations 
carvalho lucas wang west 
high dimensional sparse factor models latent factor regression 
isds discussion duke uni versity 

efficient bayesian model averaging factor analysis 
isds discussion duke university 
stanford 
bayesian inferences predictors conception probabilities 
biometrics 
ferguson 
bayesian analysis nonparametric problems 
annals statistics 
ferguson 
prior distributions spaces probability measures 
annals statistics 
holmes held 

bayesian auxiliary variable models binary multinomial regression 
bayesian analysis 
lopes west 

bayesian model assessment factor analysis 
statistica sinica 
maceachern ller 
estimating mixture dirichlet process models 
journal computational graphical statistics 
west 
bayesian factor regression models large small paradigm 
bayesian statistics 

