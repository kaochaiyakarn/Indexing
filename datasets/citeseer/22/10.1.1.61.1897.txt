training expansion bridging theory practice maria computer science dept carnegie mellon univ pittsburgh pa cs cmu edu avrim blum computer science dept carnegie mellon univ pittsburgh pa avrim cs cmu edu ke yang computer science dept carnegie mellon univ pittsburgh pa cs cmu edu training method combining labeled unlabeled data examples thought containing distinct sets features 
number practical successes previous theoretical analyses needed strong assumptions data satisfied practice 
propose weaker expansion assumption underlying data distribution prove sufficient iterative cotraining succeed appropriately strong pac learning algorithms feature set extent necessary 
expansion assumption fact motivates iterative nature original training algorithm stronger assumptions independence label allow simpler shot training succeed 
heuristically analyze effect performance noise data 
predicted behavior qualitatively matched synthetic experiments expander graphs 
machine learning case unlabeled data substantially cheaper plentiful labeled data result number methods developed unlabeled data try improve performance :10.1.1.114.9164
training method substantial success scenarios examples thought containing distinct sufficient feature sets 
specifically labeled example takes form hx wherex andx parts example label 
assumes existence respective feature sets 
intuitively means example contains views view contains sufficient information determine label example 
redundancy implies underlying structure unlabeled data need consistent structure unlabeled data informative 
particular idea iterative training small labeled sample train initial respective views iteratively bootstrap unlabeled confident label examples learning algorithm view improving classifier 
example webpage classification webpages contain text hyperlinks pointing 
small labeled sample learn says link words advisor points page page probably positive example faculty member home page find unlabeled example property label page learning algorithm uses text page 
approach variants variety learning problems including named entity classification text classification natural language processing large scale document classification visual detectors 
training effectively requires distinct properties underlying data distribution order 
principle exist low error classifiers view 
second views hand highly correlated need examples confident vice versa training algorithm 
unfortunately previous theoretical analyses needed strong assumptions second type order prove guarantees 
include conditional independence label assumption weak rule dependence 
primary contribution theoretical analysis substantially relaxes strength second assumption just form expansion underlying distribution natural analog graph theoretic notions expansion conductance show sense necessary condition training succeed 
need fairly strong assumption learning algorithms produce confident wrong formally algorithms able learn positive data give heuristic analysis case hold 
key feature assuming expansion data specifically motivates iterative nature training algorithm 
previous assumptions analyzed imply strong form expansion shot version training succeed see section 
fact theoretical guarantees exactly type 
distributions easily satisfy weaker condition allowing shot learning describe natural situations form 
additional property results algorithmic nature 
sufficiently strong efficient pac learning algorithms target function feature set achieve efficient pac style guarantees training 
mentioned need stronger assumption base learning algorithms see section 
letx fxi xi formally defining expansion assumption connecting standard graph theoretic notions expansion conductance 
prove statement expansion sufficient iterative training succeed strong base learning algorithms view proving bounds number iterations needed converge 
section heuristically analyze effect imperfect feature sets training accuracy 
section experiments synthetic expander graph data qualitatively bear analyses 
notations definitions assumptions assume examples drawn instance wherex andx correspond different views example 
hi denote target function letx andx denote positive negative regions simplicity assume doing binary classification 
assume view sufficient correct classification decomposed view probability mass 
fori ci xi think ofx asx letx xi denote marginal distribution andx respectively 
order discuss iterative training need able talk hypothesis confident confident example 
convenience identify confident confident positive 
means think hypothesis subset confident positive means opinion 
away initialization phase training labeled data generate initial hypothesis assume initial 
goal training bootstrap sets unlabeled data 
prove guarantees iterative training assumptions learning algorithms views able learn positive data expanding defined section 
assumption base learning algorithms views assume learning algorithms view able pac learn positive data 
specifically access examples ithe algorithm able produce hix sided error probability error iis algorithms type naturally thought predicting positive confidence don know fitting framework 
examples concept classes learnable positive data include conjunctions cnf rectangles see 
instance case axis parallel rectangles simple algorithm achieves guarantee just output smallest rectangle enclosing positive examples seen 
wanted consider algorithms confident directions just confident positive notion reliable useful learning due rivest sloan 
fewer classes functions learnable manner 
addition nice feature assumption pr pr expand 
especially natural positive class large amount cohesion consists documents negatives documents topics 
note effectively assuming algorithms correct confident relax heuristic analysis section 
expansion assumption underlying distribution fors ands denote event example hx si 
think ofs ands confident sets view pr denotes probability mass examples confident views denotes probability mass examples confident just 
section probabilities respect tod 
say definition expanding say expanding respect hypothesis holds alls denote ithe set 
get feel definition notice expansion sense necessary iterative training succeed ifs ands confident sets expand see examples hypothesis help 
js pr section show definition fact sufficient 
see weaker definition previously considered requirements helpful consider slightly stronger kind expansion call left right expansion 
definition right expanding expansion requires pair expand strictly necessary 
occasional pairs expand pairs rare encountered confident sets training process ok left expanding holds indices reversed 
left right expanding properties 
js pr immediately obvious left right expansion fact implies definition see appendix converse necessarily true 
introduce notion reasons 
useful intuition confident set inx set small pr si train classifier learns positive data conditional distribution error distribution definition implies confident set iwill noticeably larger probability clear useful training initial stages 
secondly notion helps clarify assumptions restrictive considered previously 
specifically independence label independence label implies ands js pr 
js pr tiny 
means expand factor def 
fact expands nearly ofx 
weak dependence weak dependence relaxation conditional independence requires alls 
restrictive 
notice pr js js implies definition weak dependence 
sufficiently small ifs small expands nearly ofx 
means conditional independence algorithm pac learns positive data trains conditional distribution bys driving error conditional distribution perform training just iteration 
connections standard graph theoretic notions expansion definition expansion definition natural analog standard graphtheoretic notion edge expansion conductance 
markov chain said high conductance stationary distribution set probability probability mass transitions times probability ofs 
see 
graph high edge expansion random walk graph high conductance 
stationary distribution walk viewed having equal probability edge equivalent saying partition graph pieces number edges crossing partition fraction number edges smaller half 
connect definition think 
known example random degree bipartite graph high probability expanding fact motivates synthetic data experiments section 
examples give simple examples satisfy expansion weak dependence 
example target function view axis parallel rectangle 
suppose random positive example looks andx uniformly distributed rectangles highly dependent way specifically identical tox random coordinate re randomized rectangle 
distribution satisfy weak dependence disjoint axes tjs hard verify expanding 
example imagine learning problem data inx falls different clusters positive class union clusters negative class union 
imagine likewise true look atx simplicity suppose cluster probability mass independence label say positive equally positive 
suppose weaker associated cj inx inci 
distribution clearly weak dependence property 
say learning algorithm assumes cluster label hypothesis rules split clusters 
graph clusters associated expander graph distributions expanding respect toh 
particular labeled learning algorithm generalize tox entire propagated nodes associated 
sn main result main result 
assume expanding respect hypothesis initial confident init target function written views anda learning positive data 
iterative training consider proceeds rounds 
confident sets view start 
feeding examples tod conditioned si 
take unlabeled examples current predictors confident feed positive 
error confidence parameters theorem 
simultaneously witha ifx sn 
pr pr pr pr pr pr pre determined number specified theorem algorithm terminates outputs predictor labels ias positive negative 
stating lemmas useful analysis 
lemmas lets tj hj 
probabilities respect tod 
lemma pr pr js pr pr pr pr get pr pr 
pr follows expansion property pr pr pr implies lemma pr pr 
pr js js pr 
proof js js get pr pr 
pr follows expansion property pr 


pr js 
pr 
proof js js log fin sn sn theorem final desired accuracy confidence parameters 
achieve error probability running training init rounds time anda accuracy pr si si confidence parameters set 
proof sketch assume fori si confident sets view training 
pr si si qi pr si si andi pi probabilities respect tod 
interested bounding pr si si technically easier si si show probability fin obviously implies 
guarantees ona anda round get probability fin fin si si si fin si si fin 
particular implies probability pr pr init 
consider 
probability fin si lemma obtain probability si si log fin si si ifi pr si si 
similarly applying lemma obtain probability pr si si 
assume case learning anda successful note happens probability fin 
observations imply long soi init 
means init iterations training get situation 
point notice rounds drops factor 
total init rounds predictor desired accuracy desired confidence 
heuristic analysis error propagation experiments far min min assumed existence perfect classifiers view andx vice versa 
addition assumed correctly labeled positive examples input learning algorithms able generalize way sided error confident wrong 
section give heuristic analysis case assumptions relaxed synthetic experiments expander graphs 
heuristic analysis error propagation confident iteration define purity precision prd si coverage recall prd si si jc 
define opposite coverage prd si si jc 
previously 
imagine fraction examples views disagree positive negative regions expand uniformly rate natural assume form increase accuracy negative accuracy negative accuracy positive accuracy positive accuracy accuracy iteration iteration red opp cov 
increase catch 
accuracy negative accuracy positive accuracy iteration training noise rates respectively 
solid line indicates accuracy green dashed increasing curve accuracy positives dashed decreasing curve accuracy negatives 
corresponds positive negative parts confident sets region expanding way proof theorem fraction new edges going examples label 
examining simple observations 
initially coverage low steps get expect coverage increase exponentially purity drop linearly 
coverage gets large begins saturate purity high time dropping rapidly exponential particular calculation omitted shows positive negative accuracy increases point drops 
qualitative behavior borne experiments 
experiments performed experiments synthetic data lines example noise added section 
specifically create graph 
nodes side represent positive clusters non side represent negative clusters 
connect node left nodes right neighbor chosen probability random node class probability random node opposite class 
initial confident propagate confidence rounds training monitoring percentage positive class covered percent negative class mistakenly covered accuracy 
plots experiments shown different noise rates 
seen qualitatively match expect coverage increases exponentially accuracy negatives drops exponentially somewhat delayed 
point crossover predicted roughly corresponds point accuracy starts drop 
training method unlabeled data examples partitioned views view roughly sufficient achieve classification views highly correlated 
previous theoretical required instantiating condition strong sense independence label form weak dependence 
argue right condition weaker expansion property underlying distribution positive examples show sufficient extent necessary 
expansion property especially interesting directly motivates iterative nature practical training algorithms rigorous analysis iterative training setting demonstrates advantages shot versions 
supported part nsf ccr nsf itr ccr nsf itr iis 
abney 
bootstrapping 
proceedings th annual meeting association computational linguistics acl pages 
blum mitchell 
combining labeled unlabeled data training 
proc 
th annual conference computational learning theory pages 
collins singer 
unsupervised models named entity classification 
sigdat conf 
empirical methods nlp large corpora pages 
dasgupta littman mcallester 
pac generalization bounds training 
advances neural information processing systems 
mit press 
ghani 
combining labeled unlabeled data text classification large number categories 
proceedings ieee international conference data mining 
joachims 
transductive inference text classification support vector machines 
proceedings th international conference machine learning pages 
kearns li valiant 
learning boolean formulae 
jacm 
levin paul viola yoav freund 
unsupervised improvement visual detectors training 
proc 
th ieee international conf 
computer vision pages 
motwani raghavan 
randomized algorithms 
cambridge university press 
nigam ghani 
analyzing effectiveness applicability training 
proc 
acm cikm int 
conf 
information knowledge management pages 
nigam mccallum thrun mitchell 
text classification labeled unlabeled documents em 
machine learning 
park zhang 
large scale unstructured document classification unlabeled data syntactic information 
pakdd lncs vol 
pages 
springer 
pierce cardie 
limitations training natural language learning large datasets 
proc 
conference empirical methods nlp pages 
pr rivest sloan 
learning complicated concepts reliably usefully 
proceedings workshop computational learning theory pages 
david yarowsky 
unsupervised word sense disambiguation rivaling supervised methods 
meeting association computational linguistics pages 
zhu pr js pr ghahramani pr lafferty 
pr pr pr pr pr semi supervised learning gaussian fields harmonic functions 
proc 
th international conf 
machine learning pages 
relating definitions show definition implies definition 
theorem ifd satisfies left right expansion definition satisfies expansion definition 
proof prove contrapositive 
suppose exists pr pr 
assume loss generality pr pr 
pr pr follows 
pr 
implies pr pr pr 
notice pr 
similarly pr get failure expansion direction 
completes proof 
pr pr pr pr 
