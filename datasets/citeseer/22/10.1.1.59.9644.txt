constraint induction multi objective regression trees jan sa zeroski katholieke universiteit leuven dept computer science celestijnenlaan leuven belgium jan cs kuleuven jozef stefan institute dept knowledge technologies ljubljana slovenia saso dzeroski ijs si 
constrained inductive systems key component inductive databases responsible building models satisfy constraints inductive queries 
propose constraint system building multi objective regression trees 
multi objective regression tree decision tree capable predicting numeric variables 
focus size accuracy constraints 
specifying maximum size minimum accuracy user trade size interpretability accuracy 
approach build large tree training data prune second step satisfy user constraints 
advantage tree stored inductive database answering inductive queries different constraints 
size accuracy constraints briefly discuss syntactic constraints 
evaluate system number real world data sets measure size versus accuracy trade 
idea inductive databases tightly integrate databases data mining 
inductive database stores data models obtained running mining algorithms data 
means query language user retrieve particular models 
example user query system decision tree smaller nodes accuracy particular attribute top node 
database include model satisfying constraints induction algorithm called construct 
propose constraint induction algorithm multiobjective regression trees 
regression trees capable predicting numeric variables 
main advantages building separate regression tree target single mort usually smaller total size individual trees variables mort dependencies different target variables 
approach propose build large tree training data prune second step satisfy user constraints 
advantage tree stored inductive database answering inductive queries different constraints 
pruning algorithm propose extension pruning algorithm classification trees developed garofalakis turn earlier bratko almuallim 
dynamic programming algorithm searches subtree tree satisfies size accuracy constraints 
minimize tree size return smallest tree satisfying minimum accuracy constraint maximize accuracy return accurate tree satisfying maximum size constraint 
extending pruning algorithm extensive empirical evaluation measuring size versus accuracy trade real world data sets 
evaluation shows accuracy close set single objective regression trees size cases tree size reduced significantly increasing interpretability expense small accuracy loss 
rest organized follows 
section briefly discuss induction algorithm 
section reviews pruning algorithm garofalakis section extends 
accuracy syntactic constraints discussed section 
empirical evaluation follows section section states main 
multi objective regression trees regression trees capable predicting numeric target variables 
example mort depicted fig 

leaf stores vector components predictions different target variables 
fig 

mort predicting numeric variables 
introduced special instance predictive clustering trees 
framework tree viewed hierarchy clusters corresponds cluster containing data recursively partitioned smaller clusters moving tree 
constructed standard top induction algorithm similar cart 
heuristic algorithm selecting attribute tests procedure tree computed return tree error tree tree error leaf error leaf tree error tree error tree tree computed true return tree error procedure procedure leaf return tree remove children tree fig 

constraint decision tree pruning algorithm 
internal nodes intra cluster variation summed subsets induced test 
intra cluster variation defined var yt number examples cluster number target variables var yt variance target variable cluster 
minimizing intra cluster variation results homogeneous leaves turn results accurate predictions predicted vector leaf vector mean target vectors training examples belonging 
details 
constraint decision tree pruning fig 
defines pruning method proposed garofalakis computing maximum tree size subtree tree rooted node maximum accuracy minimum error 
called find nodes included solution called remove nodes 
employs dynamic programming compute tree error error minimum error subtree rooted containing nodes 
subtree tree pruned leaf tree children consider binary trees minimum error subtree size 
algorithm computes minimum possibilities loop starting line 
possibility pruned leaf taken account initializing error leaf error line 
flag tree computed avoid repeated computation information 
completes tree stores maximum size left subtree minimum error subtree nodes rooted note tree subtree consists node called prune nodes belong minimum error subtree 
time space complexity algorithm nk size tree 
size constraints pruning algorithm discussed previous section originally developed classification setting error measure number misclassified examples minimum description length cost 
difficult see algorithm combination error measure additive measure holds data set partitioned number subsets error computed set equal sum errors computed subsets partition 
definition additive error measure 
error measure additive iff data set partitioned subsets di holds di 
additivity property error measure lines algorithm 
error measures multi objective regression setting satisfy additivity property squared error absolute error 
definition squared absolute error 
data set examples targets squared error defined se yt absolute error ae yt yt actual predicted value target variable example obviously pruning algorithm minimize error measures just plugging line 
note minimizing squared error implicitly minimizes error measures monotonically increasing function root mean squared error rmse 
holds absolute error mean absolute error mae 
pruning algorithm trivially extended error measures 
empirical evaluation section pruning algorithm combination squared error definition 
section number remarks 
obtain results required heuristic building tree compatible error measure heuristic designed optimize error measure pruning algorithm 
case say requirement holds intra cluster variation heuristic locally optimizes squared error 
locally optimizing error measure best choice context classification trees information gain heuristic accuracy 
error measures regression tasks pearson correlation additive monotonically increasing function additive measure 
error measures minimized pruning algorithm fig 

garofalakis proposes method pushing constraints tree building phase 
tree building efficient disadvantage resulting tree specific constraints query anymore answering queries constraints 
pushing constraints case difficult case classification trees 
reason constraint pushing algorithm requires computation lower bound error partially built tree 
knowledge lower bound defined regression trees 
focus similar approach possible predictive clustering trees general long error measure additivity property 
example consider multi objective classification trees error measure number misclassified examples summed different target variables 
multi objective trees numeric nominal target variables define additive error measure weighted sum measure nominal variables numeric variables 
maximum error syntactic constraints pruning algorithm find subtree minimum error maximum size constraint 
algorithm solving dual problem maximum error constraint find smallest tree satisfies constraint 
accomplish constructs sequence pruned trees algorithm fig 
increasing values size constraint km tree satisfies maximum error constraint 
resulting tree smallest tree having error maximum error constraint 
computing sequence trees computationally cheap pruning algorithm access data leaf error computed stored node running pruning algorithm 
tree values small trees reused constructing larger trees 
multi objective regression setting approach specify maximum error summed target variables 
approach specify bound individual target variable 
useful application demands target variables predicted accurately 
size error constraints syntactic constraints important practice 
focus discuss briefly 
syntactic constraints follows context decision trees 
suppose domain expert knows attributes important application 
syntactic constraint mine decision tree attribute top node 
trees different attributes top node equally accurate attribute selected expert probably easy interpret 
clus system empirical evaluation supports type syntactic constraints 
idea user specify partial tree subtree including root node inductive query 
induction algorithm initialized partial tree regular top induction method complete 
ability syntactic partial tree constraints allows greater involvement user construction decision tree greater user influence final result 
domain knowledge user taken account way 
empirical evaluation goal empirical evaluation fold 
evaluate size versus error trade possible size constraints real world applications 
second compare single objective multi objective regression 
hypothesis test single multi objective tree size equally accurate set single objective trees target variable size having single small multi objective model equally accurate advantageous easier interpret set trees 
explicitly represent dependencies different targets 
tree fig 
shows negative influence targets negative influence target positive effect second 
experimental setup size error syntactic constraints implemented clus clus system building clustering trees general particular 
data sets listed properties table 
data sets ecological nature 
data set represents multi objective regression problem number target variables varies 
detailed description data sets included table 
data set run clus single objective mode target variable multi objective mode 
fold cross validation estimate performance resulting trees 
run build large tree store generate subtrees tree pruning algorithm discussed section different values size constraint set pruning algorithm minimize squared error training set 
follow approach proposed 
note algorithm combination separate validation set 
include results obtained system weka toolkit 
note supports single objective regression 
clus available authors request 
table 
data set properties domain number instances number input attributes attr number target attributes 
results domain task attr real simulated soil quality coll 
groups coll 
groups coll 
species soil quality water quality plants animals chemical fig 
fig 
results 
experiment root mean squared error rmse average pearson correlation averaged target variables reported 
data sets results multi objective tree close set single objective trees sorts especially large tree sizes 
results slightly favor sorts 
increased interpretability offered comes price small increase error 
exception soil quality perform little better sorts 
effect explained fact target variables highly correlated data set 
largest performance difference obtained soil quality species 
sorts perform clearly better 
number target variables high 
note implies total size sorts times size mort 
investigate effect plotted results total model size horizontal axis fig 
fig 

results show total size error obtained mort data sets clearly smaller set sorts 
measured error similar 
observe error curves typically flat large size interval 
tree size cases kept small loosing accuracy 
graphs fig 
fig 
domain expert easily select tree trade size interpretability accuracy 
interesting note overfitting occurs soil quality 
data sets error decreases tree size constant value increase larger trees 
graphs include results regression tree mode 
results close results obtained clus 
size trees situated flat part error curve 
data sets generates trees large 
extreme case simulated generates tree nodes 
setting size constraint unnecessarily large trees easily avoided 
rmse rmse rmse size rmse targets targets real soil quality coll bio 
size rmse targets targets max size nodes max size nodes simulated size rmse soil quality groups size size rmse rmse targets targets soil quality coll 
groups soil quality coll 
species size rmse sorts rmse mort rmse sorts mort fig 

rmse correlation different values size constraint 
rmse rmse targets targets soil quality size rmse water quality animals size rmse targets targets max size nodes max size nodes water quality plants size rmse water quality chemical size rmse sorts rmse mort rmse sorts mort fig 

rmse correlation different values size constraint 
summarize size constraints choice interpretability important small loss accuracy tolerated 
accuracy important larger mort preferable set sorts total size larger 
proposed system constrained induction multi objective regression trees 
supports size error syntactic constraints works steps 
step large tree built satisfies syntactic constraints 
tree stored inductive database second step generate trees satisfy particular size error constraints 
accomplish extended pruning algorithm introduced garofalakis 
modes operation supported maximum size constraint return subtree smallest error maximum error constraint return smallest subtree satisfies constraint 
focused pruning algorithm extended predictive clustering trees general 
multi objective classification multi objective prediction nominal numeric targets 
empirical evaluation tested approach number real world data sets 
evaluation shows accuracy close rmse rmse rmse targets targets targets targets real soil quality coll bio 
soil quality coll 
groups targets targets total size nodes total size nodes simulated soil quality groups soil quality coll 
species sorts rmse mort rmse sorts mort fig 

rmse correlation versus total model size 
rmse rmse targets targets soil quality water quality animals targets targets total size nodes total size nodes water quality plants water quality chemical sorts rmse mort rmse sorts mort fig 

rmse correlation versus total model size 
set single objective regression trees size cases tree size reduced significantly increasing interpretability expense small accuracy loss 
size constraints choice interpretability important small loss accuracy tolerated 
acknowledgments authors grateful hendrik blockeel provided valuable comments text empirical evaluation 
jan supported research fund leuven 

almuallim 
efficient algorithm optimal pruning decision trees 
artificial intelligence 

blockeel de raedt ramon 
top induction clustering trees 
proceedings th international conference machine learning pages 

bratko 
trading accuracy simplicity decision trees 
machine learning 

breiman friedman olshen stone 
classification regression trees 
wadsworth belmont 

de raedt 
perspective inductive databases 
sigkdd explorations 

dem sar zeroski 
modelling pollen dispersal genetically modified field 

annual meeting ecological society america montreal canada 
august 

dem sar zeroski henning krogh larsen 
multiobjective classification model communities soil 
ecological modelling 
appear 

zeroski dem sar 
predicting chemical parameters river water quality data 
applied intelligence 

zeroski 
analysing effect field characteristics gene flow varieties volunteers regression trees 

submitted nd international conference existence gm non gm agricultural supply chains 
montpellier france 
november 

garofalakis rastogi shim 
building decision trees constraints 
data mining knowledge discovery 

imielinski mannila 
database perspective knowledge discovery 
communications acm 

zeroski wieland 
application machine learning techniques analysis soil ecological data bases relationships habitat features community characteristics 
soil biology biochemistry 

quinlan 
programs machine learning 
morgan kaufmann series machine learning 
morgan kaufmann 

witten frank 
data mining practical machine learning tools techniques 
morgan kaufmann 
nd edition 

