nonlinear dimensionality reduction semidefinite programming kernel matrix factorization kilian weinberger benjamin lawrence saul department computer information science university pennsylvania philadelphia pa seas upenn edu describe algorithm nonlinear dimensionality reduction semidefinite programming kernel matrix factorization 
algorithm learns kernel matrix high dimensional data lies near low dimensional manifold 
earlier kernel matrix learned maximizing variance feature space preserving distances angles nearest neighbors 
adapting ideas semi supervised learning graphs show full kernel matrix approximated product smaller matrices 
representing kernel matrix way reformulate semidefinite program terms smaller submatrix inner products randomly chosen landmarks 
new framework leads order magnitude reductions computation time possible study larger problems manifold learning 
large family graph algorithms emerged analyzing high dimensional data lies near low dimensional manifold :10.1.1.111.3313
algorithms derive low dimensional embeddings top bottom eigenvectors specially constructed matrices 
directly indirectly matrices related kernel matrices inner products nonlinear feature space 
algorithms viewed kernel methods feature spaces unfold manifold data sampled 
supported nsf award 
introduced semidefinite embedding sde algorithm manifold learning semidefinite programming 
sde learns kernel matrix maximizing variance feature space preserving distances angles nearest neighbors 
interesting properties main optimization convex guaranteed preserve certain aspects local geometry method yields definite kernel matrix kernel matrix provides estimate underlying manifold dimensionality method rely estimating geodesic distances faraway points manifold 
particular combination advantages appears unique sde 
main disadvantage sde relative algorithms manifold learning time required solve large problems semidefinite programming 
earlier sde limited data sets examples problems size typically required hours computation mid range desktop computer 
describe new framework allowed reproduce original results small fraction time study larger problems manifold learning 
start showing sampled manifolds entire kernel matrix accurately reconstructed smaller submatrix inner products randomly chosen landmarks 
particular letting denote full kernel matrix write submatrix inner products landmarks linear transformation derived solving sparse set linear equations 
factorization eq 
enables reformulate semidefinite program terms smaller matrix yielding order magnitude reductions computation time 
framework interesting connections previous manifold learning kernel methods 
landmark methods originally developed accelerate multidimensional scaling procedure isomap subsequently applied fast embedding sparse similarity graphs 
intuitively methods papers idea triangulation locating points low dimensional space distances small set landmarks 
idea viewed application nystr method particular way extrapolating full kernel matrix sub blocks 
worth emphasizing landmarks intuition 
sde directly estimate geodesic distances faraway inputs manifold isomap 
opposed nystr method approach better described adaptation ideas semisupervised learning graphs 
approach somewhat novel ideas transductive inference computational savings purely unsupervised setting 
manage constraints appear semidefinite programming problems adapted certain ideas large scale training support vector machines 
organized follows 
section review earlier manifold learning semidefinite programming 
section investigate kernel matrix factorization eq 
deriving linear transformation reconstructs examples landmarks showing simplifies semidefinite program manifold learning 
section gives experimental results data sets images text 
conclude section 
semidefinite embedding briefly review algorithm sde details previous 
input algorithm takes high dimensional vectors 
xn output produces low dimensional vectors 
yn 
inputs xi assumed lie near manifold embedded dimensions typically goal algorithm estimate dimensionality output faithful embedding reveals structure manifold 
main idea sde aptly described maximum variance unfolding 
algorithm attempts maximize variance embedding subject constraint distances angles nearby inputs preserved 
resulting transformation inputs outputs looks locally rotation plus translation represents isometry 
picture transformation dimensions imagine flag pulling corners 
step algorithm compute neighbors input 
matrix defined ij inputs xi xj nearest neighbors exists input nearest neighbors ij 
constraints preserve distances angles nearest neighbors written yi yj xi xj ij 
eliminate translational degree freedom embedding outputs constrained centered origin yi 
algorithm attempts unfold inputs maximizing variance var yi preserving local distances angles eq 

maximizing variance embedding turns useful surrogate minimizing dimensionality computationally tractable 
optimization formulated instance semidefinite programming 
kij yi yj denote gram kernel matrix outputs 
shown earlier eqs 
written entirely terms elements matrix 
learn kernel matrix solving semidefinite program 
maximize trace subject 

ij kii kij kjj xi xj kernel pca embedding derived eigenvalues eigenvectors kernel matrix particular algorithm outputs top eigenvalues eigenvectors 
dimensionality embedding suggested number appreciably non zero eigenvalues 
sum algorithm steps computing nearest neighbors ii computing kernel matrix iii computing top eigenvectors 
computation time typically dominated semidefinite program learn kernel matrix 
earlier step limited problems examples nearest neighbors problems size typically required hours computation mid range desktop computer 
kernel matrix factorization practice sde scales poorly large data sets solve semidefinite program matrices number examples 
note computation time prohibitive despite polynomial time guarantees convergence semidefinite programming 
section show sampled manifolds kernel matrix approximately factored product smaller matrices 
representation derive simpler semidefinite programs optimization previous section 
sketch algorithm sketching basic argument factorization eq 

argument steps 
derive linear transformation approximately reconstructing entire data set high di randomly chosen inputs xi puts designated landmarks 
particular denoting landmarks reconstructed inputs xi linear transformation xi qi 
linear transformation derived sparse weighted graph node represents input weights propagate positions landmarks remaining nodes 
situation analogous semi supervised learning large graphs nodes represent labeled unlabeled examples transductive inferences diffusion graph 
setting landmarks correspond labeled examples reconstructed inputs unlabeled examples vectors actual labels 
show linear transformation reconstruct unfolded data set mapping inputs xi outputs examples sdp solver csdp time complexity iteration sparse problems target matrices constraints 
large constant factors associated complexity estimates 
yi particular denoting unfolded landmarks reconstructed outputs yi argue yi yi yi qi 
connection eqs 
follow particular construction weighted graph yields linear transformation weighted graph derived appealing symmetries linear reconstruction coefficients similar intuition algorithm manifold learning locally linear embedding lle 
kernel matrix factorization eq 
follows approximation kij yi yj yi yj 
particular substituting eq 
eq 
gives approximate factorization submatrix inner products unfolded landmark positions 
reconstructing landmarks derive linear transformation eqs 
assume high dimensional inputs xi sampled low dimensional manifold 
neighborhood point manifold locally approximated linear subspace 
approximation hope reconstruct input weighted sum nearest neighbors small 
value analogous necessarily equal value define neighborhoods previous section 
reconstruction weights minimizing error function xi wij xj subject constraint wij wij xj nearest neighbor xi 
sum constraint rows ensures reconstruction weights invariant choice origin input space 
small regularizer weight decay added error function unique global minimum 
loss generality identify inputs 
xm landmarks 
ask question possible reconstruct approximately remaining inputs just landmarks weights wij 
sufficiently large unique reconstruction obtained minimizing eq 
respect xi rewrite reconstruction error function inputs form ij xi xj ij identity matrix 
useful partition matrix blocks distinguishing landmarks unknown inputs uu terms matrix solution minimum reconstruction error linear transforma tion eq 
im uu ul 
example minimum error reconstruction shown fig 

panels show inputs sampled swiss roll approximate reconstructions eq 
eq 
nearest neighbors landmarks 
intuitively imagine matrix ij eq 
defining sparse weighted graph connecting nearby inputs 
linear transformation reconstructing inputs landmarks analogous manner semi supervised algorithms graphs propagate information labeled unlabeled examples 
justify eq 
imagine data set unfolded way preserves distances angles nearby inputs 
noted previous weights wij minimize reconstruction error eq 
invariant translations rotations input nearest neighbors 
roughly speaking unfolding looks locally rotation plus translation weights wij reconstruct inputs xi neighbors reconstruct outputs yi theirs 
line reasoning yields eq 

suggests learn faithfully embed just landmarks lower dimensional space remainder inputs unfolded simple matrix multiplication 
embedding landmarks straightforward reformulate semidefinite program sdp kernel matrix kij yi yj section terms smaller matrix 
particular appealing factorization consider sdp maximize trace subject 
ij ij 
ij ii ij jj xi xj optimization nearly quite identical previous sdp substitution difference changed equality constraints eq 
inequalities 
sdp section guaranteed feasible constraints satisfied kij xi xj assuming inputs centered origin 
matrix factorization eq 
approximate relax distance constraints preserve feasibility 
changing equalities inequalities simplest possible relaxation trivial solution provides guarantee feasibility 
practice relaxation appear change solutions sdp significant way variance maximization inherent objective function tends saturate pairwise distance constraints enforced strict equalities 
summarize procedure unfolding inputs xi kernel matrix factorization eq 
follows compute reconstruction weights wij minimize error function eq 
ii choose landmarks compute linear transformation eq 
iii solve sdp landmark kernel matrix iv derive low dimensional embedding landmarks eigenvectors eigenvalues reconstruct outputs yi eq 

free parameters algorithm number nearest neighbors derive locally linear reconstructions number nearest neighbors generate distance constraints sdp number landmarks constrains rank kernel matrix 
follows refer algorithm landmark sde simply sde 
sde faster sde main optimization performed matrices computation time semidefinite programming depends matrix size number constraints 
apparent difficulty sde sde number constraints constraints sparse naive implementation sde slower sde 
difficulty practice solving semidefinite program sde explicitly monitoring small fraction original constraints 
start feed initial subset constraints inputs sampled swiss roll linear reconstruction nearest neighbors landmarks denoted black embedding sde distance angle constraints nearest neighbors computed minutes 
sdp solver consisting constraint centering constraint distance constraints landmarks nearest neighbors 
solution violates constraints added problem solved 
process repeated constraints satisfied 
note incremental scheme possible relaxation distance constraints equalities inequalities 
large scale training support vector machines constraints sde redundant simple heuristics prune constraints yield magnitude speedups 
note centering constraints sde enforced 
experimental results experiments performed matlab evaluate performance sde various data sets 
sdps solved csdp optimization toolbox 
particular concern speed accuracy sde relative earlier implementations sde 
data set shown top left panel fig 
consisted inputs sampled dimensional swiss roll 
panels fig 
show input reconstruction landmarks nearest neighbors sde constraining distances angles nearest neighbors 
computation took minutes mid range desktop computer 
table shows constraints word nearest neighbors may won men passengers soldiers officers iraq states israel china noriega drugs computers missiles equipment programs january july october august march germany canada africa arabia marks recession environment yen season afternoon california minnesota arizona florida georgia republican democratic strong conservative phone government pentagon airline army bush table selected words nearest neighbors order increasing distance nonlinear dimensionality reduction sde 
dimensional embedding dimensional bigram distributions computed sde minutes 
explicitly enforced sdp solver find feasible solution 
interestingly similarly faithful embeddings obtained shorter times landmarks input reconstructions cases considerably worse quality 
worth mentioning adding low variance gaussian noise inputs significant impact algorithm performance 
second data set created common words arpa north american business news corpus 
words represented discrete probability distribution words possibly follow 
distributions estimated maximum likelihood bigram model 
embedding high dimensional distributions performed sde minutes variance embedding revealed eigenvalue spectrum landmark kernel matrix essentially confined dimensions 
table shows selection words nearest neighbors low dimensional embedding 
despite massive dimensionality reduction semantically meaningful neighborhoods seen preserved 
third experiment performed color images teapot viewed different angles plane 
vectorized image dimensionality resulting bytes color information pixels 
previous shown sde represents angular mode variability data set perfect circle 
fig 
compares embeddings sde normal sde lle 
eigenvalue spectrum sde top error rate nearest neighbors classification test set usps handwritten digits 
error rate plotted dimensionality embeddings pca sde 
seen sde preserves neighborhood structure digits fairly dimensions 
bottom normalized eigenvalue spectra sde pca 
reveals dimensions appreciable variance 
similar sde revealing variance embedding concentrated dimensions 
results sde exactly reproduce results sde data set difference smaller increasing number landmarks expense computation time 
shown fig 
sde took seconds slower sde particular data set 
increase computation time simple explanations peculiar data set 
data set small sde incurs overhead setup negligible large data sets 
second data set images particular cyclic structure easily broken monitored constraints sampled evenly 
particular data set suited incremental scheme adding unenforced constraints sde large number sdp required resulting longer computation time sde 
see table 
final experiment performed entire data set usps handwritten digits 
inputs pixel grayscale images scanned digits 
table shows inequality constraints needed explicitly monitored sdp solver sde find feasible solution 
possible obtain embedding minutes earlier implementations sde handle problems relative speedup sde versus sde data sets different numbers examples landmarks 
speedups orders magnitude observed larger data sets 
small data sets sde faster sde 
size 
evaluate embeddings sde compared nearest neighbor classification error rates pca 
top plot fig 
shows classification error rate nearest neighbors training images classify test images versus dimensionality embeddings sde pca 
error rate sde drops rapidly dimensionality nearly matching error rate actual images dimensions 
contrast pca requires dimensions overtake sde 
bar plot bottom fig 
shows normalized eigenvalue spectra sde pca 
plot clear sde concentrates variance embedding fewer dimensions pca 
sde outperform sde 
shows speedup sde versus sde data sets 
surprisingly relative speedup grows proportion size data set 
small data sets generally unfolded faster sde larger data sets unfolded times faster sde 
larger data sets sde remains viable option 
developed faster algorithm manifold learning semidefinite programming 
aspects algorithm investigating including interplay number placement landmarks definition local neighborhoods quality comparison embeddings sde lle sde color images rotating teapot 
vectorized images dimension 
lle sde yield similar slightly irregular results sde 
normalized sde sde divided trace kernel matrices reveal variances embeddings concentrated dimensions lle reveal sort information 
data set constraints monitored time secs bigrams usps digits swiss roll table total number constraints versus number constraints explicitly monitored sdp solver sde data sets 
numbers inputs landmarks shown computation times 
speedup sde largely derived omitting redundant constraints 
resulting reconstructions embeddings 
initial results promising show manifold learning semidefinite programming scale larger data sets originally imagined earlier 
practical applications sde framework interesting way combines ideas different lines 
sde appeals symmetry heart lle sde 
linear reconstructions yield factorization kernel matrix eq 
reminiscent semi supervised algorithms propagating labeled information large graphs unlabeled examples 
somewhat different intuition computational gains sde similar obtained landmark methods isomap 
applied sde minutes data sets examples exist larger data sets algorithm remains impractical 
insights required 
related developed simple sample extension sde analogous similar extensions spectral methods 
algorithmic advances may emerge dual formulation maximum variance unfolding related problem computing fastest mixing markov chains graphs 
hopeful combination complementary approaches lead faster powerful algorithms manifold learning semidefinite programming 
acknowledgments grateful ali university pennsylvania discussions semidefinite programming anonymous reviewers useful comments 
belkin niyogi 
regularization semi supervised learning large graphs 
proceedings seventeenth annual conference computational learning theory colt pages banff canada 
belkin niyogi 
laplacian eigenmaps dimensionality reduction data representation 
neural computation 
bengio 
vincent 
extensions lle isomap mds eigenmaps spectral clustering 
thrun saul sch lkopf editors advances neural information processing systems cambridge ma 
mit press 
borchers 
csdp library semidefinite programming 
optimization methods software 
brand 
charting manifold 
becker thrun obermayer editors advances neural information processing systems pages cambridge ma 
mit press 
cristianini shawe taylor 
support vector machines 
cambridge university press cambridge uk 
de silva tenenbaum 
global versus local methods nonlinear dimensionality reduction 
becker thrun obermayer editors advances neural information processing systems pages cambridge ma 
mit press 
donoho grimes 
hessian eigenmaps locally linear embedding techniques highdimensional data 
proceedings national academy arts sciences 
ham lee mika sch lkopf 
kernel view dimensionality reduction manifolds 
proceedings international conference machine learning icml pages banff canada 
hull 
database handwritten text recognition research 
ieee transaction pattern analysis machine intelligence may 
platt 
fast embedding sparse similarity graphs 
thrun saul sch lkopf editors advances neural information processing systems cambridge ma 
mit press 
platt 
fastmap landmark mds nystr algorithms 
proceedings tenth international workshop artificial intelligence statistics wi january 
roweis saul 
nonlinear dimensionality reduction locally linear embedding 
science 
saul roweis 
think globally fit locally unsupervised learning low dimensional manifolds 
journal machine learning research 
sch lkopf smola 
ller 
nonlinear component analysis kernel eigenvalue problem 
neural computation 
smola kondor 
kernels regularization graphs 
proceedings sixteenth annual conference computational learning theory kernel workshop washington 
sun boyd xiao diaconis 
fastest mixing markov process graph connection maximum variance unfolding problem 
siam review submitted 
szummer jaakkola 
partially labeled classification markov random walks 
dietterich becker ghahramani editors advances neural information processing systems cambridge ma 
mit press 
tenenbaum de silva langford 
global geometric framework nonlinear dimensionality reduction 
science 
vandenberghe boyd 
semidefinite programming 
siam review march 
weinberger saul 
unsupervised learning image manifolds semidefinite programming 
proceedings ieee conference computer vision pattern recognition cvpr volume pages washington 
weinberger sha saul 
learning kernel matrix nonlinear dimensionality reduction 
proceedings international conference machine learning icml pages banff canada 
williams 
connection kernel pca metric multidimensional scaling 
leen dietterich tresp editors advances neural information processing systems pages cambridge ma 
mit press 
christopher williams matthias seeger 
nystr method speed kernel machines 
leen dietterich tresp editors neural information processing systems pages cambridge ma 
mit press 
zhang zha 
principal manifolds nonlinear dimensionality reduction local tangent space alignment 
siam journal scientific computing press 
zhou bousquet lai weston sch lkopf 
learning local global consistency 
thrun saul sch lkopf editors advances neural information processing systems pages cambridge ma 
mit press 
zhu ghahramani lafferty 
semisupervised learning gaussian fields harmonic functions 
proceedings twentieth international conference machine learning icml pages washington 
