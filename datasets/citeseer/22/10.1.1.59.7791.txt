bayesian hierarchical clustering katherine heller heller gatsby ucl ac uk zoubin ghahramani zoubin gatsby ucl ac uk gatsby computational neuroscience unit ucl london wc ar uk novel algorithm agglomerative hierarchical clustering evaluating marginal likelihoods probabilistic model 
algorithm advantages traditional distance agglomerative clustering algorithms 
defines probabilistic model data compute predictive distribution test point probability belonging existing clusters tree 
uses model criterion decide merging clusters ad hoc distance metric 
bayesian hypothesis testing decide merges advantageous output recommended depth tree 
algorithm interpreted novel fast bottom approximate inference method dirichlet process countably infinite mixture model dpm 
provides new lower bound marginal likelihood dpm summing exponentially clusterings data polynomial time 
describe procedures learning model hyperparameters computing predictive distribution extensions algorithm 
experimental results synthetic real world data sets demonstrate useful properties algorithm 

hierarchical clustering frequently methods unsupervised learning 
set data points output binary tree dendrogram leaves data points internal nodes represent nested clusters various sizes 
tree organizes clusters hierarchically hope hierarchy agrees intuitive appearing proceedings nd international conference machine learning bonn germany 
copyright author owner 
organization real world data 
hierarchical structures ubiquitous natural world 
example evolutionary tree living organisms consequently features organisms sequences homologous genes natural hierarchy 
hierarchical structures natural representation data generated evolutionary processes 
example internet newsgroups emails documents newswire organized increasingly broad topic domains 
traditional method hierarchically clustering data duda hart bottomup agglomerative algorithm 
starts data point assigned cluster iteratively merges closest clusters data belongs single cluster 
nearest pair clusters chosen distance measure euclidean distance cluster means distance nearest points 
limitations traditional hierarchical clustering algorithm 
algorithm provides guide choosing correct number clusters level prune tree 
difficult know distance metric choose especially structured data images sequences 
traditional algorithm define probabilistic model data hard ask clustering compare models predictions cluster new data existing hierarchy 
statistical inference overcome limitations 
previous uses probabilistic methods perform hierarchical clustering discussed section 
bayesian hierarchical clustering algorithm uses marginal likelihoods decide clusters merge avoid overfitting 
basically asks probability data potential merge generated mixture component compares exponentially hypotheses lower levels tree section 
generative model algorithm dirichlet process mixture model countably infinite mix ture model algorithm viewed fast bottom agglomerative way performing approximate inference dpm 
giving weight possible partitions data clusters intractable require sampling methods algorithm efficiently computes weight exponentially partitions consistent tree structure section 

algorithm bayesian hierarchical clustering algorithm similar traditional agglomerative clustering pass bottom method initializes data point cluster iteratively merges pairs clusters 
see main difference algorithm uses statistical hypothesis test choose clusters merge 

denote entire data set di set data points leaves subtree ti 
algorithm initialized trivial trees ti 
containing single data point di 
stage algorithm considers merging pairs existing trees 
example ti tj merged new tree tk associated set data dk di dj see 
considering merge hypotheses compared 
hypothesis denote data dk fact generated independently identically probabilistic model unknown parameters 
imagine probabilistic model multivariate gaussian parameters crucial emphasize different types data different probabilistic models may appropriate 
evaluate probability data hypothesis need specify prior parameters model hyperparameters 
ingredients compute probability data dk dk dk dk calculates probability data dk generated parameter values assuming model form 
natural model criterion measuring data fit cluster 
choose models conjugate priors normal inverse wishart priors normal continuous data dirichlet priors multinomial bayesian hierarchical clustering dk tk di ti 
schematic portion tree ti tj merged tk associated data sets di dj merged dk 
example tree data points 
clusterings tree consistent partitions data 
clustering partition 
discrete data integral tractable 
conjugate priors integrals simple functions sufficient statistics dk 
example case gaussians function sample mean covariance data dk 
alternative hypothesis data dk clusters 
summing exponentially possible ways dividing dk clusters intractable 
restrict clusterings partition data manner consistent subtrees ti tj compute sum efficiently recursion 
elaborate notion tree consistent partitions section 
probability data restricted alternative hypothesis simply product subtrees dk di ti dj tj probability data set tree di ti defined 
combining probability data ses weighted prior points def dk belong cluster obtain marginal probability data tree tk dk tk kp dk di ti dj tj equation defined recursively term considers hypothesis single cluster dk second term efficiently sums clusterings data dk consistent tree structure see 
section show equation derive approximation marginal likelihood dirichlet process mixture model fact provides new lower bound marginal likelihood 
important confuse marginal likelihood equation integrates parameters cluster marginal likelihood dpm integrates clusterings parameters 
show prior merged hypothesis computed bottom dpm 
posterior def probability merged hypothesis rk dk obtained bayes rule rk kp dk kp dk hk di ti dj tj bayesian hierarchical clustering quantity decide greedily trees merge determine merges final hierarchy structure justified 
general algorithm simple see 
input data 
model prior initialize number clusters di 
find pair di dj highest probability merged hypothesis rk kp dk dk tk merge dk di dj tk ti tj delete di dj output bayesian mixture model tree node mixture component tree cut points rk 
bayesian hierarchical clustering algorithm bayesian hierarchical clustering algorithm desirable properties absent traditional hierarchical clustering 
example allows define predictive distributions new data points decides merges advantageous suggests natural places cut tree statistical model comparison criterion rk customized different kinds data choosing appropriate models mixture components 

approximate inference dirichlet process mixture model algorithm approximate inference method dirichlet process mixture models dpm 
dirichlet process mixture models consider limit infinitely components finite mixture model 
allowing infinitely components possible realistically model kinds complicated distributions expect real problems 
briefly review starting finite mixture models 
consider finite mixture model components ci ci 
cluster indicator variable data point parameters multinomial distribution ci pj parameters jth component 

parameters component conjugate priors section multinomial parameters conjugate dirichlet prior 
data set 
marginal likelihood mixture model 
marginal likelihood re written 
cn dp standard dirichlet integral 
quantity defined limit 
number possible settings grows diverges number possible ways partitioning points remains finite roughly nn 
denote set possible partitioning data points re write rasmussen provides thorough analysis gaussian components markov chain monte carlo mcmc algorithm sampling partitionings interesting property probability new data point belonging cluster number points cluster blackwell macqueen controls probability new point creating new cluster 
point data set possible clustering different partition data denote placing brackets data point indices 

individual cluster 
nonempty subset data yielding possible clusters combined ways form clusterings partitions data set 
organize subset clusters tree 
combining clusters obtain tree consistent partitions data see 
summing possible partitions data mcmc algorithm computes sum exponentially tree consistent partitions particular tree built greedily bottom 
seen fast deterministic alternative mcmc approximations 
returning algorithm dpm concentration hyperparameter defines prior partitions nk data points dk value directly related expected number clusters prior merged hypothesis relative mass nk points belonging cluster versus partitions nk data points consistent tree structure 
computed bottom tree built 
initialize leaf di internal node dk nk nk dk 
algorithm computing prior merging right indexes right left subtree tk value computed right left child internal node lemma marginal likelihood dpm dk mv mv nv mv nk bayesian hierarchical clustering set possible partitionings dk mv number clusters partitioning number points cluster partitioning follows equation fractional term sum second product term dk explicit dependence dropped 
theorem quantity computed bayesian hierarchical clustering algorithm dk tk vt mv mv nv dk mv vt set tree consistent partitionings dk 
proof rewriting equation algorithm substitute obtain dk tk dk nk di ti dj tj dk proceed give proof induction 
base case leaf node second term equation drops subtrees 
nk dk yielding dk tk dk expect leaf node 
inductive step note term just trivial partition nk points single cluster 
inductive hypothesis di ti vt nv di similarly dj tj vti set tree consistent partitionings di dj 
combining terms obtain di ti dj tj dk dk vt vt mv mv nv dk mv dk set non trivial tree consistent dk 
trivial partition mv nk 
combining trivial non trivial terms get sum tree consistent partitions yielding result theorem 
completes proof 
way get result expand substitute algorithm 
corollary binary tree tk data points dk leaves lower bound marginal likelihood dpm dk nk dk tk dk proof proof corollary follows trivially multiplying dk tk ratio denominator dk denominator dk lemma nk fact tree consistent partitions subset partitions data 
proposition number tree consistent partitions exponential number data points balanced binary trees 
proof ti ci tree consistent partitions di tj cj tree consistent partitions dj tk ti tj merging tree consistent partitions dk di dj obtained combining partitions adding partition data dk cluster 
leaves ci 
balanced binary tree depth number tree consistent partitions grows number data points grows summary sums probabilities partitions weighted prior mass assigned partition dpm 
computational complexity constructing tree complexity computing marginal likelihood log complexity computing predictive distribution see section 

learning prediction learning hyperparameters 
setting hyperparameters root node tree approximates probability data particular hyperparameters 
model concentration parameter dpm hyperparameters probabilistic model defining component mixture 
root node marginal likelihood model comparison different settings hyperparameters 
fixed tree optimize hyperparameters gradients 
combines results computation ent dk tk dk subtrees tk gradients computed bottom tree built 
similarly dk tk depends turn depends di propagated subtrees 
allows construct em algorithm find best tree structure viterbi step optimize hyperparameters step 
experiments optimized hyperparameters simple line search gaussian components 
simple empirical approach set hyperparameters fitting single model data set 
details hyperparameter optimization tech report heller ghahramani 
predictive distribution 
tree probability new test point data computed recursing tree starting bayesian hierarchical clustering purity marginal likelihood 
log marginal likelihood evidence vs purity iterations hyperparameter optimization root node 
node represents cluster associated predictive distribution dk dk 
predictive distribution sums nodes weighted posterior probabilities dk def set nodes tree rk nk ri weight cluster nk set nodes path root node parent node expression derived rearranging sum tree consistent partitionings sum clusters tree noting cluster appear partitionings 
gaussian components conjugate priors results predictive distribution mixture multivariate distributions 
show examples results section 

results compared bayesian hierarchical clustering traditional hierarchical clustering average single complete linkage euclidean distance metric datasets real synthetic 
compared algorithm average linkage hierarchical clustering toy problems 
problems able compare different hierarchies generated algorithms visualize clusterings predictive distributions 
real datasets random examples class classes attributes glass examples classes attributes datasets uci repository cedar buffalo digits random examples class classes attributes cmu newsgroups dataset examples classes rec sport baseball bayesian hierarchical clustering data set single linkage complete linkage average linkage bhc synthetic newsgroups digits digits glass table 
purity scores kinds traditional agglomerative clustering bayesian hierarchical clustering 
mean scores fold cross validation standard errors shown 
rec sport hockey rec autos sci space attributes 
synthetic data generated mixture gaussians examples classes attributes 
synthetic glass toy datasets modeled gaussians digits newsgroup datasets binarized modeled 
binarized digits dataset thresholding greyscale value dataset attribute value zero non zero 
ran algorithms digits digits 
newsgroup dataset rainbow mccallum list words appearing fewer times ignored 
dataset binarized word presence absence document 
classification datasets labels data points known computed measure dendrogram clusters known labels called dendrogram purity 
marginal likelihood tree structure data highly correlated purity 
iterations different hyperparameters correlation see 
table shows results datasets 
datasets glass bhc highest purity trees 
glass gaussian assumption may poor 
highlights importance model choice 
similarly classical distance hierarchical clustering methods poor choice distance metric may result poor clusterings 
advantage bhc algorithm fully addressed purity scores tends create hierarchies structure particularly high levels 
compares top levels tree leaves 

cn known discrete class labels data points leaves 
pick leaf uniformly random pick leaf uniformly class cj 
find smallest subtree containing measure fraction leaves subtree class 
expected value fraction dendrogram purity computed exactly bottom recursion dendrogram 
purity iff leaves class contained pure subtree 
baseball pitch hit data game team play hockey round car dealer drive car space nasa space nasa orbit quebec jet boston data pitcher boston ball car baseball engine car player space vehicle dealer driver team game hockey 
top level structure bhc left vs average linkage hc newsgroup dataset 
words shown node highest mutual information cluster documents node versus sibling occur higher frequency cluster 
number documents cluster 
merges newsgroups hierarchy examples words highest information gain bhc 
continuing look lower levels improve 
perform similarly 
full dendrograms dataset dendrograms digits dataset available tech report heller ghahramani 

related related inspired previous probabilistic approaches clustering briefly review 
stolcke omohundro described algorithm agglomerative model merging marginal likelihoods context hidden markov model structure induction williams neal describe gaussian diffusion hierarchical generative models respectively inference done mcmc methods 
similarly kemp 
hierarchical generative model data mutation process 
banfield raftery approximate considerable amount decision tree bayesian tree structures classification regression closely related 
method likelihood ratio test statistic compute marginal likelihood clusters agglomerative algorithm 
vaithyanathan dom perform hierarchical clustering multinomial data consisting vector features 
clusters specified terms subset features common distributions 
segal 
probabilistic abstraction hierarchies pah learn hierarchical model node contains probabilistic model hierarchy favors placing similar models neighboring nodes tree measured distance function models 
ramoni 
agglomerative algorithm merging time series greedily maximizing marginal likelihood 
friedman proposed greedy agglomerative algorithm marginal likelihood simultaneously clusters rows columns gene expression data 
algorithm different algorithms ways 
williams neal kemp fact hierarchical generative model data hierarchical way organizing nested clusters 
second algorithm derived dirichlet process mixtures 
third hypothesis test core algorithm tests single merged hypothesis alternative exponentially clusterings data vs clusters stage 
lastly algorithm iterative method em require sampling mcmc significantly faster algorithms 

discussion novel algorithm bayesian hierarchical clustering dirichlet process mixtures 
algorithm advantages traditional approaches highlighted 
prediction hyperparameter optimization procedures shown algorithm provides competitive clusterings real world data measured purity respect known labels 
algorithm seen extremely fast alternative mcmc inference 
limitations algorithm include inherent greediness computational complexity quadratic number data points lack incorporation tree uncertainty 
plan try bhc complex component models realistic data bayesian hierarchical clustering require approximations component marginal likelihoods 
plan extend bhc systematically incorporate hyperparameter optimization improve running time log exploiting randomized version algorithm 
need compare novel fast inference algorithm inference algorithms mcmc rasmussen ep minka ghahramani variational bayes blei jordan 
hope explore idea computing alternative tree structures order create manipulable tradeoff computation time tightness lower bound 
exciting avenues area 
david mackay members gatsby unit members cmu useful comments 
project partially supported eu pascal network excellence zg partially supported cmu darpa calo project 
banfield raftery 

model gaussian non gaussian clustering 
biometrics 
blackwell macqueen 
ferguson distributions polya urn schemes 
ann 
stats 
blei jordan 

variational methods dirichlet process mixture models technical report 
uc berkeley 
duda hart 

pattern classification scene analysis 
wiley 
friedman 

probabilistic agglomerative clustering gene expression profiles technical report 
university 
heller ghahramani 

bayesian hierarchical clustering technical report 
gatsby computational neuroscience unit 
kemp griffiths tenenbaum 

semi supervised learning trees 
nips 
mccallum 

bow toolkit statistical language modeling text retrieval classification clustering 
www cs cmu edu mccallum bow 
minka ghahramani 

expectation propagation infinite mixtures 
nips workshop nonparametric bayesian methods infinite models 
neal 

density modeling clustering dirichlet diffusion trees 
bayesian statistics pp 

ramoni sebastiani 

cluster analysis gene expression dynamics 
proc nat acad sci 
rasmussen 

infinite gaussian mixture model 
nips pp 

segal koller ormoneit 

probabilistic abstraction hierarchies 
nips 
stolcke omohundro 

hidden markov model induction bayesian model merging 
nips pp 

vaithyanathan dom 

model hierarchical clustering 
uai 
williams 

mcmc approach hierarchical mixture modelling 
nips 
average linkage hierarchical clustering bayesian hierarchical clustering average linkage hierarchical clustering average linkage hierarchical clustering 
toy examples column 
row shows original data sets 
second row gives dendrograms resulting average linkage hierarchical clustering number axis corresponds data point displayed plots fourth row 
third row gives dendrograms resulting bayesian hierarchical clustering red dashed lines merges algorithm prefers priors 
numbers branches log odds merging log rk 
fourth row shows clusterings algorithm gaussian components tree cut red rk dashed branches rk 
row shows predictive densities resulting algorithm 
bhc algorithm behaves structurally sensibly traditional algorithm 
note point middle column clustered green class substantially closer points red class 
column traditional algorithm prefers merge cluster cluster bhc cluster cluster 
column bhc algorithm clusters points upper lower horizontal parallels traditional algorithm merges subsets points example cluster cluster 

