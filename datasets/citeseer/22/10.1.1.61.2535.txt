architectures efficient implementation particle filters dissertation graduate school partial fulfillment requirements degree doctor philosophy electrical engineering stony brook university august stony brook university graduate school dissertation committee candidate ph 
degree recommend acceptance dissertation djuri advisor dissertation professor department electrical computer engineering john murray chairperson defense associate professor department electrical computer engineering hong assistant professor department electrical computer engineering kenny qian ye assistant professor department applied mathematics statistics dissertation accepted graduate school ii graduate school dissertation architectures efficient implementation particle filters doctor philosophy electrical engineering stony brook university particle filters sequential monte carlo methods numerous problems time varying signals real time objective estimate various unknowns signal detect events described signals 
standard solutions problems applications kalman filters extended kalman filters 
situations problems nonlinear noise distorts signals non gaussian kalman filters provide solution may far optimal 
particle filters intriguing alternative kalman filters due excellent performance difficult problems including communications signal processing navigation computer vision 
particle filters focus wide research immense literature theory 
works recognize complexity computational intensity filters effort directed implementation filters hardware 
objective dissertation develop design build efficient hardware particle filters bring closer practical applications 
fact particle filters outperform traditional filtering methods complex practical scenarios coupled challenges related decreasing computational complexity improving real time performance 
main goals dissertation develop modify particle filter algorithms develop physically feasible hardware architectures allow improving processing speed particle filters 
issues tackled include reduction computational complexity improving scalability parallel implementation reducing memory requirements 
resulted development hardware prototype particle filter 
speed improvement comparison implementation state art digital signal processors times 
iii contents list figures 
vi list tables 
ix acknowledgments 
motivation 
challenges contributions 
organization dissertation 
published 
background theory particle filtering background 
description dynamic signals 
filtering predictive smoothing densities 
particle filters 
generation particles 
importance step 
resampling 
progress 
particle filters bearings tracking problem 
gaussian particle filtering 
gpf bearings tracking 
characterization particle filtering algorithms architectures vlsi signal processing 
joint algorithm architecture development 
types signal processing architectures 
basic terminology 
algorithm modifications 
sirfs functional view 
complexity 
concurrency operations 
architectures 
temporarily concurrent architecture 
parallel architecture 
memory schemes resampling 
iv algorithms architectures particle filters new resampling algorithms 
residual systematic resampling algorithm 
deterministic resampling 
particle filtering performance complexity 
residual resampling fixed point 
proposed resampling scheme 
logic structure 
architectures sirfs rsr algorithm 
adjusting rsr algorithm hardware implementation 
logic structure 
architectures sirfs bearings tracking 
fpga implementation results 
algorithms architectures distributed particle filters centralized resampling 
distributed rpa 
distributed rna 
effects resampling obtained estimates 
performance analysis 
particle filter architectures distributed resampling 
distributed rpa architectures 
distributed rna architectures 
space exploration distributed particle filter rna local exchange 
final remarks 
architectures gaussian particle filters algorithmic modifications complexity characterization 
temporal concurrency 
spatial concurrency 
computational complexity characteristic 
implementation issues 
sequential processing 
concurrent processing 
architectures resource requirements 
comparisons tradeoffs sirfs gpfs 
energy speed constraints 
area requirements fpga 
summary 
extensions 
bibliography list figures block diagram parallel particle filter 
systematic resampling example particles 
illustration tracking problem 
functional view sirfs 
timing diagram sirf 
architecture distributed particle filter cu pes 
speedup versus number pes distributed sirf 
spatial implementation sirf assumed 
average maximum number particles exchanged network particles 
types memory usages indexes positions replicated particles indexes replication factors indexes arranged positions replication factors 
residual systematic resampling example particles 
resampling functions partial resampling algorithms pr pr pr 
opr method combined pr method final computation weights replication factors 
performance pr algorithm different threshold values applied joint detection estimation problem wireless communications 
comparison pr pr opr algorithms systematic resampling applied joint detection estimation problem wireless communications 
number times track lost pr pr sr applied bearings tracking problem 
timing sirf rsr pr methods opr method 
timing sirfs operations overlapped share hardware resources 
comparison exact resampling resampling tagging 
particles ordered weights particle largest weight 
logic diagram illustrates structure proposed resampling scheme 
assumed particles weights provided prior resampling 
resampled particles stored separate memory 
architecture sirfs rsr algorithm 
vi architecture rsr algorithm combined particle allocation 
architecture memory related operations sampling step 
architecture sampling unit bearings tracking problem 
architecture weight computation step bearings tracking problem 
implementation exponential function xilinx virtex ii pro fpga 
sequence operations performed th pe cu centralized resampling rpa 
direction communication data sent 
abbreviations sample importance resampling pa particle allocation 
example particle exchange rpa algorithm 
example particle exchange rna algorithms regrouping adaptive regrouping local exchange 
sum weights group 
routing rna regrouping andd andd 
percentage divergent tracks mse versus number particles different levels parallelism 
case pes rna local exchange applied 
architecture sirf distributed rpa pes 
cu implemented support pipelining particle routing sampling steps 
timing diagrams sirf distributed rpa 
communication interconnection network shown shortage particles 
architectures particle filters pes support rna algorithms support rna adaptive regrouping 
execution time memory requirements versus number pes rna local exchange sirf particles 
parallel gpf model pes 
sampling period gpfs sirfs versus number particles 
filters implemented analog dsp adc 
dataflow parallel gpf 
timing diagram gpf 
minimum sampling period versus number pes parallel gpfs sirfs 
spatial implementation particle filters assumed 
block diagram gpf 
block diagram generation conditioning particles 
block diagram mean covariance computation step 
block diagram time multiplexed central unit gpfs 
energy versus number particles sirfs gpfs single pe different sampling rates 
energy versus number particles sirfs gpfs implemented pes different maximum sampling rates 
vii percentage number slices multipliers block pes estimated xilinx virtex ii pro chips 
area number slices number block rams versus number particles 
area evaluated different sampling frequencies necessary number pes particle filters satisfy sampling frequency requirements 
viii list tables comparison number operations different resampling algorithms 
memory capacity different resampling algorithms 
pattern memory access different resampling algorithms 
resampling quantization simple truncation 
rounding truncation scheme tags 
resampling proposed scheme 
resource utilization sirf implementation bearings tracking problem xilinx xc vp device 
comparison rpa rna steps 
number memory bits slices block multipliers distributed particle filter implementation rna local exchange 
virtex ii pro chips fitted particle filter parameters listed 
star shows parameter determined choosing chip 
comparison number operations memory requirements sirfs gpfs pe 
sampling period particle filter analyzed 
operations particle generation weight calculation steps considered 
effect model increase algorithmic architectural parameters 
comparison parameters particle filtering algorithms 
random number generators denoted rng mul respectively 
number multipliers calculated bearings tracking problem values generic particle filter 
ix acknowledgments advisor professor djuri supporting years giving lot freedom excellent guidance exploring different aspects problem 
advisor professor hong motivation guidance issues related vlsi design 
professors john murray kenny ye time review dissertation 
friends colleagues stony brook university pleasure working years 
include worked closely project monica fernandez de cock huang jae chan lim mahesh members cosine lab 
stephen hiring intern symbol technologies working years 
members bluetooth rfid teams symbol technologies including gary schneider raj mike adam levine frank sean connolly 
financial support research came nsf award ccr gratefully acknowledged 
chapter motivation field digital signal processing driven advances signal processing algorithms large scale integrated vlsi technologies 
time dsp applications impose numerous challenges implementation dsp systems 
particle filters represent existing important challenges implementation 
applications nowadays particle filters considerable improvements performance meet stringent requirements real time processing 
applications include wireless communications robotics navigation tracking systems sequential adaptive signal processing needed 
common problem applications requirement dynamic signal parameters states estimated detected real time 
example bearings tracking critical estimate position velocity object dimensional plane noisy measurements angular positions object 
wireless communications signals received joint operation estimation detection estimate time varying channel detect transmitted symbols 
applications particle filters outperform traditional methods 
computationally intensive main drawback 
literature particle filtering theory 
best knowledge serious efforts designing hardware particle filters 
dissertation intention fill gap 
main design objective develop hardware architectures support high speed particle filtering 
practical applications large number particles needs computing estimates desired states 
number particles increases speed particle filter seriously affected 
algorithms applied bearings tracking problem 
particles single state art digital signal processor dsp obtained speeds khz 
clearly speed seriously limit range applications particle filter real time processing 
meeting speed requirements real time applications necessary high throughput designs ability process large number particles time 
challenges contributions particle filters perform basic operations generation new particles sampling space unobserved states computation particle weights probability masses associated particles resampling process removing particles small weights replacing particles large weights 
steps basis commonly type particle filters called sample importance resampling filters sirf 
particle generation weight computation computationally intensive steps 
particle filtering speed increased algorithmic modifications architecture development 
main challenges speed increase algorithmic level include reducing number operations exploiting operational concurrency particle generation weight computation steps 
resampling computationally intensive increase speed need modifications algorithm allow overlapping operations particle generation 
high speed requirements impose mapping particle filtering operations hardware blocks blocks perform operations concurrently 
hardware platforms support concurrent processing fpga asic 
chosen fpga platform standard choice prototyping phase flexibility 
challenges architectural levels include choice implementation complex nonlinear operators dealing possible multidimensional structures hardware choosing proper level pipelining hardware block 
result maximum achievable speed proportional number particles tclk clock period 
increase speed target parallel implementation multiple processing elements pes pe processes fraction total number particles single central unit cu controls operations pes shown 
specifically generation new particles computation weights processed parallel pes 
cu performs resampling coordinates operations including synchronization estimation required unknowns 
interesting feature particle filters particular time instant particle generation weight computation operations different particles independent allows concurrent processing particles different pes 
filter affected resampling step 
resampling disadvantages parallel hardware implementation viewpoint sampling period memory requirements increased data exchange implementations multiple pes major bottleneck parallel design complexity cu greatly increased 
effects resampling performance complexity reduced modified resampling algorithms architectures allow local deterministic data exchange patterns 
parallel hardware implementations resampling bottleneck due necessity exchanging large number particles interconnection network 
type particle filters called gaussian particle filter gpf require resampling 
modified gpf algorithm way processing speed twice higher speed sirf algorithm main advantage gpf 
data exchange gpfs negligible significantly lower sirfs deterministic 
input sample importance pe sample importance pe resampling synchronization output calculation central unit sample importance sample importance block diagram parallel particle filter 
gpf better candidate parallel implementation 
attractive feature gpf opposed sirf implemented storing particles successive time instants 
eliminates need memories hardware provides freedom large number particles processing constrained size memory 
organization dissertation document organized chapters 
chapter motivation contributions results described 
chapter short background theory particle filters 
dissertation performances particle filters estimated particle filter prototype built bearings tracking applications 
bearings tracking model introduced pseudocode sirf gpf bearings tracking 
basic terminology vlsi signal processing architectures chapter 
particle filter algorithm analyzed standpoint concurrency operations parallelism 
temporally concurrent architectures non parallel parallel particle filters proposed 
new resampling algorithms suitable hardware implementation proposed chapter 
new algorithms reduce complexity hardware dsp realization addressing common issues decreasing number operations number memories memory access 
algorithms allow higher processing speed overlapping time resampling step particle filtering steps 
resampling dependent particular application analysis appropriate types particle filters resampling 
way dealing fixed point pe pe output residual resampling algorithm corresponding architecture 
hardware architectures sirfs results fpga implementation sirf bearings tracking problem 
chapter propose resampling algorithms architectures parallel implementation sirfs 
proposed algorithms improve scalability filter architectures affected resampling process 
main advantages new resampling algorithms communication interconnection network reduced deterministic results simpler network structure increased sampling frequency 
architectural part analysis area speed sirf implementation estimated different number particles different level parallelism fpga implementation 
chapter analyze algorithmic architectural characteristics gpfs 
gpf algorithm modified way need storing particles memories successive recursions 
analyzed gpf bearings tracking problem results compared results sirf terms computational complexity potential throughput hardware energy 
current state research directions summarized chapter 
published dissertation previously published material finite precision effect performance complexity particle filters bearing tracking hong djuri ieee asilomar conference signals systems computers 
section efficient fixed point implementation residual systematic resampling scheme high speed particle filters hong andp djuri ieee signal processing letters 
section new resampling algorithms particle filters djuri hong icassp 
section resampling algorithms particle filters suitable parallel vlsi implementation djuri hong ciss 
section resampling algorithms particle filters computational complexity perspective djuri hong eurasip journal applied signal processing 
section algorithmic modification particle filters hardware implementation djuri hong eusipco 
chapter design complexity comparison method loop signal processing algorithms particle filters hong djuri iscas 
section architectures memory schemes sampling resampling particle filters hong djuri dsp spe workshop 
section dissertation contains submitted papers design study practical physical implementation gaussian particle filters djuri hong submitted ieee transactions circuits systems 
chapter design implementation flexible resampling mechanism high speed parallel particle filters hong chin djuri submitted journal vlsi signal processings 
section resampling algorithms architectures distributed particle filters djuri hong submitted ieee transactions signal processing 
chapter chapter background theory particle filtering background practice commonly devices sequential signal processing apply algorithms kalman filter 
addressed problems linear unknowns noise additive gaussian kalman filter optimal solution 
cases nonlinear signal models non gaussian noise uses approximations popular choice extended kalman filter 
situations filter poor performance large biases divergence lack robustness 
alternative approaches sequential processing nonlinear signals gaussian sum filtering approximations moments densities unscented kalman related filters methods relevant densities evaluated deterministic grids approaches exploit markov chain monte carlo mcmc sampling including gibbs sampler metropolis hastings scheme :10.1.1.48.6455
attention group methods known sequential importance sampling procedures 
known particle filtering bootstrap filtering fittest condensation algorithms interacting particle approximations :10.1.1.126.7850
dissertation refer particle filtering methods 
derivation main principle recursive generation random measures approximate distributions unknowns 
random measures composed particles samples drawn relevant distributions importance weights particles 
random measures allow computation sorts estimates unknowns including minimum mean square error mmse maximum posteriori map estimates 
new observations available particles weights propagated exploiting bayes theorem concept sequential importance sampling :10.1.1.110.383
description dynamic signals particle filters non linear problems interest tracking detection dynamic signals 
signals described system equations model evolution time equations usually form xn fn xn un zn gn xn vn discrete time index xn signal vector interest zn vector observations 
symbols un vn noise vectors fn signal transition function gn measurement function 
analytical forms fn assumed known 
densities un vn parametric known parameters may unknown un vn independent 
objectives estimate recursively signal xn observations zn 
filtering predictive smoothing densities densities play critical role sequential signal processing known filtering density xn predictive density xn smoothing density xn sequential signal processing apply filtering prediction smoothing respectively carry information unknowns 
recursive expressions densities provide necessary operations filtering smoothing prediction implemented 
filtering write xn zn xn xn xn xn xn dxn zn xn xn xn xn dxn dxn see high dimensional integrations needed recursion filtering density time xn filtering density time xn 
situation similar predictive density recursive relationship direct xn smoothing density xn xn xn xn xn dxn xn xn xn dxn 
xn obviously recursion smoothing carried backwards xn obtained xn 
convenience assumed fixed parameters model known proceed assumption rest dissertation 
equations integration key operation closed form solutions possible small number situations 
important case linear model additive gaussian noise solution known kalman filter 
models nonlinear noise non gaussian resorts approximated solutions example obtained extended kalman filter sum filters 
particle filters particle filters base operation representing relevant densities random measures composed particles weights compute integrals monte carlo methods 
specifically time instant random measure defined th particle signal time th trajectory signal weight th particle trajectory time instant particles obtained observations trajectories form drawn density approximate density 
example estimate needed function estimate easily computed 
implementation particle filters important operations 
generation particles sample step 
computation particle weights importance step 
resampling 
steps form particle filter called sequential importance sampling sis filter 
filter performs operations called sample importance resampling filter sirf 
generation particles performed drawing importance density function xn 
choose importance function form generation particles xk compute weights particles recursively 
particles drawn xn 
importance density xn xn plays pivotal role design particle filters generates particles represent desired density 
drawn particles regions signal space density negligible values estimates obtained particles weights poor subsequent tracking signal diverge 
contrast particles regions probability mass significant particle filter improved performance 
role importance density understood literature various strate gies proposed design 
argue xn zn optimal importance function 
drawbacks difficult sampling updating particle weights requires integrations 
suboptimal functions proposed local linearizations gaussian approximations unscented transform 
different approach introduced samples obtained step procedure proposed filter known auxiliary particle filter 
method drawing particles resampling carried continuous approximation posterior density xn 
importance step importance step consists steps computation weights normalization 
step weights evaluated proportionality constant subsequently normalized 
importance function form weights updated normalization carried resampling zn 

important problem particle filters weights particles degenerate 
words time progresses weights large remaining decrease value point negligible 
idea resampling remove trajectories small weights focus trajectories dominating 
resampling introduced proposed sis various works 
detailed theoretical discussion resampling sis context 
standard algorithms resampling different variants stratified sampling residual resampling rr branching corrections systematic resampling sr resampling methods rejection control 
formally basic random resampling algorithm performed follows 
drawn independently probability proportional wn new weights associated samples ai 
return new random measure represents indexes memory particles stored result resampling 
representation particle filter algorithm provides certain level generality 
example sirf stratified resampling implemented choosing uniform resampling function corresponds case resampling 
auxiliary sample importance resampling filter zn xn xn implemented setting mean mode value associated density xn 
resampling algorithms instance rr deal array replication factors array indexes replication factors show times particle replicated result resampling 
resampling sampled support defined particle resampling important particle filtering prevents particle filter weight degeneracy 
hand frequent resampling may lead particle attrition degradation support particle filter 
loss diversity occurs resulting random measure contains copies particle 
resampling improves estimation states concentrating particles domains higher posterior probability 
reduces accuracy current estimate increasing variance estimate resampling 
resampling applied caution 
common technique assessing need resampling calculates effective sample size defined variation weights 
simple way reduce average number resampling operations perform resampling time instants 
commonly metric deciding resample effective sample size number particles represent distribution associated weights equal 
estimators samples size proposed 
direct approach resampling complexity order log number particles 
complexity reduced ordered uniform variates obtained 
review systematic resampling residual resampling algorithms section resampling algorithms reviewed 
note consider rejection control algorithms suitable high speed implementations 
time resampling determined execution time random variable 
words algorithm requires random number generation number random draws predicted variable due random rejections 
sr algorithm performs resampling way basic random resampling algorithm exception 
drawing independently number resampled particles uses uniform random number graphically illustrates sr methods case particles weights table 
sr calculates cumulative sum weights compares updated uniform number uniform number generated drawing uniform distribution updated number replications particle determined number times updated uniform number range 
particle belong range particle replicated twice shown arrows correspond particle 
particles replicated 
particle discarded appears range 
particle systematic resampling example particles 
purpose generation array replication factors time instant sr input array weights input output number particles respectively 
method sr generate random number initializing cumulative sum weights main loop replication factor counter updating cumulative sum weights resampling loop storing replication factors pseudocode systematic resampling sr algorithm 
shown pseudocode 
input number particles number particles generated resampling array normalized weights importance step 
output array replication factors shows times particle replicated 
important observe sr implemented loops 
rr algorithm shown pseudocode 
meaning pseudocode 
rr composed steps 
step number replications particles calculated 
method guarantee number resampled particles residual number particles computed 
residual weights computed 
second step requires resampling residuals produces final particles 
pseudocode step performed sr input output particles 
sr algorithm applied residual weights obtained replication factors produced steps summed 
normalized new weights purpose generation array replication factors input time instant rr 
array weights method input output number particles respectively 
rr number particles left resampling 
step number replications computed 
residues weights 
updating number particles left resampling 
second step processing residuals 
normalization residual weights 
wr wr rr sr wr sr residual weights 
updating replication factors 
pseudocode residual resampling rr algorithm 
best case terms speed execution rr algorithm occurs example integer 
case step iterations 
worst case rr arises 
example particle weight range remaining particles weights step requires resampling residual particles 
progress advances particle filtering improvements various methods include design importance functions resampling strategies decreased computational complexity development suboptimal algorithms reduced computational complexity results convergence particle filters 
new contributions represent novelties applications particle filters 
include target tracking navigation blind deconvolution digital communication channels joint channel estimation detection rayleigh fading channels digital enhancement speech audio signals time varying spectrum estimation computer vision portfolio allocation sequential estimation signals model uncertainty 
particle filters bearings tracking problem dissertation particle filters applied bearings tracking problem illustrated positions object time instants shown 
measurements taken sensor track object bearings angles zn respect sensor fixed intervals 
range object distance sensor measured 
unknown states interested estimate position velocity tracked object cartesian coordinate system xn xn yn 
rn zn zn trajectory illustration tracking problem 
object moves plane state model xn xn un xn xn yn un denote cartesian coordinates target vx vy denote velocities directions respectively 
system noise zero mean gaussian white noise un qi identity matrix 
initial state describes targets initial position velocity 
prior initial state needs specified model assume 
measurements consist true bearing target corrupted gaussian error term 
measurement equation written zn tan yn xn vn measurement noise zero mean gaussian white noise vn 
performance analysis sirfs applied bearings tracking problem 
sirf algorithm bearings tracking utilizes usual steps sampling importance resampling output calculation shown pseudocode 
choose prior importance function allows simplest implementation 
purpose obtain estimates position velocity xn yn 
input observation zn particles 
method 
sampling step draw sample xn xn obtain xn xn xn xn xn yn yn yn yn yn 
importance step calculate weights zn normalize weights zn atan form 
resample particles obtain new particles weights mm 
calculate outputs xn xn yn yn pseudocode sirfs applied bearings tracking problem 
gaussian particle filtering seen sirfs operate propagating filtering predictive densities recursively time 
gpfs hand operate approximating desired densities gaussians 
mean variance densities propagated recursively time 
brief gpfs class gaussian filters monte carlo particle filter methods employed obtain estimates mean covariance relevant densities estimates recursively updated time 
propagation moments particle set significantly simplify parallel implementation gpf 
approximation filtering predictive densities unimodal gaussian distributions restricts application gpfs broad class models approximation valid 
gpf considerably simplified prior density importance function 
means xn xn xn xn 
gpf operations shown pseudocode 
step conditioning particles drawn gaussian density mean vector covariance matrix computed previous particle filter recursion 
conditioning particles sample step particles drawn xn xn 
weights computed normalized way sirfs 
particles weights compute mean vector covariance matrix 
purpose obtain estimates states input observation zn previous estimates setup method mean covariance prior information 

gpf time update algorithm 
draw conditioning particles xn obtain mm 
generate particles drawing samples xn xn obtain gpf measurement update algorithm 
calculate weights normalize weights zn 

estimate mean covariance filtering distribution pseudocode gpf algorithm prior density importance function 
gpf bearings tracking operations gpfs bearings tracking problem shown pseudocode 
conditioning particles drawn form gaussian distribution parameters vector triangular matrix 
step requires generation gaussian random numbers particle filtering iteration 
independent gaussian random numbers zero mean unit variance labeled 
order compute random numbers proper variance necessary find matrix cn cn particle generation weight computation steps sirfs state observation equations bearings tracking model 
output estimate gpf mean position velocity 
mean coefficients covariance matrix computed particle filtering recursion 
purpose obtain estimates position velocity xn yn 
input observation zn previous estimates cn cn cholesky decomposed covariance matrix method 
draw conditioning particles xn xn vx yn yn vy 
generate particles xn xn xn xn xn yn yn yn yn yn 
calculate weights normalize weights 
estimate mean covariance zn atan form form pseudocode gpf applied bearings tracking problem 
chapter characterization particle filtering algorithms architectures chapter architectural algorithmic properties sirfs analyzed 
chapter starts brief field vlsi signal processing 
modifications sequential sirfs suitable hardware implementation 
modifications exploiting operational concurrency 
parallel algorithms architectures sirfs shown 
savings size memories number memory accesses considered different addressing schemes 
vlsi signal processing joint algorithm architecture development new applications possible nowadays reducing complexity signal processing algorithms despite remarkable improvements vlsi technologies 
technology evolves entirely new domains application open 
technology improvements allowed monte carlo sampling techniques complex simulations applying techniques slower real time systems 
dissertation show high speed implementation sequential importance sampling algorithms feasible current technology 
results published apparent dramatic power reduction speed improvements stem optimization highest level design hierarchy 
particular case studies indicate high level decisions regarding selection optimization algorithms architectures improve design performance metrics effectively gate circuit level optimizations 
suggests estimation architectural parameters needed facilitate rapid high level design exploration especially goal achieve area efficient high speed circuit implementation 
important aspects field vlsi signal processing focusing joint study algorithms architectures custom implementation digital signal processing systems 
main design goal high speed extremely important algorithms match architectures 
matching algorithms architectures number operations decreased overhead caused memory access data communication reduced 
reducing overhead exploiting concurrency exists algorithm hardware significant increase speed achieved 
new functionality takes advantage mathematical approaches fast algorithms known application potentially benefit hardware performance improvements algorithm efficiency breakthroughs 
dissertation benefit derived joint algorithm architecture development particle filters 
inline current trend hardware intensive signal processing implementations 
particle filters complex algorithms main goal dissertation increase speed particle filters 
focusing speed improvements hope particle filters appropriate real time signal processing applications 
speed increased gradual modification algorithms developing architectures algorithms 
dissertation propose set modifications particle filter algorithm standpoint hardware implementation set corresponding architectures 
types signal processing architectures main architectures signal processing programmable dsp application domain specific processors adsp application specific processors asp 
dsp processors dsp cores low speed high flexibility necessary 
designed execute fixed limited set algorithms restricted number adjustable parameters 
higher speed achieved designed space narrowed 
optimized execute single algorithm 
result efficient implementation expense flexibility 
change specifications redesign required 
system level point view expect particle filter subsystem multifunctional system radar communication system 
functionality algorithm change system speed main design goal design particle filter fixed function subsystem 
dissertation asp architectures single chip implementation particle filters considered 
basic terminology latency algorithm time takes generate output value corresponding input value 
throughput defined reciprocal time difference successive outputs 
define execution time particle filters time necessary process observation particle filter 
execution time corresponds sampling period 
minimum execution time defined minimum sampling period achieved particle filters unrestricted hardware resources 
minimum sampling period algorithm defined recursive parts 
nonrecursive cited parts usually limit sample rate 
concurrency metrics capture ability resource accessed parallel 
studies determine amount available concurrency conducted results influenced architecture compiler design 
dissertation concurrency operations spatial concurrency considered 
concurrency operations type temporal concurrency quantifies expected number operations simultaneously executed 
referred functional parallelism 
main technique chaining 
chaining possible pipeline arithmetic operations functional units 
output functional unit input functional unit 
chaining done loop fusion algorithm level 
loop fusion allows combining loops executed number times indices 
order obtain best performances parallel system application partitioned set tasks executed concurrently spatial concurrency 
evaluating ability making algorithm parallel degree parallelism data dependency considered 
degree parallelism measure numbers threads computation carried simultaneously 
data dependencies result precedence constraints operations 
data dependencies studied temporal spatial locality 
concept locality heavily studied decades 
temporal locality described tendency program reuse data instructions 
spatial locality tendency program data instructions neighboring 
topology metrics characterize relative arrangement operations regard functionality 
speed efficiency metrics performance evaluation parallel systems 
speed defined ratio execution time best possible serial algorithm single processor execution time chosen algorithm parallel system processors 
efficiency defined ratio speed number processors 
main requirement increase speed particle filters mapping algorithm architecture performed cases 
means particle filtering operation hardware block 
way highest speed achieved expense hardware resources 
hand functional multiplexing significant increase performance mapping 
functions multiplexed hardware block execute different operations different time instants 
algorithm modifications sirfs functional view functional view sirf shown 
input sample observation zn sirfs sequentially perform particle generation weight computation normalization resampling output calculation 
operations critical path means optimization techniques speed applied steps 
particle filter iterative algorithm means new particle generation step start random measure resampling step computed 
sirfs belong class block processing algorithms sirf operations works block data 
sirf operations large latencies 
particle generation weight calculation normalization resampling functional view sirfs 
output calculation consider sirf topology clear pseudocode particle generation weight computation model dependent 
resampling depend accepted state space model important property sirfs 
allows analyzing algorithmic architectural properties sirfs generic state space model 
complexity complexity sirfs depends complexity dimension underlying state space model 
number operations sample sirfs immense 
input sample bearings tracking problem exponential atan complex functions calculated gaussian random numbers generated 
order get acceptable performances number particles order thousands 
main difference sirfs dsp algorithms seen number operations sirfs perform type operations 
sirfs solve non linear problems operate non linear functions 
noticed pseudocode non linear computations placed importance step 
general non linear functions sample step places generation random numbers process equation 
type non linear functions depends problem hand 
common non linear function exponential function frequent assumption observation noise gaussian 
order complexity algorithm obtained counting number type involved arithmetic operations 
performances sirfs bearings tracking problem evaluated popular ti tms dsp generation texas instruments processors chosen dsp library contains transcendental functions random number generators 
number clock cycles 
choose maximum clock rate processor mhz account control overhead maximum sampling rate khz 
concurrency operations normalization normalization step requires additional loop iterations divisions observation 
noted normalization represents unnecessary step merged resampling computation importance weights 
case normalization block input resampling block non normalized weights sum weights wn 
avoiding normalization requires additional changes depend resampling carried time instant 
particle filters perform resampling time instant arguments resampling routine called pseudocode rn sr wn added non normalized weights 
weights normalized uniform random number systematic resampling routine drawn wn updated wn particle filters perform resampling time instant modifications computation importance weights necessary 
pseudocode see calculation new weight requires multiplication weight previous time instant 
weights scaled multiplication cause dynamic range problems weight scaled point 
order avoid divisions sum weights wn ln wn calculated particle filtering recursion added exponent exponential function 
approach division wn performed resampling step significantly reduces dynamic range problem fixed precision arithmetics usually appears division 
computational burden reduced normalization requires divisions 
concurrency sample importance resampling steps sirf pseudocode implemented way separate loop iterations particle filtering operations 
way temporal locality exists sirfs utilized 
temporal locality preserved particle generation weight computation steps steps local calculated data 
particle generated sampling step immediately weight computation 
data dependencies particles weight computation particle generation steps overlapped time corresponds process loop fusion algorithmic level 
addition output calculation overlapped time weight computation particle generation steps soon particle weight known possible execute multiple accumulate mac operation output calculation step 
resampling step overlapped time weight computation step necessary sum weights known resampling starts 
sum weights known weights weight computation step calculated 
possible resampling overlap time particle generation step 
soon particle resampled input particle generation step 
sirf operations overlapped time shown pseudocode 
loops sample importance output calculation steps fused 
normalization 
non normalized weights output calculation necessary divide final result output calculation sum weights output scaling 
resampling fused particle generation step 
particles replicated particle generation step directly 
counters loops address particles previous time instant placing new sampled particles count number times particle replicated 
architectures temporarily concurrent architecture order achieve minimum execution time mapping particle filtering operations hardware resources done allows utilizing operational concurrency 
operations executed time blocks pipelined hardware implementation 
shows timing diagram latencies different operations 
timing diagram algorithm described pseudocode architecture 
output block generated clock speed 
exception output calculation block output generated particle filter sampling period 
particle generation weight calculation output calculation part resampling pipelined possible operations performed loop variables locally concept temporal locality 
latency operations ls li tclk tclk clock period number particles ls li latencies due fine grained pipelining particle generation weight computation steps respectively 
resampling pipelined particle generation step 
problem overlapping steps fact duration existing resampling methods deterministic 
example sr algorithm pseudocode loop number recursion dependent distribution particle weights 
new algorithms suitable pipelining resampling particle generation step proposed described chapters 
systematic resampling algorithm considered 
sr algorithm takes clock cycles worst case 
means cycles particle resampled 
particles particle generation step sirf recursion start clock cycles resampling step 
minimum execution time non distributed sirf tclk sum pipelining latencies sirf blocks 
purpose obtain estimates position velocity xn yn 
input observation zn particles initial setup wn xn yn method sampling step 
xn xn xn xn xn yn yn yn yn yn 
importance step zn atan wn wn 
output calculation xn xn xn yn yn yn 
resample particles instance sr algorithm sr wn output scaling xn xn wn wn yn yn wn wn pseudocode sirf bearings tracking operation overlapping 
generation particles weight computation output calculation sirf parallel architecture resampling timing diagram sirf 
generation particles weight computation output calculation distributed architecture sirfs shown 
consists processing elements pes central unit cu 
data dependencies particle generation computation weights steps easily parallelized pipelined 
segment particle filtering data parallel single instruction multiple data simd algorithm 
particle generation weight computation particles partitioned pes pe performs operations time different particles pe responsible processing particles integers 
cu carries partial full resampling particle routing control 
full resampling means resampling procedure performed logic unit 
chapters show resampling distributed pes cu responsible small portion resampling 
pe pe cu architecture distributed particle filter cu pes 
parallel implementation distinguish operations carry resampling task 
computation involves bare resampling procedure result array indexes show replicated particles addresses 

communication represents exchanging particles pes resampling results 
refer particle routing 
particle routing defines protocol network architecture exchanging particles main focus chapter 
pe pe 
scheduling includes determining particles pes routed stored locally placing particles destination pes addressing indexes 
refer particle allocation 
pes minimum execution time tclk 
consider pipelining latency non parallel implementation processing time reduced times 
goal parallel implementation develop algorithms architectures reach minimum execution time 
strategy achieving minimum execution time allow deterministic communication particle routing 
overlap particle routing sampling step allow pipelining hardware operations particle routing increase execution time sirf 
speedup versus number pes different case spatial implementation sirf 
platform fpga asic allows spatial implementation 
curve saturates faster small closer value constant latency significant gain increasing level parallelism close course fully parallel implementation practical take advantage pipelining causes complex communication protocols gain speed smaller simulations assumed tclk remained increased level parallelism 
speedup number pes speedup versus number pes distributed sirf 
spatial implementation sirf assumed 
show communication pattern non deterministic connections pes changed sampling period 
number particles produces resampling important note random number depends distribution weights 
pes surplus particles need exchange particles pes shortage particles number changes sampling period necessary connect different pes order perform particle routing 
number particles exchanged pes nm 
mean maximum number particles exchanged network sirf applied bearings tracking problem shown 
curve mean shows average number particles curve max maximum number particles exchanged interconnection network pes case pes connected cu single bus execution time corresponds worst case number particles exchanged network 
means cases execution time specified 
number particles exchanged communication network number processing elements mean mean max max average maximum number particles exchanged network particles 
memory schemes resampling proper particle allocation reduce number memory accesses size memory storing particles 
allocation performed index addressing execution overlapped time particle generation step 
different outputs resampling particles weights example shown considered 
indexes represent positions replicated particles 
example means particle replaces particle 
particle allocation easily overlapped particle generation set resampled particles 
randomness resampling output difficult realize place storage additional temporary memory storing resampled particles necessary 
particle replicated twice occupies locations particles 
particle replicated stored memory rewritten 
refer method particle allocation index addressing 
indexes represent number times particle replicated 
example means particle replicated twice 
refer method particle allocation replication factors 
method requires additional memory particles memory storing indexes 
additional memory storing particles necessary particles replicated positions discarded particles 
call method particle allocation arranged indexes positions replication factors 
addresses replicated particles discarded particles number times replicated replication factor stored 
indexes arranged way replicated particles placed upper discarded particles lower part index memory 
replicated particles take addresses discarded particle address 
knows advance addresses discarded particles need additional memory storing resampled particles new particles placed addresses occupied particles discarded 
useful particle filters applied multi dimensional models avoids need excessive memory storing temporary particles 
types memory usages indexes positions replicated particles indexes replication factors indexes arranged positions replication factors 
sr particles allocated resampling procedure need index array 
additional particle memory number memory access doubled particles stored read memory sampling step 
memory access bottleneck signal processing applications 
chaining functional units concurrent operations notably reduce memory access 
chaining allows accessing intermediate results registers memory 
savings achieved loop fusion shown pseudocode weigh computation readings memory avoided particles registers 
normalization reading writing weight memory avoided 
particle allocation step particles allocated resampling step particle allocation pipelined particle generation step need storing reading resampled particles memory 
saves writing reading operations 
final result analysis exploiting concurrency operations memory access significantly reduced readings writings saved 
allows increasing sampling rate reducing power consumption 
chapter algorithms architectures particle filters chapter new resampling algorithms allow faster execution reducing memory requirements described 
architectures support algorithms 
changes resampling algorithm necessary fixed explained 
results implementation sirf state art fpga platform 
new resampling algorithms main goals section development resampling methods allow increased speeds sirfs require memory achieve fixed timings regardless statistics particles computationally intensive 
development algorithms critical practical implementations 
performance algorithms analyzed executed dsp specially designed hardware 
investigate sequential resampling algorithms analyze computational complexity metrics including number operations class type operation performing behavioral profiling 
main feature random resampling algorithm referred residual systematic resampling rsr described section perform resampling fixed time depend number particles output resampling procedure 
deterministic algorithms discussed section threshold algorithms particles moderate weights resampled 
significant savings achieved computations number times memories accessed 
show characteristic types deterministic algorithms low complexity algorithm algorithm allows overlapping resampling operation particle generation computation 
random deterministic algorithms reduce number operations number memory accesses 
represent alternative existing algorithms simulations 
performance complexity analysis section 
residual systematic resampling algorithm propose new resampling algorithm stratified resampling refer residual systematic resampling rsr 
similar rr rsr calculates number times particle replicated avoids second iteration rr residual particles need resampled 
recall rr number replications specific particle determined loop truncating product number particles particle weight pseudocode 
rsr updated uniform random number formed different fashion allows iteration loop processing time independent distribution weights input 
rsr algorithm input output resampled particles summarized pseudocode 
purpose generation array replication factors input time instant 
array weights method input output number particles respectively rsr generate random number main loop computation replication factors updating random number pseudocode residual systematic resampling rsr algorithm 
pseudocode input output number particles resampling procedure 
common different cases including 
sr rsr second steps residual resampling algorithm 

large deviations previous estimates beneficial change number particles 
larger smaller 
case parallel resampling described resampling parallel element surplus mor shortage mof particles 
graphically illustrates rsr method case particles weights table 
rsr algorithm draws uniform random number way sr updates display uniform number updated origin currently considered weight sr propagated origin coordinate system 
difference updated uniform number current weight propagated 
shows calculated initial uniform random number particle 
particle discarded andr 
important note sr rsr produce identical resampling result 
residual systematic resampling example particles 
rsr method natural particle allocation replication factor arranged indexes rsr produces replication factors 
particle generation step loop number iterations corresponds replication factors replicated particle 
difference sr rsr methods way inner loop resampling step sr particle generation step rsr performed 
number replicated particles random loop sr unspecified number operations 
allow unspecified number iterations complicated control structures hardware needed 
main advantage approach loop sr replaced loop known number iterations 
deterministic resampling overview literature threshold resampling algorithms combination residual resampling rejection control result non deterministic timing increased complexity 
develop threshold algorithms purpose reduce complexity processing time 
refer methods partial resampling pr part particles resampled 
partial resampling particles grouped separate classes composed particles moderate weights dominating negligible weights 
particles moderate weights resampled negligible dominating particles resampled 
clear average resampling performed faster particles moderate weights resampled 
propose pr algorithms differ resampling function 
particle partial resampling sub optimal algorithms partial resampling seen way partial correction variance weights time instant 
pr methods consist steps particles classified moderate negligible dominating determines number times particle replicated 
step pr weight particle compared high low thresholds th tl respectively th tl th 
number particles weights greater th tl denoted nh nl respectively 
sum weights resampled particles computed sum dominating wh nh wn wl nl wn resampling functions th negligible weights tl 
define different types resampling distinct resampling function partial resampling algorithm pr shown corresponds stratified resampling case 
number particles input output resampling procedure equal nh nl 
resampling function wn th tl wh wl nh nl second step performed resampling algorithm 
example rsr algorithm called rsr nh nl nh nl wh wl rsr performed nh nl particles negligible dominating weights 
weights normalized processed rsr method 
second partial resampling algorithm pr shown 
assumption negligible particles discarded resampling consequently particles negligible weights resampling procedure 
particles dominating weights replace negligible weights certainty 
resampling function wn wl nh th wh wl nh nl tl th number times particle replicated rsr nh nh th 
nl wl nh wh wl weights satisfy condition nh input particles nh nl particles produced output 
third partial resampling algorithm pr shown 
weights particles threshold th scaled number 
pr deterministic algorithm resampling function nh nl th tl th number replications dominating particle may particle necessary rounding operation 
way resolving problem assign nt nl nl nh nh dominating particles replicated nl times nh rest nh nt dominating particles replicated nl nh times 
weights calculated represents positions particles moderate weights wl nh nl wherem positions particles dominating weights particles dominating negligible weights 
tl th tl th tl th resampling functions partial resampling algorithms pr pr pr 
way performing partial resampling set thresholds 
idea perform initial classification particles weights computed carry actual resampling particle generation step 
resampling consists steps pr algorithm classification particles overlapped weight computation 
refer method overlapped partial resampling opr 
problem classification particles necessity knowing sum non normalized weights advance 
problem resolved follows 
particles partitioned weights 
thresholds group defined ti ti number groups ti ti 
selection thresholds problem dependent 
thresholds define moderate group particles satisfy tk tk 
particles weights greater tk dominant particles ones weights tk negligible particles 
provide simple example works 
thresholds non normalized particles compared thresholds properly grouped 
obtaining sum weights second group group particles moderate weights 
group contains particles negligible weights third group composed particles dominating weights 
additional loop necessary determine number times dominating particles replicated 
complexity loop order orders magnitude lower complexity second step pr algorithm 
weights classified possible apply similar logic second resampling step pr pr algorithms 
particles replicated twice weights calculated formulae weights pr method 
initial weights sum classification pr algorithm sum opr method combined pr method final computation weights replication factors 
discussion pr pr pr algorithms step requires loop iterations worst case number computations comparisons iteration classification groups 
resampling pr algorithm performed nl nh particles 
worst case pr algorithm occurs nl nh means particles resampled implying improvements implementation standpoint 
main purpose pr algorithm improve worst case timing pr algorithm 
nh dominating particles resampled 
input number particles resampling procedure nh output number particles nh nl 
rsr algorithm resampling complexity second step nh 
pr pr contain loops timings depend weight statistics 
advantages real time implementation comparison rsr loop iterations processing time depend weight statistics 
pr algorithm stratified resampling 
number times dominating particle replicated calculated step depends current distribution particle weights thresholds 
number calculated time means need loop second step 
pr simpler operations rsr algorithm 
pr algorithms advantages perspective hardware implementation resampling performed faster average done smaller number particles possibility overlapping resampling particle generation weight computation resampling parallel implementation number exchanged particles processing elements smaller particles replicated replaced 
problems algorithms 
nl resampling necessary 
nl time pr algorithms perform resampling useful 
application opr algorithm requires method fast classification 
hardware dsp implementation suitable define thresholds power 
take ti 
group determined position significant fixed point representation weights 
memory allocation groups static dynamic 
static allocation requires memory banks size bank equal number particles particles located groups 
dynamic allocation efficient implemented ways similar linked lists element group contains fields field address particle field points element list 
dynamic allocation requires memory capacity words 
expected overlapping increases resources 
particle filtering performance complexity performance analysis proposed resampling algorithms applied performance evaluated joint detection estimation problem communication bearings tracking problem 
joint detection estimation experiment considered rayleigh fading channel additive gaussian noise differentially encoded bpsk modulation scheme 
detector implemented channel normalized doppler spreads bd corresponds fast fading 
ar process model channel 
ar coefficients obtained method suggested 
proposed detectors compared detector performs matched filtering detection assuming channel known exactly receiver 
number particles 
bit error rate ber versus signal noise ratio snr depicted pr algorithm different sets thresholds th tl 
pr algorithm thresholds denoted pr thresholds pr 
mf systematic resampling performed shown 
observed ber similar types resampling 
best results obtained thresholds 
effective number particles largest comparison pr algorithm greater th smaller tl 
logical result pr particles concentrated narrower area thresholds producing way larger effective sample size 
pr thresholds slightly outperforms systematic resampling algorithm bit surprising 
reason particles moderate weights unnecessarily resampled pr algorithm 
result obtained different values doppler spread 
ber versus snr shown different resampling algorithms pr pr opr sr thresholds pr pr 
opr uses groups thresholds power 
results comparable 
opr pr algorithms slightly outperform algorithms 
bearings tracking tested performance sirfs applying resampling algorithms tracking different initial conditions 
experiment pr pr ber snr matched filter sr sr effective performance pr algorithm different threshold values applied joint detection estimation problem wireless communications 
ber snr pr pr pr matched filter sr sr effective comparison pr pr opr algorithms systematic resampling applied joint detection estimation problem wireless communications 
sets threshold values th tl 
show number times track lost versus number particles different pairs thresholds 
consider track lost particles zero weights 
pr algorithm thresholds denoted pr thresholds thresholds pr 
algorithms sr sr performed th observation pr pr 
resampling algorithms show similar performances 
best results pr pr obtained thresholds 
opr pr pr lost track number particles sr sr th time pr pr pr pr number times track lost pr pr sr applied bearings tracking problem 
complexity analysis complexity proposed resampling algorithms evaluated 
consider computation complexity memory requirements 
benefits proposed algorithms concurrency hardware exploited 
computational complexity table provide comparison different resampling algorithms 
results rr obtained worst case scenario 
complexity rr rsr pr algorithms complexity sr algorithm max input output numbers particles resampling procedure 
sr rr rsr pr multiplications additions comparisons table comparison number operations different resampling algorithms 
number particles input resampling algorithm equal number particles output rr algorithm far complex 
number additions sr rsr algorithms rsr algorithm performs multiplications 
multiplication complex addition view sr complex algorithm 
power multiplications avoided rsr algorithm complex 
resampling algorithms sr rsr pr implemented texas instruments ti floating point digital signal processor dsp tms xx 
steps profiling brought fold speed number resampled particles 
particle allocation step considered 
number clock cycles particle rsr pr 
sr algorithm fixed timing 
mean duration cycles particle standard deviation 
processor tms cycle time ns processing rsr particles took 
memory requirements analysis considered memory requirement resampling complete sirf 
memory size weights memory access weight computation depend resampling algorithm 
consider particle allocation indexes index addressing sr algorithm arranged indexing rsr pr pr opr see section 
particle allocation methods sr algorithm memories storing particles 
table see size memories rsr pr pr algorithms 
difference methods size index memory 
rsr algorithm uses particle allocation arranged indexes index memory size wherem words storing addresses particles replicated discarded 
words represent replication factors 
number resampled particles worst case pr algorithm corresponds number particles rsr algorithm 
index memories size 
implementation standpoint promising algorithm pr algorithm 
simplest requires smallest size memory 
replication factor dominating particles moderate particles 
size index memory pr requires additional bit represent particle dominant moderate 
opr algorithm needs largest index memory 
sirf steps overlapped requires different access pattern deterministic algorithms 
due possible overwriting indexes formed weight computation step ones read particle generation necessary index memory banks mi mi 
furthermore particle generation weight computation access memories alternately 
writing mi performed resampling step time instance memory particle generation reading 
memory bank mi alternately 
compare memory requirements opr algorithm pr algorithm clear opr requires times memory storing indexes resampling 
sr algorithm memories storing particles capacities nsm necessary refer ms ms table 
disadvantage sr algorithm indexes additional access state memories resampling step 
sirf uses sr algorithm indexes particle allocation step pipelined sample step states state memories read written sample step 
particles read memory ms written memory ms sample step read ms written ms sample step 
alternating process switching memories requires additional logic 
algorithms require alternating process labeled alt 
table 
table see particles written read state memory time 
advantage rsr pr opr algorithm sr algorithm state memory 
advantage paid additional logic handling index array 
sr indexes sr indexes rsr pr pr opr states nsm nsm nsm nsm nsm nsm weights indexes table memory capacity different resampling algorithms 
sr indexes sr indexes rsr pr pr ppr sample read states ms states ms ms alt states ms states ms indexes indexes indexes mi mi alt 
write states ms states ms ms alt 
states ms states ms importance read write weights weights weights weights resample read weights states ms weights weights weights write states ms indexes indexes indexes mi mi alt 
table pattern memory access different resampling algorithms 
sirf speed improvements sirf sampling frequency increased hardware exploiting temporal concurrency 
data dependencies particles particle generation weight computation operations steps overlapped 
furthermore number memory accesses reduced weight computation values states need read memory registers 
order achieve higher speed particle filter algorithms modified way implicit normalization step 
normalization rsr method approached way section 
normalization pr methods avoided including information sum weights wn thresholds thn 
timing operations hardware implementation blocks fine grain pipelined shown 
particle generation weight calculation operations overlapped time normalization avoided 
symbol constant hardware latency defined depth pipelining particle generation weight computation tclk clock period number particles minimum processing time basic sirf operations 
sr suitable hardware implementations fixed minimal timings required processing time depends weight distribution longer 
order resampling operation performed clock cycles rsr pr algorithms particle allocation arranged indexes 
minimum sirf sampling period achieved tclk 
opr combination pr algorithm allows higher sampling frequencies 
opr classification particles overlapped weight calculation shown 
symbol lr constant latency part opr algorithm determines group contains moderate negligible dominating particles 
latency lr proportional number orp groups 
speed sirf increased twice consider pipelined hardware implementation 
obvious sirf processing time reduced lr tclk 
generation particles weight computation resampling generation particles weight computation resampling timing sirf rsr pr methods opr method 
additional speed improvements sirf rsr rsr step duration sample importance steps different hardware resources possible run concurrently sirfs hardware time 
coarse timing diagram sirf operations operations sirf gray 
input observation time instant denoted zn 
filters way resampling step sirf executed concurrently sample importance steps filter 
example sirf performs particle generation weight computation time instants sirf time instants 
resampling sirf corresponds processing observation zn performed concurrently sample importance steps sirf processes observation zn executing sirfs model structure different parameter possible 
feature desirable applications estimates sirfs compared different parameter setups 
running sirfs concurrently achieved expense doubling memory requirements increasing controller complexity increasing logic area requirements data path 
concurrent execution sirfs sr algorithm possible high speed requirements 
shown duration sr empty time slots concurrent particle filter runs maximum speed 
final remarks summarize impact proposed resampling algorithms sirf speed memory requirements 

rsr improved residual resampling algorithm higher speed fixed processing time 
hardware implementations better algorithm resampling executed standard computers 
generation particles weight computation resampling generation particles weight computation resampling generation particles weight computation resampling timing sirfs operations overlapped share hardware resources 

memory requirements reduced 
number memory access size memory reduced rsr pr algorithms multidimensional state space models 
methods appropriate hardware dsp applications available memory limited 
state space model onedimensional purpose adding index memory introducing complex control 
case sr algorithm recommended 

hardware implementation temporal concurrency sirf sampling frequency considerably improved 
best results achieved opr algorithm expense hardware resources 

average amount operations reduced 
true pr pr pr perform resampling smaller number particles 
desirable pc simulations dsp applications 
residual resampling fixed point low complexity rr scheme particle filters section 
proposed scheme uses simple particle tagging method compensate possible error caused finite precision quantization resampling step particle filtering 
scheme guarantees number particles resampling equal number particles resampling 
resulting scheme suitable high speed physical realization number particles power 
proposed resampling scheme correct functioning resampling necessary sum weights normalization equal 
condition satisfied vlsi implementation due finite precision effect 
rr number replicated particles calculated truncation rounding product case truncation number particles produced step general necessary process residues order compensate number particles see pseudocode 
rr algorithm suitable fixed point implementation considered 
residues processed memory addressing scheme tagging method procedure guarantees correct number particles resampling 
hardware implementation fixed point number representation weights quantized bits excluding sign bit log 
naive approach quantize value weight simple truncation 
integer representation bits corresponds number times particle replicated 
simple truncation may result total number replicated particles illustrated table andk 
second column represents decimal values particle weights third column binary representation bits 
fourth column provides replication factor decimal equivalent value bits indicated bold 
table particle replicated twice particle replicated particles eliminated 
quantization sum resampled particles equal 
general may equal particles weights quantized replication weights factor table resampling quantization simple truncation 
solving problem consider conventional rounding method 
rounding employed possible sum replicated particles bits rounding scheme result tag status truncate truncate tag truncate tag round truncate truncate tag truncate tag truncate tag table rounding truncation scheme tags 
larger noted simple truncation rounding methods complicate hardware 
example sum replicated particles hardware decide particles additionally replicate total number particles hand sum replicated particles larger hardware choose replicated particles removal 
scenarios require additional iteration scanning weights reevaluation selection particles additional replication removal 
modification necessary efficient hardware implementation 
order resolve problem having resampled particles propose weights quantized additional bits log excluding sign bit 
additional bits create tags 
final quantization performed consists rounding truncation shown table 
entries column bits binary representation weight second column applied rounding scheme third column resulting significant bit column tag status 
notice bit pattern rounded tagged adder needed incorporate carry propagation significant bit 
bit pattern rounded simple bit reversal sufficient 
difference tag tag indicate tag higher priority replication 
simplicity implementation distinguished especially value large 
particles tag total number replicated particles tagged particles may replicated 
note andt sums particles tags tag tag tag respectively 
ort tagged particles written case reason tagging rounding cases avoid situation exceeds may possible exclude particles important located physically memory location described section 
proposed scheme illustrated table example table 
modified quantization scheme ensures sum replication factors closer resampling quantization simple truncation 
addition single iteration scanning weights necessary saves computation time particles weights quantized round replication weights truncate factor table resampling proposed scheme 
cuts processing time half minimizes hardware complexity 
special cases considered carefully 
situation particle weight equal rest zero 
special modification scheme get weights zero considers significant bits 
avoid problem weight decimal representation represented tagging method guarantee total number replicated particles second possible weights zero 
may happen estimate state estimated diverges possible accurately compute weights finite precision 
situation detected weight calculation stage prior resampling resampling performed 
third special case occurs number particles resampling greater due rounding 
algorithm produces correct number particles number particles proportional respective weights 
order illustrate performance proposed resampling scheme weights randomly generated weight distributions resampling compared full precision resampling 
illustrates replication factors tagging 
results obtained simulating logic structure scheme random set normalized weights implement resampling 
shown scheme tagging close full precision resampling plotted solid line 
sum replicated particles illustrated resampling tagging additional simulation results published 
logic structure logic structure proposed scheme shown 
particles weights stored memory provided prior resampling 
address access particles weights 
weight read decoded 
integer value bits representing loaded counter particle replication 
particle replicated memory starting lowest address particle tagged tag tag written memory starting highest address 
ensure replicated particles tagged particles written discarded 
total number replicated particles possible problem resolved having small memory storing tagged particles tag particles copied resampled resampled weight distribution exact tag tag index comparison exact resampling resampling tagging 
particles ordered weights particle largest weight 
memory 
condition checked adding addresses memory locating insertion replicated particles tagged particles 
sum addresses tagged particles tag inserted starting address particles right assumed number particles power scheme extended handle arbitrary number particles 
parallelization possible high throughput applications 
address counter address address particle memory weight memory data bit bit bit bit memory tag counter tag tag particle memory increment decrement logic diagram illustrates structure proposed resampling scheme 
assumed particles weights provided prior resampling 
resampled particles stored separate memory 
architectures sirfs rsr algorithm section architectures sirfs 
rsr algorithm modified suitable hardware implementation 
architecture rsr memory related operations sample step analyzed 
architectures remain unchanged irrespective model sirf applied 
architectures sample state space model operations importance steps 
architectures sirf bearings tracking model implemented fpga xilinx virtex ii pro platform 
resource utilization latency design 
adjusting rsr algorithm hardware implementation section changes algorithm shown pseudocode 
resampling performed non normalized weights 
number operations reduced recognizing operations repeat pseudocode 
particle allocation arranged indexes incorporated 
modified rsr shown pseudocode 
non normalized weights incorporated replacing lines pseudocode wn sum weights time instant see line line pseudocode multiplication division weights factor wn 
pseudocode code modified way multiplication inside loop 
done multiplying expression line pseudocode wn factor wn expressions lines pseudocode 
order avoid multiplying factor wn modified way random number generated 
generated range range wn line pseudocode 
multiplication inside loop division loop pseudocode 
weights normalized need division 
second part pseudocode indexes replicated discarded particles generated particle allocation method arranged indexes see section 
index replicated particles set zero discarded shown line 
replication factor greater zero address replicated particles stored indr index incremented points address replicated particle stored line 
similarly address discarded particles index updated replication factor equal zero 
purpose generation arrays replicated discarded particle indexes ir id array replication factors time instant 
input array weights number particles sum weights wn 
method ir id rsr wn wn 
generating random number 
wn calculating constant 

indr set initial values indexes 

main resampling loop 

temp temporary variable 

indr temp storing replication factor 

temp indr updating uniform random number 

indr particle allocation 
indr indr indr storing address replicated particle 


storing address discarded particle 


pseudocode modified residual systematic resampling rsr algorithm 
way memory related operations sample step performed shown pseudocode 
loop line address array replicated indexes number iterations loop determined number replicated particles 
internal loop line number iterations equal replication factors 
number iterations loops operations sample step performed particles replicated indr 
indr sampled particles written addresses discarded particles lines 
sampled particles rewrites original replicated particle line replicated particle stored variable reg 
logic structure scheme rsr algorithm combined particle allocation method arranged indexes 
addresses replicated discarded particles stored memory arranged way replicated particles placed upper discarded particles lower part index memory 
indexes replicated particles incremented indexes discarded particles decremented 
replicated factors stored separate memory 
control block controlling operations writing reading particle memory derived replicated factors elaborated 
architectures algorithms shown figures respectively 
weights stored memory addressed address counter corresponds variable pseudocode 
purpose generation array particles input time instant 
arrays replicated discarded particle indexes ir id array replication factors array particles method time instant 

sample ir id indr set initial values indexes 

indr length indr main sampling loop 

reg indr replicated particle stored variable reg 

indr sample reg particle generation performed 
indr indr replicated particle overwritten 

indr 

sample reg particle generation performed replicated particle written address discarded particle 


pseudocode memory related operations sample step 
pointers replicated particles addr read en read port addr write en data write port sampled particles instant resampled particles instant sample importance resample control replicated discarded particle memory replication factor memory architecture sirfs rsr algorithm index generator block arithmetics lines pseudocode implemented 
index generator simple contains adders multiplier 
part represents architecture particle allocation step 
indexes replicated discarded particles stored index memory replication factors stored memory 
indexes stored multiplexor 
replication factor greater zero enabled content incremented 
delay cycle delay multiplexor allows writing particle index memory time replication factor written memory 
replication factor zero enabled index discarded particle written memory written memory 
resampling particle allocation counter particle index mem 
index generator replication factors comp 
en en en counter counter delay ind ind address write en address data data replicated discarded index memory replicated factors memory architecture rsr algorithm combined particle allocation 
main blocks address generation address control particle generation storing 
memory storing particles particles read written sampling step 
reading writing operations performed concurrently dual port memory 
arithmetics sampling step implemented sampling unit 
delay read write operations memory determined pipeline latency sample unit 
delay 
memory indexes address particles memory way shown pseudocode 
represents variable value replication factor read memory written 
time instant output comparator set memory addressed 
replicated particle read memory written register reg clock cycle 
write enable signal register derived comparator 
clock cycle output flip flop ff enabled initiates decrementing 
flip flop reset incremented 
address generation en en counter counter ind ind en addr addr replicated discarded index memory replicated factor memory ff counter comp delay comp addr addr read port write port data delay address control particle generation storing re write en sample unit latency architecture memory related operations sampling step 
architectures sirfs bearings tracking section architectures resampling step memory related operations particle generation step generic particle filter 
section particle filtering operations model dependent particle generation weight computation 
implementation temporally concurrent order allow achieving maximum speed 
shows block diagram sampling unit part architecture 
arithmetic operations particle generation step described pseudocode 
sampling unit contains noise generators implemented box muller approach 
noise generator combination lookup tables arithmetic logic 
lookup tables eliminates large latency generated arithmetic logic unit 
buffers associated input vectors xn yn output vectors xn yn shown 
arithmetic operations weight computation step illustrated pseudocode 
main operations multiplications trigonometric function arctan exponent function exp 
unrolled cordic operator atan exp regular implementation structure 
additional logic correction angle fault problem algorithm atan 
purpose correction resolve ambiguity atan values identical physical locations different 
input zn subtracted output atan block 
atan implementation fine grained pipelined necessary delay input signal 
latency delay element pipelining depth atan block 
subtracted values squared multiplied constant exponential operation performed 
vx vy random number generator random number generator vx vy architecture sampling unit bearings tracking problem 
variables design converted fixed point representation 
position coordinates represented bits input zn represented bits 
observed final results weights equal finite precision arithmetic variable greater 
order reduce area requirements multipliers lowest possible number bits loss performance 
multiplier multiples bit numbers 
output comparator select signal output multiplexor 
output logic wn 
latency delay element delay equal pipelining depth exponential block 
atan core delay exp comp delay mux sel architecture weight computation step bearings tracking problem 
architecture exponential function shown 
uses cordic core xilinx virtex ii pro library 
outputs core sinh cosh input restricted range 
requires exponent exponential function expressed sum integer fractional part 
exponential function integer part exponent pre calculated stored look table represented rom bit words 
delay unit rom outputs rom cordic core synchronized 
latency delay equal pipelining depth cordic core 
signal delayed signal multiplied 
exp sinh cosh core rom implementation exponential function xilinx virtex ii pro fpga 
fpga implementation results section results implementation architectures described sections xilinx virtex ii pro fpga platform 
architectures captured verilog hdl design verified mentor graphics 
verification verilog description input xilinx development system synthesized mapped placed routed design xilinx virtex ii pro device xc vp 
resources utilization results combination number logic slices multiplier blocks memory bits 
resource requirements sirf shown table 
see amount resources small resources space parallel design higher speed required 
implementation sample importance resampling blocks architectures previous sections 
finite precision approximation variables performed systemc language similar approach 
particles represented bits sign bit bit left bits right decimal point weights represented bits bit decimal bits fractional part 
final memory requirements memories storing particles memory storing weights memory storing replication factors indexes 
bit memory indexes specifies maximum number particles processed particle filter 
storage space bits 
delay design restricted size xilinx virtex ii pro block rams 
particle generation step memories random number generation storing particles 
memory importance step storing weights resampling step storing indexes replication factors 
speed implementation limited speed slowest block cordic block 
best case clock frequency fine grained pipelined bit cordic mhz 
safe side clock frequency design chosen mhz 
resource sample importance resampling total available resources slices block rams block multipliers table resource utilization sirf implementation bearings tracking problem xilinx xc vp device 
chapter algorithms architectures distributed particle filters section propose novel resampling algorithms architectures efficient distributed implementation particle filters 
proposed algorithms improve scalability filter architectures affected resampling process 
main design goal section minimize execution time sirf 
done exploiting data parallelism pipelining operations 
order decrease sirf execution time algorithm allows distributed resampling reduced communication interconnection network proposed 
algorithm section named distributed resampling proportional allocation rpa 
yields resampling result sequential resampling method example systematic resampling 
improvement execution time achieved making communication interconnection network deterministic local 
algorithms non proportional sampling resampling non proportional allocation rna section 
different architectures suitable distributed rpa rna algorithms discussed section 
objective architectures pipeline communication interconnection network particle routing subsequent sampling step described section 
evaluate architecture parameters fpga platform 
centralized resampling centralized resampling straightforward approach implementing sirfs architecture 
particle generation weight computation executed parallel pes 
cu carries resampling particle routing control 
sequence operations directions communication shown 
cu responsible full resampling performed sequentially resampling time scaled communication requirements implementation immense 
cu collects weights order perform resampling returns indexes replication factors pes 
assume particle allocation arranged indexes applied 
communication weights indexes deterministic particles routed non deterministic fashion 
number particles transferred cu direction communication pe cu pe particle surplus resampling cu pe pe particle shortage 
amount particles transferred network worst case 
fully connected network scalability implementation significantly affected sequential resampling particle routing 
pe pa weights replication factors particles cu pa pe intra pa particles sequence operations performed th pe cu centralized resampling rpa 
direction communication data sent 
abbreviations sample importance resampling pa particle allocation 
distributed rpa section method stratified sampling proportional allocation described 
sample space divided disjoint areas strata stratum corresponds pe 
density particle weights written mixture densities restricted corresponding strata 
proportional allocation strata means samples drawn strata larger weights 
weights strata known number particles stratum replicates calculated rsr process denoted inter resampling treats pes single particles 
resampling performed inside strata referred intra resampling 
resampling algorithm accelerated loop transformation specifically loop distribution allows having inner loop run parallel pes intra resampling small sequential centralized pre processing 
weight pe calculated sum weights particles inside cu inter pa pe diagram sequence operations performed pe cu shown 
algorithm rpa shown pseudocode 
inputs algorithm pe weights output number particles pe produce resampling mw rsr algorithm applied get propagating uniform random number similar fashion systematic resampling algorithm 
algorithm obtained truncating minimum value truncated product minimum value zero 
resampling performed pe parallel intra resampling step 
input intra resampling algorithm number particles generated resampling procedure 
stress difference results rpa sequential resampling 
purpose calculation number particles intra resampling algorithm 
input array pe weights method generating random number inter resampling loop 
calculating replication factors 
send updating uniform random number 
parallel intra resampling pes pseudocode distributed rpa algorithm utilizes rsr approach 
rsr algorithm attractive hardware implementation loop loops systematic resampling easily pipelined calculate replication factor clock cycle easily deals different number particles input output 
systematic resampling loop unknown number iterations difficult apply pipelining 
course resampling result obtained residual systematic resampling applied inter resampling algorithms 
example particle exchange rpa algorithm shown 
sirf architecture pes considered pe processes particles 
distribution normalized pe weights resampling table 
inter resampling number particles pe produce determined respectively 
pes particles 
example pe sends particles pe pe sends particles pe 
main difference centralized distributed rpa lies reducing amount deterministic communication move resampling cu pes 
time resampling procedure distributed rpa reduced times wherem corresponds intra resampling time weights pes resampling pe weights resampling sum example particle exchange rpa algorithm 
time inter resampling 
important note inter resampling requires global communication pes intra resampling completely local pes 
words representing weights indexes exchanged centralized resampling reduced words rpa 
scalability implementation affected particle routing step unchanged 
assume equal clock period resampling particle filters steps tex lp tclk represents delay inter resampling delay particle routing 
pes cu connected single bus delay dominant 
scalability design affected bus structure gain pursuing parallel implementation 
efficient architecture uses buses supports pipelining particle routing sample step proposed section 
distributed rna distributed rpa allows distributed parallel implementation resampling requires complicated scheme particle routing implies complex cu design area increase 
need additional global pre processing step inter resampling introduces extra delay 
problems solved rna algorithm 
term group pe group formed pes 
application sampling key decisions choose strata samples generate stratum 
rpa rna strata induced neighborhood criteria particles inside group form stratum 
rpa number particles drawn proportional weight stratum 
hand rna number particles group resampling fixed equal number particles group full independent resampling performed group 
addition routing particles groups resampling necessary due possibility having unequally distributed weights 
main advantage rna routing particles deterministic planned advance designer 
important difference comparison distributed rpa particle routing step represents biggest problem implementation due unpredictability 
characteristics rna weights resampling equal equal inside groups fork general particle filter algorithm rna outlined pseudocode 

exchange particles groups deterministically 

generate particles group parallel sampling xn 
perform importance step group parallel 
weights calculated zn 
normalize weights particles sum weights group 
perform resampling inside groups obtain new random measures 
go step 
pseudocode particle filter steps distributed rna 
differences comparison original sir filter 
normalization performed local sum resampling performed locally group weights equal inside group 
analysis consider particle filters normalization step avoided simple modification resampling algorithm shown section 
comparison resampling particle routing steps rpa rna shown table 
rpa rna inter resampling calculate intra resampling input input output output particles particles particle depends deterministic routing particle distribution table comparison rpa rna steps 
distinguish methods particle exchange resampling regrouping adaptive regrouping local exchange 
methods example described 
description methods provided sequel 
example particle exchange rna algorithms regrouping adaptive regrouping local exchange 
sum weights group 
distributed rna regrouping rna regrouping resampling particle routing performed inside groups rpa method 
example pe pe form pe pe group 
rpa algorithm applied groups 
result pe pe produce particles resampling particles pe transferred pe 
sampling instant pes rearranged form different groups 
example new groups composed pe pe pe 
time instant regrouping performed particles exchanged pes variance reduced 
rna regrouping resampling particle routing done parallel groups group consists pes 
example pes divided groups group pes 
particle filter steps sample importance resampling performed group concurrently independently steps rna algorithm 
rna regrouping particles implicitly exchanged pes pes 
period regrouping denoted distribution factor interesting analyze meaning distribution factor particles non zero weights pe interested number cycles needed particles propagate pes 
exactly determined distribution factor shown 
assume pe non zero weights 
sampling period pes belong group pe receive non zero weights pe resampling 
propagation weights particle routing dark color 
subsequent time instant groups formed different pes non zero particles propagated pes 
full resampling done sampling periods 
criteria choosing parameters minimal routing rna regrouping andd andd 
distribution factor simplicity communication network distributed controllers 
optimal parameters chosen minimum curve rd feasible values case square grid mesh structure optimal choice parameters 
non optimal optimal regrouping shown figures 
simplicity controllers main design goals restrict number pes groups 
number pes group larger complicated controller necessary order perform fast particle routing described section 
local controllers simple pe surplus shortage particles 
choosing small value cause high distribution factor large number periods full propagation particles achieved 
andk minimal distribution factor 
rna adaptive regrouping rna regrouping uses predefined fixed rules form groups take advantage knowing distribution group weights 
utilizing knowledge possible achieve faster weight balancing smaller regrouping fixed rules 
rna adaptive regrouping forms groups pes largest smallest group weights 
example pe pe largest smallest pe weights form group 
group formed remaining pes 
inside groups rpa algorithm applied 
weights resampling calculated step pseudocode 
method utilizes load balancing algorithm simple greedy algorithm associates heavily lightly loaded groups 
main disadvantages rna adaptive regrouping groups contain pes connections pes local general 
distributed rna local exchange rna regrouping rpa algorithm performed inside groups particle routing process random done smaller set particles 
randomness particle routing difficult pipelining particle routing sampling steps 
example rna algorithm local exchange shown 
resampling done inside pe particles exchanged deterministic way neighboring pes 
routing done local communication 
amount particles sent pes fixed defined advance 
example 
important difference comparison rna regrouping particles routed pes group non deterministically 
groups formed pe weights resampling set local communication give rise large number periods full resampling achieved restricts level parallelism 
communication pes local choice architectural parameters similar case rna regrouping 
local communication give rise large number periods full resampling achieved restricts level parallelism 
effects resampling obtained estimates sirfs output estimate resampling calculated states particles arbitrary function represents normalized importance weight 
parallel implementation estimate written form represents expected value distribution th pe 
es applying distributed rpa form represents number times particle replicated resampling estimate applying distributed rna form number replications particle calculated easy show equal means estimates unbiased 
result expected types sampling due theorem cochran claims stratum sample estimate unbiased estimate unbiased estimate population mean 
sirfs full resampling considered special cases rna algorithm 
case resampling performed inside pe 
second case resampling performed single particle 
input output resampling particle resampling 
easy compare var general 
observed simulations difference variances weights equally distributed pes 
variance rna algorithm greater pe non zero weights 
problem resolved exchanging particles pes resampling deterministically step rna algorithm 
performance analysis section performances sequential particle filter particle filter distributed rna local exchange different number pes compared 
particle filters applied bearings tracking problem model initial conditions 
mean square error mse percentage diverged tracks chosen performance metrics 
consider track diverges importance weights negligible values mean square error outside pre defined limits 
architectural model chosen particle filter distributed rna cube torus type network 
consider ary ary ary torus networks 
model assumed pe single input output port communication protocol full duplex 
deterministic particle routing done way particles sent pe pe left 
way particles routed statically scheduled communication pattern 
simulation results shown 
see mses comparable 
important result initial conditions model time instant small amount particles survives 
deterministic routing solves problem having pe particles large weights rest negligible weights 
particle filter architectures distributed resampling distributed rpa architectures possible architecture distributed rpa pes allows pipelining particle routing step sampling step shown 
main idea store particles routed pes dedicated memories cu fast interface capable reading particles cu routing pes clock cycle 
particles replicated result resampling stored local memories surplus particles particles stored cu memories example memory mem lost track average mse coordinate number particles number particles sequential pes pes pes sequential pes pes pes percentage divergent tracks mse versus number particles different levels parallelism 
case pes rna local exchange applied 
mem pe mem pe mem pe mem pe mem mem mem global interconnection network mem mem mem local interconnection network mem mem mem mem mem mem architecture sirf distributed rpa pes 
cu implemented support pipelining particle routing sampling steps 
store surplus particles pe routed pe 
shortage particles reads particles interface connected memories 
routing performed steps 
particles pes particle surplus sent cu global interconnection network 
routing performed block inside cu buses bi 
connected corresponding memories bus acts master bus 
particles transferred destination pes global interconnection network 
size memories determined worst case pe acquires particles pe words word consists particles replication factors 
memory requirements words times sequential case 
timing diagram pe particle shortage communication cu 
resampling performed steps 
cu performs inter resampling sends output number particles cu calculates amount data transferred pes 

pes perform intra resampling particles stored local memory surplus sent cu 

particles allocated corresponding memories 
pes information particles routed cu 

sampling step pe reads particles local memories 
cu pe interconnection network cu start instant sample importance intra resampling cycles cycles cycles inter resampling start instant sample importance cycles routing cycles resampled particles timing diagrams sirf distributed rpa 
communication interconnection network shown shortage particles 
pes shortage particles acquire rest particles shown 
architecture execution time close minimum execution time expense increased resources 
parallel buses pes cu parallel buses inside cu 
area increased particles additionally stored inside cu 
clock speed limited memory access complexity cu 
design methodology implementation results distributed rpa asic 
distributed rna architectures distinguish architectures sirfs utilize rna 
particle filter architecture rna algorithms pes 
connections local especially suitable rna adaptive regrouping 
lines represent buses particle routing 
algorithm running cu configures switches pes access bus time 
case rna fixed regrouping switches configured fixed order 
example switches configured sequence repeated 
rna adaptive regrouping switches configured connect pes largest smallest weights 
rna local exchange run architecture 
simpler architecture shown 
network topology chosen mesh 
network static local interconnections 
cu simple functions collecting partial sums weights outputs returning final sum weights pes control 
cu connected pes single bus 
rna adaptive regrouping applied pes physically connected 
pe pe cu pe pe architectures particle filters pes support rna algorithms support rna adaptive regrouping 
architectures complex higher level parallelism 
scalable architecture support methods rna regrouping adaptive fixed asic implementation 
space exploration distributed particle filter rna local exchange area speed distributed particle filter rna local exchange particle allocation replication factor estimated bearings tracking problem 
parameters model 
range interest restricted region 
benchmark chosen hardware platform xilinx virtex ii pro 
resources analyzed combination number logic slices multiplier blocks memory bits 
finite precision approximation variables performed systemc language 
particles represented bits sign bit bits left bits right decimal point weights represented bits bit decimal bits fractional part 
complex mathematical functions implemented cordic gaussian random number generator implemented box muller method 
implementation spatially concurrent order achieve maximum speed 
execution time functions latency clock period lp tclk ns 
number particles particle filter uses determines execution time filter 
area graph bounded bold line represents design space area virtex ii pro family 
smaller design space determined logic blocks increases level parallelism large memory size 
pe pe cu pe pe area level parallelism memory bits mbits multiplier blocks number slices xilinx chip fits design xc vp xc vp xc vp xc vp table number memory bits slices block multipliers distributed particle filter implementation rna local exchange 
virtex ii pro chips fitted particle filter parameters listed 
star shows parameter determined choosing chip 
represents number particles particle filter running size memory design 
means implementation supports particle filters particles 
distributed implementation particles implemented memory banks 
memory requirements analyzed different topologies 
black line memory requirements represented case choose memories exact size 
grey line memory requirements ii pro shown 
design space virtex ii pro devices bold lines represent available block ram smallest xc vp largest xc vp virtex ii pro devices 
example design space xc vp bottom bold line 
virtex ii pro fpga memories implemented coarse kbits block rams 
example bit weights fit block ram memory occupy block ram modules 
case utilization block ram modules low portion occupied 
example amount block rams necessary curves different overlapped 
approximate number block ram modules calculated bm ks ks number bits size block ram memory kbit 
consider box muller generator implemented block ram module random number generators parallel pe 
additional memory due usage memory random number generators especially contributes number bits low large 
interesting compare number memory slices number number bits corresponding values virtex ii pro family shown table 
table number particles 
number slices components dataflow calculated multiplied factor order take account controllers unused slices 
represents parameter memory number slices multipliers blocks determines choice xilinx chip 
table corresponding xilinx chip shown 
lower level parallelism design memory dominated higher level parallelism logic dominated 
design pes fit commercial virtex ii pro fpgas 
sample period memory requirements mbits virtex ii pro design space number pes number pes xc vp xc vp number particles number particles execution time memory requirements versus number pes rna local exchange sirf particles 
final remarks section methods distributing resampling step suitable distributed real time fpga implementation proposed 
practical guidelines choosing resampling method depend primarily desired performance communication pattern complexity cu 
sirf performance centralized resampling rpa algorithm sequentially implemented sirf 
advantages centralized resampling rpa algorithm faster simpler cu 
hand rna algorithm trades sirf performance speed improvement 
rpa algorithm choice necessary preserve performance significant increase complexity 
communication pattern rpa algorithm non deterministic 
requires point point network achieve minimum execution time 
rna algorithm achieve minimum execution time architecture consists local connections 
communication pattern rna algorithm regrouping rna algorithm local exchange rpa algorithm 
size group larger rna algorithm regrouping suffers non deterministic communication pattern 
amount particles exchanged inside groups smaller rpa algorithm 
complexity cu rpa algorithm high implement complex routing protocol point point network 
cu rna algorithm local exchange simple responsible particle routing resampling 
rna algorithm regrouping control units group groups contain pes 
speed important required design time low low complexity cu scheduling protocol interconnection networks rna algorithm local exchange preferred solution 
chapter architectures gaussian particle filters chapter analyze algorithmic architectural characteristics gaussian particle filters gpfs 
comparison traditional sirfs gpfs show great potential high speed implementation exploiting operational concurrency 
gpfs inherently suitable parallel implementation multiple pes single cu 
characteristics propose modified gpf algorithm suitable parallel hardware realization 
modified algorithm need storing particles memories successive recursions 
property particular interest developing flexible architectures particle filtering memory size constraint number particles 
computational complexity gpfs higher sirfs gpfs implemented resampling procedure 
entails higher concurrency exploited greater throughput 
chapter organized follows 
section presents proposed modifications gpfs analysis algorithmic complexity gpfs terms temporal spatial concurrency 
section discusses implementation issues sequential concurrent implementations gpfs 
data flow analysis high level architecture characterization section 
lower level comparison sirfs gpfs terms resource utilization speed section 
algorithmic modifications complexity characterization temporal concurrency gpf algorithms pseudocode pseudocode 
observed gpf contains loops iterations loop calculation step pseudocode 
results step steps values states weights saved memory processing 
steps number iterations suitable loop fusion 
steps pseudocode easily fused 
weight normalization step requires weights known order form sum weights appropriate loop fusion 
modify algorithm normalization necessary non normalized weights calculating mean covariance coefficients 
course mean covariance coefficients scaled 
step fused original form mean known computation covariance coefficients particles processed 
step rewritten follows wn 
second term right constant calculated outside loop 
term modified step fused previous steps 
fused steps pseudocode 
note particles need saved advantageous hardware implementation 
additional processing outside main loop necessary pseudocode involves final computation mean covariance step calculation final covariance coefficients step cholesky decomposition step 
cholesky decomposition needed gaussian random number generation step pseudocode utilizes cn square root covariance matrix 
non parallel implementations single pe step pseudocode necessary 
spatial concurrency spatial concurrency exploited developing parallel architectures mapping independent operations multiple hardware units operate parallel 
spatial concurrency exploited multiple pes execution 
seen gpf algorithm sample weight calculation steps particles independent particles 
operations iteration loop pseudocode independent 
loop loop carried dependence 
loop level parallelism exploited increasing throughput 
done having multiple independent pes processing fraction total number particles 
number pes defines degree parallelism 
maximum degree parallelism obtained pe consists particle 
dependence exists iterations mean covariance estimation steps 
parallelize step partial sums mean covariance calculated pes added cu step pseudocode 
pseudocode quantities subscript represent result operations th pe 
final addition partial sums pes communicate cu amount data transferred corresponds dimension state space model 
summary modified algorithm perform pes concurrent operations input observation zn previous estimates matrix cn cn mean covariance prior information 
setup sum weight mean covariance elements th pe 
method 
draw conditioning particle xn obtain 
draw sample xn obtain 
calculate weight zn 
update current sum weights 
update xn kn pseudocode part gpf algorithm runs parallel pes loop fusion applied 
symbol denotes th pe denotes total number pes 
input kn kn setup sum weights wn initial mean covariance 
method 
collect update central sum weights mean covariance wn wn 

scale mean covariance wn wn 
compute covariance estimate 
compute cholesky decomposition matrix order obtain cn 
pseudocode part gpf algorithm runs sequentially cu exchanges data pes 
particles cu carry sequential post processing 
computational complexity characteristic comparison number operations memory requirements recursion sirfs gpfs shown table 
number operations particle generation weight calculation filters 
additional complexity gpfs added due generation conditioning particles computation mean vector covariance matrix states 
stress complexity steps related dimensionality model ns 
gpf number multiplication operations computations mean vector covariance matrix equal ns ns sirf number multiplications calculation mean ns 
hand memory requirements sirfs dominant 
increase increase dimension model shown 
table ns data particle storing ns states weight index result resampling process 
algorithms gaussian random multiplication operations memory requirements number generator drawing conditioning particles computation estimate gpf nsm ns ns ns ns sirf nsm nsm ns table comparison number operations memory requirements sirfs gpfs pe 
sampling period particle filter analyzed 
operations particle generation weight calculation steps considered 
pe pe cu parallel gpf model pes 
amount data exchange required sirfs gpfs consider sirfs rpa described section 
filters operations data dependencies mapped pes order allow parallel processing 
gpfs operations described pseudocode 
sirfs parallel resampling particle generation weight calculation partial resampling mapped pes 
parallel architecture gpfs consists pes cu shown 
pe pe ns dimensional model amount data acquired cu pes ns ns ns ns ns data covariance matrix symmetric ns number data mean vector corresponds sum weights cu sends back pes calculated mean cholesky factorization cn final covariance matrix 
additional ns ns ns data 
data sent pe beneficial mechanisms allow simultaneous transfer data cu pes 
bearings tracking problem ns number data sent pe cu number data sent cu back pes 
data exchange depicted 
number data transferred interconnection network smaller case sirfs fixed time 
feature greatly increases scalability filter ensures data exchange dominant operation gpfs 
contrast worst case sirfs particles sent interconnection network 
parameter effects algorithmic parameters effects architectural parameters sirf gpf sequential implementation spatial implementation number operations linear increase quadratic increase sample period area number particles increase model dimension sampling period sampling period area memory requirements data exchange proportional proportional sampling period sampling period finite precision increase number sampling period sampling period area word length operations table effect model increase algorithmic architectural parameters 
complexity characteristics sirfs gpfs summarized table 
table effects increase model dimension different particle filter parameters shown resources requirements 
summary effects follows 
gpfs number operations particle increases quadratically model dimension 
significantly affects complexity units drawing conditioning particles covariance estimation complexity cu 
sirfs number operations increases linearly model dimension 

number particles needed achieve required accuracy increases model dimension affects sampling periods sirfs gpfs 
sirfs affected additional time accessing memories 
significant area increase spatial implementation sirfs physical memories necessary store particles weights 

gpfs data exchange requirements increase quadratically model dimension amount data transferred pes cu orders magnitude lower sirfs 
data exchange pattern gpfs deterministic 

gpfs complexity mathematical operations increases resulting large word length finite precision processing 
cases floating point implementation feasible option implies requirements increased area increased sampling period 
finite precision processing sirfs affected increase model dimension 
implementation issues sequential processing sequential processing applied dsps concurrency exploited compiler depends number arithmetic units processors 
computation complexity mainly dominated number mathematical computations memory access 
sequential implementation sirfs faster mathematical operations fewer simpler operations gpfs 
illustrate consider bearings tracking problem model dimension dsp 
shows curves correspond execution times processing particles sir gpf algorithms 
curves represent sampling period function number particles analog devices floating point dsp adc 
similar results obtained implementation texas instruments processors 
sequential implementations sampling period increases linearly number particles 
sampling period ms number particles sampling period gpfs sirfs versus number particles 
filters implemented analog dsp adc 
moderate number particles deduce sirfs achieve faster sampling periods gpfs 
number particles large dsp satisfy high throughput constraint multiple processors utilized 
realistic assume multiprocessor configurations transfer particle interconnection network takes time clock period 
time increase shared memory 
data exchange interconnection network bottleneck sirfs gpfs better candidates multiprocessor applications 
gpfs higher scalability sirfs flexible terms maximum number particles 
gpf sirf concurrent processing temporal spatial concurrency exploited asic fpga implementations 
platforms particle filters executed dedicated operators duplication hardware 
dataflow parallel gpf implementation 
important note processing logic element gpf critical path 
sirf hand output calculation critical path done parallel sample step 
drawing conditioning particles generation particles weight calculation updating mean covariance collect data mean cov calculation cholesky dec drawing conditioning particles generation particles weight calculation updating mean covariance dataflow parallel gpf 
shows timing diagram latencies various operations gpfs including data transfer 
outputs logic blocks data flow generated clock speed output updating mean covariance block step pseudocode output cu generated particle filter sampling speed 
minimum sampling period achieved parallel gpfs different number particles processing elements 
sirfs resampling distributed pes minimum sampling period achieved lsir tclk lsir latency processing pes latency communication particle allocation 
case gpfs minimum sampling period lgp tclk lgp net latency critical path 
latency lgp li accounts start draw conditioning particles draw particles calculate update sum weights states collect partial sums cu calculate mean cov pe timing diagram gpf 
cholesky decomposition latencies various blocks inside pes 
constant term li accounts latency cu data exchange pes cu 
see gpfs larger constant latency cu large number particles latency gpfs smaller sirfs 
primarily sirfs resampling required sequential dependent result processing particles pe requires particle redistribution execution time proportional simulations assume clock period equal filters tclk ns lgp lsir sirf 
minimum execution time sirfs black line gpfs gray line 
large total latency affects scalability gpfs small 
hand large sampling period gpfs twice smaller sirfs 
gpfs appropriate high speed applications require large number particles platforms resources spatial parallel implementation 
sample period number processing elements sirf sirf sirf gpf gpf gpf minimum sampling period versus number pes parallel gpfs sirfs 
spatial implementation particle filters assumed 
architectures resource requirements consider spatial implementations gpfs applied bearings tracking mapping operation hardware resource 
bit representation particles chosen 
mean square error mse tracking performance evaluation criterion fixed precision analysis error due finite precision arithmetic kept limits floating point value 
simulations indicate steps pseudocode sensitive finite precision effects 
mse defined limits achieved bit representation operations steps 
step pseudocode steps pseudocode sensitive finite precision effects 
operations cholesky decomposition requires input matrix positive definite condition may satisfied representations small number bits 
order alleviate problem bits precision necessary operations step pseudocode 
similarly operations executed cu pseudocode require bits 
gpf major computational units generation conditioning particles particle generation weight computation mean covariance calculation central unit 
operation block diagram gpf shown 
gpf works way operation cu dependent result operation pes vice versa 
pes cu operate simultaneously 
particles processed pes send computed mean covariance matrix sum weights wn cu 
cu starts operations receiving data pes pes remain idle time 
operation cu complete results sent pes recursion started 
particle generation step comprises conditioning particles producing final particles inputs steps weight computation updating mean covariance matrix 
weight computation step takes input observations computes bit weights 
step calculation exponential functions attained coordinate rotation digital computer cordic algorithms 
output internal operations updating mean covariance matrix represented bits 
bit outputs generated recursion assume single bit bus connects pes cu 
blocks generation conditioning particles particle generation weight computation consume produce particles 
block updating mean covariance mcc consumes particles producing sampling period 
buffering necessary mcc block start computing mean covariance coefficients corresponding weights computed weight computation block 
latency buffer equal latency weight computation block 
generation conditioning particles gcp step decomposed covariance matrix cn mean obtained cu generation conditioning particles 
matrix cn triangular matrix number multiplications step 
multipliers pipelined operate concurrently producing conditioning particles clock cycles 
latency multiplier adders 
outputs vx vy computed different number operators introduce additional delay different state order obtain conditioning particles time instant output 
step requires random number generators 
produce gaussian generation conditioning particles generation particles buf weight calculation updating mean variance block diagram gpf 
var central unit random numbers properties required model 
look table approach detailed 
block diagram gcp step shown 
initially mean decomposed covariance elements obtained input pins cu 
requires additional control multiplexor input block 
particle generation weight computation typical operations processed particle generation weight computation steps identical sirf particles need stored memories 
architectures steps section 
mean covariance calculate mcc mcc step partial covariance matrix mean vector calculated 
number multiplication operations equal ns ns ns ns represents dimension model 
bearings tracking problem ns multiplications required 
outputs accumulated accumulators necessary 
blocks operate concurrently particles clock cycles 
latency critical path consists latencies multipliers accumulator 
central unit cu inputs outputs cu produced sampling period 
output output particle filter cn inputs gcp block particle filtering recursion 
sum particles wn cu mean covariance elements mcc computed non normalized weights elements properly scaled cu 
operations cu done sampling period time multiplexing performed order preserve hardware resources 
divider scaling mean covariance elements step pseudocode multiplier adjusting covariance elements step pseudocode 
number divisions ns mean covariance matrices need scaling sum weights 
adjusting covariance elements requires addition multiplication operations 
cholesky decomposition block inputs outputs 
performs complex operations square roots divisions 
processing rng rng rng rng vx vy block diagram generation conditioning particles 
operation depends result previous cholesky decomposition suitable time multiplexing 
comparisons tradeoffs sirfs gpfs energy speed constraints modeling high level energy performed module level estimating power functions elements associated module adders multipliers registers memories 
power module initially estimated xilinx spreadsheet power tools verified xilinx 
energy single pe implementation sirfs gpfs calculated particle filter sampling period shown respectively 
filters applied bearings tracking problem 
energy estimated various number particles various maximum sampling frequencies fs khz 
minimum depth pipelining functional blocks satisfies sampling frequencies calculated applied order reduce energy 
number bits sirf variables steps gpf 
step vx vy vx vy block diagram mean covariance computation step 
pseudocode steps pseudocode represented fixed point bits 
operating clock frequency defined speed cordic slowest individual unit data path 
analysis clock frequency set mhz 
clear smaller number particles lower frequencies sirfs dissipate energy gpfs 
important note increase number particles energy sirfs comparable higher energy gpfs particles 
reasons faster increase energy sirfs 
sirfs memory dominant increase number particles size memory increase results higher energy 
secondly number operations cu sirfs function gpfs number operations cu constant energy cu sirfs increases linearly energy gpfs stays constant 
sirf implementations single pe achieve higher requirements processing particles khz 
energy parallel implementation sirfs gpfs utilize multiple pes 
energy sirfs calculated worst case data exchange pes cu corresponds transferring particles 
data exchange modeled shared memory 
see energy sirfs lower energy gpfs number particles acc acc acc acc mac mac mac mac mac mac mac mac mac mac vx vy var var var var var var var var var var energy mj buf div buf buf var cholesky buf div buf buf buf dec block diagram time multiplexed central unit gpfs 
number particles sirf energy khz sirf khz sirf khz sirf khz sirf energy mj number particles gpf energy energy versus number particles sirfs gpfs single pe different sampling rates 
low 
particles khz sampling speed energy sirfs higher energy gpfs 
area requirements fpga percentage required resources hardware blocks pes gpf shown 
xilinx virtex ii pro fpga platform estimate resource area requirements 
pes calculate covariance mean coefficients clock cycle fully spatial design 
considered resources number occupied slices number block multipliers 
reason domination step covariance matrix mean estimated large number bits fixed point representation 
able calculate mean covariance coefficients clock cycle multipliers required 
bit representation multiplier occupies multiplier blocks bits virtex ii pro chip 
lower dimensional models ratio resources hardware blocks look different 
example dimensional tracking models covariance coefficients mean coefficients necessary estimation importance step 
case area importance step dominate 
khz gpf khz gpf khz gpf khz gpf energy mj number particles sirf energy khz sirf khz sirf khz sirf energy mj number particles gpf energy khz gpf khz gpf khz gpf energy versus number particles sirfs gpfs implemented pes different maximum sampling rates 
operations take place cu sequential due data dependency computationally intensive tasks square division multiplication addition 
cu candidate time multiplexed implementation hardware resources shared time various operations 
implementation minimizes hardware degrading execution throughput 
estimate sampling frequency gpf reduced time multiplexed implementation comparison spatial implementation 
area saving cu 
tight performance criterion necessary precision gpf cu bits 
coefficients covariance matrix small truncation may entail violation positive definiteness covariance matrix 
matrix decomposed recursion proceed 
cu realized floating point library xilinx ii pro estimating area latency clock frequency slowest floating point block divider twice speed slowest synthesized fixed point block bit precision 
gpf implemented single pe particles sampling frequency decreased times latency increased times logic area increased times comparison area particle filter time multiplexed cu uses bit precision fixed point arithmetics 
floating point implementation alternate solution maintenance precision key issue 
throughput area affected significantly 
evaluated compared area requirements sirfs gpfs 
resource requirements represented number virtex ii pro logic slices block rams 
area evaluated various number particles various sampling frequencies 
number pes adaptively changed particle filters meet sampling frequency requirements 
example sirfs fs choose implementation pes necessary satisfy requirements 
gpf algorithm contain memory storing particles 
uses block ram random number generator implementing box muller method 
clear sirfs memory dominated gpfs logic random number gen time update sample importance estimate slices multiplier blocks percentage number slices multipliers block pes estimated xilinx virtex ii pro chips 
dominated 
number logic slices number particles area sirfs gpfs khz sirf khz sirf khz sirf khz gpf khz gpf khz gpf number block rams number particles memory sirfs gpfs khz sirf khz sirf khz sirf khz gpf khz gpf khz gpf area number slices number block rams versus number particles 
area evaluated different sampling frequencies necessary number pes particle filters satisfy sampling frequency requirements 
chapter chapter summarizes principle ideas contributions dissertation 
research efforts attempt address set challenging problems rise new set problems solved 
chapter starts summary dissertation followed directions research 
summary despite amazing progress signal processing field difficult scientific engineering problems resolved traditional signal processing algorithms 
particle filters latest innovations attempt bridge existing gap doable filtering theory 
applied fields including wireless communications navigation systems sonar robotics shown outperform traditional filters complex practical scenarios 
particle filters computationally intensive main drawback 
dissertation physically feasible vlsi architecture particle filters developed emphasis high speed design 
joint algorithmic architectural design adopted 
basic algorithmic challenges sir algorithm reducing computational intensity algorithm accelerating process resampling 
proposed new resampling algorithms processing time random suitable hardware implementation 
new resampling algorithms reduce number operations memory access allow overlapping resampling step weight computation particle generation 
algorithms developed aim improving hardware implementation algorithms considered resampling methods simulations standard computers reduce execution time 
particle filtering algorithms considered gpf require resampling simpler implementation 
particle filtering algorithms modified higher speed lower memory requirements achieved proposed architectures 
memory requirements sampling period modified sirfs gpfs shown table 
modified sirfs contain loops modified gpfs 
spatial implementations operation mapped dedicated hardware block gpf twice faster sirf execution time recursion 
gpf implemented need storing particles 
memory requirement number memory access reduced modified sirf 
reduction especially obvious multi dimensional models approaches fold reduction dimension increases 
hand computational complexity gpf higher 
example requires times random number generators multipliers 
parameter original sirf modifies sirf modified gpf sampling period particle weights memory nsm ns memory access ns ns computational complexity medium high ns rng mul ns rng mul ns rng mul table comparison parameters particle filtering algorithms 
random number generators denoted rng mul respectively 
number multipliers calculated bearings tracking problem values generic particle filter 
sirf implemented fpga bearings tracking problem sampling frequency achieved implementation particles khz 
times increase comparison implementation state art dsp processors 
additional speed improvements achieved parallel implementation 
main algorithmic architectural challenges reducing communication requirements resampling step reducing complexity central unit 
new parallel resampling algorithms communication interconnection network deterministic developed corresponding architectures communication protocols proposed 
parallel algorithms architectures gpfs developed 
communication requirements gpfs significantly lower 
number data sent proportional sirf proportional nsm 
communication requirements sirf high modifications allow deterministic communication possible overlap time particle exchange particle generation step 
way sampling period sirfs increased due communication proportional complexity central unit gpf high perform cholesky decomposition complexity central unit new parallel sirf algorithm low responsible communication 
extensions extended directions including comparison different types particle filtering algorithms developing automated procedure floating fixed point conversion particle filters developing application domain specific reconfigurable architectures particle filters 
types particle filters commonly 
instance mixture kalman filter wireless communications joint channel estimation symbol detection 
possible research direction involve comparison different types particle filters software hardware implementation point view 
interestingly type particle filters imposes different requirements efficient hardware implementation 
example mixture kalman filter uses smaller number particles sirf 
main emphasis design exploiting functional data parallelism benefits parallel architectures described chapter 
important part hardware implementation particle filter conversion floating fixed point arithmetic 
mapping operations hardware block performed important small number bits fixed point representation order reduce area requirements 
finite precision analysis performed approach similar described 
ad hoc method requires extensive simulations 
change input parameters simulation run 
reconfigurable domain specific particle filter implementations require faster reliable finite precision analysis 
main goal dissertation high speed implementation particle filters application specific architectures 
optimized particular algorithm provide highest throughput 
architectures flexible 
interesting research direction implement specific processor particle filters 
processor combination dedicated hardware blocks programmable blocks achieve higher throughput commercial dsps provide certain level flexibility 
domains applications considered tracking navigation 
processor able handle different tracking navigation models 
real time signal processing algorithms including particle filters blocks data frames 
systems level hierarchy obvious data frames processed logic blocks global level elements frame processed loop fashion local level 
currently process developing reconfigurable architectures particle filters incorporate block level pipelining 
block level pipelining achieve objectives 
possible maintain concurrency processing block providing correct synchronization processing blocks proper execution 
second control signals data clock local hardware implementation easier terms maintaining performance minimizing clock skews data routing 
addition change logic affects buffer configuration controller reconfigurable design core reuse possible 
bibliography west bayesian dynamic factor models portfolio allocation journal business economic statistics 
sorenson nonlinear bayesian estimation gaussian sum approximation ieee transactions automatic control pp 

moore optimal filtering 
englewood cliffs nj prentice hall 
hong djuri architectures memory schemes sampling resampling particle filters accepted publication th digital signal processing rd signal processing education workshops 
bergman recursive bayesian estimation navigation tracking applications 
ph thesis department electrical engineering link ping sweden dissertation 
bernardo smith bayesian theory 
new york john wiley sons 
best gilks dynamic conditional independence models markov chain monte carlo methods journal american statistical association vol 
pp 

hong djuri finite precision effect performance complexity particle filters bearing tracking proceedings th ieee asilomar conference signals systems computers pacific grove ca november 
hong djuri performance complexity analysis adaptive particle filtering tracking applications proceedings th ieee asilomar conference signals systems computers pacific grove ca november 
djuri hong new resampling algorithms particle filters proceedings icassp hong kong 
djuri hong resampling algorithms architectures distributed particle filters submitted ieee transactions signal processing 
djuri hong resampling algorithms particle filters suitable parallel vlsi implementation proceedings ciss baltimore md 
djuri hong resampling algorithms particle filters computational complexity perspective accepted publication eurasip journal applied signal processing 
djuri hong algorithmic modification particle filters hardware implementation accepted publication eusipco 
djuri hong design study practical physical implementation gaussian particle filters submitted ieee transactions circuits systems 
bucy bayes theorem digital realization nonlinear filters journal sciences vol 
pp 

carlin polson stoffer monte carlo approach nonnormal nonlinear state space modeling journal american statistical association vol 
pp 

carpenter clifford improved particle filter non linear problems iee proceedings radar sonar navigation vol 
pp 

carter kohn gibbs sampling state space models biometrika vol 
pp 

carter kohn markov chain monte carlo conditionally gaussian state space models biometrika vol 
pp 

chen wang liu monte carlo filter adaptive detection fading channels proceedings asilomar conference signals systems computers asilomar ca 
chen wang liu adaptive joint detection decoding flat fading channels mixture kalman filtering ieee transactions information theory vol 
pp 

clapp godsill fixed lag smoothing sequential importance sampling bayesian statistics eds 
bernardo berger dawid smith oxford oxford university press pp 

clapp godsill improvement strategies monte carlo particle filters sequential monte carlo methods practice doucet de freitas gordon eds new york springer verlag 
sampling techniques london wiley nd edition 
compton hauck reconfigurable computing survey systems software pdf acm computing surveys vol 

pp 

june 
cray research cray mp cray mp computer systems egan mn 
del moral lyons non linear filtering branching interacting particle systems markov processes related fields vol 
pp 

culler singh gupta parallel computer architecture hardware software approach morgan kaufmann publishers st edition august 
huang curse dimensionality particle filters fifth onr workshop target tracking sensor fusion newport ri june 
digital core design pipelined floating point libraries available www dcd pl analysis synchronous dynamic load balancing algorithms advances parallel computing vol pp 

devroye non uniform random variate generation springer verlag 
djuri sequential estimation signals model uncertainty sequential monte carlo methods practice doucet de freitas gordon eds new york springer verlag 
djuri zhang huang guez particle filtering ieee signal processing magazine vol 
pp 

doucet godsill andrieu sequential monte carlo sampling methods bayesian filtering statistics computing pp 

doucet godsill west monte carlo filtering smoothing application time varying spectral estimation proceedings international conference acoustics speech signal processing istanbul turkey 
doucet de freitas gordon eds sequential monte carlo methods practice new york springer verlag 
duncan survey parallel computer architectures ieee computer pp 

sequential monte carlo methods filter theory phd thesis merton college university oxford 
monte carlo concepts algorithms applications springer series operational research springer verlag st edition 
fr applied state space modeling non gaussian time series integration kalman filtering statistics computing vol 
pp 

peng gajski system design practical guide specc kluwer academic publishers june geweke acceleration monte carlo integration bayesian inference journal econometrics vol 
pp 

geweke bayesian inference econometric models monte carlo integration econometrica vol 
pp 


danger 
efficient fpga implementation gaussian noise generator communication channel emulation proceedings ieee conference lebanon 
gilks moving target monte carlo inference dynamic bayesian models journal royal statistical society vol 

godsill rayner digital audio restoration statistical model approach 
new york springer verlag 
gordon salmond smith novel approach nonlinear non gaussian bayesian estimation iee proceedings vol 
pp 

behavioral level guide property design characterization ph thesis university california berkeley 
haller dedicated vlsi architectures adaptive interference suppression wireless communication systems circuits systems wireless communications kluwer academic publishers 
harvey forecasting structural time series models kalman filter 
cambridge cambridge university press 
hennessy patterson computer architecture quantitative approach rd edition morgan kauffmann publishers may 
corbett message passing algorithms simd torus coteries acm sigarch computer architecture news pp 
july 
hong andp djuri efficient fixed point implementation residual systematic resampling scheme high speed particle filters accepted publication ieee signal processing letters 
hong chin djuri design implementation flexible resampling mechanism high speed parallel particle filters submitted journal vlsi signal processings 
hong djuri design complexity comparison method loop signal processing algorithms particle filters accepted publication iscas 
isard blake condensation conditional density propagation visual tracking international journal computer vision vol 
pp 


ito xiong gaussian filters nonlinear filtering problems ieee automatic control vol 
pp 

stochastic processes filtering theory new york academic press 
jouppi wall available instruction level parallelism superscalar machines rd international symposium architectural support programming languages operating systems pp 

julier uhlmann durrant whyte new method nonlinear transformation means covariances filters estimators ieee transactions automatic control vol 
pp 

kalman new approach linear filtering prediction problems journal basic engineering transactions asme ser 
pp 

kalman bucy new results linear filtering prediction theory journal basic engineering transactions asme ser 
pp 

kanazawa koller russel stochastic simulation algorithms dynamic probabilistic networks proceedings eleventh annual conference uncertainty ai uai pp 

design space exploration stream dataflow architectures phd thesis delft university technology 
kim ki il kum sung fixed point optimization utility digital signal processing programs ieee transaction circuits analog digital signal processing pp 
vol 
nov 
kitagawa non gaussian state space modeling nonstationary time series journal american statistical association vol 
pp 

van dijk bayesian estimates system equation parameters application integration monte carlo econometrica vol 
pp 

kong liu wong sequential imputations bayesian missing data problems journal american statistical association vol 
pp 

djuri gaussian particle filtering proceedings ssp singapore 
monte carlo dynamic state space models applications communications phd 
thesis stony brook university dec 
kramer sorenson recursive bayesian estimation piece wise constant approximations automatica vol 
pp 

kumar measuring parallelism computation intensive scientific engineering applications ieee transactions computers vol 
pp 

kuck structure computers computations john wiley new york ny 
liu chen blind deconvolution sequential imputations journal american statistical association vol 
pp 

liu chen sequential monte carlo methods dynamic systems journal american statistical association vol 
pp 

liu chen wong rejection control sequential importance sampling journal american statistical association vol 
pp 

liu chen wong theoretical framework sequential sampling resampling sequential monte carlo methods practice pp new york springer verlag 
ljung theory practice recursive identification 
cambridge ma mit press 
maccormick blake probabilistic exclusion principle tracking multiple objects proceedings international conference computer vision pp :10.1.1.126.7850

approximate non gaussian filtering linear state observation relations ieee transactions automatic control vol 
pp 

kalman filter models journal american statistical association vol 
pp 

miller prasanna kumar stout meshes reconfigurable buses proc 
fifth mit conf 
advanced research vlsi pp 

choi jang prasanna model methodology application specific energy efficient data path design fpgas ieee th international conference application specific systems architectures processors asap 
le gland improving regularized particle filters sequential monte carlo methods practice doucet de freitas gordon eds new york springer verlag 
nicolau fisher measuring parallelism available long instruction word architectures ieee transactions computers vol 
pp 

progressive correction regularized particle filters proceedings rd international conference information fusion 
vlsi digital signal processing systems design implementation john wiley sons 
patterson hennessy computer architecture quantitative approach morgan kaufmann san mateo ca 
architectures digital signal processing john wiley sons ny 
pitt shepard filtering simulation auxiliary particle filters journal american statistical association vol 
pp 


pole west efficient numerical integration dynamic models research report department statistics university warwick 
andrieu doucet fitzgerald particle filtering demodulation fading channels non gaussian additive noise ieee transactions communications vol 
pp 

rabaey pedram eds low power design kluwer academic publisher 
rabaey gass brodersen vlsi design implementation fuels signal processing revolution ieee signal processing magazine pp january 
raghunathan jha dey high level power analysis optimization kluwer academic publisher 
richards show chips architectures algorithms reflections exponential growth digital signal processing capability submitted ieee signal processing magazine 
ripley stochastic simulation new york wiley 
rubin bernardo de groot lindley smith bayesian statistics oxford university press pp 
pipelined parallel computer architectures harper collins college publisher 
sorenson recursive bayesian estimation gaussian sums automatica vol 
pp 

sorenson recursive estimation nonlinear dynamic systems bayesian analysis time series dynamic models ed 
new york dekker 
systemc language manual available open systemc initiative systemc org 
tanizaki nonlinear filters estimation applications 
lecture notes economics mathematical systems vol 

new york springer verlag 
texas instruments tms dsp library programmers august 
flynn detection parallel execution parallel instructions ieee transactions computers vol 
pp 

van der merwe doucet de freitas wan particle filter advances neural information processing nips 
van embedded multimedia systems silicon www ics ele tue nl jef education 
cordic trigonometric computing technique ire trans 
electronic computing vol ec pp sept 
wang unified view cordic processor design application specific professors ch ec pp 
kluwer academic press 
west harrison dynamic generalized linear models bayesian forecasting discussion journal american statistical association vol 
pp 

wolfe automatic vectorization data dependence optimization parallel computers parallel processing supercomputing artificial intelligence hwang de groot eds ch 
mcgraw hill new york ny 
wolfe high performance compilers parallel computing addison wesley publishing 
adaptive receiver identification flat fading channels international conference acoustics speech signal processing 
monte carlo techniques problems optimal data processing automatic remote control vol 
pp 

xilinx virtex ii pro fpga functional description available www xilinx com june 

