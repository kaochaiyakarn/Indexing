forward stagewise regression monotone lasso trevor hastie jonathan taylor robert tibshirani walther march consider angle regression forward stagewise algorithms solving penalized squares regression problems 
efron 
proven angle regression algorithm small modification solves lasso constrained regression problem 
give analogous result incremental forward stagewise regression showing fits monotone version lasso 
study condition coefficient paths lasso monotone different algorithms coincide 
compare lasso forward stagewise procedures simulation study involving large number correlated predictors 
keywords regression lasso stagewise 
ams classification depts 
statistics health research policy sequoia hall stanford univ ca 
hastie stat stanford edu depts 
statistics stanford univ ca 
stat stanford edu depts 
health research policy statistics stanford univ stat stanford edu dept statistics stanford univ walther stat stanford edu lasso related methods lasso related methods lasso tibshirani method regularizing squares regression 
suppose predictor measurements xij pand outcome measurement yi observed cases lasso fits linear model criterion min yi xij xj subject tuning parameter large just gives usual squares estimates 
smaller values produce shrunken estimates components equal zero 
choosing thought choosing number predictors include regression model 
lasso select predictors subset selection methods 
smooth optimization problem variable subset selection applied larger problems large 
criterion leads quadratic programming problem standard numerical analysis methods solve 
shows example simulated data predictors details data generation model 
top panel shows coefficient profiles lasso solutions bound increased point full squares solutions obtained right 
notice piecewise linear nature lasso profiles 
efron 
exploited fact derive simple algorithm angle regression simultaneously solving entire set lasso problems values 
angle regression kind democratic version commonly forward stepwise algorithm 
forward stepwise regression starts coefficients equal zero enters model predictor correlated response variable replaced residual fit predictor correlated residual entered model 
process repeated predictors entered 
lasso related methods coefficients coefficients coefficients lasso lar forward stagewise norm standardized coefficient profiles simulated example 
norm computed standardized variables 
coefficients original scale details visible 
lasso starts differ lar broken vertical line grey coefficient passes zero 
forward stagewise starts differ lar lasso dotted vertical line grey coefficient goes flat turning back zero 
lasso related methods angle regression uses similar strategy enters predictor deserves coefficient predictor increased point predictor correlation current residual 
new predictor entered process continued 
algorithm gives details 
algorithm angle regression 
standardize predictors mean zero variance 
start residual 
find predictor xj correlated 
move squares coefficient xj competitor xk correlation current residual xj 

move direction defined joint squares coefficient current residual xj xk competitor xl correlation current residual 

continue way predictors entered 
steps arrive full squares solutions 
profiles lar shown middle panel 
look similar lasso solutions especially 
discrepancy place marked vertical broken line lasso profiles 
lar profile passes zero point lasso profile hits zero stays 
similarity coincidence 
turns modification lar procedure exactly produces set lasso solutions modification needed follows non zero coefficient hits zero drop active set recompute current joint squares direction 
lar algorithm extremely efficient requiring order computation single squares fit predictors 
angle regression takes steps get full squares estimates 
steps quite similar 
lasso related methods motivation angle regression arose efforts understand simpler procedure incremental forward stagewise regression relation lasso 
algorithm gives details 
bottom panel algorithm incremental forward stagewise regression 
start 
find predictor xj correlated 
update sign corr xj 
update jxj repeat steps predictor correlation shows coefficient profiles incremental forward stagewise regression 
notice similar lasso lar tend smoother 
kind forward stagewise procedure inspired boosting adaptive non linear function fitting method received attention see friedman 

boosting set variables large space binary trees selected shrunk added current model efron 
shown infinitesimal version forward stagewise procedure stepsize closely related angle regression 
particular limit incremental forward stagewise regression equivalent variant angle regression negative squares direction step algorithm 
suppose predictors model current residual ands signs correlations equal absolute value respectively 
move coefficients minimize ri xi xi subject 
turns positivity constraint needed variables simplicity illustration positivity occurs automatically 
forward stagewise monotone lasso contrast angle regression takes squares step signs 
non negativity arises fact step incremental forward stagewise procedure coefficient predictor increased direction correlation current residual 
fact result infinitesimal version forward stagewise regression shown 
henceforth refer infinitesimal version forward stagewise fs version algorithm incremental forward stagewise fs 
top left panel shows residual sum squares procedures function norm coefficient vector 
expected lasso curve sits 
summary see forward stagewise lar algorithms nearly solve penalized regression problem 
natural ask problems forward stagewise lar algorithms solving 
keith knight asked question discussion efron 

provide answers question 
main contributions characterize forward stagewise monotone version lasso extended space variables consisting variable negative 
study condition profiles methods monotone methods coincide 
compare lasso forward stagewise procedures simulation study involving large number correlated predictors 
forward stagewise monotone lasso section consider expanded representation lasso problem facilitates clearer understanding forward stagewise procedure 
predictor xj include negative version xj resulting expanded data set predictors 
matrix notation create expanded data matrix 
framework lasso problem forward stagewise monotone lasso argmin yi subject xij xij equivalent standard representation solving lasso problem quadratic programming kkt conditions ensure greater zero time 
augmenting data negative variables positive lasso enlarged space equivalent original lasso problem 
top pair panels shows coefficient paths positive negative variables lasso solution 
think paths solutions sequence lasso problems obtained incrementally increasing bound 
lower pair plots additional constraint imposed sequence lasso problems coefficient paths constrained monotone non decreasing 
monotone paths exactly equivalent paths forward stagewise algorithm 
mean collapsed versions paths subtracting coefficients negative versions variables corresponding coefficients positive versions exactly forward stagewise paths lower panel 
leads characterize forward stagewise algorithm monotone version lasso 
extra restrictions additional form regularization leading smoother coefficient profiles 
expanded space variables creates natural analog boosting operates large dictionary binary trees 
tree negative available 
expanded space equivalent algorithm algorithm 
obvious algorithm generates monotone coefficient paths indexed number steps total distance stepped 
drawing results efron 
show theorem limit leads exactly monotone representation 
define notion arc length 
definition 
suppose dimensional differentiable curve forward stagewise monotone lasso coefficients positive coefficients negative coefficients positive coefficients negative lasso forward stagewise norm standardized expanded coefficient profiles simulated example 
norm computed standardized variables 
coefficients original scale details visible 
forward stagewise monotone lasso algorithm monotone incremental forward stagewise regression 
start 
find predictor xj positively correlated 
update 

update xj repeat steps predictor correlation 
thel arc length tv ds named arc length tv total variation thel arc length sum total variation measures coordinate functions measure roughness curve 
piecewise differentiable continuous curve arc length sum arc lengths differentiable pieces 
lemma easily proved lemma 
coordinates monotone piecewise differentiable 
arc length norm monotone coefficient profile 
convenient expanded representation wecan revert original representation coefficients original representation simply paired differences norm lasso coefficients representation coefficient pair non zero time see proof part theorem 
arc length forward stagewise path norm expanded representation equal arc length original representation 
norm original representation 
forward stagewise monotone lasso lasso path squared error loss characterized point path solution convex optimization problem 
unfortunately monotonicity restriction forward stagewise algorithm appears preclude succinct characterization 
alternatively show lasso path solution differential equation characterizes path terms series optimal moves 
show forward stagewise path solution closely related differential equation restricts optimal moves monotone 
remainder section characterize forward stagewise path terms sequence monotone moves compare moves restrictive moves lasso theorem leads define monotone lasso path defined differential equation derivatives giving move directions current position definitions 
lasso characterized solution related differential equation show monotone lasso locally optimal terms arclength optimal move unit increase arc length coefficient profile 
lasso optimal move unit increase norm coefficients theorem show forward stagewise algorithm computes monotone lasso solution path shares optimality properties 
proposition 
generalize results loss functions section 
theorem 
point lasso forward stagewise path expanded variable space active set variables achieving maximal correlation current residual 
lasso coefficients move direction coefficients squares fit xa 
forward stagewise coefficients move direction coefficients non negative squares fit xa case coefficients change fixed direction pursued events occurs forward stagewise monotone lasso variable attains maximal correlation joins coefficient variable active set reaches point leaves lasso residuals match unrestricted squares fit 
occur direction recomputed 
proof theorem assembled results proved efron 
lasso results osborne 
rosset zhu 
convenience give simple proof appendix convex optimality conditions 
theorem leads define monotone lasso solution differential equation characterized terms positive path derivatives 
fs algorithm computes solution 
theorem stated terms point lasso fs paths 
fact moves defined starting value definition 
coefficient linear model expanded variable set 
leta active set variables achieving maximal correlation 
lasso move direction defined xt squares coefficient xa 

monotone lasso move direction ml defined ml xt non negative squares coefficient xa 
forward stagewise monotone lasso normalizations essential turn convenient parametrize coefficient paths section 
shows residual sum squares rss curves lasso forward stagewise algorithms applied simulation example 
appears example lasso decreases rss rapidly function norm coefficients forward stagewise wins terms arc length 
turns case characterization local optimality procedures 
residual sum squares norm lasso forward stagewise lar residual sum squares arc length lasso forward stagewise lar rss simulation example function norm left panel arc length right panel coefficient paths lasso forward stagewise angle regression 
theorem 
coefficient vector expanded variable space 
lasso monotone lasso move directions defined definition optimal sense 
lasso move decreases residual sum squares optimal quadratic rate respect coefficient norm 
monotone lasso move decreases residual sum squares optimal quadratic rate respect coefficient arc length 
forward stagewise monotone lasso intuition distinction think forward stagewise form boosting 
pay cost terms effort move number trees captured arc length 
lasso get rewarded decreasing coefficient zero 
monotonicity constraint results smoother coefficient profiles shorter arc lengths 
proof follows closely material section efron 

directions fixed fixed paths piecewise linear residual sum squares curves piecewise quadratic 
proof lasso 
consider move direction define 
assuming dj norm changed coefficient 
compute path derivative 
dt dt 
maximal correlation active set derivative minimized allowing elements dj ato nonzero 
da derivative 
seek da smallest hessian 
dt minimizing rayleigh quotient equivalent minimizing subject 
forward stagewise monotone lasso straightforward show solution da xt xa 
xt ar equivalent lasso move 
sequence lasso moves result optimal piecewise quadratic rss drop curve function norm 
proof monotone lasso 
arc length path easily seen weget 
dt dt minimized selecting dj minimizing value 
hessian dta xt minimize subject dj 
equivalent optimization problem subject dj dj 
straightforward show kkt conditions solution identical solution appendix direction forward stagewise move 
graphs suggest gap bigger function arc length norm 
fact case seen proof theorem 
function norm starting point downward gradient lasso fs hessian forward stagewise monotone lasso smaller lasso 
function arc length gradient lasso larger fs negative 
armed lasso monotone lasso move directions definition characterize paths solutions differential equations 
definition 
monotone lasso coefficient path dataset solution differential equation ml initial condition 
directions ml standardized unit norm solution curve unit speed parametrized arc length 
order solve need track entire path solution provided forward stagewise algorithm 
proposition 
forward stagewise algorithm dataset square error loss computes monotone lasso path starts increments coefficients continuously monotone lasso moves 
specifically initialize set ml corresponding active set 


value aj changes aj 
compute ml 

exit defined withl 
characterize lasso path similar fashion 
proposition 
lasso coefficient path dataset solution differential equation initial condition 
forward stagewise general convex loss functions case coefficients non negative shown normalization guarantees solution path parametrized norm 
characterizations draw similarities lasso monotone lasso 
shortcomings analysis monotone lasso define lasso solution explicitly point path solution optimization problem unable monotone lasso 
lasso monotone lasso paths unrestricted squares solution 
squares solution zero residuals infinitely solution coefficients 
lasso path leads unique zero residual solution having minimum norm 
construction monotone lasso path produces unique zero residual solution circumstances unable characterize 
forward stagewise general convex loss functions gradient boosting friedman hastie loss functions squared error typical candidates binomial log likelihood adaboost loss binary classification problems 
linear model simplification applicable 
concrete example consider linear logistic regression model expanded space log pr pr xt 
negative binomial log yi log pi yi log pi pi xt xt 
forward stagewise general convex loss functions generally consider case linear model loss function form yi 
analog algorithm general case algorithm 
algorithm generalized monotone incremental forward stagewise regression 
start 
find predictor xj largest negative gradient element evaluated current predictor 

update 

update predictors xi repeat steps times binomial case negative gradient step 
response vector vector fitted probabilities 
apply logic forward stagewise squared error loss situation quadratic approximation loss current 
yi forward stagewise general convex loss functions diagonal matrix entries ii yi 
case logistic regression ii current probabilities yi 
minimizing gives newton update expressed coefficients weighted squares fit ready define monotone lasso move general loss 
definition 
monotone lasso move direction ml point expanded data loss function 
compute elements zero return 
establish active set indices max xt 
coefficients weighted positive squares fit xa weights 
define 
easy check definition coincides definition squared error loss 
general piecewise constant 
expect piecewise smooth breaks active sets change 
definition 
monotone lasso coefficient path dataset loss solution differential equation ml initial condition 
discussion criteria definitions exactly analogous generalizations lasso 
squared error loss solution paths general piecewise smooth nonlinear efficient exact path algorithms available 
rosset show long loss function quadratic piecewise linear mixture paths piecewise linear tracked 
general convex loss lasso problems lasso solutions available point path 
park hastie develop efficient algorithms obtaining lasso path generalized linear model family loss functions including logistic regression 
monotone lasso case squared error loss solutions available specific points path explicitly computing path 
situation worse general exact algorithms available tracking path 
friedman popescu develop efficient stepping algorithms finding forward stagewise solutions variety losses 
discussion criteria saw top right panel residual sum squares rss yi xij methods function arc length 
curve forward stagewise sequence sits curves methods 
able show local optimality forward stagewise theorem 
initially thought forward stagewise enjoy global optimality criterion lasso candidate criterion arc length forward stagewise coefficient minimizes rss 
true lasso paths monotone procedures coincide arc length norm 
general case 
lemma 
general exist coefficient profile minimizes rss set curves having arc length 
monotonicity profiles proof 
construct unit speed coefficient path origin lasso solution 
arc length norm equal minimum value rss curves having arc length 
solution problem agree lasso solution 
right hand panel case lasso forward stagewise profiles different 
possible global formulations problem involve integrated loss 
tv non decreasing set monotone functions having arc length point class monotone arc length norm asking monotone path solves sequence lasso problems subject constraint 
candidate criterion forward stagewise algorithm minimizes integrated residual sum squares argmin yi xij integrated loss continuous strictly convex functional exists unique optimal path solves 
turns forward stagewise solution optimal path 
computed exact solution simulation example discretized sequence values arc length 
compares results forward stagewise solution points 
compute cumulative mean rss plot difference exact fs 
keeping greedy nature fs initially wins exact procedure 
fs optimize criterion 
monotonicity profiles say example generated 
data generated model sin monotonicity profiles difference cumulative mean rss arc length simulation example provides counter example candidate 
shown difference mean cumulative rss exact solution criterion forward stagewise computed discretized set values arc length 
initially forward stagewise wins exact solution 
equally spaced values 
predictors piecewise linear basis functions knots 
shows successive approximations sin different methods equally spaced solutions paths 
column uses piecewise constant basis functions tj place piecewise linear ones tj tj 
shows coefficient profiles 
notice profiles monotone non decreasing non increasing profiles algorithms coincide 
fact monotonicity coincidence follows definitions 
case zero crossing events lar lasso coincide 
addition monotonicity means positive coefficients decreased vice versa nonnegative squares move forward stagewise procedure squares move lar 
monotone 
denote matrix standardized predictors xa denote subset columns multiplied set arbitrary signs 
sa diagonal matrix sj values 
results efron 
show necessary sufficient monotonicity profiles lasso forward stagewise lar lar haar successive approximations sin equally spaced solutions paths example 
columns piecewise linear bases column uses piecewise constant bases methods coincide 
lasso versus forward stagewise better 
coefficients lasso fs lar haar norm standardized coefficient profiles data piecewise constant basis functions 
coefficients monotone lasso fs lar coincide 
condition path monotone sa sa words subsets predictors sign changes predictors inverse covariance matrix diagonally dominant means diagonal element big sum elements row 
prove theorem appendix 
theorem 
condition holds piecewise constant bases lasso fs lar solutions coincide 
lasso versus forward stagewise better 
current interest infinitesimal forward stagewise procedure due connection squares boosting see hastie 
chapter lasso versus forward stagewise better 

boosting adaptive regression procedure builds model sum flexible pieces regression trees 
hastie 
view squares boosting forward stagewise procedure applied set trees induced data 
results show forward stagewise behaves monotone version lasso locally optimal regard arclength 
contrast lasso constrained 
begs question large number predictors algorithm better 
monotone lasso tend slow search allowing sudden changes direction occur lasso 
thing 
investigate carried simulation study 
data consists measurements gaussian variables strongly correlated groups 
true model nonzero coefficients variables drawn group coefficient values drawn standard gaussian 
gaussian noise added noise signal ratio 
grouping variables intended mimic correlations nearby trees boosting forward stagewise algorithm setup intended idealized version gradient boosting shrinkage 
shows coefficient paths lasso forward stagewise single realization model 
coefficient profiles similar early stages paths 
stages forward stagewise paths smoother fact exactly monotone lasso fluctuate widely 
due strong correlations subsets variables 
test error performance models similar achieve minimum 
stages forward stagewise takes longer overfit consequence smoother paths 
conclude problems large numbers correlated predictors forward stagewise procedure associated arc length criterion preferable lasso norm criterion 
suggests general boosting type applications incremental forward stagewise algorithms currently preferable algorithms try solve equivalent lasso problem 
acknowledgments authors steven boyd jerome friedman rosset ben van roy ji zhu helpful discussions 
hastie lasso versus forward stagewise better 
standardized coefficients lasso standardized coefficients forward stagewise comparison lasso forward stagewise paths simulated regression data 
number samples number variables 
forward stagewise paths fluctuate lasso final stages algorithms 
appendix proofs mean squared error oo oo oooo oo oo oo oo oo oo ooooo oo ooooo oo oo ooo oo ooooo ooo oooo lasso forward stagewise mean squared error lasso forward stagewise simulated data 
despite difference coefficient paths models perform similarly critical part regularization path 
right tail lasso appears overfit rapidly 
partially supported dms dms national science foundation ca national institutes health 
tibshirani partially supported national science foundation dms national institutes health contract hv 
appendix proofs proof theorem part 
lagrangian corresponding yi xij xij appendix proofs kkt conditions 
xj xj residual vector 
deduce aspects solution 
solution corresponds unrestricted squares fit 

likewise 

give intuitive result pair positive time 



xt xt lasso path exists 
define active set set indices variables positive coefficients assume suitably small set changed 
define corresponding coefficients 
deduction xa 
xa xa 
appendix proofs remains constant coefficients change linearly 
xa xa ar claimed 
change variable catches terms case augmented direction recalculated 
change coefficient attempts pass zero case removed proof theorem part 
step monotone incremental forward stagewise algorithm algorithm selects variable having largest correlation residuals moves coefficient 
may set variables competing maximal correlation succession moves divided nj jn augmented variable js coefficient 
limit decreases increases expect active set variables tied terms largest correlation sequence moves total arc length distributed set proportions efron 
showed lemma sufficiently small set change 
suppose reflecting equal correlations 
limiting sequence moves positive components maintain equal correlation residuals subject constraints decrease residual sum squares fast possible 
consider optimization problem min xa subject captures third requirements 
lagrangian kkt conditions xa appendix proofs 
vector components note shows correlations residual remain equal second requirement 
consider second optimization problem statement theorem min xa subject 
corresponding kkt conditions xa show solves solution 
get xa xa 
multiply get xa 
easy check satisfy 
appendix proofs variables may drop active set correlation decrease faster positive coefficients 
directions pursued variable catches terms correlation point procedure stops updated direction recomputed 
proof theorem 
need verify piecewise constant basis functions holds sign matrix sa 
suppose piecewise constant basis functions knots tk 
nj xi tj number observed right th knot 
loss generality assume nj predictor contributes model observed right left knot 
simple calculation shows normalizing columns ij ni ni nj nj covariance function brownian bridge bs normalized unit variance time points sk sj nk prove diagonally dominant choice knots principal minor exactly form xa smaller fewer knots proven xt diagonally dominant lasso paths monotone 
prove xtx diagonally dominant computing xtx way compute xtx compute density bs sk sk appendix proofs read inverse exponent density 
turn computing density note bs sk sk standard brownian motion vj sj sj wv vk simple show constant multiple exponent density wv vk evaluated wk elements vj vj vj vj vj vj vj vj vj vj vj vj vj vj vj vj diagonal entries non positive show multiplying left right sa increase entries vector 
prove vj vj vj vj vj vj vj vj vj vj vj vj vj 
scaling combining fractions implied 
remains prove inequality holds 
just jensen inequality define point distribution placing mass mass distributed law 
note general conditions monotonicity derived 
clear verified practice 
explore 
efron hastie johnstone tibshirani 
angle regression annals statistics 
fleming 
deterministic stochastic optimal control springer verlag new york 
friedman 
greedy function approximation gradient boosting machine annals statistics 
friedman hastie tibshirani 
additive logistic regression statistical view boosting discussion annals statistics 
friedman popescu 
gradient directed regularization technical report stanford university 
hastie tibshirani friedman 
elements statistical learning data mining inference prediction springer verlag new york 
osborne 
new approach variable selection squares problems ima journal numerical analysis 
park 
hastie 
regularization path algorithms generalized linear models technical report stanford university 
rosset 
tracking curved regularized optimization solution paths advances neural information processing systems nips vol 
mit press cambridge ma 
appear 
rosset zhu 
piecewise linear regularized solution paths technical report stanford university 
www stat stanford edu papers piecewise ps 
tibshirani 
regression shrinkage selection lasso royal 
statist 
soc 

