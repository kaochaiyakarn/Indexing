randomized algorithms fast bayesian hierarchical clustering katherine heller zoubin ghahramani gatsby computational neuroscience unit university college london london wc ar uk heller zoubin gatsby ucl ac uk october new algorithms fast bayesian hierarchical clustering large data sets 
bayesian hierarchical clustering bhc method agglomerative hierarchical clustering evaluating marginal likelihoods probabilistic model 
bhc advantages traditional agglomerative clustering algorithms 
defines probabilistic model data uses bayesian hypothesis testing decide merges advantageous output recommended depth tree 
algorithm interpreted novel fast bottom approximate inference method dirichlet process countably infinite mixture model dpm 
original bhc algorithm computational complexity new randomized algorithms log 
hierarchical structures ubiquitous natural world 
example evolutionary tree living organisms consequently features organisms sequences homologous genes natural hierarchy 
hierarchical structures natural representation data generated evolutionary processes 
example internet newsgroups emails documents newswire organized increasingly broad topic domains 
hierarchical clustering frequently methods unsupervised learning 
set data points output binary tree dendrogram leaves data points internal nodes represent nested clusters various sizes 
tree organizes clusters hierarchically hope hierarchy agrees intuitive organization real world data 
traditional method hierarchically clustering data bottom agglomerative algorithm 
starts data point assigned cluster iteratively merges closest clusters data belongs single cluster 
nearest pair clusters chosen distance measure euclidean distance cluster means distance nearest points 
limitations traditional hierarchical clustering algorithm 
algorithm provides guide choosing correct number clusters level prune tree 
difficult know distance metric choose especially structured data images sequences 
traditional algorithm define probabilistic model data hard ask clustering compare models predictions cluster new data existing hierarchy 
statistical inference overcome limitations 
previous uses probabilistic methods perform hierarchical clustering discussed detail technical report 
bayesian hierarchical clustering algorithm uses marginal likelihoods decide clusters merge avoid overfitting 
basically asks probability data potential merge generated mixture component compares exponentially hypotheses lower levels tree section 
bhc algorithm complexity number data points 
practice run bayesian hierarchical clustering algorithm large data sets 
dk tk di ti schematic portion tree ti tj merged tk associated data sets di dj merged dk 
example tree data points 
clusterings tree consistent partitions data 
clustering tree consistent partition 
fast versions bhc randomized algorithms log complexity section 
bhc algorithm bayesian hierarchical clustering algorithm similar traditional agglomerative clustering pass bottom method initializes data point cluster iteratively merges pairs clusters 
see main difference algorithm uses statistical hypothesis test choose clusters merge 

denote entire data set di set data points leaves subtree ti 
algorithm initialized trivial trees ti 
containing single data point di 
stage algorithm considers merging pairs existing trees 
example ti tj merged new tree tk associated set data dk di dj see 
considering merge hypotheses compared 
hypothesis denote hk data dk fact generated independently identically probabilistic model unknown parameters 
imagine probabilistic model multivariate gaussian parameters crucial emphasize different types data different probabilistic models may appropriate 
evaluate probability data hypothesis need specify prior parameters model hyperparameters 
ingredients compute probability data dk hk dk dk dk calculates probability data dk generated parameter values assuming model form 
natural model criterion measuring data fit cluster 
choose models conjugate priors normal inverse wishart priors normal continuous data dirichlet priors multinomial discrete data integral tractable 
conjugate priors integrals simple functions sufficient statistics dk 
example case gaussians function sample mean covariance data dk 
alternative hypothesis data dk clusters 
summing exponentially possible ways dividing dk clusters intractable 
restrict clusterings partition data manner consistent subtrees ti tj compute sum efficiently recursion 
elaborate notion tree consistent partitions 
probability data restricted alternative hypothesis simply product subtrees dk di ti dj tj probability data set tree di ti defined 
combining probability data hypotheses hk hk weighted prior points def obtain marginal probability data tree tk dk belong cluster dk tk kp dk di ti dj tj equation defined recursively term considers hypothesis single cluster dk second term efficiently sums clusterings data dk consistent tree structure see 
tech report show equation derive approximation marginal likelihood dirichlet process mixture model fact provides new lower bound marginal likelihood 
show prior merged hypothesis computed bottom dpm 
posterior probability merged hypothesis rk def dk obtained bayes rule rk kp dk kp dk hk di ti dj tj quantity decide greedily trees merge determine merges final hierarchy structure justified 
general algorithm simple see 
input data 
model prior initialize number clusters di 
find pair di dj highest probability merged hypothesis rk eq 
merge dk di dj tk ti tj delete di dj output bayesian mixture model tree node mixture component tree cut points rk bayesian hierarchical clustering bhc algorithm bayesian hierarchical clustering algorithm desirable properties absent traditional hierarchical clustering 
example allows define predictive distributions new data points decides merges advantageous suggests natural places cut tree statistical model comparison criterion rk customized different kinds data choosing appropriate models mixture components 
tree probability new test point data computed recursing tree starting root node 
node represents cluster associated predictive distribution dk dk 
predictive distribution sums nodes weighted posterior probabilities dk def set nodes tree rk nk ri weight cluster nk set nodes path root node parent node gaussian components example conjugate priors results predictive distribution mixture multivariate distributions 
give detailed explanation show examples tech report 
summary sums probabilities tree consistent partitions weighted prior mass assigned partition dpm 
computational complexity constructing tree complexity computing marginal likelihood complexity computing predictive distribution 
randomized bhc algorithms goal run bayesian hierarchical clustering large datasets accomplish need bhc algorithm small computational complexity 
bhc algorithm section computation dominated pairwise comparisons data points lowest levels 
wasteful limits large data sets 
aim capitalize inefficiency combined powerful resource randomized algorithms create faster bhc algorithm 
propose randomized algorithm fast bayesian hierarchical clustering input data 
pick points randomly run bhc obtaining tree filter top level tree obtaining dl dr dl dr recurse run dl dr output bayesian mixture model tree node mixture component randomized bayesian hierarchical clustering algorithm algorithm takes data set randomly selects subset data points data set 
original bhc algorithm run subset data points obtaining tree remaining data points filtered top level merge tree filter algorithm takes top level partitioning tree priors computed bhc algorithm remaining data points 
takes remaining data point xi computes probabilities xi belongs left subtree right subtree cluster 
data point added highest probability cluster subtree 
assignments data points left right subtrees returned algorithm runs separately subtree 
constant may reduced data set smaller 
input initialize dl dr foreach compute lp rp dl dl dr dr output dl dr filter algorithm algorithm rests assumptions 
assumption top level clustering built subset data points approximation top level clustering 
means assignments data points dl dr similar subsample filter compared running full bhc algorithm 
second assumption bhc algorithm tends produce roughly balanced trees points top level clusters points branch points 
necessary maintain smaller running time 
proposition algorithm log number operations required run expressed recursively ops ops ops term comes running bhc data points term comes filter get algorithm 
expanding expression levels recursion letting ops 
gives log terms 
generalize takes values yields result merely adjusting base log 
practice level comparable reached algorithm simply call bhc points 
generally interested top levels tree may sense truncate algorithm say levels avoid running levels individual points clusters 
truncated algorithm nl 
propose alternative randomized algorithm em 
algorithm takes advantage fact bhc run clusters points individual points cluster clusters input data 
subsample points randomly foreach point create cluster filter clusters refine clusters running steps hard em point reassign probable cluster run bhc clusters output em output bayesian mixture model tree node mixture component proposition algorithm randomized algorithm em number operations alternate algorithm nm nm term comes filtering step filters points clusters term running em bhc step 
small algorithm linear results compared bayesian hierarchical clustering traditional hierarchical clustering methods 
extensive results available tech report showing significant improvements tree structure clustering classification ability 
reproduce small examples comparing bhc average linkage hierarchical clustering 
cedar buffalo digits data set images digits attributes ran algorithms examples digits 
compare structure trees generated average linkage hierarchical clustering versus bayesian hierarchical clustering see bhc tree structure desirable particularly highest levels tree immediately finds distinct clusters digits 
compared bhc algorithms examples newsgroups rec sport baseball rec sport hockey rec autos sci space cmu newsgroups dataset 
dataset rainbow list words appearing fewer times ignored 
dataset binarized word presence absence document 
average linkage hierarchical clustering tree performing average linkage hierarchical clustering examples digits 
numbers axis correspond true class label example axis corresponds distance merged clusters 
tree performing bayesian hierarchical clustering dataset 
higher values axis correspond higher levels hierarchy merges exact numbers irrelevant 
red dashed lines correspond merges negative posterior log probabilities labeled values 
baseball pitch hit data game team play hockey round car dealer drive car space nasa space nasa orbit data pitcher boston ball car baseball engine car player space top level structure bhc left vs average linkage hc newsgroup dataset 
words shown node highest mutual information cluster documents node versus sibling occur higher frequency cluster 
number documents cluster 
compares top levels merges newsgroups hierarchy examples word attributes highest information gain bhc 
continuing look lower levels improve 
traditional hierarchical clustering algorithms perform similarly 
full dendrograms dataset plus results tech report 
currently working apply algorithm larger datasets 
heller ghahramani 
bayesian hierarchical clustering 
technical report gatsby computational neuroscience unit 
quebec jet boston vehicle dealer driver duda hart 
pattern classification scene analysis 
wiley 
team game hockey motwani raghavan 
randomized algorithms 
cambridge university press 
mccallum 
bow toolkit statistical language modeling text retrieval classification clustering 
www cs cmu edu mccallum bow 

