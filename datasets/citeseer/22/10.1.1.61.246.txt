helsinki university technology publications computer information science report april building blocks variational bayesian learning latent variable models valpola markus juha karhunen helsinki university technology technische universit helsinki universite de technologie helsinki distribution helsinki university technology department computer science engineering laboratory computer information science box fi finland tel 
fax report downloadable www cis hut fi publications isbn issn building blocks variational bayesian learning latent variable models valpola markus juha karhunen helsinki university technology adaptive informatics research centre box fi hut espoo finland email firstname lastname fi url www cis hut fi projects bayes fax april introduce standardised building blocks designed variational bayesian learning 
blocks include gaussian variables summation multiplication nonlinearity delay 
large variety latent variable models constructed blocks including variance models nonlinear modelling lacking existing variational systems 
introduced blocks designed fit yield efficient update rules 
practical implementation various models easy associated software package derives learning formulas automatically specific model structure fixed 
variational bayesian learning provides cost function updating variables model optimising model structure 
computations carried locally resulting linear computational complexity 
experimental results structures including new hierarchical nonlinear model variances means 
test results demonstrate performance usefulness introduced method 
various generative modelling approaches provided powerful statistical learning methods neural networks graphical models years 
methods aim finding appropriate model explains internal structure regularities observations 
assumed regularities caused certain latent variables called factors sources hidden variables hidden causes generated observed data unknown mapping 
unsupervised learning goal identify unknown latent variables generative mapping supervised learning suffices estimate generative mapping 
expectation maximisation em algorithm learning latent variable models :10.1.1.114.4996
distribution latent variables modelled model parameters maximum likelihood maximum posteriori estimators 
point estimates determination correct model order overfitting ubiquitous difficult problems 
full bayesian approaches making complete posterior distribution gained lot attention 
exact treatment posterior distribution intractable simple toy problems resort suitable approximations 
called laplacian approximation method employs gaussian approximation peak posterior distribution 
method suffers overfitting 
real world problems perform adequately largely way better alternatives 
markov chain monte carlo mcmc techniques popular supervised learning tasks providing estimation results 
unfortunately computational load high restricts mcmc large scale unsupervised learning problems parameters variables estimated numerous 
instance case study unsupervised learning brain imaging data 
mcmc scaled toy example resorted point estimates real data 
ensemble learning variational bayesian methods gained increasing attention years :10.1.1.36.2841
largely avoids overfitting allows estimation model order structure computational load reasonable compared mcmc methods 
variational bayesian learning employed supervised problems popular unsupervised modelling 
authors successfully applied techniques linear factor analysis independent component analysis ica various extensions :10.1.1.107.3613
include linear independent factor analysis extensions basic linear ica model mlp networks modelling nonlinear observation mappings nonlinear dynamics latent variables source signals :10.1.1.49.62
variational bayesian learning applied large discrete models nonlinear belief networks hidden markov models :10.1.1.112.3902
introduce small number basic blocks building latent variable models learned variational bayesian learning 
blocks introduced earlier conference papers applications 
studied hierarchical models variance sources signal processing point view 
comprehensive presentation block framework 
approach suitable unsupervised learning tasks considered principle applied supervised learning 
wide variety factor analysis type latent variable models constructed combining basic blocks suitably 
variational bayesian learning provides cost function updating variables optimising model structure 
blocks designed fit yield efficient update rules 
maximally factorial posterior approximation required computations performed locally 
results linear computational complexity function number connections model 
bayes blocks software package open source python implementation freely downloaded 
basic building block gaussian variable node 
uses input values mean variance 
building blocks include addition multiplication nodes delay gaussian variable followed nonlinearity 
known model structures constructed blocks 
introduce novel model structures extending known linear structures nonlinearities variance modelling 
examples 
key idea developing blocks connections blocks chosen model fixed particular model selected specified cost function updating rules needed learning computed automatically 
user need understand underlying mathematics derivations done software package 
allows rapid prototyping 
bayes blocks bring different methods unified framework implementing corresponding structure blocks results methods initialisation 
different methods compared directly cost function combined find better models 
updates minimise global cost function guaranteed converge algorithms loopy belief propagation extended kalman smoothing expectation propagation 
introduced general purpose algorithm called variational message passing 
resembles framework uses variational bayesian learning factorised approximations 
vibes framework allows discrete variables nonlinearities nonstationary variance 
posterior approximation need fully factorised leads accurate model 
optimisation proceeds cycling factor revising approximate posterior distribution 
messages contain certain expectations posterior approximation sent network 
view variational bayesian learning extension em algorithm 
algorithms apply combinations discrete linear gaussian models 
experiments variational bayesian model structure selection outperformed bayesian information criterion relatively small computational cost reliable annealed importance sampling number samples high computational cost 
major difference approach compared related methods concentrate mainly situations handy conjugate prior posterior distributions available 
life easier hand blocks combined freely allowing richer model structures 
instance modelling variance way described section possible gamma distribution precision parameter gaussian node 
price pay advantage minimum cost function iteratively solved analytically conjugate distributions applied 
cost function evaluated analytically bayes blocks framework 
note different approaches fit 
similar graphical models learned sampling algorithms variational bayesian learning 
instance bugs software package uses gibbs sampling bayesian inference 
supports mixture models nonlinearities nonstationary variance 
software packages concentrated discrete bayesian networks 
notably bayes net toolbox bayesian learning inference types directed graphical models methods :10.1.1.25.1216
includes decision theoretic nodes 
sense general 
limitation bayes net toolbox supports latent continuous nodes gaussian conditional gaussian distributions :10.1.1.25.1216
system generates code efficient implementations algorithms bayes networks 
currently algorithm schemas include em means discrete model selection 
system support continuous hidden variables nonlinearities variational methods mcmc temporal models 
greatest strengths code generation approach compared software library possibility automatically optimising code domain information 
independent component analysis community traditionally observation noise modelled way 
modelled noise variance assumed constant value estimated available observations required 
flexible variance models highly desirable wide variety situations 
known real world signals data sets nonstationary roughly stationary fairly short intervals 
quite am level signal varies markedly function time position means variance nonstationary 
examples include financial data sets speech signals natural images 
demonstrated higher order statistical properties natural images signals explained stochastic model stationary gaussian process nonstationary variance 
variance models useful explaining volatility financial time series detecting outliers data 
utilising nonstationarity variance possible perform blind source separation certain conditions :10.1.1.107.3613
authors introduced hierarchical models related discussed 
models subspaces dependent features single feature components 
kind models proposed context independent component analysis topographic self organising maps :10.1.1.35.4531
problem methods difficult learn structure model compare different model structures 
remainder organised follows 
section briefly basic concepts variational bayesian learning 
section introduce building blocks nodes section discuss variational bayesian computations 
section show examples different types models constructed building blocks 
section deals learning potential problems related section experimental results structures section 
followed short discussion section 
variational bayesian learning bayesian data analysis estimation methods uncertain quantities modelled terms joint probability density function pdf 
key principle construct joint posterior pdf unknown quantities model data sample 
posterior density contains relevant information unknown variables parameters 
denote set model parameters unknown variables wish estimate data set posterior probability density parameters data obtained bayes rule likelihood parameters data prior pdf parameters normalising term called evidence 
evidence directly understood marginal probability observed data assuming chosen model evaluating evidences different models hi choose model describes observed data highest probability 
variational bayesian learning fairly introduced approximate fully bayesian method popular properties 
key idea approximate exact posterior distribution distribution computationally easier handle 
approximating distribution usually chosen product independent distributions parameter set similar parameters 
variational bayesian learning employs kullback leibler kl information divergence probability distributions 
kl information defined cost function jkl ln dv measures difference probability mass densities 
minimum value achieved densities 
kl information minimise misfit actual posterior pdf parametric approximation 
exact kl information jkl densities yield practical cost function normalising term needed computing usually evaluated 
cost function variational bayesian learning defined ckl jkl ln subscripts pdf assumed arguments omitted keeping notation simpler 
accurately show dependence chosen model conditioning pdf dropped dependence notational simplicity 
see somewhat complete discussion bayesian methods ensemble learning 
slight manipulation yields ckl ln require 
cost function ckl consists parts cq ln ln cp ln ln shorthand notation denotes expectation respect approximate pdf 
addition cost function ckl provides bound evidence 
jkl nonnegative follows directly ckl ln shows negative cost function bounds log evidence 
worth noting variational bayesian ensemble learning derived information theoretic minimum description length coding 
considerations arguments helping understand common problems certain aspects learning 
dependency structure parameters method bayesian networks 
variables seen nodes graph 
variable conditioned parents 
difficult part cost function expectation lnp computed approximation posterior pdf 
logarithm splits product simple terms sum 
simple terms computed constant time computational complexity linear 
general computation time constant parents independent posterior pdf approximation 
condition satisfied joint distribution parents decouples product approximate distributions parents 
term depending parents depends parent 
independence requirement violated variable receives inputs latent variable multiple paths latent variables dependent having non joint distribution 
illustrates flow information network qualitatively different cases 
choice multivariate gaussian density diagonal covariance matrix 
crude approximation adequate finding dash lined nodes connections ignored updating shadowed node 
left general markov blanket needs considered 
right completely factorial posterior approximation multiple computational paths leads decoupled problem 
nodes updated locally 
region mass actual posterior density concentrated 
mean values components gaussian approximation provide reasonably point estimates corresponding parameters variables respective variances measure reliability estimates 
occasionally diagonal gaussian approximation crude 
problem considered context independent component analysis giving means remedy situation 
account posterior dependencies posterior pdf approximation accurate usually increases computational load significantly 
earlier considered networks multiple computational paths papers example 
computational load variational bayesian learning roughly quadratically proportional number unknown variables mlp network model 
building blocks nodes introduced associated structural constraints provide effective means combating drawbacks mentioned 
updating node takes place locally multiple paths 
result computational load scales linearly number estimated quantities 
cost function learning formulas unknown quantities estimated evaluated automatically specific model selected connections blocks model fixed 
important advantage proposed block approach 
node types section different types nodes easily combined 
variational bayesian inference algorithm nodes discussed section 
subfigure left circle represents gaussian node corresponding latent variable conditioned mean variance exp 
second subfigure addition multiplication nodes form affine mapping third subfigure nonlinearity applied immediately gaussian variable 
rightmost subfigure delay operator delays time dependent signal time unit 
general building blocks divided variable nodes computation nodes constants 
variable node corresponds random variable observed hidden 
type variable node gaussian node framework 
computation nodes addition node multiplication node nonlinearity delay node 
shall refer inputs outputs nodes 
variable node inputs parameters conditional distribution variable represented node output value variable 
computation nodes output fixed function inputs 
symbols various nodes shown 
addition multiplication nodes included typically combined represent effect linear transformation symbol 
output signal node input zero nodes called children node 
constants nodes inputs 
output fixed value determined creation node 
nodes structured vectors matrices 
assume example data matrix 

called time index dimensional observation vector 
note correspond time real world different point different people 
implementation nodes vectors values indexed observations scalars values constants weights 
data represented vector nodes 
scalar node parent vector node child vector node 
gaussian node gaussian node variable node basic element building hierarchical models 
leftmost subfigure shows schematic diagram gaussian node 
output value gaussian random variable conditioned inputs denote generally probability density function gaussian random variable mx having mean mx variance conditional probability function cpf variable exp 
generative model gaussian node takes mean input adds gaussian noise innovation variance exp 
variables latent observed 
observing variable means fixing output value data 
section devoted inferring distribution latent variables observed variables 
inferring distribution variables independent called learning 
computation nodes addition multiplication nodes summing multiplying variables 
standard mathematical operations typically construct linear mappings variables 
task automated software general nodes connected ways 
addition node inputs denoted sn gives sum inputs output si 
similarly output multiplication node product inputs si 
nonlinear computation node constructing nonlinear mappings variable nodes 
nonlinearity exp chosen required expectations solved analytically 
implemented nonlinearity computations carried analytically cut function max 
possible nonlinearities discussed section 
delay node delay operation model dynamics 
node operates time dependent signals 
transforms inputs 
outputs 
scalar parameter provides starting distribution dynamical process 
symbol rightmost subfigure fig 
illustrating delay node standard notation unit delay signal processing temporal neural networks 
models containing delay node called dynamic models called static 
variational bayesian inference bayes blocks section give equations needed computation nodes introduced section 
generally speaking node propagates forward direction distribution output inputs 
backward direction dependency cost function children output parent propagated 
potentials combined form posterior distribution variable 
direct analogy bayes rule prior forward likelihood backward combined form posterior distribution 
show potentials directions fully determined values consist certain expectations distribution forward direction gradients cost function expectations backward direction 
discuss detail properties node 
note delay node process signals just 
formulas needed associated expectations gradients 
gaussian node recall gaussian node section 
variance parameterised exponential function exp 
mean expected exponential exp input suffice evaluating cost function shown shortly 
consequently cost function minimised gradients respect expectations 
gradients computed backwards children nodes learning method differs clearly standard back propagation 
important reason parameterisation exp prior variance gaussian random variable posterior distribution approximately gaussian provided prior mean gaussian see example section 
conjugate prior distribution inverse prior variance gaussian random variable gamma distribution 
gamma prior pdf causes posterior distribution gamma mathematically convenient 
conjugate prior pdf second parameter gamma distribution quite intractable 
gamma distribution suitable developing hierarchical variance models 
logarithm gamma distributed variable approximately gaussian distributed justifying adopted parameterisation exp 
noted gamma exp distributions prior pdfs mainly estimation posterior pdf mathematically tractable claim choices correct 
cost function recall approximating joint posterior pdf random variables maximally factorial manner 
decouples product individual distributions 
assumed statistically independent posteriori 
posterior approximation gaussian variable defined gaussian mean variance 
utilising part cp kullback leibler cost function arising data defined eq 
computed closed form 
gaussian node cost cs ln exp var ln derivation appendix slightly different notation 
observed variables term arising cost function ckl 
latent variables contribute cost function ckl part cq defined eq 
resulting expectation ln term cs ln ds ln negative entropy gaussian variable variance parameters defining approximation posterior distribution mean variance optimised learning 
output latent gaussian node trivially provides mean variance var expected exponential easily shown exp exp outputs nodes corresponding observations known scalar values distributions 
nodes var exp exp important considerations far cost function gaussian node computed analytically closed form 
requires posterior approximation gaussian mean variance var mean input mean expected exponential exp variance input computed 
summarise shown gaussian nodes connected costs evaluated analytically 
derivatives cost function respect expectations mean variance parents messages children parents 
derived directly eq 
form cs exp cs exp var cs cs exp var updating posterior distribution 
posterior distribution latent gaussian node updated follows 

distribution affects terms cost function cs arising variable cs cs cp terms children denoted ch gradients cost ch respect var exp computed equations 

terms cp depend shown see appendix form cp cs ch ms exp cp cp cp 
exp 
minimum cs cs cs ch solved 
done analytically corresponding case called free form solution see details sopt sopt 
minimum obtained iteratively 
iterative minimisation carried efficiently newton method posterior mean fixed point iteration posterior variance minimisation procedure discussed detail appendix note constants dropped depend es 
addition multiplication nodes consider addition node 
mean variance expected exponential output addition node evaluated straightforward way 
assuming inputs si statistically independent expectations respectively si var exp si si si var si exp si proof appendix 
consider multiplication node 
assuming independence inputs si mean variance output take form see appendix si var si si si var si si multiplication node expected exponential evaluated knowing exact distribution inputs 
formulas inputs generality practice carried needed calculations pairwise 
general formula variance occasionally take small negative value due minor imprecisions appearing computations 
problem arise pairwise computations 
propagation forward direction covered 
form cost function propagating children parents assumed form 
true case addition multiplication nodes see appendix proof 
gradients cost function respect different expectations need propagated backwards identify cost function parent 
required formulas obtained straightforward manner eqs 

gradients addition node var var exp 
exp exp multiplication node var var var var 
var addition multiplication nodes added gaussian nodes costs retain form 
proofs appendices 
nonlinearity node serious problem arising nonlinear functions impossible compute required expectations analytically 
describe particular nonlinearity detail discuss options extending nonlinearities implementation underway 
ghahramani roweis shown nonlinear function exp eq :10.1.1.16.3419
mean variance analytical expressions shortly provided gaussian input 
graphical network structures condition fulfilled require nonlinearity inserted immediately gaussian node 
type exponential function frequently standard radial basis function networks different manner :10.1.1.16.3419
exponential function depends euclidean distance center point case depends input variable directly 
second moments function respect distribution exp exp formula provides directly mean variance obtained applying familiar formula var expected exponential exp evaluated analytically limits somewhat nonlinear node :10.1.1.16.3419
updating nonlinear node directly gaussian node takes place similarly updating plain gaussian node 
gradients cp respect var evaluated assuming arise quadratic term 
assumption holds nonlinearity propagate mean gaussian nodes 
update formulas appendix possibility nonlinearity error function exp dr mean evaluated analytically variance approximated :10.1.1.112.3902
increasing variance increases value cost function suffices minimise upper bound cost function finding solution 
apply error function mlp multilayer perceptron networks manner different :10.1.1.112.3902
applied hyperbolic tangent function tanh approximating iteratively gaussian 
approximate sigmoidal function gauss hermite quadrature 
alternative considered 
problem cost function mean variance computed analytically 
possible nodes authors implemented new variable nodes bayes blocks software library 
mixture gaussians mog node rectified gaussian node 
mog model sufficiently behaving distribution 
independent factor analysis ifa method introduced mog distribution sources resulting probabilistic version independent component analysis ica :10.1.1.107.3613:10.1.1.49.62
second new node type rectified gaussian variable introduced 
omitting negative values retaining positive ones variable originally gaussian distributed block allows modelling variables having positive values 
variables commonplace example digital image processing picture elements pixels non negative values 
cost functions update rules mog rectified gaussian node derived 
postpone detailed discussion nodes forthcoming papers keep length reasonable 
early conference introduced blocks time blocks proposed handling discrete models variables 
switch picks th continuous valued input signal output signal 
discrete variable soft max prior derived continuous valued input signals ci node 
omitted nodes performance turned adequate 
reason assuming parents nodes independent restrictive 
instance building mixture gaussians discrete gaussian variables switches possible construction loses node type var exp gaussian node addition node multiplication node nonlinearity constant exp table forward messages expectations provided output different types nodes 
numbers parentheses refer defining equations 
multiplication nonlinearity provide expected exponential 
input type var exp mean gaussian node variance gaussian node addendum factor table backward messages gradients cost function certain expectations 
numbers parentheses refer defining equations 
gradients gaussian node derived eq 

gaussian node requires corresponding expectations inputs var exp addition multiplication nodes require type input expectations required provide output 
communication nonlinearity gaussian parent node described appendix specialised mog node fewer assumptions 
discrete node switches 
action utility nodes extend library decision theory control 
addition messages variational bayesian cost function network propagate messages utility 
describe system slightly different framework 
combining nodes expectations provided outputs required inputs different nodes summarised tables respectively 
see variance input gaussian node requires expected exponential incoming signal 
computed nonlinear multiplication nodes 
nodes combined freely 
connecting nodes restrictions taken account left gaussian variable constant variance exp mean right variance source added providing non constant variance input output source signal 
variance source prior mean prior variance exp 

general network directed acyclic graph dag 
delay nodes exception past values node parents nodes 
violation real sense structure unfolded time resulting network dag 

nonlinearity placed immediately gaussian node 
output expectations equations computed gaussian inputs 
nonlinearity breaks general form likelihood 
handled special update rules gaussian followed nonlinearity appendix 

outputs multiplication nonlinear nodes variance inputs gaussian node 
expected exponential evaluated 
restrictions evident tables 
computational path latent variable variable 
independency assumptions equations violated variational bayesian learning complicated recall 
note network may contain loops underlying undirected network cyclic 
note second third fourth restrictions circumvented inserting mediating gaussian nodes 
mediating gaussian node variance input variable called variance source discussed 
nonstationary variance currently models means gaussian nodes hierarchical dynamical models 
real world situations variance var var var distribution plotted exp 
note var distribution gaussian 
corresponds right subfigure fig 
exp 
constant difficult model 
modelling variance variance source depicted schematically 
variance source regular gaussian node output input variance gaussian node 
variance source convert prediction mean prediction variance allowing build hierarchical dynamical models variance 
output gaussian node variance source attached see right subfigure fig 
general super gaussian distribution 
distribution typically characterised long tails high peak formally defined having positive value kurtosis see detailed discussion :10.1.1.107.3613
property proved example shown nonstationary variance amplitude increases kurtosis 
output signal stationary gaussian variance source depicted left subfigure fig 
naturally gaussian distributed zero kurtosis 
variance source useful modelling natural signals speech images typically super gaussian modelling outliers observations 
linear independent factor analysis instances exist nodes quite similar role chosen structure 
assuming th node corresponds scalar variable yi convenient vector yn jointly denote corresponding scalar variables 
yn 
notation figures 
represent scalar source nodes corresponding variables si source vector scalar nodes corresponding observations xi observation vector 
addition multiplication nodes building affine model structures linear factor analysis fa left independent factor analysis ifa right 
transformation nx gaussian source nodes gaussian observation nodes 
vector denotes bias vector nx denotes gaussian noise gaussian node 
model corresponds standard linear factor analysis fa assuming sources si mutually uncorrelated see example :10.1.1.107.3613
gaussianity assumed source si non gaussian prior model describes linear independent factor analysis ifa 
linear ifa introduced variational bayesian learning estimating model parts estimated expectation maximisation em algorithm :10.1.1.49.62
attias gaussians source model option variance source achieve super gaussian source model 
depicts model structures linear factor analysis independent factor analysis 
hierarchical variance model right subfigure presents hierarchical model variance shows constructed learning simpler structures shown left middle fig 

necessary learning hierarchical model having different types nodes scratch completely unsupervised manner demanding task quite probably unsatisfactory local minimum 
final rightmost variance model fig 
somewhat involved contains nonlinearities hierarchical modelling variances 
going mathematical details simpler models fig 
point considered earlier papers related simpler block models 
hierarchical nonlinear model data discussed modelling variance 
model construction hierarchical variance model stages simpler models 
left variance source attached gaussian observation node 
nodes represent vectors 
middle layer sources variance sources attached added 
layers connected nonlinearity affine mapping 
right layer added top form final hierarchical variance model 
applied example nonlinear ica blind source separation 
experimental results show block model performs adequately nonlinear bss problem results slightly poorer earlier computationally demanding model multiple computational paths 
considered hierarchical modelling variance block approach nonlinearities 
experimental results biomedical meg data demonstrate usefulness hierarchical modelling variances existence variance sources real world data 
learning starts simple structure shown left subfigure fig 

variance source attached gaussian observation node 
nodes represent vectors output vector variance source th observation data vector 
vectors dimension component variance vector models variance respective component observation vector 
mathematically simple model obeys equations nx nu vectors denote constant means bias terms data vector variance variable vector respectively 
additive noise vector nx determines variances components 
gaussian distribution zero mean variance exp nx exp precisely shorthand notation exp means component nx gaussian distributed zero mean variance defined respective component vector exp 
exponential function exp applied separately component vector 
similarly nu exp components vector define variances zero mean gaussian variables nu 
consider intermediate model shown middle subfigure fig 

second learning stage layer sources variance sources attached added 
sources represented source vector variances respective components variance vector quite similarly left subfigure 
vector node source vector variance vector represents affine transformation transformation matrix including bias term 
prior mean gaussian variance source having output form bias vector vector componentwise nonlinear functions 
quite similarly vector node observation vector yields output affine transformation bias vector 
turn provides input prior mean gaussian node modelling observation vector 
mathematical equations corresponding model represented graphically middle subfigure fig 
nx nu ns nu compared simplest model observe source vector second upper layer associated variance vector quite similar form eqs 

models data vector associated variance vector bottom layer differ simple model contain additional terms respectively 
terms nonlinear transformation source vector coming upper layer multiplied linear mixing matrices 
noise terms nx nu ns nu eqs 
modelled similar zero mean gaussian distributions eqs 

stage learning layer added top network shown middle subfigure fig 

resulting structure shown right subfigure 
added new layer quite similar layer added second stage 
prior variances represented vector model source vector turn affects affine transformation mean mediating variance node 
source vector provides prior mean source affine transformation 
model equations data vector associated variance vector remain intermediate model shown graphically middle subfigure fig 

model equations second third layer sources respective variance vectors rightmost subfigure fig 
ns nu ns nu vectors represent constant means biases respective models mixing matrices matching nu ns nu similar zero dimensions 
vectors ns mean gaussian distributions eqs 

noted resulting network number nodes size layers different different layers 
additional layers appended manner 
final network right subfigure fig 
utilises variance nodes building hierarchical model means variances 
variance sources model correspond nonlinear model latent variables hidden layer 
mentioned considered nonlinear hierarchical model 
note computation nodes hidden nodes result multiple paths latent variables upper layer observations 
type structure quadratic computational complexity opposed linear networks 
linear dynamic models sources variances useful complement linear factor analysis model nx model structures 
linear gaussian state space model left model complemented super gaussian innovation process sources middle dynamic model variances sources recurrent dynamic model right 
recursive step prediction model source vector bs ns noise term ns called innovation process 
dynamic model type example kalman filtering estimation algorithms applied 
left subfigure fig 
depicts structure arising eqs 
built blocks 
straightforward extension variance sources sources innovation process super gaussian 
variance signal characterises innovation process effect telling signal differs predicted direction changing 
graphical model extension depicted middle subfigure fig 

mathematical equations describing model written similar manner hierarchical variance models previous subsection 
extension model variance sources dynamically step recursive prediction model cu nu 
model depicted graphically rightmost subfigure fig 

context simplest possible identity dynamical mapping ns 
models introduced subsection tested experimentally 
hierarchical priors desirable priors parameters restrictive 
common type vague prior hierarchical prior 
example priors elements aij mixing matrix defined gaussian distributions aij aij exp va exp va 
priors quantities va va flat gaussian distributions constants depending scale data 
going hierarchy distribution column matrix component vector 
top number required constant priors small 
little information provided needed priori 
kind hierarchical priors experiments 
learning discuss learning procedure describing briefly problems related learning handled 
updating network nodes network communicate parents children providing certain expectations feedforward direction parents children gradients cost function respect expectations feedback direction children parents 
expectations gradients summarised tables 
basic element updating network update single node assuming rest network fixed 
computation nodes simple time child node asks expectations date computational node asks parents expectations updates ones 
vice versa parents ask gradients date node asks children gradients updates ones 
updates analytical formulas section 
variable node updated input expectations output gradients need date 
posterior approximation adjusted minimise cost function explained section 
minimisation analytical iterative depending situation 
signals propagating outwards node output expectations input gradients variable node functions updated process 
update guaranteed increase cost function 
sweep updating means updating node 
order done critical system 
useful update variable twice updating neighbours happen ordering updates done sweeps 
ordering variable node updated descendants updated 
basically variable node updated input gradients output expectations labeled outdated updated node asks information 
possible different measures improve learning process 
measures avoiding local minima described subsection 
enhancement speeding learning 
basic idea time parameters describing changing fairly linearly consecutive sweeps 
line search direction provides faster learning discussed 
apply line search tenth sweep allowing consecutive updates fairly linear 
learning model typically takes thousands sweeps convergence 
cost function decreases monotonically update 
typically decrease gets smaller time monotonically 
care taken selecting stopping criterion 
chosen learning process decrease cost previous sweeps lower predefined threshold 
structural learning local minima chosen model pre specified structure flexibility 
number nodes fixed advance optimal number estimated variational bayesian learning unnecessary connections pruned away 
factorial posterior approximation leads automatic pruning connections model 
data estimate parameters directions remain ill determined 
causes posterior distribution directions roughly equal prior distribution 
variational bayesian learning factorial posterior approximation ill determined directions tend get aligned axes parameter space factorial approximation accurate 
pruning tendency easy instance sparsely connected models learning algorithm automatically selects small amount determined parameters 
early stages learning pruning harmful large parts model get pruned away sensible representation 
corresponds situation learning scheme ends local minimum cost function 
posterior approximation takes account posterior dependences advantage far local minima factorial posterior approximation 
bayesian learning algorithms linear time complexity avoid local minima general 
suitable choices model structure countermeasures included learning scheme alleviate problem greatly 
means avoiding getting stuck local minima learning takes place stages starting simpler structures learned proceeding complicated hierarchic structures 
example technique section 
new parts network initialised appropriately 
instance principal component analysis pca independent component analysis ica vector quantisation kernel pca 
best option depends application 
useful try different methods select providing smallest value cost function learned model 
ways handle initialisation fix sources learn weights model fix weights learn sources corresponding observations 
fixed variables released gradually see section 
automatic pruning discouraged initially omitting term var var multiplication nodes eq 

effectively means mean optimistically adjusted uncertainty 
way cost function may increase due may pay escaping early pruning 
new sources si components source vector layer generated pruned sources removed time time 
activations sources reset times 
sources re adjusted places keeping mapping parameters fixed 
helps sources stuck local minimum 
experimental results bayes blocks software applied problems 
considered models variance 
main application analysis meg measurements human brain 
addition features corresponding brain activity data contained artifacts muscle activity induced patient biting teeth 
linear ica applied data able separate original causes degree dependencies remained sources 
additional layer called variance sources find correlations variances innovation processes ordinary sources 
able capture phenomena related biting artifact rhythmic activity 
astrophysical problem separating young old star populations set elliptical galaxy spectra studied authors 
observed quantities energies positive mixing process known positive necessary subsequent astrophysical analysis feasible include constraints model 
standard technique putting positive prior sources unfortunate technical shortcoming inducing sparsely distributed factors deemed inappropriate specific application 
get rid induced sparsity keep positivity constraint nonnegativity forced rectification nonlinearities 
addition finding meaningful factorisation specifications needed met related handling missing values measurements errors predictive capabilities model 
nonlinear model relational data applied analysis go 
difficult part game state evaluation determine groups stones get captured 
model similar described section built features pairs groups including probability getting captured 
learned model applied new game states estimates propagate network pairs 
structure network determined game state 
approach inference relational databases 
sets experiments additional examples 
difficult toy problem illustrates hierarchy variance modelling second studies inference missing values speech spectra third dynamical model image sequences 
samples image patches extended bars problem 
bars include standard variance bars horizontal vertical directions 
instance patch bottom left corner shows activation standard horizontal bar horizontal variance bar middle 
bars problem experimental problem studied testing hierarchical nonlinear variance model extension known bars problem 
data set consisted image patches having pixels 
contained horizontal vertical bars 
addition regular bars problem extended include horizontal vertical variance bars characterized manifested higher variance 
samples image patches shown 
data generated choosing vertical horizontal orientations active probability 
orientation active probability bar row column active 
orientations regular bars row column variance bars rows columns wide 
intensities grey level values bars drawn normalised positive exponential distribution having pdf exp 
regular bars additive variance bars produce additive gaussian noise having standard deviation intensity 
gaussian noise standard deviation added pixel 
network built stages shown 
initialised single layer nodes corresponding dimensional data vector 
second layer nodes created sweep third layer nodes sweep 
creating layer sources updated sweeps pruning discouraged sweeps 
new nodes added twice second layer third layer sweeps 
sources updated sweeps pruning discouraged sweeps 
source activations reset sweeps sources updated sweeps 
dead nodes removed sweeps 
multistage training procedure designed avoid suboptimal local solutions discussed section 
demonstrates algorithm finds generative model quite similar generation process 
sources third layer correspond horizontal vertical orientations sources second layer correspond bars 
element weight matrices depicted pixel appropriate grey level value fig 

pixels ordered similarly patches vertical bars left horizontal bars right 
regular bars mixing matrix reconstructed accurately variance bars mixing matrix exhibit noise 
distinction horizontal vertical orientations clearly visible mixing matrix 
results extended bars problem posterior means weight matrices learning 
sources second layer ordered visualisation purposes weight mixing matrices 
elements matrices depicted pixels having corresponding grey level values 
pixels weight matrices correspond patches weight matrices 
comparison experiment simplified learning procedure run demonstrate importance local optima 
creation pruning layers done methods avoiding local minima addition nodes discouraging pruning resetting sources disabled 
resulting weights seen 
time learning ends suboptimal local optimum cost function 
left cost function plotted number learning sweeps 
solid curve main experiment dashed curve comparison experiment 
peaks appear nodes added 
right resulting weights comparison experiment plotted 
prior likelihood posterior approximation typical example illustrating posterior approximation variance source 
bars second horizontal bar bottom mixed source variance bars share source regular bar fourth vertical bar left appears twice sources just suppresses variance 
resulting cost function worse compared main experiment 
ratio model evidences roughly exp 
illustrates formation posterior distribution typical single variable 
component variance source comparison experiment 
prior means distribution parents especially likelihood means potential children component 
assuming posteriors variables accurate plot true posterior variable compare gaussian posterior approximation 
difference measured kullback leibler divergence 
missing values speech spectra hierarchical nonlinear factor analysis number layers gaussian variables bottom layer corresponding data 
nonlinearity linear mixture mapping layer layers 
resembles model structure section 
model structure depicted left subfigure fig 

model equations nh cs nx nh nx gaussian noise terms nonlinearity exp operates element argument vector separately 
note included short cut mapping sources observations 
means hidden nodes need model deviations linearity 
compared methods 
factor analysis fa linear method described section 
special case dimensionality zero 
nonlinear factor analysis nfa differs mediating variables nx 
note nfa multiple computational paths leads higher computational complexity compared 
self organising map som differs methods 
rectangular map number map units associated model vectors points data space 
data point matched closest map unit 
model vectors best matching unit neighbours map moved slightly data point 
see details 
data set consisted speech spectrograms finnish subjects 
short term spectra windowed dimensions standard preprocessing procedure speech recognition 
clear dynamic source model give better reconstructions case temporal information left ease comparison models 
half samples test data missing values 
missing values set different ways measure different properties algorithms 
percent values set randomly patches 
right subfigure 
training testing sets randomly permuted setting missing values patches setting 
percent values set randomly independent neighbours 
easier setting simple smoothing nearby values give fine reconstructions 
data missing values original data reconstruction left model structure hierarchical nonlinear factor analysis 
right speech data missing values setting reconstruction 
generalisation nonlinearity patches 

high dimensionality permuted different experimental settings speech data measuring different properties algorithms 

training testing sets permuted percent values set independently neighbours 
tried optimise method describe got best results 
self organising map run som toolbox long learning time map units random 
methods optimisation minimising cost function approximation 
nfa learned sweeps data matlab implementation 
varying number sources tried best ones result 
optimal number sources size hidden layer 
large number algorithm effectively prune parts needed 
factor analysis fa number sources 
hierarchical nonlinear factor analysis number sources top layer varied best runs cost function selected 
runs size top layer varied size middle layer determined learning turned vary 
run sweeps data 
experiment nfa took hours processor time fa som faster 
runs conducted different random data missing value pattern setting method 
number runs cell nfa som 
fa converges solution 
mean standard deviation mean square reconstruction error fa nfa som setting setting setting setting order results setting follow expectations nonlinearity models 
som highest nonlinearity gives best reconstructions nfa fa follow order 
results vary potential develop better learning schemes find better solutions 
sources hidden layer emulate computation nodes active 
avoiding situation learning help find nonlinear better solutions 
setting due permutation test set contains vectors similar training set 
generalisation important setting 
som able details corresponding individual samples better due high number parameters 
compared setting som benefits lot clearly best reconstructions benefit marginally 
settings require accurate expressive power high dimensionality turned differ 
basic som intrinsic dimensions clearly poorer accuracy 
nonlinear effects important settings nfa marginally better fa 
better nfa latent variables counting 
conclude lies fa nfa performance 
applicable high dimensional problems middle layer model higher dimensional soms quickly intractable due exponential number parameters 
part nonlinearity increasing computational complexity dramatically 
fa better som expressivity high dimensions important som better nonlinear effects important 
extensions fa nfa performed better fa setting 
may possible enhance performance nfa new learning schemes especially fa limits 
hand fa best low computational complexity determining factor 
variance model image sequences section experiment dynamical model variances applied image sequence analysis reported 
motivation modelling variances natural signals exists higher order dependencies characterised correlated variances signals 
postulate able better catch dynamics video sequence modelling variances features features 
case shown 
model considered summarised set equations diag exp vx diag exp bu diag exp vu acronym referring model 
linear mapping sources observations constrained sparse assigning source circular region image patch outside connections allowed 
regions highly overlapping 
variances innovation process sources linear dynamical model 
noted modelling variances sources manner impossible restricted conjugate priors 
sparsity crucial computational complexity learning algorithm depends number connections 
goal reached different kind approach 
constraining mapping sparse learning allowed full number iterations pruned cost function explained section 
basis image sequences tends get sparse anyway waste computational resources wait weights linear mapping tend zero 
comparison purposes postulate model dynamical relations sought directly sources leading model equations diag exp vx bs diag exp shall refer model 
data video image sequence dimensions 
data consisted subsequent digital images size 
part data set shown 
models learned iterating learning algorithm times stage sufficient convergence attained 
hint superiority model provided difference cost models bits frame coding interpretation see 
evaluate performance models considered simple prediction task frame predicted previous ones 
predictive distributions models approximately computed posterior approximation 
means predictive distributions similar models 
shows means model sequence 
means interesting mainly reflect situation previous frame 
model provides rich model variances 
standard deviations predictive distribution shown 
white stands large variance black small 
clearly model able increase predicted variance area high motion activity provide better predictions 
offer quantitative support claim computing predictive perplexities models 
predictive perplexity widely language modelling defined perplexity exp log xi predictive perplexities sequence shown 
naturally predictions get worse movement video 
model able handle better compared model 
difference directly read comparing cost functions 
possible applications model image sequences include video compression motion detection early stages computer vision making hypotheses biological vision 
discussion distinctive factors different bayesian approaches type posterior approximation 
concentrated large sequence frames data experiment 
means predictive distribution model 
standard deviations predictive distribution model 
pred 
perplexity frame predictive perplexities 
learning tasks point estimates prone overfitting sampling mcmc methods particle filters slow 
problems tackled particle filters vary dimensionality latent space section dimensional 
variational bayesian learning provide compromise point estimates sampling methods 
posterior distribution consists clusters solution modes 
depends posterior approximation clusters modelled 
case expectation jkl taken approximate distribution practice leads modelling single mode 
expectation propagation kullback leibler divergence formed differently leading modelling modes 
sampling supposed take place modes 
purpose finding single representative posterior probability mass approach better 
fact expectation true posterior known bayes estimate degenerate due symmetry 
instance factor analysis type models posterior symmetric permutation factors 
number permutations gives hint infeasibility accurately modelling modes high dimensional problem 
best find mode parameters modes time dependent variables feasible 
variational bayesian methods vary depending posterior approximation 
variables assumed independent posteriori 
chosen model individual distributions gaussians 
different conjugate distributions instance variance gaussian variable modelled gamma distribution 
conjugate distributions accurate sense practi cal restricting gaussians nodes connected freely allowing example hierarchical modelling variances 
noted effect assuming independencies far significant compared effect approximations modelling individual distributions 
scope restricted models learned purely local computations 
possible parents node independent posteriori 
accomplished factorial posterior approximation allowing multiple computational paths variables 
purely local computations result computational complexity linear number connections model 
small models afford take dependencies account 
larger models desirable model posterior dependencies disjoint groups variables assume groups statistically independent done 
experience maximally factorial posterior pdf approximation suffices cases 
model structure usually important approximation posterior pdf model 
available computation time better invested larger model simple posterior approximation 
case density estimates continuous valued latent variables offer important advantage point estimates robust overfitting provide cost function suitable learning model structures 
variational bayesian learning employing factorial posterior pdf approximation density estimates efficient point estimates 
latent variable models exhibit rotational invariances variational bayesian learning utilise choosing solution factorial approximation accurate 
basic algorithm learning inference updating variable time keeping variables fixed 
benefits completely local guaranteed converge 
drawback flow information time slow performing inference dynamical model 
alternative inference algorithms updates carried forward backward sweeps 
include particle smoothing extended kalman smoothing expectation propagation 
model needs learned time needs iterate lot anyway variational bayesian algorithm small consistent improvements sweep preferable 
important design choice node updated keeping nodes fixed 
new node types added need change global learning algorithm suffices design update rule new node type 
option update nodes 
different parameters coupled cyclic updating slow sped line search described 
note updates done order minimise global cost function cost function variables 
expectation propagation updates approximation time 
important difference updates done local cost function local approximation fitted local true posterior assuming rest approximation accurate 
reason expectation propagation may diverge 
large nonlinear problems numerous suboptimal local solutions avoided 
tricks avoid discussed section 
depends application tricks best 
important aspect procedure simple possible user 
introduced standardised nodes blocks constructing generative latent variable models 
nodes include gaussian node addition multiplication nonlinearity directly gaussian node delay node 
nodes designed fit allowing construction types latent variable models including known novel structures 
constructing new prototype models rapid user need take care learning formulas 
nodes implemented open source software package called bayes blocks 
models built blocks taught variational bayesian ensemble learning 
learning method essentially uses cost function kullback leibler information true posterior density approximation 
cost function updating unknown variables model allows optimisation number nodes chosen model type 
factorial posterior density approximation required computations carried locally propagating means variances expected exponentials full distributions 
way achieve linear computational complexity respect number connections chosen model 
initialisation avoid premature pruning nodes local minima require special attention application achieving results 
tested introduced method experimentally separate unsupervised learning problems different types models 
results demonstrate performance usefulness method 
hierarchical nonlinear factor analysis variance modelling applied extension bars problem 
algorithm find model essentially complicated way data generated 
secondly recon struct missing values speech spectra 
results consistently better linear factor analysis generally best cases requiring accurate representation high dimensionality 
third experiment carried real world video image data 
compared linear dynamical model means variances sources 
results demonstrate finding strong dependencies different sources considerably easier variances modelled 
acknowledgments antti honkela tomas alexander bayes blocks software library useful comments hans van hateren supplying video data experiments 
research funded european commission project bliss finnish center excellence programme project new information processing principles 
appendices updating gaussian node show minimise function mm exp ln scalar constants 
unique solution exists 
problem occurs gaussian posterior mean variance fitted probability distribution logarithm quadratic exponential part resulting gaussian prior log gamma likelihoods respectively kullback leibler divergence measure misfit 
special case minimum analytically cases minimisation performed iteratively 
iteration newton iteration mean fixed point iteration variance carried explained detail 
newton iteration mean newton iteration obtained mi mi mi vi mi mi vi mi mi exp 
exp newton iteration converges step second derivative remains constant 
step short second derivative decreases long second derivative increases 
stability better take short long steps 
case second derivative decreases mean decreases vice versa 
stability useful restrict growth consistently estimated 
fixed point iteration variance simple fixed point iteration rule obtained variance solving zero derivative exp exp def vi vi general fixed point iterations stable solution converge best derivative near zero 
case vi negative 
case solution unstable fixed point 
avoided weighted average trivial iteration vi vi vi vi vi vi vi def vi weight derivative close zero optimal solution achieved exactly 
holds exp exp steps follow fact requirement 
assume close 
note iteration yield estimates vi means vi 
shortens step taken 
initial estimate set summary updating method 
set min 

iterate solve new estimate mean eq 
restriction maximum step solve new estimate variance eqs 
restriction maximum step 
iteration corrections small suitable predefined threshold value 
addition multiplication nodes equations addition multiplication nodes proven section 
equation applies general assume incoming signals independent posteriori 
sn sn 
proof form cost function concerns propagation addition multiplication nodes 
formulas propagating gradients cost function expectations derived 
expectations equation follows directly linearity expectation operation proven analogously proof equation 
sn ds si si si ds si dsi si equation states variance sum independent variables sum variances 
fact basic probability theory books 
proven simple manipulation equations 
equation proven applying exp exp si exp si si equation proven applying equation si var si si si sj form cost function sj si var si sj sj form part cost function output node affects shown form cp current var exp denotes expectation quantity question 
output connected directly variable seen eq 
substituting exp current exp exp var current ln output connected multiple variables sum affected costs form 
prove form remains signals fed addition multiplication nodes 
cost function predefined form sum form regarded constant 
note delay node connections affect formulas 
shown eqs 
cp current var exp current var exp exp var seen sum zero addend exp 
means outputs product nonlinear nodes fed addition nodes 
cost function predefined form product similar variable variable regarded constant 
shown eqs 
cp current var var current var var current current var updating gaussian node followed nonlinearity gaussian variable terms cost function affects cost function children 
case nonlinearity attached changed 
cost function children written form ch current var current stands expectation current posterior estimate constants 
posterior updated minimise cost function 
get fixed point iteration update candidate snew exp approximated newton iteration update candidate snew snew exp candidates guarantee direction cost function decreases locally 
long cost function increase value step size halved 
guarantees convergence stable point 
example point estimates fail example illustrates go wrong point estimates 
dimensional data vectors modelled linear factor analysis model scalar source signal gaussian noise vector zero mean parameterised variance nk 
dimensional weight vector 
weight vector get value source just copy values dimension 
reconstruction error noise term evaluated see problems arise variance parameter likelihood goes infinity goes zero 
applies posterior density basically just likelihood multiplied finite factor 
model completely useless rated infinitely point estimates 
problems typical models estimates noise level products 
avoided fixing noise level certain 
noise model nonstationary see section problem worse infinite likelihood appears variances goes zero 
anderson moore 
optimal filtering 
prentice hall englewood cliffs nj 
attias :10.1.1.49.62
independent factor analysis 
neural computation 
attias 
variational bayesian framework graphical models 
editor advances neural information processing systems pages cambridge 
mit press 
attias 
ica graphical models variational methods 
roberts editors independent component analysis principles practice pages 
cambridge university press 
barber bishop 
ensemble learning bayesian neural networks 
bishop editor neural networks machine learning pages 
springer berlin 
beal 
variational algorithms approximate bayesian inference 
phd thesis university london uk 
beal ghahramani 
variational bayesian em algorithm incomplete data application scoring graphical model structures 
bayesian statistics pages 
bishop 
neural networks pattern recognition 
clarendon press 
bishop 
latent variable models 
jordan editor learning graphical models pages 
mit press cambridge ma usa 

cardoso 
multidimensional independent component analysis 
proc 
ieee int 
conf 
acoustics speech signal processing icassp pages seattle washington usa may 
chan 
lee sejnowski 
variational learning clusters nonsymmetric independent components 
proc 
int 
conf 
independent component analysis signal separation ica pages san diego usa 
penny roberts 
ensemble learning approach independent component analysis 
proc 
ieee workshop neural networks signal processing sydney australia december pages 
ieee press 
dayan zemel 
competition multiple cause models 
neural computation 
doucet de freitas gordon 
sequential monte carlo methods practice 
springer verlag 
frey hinton :10.1.1.112.3902
variational learning nonlinear gaussian belief networks 
neural computation 
gelman carlin stern rubin 
bayesian data analysis 
chapman hall crc press boca raton florida 
ghahramani beal 
propagation algorithms variational bayesian learning 
leen dietterich tresp editors advances neural information processing systems pages 
mit press cambridge ma usa 
ghahramani hinton 
hierarchical non linear factor analysis topographic maps 
jordan kearns solla editors advances neural information processing systems pages 
mit press cambridge ma usa 
ghahramani roweis :10.1.1.16.3419
learning nonlinear dynamical systems em algorithm 
kearns solla cohn editors advances neural information processing systems pages 
mit press cambridge ma usa 
gray fischer schumann buntine 
automatic derivation statistical algorithms em family 
advances neural information processing systems 

hierarchical variance models image sequences 
helsinki univ technology dept computer science eng espoo finland march 
master science dipl eng 
thesis 
available www cis hut fi 
kab variational bayesian method rectified factor analysis 
proc 
ieee international joint conference neural networks ijcnn pages montreal canada 
honkela valpola karhunen 
bayes blocks implementation variational bayesian building blocks framework 
proceedings st conference uncertainty artificial intelligence uai pages edinburgh scotland july 
haykin 
neural networks comprehensive foundation nd ed 
prentice hall 
haykin editor 
kalman filtering neural networks 
wiley new york 
hinton van camp 
keeping neural networks simple minimizing description length weights 
proc 
th ann 
acm conf 
computational learning theory pages santa cruz ca usa 
jen rensen winther hansen 
mean field approaches independent component analysis 
neural computation 
honkela 
speeding cyclic update schemes pattern searches 
proc 
th int 
conf 
neural information processing iconip pages singapore 
honkela valpola 
kernel pca initialisation variational bayesian nonlinear blind source separation method 
prieto editors proc 
fifth int 
conf 
independent component analysis blind signal separation ica volume lecture notes computer science pages granada spain 
springer verlag berlin 
honkela rio 
empirical evidence linear nature 
proc 
th european symposium artificial neural networks esann pages bruges belgium 
honkela valpola 
variational learning bits back coding information theoretic view bayesian learning 
ieee transactions neural networks 
honkela valpola 
unsupervised variational bayesian learning nonlinear models 
saul weiss bottou editors advances neural information processing systems 
mit press cambridge ma usa 
appear 
honkela valpola karhunen 
accelerating cyclic update algorithms parameter estimation pattern searches 
neural processing letters 
hyv rinen hoyer 
emergence phase shift invariant features decomposition natural images independent feature subspaces 
neural computation 
hyv rinen hoyer 
emergence topography complex cell properties natural images extensions ica 
solla leen 
mller editors advances neural information processing systems pages 
mit press cambridge ma usa 
hyv rinen karhunen oja :10.1.1.107.3613
independent component analysis 
wiley 
valpola 
effect form posterior approximation variational learning ica models 
proc 
th int 
symp 
independent component analysis blind signal separation ica pages nara japan 
valpola oja 
nonlinear dynamical factor analysis state change detection 
ieee trans 
neural networks may 
jordan editor 
learning graphical models 
mit press cambridge ma usa 
jordan ghahramani jaakkola saul 
variational methods graphical models 
jordan editor learning graphical models pages 
mit press cambridge ma usa 
jordan sejnowski editors 
graphical models foundations neural computation 
mit press cambridge ma usa 
kohonen 
self organizing maps 
springer rd extended edition 
kohonen kaski 
self organized formation various invariant feature filters adaptive subspace som 
neural computation 
honkela 
bayesian nonlinear independent component analysis multi layer perceptrons 
girolami editor advances independent component analysis pages 
springer verlag berlin 

ensemble learning 
girolami editor advances independent component analysis pages 
springer verlag berlin 
mackay 
practical bayesian framework backpropagation networks 
neural computation 
mackay 
developments probabilistic modelling neural networks ensemble learning 
neural networks artificial intelligence industrial applications 
proc 
rd annual symposium neural networks pages 
mackay 
ensemble learning hidden markov models 
available wol ra phy cam ac uk mackay 
mackay 
monte carlo methods 
jordan editor learning graphical models pages 
mit press cambridge ma usa 
mackay 
local minima symmetry breaking model pruning variational free energy minimization 
available www inference phy cam ac uk mackay 
mackay 
information theory inference learning algorithms 
cambridge university press 
minka 
expectation propagation approximate bayesian inference 
proceedings th conference uncertainty artificial intelligence uai pages seattle washington usa 
mackay 
ensemble learning blind source separation 
roberts editors independent component analysis principles practice pages 
cambridge university press 
murphy 
variational approximation bayesian networks discrete continuous latent variables 
proc 
th annual conf 
uncertainty artificial intelligence uai pages stockholm sweden 
murphy :10.1.1.25.1216
bayes net toolbox matlab 
computing science statistics 
neal 
bayesian learning neural networks lecture notes statistics 
springer verlag 
nolan kab 
datadriven bayesian approach finding young stellar populations early type galaxies uv optical spectra 
monthly notices royal astronomical society 
appear 
available www cis hut fi 

park 
lee 
hierarchical ica method unsupervised learning nonlinear dependencies natural images 
prieto editors proc 
th int 
conf 
independent component analysis blind signal separation ica pages granada spain 
parra spence 
higher order statistical properties arising non stationarity natural signals 
leen dietterich tresp editors advances neural information processing systems pages 
mit press cambridge ma usa 
pearl editor 
probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann publishers san francisco california 

pham 
cardoso 
blind separation instantaneous mixtures nonstationary sources 
ieee trans 
signal processing 

partially observed values 
proc 
int 
joint conf 
neural networks ijcnn pages budapest hungary 

nonlinear relational markov networks application game go 
proceedings international conference artificial neural networks icann pages warsaw poland september 

learning nonlinear state space models control 
proc 
int 
joint conf 
neural networks ijcnn pages montreal canada 
valpola karhunen 
missing values hierarchical nonlinear factor analysis 
proc 
int 
conf 
artificial neural networks neural information processing icann iconip pages istanbul turkey 
roberts editors 
independent component analysis principles practice 
cambridge univ press 
roberts 
hierarchy priors wavelets structure signal modelling ica 
signal processing february 
rowe 
multivariate bayesian statistics models source separation signal unmixing 
chapman hall crc medical college wisconsin 
schwarz 
estimating dimension model 
annals statistics 
spiegelhalter thomas best gilks 
bugs bayesian inference gibbs sampling version 
available www mrc cam ac uk bugs 
valpola karhunen 
hierarchical models variance sources 
signal processing 
valpola honkela 
bayes blocks software library 
available www cis 
hut fi projects bayes software 
valpola honkela karhunen 
ensemble learning approach nonlinear dynamic blind source separation state space models 
proc 
int 
joint conf 
neural networks ijcnn pages honolulu hawaii usa 
valpola karhunen 
unsupervised ensemble learning method nonlinear dynamic state space models 
neural computation 
valpola oja honkela karhunen 
nonlinear blind source separation variational bayesian learning 
ieice transactions fundamentals electronics communications computer sciences 
valpola karhunen 
nonlinear independent factor analysis hierarchical models 
proc 
th int 
symp 
independent component analysis blind signal separation ica pages nara japan 
valpola karhunen 
building blocks hierarchical latent variable models 
proc 
rd int 
conf 
independent component analysis signal separation ica pages san diego usa 
van hateren ruderman 
independent component analysis natural image sequences yields spatio temporal filters similar simple cells primary visual cortex 
proceedings royal society london 

selforganizing map matlab som toolbox 
proceedings matlab dsp conference pages espoo finland november 
available www cis hut fi projects 
wallace 
classification minimum message length inference 
aki editors advances computing information volume lecture notes computer science pages 
springer berlin 
bishop 
variational message passing 
journal machine learning research april 

