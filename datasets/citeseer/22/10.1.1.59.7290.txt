collective inference improves relational classification david jensen jennifer neville brian gallagher dept computer science univ massachusetts amherst governors drive amherst ma jensen cs umass edu procedures collective inference simultaneous statistical judgments variables set related data instances 
example collective inference simultaneously classify set hyperlinked documents infer legitimacy set related financial transactions 
studies indicate collective inference significantly reduce classification error compared traditional inference techniques 
investigate underlying mechanisms error reduction reviewing past collective inference characterizing different types statistical models making inference relational data 
show important differences models characterize necessary sufficient conditions reduced classification error experiments real simulated data 
categories subject descriptors artificial intelligence learning pattern recognition models 
general terms algorithms performance design theory 
keywords relational learning probabilistic relational models collective inference 

research relational learning produced novel types statistical models 
models estimate conditional joint probability distributions data :10.1.1.4.823:10.1.1.101.3165
researchers evaluated performance domains including classifying web pages tracking communicable diseases identifying topics scientific literature :10.1.1.4.823
focuses collective inference proce permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
kdd august seattle washington usa 
copyright acm 
simultaneous statistical judgments variables set related data instances 
collective inference exploit relational autocorrelation widely observed characteristic relational data value variable instance highly correlated value variable instance 
studies shown making inferences multiple data instances simultaneously collective inference significantly reduce classification error 
show reduced error attributed collective inference results clever factoring space possible statistical dependencies relational data 
factoring produces relational models parameter space incrementally larger non relational counterparts variance component error roughly equivalent 
relational information informative bias component error identical models relational information informative bias vastly reduced 
increased algorithmic complexity collective inference purchases large increase representational power minimum cost 
relational models exploit collective inference generally larger parameter spaces require larger data samples learn relational models reliably 

probabilistic relational models traditional graphical models bayesian networks dependency networks assume data consist independent identically distributed instances inference procedures models instantiate separate network data instance 
dependencies run networks assumption data instances independent 
algorithms constructing probabilistic relational models prms remove independence assumption instantiate single network represents dependencies data instances test set :10.1.1.4.823
procedure called common graphical models assume form instance dependence including prms hmms 
examples prms include relational bayesian networks relational markov networks relational dependency networks bayesian logic programs blps :10.1.1.4.823
types relational models process sufficiently general applied wide variety graphical models relational data 
type model imposes different constraints possible dependencies network 
types collective inference possible 
example consider data graph regular structure objects arranged lattice 
object lattice links immediate neighbors 
exception objects outer boundary object links positioned left right 
links undirected 
object characterized set variables includes single probabilistic variable class label variables ai attributes values known certainty 
task construct joint model probability distribution values class labels 
task multiple models infer values focus models experiments intrinsic object intrinsic model estimates joint distribution class label attributes object 
assumes objects corresponds traditional models knowledge discovery applications 
model depicted graphically plate notation common graphical modeling community 
inner box edge connecting indicates different versions node corresponding attributes ai depend outer box indicates model creates different versions network containing single node example model indicate words web page attributes ai depend topic page independent topic words page 
relational models relational model simple relational model indicating attributes object depend class label object class labels objects link away 
shows model modified plate notation integer diamond shaped annotation indicates graph distance neighboring objects multiplier edge indicates number neighboring objects 
path annotated edge outside outer box emphasizes dependence class labels adjoining objects 
value ai depends different parents neighboring objects 
example model indicate words web page depend topic page topics adjoining pages 
relational somewhat complex relational model indicates attributes object depend class label object class labels objects links away 
models allows interdependence class labels prerequisite collective inference 
examine additional models allow dependence collective inference ci model ci shown provides type dependence intrinsic adds dependence class label object class label adjoining objects 
equivalent specifying topics web pages depend adjoining pages determine words page 
relational collective inference rci model rci shown extends model adding dependence class labels neighboring objects link away 
collective models models relatively simple example data highly regular contain single object link type 
heterogeneous data require models longer complex paths objects 
example paths connecting autocorrelated objects pass intervening objects specified types 
simplicity example allows focus critical aspects learning inference relational data 
previous performance comparisons collective inference small active area research relational learning years publication chakrabarti dom indyk detailed study hypertext categorization strategies 
studies collective inference extended broadened :10.1.1.57.9485
extended basic paradigm collective inference incorporate selecting range possible actions 
example domingos richardson mining network value customers incorporates collective inference larger approach viral marketing 
table summarizes types models evaluated key papers 
studies collective inference reported large reductions error method applied 
example chakrabarti report large reductions classification error including drop error 
previous authors reported significant accuracy gains relatively simple technique collective inference 
provost show models consider autocorrelation class labels equivalent ci attributes perform small fraction class labels known :10.1.1.57.9485
studies pointed collective inference various types reduce accuracy 
example chakrabarti discuss experiment including rela tional information web pages reduces accuracy 
hypothesize additional features meant learning inference scheme overwhelmed signal noise ratio 
table previous categorization 
intr ci rci chakrabarti slattery mitchell neville jensen taskar taskar provost neville jensen collective models results appears clear collective inference capable significantly improving probabilistic inferences relational data :10.1.1.57.9485:10.1.1.4.823
important questions remain circumstances collective inference improve accuracy relational models 
reasonable explanation power collective inference lies merely larger feature space provided models ci 
models consider features expressive cousins 
experiments show explanation inadequate explain power collective inference 
show methods collective inference benefit clever factoring space dependencies 
models ci rci substantially smaller parameter spaces model benefit information propagated outside local neighborhood 
predictions class label objects essentially bundle information graph immediate neighborhood 
addition collective models known class labels known topics web pages improve inferences unknown labels 
provides new highly reliable additional feature learning inference 
increased representational power purchased incremental increase parameter space 
way ci rci emulate robust techniques simple bayesian classifiers linear regression models 
assumptions violated ci rci perform 

methods evaluate different models inference methods conducted experiments real synthetic data 
yeast protein experiments empirical experiments considered relational data yeast genome containing information genes interactions associated proteins www cs wisc edu kddcup 
gene location function autocorrelated dataset expect testbed investigating relative performance various relational models 
learning task predict gene localization cell 
locations ranging plasma membrane default error rate 
addition gene location gene boolean attributes indicating gene function 
gene may functions 
non collective models relational bayesian classifiers predict gene location function attributes 
intrinsic model considered function attributes genes 
model added function attributes genes link away interactions total attributes 
model added attributes genes links away total attributes 
collective models relational dependency networks represent component conditional probability distributions :10.1.1.4.823
ci model considered location attribute genes link away addition function attributes genes isolation 
rci model added function attributes genes link away total attributes 
gibbs iterations models laplace correction zero values 
compare approaches evaluated zero loss fold cross validation trials 
report average error folds tailed paired tests assess significance results 
synthetic experiments generate synthetic data extended example section 
generated data regular twodimensional lattice structure 
rows columns frame lattice 
objects frame train models objects frame loss estimates inference performed objects lattice including frame 
training test sets size correspond lattice objects models trained evaluated objects core lattice 
object dataset contains set attributes 
dataset objects contain class label single attribute correlated depending dataset generation parameters objects may contain additional attributes correlated generated values attributes class labels ways label relational collective 
parameters table 
collective data generation assigning object lattice initial class label 
perform gibbs sampling entire lattice 
class labels assigned object iterations final labels 
assign class labels gibbs sampling manually specified model assigns class labels object class values neighboring objects link away 
parameters model varied produce different levels autocorrelation neighboring class labels 
class labels assigned value attribute randomly drawn distribution conditioned class label object derived data generation parameters 
random values assigned attributes ai 
dataset generated measure proportion objects positive class labels dataset value outside range discarded replaced new dataset 
ensures consistency datasets reduces variance estimated model performance 
table data generation parameters 
bold numbers value column indicate default values 
name description values number core objects training set 
number core objects test set 
number attributes proportion objects known class labels autocorrelation class labels neighboring object see text 
prior probability 

conditional probability 
relational data generation training parameters model large dataset consisting lattices objects 
attributes class labels objects lattice determined collective data generation method described 
train univariate model ai attribute ai 
create lattice objects usual way assign attribute values randomly object learned model ai assign class labels object attribute values neighboring objects links away learned model 
measured bias variance model decomposition defined squared loss domingos 
loss decomposed factors bias variance noise 
calculation variance straightforward relational data calculation bias 
fortunately synthetic data experiments know probabilities generative model optimal predictions 
bias variance estimates calculated test example different training sets averaged entire test set 
repeated test sets calculate average test set bias variance 

analyzing collective inference baseline inference accuracy shows squared loss function training set size models 
data produced collective generator unexpected ci rapidly converges relatively low loss 
continues reduce loss squared loss collective data generation relational data generation 
increases 
models achieve minimum error corresponding ci model provided perfect class information continues reduce loss steady rate 
points confidence intervals shows type results relational data generator 
match data generator ci rci outperform models small 
continues corresponding high loss eventually drop ci rci declines optimal value loss high 
performs similarly dropping ci 
confidence intervals 
obtained similar results experiments yeast protein data shown table 
ci resulted lowest loss loss significantly different loss models 
table yeast protein data zero loss results 
model attributes zero loss value intrinsic ci rci responsible low error ci models 
measured bias variance probability estimates model compare decomposed loss function 
figures show results relational generator 
collective generator variance results qualitatively similar bias varied slightly range 
models bias quickly nearly level 
bias continues decline data slowly accumulate joint distributions overcome prior initial laplace correction prevent zero probabilities 
variance level intrinsic ci continues decline rci values 
points confidence intervals 
large initial gap loss ci appears directly attributable size parameter space difficulty making estimates sparse data 
difference ci pronounced objects data contain attributes 
number attributes object increased shown loss compared models including ci loss remains nearly constant 
rci show marked increases loss smaller extent 
results data produced collective generation qualitatively similar 
growth number attributes modest compared common applications relational learning algorithms classifying web pages objects hundreds thousands attributes words web page 
applications ability ci provide built factoring feature space may essential 
loss decomposition bias variance relational data generation 
negative effect large parameter spaces rci plausible explanation results experiments yeast protein data 
rci perform worse ci 
strength probabilistic dependence data generation procedures parameters determine strength probabilistic dependence attribute class label relative performance models differs strength dependence 
depicts quantity loss loss ci varies function 
largest difference models occurs uniformly distributed dependence weakest 
ci performs best relative terms correlations exist autocorrelation class labels 
relative advantage ci disappears information available 
attributes useful ci able attain non random performance 
loss function 
relative error ci correlation varies 
strength autocorrelation relational autocorrelation refers correlation values variable related objects 
widespread occurrence autocorrelation strongest motivations relational inference kind 
effects noted explored researchers collective inference including provost taskar yang :10.1.1.57.9485
shows effect increasing levels autocorrelation relative performance different models 
models consider intrinsic greatly aided loss function autocorrelation 
loss function percentage data labeled 
relation relative ordering changes slightly 
aided autocorrelation aided 
results reveal advantage ci 
autocorrelation entirely absent ci performance equal intrinsic 
ci exploit autocorrelation significantly impaired absence 
proportion known values core collective inference inferences object inform inferences 
capability particularly useful values known certainty 
example predictions topic previously unvisited webpage may aided considering known topics previously visited pages 
shows varying proportion labeled data affects ci rci performance compares alternative inference scheme models labeled default 
conducting full collective inference default models terminate inference round gibbs sampling 
results indicate advantage collective inference non collective holding factors constant 
shown relative advantage collective inference reduced data labeled 
collective inference procedures necessary percentage true labels increases 
studies actively varied percentage known labels results closely parallel provost show relative advantage iterative inference procedure procedure reduces percentage labeled data increases :10.1.1.57.9485
show general collective inference procedure performs better class skew labels known certainty 
evaluated effect yeast protein data obtaining results shown table 
fold cross validation partitions learned ci model training partition applied model entire dataset 
collective inference varied proportion data labeled test partition unlabeled training partition labeled levels produce levels 
accuracy measured unlabeled instances averaged folds 
loss increases significantly data labeled significant difference performance labeled 
table yeast protein data results partial labeling 
model percent labeled zero loss value ci ci ci 
experiments real synthetic data indicate reduced error attributed collective inference results primarily clever factoring space statistical dependencies relational data 
models represent factoring combined algorithms collective inference greatly reduce bias data strong autocorrelation minimum possible increase variance 
autocorrelation absent models practically equivalent error non relational counterparts 

acknowledgments amy mcgovern provided technical assistance experiments 
effort supported labs graduate research fellowship afrl contract number 
government authorized reproduce distribute reprints governmental purposes notwithstanding copyright notation 
views contained authors interpreted necessarily representing official policies endorsements expressed implied afrl government 

chakrabarti dom indyk 
enhanced hypertext classification hyper links proc 
acm sigmod conference pp 

domingos unified bias variance decomposition zero squared loss 
proc 
th national conference artificial intelligence pp 

domingos richardson 
mining network value customers 
proc 
th international conference knowledge discovery data mining pp 

getoor friedman koller pfeffer 
learning probabilistic relational models 
relational data mining dzeroski lavrac eds springer verlag 
getoor segal taskar koller 
probabilistic models text link structure hypertext classifi cation 
proc 
ijcai workshop text learning supervision 
getoor rhee koller small 
understanding tuberculosis epidemiology probabilistic relational models 
journal artificial intelligence medicine vol 
pp 

jensen neville 
linkage autocorrelation cause feature selection bias relational learning 
proc 
th international conference machine learning pp 

kersting de raedt 
basic principles learning bayesian logic programs 
technical report institute computer science university freiburg germany june 
provost :10.1.1.57.9485
simple relational classifier 
proc 
kdd workshop multi relational data mining pp 

neville jensen 
iterative classification relational data 
proc 
aaai workshop learning statistical models relational data pp 

neville jensen 
supporting relational knowledge discovery lessons architecture algorithm design 
proc 
icml data mining lessons learned workshop pp 

neville jensen collective classification relational dependency networks :10.1.1.4.823
proc 
kdd workshop multi relational data mining pp 

neville jensen gallagher 
simple estimators relational bayesian classifiers 
proc 
rd ieee international conference data mining pp 

slattery mitchell 
discovering test set regularities relational domains 
proc 
th international conference machine learning pp 
taskar koller 
discriminative probabilistic models relational data 
proc 
th conference uncertainty artificial intelligence pp 

taskar segal koller 
probabilistic classification clustering relational data 
proc 
th international joint conference artificial intelligence pp 

yang slattery ghani 
study approaches hypertext categorization 
journal intelligent information systems 


