approximate methods propagation uncertainty gaussian process models girard thesis submitted university glasgow degree doctor philosophy october girard thesis presents extensions gaussian process gp model approximate methods allowing model deal input uncertainty 
zero mean gps gaussian covariance function particular interest allow carry derivations exactly having shown modelling abilities predictive performance comparable neural networks rasmussen 
model observed data new input making prediction corresponds computing gaussian predictive distribution associated output mean estimate 
way predictive variance provides error bars confidence intervals estimate quantifies model degree belief best guess 
knowledge predictive variance informative manner centre thesis problems propagate model account derivative observations available derive control law cautious behaviour addressed 
task making prediction new input model noisy introduced 
assuming normally distributed input mean variance corresponding non gaussian predictive distribution computed gaussian approximation 
depending parametric form covariance function process exact approximate moments derived 
results multiple step ahead iterative forecasting nonlinear dynamic systems propagation uncertainty 
nonlinear auto regressive representation system modelled gp step ahead model iterated desired horizon 
time step uncertainty induced successive prediction propagated predictive distribution output just predicted fed back state time step 
predictive variances delayed output accounted cross covariances 
approach illustrated simulated mackey glass chaotic time series real life dynamic processes gas liquid separator ph process 
emphasis gaussian processes modelling nonlinear dynamic systems 
gps gained widespread popularity engineering community 
known modelling systems practice rendered difficult fact available data lies equilibrium regions points transient areas common approach consider equilibrium points 
derivative observations elegantly integrated gp model function observations available shown 
accord engineering practice derivative observations potentially reduce computational burden usually associated gps typically linear region summarised derivative observation function observations 
mixed training set explicit expressions predictive mean variance function output corresponding noise free noisy input derived tackled gaussian approximation 
field gps advantage models control nonlinear dynamic systems 
commonly researchers nonlinear parametric models adopted certainty equivalence principle deriving control law model predictions true system outputs 
deriving controllers cautious probing features difficult scope control community 
propagation uncertainty method applied cautious controller accounted cost function disregard variance associated model estimate 
ii declaration declare thesis composed similar dissertation submitted application degree 
describes carried september march department computing science university glasgow supervision murray smith 
contained result collaboration researchers stated chapter section 
lucky phd multi agent control research training network promoted facilitated exchange researchers gave chance meet kai gregor friends 
glad took board rod gave chance 
know quite hard gone hadn patient open minded understanding 
working carl jus 
wise am deeply grateful prof mike titterington comments suggestions explanations 
jian qing helped shaping ideas john patiently listened ernest best discussions year glasgow 
prof bill deeply respect 
greatly value discussions range 
come really appreciate glasgow beautiful city lots offer chips cheese rain included 
met robert ali louise philippe matthew henrik rolf special important everybody soup kitchen particular isa craig jane people street big share 
axel benjamin supported needed 
years taught lot 
aware don know 
understanding views changed 
hope ll able say years 
welcome changes nonlinear dynamic aspect lives 
scientific theories discoveries laws nature inventions human mind papoulis iii iv contents motivation background 
contribution outline 
joint publications 
modelling gaussian process brief historic 
gaussian random functions 
simple illustration 
covariance functions 
gp modelling noisy observations 
joint 

marginal 

conditional 
relationship bayesian parametric modelling 
dealing noisy inputs 
background 
motivation 
prediction uncertain input 
possible approximations 
analytical approximation 
gaussian approximation approximate moments 
delta method 
approximate mean variance 
approximating directly 
gaussian approximation exact moments 
case linear covariance function 
case gaussian covariance function 
qualitative comparisons 
extension learning task 
defining noisy process 
inference prediction 
summary 
vi contents modelling nonlinear dynamic systems 
iterative multi step ahead forecasting 
propagation uncertainty algorithm gaussian approximation monte carlo alternative 
step ahead prediction mackey glass chaotic time series 
gaussian approximation 
exact gaussian approximation monte carlo 
summary 
real life applications modelling gas liquid separator process 
model selection 
validation 
ph process 
mixed model identification 
step ahead predictions linear model 
concluding remarks 
including derivative observations allowing cautious control incorporating derivative observations gp model 
derivative process 
output predictive distribution new input 
prediction noisy input 
gps control 
step ahead cautious control 
exploration multi step ahead control 
probabilistic view 
summary 
mathematical formulae note taylor approximation appendix chapter mixed training set prediction noisy input 
noisy mean 
noisy variance 
cross covariance terms iterative multi step ahead forecasting 
gp modelling affine miso system 
predictive distribution 
predicting 
list figures gaussian covariance function 
gaussian covariance matrices 
realisations zero mean gp 
gaussian joint distributions 
modelling gaussian processes 
gp regression 
prior posterior samples 
prediction noise free inputs 
prediction noisy inputs monte carlo approximation 
distribution samples 
dealing noisy test input 
prediction noisy inputs approximate moments gaussian approximation prediction noisy inputs exact moments gaussian approximation 
predictive distributions 
losses obtained monte carlo 
mg time series steps ahead gaussian approximation 
mg time series steps ahead gaussian approximation 
mg time series steps ahead gaussian approximation 
mg time series step ahead predictions 
mg time series error predicted points 
mg time series steps ahead monte carlo approximation 
mg time series predictive distributions various 
mg time series evolution losses 
process plant jozef stefan institute 
identification structure 
identification validation signals 
step ahead predictions 
simulation pressure signal 
ph process 
ph data 
residuals fitting linear model 
step ahead predictions ph signal 
step ahead predictions ph signal means 
step ahead predictions ph signal variances 
vii viii list figures ph steps ahead 
list tables model comparison 
model comparison 
model comparison 
negative log likelihood training error model 
test errors simulations 
evolution test errors 
ix list tables chapter various extensions gaussian process model engineering applications 
central prediction system output new noisy random input 
problem addressed analytical approximation consists computing mean variance corresponding predictive distribution gaussian approximation 
results applied iterative multiple step ahead prediction nonlinear time series showing uncertainty induced successive prediction propagated ahead time 
methodology propagation uncertainty framework control nonlinear dynamic systems case derivative observations system available 
motivation background mathematical modelling seeks describe model phenomenon system observations measurements 
weather wish forecast dynamics aeroplane want simulate getting understanding minds 
possible levels description choice general dictated knowledge phenomenon goals tools computational resources 
thesis emphasis systems responses outputs correspond causes inputs 
supervised learning setting modelling corresponds finding underlying data chapter 
generative mechanism mapping input causal space output observa tional space 
empirical models considered called black box models require detailed understanding process system study opposed principles models 
functional mappings conveniently divided classes parametric nonparametric 
parametric models assume form mapping parametric form fixed advance nonparametric class allowing greater flexibility re fer hastie bishop general textbooks gershenfeld broad audience mathematical modelling 
gp alternative gaussian processes gps came attention machine learning community nineties neal showed bayesian treatment neural networks hidden layer converged gaussian process number hidden neurons tends infinity suitable priors weights neal neal 
increasingly popular rasmussen carried thorough empirical comparison gp widely models showing instances gp outperforms rasmussen 
great deal research done dealing diverse aspects model 
thorough introductions model relation bayesian kernel models williams mackay seeger 
probabilistic nature gp model allows directly define space admissible func tions relating inputs outputs simply specifying mean covariance functions process 
framework observations correspond incomplete realisation process 
parametric form pre supposed albeit functional family come model powerful flexible gaussian assumption keeping derivations analytically tractable simple 
predictions uncertainties commonly model generalise measurements predictions new observations 
predicting system response new input satis factory 
general wish estimate quantity need quantify 
motivation background degree belief estimate un certain prediction 
trust 
gp model learning task teaching observations corresponds tuning parameters covariance function process data hand simply done maximum likelihood manner 
model new input conditional past observations naturally obtain prediction uncertainty attached respectively mean variance predictive distribution output 
distribution gaussian readily obtained definition conditional probabilities consequence gp assumption 
perfect world 
inputs uncertain 
noisy inputs arise different situations instance faulty sensors system senses inputs imperfectly 
dealing noisy inputs known difficult task scope research 
statistics literature models dealing noisy regressors known errors variables models kendall stuart problem tackled deterministic freedman bayesian tas stephens approaches recover unknowns 
machine learning community shown closed form solutions exist gaussian basis function networks ahmad tresp mixture models proved deal naturally missing features tresp ghahramani jordan 
cases major diffi culty stems unknown probability distribution input assumed learnt data 
thesis task dealing noisy inputs handled analytical approximation assuming normally distributed inputs 
accounting time mentioned dynamic systems properties behaviour vary time modelled similar way provided suitable representation ses takens 
time series analysis box common approach assume possibly nonlinear relationship past values observed time series 
chapter 
nonlinear auto regressive nar representation system step ahead model identified challenging task forecast value time series say steps ahead time 
known multiple step ahead iterative forecasting step ahead model iterated desired horizon farmer judd small 
obvious naive way performing iterative prediction feed back previous estimates mean gaussian predictive distribution gp case shown sub optimal solution ahmad tresp tresp hofmann 
modelling nonlinear dynamic systems gps infancy murray smith murray smith girard 
possible reason simply engineering community parametric models probabilistic gp model gained widespread popularity 
gps appear suited modelling systems 
identification nonlinear dynamic systems experimental data rendered difficult fact usually data lie equilibrium points sparse data available transient regions far equilibrium 
conditions gp proves efficient model retains available data performs inference conditional current state local data 
uncertainty model predictions dependent local data density model complexity directly relates amount available data complex models needing evidence 
propagation uncertainty predicting ahead time incorporation derivative observations variance cautious control systems hopefully contribute wider gaussian processes dynamic systems modelling 
contribution outline chapter briefly introduces gaussian random functions gp machinery model regression tasks 
thesis zero mean processes considered covariance function defines process 
parametric function inputs gives covariances possibly nonlinear auto regressive inputs structure control inputs 
observation time function time possibly delayed values 

contribution outline corresponding outputs 
popular covariance function gaussian squared exponential conveying belief functions drawn process smooth continuous 
points close input space lead outputs correlated points apart covariance decaying exponentially 
having proved useful covariance function applications rasmussen convenient enables derivations analytically carried exactly 
assuming gp model identified clean noise free inputs task making prediction noisy input addressed chapter 
involves integration predictive distribution input distribution done approximations 
problem solved analytical approach consists computing mean variance new non gaussian predictive distribution approach refer gaussian approxi mation 
depending form covariance function process moments derived exactly cases gaussian linear kernels approximately taylor ap proximation covariance function hand 
simple static example analytical approach compared numerical approximation integral approximating true predictive distri bution simple monte carlo 
experiments taylor approximation validated computing approximate moments gaussian covariance function comparing exact ones approximation briefly discussed appendix 
chapter indicate challenging task training model inputs noisy tackled similar gaussian approximation 
chapter results previous chapters applied modelling multiple step ahead iterative forecasting nonlinear dynamic systems 
results derived chapter methodology propagate uncertainty induced successive prediction suggested 
iteration predictive distribution output just predicted fed back state time step 
state random vector mean composed delayed predictive means covariance matrix corresponding predictive variances diagonal 
predictive variances fed back cross covariances chapter 
delayed outputs accounted resulting full input covariance matrix model proceeds ahead time 
mackey glass chaotic time series iterative prediction propagation uncertainty obtained gaussian approximation compared monte carlo solution 
naive approach delayed predictive means shown lead poor predictions highlighting importance accounting uncertainty induced successive predictions 
chapter illustrates modelling gaussian process real life applications 
gas liquid separator process part plant situated jozef stefan institute slovenia 
measurements gas pressure water level reservoir subject controlled openness pressure valve aim model gas pressure 
subset selection approach automatic relevance determination tool neal mackay identification zero mean gp gaussian covariance function discussed 
number simulations infinite step ahead prediction test signal performed predictions obtained propagation uncertainty examined depending point simulations started 
application challenging ph process benchmark henson se borg measured ph subject control inputs 
process known nonlinear identification linear model gp residuals leads better step ahead predictive performance gp 
mixed model iterative step ahead prediction new signal propagation uncertainty performed 
modified version propagation uncertainty algorithm account interaction gp linear model time step 
experiment iterative scheme compared direct multi step ahead prediction method model trained directly predict steps ahead 
chapter presents extensions model 
incorporation deriva tive observations gp model interest reasons 
local linear models commonly engineering applications taken 
joint publications account model function observations available 
derivative observations po reduce computational burden usually associated gps derivative points lead similar performance model function observations 
con derivative process gp gp derivative observations relatively easily elegantly incorporated model 
mixed training set re predictive mean variance function output corresponding new input composed functional part derivatives part mixed components arising variance 
gaussian approximation address problem predicting noisy input details computation predictive mean variance particular case gaussian covariance function appendix section 
importance uncertainty associated point prediction outlined quantifies degree belief estimate 
natural prediction decision making process uncertainty attached accounted order cautious decisions 
line thought control context knowledge predic tive variance informative manner derive cautious cost function murray smith murray smith murray smith 
propagation uncertainty method applied cautious controller multi step ahead control miso multi input single output system 
appendix provides useful mathematical formulae thesis 
multi disciplinary nature involving aspects computing science statistics control engi neering led derivations transparent possible broad readership order ease task potentially interested researchers engineers accessing implementing ideas 
joint publications idea propagating uncertainties gaussian processes followed discussions rod murray smith carl edward rasmussen 
derivations approximate moments chapter 
predicting noisy input started carl edward rasmussen girard 
qui candela exact moments derived case gaussian covariance function girard qui candela outlined results directly apply relevance vector machine model candela candela girard 
document worked derivation simpler expressions link different cases consistent manner 
training gp model noisy inputs approach section significantly different girard murray smith 
am grateful professor mike titterington murray smith useful comments 
derived expressions incorporation derivative observations gp model implemented tested jus 
preliminary results submitted ifac world congress held 
current document additive white noise observed outputs considered worked coloured noise models ar ma arma murray smith girard 
taken part evaluation gp model methodology chapter various dynamic systems 
murray smith introduced application gps control context propagation uncertainty applied case led murray smith 
remainder document person plural 
document sole responsibility believe represents achievement collaborative useful discussions comments researchers met years 
anonymous reviewer comments results technical report helped revise approach 
chapter modelling gaussian process chapter intends provide comprehensive gaussian random functions gaus sian processes modelling purposes 
emphasis mathematical theory concepts involved practical utility ends 
hope simple illustrations convince unfamiliar reader potential flexibility model 
sec tion simply recall bayesian approach parametric modelling gps relate framework 
brief historic chapter gaussian processes mackay mackay goes back model time series analysis lauritzen 
model known kriging developed done field cressie 
geophysics pioneered bayesian formulation inverse problems gps 
model clearly formulated solve regression problems statistics hagan gained popularity machine learning community mainly works neal neal rasmussen 
bayesian interpretation model williams rasmussen williams neal mackay detailed introductions williams mackay seeger 
particular relation gp model gen chapter 
modelling gaussian process radial basis functions spline smoothing methods kernel models support relevance vector machines refer mackay tipping kimeldorf wahba models 
gaussian random functions random functions complex mathematical objects gaussian process simplest ran dom function fully characterised mean covariance functions papoulis 
argument time usually called gaussian stochastic process gaussian random field argument represents position say 
stochastic field mean function covariance function 
denote gaussian process gp variables gaussian process thought generalisation multivariate gaussian random vari ables infinite sets process gaussian joint distributions multivariate normal 
set inputs corresponding random dimensional normal distribution vector expectations mean values matrix covariances pairs points covariances outputs covariance function evaluated corre sponding inputs 
gaussian random functions thesis consider zero mean processes assuming prior information available contradict hypothesis 
example see assumption overly restrictive practice great variety functions generated zero mean process 
rea son constant mean assumption interested second order stationary processes recall process second order stationary constant mean covariance function depends distance inputs 
respect constant mean loss generality zero mean assumption natural 
refer hagan cressie non constant mean processes 
remainder thesis consider object dimensions capable facing dimensional probability density 
get feeling represents visualising tions process 
hope illustration highlight wide variety functions produced process zero mean simple covariance function 
simple illustration consider gaussian process dimensional zero mean covariance function corresponds variance 
correlation length represents length successive values strongly correlated correlation diminishing exponentially distance points increases defined 
look particular covariance function greater detail 
gp gp gp gp covariance data sample mean 
add extra constant term covariance function reflecting far mean function expected fluctuate mean process mackay gibbs 
function 
practice zero mean simplification dealt centring data chapter 
modelling gaussian process gaussian covariance function gaussian covariance function 
relates width kernel parameter amplitude 
parameter shows covariance functions gp continuous gp dashed 
gp implies correlation length corresponding broader function gp correlation length smaller variance gp implies maximum corresponding lower amplitude gp 
covariance matrix gp covariance matrix gp 
symmetric covariance matrices gaussian kernel amplitude covariances variances diagonal controlled 
gp left smaller gp points apart correlated 
values knowledge parameters enables compute 
gaussian random functions matrix covariances corresponding outputs 
shows covariance matrices formed gaussian kernel 
distance inputs increases covariances points gp left decreases rapidly gp right smaller value parameter 
note smaller variances gp diagonal terms controlled parameter 
illustrate action impact parameters possible realisations sample functions process 
left shows typical realisations drawn gp con lines gp dashed lines 
notice realisations smooth characteristic covariance function 
samples gp vary rapidly horizontal direction gp having larger amplitude variation 
observations related parameters large corresponds small correlation length implying rapid horizontal variations inputs large variance parameter allows realisations larger fluctuations away zero mean 
think knobs controlling horizontal vertical variations 
noted earlier gp set dimensional normal distribution 
right panel normalised histogram plots samples gp left gp right true probability distribution random variable gp gp 
illustrate point shows scatter plot samples gp left gp right 
plotted normalised joint distribution random variables computed true mean covariances 
recall covariance function gives covariances function 
particular case covariance function weighted function weighted euclidean distance inputs 
value gp compared gp see gp presents strong correlation corresponds uncorrelated random variables easily shown realisations gaussian process obtained convolution white noise square root covariance function process 
chapter 
modelling gaussian process gp samples gp gp gp left panel smooth samples gp continuous lines gp dashed lines zero mean gaussian processes gaussian covariance function different values parameters 
short correlation length larger parameters gp imply realisations rapid horizontal variation larger fluctuations zero 
corresponding random variable normally distributed zero mean variance gp gp right panel shows normalised histogram plot samples true corresponding dimensional density function red line 
inputs apart 
gp larger far apart allow strongly correlated 
covariance functions consider zero mean processes needed characterise gp covariance function 
simple example hopefully highlighted central role conveys information kind function generated process 
covariance function deter mines properties samples drawn gp applied regression controls data smoothed estimating underlying function 
wide choice valid covariance functions 
form covariance function admissible provided generates non negative definite covariance matrix 
covariance function satisfy 
gaussian random functions joint probability gp joint probability gp joint distribution top bottom gp left gp right 
gp distance large wrt allow noticeable correlations corresponding 
gp see distance inputs increases correlation decreases 
finite set points arbitrary real coefficients 
stationary covariance function previously mentioned process second order stationary constant mean note 
practice isotropic covariance functions widely 
invariant translation covariance depend values corresponding inputs distance separating 
defined widely see 
cressie 
process called intrinsically stationary 
intrinsic stationarity weaker second order stationarity considered holds semi 
chapter 
modelling gaussian process euclidean distance 
general class stationary covariance functions form gamma function modified bessel function second kind order differentiability parameter 
controls smoothness typical sample functions times differentiable 
class allows express prior lack knowledge sample function differentiability 
derived compactly supported kernels covariance functions vanish distance inputs larger cer tain cut distance class 
thesis kernels especially interesting allow computationally efficient sparse matrix techniques property appreciated invert matrices size data set 
approaches gaussian squared exponential covariance function 
covariance function sample functions infinitely derivatives fore smooth continuous observed example section 
machine learning community covariance function popular choice rasmussen demonstrated gp covariance function performed better popular models neural networks rasmussen 
usually expressed parameter referred roughness parameter relates correlation length direction 
mentioned correlation length represents length successive values strongly correlated correlation diminishing exponentially distance points increases 
large typical function expected nearly constant direction corresponding input feature thought irrelevant automatic relevance determination tool mackay neal neal mackay 
consider diagonal full matrix allowing modelling interactions different input dimensions covariance function referred simply gaussian covariance function statistics squared exponential covariance function machine learning community 

gaussian random functions williams 
parameter variance process controlling vertical scale variation relative zero mean process output space vertical amplitude variation typical function 
see chapter covariance function special importance allows exact evaluation integrals involved dealing noisy inputs 
non stationary covariance function covariances believed depend distance points input space values inputs take consider non stationary covariance functions 
simplest corresponding linear trend component 
covariance function easily derived considering linear model gaussian prior parameters 
similarly covariance discussed relationship gps parametric models ready feel tight link bayesian parametric modelling gp model form covariance function thought dictated form parametric model assumed place 
schervish class non stationary covariance functions introduced includes non stationary version covariance 
gibbs uses similar chapter 
modelling gaussian process form gaussian covariance function allowing spatially varying length scales letting function 
neal neal williams derives analytically covariance function corresponding networks sigmoidal gaussian hidden units williams corresponds multi layer perceptron neural network sigmoidal transfer function range input augmented unit entry analogy bias term covariance matrix normally distributed hidden unit weight 
alternative non stationary gp mixture stationary processes allowing variable smoothness different parts input space done tresp rasmussen ghahramani shi 
gp modelling noisy observations turn gaussian processes regression problems 
set dimensional inputs corresponding observed scalar outputs wish find mapping inputs outputs able predictions system responses new inputs 
due external disturbances measurement noise observations seen noisy versions true system responses 
realistic noise models covariance function implemented rasmussen www gatsby ucl ac uk carl code form considered mackay gibbs goldberg murray smith girard restrict attention additive white noise priori unknown variance noise independent identically distributed observations 
data generative mechanism written 
gp modelling noisy observations noise free response system input 
joint 
moment wish specify form covariance function simply assume depends set unknown parameters 
zero mean gp covariance function joint probability distribution normal zero mean vector covariance matrix noise white variance simply identity matrix 
split sets purpose vector scalar similarly corresponding inputs 
splitting accordingly write joint distribution follows matrix giving covariances zero vector giving covariances variance 
joint distribution perform learning prediction task respectively marginalizing conditioning observed data see 

marginal 
observed likelihood data corresponds appropriate marginal part 
corresponds joint probability distribution evaluated refer appendix brief note joint marginal conditional gaussian distributions 
chapter 
modelling gaussian process 
modelling gaussian processes circled variables random variables non circled ones observed 
corresponding set random variables joint multivariate gaussian distribution 
additive independent white noise assumption corresponding circled jointly normally distributed 
marginal part joint gives probability data posterior predictive distribution equivalently corresponding obtained conditioning data 
observed data data covariance matrix 
set free parameters parameters covariance function noise variance 
take maximum likelihood approach find unknown parameters minimising denotes determinant 
doing conjugate gradient optimisation technique line search rasmussen requires computation derivatives respect parameter 
gp modelling noisy observations denotes trace 
requires inversion covariance matrix iteration computationally demanding increases techniques developed reduce computational cost see williams seeger seeger murray smith pearlmutter shi 
note non zero mean processes method restricted residual maximum likelihood patterson thompson 
refer rasmussen neal bayesian treatment priors put parameters covariance function 

conditional having set parameters predictive posterior distribution corresponding new input readily obtained conditioning joint probability distribution observed data 
shown von mises conditional distribution gaussian mean variance general interested predictive distribution noise free 
denoting predictive mean predictive variance directly probable output estimate response system associated uncertainty define confidence interval predictor error bars 
data directly making predictions uncertainty model predictions depends local data density model complexity relates amount distribution available data williams 
note particular ordering inputs corresponding smoothing filtering 
corresponds extrapolation task prediction case 
chapter 
modelling gaussian process alternative ways writing predictive mean giving insight estimate observations seen weighted sum observed targets vector weights called smoothing effective kernel linear combination number data points increases value beta coefficients larger amplitude smoothing kernel alpha coefficients smaller 
relates directly behaviour depending number data points far close new training inputs 
example illustrates gp modelling noisy data 
particular case choose data come realisation zero mean gaussian process gaussian covariance function corresponding correlation length dimensional argument 
select training cases random corrupt outputs white noise variance 
starting optimisation minus log likelihood initial guess parameters converges iterations 
maximum likelihood ml parameters 
estimated estimated values satisfactory considering small number data points unevenly spread input space 
predictions 
left shows underlying function realisation true underlying gp training cases crosses predictions error bars circles 
right upper plot corresponds covariances test training cases crosses circles bottom plot smoothing kernels 
training points predictive variance small model confident prediction error bars significantly larger test input lies region training inputs 
plot covariances 
gp modelling noisy observations prediction covariance training inputs smoothing kernel left gp modelling noisy observations crosses 
mean predictions associated error bars circles true function continuous line 
right covariances test training inputs upper plots smoothing kernels bottom plots 
crosses correspond circles 
test training inputs indicates diminish rapidly smaller values 
shows samples drawn zero mean gp prior dashed lines predictive posterior process conditioned training data test inputs tions drawn dimensional normal distribution mean vector covariance matrix computed 
notice edge effect samples start diverging due lack training data 
experiments assess predictive performance model computing average squared error average negative log predictive density true output model estimate predictive mean associated predictive variance average number test points 
measure predictive perfor chapter 
modelling gaussian process samples prior posterior samples prior process dashed lines posterior dotted lines conditioned training data crosses 
mance greater interest accounts model uncertainty predictive variance 
trades quality estimate accuracy model confident model prediction 
example obtain prediction 
smaller negative better prediction relationship bayesian parametric modelling reader familiar parametric models maximum likelihood approach section interest simply recall bayesian approach parametric modelling gps fit framework 
bayesian paradigm rests bayes formula comes double defini tion joint probability density product marginal conditional densities continuous variables normalised probability density 
definition 
relationship bayesian parametric modelling marginal probability density obtained integrating conditional probability density definitions follows joint probability density similarly consider marginal conditional write bayes rule simply obtained equating second equality obtained 
bayes formula appear mathematical tautology beauty bayesian approach interpret combination states information translated probability densities 
equation tells update state information go 
unconditioned called prior convey idea represents state knowledge observing logic conditioned posterior seen function likelihood denominator independent variable interest normalising leading constant called evidence marginal likelihood obtained integrating parameters 
normalising constant posterior observed propor tional likelihood multiplied prior 
chapter 
modelling gaussian process applied parametric data modelling corresponds parameters model de observed data 
equation enables update prior parameters corresponds probability data parameters fixed seen function parameters likelihood 
fixed corresponds generative model data white noise assumption reads 
predictive distribution corresponding new obtained integration posterior distribution parameters general assumed posterior independent new input 
note normalised distributions require evaluation usually intractable simple cases 
solved numerically markov chain monte carlo mcmc methods get samples posterior neal mackay approximate analytical treatment computing maximum posteriori map estimator maximisation posterior mackay 
seen gp prior imposed directly joint represents new function output set equivalently 
independent noise assumption corresponds marginal probability distribution data predictive distribution obtained equation 
thought recalling point important misleading calling gp model bayesian times done prior put space functions comes probabilistic nature 
relationship bayesian parametric modelling model bayes formula 
basic form considered setting hyper priors parameters covariance function gp machinery relies definition conditional probabilities bayes formula 
helpful interpret follows random variable rv coming gp zero mean covariance function zero mean variance 
assuming independent inputs corresponding rvs coming gp joint normal distribution implying random functions complicated mathematical objects way thought tool stems feasibility modelling covariances 
assume corresponding jointly normal zero mean simplicity mean having determine parameters entries covariance matrix observations 
enter realm stochastic processes view incomplete realisation gaussian random function zero mean covariance function covariances pairs points simply obtained computing typically depends parameters case covariance function 
point gp model explicitly model correlations observations 
chapter 
modelling gaussian process chapter dealing noisy inputs previous chapter saw gp model predictive distribution output corresponding new noise free input gaussian 
address problem predicting system output input noisy uncertain 
case integration input distribution leads non gaussian predictive distribution 
analytical approach consists computing mean variance distribution gaussian approximation 
show depending form covariance function process evaluate moments exactly approximately taylor approximation covariance function 
simple static numerical example compare gaussian approximation numerical approximation true predictive distribution simple monte carlo 
section indicate similar approximation taken deal challenging problem training gp model noisy inputs 
assuming gaussian covariance function show mean covariance function noisy non gaussian process derived accounting input noise 
far considered inference task noise free inputs situations inputs noisy uncertain ultimately missing complete noise 
noisy inputs arise different situations depending nature particular application 
chapter 
dealing noisy inputs background statistics models dealing uncertain inputs known errors variables models kendall stuart 
principal error variables models classical model model 
classical model observed input seen varying true value situation arising system senses inputs imperfectly observe noise corrupted version true inputs 
model observed input fixed true subject random errors zero mean 
model useful situations observed input set value unobserved true input varies setting 
estimating unknowns model parameters true inputs difficult task nonlinear models techniques consist substituting single value unseen input done regression calibration moment reconstruction freedman 
problem addressed stochastic version em algorithm treating true inputs hidden variables 
stephens bayesian approach taken infer unknown parameters mcmc techniques showing framework formulation type model classical model distinction 
machine learning community emphasis recovering true value missing noisy data estimation parameters model data missing 
ahmad tresp show closed form solutions exist gaussian basis function networks case noisy features solution depends form noise 
mixture models proved deal naturally missing features dual em algorithm ghahramani jordan 
feed forward neural network missing features integrated order compute likelihood data requiring model input density 
tresp estimate unknown input distribution directly data gaussian mixture model 
chapter new input assumed corrupted white noise assume gaussian probability distribution 
motivation see greater detail chapter interest prediction noisy input motivated iterative multiple step ahead prediction time series 
observed 
time series 
assuming simple model wish predict value time series say time 
having formed input output pairs input corresponds de layed value time series train gp learn mapping 
step ahead model prediction done iterating model predicting 
time series known time gp model predictive distribution readily obtained evaluated 
time step naive approach simply estimate evaluate 
see approach advisable reasons confident estimate leading predictions unrealistically small uncertainties throwing away valuable information variance associated estimate 
wish account uncertainty 
means able derive predictive distribution corresponding normally distributed ran evaluate need able dom input 
necessity able prediction uncertain noisy input obviously relevant static problems 
real experiments applications sensors detectors corrupted different sources disturbances 
observe noise corrupted version true input system senses input imperfectly 
model account extra uncertainty opposed uncertainty usually acknowledged observed outputs model confident misleading potentially dangerous say model output decision making process critical appli cation 
note static case approach suggest assumes prior knowledge input noise variance 
section problem making prediction noisy input gaussian process model highlight analytical approximation take leading computation predictive mean variance new predictive distribution 
chapter 
dealing noisy inputs prediction uncertain input assume data noise free zero mean gp covariance function model input output relationship zero mean variance 
saw chapter model new test input observed data predictive distribution corresponding output readily obtained 
distribution gaussian mean variance respectively 
shows predictive means dashed line error bars dotted lines computed test inputs 
gaussian process zero mean gaussian covariance function equation trained input output pairs crosses outputs correspond noise corrupted versions noise level 
near data points predictive variance model uncertainty small increasing test inputs moved far away training ones 
new input corrupted noise 
wish prediction need integrate predictive distribution input distribution 
nonlinear function note previous chapter denoted new input corresponding function output change notation simplicity 
notation indicates distributed normalised probability density 
bounds indicated assumed integrals evaluated 
prediction uncertain input predictions noise free inputs predictive means dashed line error bars dotted lines corresponding test inputs 
zero mean gp trained training points crosses learn underlying function continuous line 
new predictive distribution gaussian integral solved resorting approximations 
possible approximations techniques available approximate intractable integrals kind 
approximation meth ods divided deterministic approximations monte carlo numerical methods 
popular deterministic approaches variational methods laplace method gaussian ture consist analytical approximations integral 
refer mackay review methods 
numerical methods relying markov chain monte carlo sampling techniques evaluate numerically approximating true distribution see 
neal 
case numerical approximation simple monte carlo straightforward simply need sample gaussian distribution 
sample distribution see www gatsby ucl ac uk 
chapter 
dealing noisy inputs normal mean variance equations numerical approximation mixture gaussians identical mixing proportions number samples grows approximate distribution tend true distribution 
refer titterington review finite mixture models 
true test inputs left right observe asterisks 
samples centred noisy variance compute corresponding predictive means crosses error bars dots 
observed prediction prediction observed monte carlo approximation prediction noisy input asterisk 
true input distribution left right circles indicate output corresponding noise free 
samples mean variance compute crosses associated error bars dots 
histograms samples predictions shown 
cir cle asterisk indicate noise free noisy inputs respectively 
having computed losses squared error negative log predictive density predictions associated find input value loss minimum indicated triangle 
note closer true input compared observed input 
focus analytical approximation consists computing 
prediction uncertain input histogram histogram observed observed histogram samples predictions true input circle left right 
plotted observed noisy input asterisk taken mean sample leads minimum loss triangle 
moments mean variance 
analytical approximation distinguish mean variance gaussian predictive distribution noise free case denote mean variance non gaussian predictive distribution corresponding interpreted gaussian approximation mean variance respectively chapter 
dealing noisy inputs write short replacing expressions new predictive mean variance input noise distribution 
solvable integrals basically depends form covariance function 
note equations readily obtained law iterated expectations conditional variances see appendix 

gaussian approximation approximate moments 
covariance function linear gaussian polynomial mixture compute integrals exactly obtain exact mean variance 
section derive exact moments linear gaussian covariance functions 

approximate number ways 
interested closed form approximate solutions evaluate integrals taylor ap proximation covariance function mean obtain approximate mean variance 
note second case requires approximations required form variance function definitely solve integrals exactly simply prefer able integrals tractable cost long tedious calculations 
assuming access software mathematica matlab symbolic toolbox compute derivatives solutions obtained proposed approximation provide suitable performance implementation trade 
situation highlights analytical approximation take 
turn evaluation mean variance case covariance function approximations needed evaluate integrals analytically 
gaussian approximation approximate moments going approximate integrals analytically obtain approximate moments taylor approximation covariance function 
delta method delta method called moment approximation consists approximating integrand taylor polynomial 
dimensional case delta method stated follows lindley papoulis random variable mean variance chapter 
dealing noisy inputs integrate approximations numerical analytical compute mean variance depending 
approximate exact moments moments dealing noisy test input gp model data new input predictive distribution corresponding output readily obtained 
noisy corresponding predictive distribution obtained integration 
nonlinear integral analytically intractable 
numerical approximation integral possible concentrate analytical approximation 
suggest computing mean variance new predictive distribution done exactly approximately depending parametric form covariance function 

sufficiently small behaved write second derivatives evaluated 
results simply obtained considering expansion taylor series second order expectation sides directly find approximation 
variance 
gaussian approximation approximate moments corresponds approximation second order estimate neglecting term leading 
approximation motivated fact taylor approximation useful small standard deviations small chebychev inequality depart little rare occasions small lindley 
obviously conditions fulfil taylor series possible neighbourhood avoid anomalies behaviour away 
lindley state conditions assume covariance function expressions valid 
approximate mean variance approximate mean corresponds second order taylor polynomial approximation mean directly approximate mean note seen order estimate consider leading 
chapter 
dealing noisy inputs noise free predictive mean computed see appendix note approximation 
similarly approximate variance second order taylor approximation 
find simplification noise free predictive variance 
approximation comes discarding terms higher order discussed previous section 
similarly approximating approximate mean variance composed noise free predictive moments plus cor terms 
assumed diagonal correction terms consist sum derivatives 
gaussian approximation approximate moments covariance function input dimension weighted variance new test input direction 
illustrates results 
noise free inputs observe asterisks sampled distributions plotted 
circles indicate function output corresponding noise free approximate means associated uncertainties plotted triangles dotted lines 
compare naive noise free means error bars account noise input 
right hand side displays plots covariance function squared exponential second derivatives appear expressions approximate mean variance continuous lines correspond covariances inputs dashed lines circles indicating covariances training points 
sqrt ap ap approximate naive sqrt observed ap gaussian approximation prediction asterisk noisy version true noise variance 
left approximate mean uncertainty triangles 
noise free moments indicated crosses circles show function outputs corresponding noise free right top bottom covariance function derivatives noisy test inputs continuous line dashed line circles indicating covariances training inputs 
chapter 
dealing noisy inputs similarly approximating directly girard girard derived approximate mean variance solving equations directly replacing second order taylor approximations respectively 
applying directly substituting write simplification obtain 
gaussian approximation exact moments obviously taylor approximation implies second order taylor approxima tion covariance function 
results obtained working covariance function approach lacks flexibility highlight depen dence form covariance function clear exact moments computed particular cases illustrate 
gaussian approximation exact moments special cases linear gaussian squared exponential covariance functions evaluate integrals exactly 
case linear covariance function write linear covariance function 
noise free case prediction leads gaussian distribution mean variance subscript indicating exact linear case predicting noisy input predictive mean variance denoted chapter 
dealing noisy inputs formula giving expectation quadratic form gaussian see appendix directly obtain hand variance simplification terms 
alternatively terms noise free variance note write linear case new predictive mean noise free expect case predictive mean variance exactly correspond approximate moments obtain order approximation covariance function 
case gaussian covariance function noted previous chapter gaussian covariance function 
gaussian approximation exact moments special importance shown gp covariance function performed popular nonlinear models neural networks rasmussen 
denote noise free predictive mean variance 
predicting need compute directly notational convenience write gaussian covariance function 
product gaussians formula see appendix directly evaluation need product twice leading write random 
normalised probability density denote variables involved chapter 
dealing noisy inputs exact predictive mean replacing expression directly check expect 
assuming diagonal element diagonal 
recall noise free case relates variation direction linked correlation length 
length scale widened proportionally noise variance 
vertical amplitude variation formally controlled accordingly weighted 
flattening phenomenon increased correlation length decreased vertical amplitude 
useful write corrected version 
matrix inversion lemma leading gaussian covariance function simplifies matrix inversion lemma 
compared noise free covariances new noisy input training inputs weighted accounting uncertainty associated 

gaussian approximation exact moments exact predictive variance variance replacing expression find new predictive variance 
recalling predictive variance corresponding noise free computed gaussian covariance function check 
done predictive mean find form gaus sian covariance function appears weighted correction term 
matrix inversion lemma write chapter 
dealing noisy inputs 
show leading terms plus correction terms rewrite variance having replaced expression 
shown predictive mean variance obtained gaussian case tend approximate mean variance tends zero 
approximate mo ments shows exact predictive mean error bars triangles obtained predicting noisy inputs asterisks 
qualitative comparisons simple dimensional static example chapter compare predictive distributions monte carlo approximation gaussian approximation moments computed exactly approximately 
notations denotes monte carlo approximation true predictive distribution corresponding noisy input 
qualitative comparisons sqrt ex ex sqrt ex exact naive observed triangles indicate exact predictive means gaussian approximation accounting uncertainty noisy inputs asterisks 
denotes gaussian approximation computes mean variance distribution specifically moments computed taylor approximation computed exactly gaussian covariance function denotes naive predictive mean variances account noise input 
shows predictive distribution continuous dashed dots asterisks true noise free input left right input noise variance observe naive approach leads narrow distribution peaked mean value account uncertainty input 
example distribution defined approximate moments strongly similar defined exact ones supporting idea approximation valid useful 
monte carlo approximation true distribution highlights true distribution non gaussian 
gaussian approximation appropriate spreads area weight true distribution lie 
shows histogram losses squared error left minus log predictive density right computed samples monte carlo approxima tion predicting left right 
predictions average chapter 
dealing noisy inputs mc prediction ex ap ap ex predictive distributions axis obtained predicting noisy input mc prediction numerical approximation simple monte carlo correspond gaussian approximation moments computed exactly approximately naive predictive distribution account noise input 
losses computed sample means sample variances 
obtain 
naive approach leads 
samples samples samples samples squared error minus log likelihood computed samples monte carlo approximation observed noisy left right 

extension learning task extension learning task similar approximation taken chapter solve challenging prob lem training model presence noisy uncertain inputs 
assumption inputs independent normally distributed 
defining noisy process noise free recall gp prior zero mean covariance function implies consider situation noisy inputs sensed system 
process gaussian anymore determine mean covariance function shall call noisy process noted seeger page 
law iterated expectations write 
law conditional variances tells 
extending result covariances leads allow noise vary input 
denote noisy covariance function giving covariance 
assuming inputs independent characteristics define solvable integral depends form covariance function 
chapter 
dealing noisy inputs approximation taylor expansion consider case covariance function integral analytically intractable 
assuming perform taylor expansion approximate function second order polynomial evaluated 
replacing approximation integrating integral respect respect obtain assume inputs corrupted type noise variance expression simplifies note covariance function verify valid covariance function leading positive semi definite covariance matrix 
exact gaussian case positive 
particular 
positive need 
covariance function gaussian evaluate exactly 
notation gaussian covariance function case gaussian kernel implies condition input noise variance 
dimension 
extension learning task need evaluate product gaussians integrating leads integrating result respect noisy covariance function written assuming obtain diagonal parameter weighted corresponding uncertainty input dimension 
write note prior knowledge input noise variance fixed values preferable learn single parameter place 
inference prediction covariance function parameters learning prediction tasks difficult noise free case section previous chapter 
sim ply minimisation likelihood respect parameters 
chapter 
dealing noisy inputs similarly prediction new noise free input simply obtained conditioning training data 
predictive distribution corresponding output gaussian mean variance vector observed targets 
vector covariances noise free input noisy training inputs directly considering letting go zero 
consider case noise variance inputs simply similarly 
variance new input length scales bounded meaning corresponding process allow functions high resolution 
noted new kernel effect certainly obtained setting suitable prior 
noise inputs needs white extension coloured noise suggested murray smith girard straightforward 
summary central chapter prediction noisy input 
analytical approxi mation allowing gp model compute mean variance predictive distribution output corresponding input corrupted noise zero mean variance 
recall noise free case predictive distribution output correspond ing new gaussian mean variance new input noisy 
summary integrate input distribution leading intractable integral 
case analytical approximation propose consists computing mean variance corresponding new predictive distribution gaussian approximation 
mo ments respectively computed exactly approximately depending form covariance function process 
shown gaussian linear covariance functions noisy mean vari ance computed exactly linear case case gaussian kernel 
general covariance functions suggested tay lor approximation leading approximate moments 
simple numerical comparison gaussian approximation numerical approximation intractable integral simple monte carlo equation shown validity approach 
section introduced similar approximation deal difficult task learning model accounts noise inputs 
case gaussian covariance function shown model accounting extra noise opposed output noise gaussian kernel length scales bounded proportionally input noise variance 
proceed modelling nonlinear dynamic systems application pre diction noisy input iterative multi step ahead prediction task propagation un certainty 
chapter 
dealing noisy inputs chapter modelling nonlinear dynamic systems main objectives time series analysis forecasting 
step ahead models relatively easily obtained multiple step ahead ones constitute far challenging problem 
focus modelling nonlinear dynamic systems propose apply methodology previous chapters iterative multiple step ahead prediction time series propagation uncertainty 
assuming gp trained minimise step ahead predictions show formally incorporate uncertainty intermediate regressor values induced successive prediction predict ahead time updating uncertainty current prediction 
illustrate approach simulated mackey glass chaotic time series compare propagation uncertainty algorithm gaussian approximation monte carlo alternative 
comes dynamic systems modelling system identification branch dealing general process extracting information system measured input output data 
model identified simulation prediction controller design analysis 
refer ljung stoica general textbooks system identification 
typ ically distinguish fundamental models derived principles empirical models 
reason empirical models preferred detailed understanding chapter 
modelling nonlinear dynamic systems process required develop model 
empirical model class find state space kalman filters welch bishop julier uhlmann input output models 
distinct theories developed representations possible convert identified input output model state space model see 
phan 
consider input output class represented nonlinear auto regressive nar model 
discrete time system output time corresponding state 
additive noise term reflects fact output exact function past data 
representation motivated viewing observed dimensional time series projection underlying dynamics lie higher dimensional space takens 
order nar model corresponds number delayed outputs referred lag embedding dimension gives dimension reconstructed space casdagli farmer 
important note representation implicitly assumes statistical properties data time independent task reduced learning static mapping pointed bishop pages feed forward neural networks 
roughly speaking nonlinear system identification involves model struc ture selection choice model mapping selection noise modelling parameter estimation model validation 
commonly neural networks nonlin ear mapping principe kuo principe principe kuo bakker 
support vector regressors developed prediction purposes mukherjee ller extension linear dynamical systems see roweis ghahramani review kernels suggested buc 
gaussian processes infancy modelling dynamic systems murray smith murray smith girard 

particularly interested multi step ahead time series prediction challenging problem step ahead prediction task 
problem follows time series known say time wish predict get predictive distribution output time predictive horizon 
corresponds missing noisy data modelling problem missing 
currently predicting ahead time achieved directly iteratively 
direct method model explicitly trained learn predict steps ahead assuming model tailored fixed horizon difficult fix advance 
large system nonlinear drawback direct method general require large amount data get model larger amount missing data targets inputs 
focus iterative approach consists iteration step ahead model desired horizon 
case different ways dealing missing data 
numerical solutions consist integrating unknown missing variables weighted conditional probability density done albeit classification context tresp hofmann tresp hofmann 
naive way iterating step ahead model simply substitute single value missing value 
approach shown optimal ahmad tresp tresp hofmann illustrate numerical examples 
tresp hofmann stochastic sampling shown superior simply iterating system extended kalman filter 
obvious reasons favouring iterative method direct accurate step ahead models usually easy get step ahead forecast available prediction horizon 
known drawback iterative approach accumulation errors predict ahead time subsequent iterated prediction uses information step prediction 
small judd judd small aim improving long term predictions eliminating systematic errors induced successive short term prediction 
suggest eliminate error contrary missing variables seen noisy variables complete noise 
chapter 
modelling nonlinear dynamic systems propagate model predict ahead time 
iterative multi step ahead forecasting going show propagate model uncertainty induced suc prediction 
time step feed back predictive mean single point estimate predictive variance represents associated uncertainty 
consider nar model functional modelled zero mean gp need ance function 
richer noise models arma models considered murray smith girard simply assume white noise 
step ahead model order get 
time series assumed known time predictive distribution readily obtained 
equations 
naive approach point estimate consider suggest feed back predictive distribution account model uncertainty estimate mean state random vector zero covariance matrix apart entry takes back chapter task making prediction random input 
approach linked extended kalman filter summarises past data estimate mean covariance variables 

iterative multi step ahead forecasting propagation uncertainty algorithm gaussian approximation approximation chapter compute mean variance 
interpreting approximation gaussian write mean covariance matrix 
saw predictive mean variance computed exactly approximately depending form covariance function 
time step feed back repeat process desired horizon 
compute sketch proceed 

compute 
compute 
chapter 
modelling nonlinear dynamic systems repeating procedure 
time random input mean formed delayed previously predicted means covariance matrix delayed predictive variances diagonal need compute cross covariance terms input covariance matrix 
correspond covariances delayed outputs 
fill input covariance matrix progress ahead time equivalent computing cross covariances output input time step discarding oldest element 
simplicity notation write 
need compute 
expectation product replacing expression 
iterative multi step ahead forecasting chapter evaluate integral exactly approximately depending form 
denoting notational convenience integral wish evaluate 
case gaussian covariance function derive expressions cross covariances exactly case gaussian covariance function 
section previous chapter write gaussian covariance function wish solve 
substituting matrix inversion lemma write leading 

product gaussians gives short notation cross covariance terms chapter 
modelling nonlinear dynamic systems predictive mean obtained gaussian case 
recalling write check case zero 
write general case evaluate exactly previous chapter taylor approximation covariance function 
dimensional case need compute 
second order approximation covariance function write extending result dimensional inputs cross covariance terms simplifying 

iterative multi step ahead forecasting cross covariance terms input covariance matrix time 
note obtained result simply considered order taylor approximation covariance function 
compute full input covariance matrix time step 
advantage approach account uncertainty induced successive prediction cross covariances delayed predicted variables enabling access full joint distribution delayed outputs 
turn numerical solution propagation uncertainty iterative method 
monte carlo alternative seen time series assumed known time simply compute equations 
time step propagating uncertainty induced implies evaluation set training data zero 
computing mean variance done gaus sian approximation approximate integral numerically simple monte carlo number samples sample know note point sampling equivalent sampling letting 
chapter 
modelling nonlinear dynamic systems corresponding output normally distributed 
write distribution seen mixture gaussians mixing weight 
things start complicating normal mixture gaussians 
compute working distribution approximation consists considering gaussian mixture time 
write loop consider 
evaluate sample loop single gaussian leads mixture gaussians implying mixtures gaussians time step behaving branching process compu 
iterative multi step ahead forecasting tractable 
leads second approximation consider sample time desired horizon sample compute loop sample compute 
loop way effectively obtain gaussians output distribution time step approx true mixture gaussians distributions 

compute draw sample shift time window form compute experiments consider simplified version algorithm loop chapter 
modelling nonlinear dynamic systems loop notable difference expect algorithm previous terms smoothness state sample time step algorithm state com posed delayed sampled outputs sampled corresponding dimensional gaussian distribution treating independently 
hand previous algorithm state sample comes dimensional distribution full covariance matrix introducing smoothness 
sampling dimensional distribution cumbersome large 
investigated point speculate approximation great impact numerical approxima tion true distribution 
step ahead prediction mackey glass chaotic time series numerical example intended illustrate propagation uncertainty algorithm 
step ahead prediction mackey glass time series going compare predictions gaussian approximation particular propagating uncertainty assess quality predictions computed approximately comparing computed exactly case gaussian covariance function compare predictions naive approach ac count uncertainty induced successive predictions feeds back predictive means predicting ahead time 
compare exact gaussian approximation numerical approximation propagation uncertainty algorithm monte carlo 
recall predicting naive approach simply computes equations gaussian approximation predictive mean vari ance case gaussian covariance function approximate moments computed second order taylor approximation gaussian ance function 
step ahead prediction mackey glass chaotic time series taylor approximation 
mackey glass chaotic system constitutes known challenging benchmark ahead prediction task due strong non linearity mackey glass 
consider 
continuous time model discretised series re sampled period normalised 
assume step ahead nar model form vector outputs taken random series corrupt white noise variance corresponding matrix inputs 
train zero mean gaussian process gaussian covariance function set step ahead prediction training data obtain average squared error average negative density 
validation test set 
proceed step ahead predictions test series compare different methods 
gaussian approximation compare predictions obtained propagation uncertainty naive approach 
figures show predictions steps ahead different starting points test series 
left plots show mean predictions triangles circles indicating obtained propagating uncertainty computing means exactly approximately respectively 
dots correspond mean predictions naive approach 
clarity corresponding error bars standard deviation shown right plots 
see chapter section measures predictive performance introduced 
chapter 
modelling nonlinear dynamic systems predictive means time series ex ap iterative method action predictions steps ahead mackey glass time series continuous line 
left predictive means triangles circles corresponding propagation uncertainty naive approach error bars dots 
right confidence intervals error bars standard deviation different methods 
predictive means 
note predictive mean closest true value 
error bars predictions steps ahead starting 
legend step ahead prediction mackey glass chaotic time series predictive means error bars predictions steps ahead starting legend 
leads best prediction 
common figures fact naive method lead cases better mean predictions methods propagating uncertainty case associated uncertainties tight representative estimates 
particular mean prediction closer true value 
associated error bars small include true value 
mean predictions naive approach may significantly differ true time series model confident estimates 
terms mean predictions methods give similar estimates steps ahead start diverging 
illustrates case leads better predic tion compared naive approach error bars obtained propagating uncertainty informative 
difficult interpret interesting note error bars vary simply increase expect model uncertain predictions large amount information small 
general model uncertainty starts increasing quite rapidly prevent model rectifying leading smaller error bars confidence estimates particular simulation 
chapter 
modelling nonlinear dynamic systems shows step left step right ahead predictive means con intervals note plots point step ahead prediction 
upper plots correspond predictive means naive approach error bars tight distinguish means 
middle bottom plots correspond respectively shaded area represents uncertainty region 
ap ex step ahead predictions ap ex step ahead predictions step left step right ahead predictions test time series continuous thin line 
top bottom moments naive approach propagation uncertainty approach moments computed approximately exactly 
crosses indicate predictive means shaded regions correspond confidence uncertainty intervals 
note point step ahead prediction 
inadequacy naive approach evident error bars anymore rep model uncertainty 
approximate moments computed gaussian covariance function plots provide direct qualitative comparison 
step ahead predictions predictions look similar 
see generally cautious larger error bars 
interested having variances reflect quality estimates inaccurate mean predictions large error bars 
requirement fulfilled step ahead predictions apart step ahead predictions feel confident approximate moments lead sensible results 
point predicted shows corresponding squared error left step ahead prediction mackey glass chaotic time series minus log predictive density right 
clarity omit naive approach large compared obtained 
step ahead prediction ap ex left squared error log predictive density 
step ahead prediction ap ex predicted point approaches 
right minus provides quantitative evidence 
terms measures quality predictions comparing estimate predictive mean true value points naive approach going better 
terms accounts model uncertainty results far worse naive method propagating uncertainty 
plot demonstrates effectively comparable 
exact gaussian approximation monte carlo turn comparing exact moments obtained gaussian approximation monte carlo approximation propagation uncertainty algorithm 
monte carlo approximation compute samples time step resulting matrix predictive means similarly variances number starting points 
shows predictive means average compute losses thick line means crosses 
right chapter 
modelling nonlinear dynamic systems plot shows corresponding uncertainties error bars 
predictive means predictive means left error bars right samples average thick line crosses 
predictive error bars steps ahead error bars similar monte carlo approximation include samples 
plot predictive distributions shows steps ahead gaussian approximation close true distribution 
number steps increases true distribution approximated departs gaussian distribution assumed 
shows evolution average losses increasing methods 
losses correspond computed average sample mean variance 
results monte carlo approximation look surprising keep mind es quality approximation losses representative distribution normal 
summary assuming step ahead nar model form 
summary predictive distribution continuous line approximated gaussian approximation crosses av 
ap ex mc av 
mc ap ex evolution average squared error left minus log predictive density logscale right number steps ahead increases chapter 
modelling nonlinear dynamic systems modelled gp zero mean covariance function shown derive approximate predictive distribution uncertainty induced successive prediction accounted 
time results previous chapter write vector expectations covariance matrix corresponding random input 
setting mean vector formed delayed previously predicted means output estimates covariance matrix delayed predictive variances diagonal associated uncertainties cross covariances delayed outputs see section 
mackey glass chaotic system compare iterative step ahead prediction time series obtained propagation uncertainty algorithm gaussian approxima tion discussed monte carlo numerical approximation see section 
experiment shows number steps increases true distribution approximated monte carlo departs gaussian assumption 
gaussian approximation proves valid steps ahead 
compared naive approach predicts ahead time considering output estimate time step treating true observed value approximate tion uncertainty algorithm shows improve predictive performance model 
naive method consistently confident estimates associated uncertainties longer representative tight error bars encompassing true values 
hand model propagating uncertainty proceeds ahead time leads informative error bars estimates 
importantly model see predictive variance increasing predictive horizon 
chapter real life applications illustrate modelling forecasting real life processes gaussian process model methodology chapter focusing iterative step ahead predictions gaussian approximation 
gas liquid separator process identification structure performed subset selection approach automatic relevance determination tool occam razor principle 
test pressure signal simulated model propagation uncertainty 
second application ph process 
fitting linear model gp model residuals mixed model perform iterative step ahead prediction test ph signal propagation uncertainty 
modified propagation uncertainty algorithm accounting linear part 
iterative approach compared direct multi step ahead prediction method model tailored predicting steps ahead 
modelling gas liquid separator process gas liquid separator process part plant situated jozef stefan institute slovenia serves test bed various control engineering problems 
case gas simply oxygen liquid water 
pump feeds gas water mixture separator gas separated water 
internal pressure separator increases pressure valve opened blow gas separator unit 
chapter 
real life applications gas liquid separator part process plant jozef stefan institute slovenia 
pump feeds gas water mixture reservoir gas separated water 
shows schematic diagram identification structure 
separator valves top controlling pressure bottom controlling water level inside separator 
openness pressure level valves indicated respectively 
corresponds desired set point water level 
separator multivariate process inputs outputs controlled distinct univariate processes simplicity 
pressure controlled predictive controller water level pi controller 
pressure dynamics depend water level identification dynamics achieved controlling pressure valve varying water level 
pi controller gas liquid separator schematic representation identification pressure dynamics provided 

modelling gas liquid separator process application distinguish quantisation noise due transformation continuous digital signals noise measurement devices sensor noise 
valve openness signal pseudo random sequence discrete levels noise free noise water level signal coloured due controller gas pressure measurement 
typically noise broadband continuous spectrum range process dynamics behaviour similar white noise 
model selection denote measured gas pressure wish model measured controlled water level pressure valve openness 
shows signals identification validation gp model 
identification signals validation signals identification left validation right signals modelling gas liquid separator 
top bottom pressure water level openness pressure valve function time 
modelling pressure separator assume nonlinear autoregressive inputs structure gas pressure time sample delayed water level openness pressure valve control inputs white noise experiments subtract sample mean pressure signals 
chapter 
real life applications unknown variance 
identification signals form matrix inputs vector corresponding targets 
interactions exist delayed water level gas pressure separator gp gaussian covariance function diagonal means parameter delayed variable 
automatic relevance determination tool developed neal mackay bayesian treatment neural networks neal mackay parameters give indication relative importance delayed variable 
large implies large contribution input direction covariance interpreted important input prediction 
seen inversely proportional horizontal scale variation direction 
large corresponds rapid variations direction contrast small associated large correlation length slow variation direction implying change state little effect corresponding outputs 
note inputs normalised vary range compare weights different variables variable may small compared variable large variable considered 
respect compare relative variable denote weight corresponding delayed variable reflecting relevance corresponding delayed variable 
know priori optimal number delayed variables perform method subset selection train gps different structures varying number delayed variables state 
consider straightforward models decreasing values 
modelling gas liquid separator process 
test rationale choice keep relevant previous models treating variable time 
table indicates maximum likelihood ml estimates parameters corresponding delayed pressure variables appearing differ ent models 
similarly tables report ml estimates parameters associated delayed water level valve openness respectively 
table ml estimate parameters delayed pressure values different models clarity indicate corresponding delayed variable brackets 
model table ml estimate parameters corresponding delayed values water level 
model starting model see table weight assigned corresponding orders magnitude larger orders magnitude chapter 
real life applications table ml estimate parameters corresponding delayed values valve openness 
model larger 
similarly table shows weights associated order order magnitude larger weight associated 
pressure far relevant delayed value valve openness 
keeping delayed variables associated order state structure 
training gp structure find relevant delayed variable pressure water level openness valve 
delayed variables form state 
compare models rate predictive performance 
model compute step ahead prediction identification data corresponding average squared error average negative log predictive density table indicates results obtained minus log likelihood model learning 
table negative log likelihood losses achieved model step ahead prediction identification data 
model highest probability smallest minus log likelihood lowest 
terms losses best worst 
fact considers better indication importance delayed ignores delayed variable 

modelling gas liquid separator process poorer result compared explained fact model forced find weighting highlighted greater significance respectively 
compute ratios corresponding ratios train ing errors best model worst model find respectively meaning models quite similar 
argued increase model complexity worth computational effort terms predictive formance model compared simpler occam razor principle rasmussen ghahramani favouring simple models face complexity performance trade 
note artificially set zero parameters corresponding minus log likelihood increases making 
highlights fact simply ignoring inputs thought relevant model re trained allow re weighting remaining parameters 
light results choose model pressure signal structure model recall model training gp gaussian covariance function corresponding corresponding respectively 
estimated noise level 
validation step ahead prediction test model step ahead prediction validation pressure signal composed points 
left shows mean predictions error bars added model estimate output noise variance predict noisy chapter 
real life applications time series 
corresponding average losses obtained 
step ahead predictions log predictive variances step ahead prediction test series starting ms thin line 
left mean predictions thick continuous line error bars dotted lines 
right plot predictive variances 
plot log predictive variances right clearly indicates regions model accurate confident predictions correspond regions pressure varies slowly 
explained fact model trained rapidly varying signals see identification signals deal slowly varying signals explaining greater uncertainty constant shooting mean predictions regions 
note peak due rapid changes pressure signal slowly varying regions 
simulation know priori maximum prediction horizon horizon pre reasonably run simulation test set 
practice simulation infinite step ahead prediction corresponds steps ahead prediction length test series 
simple model propagation uncertainty gaussian approximation 
modelling gas liquid separator process straightforward water level openness valve signals assumed known delayed value pressure state computed predict ahead time 
assuming know pressure time simply compute 
provided pass exact model 
compute 
compute 

test set 
experiment mean variance time step computed exact equations corresponding gaussian covariance function 
simulation step ahead prediction test signal shown 
compare simulation propagation uncertainty explained naive simulation chapter 
real life applications simulation naive method simulation propagation uncertainty simulation test pressure signal thin line ms mean predictions thick continuous line associated error bars dotted lines naive approach left propagation uncertainty right 
account uncertainty induced successive prediction 
left plot shows predictions right 
mean predictions extremely similar difference order 
notice shooting model estimates slowly varying areas probably due fact model trained rapidly varying signals 
quite clear plots predictive variances standard deviations squared obtained show variability obtained range 
simulation upper script refers 
terms negative log predictive density propagation uncertainty better naive approach 
simulation started point test set step ahead pre diction errors indicates point 
expect link point start simulation average performance model variation naive propagation uncertainty approaches 
starting point point step ahead predictive losses expect average losses simulation better obtained start ing bad point methods differ slightly difference increasing 
modelling gas liquid separator process starting point model unsure prediction 
order validate hypothesis ran experiments start simulation corresponding point step ahead losses worst start simulation corresponding best step ahead squared error point start simulation point corresponding best step ahead negative log predictive density point 
fact best step ahead squared error negative log predictive density coincide point highlights difference measures goodness predictions 
recall squared error loss function estimate predictive mean negative log predictive density trades estimate uncertainty attached accurate estimate large variance larger estimate small variance 
terms average obtain squared error 
table reports value average negative log predictive density computed simulation 
assess difference methods compute ratio corresponding losses closer ratio similar methods 
table conclude consistently better naive approach note corresponds shortest simulation points imply prediction expected performance simulation affected starting point slightly better losses 
difference methods greater starting bad point ratio points respectively 
order compare fairly losses simulations average number points stopped simulations steps 
chapter 
real life applications table average negative log predictive density different simulations 
simulation showing propagation uncertainty copes better model uncertainty naive method simply ignores 
reasoning fact generalised point necessarily starting point simulation general model passed bad point expect predictions accurate model reliable methods possibly diverging systems model catch state 
application particular propagation uncertainty algorithm greatly im prove predictions system dynamic nonlinear gp simple structure delayed variables capture dynamics model predictions length test signal 
ph process turn challenging problem modelling nonlinear ph process depicted 
process consists acid stream buffer stream base stream mixed tank 
ph measured variable study controlled manipulating base flow rate flow control valve 
real dynamic system known complexity contains various nonlinear elements 
refer henson details 
investigation identification process 
ph process schematic diagram ph process 
structure suitable model ph signal control input 
shows portions signals identification left validation right model 
identification signals validation signals portions real signals identification left validation right model 
upper plots correspond measured ph function time lower ones control input 
terms trade model complexity average error training set 
chapter 
real life applications mixed model identification signals long points gp handle large data set sub sample identification signals resulting matrix training inputs corresponding vector target ph values 
process nonlinear great deal variation ph signal explained linear model decide fit linear model data model residuals gp gaussian covariance function 
classical linear model bias augmented input unit entry care bias 
squares ls linear parameters linear model average squared error training set shows portion residuals 
parameters linear model corresponding ph residuals plot subset residuals fitting linear model 
refer chapter section 

ph process assuming model residuals gp gaussian covariance function 
gp provides predictive distribution corresponds estimate residual time 
blending linear gp models estimate system output uncertainty mixed model training errors average errors step ahead predictions training set fitted gp directly original data obtained 
training errors linear gp model order magnitude better obtained gp 
shows portion onestep ahead prediction validation ph signal contains points 
test errors compared gp 
step ahead predictive means step step ahead prediction validation ph signal continuous line ahead predictive variances ms model mean predictions dotted line 
right plot shows corresponding predictive variances 
chapter 
real life applications step ahead predictions linear model wish multiple step ahead predictions propagate uncertainty need update algorithm section previous chapter account linear part time step 
modified algorithm simplicity denote input time 
time step refers contribution linear model gp 
algorithm follows augmented input vector zero 
ph process 


desired horizon 
time step predictive mean variance gp computed exactly gaussian kernel equations 
need compute predictive mean variance linear model cross covariance linear gp models arising predictive variance output time step cross covariance terms input covariance matrix 
linear part general time step input straightforward linear contribution mean variance vector parameters bias term augmented input vector 
variance output distribution mixed model output mean chapter 
real life applications 
need compute denotes training case 
seen chapter section integral evaluated exactly approximately depending form covariance function 
case gaussian kernel experiment directly write input input distribution time random vector normally distributed mean covariance matrix delayed variances diagonal elements remaining elements corresponding delayed control input zero 
seen section computing cross covariances delayed outputs time step corresponds computing discarding element 
know equation gaussian covariance function 
linear part simply going apply results step ahead prediction test ph signal 
furthermore compare predictive performance iterative method direct model trained predict steps ahead consider linear model gp 

ph process results perform step ahead predictions starting points taken random test set 
shows iterative step ahead predictive means shows predictive variances obtained naive approach certainty principle estimates time step propagation uncertainty exact moments 
note time index point step ahead prediction 
ex ex means steps ahead means steps ahead ex ex means steps ahead means steps ahead true ph values continuous line predictive means dotted lines steps ahead note point step ahead prediction axis simply indicates sample number time 
top plots indicate means naive approach bottom ones obtained propagating uncertainty 
see step ahead predictive means similar start differing 
step ahead prediction indicated arrow chapter 
real life applications ex ex variances steps ahead variances steps ahead ex ex variances steps ahead variances steps ahead step ahead predictive variances different values 
legend 

ph process naive approach overshoots prediction see arrow 
behaviour occurs point 
terms variances note prediction horizon predictive variances naive approach small 
variances order magnitude different stay range blowing effect large 
regardless amplitude variances see models necessarily agree points certain ones prediction uncertain similarly step ahead prediction 
look closely predictions steps ahead different starting points 
point variance exact method steps ahead maximum point minimum 
shows prediction steps ahead starting crosses indicating moments dots 
plots see starting mean predictions methods really 
interestingly variances start decreasing steps ahead 
starting mean predictions get worse uncertainty model increases case naive model 
values average errors table confirm remarks concerning iterative predictions propagation uncertainty 
table average squared error negative log predictive density different values 
increases order magnitude better terms 
large values obtained naive approach explained fact increases model uncertainty stays small estimates accurate 
report losses chapter 
real life applications predictive means predictive means predictive variances log scale predictive variances log scale top plots ph signal continuous line steps ahead starting point variance steps ahead maximum 
crosses correspond means left variances right obtained propagating uncertainty dots naive approach 
bottom plots starting point variance steps ahead minimum 

concluding remarks obtained training model directly predict steps ahead 
see relatively short prediction horizons iterative method propagation uncertainty performs better direct method due accuracy step ahead model 
predictive horizon grows larger direct model shows better predictive performance 
number missing data increases particular example direct model face difficulties learn mapping number data points large training cases available learn step ahead model input dimension 
concluding remarks chapter highlighted gp model effectively model forecast real dynamic systems 
particular gas liquid process seen parameters covariance function identify structure model 
simply recall ard tool caution 
parameters associated delayed variables compared parameters different variables vary different ranges 
simply setting zero parameters relevant variables model corresponding simpler structure ignoring variables re trained saw likelihood setting zero parameters corresponding variables accounted 
important intuitive idea model trained rich rapidly varying signal automatically generalise simpler signals true see gas liquid system 
model trained rapidly varying signals mean predictions shooting regions test signal varies slowly 
fortunately reflected larger predictive variances indicating model confident estimates 
comparison predictions obtained propagation uncertainty indi cate general propagating uncertainty improve multiple step ahead predictions respect predictive variances 
model passes new region ing larger predictive variance corresponding outputs model propagating uncertainty able catch state rectify predictions 
chapter 
real life applications shown linear model elegantly added accounted step ahead iterative prediction propagation uncertainty 
likewise going show derivative observations easily accounted model extend propagation uncertainty algorithm case 
chapter including derivative observations allowing cautious control chapter methodology gaussian processes approach developed account uncertainty important applications 
section show derivative observations incorporated gp model function observations available particular importance engineering applications identification nonlinear dynamic systems experimental data 
mixed training set derive expressions predictive mean variance function output corresponding noisy input gaussian approximation 
second part chapter devoted gaussian processes control context 
particular interest model predictive control framework adaptive controllers controllers cautious feature arising objective function disregard model uncertainty 
incorporating derivative observations gp model accounting derivative observations directly measured obtained gp framework addressed 
explicitly write expressions predictive mean variance function output accounting mixed nature training set 
results derive mean variance chapter 
including derivative observations allowing cautious control output corresponding noisy test input 
derivative process consider gaussian process dimensional argument mean function covariance function 
shown derivative process denote gaussian process mean covariance function respectively cross covariance function processes general case argument dimensional consider derivative time denoting derivative direction 
case covariance function say cross covariance refer appendix operations random functions 
output predictive distribution new input consider case observations function output observations derivative direction allow number points differ derivative observations need taken inputs function observations 

incorporating derivative observations gp model derivative 
note simplification considering noise free function derivative observations 
refer data set mixed training set 
complete vector observations partition data covariance matrix accordingly blocks corresponding function observations derivative observations cross covariances 
denotes matrix covariances function observations matrix covariances function observations derivatives direction matrix covariances derivative observations direction tion direction general matrix covariances derivative observations direc 
covariances readily computed covariance function formulae 
full vector observations covariance matrix learning achieved section chapter minimising matrix inputs function derivative observations parameters covariance function see 
note model need trained full data set training model achieved function observations 
providing derivative observations affect model terms carrying information understand derivative dimension 
chapter 
including derivative observations allowing cautious control change value parameters learnt added model possibly place function observations making predictions enabling speed computations 
new input usual gp case chapter section predictive distribution corresponding function point gaussian mean variance respectively 
computing covariances function output new observations training points distinguish function derivative observations write mean similarly 
incorporating derivative observations gp model variance 
mean variance relative function observations moments relative derivative observations part accounting cross covariance function derivative observations 
pre mean variance written reflecting mixed nature data set 
prediction noisy input turn problem making prediction noisy random 
gaussian approximation chapter section 
compute mean variance non gaussian predictive distribution 
seen moments chapter 
including derivative observations allowing cautious control equations respectively 
replacing expression written 
predictive mean corresponding noisy corresponds sum noisy predictive mean function observations available case derived chapter corresponding derivative observa tions dimension 
similar expression variance 
replacing expression 

incorporating derivative observations gp model putting different terms 
letting write due mixed nature training set new predictive variance sum new variances accounts cross terms 
case gaussian kernel chapter section compute noisy predictive mean variance exactly 
find see appendix section detailed calculations chapter 
including derivative observations allowing cautious control component entry element 
wish model modelling nonlinear dynamic system perform iterative step ahead prediction propagation uncertainty simply apply algorithm chapter section 
case input covariance matrix slightly changed need account derivative observations computing cross covariance terms 
new expressions terms appendix subsection 
refer numerical results 
gps control turn gaussian processes control nonlinear dynamic systems 
control community nonlinear control nonlinear parametric models neural networks gps appeared control scene murray smith 
nonlinear model predictive control methodology refers class control algorithms nonlinear dynamic model plant predict optimise behaviour process henson qin badgwell 
formulated follows henson sequence control moves computed minimise objective function referred loss cost function includes predicted values controlled outputs obtained nonlinear model 
control input computed time step 
gps control implemented prediction horizon moved step forward procedure repeated new process measurements 
ability controllers deal constraints multi variable systems popular industry 
approach relies heavily quality nonlinear model process 
common available data lie equilibrium points sparse data available transient regions identification performance parametric model 
respect gaussian processes appear better suited task data directly prediction 
way uncertainty model predictions dependent local data density model complexity directly relates amount available data 
gps successfully tested ko constraints variance propagation uncertainty algorithm proposed chapter compute outputs 
deriving control moves researchers considered cost functions model predictions true system outputs astr 
adaptive controllers certainty equivalence principle account uncertainty model predictions control actions taken influence uncertainty 
con achieving caution accounting model uncertainty probing exploration going unexplored regions scope interest control community wittenmark 
assuming quadratic cost function kind dual controller aims finding control input minimises stage criterion signal controlled output 
numerical cost associated optimisation usually led sub optimal ad hoc solutions control behaviour signal depending model accuracy fabri 
controller cautious feature multi step ahead optimisation control moves propagation uncertainty 
chapter 
including derivative observations allowing cautious control step ahead cautious control consider nonlinear multi input single output miso system controlled output current manipulated input assumed white noise unknown variance 
denote state composed delayed outputs term inputs 
wish control system cost function includes penalty term control effort expanding cost time written cost function accounts naturally uncertainty associated arises doing simple manipulations noted murray smith 
researchers ignored variance term related model place difficult obtain uncertainty compute derivatives cost function respect 
optimal control input solving gp zero mean covariance function simply predictive mean associated variance 
gps control data covariance matrix computed pairs training 
case straightforward refer murray smith murray smith various examples illustrating controller disregard variance information leads robust control action adaptation 
exploration multi step ahead control total cost controlling system say time control horizon simplicity notation denote cost time want propagate uncertainty making predictions ahead time assuming covariance function process gaussian chapter 
denote predictive mean variance short corresponding chapter 
including derivative observations allowing cautious control 

minimum achieved solving multi step ahead control system consists finding control signal total cost minimum horizon 
wish find local approximation consists computing optimal control input time step find minimum 
resulting control signal composed local optimal contributions far optimal 
murray smith minimisation total cost requires computation sensitivity equations tell change control signal time affects total cost 
need compute cost time independent depends 
sensitivity equations need solve 

gps control case gaussian covariance function write denotes training states control inputs 
differentiation respect done term wise straightforward manner 
hand need account interactions delayed outputs 
derivatives respect obtained chain rule derivative find optimal control sequence 
refer murray smith preliminary results 
appendix section simply provide ex predictive mean variance corresponding miso system affine 
note equations directly case mixed training set formed derivative function observations provided partial derivatives computed numerically leave reader compute analytical derivatives 
probabilistic view 
far cost controlling system expressed expected squared difference ignoring control term 
natural thing compute variance chapter 
including derivative observations allowing cautious control viewing random quantity 
define expectation variance 
shown simply 
expanding square difference variable variance corresponding products variance need express 
consider standard normal 
chi squared distributed mean variance leading obtained variance side 
consider performing minimisation constrain associated variance blend knowledge meta cost function viewing latent variable 
hierarchical view enable rational decisions selecting final control policies 
similar idea suggested reinforcement learning control context gaussian process represents 
summary value function rasmussen expectation 
explored potential interest approach investigation needed appreciate validity usefulness approach 
summary chapter extensions propagation uncertainty framework 
shown predictive distribution function output corresponding new noisy input derived training set formed function derivative observations system 
results allow perform iterative multiple step ahead prediction sys tem derivative observations available case engineering applications measured data equilibrium points 
methodology account uncertainty control context 
cost function disregard output uncertainty cautious controller shown perform multi step ahead control system propagating uncertainty ahead time 
chapter deals methodological aspect refer preliminary numerical results concerning derivative observations murray smith cautious multi step ahead control miso systems 
chapter 
including derivative observations allowing cautious control chapter thesis explored number extensions gaussian process model related informative model uncertainty order improve modelling forecasting control nonlinear dynamic systems 
suggested approach account noise inputs making predictions 
derived gorithm propagate uncertainty ahead time iterative multiple step ahead prediction dynamic systems 
contributed development tools hopefully gp attractive model eyes engineering community model effective derivative observations available predictive uncertainty cost function enabling cautious control systems 
seen gaussian process set infinitely random variables practice reduced size sets interested resulting process modelled joint multi dimensional gaussian distribution 
taken bayesian ap proach priors put parameters covariance function process combined data compute posterior distributions probabilistic nature model lead distribution solution prediction task 
result neat properties gaussian assumption predictive distribution gaussian fully specified mean variance respectively estimate associated uncertainty model output 
chapter 
model addressed problem making prediction normally dis tributed random input 
solve analytical approximation compute mean variance corresponding predictive distribution gaussian approximation 
parametric form covariance function derive exact moments linear squared exponential gaussian covariance functions approximate ones relying second order taylor approximation 
similar gaussian approximation show challenging problem learning noisy inputs tackled resulting covariance function length scales weighted input uncertainty case gaussian covariance function 
emphasis application results step ahead prediction nonlinear dynamic systems predicting random input interest static case 
system senses inputs imperfectly due faulty sensors external disturbance account extra uncertainty prior knowledge input noise variance assumed case 
dynamic case show uncertainty induced successive prediction prop ahead time making iterative multiple step ahead predictions nonlinear dynamic system represented step ahead nonlinear auto regressive model 
derived algorithm mally accounts predictive variance delayed output cross covariances providing model full knowledge characteristics random state mean covariance matrix progresses ahead time 
time step predictive mean variance cross covariance elements computed exactly case gaussian covariance function taylor approximation 
experimental results simulated mackey glass chaotic time series suggest gaussian approximation compared numerical approximation true distribution simple monte carlo grounded 
aware done monte carlo approximation evaluation measures predictive performance squared error loss negative log predictive density ideal non gaussian distribu tions 
experiments validate results obtained taylor approximation gaussian approximation compare exact ones gaussian kernel 
know integrals required computation predictive mean variance solvable exactly case linear gaussian polynomial mixture covariance functions 
defined family valid covariance functions taylor approximation needed 
simply recall approximation forget check continuity properties function positive definiteness resulting approximation requirements inherent taylor approximation 
main focus gaussian covariance function predominantly modelling gaussian processes shared kernel models 
general highlight founded nature propagation uncertainty approach comparing naive method feeds back estimates viewed model true outputs 
naive approach misleading error bars pre stay small matter far ahead predict time estimates 
hand propagating uncertainty leads reliable predictions suggesting full matrix covariances accounting uncertainties delayed outputs real im pact predictions 
note approximation simply consider diagonal input covariance matrix delayed predictive variances diagonal see computational cost computing cross covariance terms low 
investigation approach generalise real systems results prove en 
show gp effectively model gas pressure gas liquid separator process 
automatic relevance determination tool neal mackay occam razor principle rasmussen ghahramani identify model best complexity performance trade 
signals identification significantly different validation model rapid slowly varying signals model successfully captures dynamics system seen performance predic tion test signal length 
process experiments consider incorporation crude model noise white coloured noise model chapter 
suggested murray smith girard full nonlinear model goldberg 
gaussian covariance function including full covariance matrix allowing correlations input dimensions done williams improve modelling interactions variables 
identification model ph chemical process showed improved lin ear model fitted original data gp model residuals 
showed elegantly incorporate linear model framework perform multiple step ahead predictions accounting linear part interaction gp model time step 
comparing propagation uncertainty naive approach tice leads unreliable predictive distributions peaked mean close respect metric true response variance longer indicator reliability model 
mackey glass system observe predictive variances exact moments gaussian approximation sim ply increase predictive horizon 
desired behaviour happens depend properties system modelled 
system comparison iterative approach direct method model trained predict steps ahead leads think iterative scheme valuable 
particular example direct model better iterative predictive horizon increases favouring direct model arguable model valid horizon prove difficult infeasible practice training model time consuming predictive horizon known advance 
known modelling nonlinear dynamic systems difficult rea sons ranging properties underlying system computational resources 
frequent problem encountered practice uneven spread data operating regime system 
available data confined equilibrium points system data measured transient regions 
common approach build local linear models blend obtain nonlinear model covering operating range murray smith murray smith johansen 
approach proves difficult comes choosing number local models type learning global local respect gps improve modelling systems flexibility way inference performed conditioning current state local data 
far major burden computational cost associated inversion data ance matrix 
techniques available reduce cost williams seeger seeger murray smith pearlmutter shi methodology accounting derivative observations original possibly appealing enables potentially reduce computational cost accord engi neering practice summarising measured data vicinity equilibrium points derivative observation 
conjunction results application gps control context promising results experiments hope model widely engineering community 
believe results general applicable kernel models relevance vector machines candela candela girard 
ultimately want invent create mathematical tools concepts explain world mimic way understand brain processes information decisions 
respect inclined bayesian approaches believe reflect way thinking 
paraphrasing jaynes confronted new situation brain forming plausible prior information evidence situation reasoning order compute updated state information called posterior re new degree belief 
considered gaussian process model bayesian setup simple form thesis proved useful modelling nonlinear systems enabled easy elegant derivation tools extensions 
preference analytical approximations numerical ones motivated fact computationally demanding numerical methods case numerical approximation straightforward 
importantly analytical gaussian approximation easier interpret numerical solution 
seen monte carlo approximation leads non gaussian distributions difficult summarise aspect chapter 
slow uptake engineering contexts 
hope results thesis encourage motivate experiments research believe potential gp model fully explored 
include incorporation time model allowing parameters ance function time dependent covariance function function time extension multiple outputs enabling modelling systems time varying dynamics correlated measured outputs 
appendix mathematical formulae recall useful formulae results thesis 
law iterated expectations conditional variances random variable rv 


shown mean variance rv expectation variance function rvs taylor series approximation taylor formula derivatives order point polynomial denotes derivative evaluated called order taylor approximation 
appendix mathematical formulae taylor formula called remainder term 
exist continuous open interval 
point expression called lagrange form remainder 
derivative satisfies interval interval reverse inequalities hold 
product taylor series 
taylor series product taylor series second order truncated series proof results maths book recommend calculus theory lecture mit open course mit edu mathematics index htm 
www math pitt edu convergence node html operations random functions results taken 
random process argument mean function covariance function 
similarly process mean covariance functions 
addition random processes 
shown mean function correlated covariance function cross covariance functions 
general arbitrary number random functions mean covariance functions say expected value equal sum expected values random functions covariance function sum covariance functions different plus sum cross covariance functions terms sum 
function appendix mathematical formulae differentiation random function derivative mean derivative covariance second mixed partial derivative 
general mean covariance function derivative order random function cross covariance functions derivatives arbitrary order matrix identities matrices basic formulae provided inverses exist denotes determinant 
matrix inversion lemma square invertible matrices necessarily dimension 
gaussian identities dimensional gaussian density marginal conditional distributions joint normal distribution matrix cross covariances 
marginal distributions conditional distributions product gaussians constant usually expressed split vectors size size write matrix identities verify simplifies 
appendix mathematical formulae expectation quadratic form gaussian 
gaussian covariance function derivatives input dimension 
vector component written second derivatives entry second derivative respect 
entry 
simply 
derivative derivative appendix note taylor approximation chapter shown predicting noisy input compute mean variance noisy non gaussian predictive distribution gaussian approximation exactly case gaussian linear covariance function approximately taylor approximation covariance function 
going look closely taylor mean approximation associated error 
sake simplicity discussion done case dimensional inputs 
new test input 
seen chapter computation variance corresponding output requires evaluation 
time consider provided integral analytically tractable resort second order taylor approximation 
interval write see appendix second order polynomial mean appendix note taylor approximation remainder written lagrange form 

say taylor approximation valid interval chosen region lies time provided behaved write 
error induced approximation depends 
lower bound error emphasising dependence 
similar bounds derived 
investigate point refer evans swartz derivation exact error bounds approximating taylor polynomial 
note assumes function approximation holds 
defined corresponding family valid covariance functions speculate covariance functions compact support vanishing distance inputs larger certain cut distance candidates 
appendix appendix chapter appendix provides detailed calculations results chapter 
section deals mixed gp formed function derivative observations section control miso system 
mixed training set prediction noisy input case training data formed derivative function observations section derive expressions mean variance referred noisy mean noisy variance predicting noisy input assuming covariance function gaussian 
subsection provides details computation cross covariance terms input covariance matrix mixed set iterative multiple step ahead prediction framework propagation uncertainty 
recall noisy predictive mean variance want compute chapter section appendix appendix chapter case gaussian covariance function expressions derived chapter section 
gaussian kernel 
case gaussian covariance function covariance function derivative process direction see appendix cross covariance derivatives direction chapter section denote 
turn computation 

mixed training set prediction noisy input noisy mean need compute case gaussian covariance function see section omitting limits summation need compute product gaussians component 
replacing expressions chapter section compute component 
replacing expressions gives appendix appendix chapter computing noisy variance compute variance need 

gaussian covariance function write 
mixed training set prediction noisy input albeit 
product gaussians write chapter section 
leads similarly component entry matrix 
write computing element need compute noisy variance appendix appendix chapter respectively previous results directly write compute element 


replacing different terms expressions cross covariance terms iterative multi step ahead forecasting suppose wish apply multiple step ahead iterative forecasting algorithm chapter section 
model formed mixed observations changed apart 
mixed training set prediction noisy input input covariance matrix 
compute correspond noisy mean variance 
time step mean random state composed delayed previously predicted means covariance matrix corresponding variances diagonal 
section cross covariance terms 
need compute denoting notational convenience 
column matrix element 
appendix appendix chapter write cross covariance terms column element 
gp modelling affine miso system consider affine nonlinear system state vector time step ahead system output current control signal white noise variance smooth nonlinear functions 
murray smith model system gaussian process zero mean covariance function reflecting functional part control part affine function just simplicity gaussian structure allowing different parameters covariance functions 

gp modelling affine miso system predictive distribution assuming observed predictive distribution readily obtained 
write predictive mean variance way highlight control input state parts equation 
simply predicting chapter compute predictive mean variance time accounting uncertainty previous points mixed functional control form covariance function 
appendix appendix chapter input corresponding mean variance computed 
need evaluate chapter directly write leading variance replacing requires expression 
computation similar done 
arrive 
gp modelling affine miso system 

expanding square replacing expressions gives write replacing expression simplification equations elements mean correspond delayed predictive means 
diagonal elements corresponding predictive variances cross covariance terms 
appendix appendix chapter bibliography astr 

adaptive control 
proc 
decision control pages 
astr wittenmark 

computer controlled systems 
theory design 
prentice hall 


review gaussian random fields correlation functions 
technical report norwegian computing center 
ahmad tresp 

solutions missing feature problem vision 
hanson cowan giles editors advances neural information processing systems volume pages 
morgan kaufmann san mateo ca 
bakker van den giles 

neural learning chaotic dynamics error propagation algorithm 
proc 
int 
joint conf 
neural networks pages 
barber bishop 

ensemble learning multi layer networks 
jordan kearns solla editors advances neural information processing systems volume pages 
mit press 
bishop 

neural networks pattern recognition 
oxford 
box jenkins 

time series analysis 
forecasting control 
prentice hall 
carlin louis 

bayes empirical bayes methods data analysis 
chap man hall 
bibliography casdagli 

nonlinear prediction chaotic times series 
physica 
chen yang hafner 

nonparametric multi step ahead prediction time series analysis 
journal royal statistical society appear 
cressie 

statistics spatial data 
wiley 
stephens 

bayesian analysis errors variables regression models 
biometrics 
evans swartz 

algorithm approximation integrals exact error bounds 
technical report department statistics university toronto 
fabri 

dual adaptive control nonlinear stochastic systems neural networks 
automatica 
farmer 

exploiting chaos predict reduce noise 
technical report la ur los alamos national laboratory new mexico 
freedman carroll 

new method dealing measurement error explanatory variables regression models 
biometrics 
appear 


classes kernels machine learning statistics perspective 
journal machine learning research 
gers eck schmidhuber 

applying lstm time series predictable time window approaches 
lecture notes computer science 
gershenfeld 

nature mathematical modeling 
cambridge university press 
ghahramani jordan 

learning incomplete data 
technical report mit center biological computational learning 
ghahramani jordan 

supervised learning incomplete data em approach 
cowan tesauro alspector editors advances neural information processing systems volume 
san mateo ca morgan kaufmann 
bibliography ghahramani roweis 

learning nonlinear dynamical systems em algo rithm 
kearns solla cohn editors advances neural information processing systems volume pages 
mit press 
gibbs 

bayesian gaussian processes regression classification 
phd thesis university cambridge 
girard murray smith 

learning gaussian process prior model uncertain inputs 
technical report tr computing science department university glasgow 
girard rasmussen murray smith 

gaussian process priors uncertain inputs multiple step ahead prediction 
technical report tr computing science department university glasgow 
girard rasmussen candela murray smith 

gaussian process priors uncertain inputs application multiple step ahead time series forecasting 
becker thrun obermayer editors advances neural information processing systems volume pages 
mit press 


compactly supported correlation functions 
journal multivariate analysis 
goldberg williams bishop 

regression input dependent noise 
jordan solla editors advances neural information processing systems volume pages 
lawrence erlbaum 
goutte 

statistical learning regularisation regression 
application system iden time series modelling 
phd thesis universite de paris 
hastie tibshirani friedman 

elements statistical learning 
data mining inference prediction 
springer 
henson 

nonlinear model predictive control current status directions 
com chemical engineering 
bibliography henson 

adaptive nonlinear control ph process 
ieee trans 
control system technology volume pages 
jordan 

improving mean field approximation mixture distributions 
learning graphical models pages 
mit press 
judd small 

long term prediction 
physica 
julier uhlmann 

new extension kalman filter nonlinear sys tems 
proceedings aerosense orlando florida 
th international symposium aerospace defense sensing simulation controls 
kendall stuart 

advanced theory statistics volume 
griffin kimeldorf wahba 

correspondence bayesian estimation stochastic processes smoothing splines 
annals mathematical statistics 
banko girard murray smith rasmussen 

case comparison identification neural network gaussian process models 
international conference intelligent control systems signal processing pages 
girard banko murray smith 

dynamic systems identification gaussian processes 
th vienna th imacs symposium mathematical modelling volume pages 
girard banko murray smith 

dynamic systems identification gaussian processes 
journal mathematical computer modelling dynamical sys tems 
appear 
girard 

identification ph process neural networks gaussian process models 
technical report dp jozef stefan institute ljubljana 
girard 

incorporating linear local models gaussian process model 
technical report dp jozef stefan institute ljubljana 
bibliography murray smith rasmussen girard 

gaussian process model predictive control 
american control conference boston 


statistical approach basic valuations problems 
chem 

min 
soc 
kuo principe 

reconstructed dynamics chaotic signal modeling 
ieee international conference neural networks volume pages 
lauritzen 

aspects thiele contributions statistics 
bulletin inter national statistical institute volume 
murray smith 

nonlinear structure identification gaussian process prior velocity approach 
control cambridge 


mixtures gaussian process priors 
technical report ms tp institut theoretische physik universit 
lindley 

probability statistics bayesian viewpoint 
cam bridge university press 
ljung 

system identification theory user 
prentice hall 
mackay 

bayesian interpolation 
neural computation 
mackay 

bayesian methods backpropagation networks 
domany van hemmen schulten editors models neural networks iii chapter pages 
springer verlag new york 
mackay 

probable networks plausible predictions review practical bayesian methods supervised neural networks 
network computation neural systems 
mackay 

gaussian processes replacement supervised neural networks 
tech nical report cavendish laboratory cambridge university 
bibliography mackay 

monte carlo methods 
jordan editor learning graphical models pages 
mackay 

information theory inference learning algorithms 
cambridge univer sity press 
mackey glass 

oscillation chaos physiological control systems 
science 


principles 
economic geology 
minka 

lower bounds approximate integrals 
available www stat cmu edu minka papers rem html 
mukherjee osuna girosi 

nonlinear prediction chaotic time series support vector machines 
amelia island florida 
ieee 
ller smola tsch sch lkopf kohlmorgen vapnik 

predicting time series support vector machines 
proceedings icann lausanne 
murray smith 

modelling human control behaviour markov chain switched bank control laws 
th ifac symposium man machine systems kyoto pages 
murray smith girard 

gaussian process priors arma noise models 
irish signals systems conference pages 
murray smith johansen 

multiple model approaches modelling control 
taylor francis 
murray smith johansen shorten 

transient dynamics equilibrium behaviour identification blended multiple model structures 
european control confer ence karlsruhe pages ba 
murray smith pearlmutter 

transformations gaussian process priors 
technical report tr university glasgow scotland uk 
bibliography murray smith 

nonlinear adaptive control non parametric gaus sian process prior models 
th ifac world congress 
international federation automatic control 
murray smith rasmussen girard 

adaptive cautious pre control gaussian process priors 
ifac international symposium system iden rotterdam 
neal 

probabilistic inference markov chain monte carlo methods 
technical report crg tr department computer science university toronto 
neal 

priors infinite networks 
technical report crg tr department computer science university toronto 
neal 

bayesian learning neural networks 
phd thesis university toronto 
neal 

monte carlo implementation gaussian process models bayesian sion classification 
technical report department statistics university toronto 
newman leonard 

matrix orientated note joint marginal conditional multivariate gaussian distributions 
technical report mit 
www robots ox ac uk index html 
hagan 

curve fitting optimal design regression discussion 
journal royal statistical society 
hagan 

bayesian inference 
kendall advanced theory statistics 
edward arnold 
schervish 

nonstationary covariance functions gaussian process regression 
thrun saul sch lkopf editors advances neural information processing systems volume 
mit press 
appear 
papoulis 

probability random variables stochastic processes 
mcgraw hill 
patterson thompson 

recovery inter block information block sizes unequal 
biometrika 
bibliography phan lim longman 

unifying input ouput state space tives predictive control 
technical report department mechanical aerospace engineering 
poggio girosi 

networks approximation learning 
proceedings ieee volume pages 
principe kuo 

dynamic modeling chaotic time series neural networks 
tesauro touretzky leen editors advances neural information processing systems volume pages 
mit press 
principe kuo 

nonlinear dynamics brain chapter prediction chaotic time series neural networks pages 
world scientific 
principe kuo 

prediction chaotic time series neural networks issue dynamic modeling 
int 
bifurcation chaos 


theory random functions application control problems 
pergamon press 
williams bishop 

upper bound bayesian error bars generalized linear regression 
technical report ncrg neural computing research group aston university 
qin badgwell 

nonlinear model predictive control chapter overview nonlinear model predictive control applications pages 
birkhauser verlag 
candela girard 

prediction uncertain input gaussian processes relevance vector machines application multiple step ahead time series forecasting 
technical report informatics mathematical modelling technical denmark 
candela girard larsen rasmussen 

propagation uncer tainty bayesian kernels models application multiple step ahead forecasting 
ieee international conference acoustics speech signal processing volume pages 
bibliography buc 

dynamical modeling kernels nonlinear time series prediction 
thrun saul sch lkopf editors advances neural information processing systems volume 
mit press 
appear 
rasmussen 

evaluation gaussian processes methods non linear re 
phd thesis university toronto 
rasmussen 

practical monte carlo implementation bayesian learning 
mozer hasselmo editors advances neural information pro cessing systems volume pages 
mit press 
rasmussen 

infinite gaussian mixture model 
solla mller editors advances neural information processing systems volume pages 
mit press 
rasmussen 

gaussian processes speed hybrid monte carlo expensive bayesian integrals 
bernardo berger dawid heckerman smith west editors bayesian statistics 
oxford university press 
rasmussen ghahramani 

occam razor 
leen dietterich tresp editors advances neural information processing systems volume pages 
mit press 
rasmussen ghahramani 

infinite mixtures gaussian process experts 
dietterich becker editor advances neural information processing systems vol ume pages 
mit press 
rasmussen 

gaussian processes reinforcement learning 
thrun saul sch lkopf editors advances neural information processing systems volume 
mit press 
appear 
roweis ghahramani 

unifying review gaussian models 
neural com putation 
bibliography murray smith 

self tuning control non linear systems gaus sian process prior models 
technical report tr department computing science university glasgow 
seeger 

bayesian gaussian process models pac bayesian generalisation error bounds sparse approximations 
phd thesis university edinburgh 
seeger 

bayesian gaussian processes machine learning 
international journal neural systems 
appear 
seeger williams lawrence 

fast forward selection speed sparse gaussian process regression 
bishop frey editors proceedings ninth international workshop ai statistics 
appear 
shi murray smith titterington 

bayesian regression classification mixtures gaussian processes 
technical report tr department comput ing science university glasgow 
small judd 

variable prediction steps long term prediction 
technical report department mathematics statistics university western australia 
smith 

uncertainty error predictability nonlinear systems 
editor nonlinear dynamics statistics pages 
birkhauser 
smith 

uncertainty dynamics predictability chaotic systems 
quart 

soc 
mohammad 

learning presence input noise stochastic em algorithm 
bayesian inference maximum entropy methods maxent workshops pages 
stoica 

system identification 
prentice hall englewood cliffs nj 
murray smith rasmussen 

deriva tive observations gaussian process models dynamic systems 
becker bibliography obermayer editors advances neural information processing systems volume pages 
mit press 
takens 

detecting strange attractors turbulence 
rand young editors dynamical systems turbulence volume pages 
springer verlag 


inverse problems quest information 
journal 
tipping 

sparse bayesian learning relevance vector machine 
journal machine learning research 
titterington smith makov 

statistical analysis finite mixture distributions 
john wiley sons 
tresp 

mixture gaussian processes 
leen editor advances neural information processing systems volume pages 
mit press 
tresp ahmad 

training neural networks deficient data 
cowan tesauro alspector editors advances neural information processing systems volume pages 
morgan kaufmann publishers tresp hofmann 

missing noisy data nonlinear time series prediction 
girosi wilson editors neural networks signal processing volume ieee signal processing society new york pages 
tresp hofmann 

nonlinear time series prediction missing noisy data 
neural computation 


central moments multidimensional gaussian distribution 
mathematical scientist 
williams 

discovering hidden features gaussian processes regression 
kearns solla editor advances neural information processing systems volume pages 
mit press 
bibliography von mises 

mathematical theory probability statistics chapter viii pages 
academic press 
wahba 

multivariate function operator estimation smoothing splines reproducing kernels 
casdagli eubank editors nonlinear modeling forecast ing sfi studies sciences complexity volume xii 
addison wesley 
welch bishop 

kalman filter 
technical report tr university north carolina 
west harrison 

bayesian forecasting dynamic models 
springer 
williams 

computation infinite neural networks 
technical report ncrg department computer science applied mathematics aston university 
williams 

computing infinite networks 
jordan editor advances neural information processing systems volume pages 
mit press 
williams 

prediction gaussian processes linear regression linear prediction 
technical report ncrg dept computer science applied mathematics 
aston university 
williams 

gaussian processes 
appear handbook brain theory neural networks second edition mit press 
williams bishop zhu 

relationship bayesian error bars input data density 
proceedings fourth iee international confer ence artificial neural networks pages 
williams rasmussen 

gaussian processes regression 
touretzky mozer hasselmo editors advances neural information processing systems volume pages 
mit press 
bibliography williams seeger 

nystr method speed kernel machines 
leen editor advances neural information processing systems volume pages 
mit press 
wittenmark 

adaptive dual control methods overview 
preprints th ifac symposium adaptive systems control signal processing hungary pages 
wright 

bayesian approach neural network modelling input uncertainty 
ieee transactions neural networks volume pages 
