obtaining calibrated probabilities boosting alexandru niculescu department computer science cornell university ithaca ny cs cornell edu boosted decision trees typically yield accuracy precision roc area 
outputs boosting calibrated posterior probabilities boosting yields poor squared error cross entropy 
empirically demonstrate adaboost predicts distorted probabilities examine calibration methods correcting distortion platt scaling isotonic regression logistic correction 
experiment boosting log loss usual exponential loss 
experiments show logistic correction boosting log loss boosting weak models decision stumps yield poor performance boosting complex models full decision trees 
platt scaling isotonic regression significantly improve probabilities predicted boosted stumps boosted trees 
calibration boosted full decision trees predict better probabilities learning methods svms neural nets bagged decision trees methods calibrated 
evaluation learning algorithms caruana niculescu boosted decision trees excellent performance metrics accuracy lift area roc curve average precision precision recall break point 
boosted decision trees poor squared error cross entropy adaboost produce probability estimates 
friedman hastie tibshirani provide explanation boosting poorly calibrated predictions 
show boosting viewed additive logistic regression model 
consequence rich caruana department computer science cornell university ithaca ny caruana cs cornell edu predictions boosting trying fit logit true probabilities opposed true probabilities 
get back probabilities logit transformation inverted 
treatment boosting large margin classifier schapire observed order obtain large margin cases close decision surface adaboost sacrifice margin easier cases 
results shifting predicted values away hurting calibration 
shifting consistent breiman interpretation boosting equalizer see breiman discussion friedman 
section demonstrate probability shifting real data 
correct boosting poor calibration experiment boosting log loss methods calibrating predictions boosted models convert calibrated posterior probabilities 
post training calibration methods logistic correction method friedman analysis boosting additive model platt scaling method platt transform svm outputs posterior probabilities isotonic regression method zadrozny elkan calibrate predictions boosted naive bayes svm decision tree models logistic correction platt scaling convert predictions probabilities transforming sigmoid 
logistic correction sigmoid parameters derived friedman analysis 
platt scaling parameters fitted data gradient descent 
isotonic regression general purpose non parametric calibration method assumes probabilities monotonic transformation just sigmoid predictions 
alternative training boosted models adaboost correcting outputs post training calibration variant boosting directly optimizes cross entropy log loss 
collins schapire singer show boosting algorithm opti log loss obtained simple modification adaboost algorithm 
collins briefly evaluate new algorithm synthetic data set acknowledge thorough evaluation real data sets necessary 
lebanon lafferty show logistic correction applied boosting exponential loss behave similarly boosting log loss demonstrate examining performance boosted stumps variety data sets 
results confirm findings boosted stumps show effect boosted trees 
experiments show boosting full decision trees usually yields better models boosting weaker stumps 
unfortunately results show boosting directly optimize log loss applying logistic correction models boosted exponential loss effective boosting weak models stumps 
methods effective boosting full decision trees 
significantly better performance obtained boosting full decision trees exponential loss calibrating predictions platt scaling isotonic regression 
calibration platt scaling isotonic regression effective calibration boosted decision trees predict better probabilities learning method compared including neural nets bagged trees random forests calibrated svms 
section analyze predictions boosted trees qualitative point view 
show boosting distorts probabilities consistent way generating sigmoid shaped reliability diagrams 
analysis motivates sigmoid map predictions probabilities 
section describes calibration methods 
section presents empirical comparison calibration methods log loss version boosting 
section compares performance boosted trees stumps learning methods 
boosting calibration section empirically examine relationship boosting predictions posterior probabilities 
way visualize relationship true posterior probabilities known reliability diagrams degroot fienberg 
construct reliability diagram prediction space discretized bins 
cases predicted value put bin second bin bin mean predicted value plotted true fraction positive cases bin 
model calibrated points fall near diagonal line 
bottom row shows reliability plots large test set stages boosting bayesian smoothed decision trees buntine 
top shows histograms predicted values models 
histograms show number steps boosting increases predicted values pushed away tend collect side decision surface 
shift away hurts calibration yields sigmoid shaped reliability plots 
shows histograms reliability diagrams boosted decision trees steps boosting test problems 
see section detail problems 
figures results measured large independent test sets training 
data sets predicted values boosting approach 
exception letter highly skewed data set positive class 
problem predicted values approach careful examination histogram shows sharp drop number cases predicted probability near 
reliability plots display sigmoid shaped reliability diagrams motivating sigmoid map predictions calibrated probabilities 
functions fitted platt method isotonic regression shown middle bottom rows 
calibration section describe methods calibrating predictions adaboost logistic correction platt scaling isotonic regression 
logistic correction describing logistic correction useful briefly review adaboost 
start example train set xi yi having equal weight 
step weak learner hi trained weighted train set 
error hi determines model weight weight training example 
equivalent formulations 
formulation friedman hastie tibshirani assumes yi hi 
output boosted model ihi friedman show adaboost builds additive logistic regression model minimizing exp yf 
show exp yf minimized log suggests applying logistic correction order get back conditional probability exp fraction positives step mean predicted value fraction positives steps mean predicted value fraction positives steps mean predicted value fraction positives steps mean predicted value fraction positives steps mean predicted value fraction positives steps mean predicted value effect boosting predicted values 
histograms predicted values top reliability diagrams bottom test set boosted trees different steps boosting cov type problem 
fraction positives fraction positives cov type adult letter letter medis slac hs mean predicted value mean predicted value mean predicted value mean predicted value mean predicted value mean predicted value mean predicted value histograms predicted values reliability diagrams boosted decision trees 
see section logistic correction works boosting simple base learners decision stumps 
base learners powerful training data fully separable correction predictions rosset poor calibration 
platt calibration equivalent formulation adaboost assumes yi hi 
output boosted model ihi model platt scaling isotonic regression treat raw uncalibrated prediction 
platt uses sigmoid map svm outputs posterior probabilities 
sigmoidal shape reliability diagrams suggest calibration method effective mg mean predicted value calibrating boosting predictions 
section closely follow description platt method platt 
output boosting equation 
get calibrated probabilities pass output boosting sigmoid exp af fitted maximum likelihood estimation calibration set fi yi 
gradient descent find solution argmin fraction positives fraction positives pi yi log pi pi exp afi questions arise sigmoid training set fi yi come avoid overfitting 
answer question train set boosting example xi yi boosting train set xi yi training example sigmoid 
unfortunately introduce unwanted bias sigmoid training set lead poor results platt 
alternate solution split data training validation set 
boosting trained training set predictions validation set fit sigmoid 
cross validation allow boosting sigmoid trained full data set 
data split parts 
fold part held aside independent calibration validation set boosting performed parts 
union validation sets fit sigmoid parameters 
platt experiments fold cross validation estimate sigmoid parameters 
second question sample model avoid overfitting calibration set 
positive examples negative examples calibration set example platt calibration uses target values respectively detailed treatment justification target values see platt 
middle row shows sigmoids fitted platt scaling 
isotonic regression alternative platt scaling isotonic regression robertson 
zadrozny elkan successfully isotonic regression calibrate predictions svms naive bayes boosted naive bayes decision trees 
isotonic regression assumes yi fi isotonic monotonically increasing function 
training set fi yi isotonic regression problem finding isotonic function argmin yi fi piecewise constant solution linear time pair adjacent pav algorithm ayer table 
case platt calibration boosting train set xi yi get train set xi yi isotonic regression introduce unwanted bias fully separable case boosting order negative examples positive examples isotonic regression output just function 
unbiased calibration set table pav algorithm algorithm 
pav algorithm estimating posterior probabilities uncalibrated model predictions 
input training set fi yi sorted fi initialize mi yi wi mk mi set wk wk wi set mk wk mk wi mi wk replace mk mi mk output stepwise constant function mi fi fj obtained methods discussed section 
isotonic regression experiments fold cv methodology platt scaling 
bottom row shows functions fitted isotonic regression 
empirical results section apply scaling methods predictions boosted trees boosted stumps binary classification problems 
adult cov type letter problems uci repository blake merz 
cov type converted binary problem treating largest class positives rest negatives 
letter converted boolean ways 
letter treats letter positive remaining letters negative yielding unbalanced problem 
letter uses letters positives negatives yielding balanced problem 
hs data set class soybean positive class 
slac particle physics problem collaborators stanford linear accelerator medis mg medical data sets 
boosted stumps examine case data separable span base learners 
boost different types stumps splitting criteria buntine ind package 
boosting overfit rosset friedman iterations boosting calibration worse see consider boosted stumps steps boosting 
left side table shows cross entropy large final test sets best boosted stump models calibration results show performance boosted stumps dramatically improved calibration optimizing log loss 
average calibration reduces cross entropy squared error see 
post training calibration methods platt iso lo protect infinite cross entropy prevent models predicting exactly table cross entropy performance boosted decision stumps boosted decision trees calibration 
qualitatively similar results obtained squared error 
boosted stumps boosted trees problem raw platt iso logloss raw platt iso logloss cov type adult letter letter medis slac hs mg mean gist table equally 
logistic correction advantage extra time required fit calibration model cross validation needed create independent test sets 
logloss column shows performance boosted stumps trained optimize log loss opposed usual exponential loss 
directly optimizing log loss performance boosted stumps percent worse calibration methods 
consistent results reported lebanon lafferty 
logistic correction need independent calibration set extra time spent training calibration models 
logistic correction directly optimizing log loss effective weak base learners level stumps 
unfortunately base learners simple boosted stumps able capture structure real data sets 
boosted level stumps optimal true model additive features model non additive interactions features friedman 
see table boosting full decision trees yields significantly better performance test problems 
level stumps decision trees complex base models data sets separable span 
means boosted decision trees expressive capture full complexity datasets 
unfortunately means expressive perfectly separate training set pushing probabilities predicted logistic correction 
logistic correction longer effective shows posterior estimates fitting sigmoid isotonic function independent test set 
right side table presents performance best boosted trees calibration 
prevent results depending specific tree style boost different styles trees 
tree types buntine ind package id cart cart mml bayes new tree types predict better probabilities unpruned laplacian smoothing provost domingos unpruned bayesian smoothing mml trees laplacian smoothing 
consider boosted models steps boosting 
expected logistic correction competitive boosting full decision trees 
calibration methods platt scaling isotonic regression provide significant improvement quality predicted probabilities 
methods reduce cross entropy squared error see 
surprisingly boosting directly optimizes log loss performance boosted trees poor 
base level models expressive boosting log loss quickly separates classes train set pushes predictions hurting calibration 
happens despite fact regularize boosting varying number iterations boosting rosset 
comparing results left right side tables see boosted trees significantly outperform boosted stumps problems 
average problems boosted trees yield lower squared error lower cross entropy boosted stumps 
boosted stumps win small margin problems nice property theoretically suggested logistic correction works 
determine performance platt scaling isotonic regression changes amount data available calibration vary size calibration set factors 
shows average squared error test problems best uncalibrated best calibrated boosted trees 
perform trials problem 
nearly horizontal line graph show squared error prior calibration 
line perfectly horizon tried boosting level decision trees 
boosted level trees outperformed boosted level stumps perform boosting full trees 
level trees complex base level models logistic correction longer effective platt scaling isotonic regression 
fraction positives fraction positives cov type adult letter letter medis slac hs mean predicted value mean predicted value mean predicted value mean predicted value mean predicted value mean predicted value mean predicted value mg mean predicted value histograms predicted values reliability diagrams boosted trees calibrated platt method 
cov type adult letter letter medis slac hs mean predicted value mean predicted value mean predicted value mean predicted value mean predicted value mean predicted value mean predicted value mg mean predicted value histograms predicted values reliability diagrams boosted trees calibrated isotonic regression 
tal test sets change data moved calibration sets 
plot shows squared error calibration platt method isotonic regression size calibration set varies small large 
learning curves show performance boosted trees improved calibration small calibration set sizes 
calibration set small cases platt scaling outperforms isotonic regression 
happens isotonic regression constrained platt scaling easier overfit calibration set small 
platt method overfitting control built see section 
squared error unscaled platt isotonic calibration set size learning curves platt scaling isotonic regression boosted trees average problems illustrate calibration transforms predictions show histograms reliability diagrams problems boosted trees steps boosting platt scaling isotonic regression 
figures show calibration undoes shift probability mass caused boosting calibration cases predicted probabilities near 
reliability plots closer diagonal shape characteristic boosting predictions gone 
problem transforming predictions platt scaling isotonic regression yields significant improvement quality predicted probabilities leading lower squared error cross entropy 
main difference isotonic regression platt scaling boosting seen comparing histograms figures 
isotonic regression generates piecewise constant function histograms quite coarse histograms generated platt scaling smooth easier interpret 
boosted trees vs methods boosted trees poor initial calibration surprising platt calibration isotonic regression significantly improves squared error 
necessarily mean calibration boosted trees yield probabilistic predictions 
compares performance boosted trees stumps learning methods calibration 
shows performance boosted decision trees bst dt svms random forests fraction positives fraction positives rf bagged decision trees bag dt neural nets ann memory learning methods knn boosted stumps bst stmp vanilla decision trees dt logistic regression naive bayes nb 
parameters learning method carefully optimized insure fair comparison 
obtain performance svms scaled svm predictions min max min 
computational issues models calibrated held validation set performing fold cv 
problem calibration method select best model trained learning algorithm validation set calibration report performance final test set 
bst dt svm rf ann bag dt knn bst stmp uncalibrated isotonic regression platt scaling log loss dt squared error performance learning algorithms large test sets calibration 
qualitatively similar obtained examines cross entropy squared error 
error bars shown confidence intervals small may difficult see 
performance boosting optimize log loss shown boosted trees stumps 
calibration platt scaling isotonic regression boosted decision trees better squared error cross entropy learning methods 
calibration boosting full decision trees yield stateof art probabilistic prediction 
boosting full trees optimize log loss directly yield performance comparable post training calibration platt scaling isotonic regression 
boosting optimize log loss directly nearly effective calibration boosting stumps boosted stumps perform worse boosting full decision trees 
boosted decision trees best methods svms random forests neural nets bagged decision see caruana niculescu description different models parameter settings evaluated 
way scale svm predictions 
platt scaling isotonic regression better 
simple scaling provide baseline performance svms 
nb trees 
platt scaling isotonic regression significantly improve performance svm models yield small improvement random forests little effect performance bagged trees neural nets 
bagged trees neural nets yield calibrated predictions benefit post calibration 
fact post calibration hurts performance neural nets slightly 
interesting note neural networks single sigmoid output unit viewed linear classifier span hidden units sigmoid output predictions 
respect neural nets similar svms boosted decision trees calibrated platt method 
platt scaling may slightly effective isotonic regression bagged trees neural nets memory learning vanilla decision trees logistic regression naive bayes 
probably sigmoid correct function mapping raw predictions generated learning algorithms posterior probabilities 
cases isotonic regression yields somewhat better squared error cross entropy platt scaling learning methods perform calibrated boosted decision trees 
learning methods benefit post training calibration boosted trees svms boosted stumps naive bayes 
calibration important learning methods 
models predict best probabilities prior calibration random forests neural nets bagged trees 
detailed discussion predicting calibrated probabilities supervised learning see niculescu caruana 
empirically demonstrated adaboost yields poorly calibrated predictions 
correct problem experimented variant boosting directly optimizes log loss calibration methods logistic correction justified friedman analysis platt scaling justified sigmoidal shape reliability plots isotonic regression general non parametric calibration method 
surprising result boosting log loss usual exponential loss works base level models weak models level stumps competitive boosting powerful models full decision trees 
similarly logistic correction works boosted stumps gives poor results boosting full decision trees level trees 
calibration methods platt scaling isotonic regression effective mapping boosting predictions calibrated posterior probabilities regardless complex base level models 
calibration methods boosted decision trees better reliability diagrams significantly improved squared error cross entropy 
compared learning algorithms boosted decision trees calibrated platt scaling isotonic regression yield best average squared error cross entropy performance 
combined excellent performance measures accuracy precision area roc curve caruana niculescu may calibrated boosted decision trees model choice applications 
acknowledgments zadrozny charles elkan isotonic regression code 
charles young slac stanford linear accelerator slac data set 
tony goddard space center help indian pines data set 
supported nsf award 
ayer ayer brunk ewing reid silverman 
empirical distribution function sampling incomplete information 
annals mathematical statistics 
blake merz blake merz 
uci repository machine learning databases 
buntine caruana buntine caruana 
ind recursive partitioning 
technical report fia nasa ames research center 
buntine buntine 
learning classification trees 
statistics computing 
caruana niculescu caruana niculescu 
empirical comparison supervised learning algorithms different performance metrics 
technical report tr cornell university 
collins michael collins robert schapire yoram singer 
logistic regression adaboost bregman distances 
machine learning 
degroot fienberg degroot fienberg 
comparison evaluation forecasters 
statistician 
friedman friedman hastie tibshirani 
additive logistic regression statistical view boosting 
annals statistics 
johnson 
support vector machine classifiers applied data 
proc 
eighth jpl airborne geoscience workshop 
lebanon lafferty lebanon lafferty 
boosting maximum likelihood exponential models 
advances neural information processing systems 
niculescu caruana niculescu caruana 
predicting probabilities supervised learning 
proc 
th international conference machine learning icml 
platt platt 
probabilistic outputs support vector machines comparison regularized likelihood methods 
adv 
large margin classifiers 
provost domingos provost domingos 
tree induction probability rankings 
machine learning 
robertson robertson wright 
order restricted statistical inference 
john wiley sons new york 
rosset rosset ji zhu trevor hastie 
boosting regularized path maximum margin classifier 
mach 
learn 
res 
schapire schapire freund bartlett lee 
boosting margin new explanation effectiveness voting methods 
annals statistics 
zadrozny elkan zadrozny elkan 
obtaining calibrated probability estimates decision trees naive bayesian classifiers 
icml 
zadrozny elkan zadrozny elkan 
transforming classifier scores accurate multiclass probability estimates 
kdd 
