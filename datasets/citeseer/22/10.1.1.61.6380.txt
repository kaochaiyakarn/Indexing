coarse sample complexity bounds active learning sanjoy dasgupta uc san diego dasgupta cs ucsd edu characterize sample complexity active learning problems terms parameter takes account distribution input space specific target hypothesis desired accuracy 
goal active learning learn classifier setting data comes unlabeled labels explicitly requested paid 
hope accurate classifier buying just labels 
far encouraging theoretical results field show hypothesis class homogeneous origin linear separators data distributed uniformly unit sphere labels correspond perfectly hypotheses separable case log labels needed learn classifier error :10.1.1.20.8521
exponentially smaller usual sample complexity learning linear classifiers supervised setting 
generalizing result non trivial 
instance hypothesis class expanded include non homogeneous linear separators just dimensions benign input distribution see target hypotheses active learning help labels needed 
fact example label complexity active learning depends heavily specific target hypothesis ranges log 
consider arbitrary hypothesis classes vc dimension learning problems separable 
characterize sample complexity active learning terms parameter takes account distribution input space specific target hypothesis desired accuracy 
specifically notice distribution induces natural topology define splitting index captures relevant local geometry vicinity scale 
show quantity fairly tightly describes sample complexity active learning active learning scheme requires labels generic active learner uses labels just get usual sample complexity supervised notation hides factors polylogarithmic 
learning 
constant instances active learning gives exponential improvement number labels needed 
look various hypothesis classes derive splitting indices target hypotheses different levels accuracy 
homogeneous linear separators uniform input distribution easily find constant direct proof efficacy active learning case 
proofs omitted want space full details examples 
sample complexity bounds motivating examples linear separators example taken :10.1.1.119.2797:10.1.1.78.5278
suppose data lie real line classifiers simple thresholding functions hw hw vc theory tells underlying distribution separable classified perfectly hypothesis order achieve error rate draw random labeled examples return classifier consistent 
suppose draw unlabeled samples lay points line hidden labels sequence followed sequence goal discover point transition occurs 
done binary search asks just log log labels 
case active learning gives exponential improvement number labels needed 
achieve label complexity proportional log 
natural step consider linear separators dimensions 
linear separators hypothesis class linear separators suppose input distribution density supported perimeter unit circle 
turns positive results dimensional case generalize target hypotheses labels needed find classifier error rate matter active learning scheme 
see consider possible target hypotheses left points positive hi points positive small slice bi probability mass 
slices bi explicitly chosen disjoint result labels needed distinguish hypotheses 
instance suppose nature chooses target hypothesis random hi 
identify target probability necessary query points half bi particular target hypotheses active learning offers improvement sample complexity 
target hypotheses instance positive negative regions evenly balanced 
consider active learning scheme left data lie circumference circle 
bi arc probability mass 
right distribution lifted trace amounts distribution mixed 

draw pool unlabeled points 
origin 
pool choose query points random positive negative point 
points queried halt 

apply binary search find boundaries positive negative perimeter circle 
define min positive mass negative mass 
hard see target hypothesis step asks labels probability say step asks log labels 
simple hypothesis class label complexity active learning run log depending specific target hypothesis 
linear separators previous examples amount unlabeled data needed exactly usual sample complexity supervised learning 
turn case helpful significantly unlabeled data 
consider distribution previous example concreteness fix uniform unit circle lift dimensions adding point third coordinate 
consist homogeneous linear separators clearly bad cases previous example persist 
suppose trace amount second distribution mixed right uniform circle 
bad linear separators cut just small portion divide perfectly half 
permits stage algorithm binary search points approximately identify places target hypothesis cuts identify positive negative point look midpoints positive negative intervals binary search points steps just log labels 
log label complexity possible presence achievable amount unlabeled data potentially enormous 
unlabeled data usual label complexity applies 
basic definitions cut splitting edges 
sample complexity supervised learning commonly expressed function error rate underlying distribution active learning previous examples demonstrate important take account target hypothesis amount unlabeled data 
main goal particular formalism accomplished 
instance space underlying distribution hypothesis class set functions vc dimension 
operating non bayesian setting measure prior space absence measure natural notion volume current version space 
distribution induce natural distance function pseudometric 
likewise define notion neighborhood 
dealing separable learning scenario labels correspond perfectly concept goal find 
sufficient whittle version space point diameter return remaining hypotheses 
likewise diameter current version space hypothesis chosen error respect worst case target 
non bayesian setting active learning reducing diameter version space 
current version space quantify amount point reduces diameter 
denote classifiers assign value remainder assign value 
think cut hypothesis space see 
example clearly helpful doesn reduce diameter say reduces average distance hypotheses measure doing reduce diameter certain direction 
notion arbitrary metric spaces captures intuition 
consider finite think element edge vertices edge represent pair hypotheses need distinguished relatively far apart way achieve target accuracy remain version space 
hope finite set edges queries remove substantial fraction 
point said split label guaranteed reduce number edges fraction max 
instance edges split target accuracy really care edges length 
define 
say subset hypotheses splittable finite edge sets splits 
paraphrasing fraction distribution useful splitting gives sense unlabeled samples needed 
points query emerge enormous pool unlabeled data 
soon parameters play roughly roles labels needed unlabeled points needed step understanding establish trivial lower bound 
lemma pick set splittable 
proof 
pick finite edge set denote number edges cut point chosen random edges length chance cutting ez 
ez rearrangement claimed 
course hope larger value 
see splitting index roughly characterizes sample complexity active learning 
lower bound start showing region hypothesis space low splitting index contain hypotheses conducive active learning 
theorem fix hypothesis space distribution suppose splittable 
active learner achieves accuracy target hypotheses confidence random sampling data needs unlabeled samples labels 
proof 
set edges length vertices 
ll show order distinguish hypotheses unlabeled samples queries needed 
pick unlabeled samples 
probability points splits put differently potential queries bad outcome edges eliminated 
case target hypothesis labels required 
examples apply lower bound simple corollary 
edge length constructed consist solely edge see typically expect course deal smaller 
cover lg st split st return st function split repeat draw unlabeled points xt 
query maximally splits qt qt remaining edges qt return remaining hypotheses generic active learner 
corollary suppose neighborhood hypotheses 
hn hi disagree sets hi disjoint different set splittable active learning scheme achieves accuracy labels target hypotheses matter unlabeled data available 
case distance metric 
hn accurately depicted star center spokes leading hi 
query cuts spoke queries needed 
upper bound show loosely matching upper bound sample complexity algorithm repeatedly halves diameter remaining version space 
half target error rate starts cover set hypotheses distance 
known possible find size ln theorem 
cover serves surrogate hypothesis class instance final hypothesis chosen 
algorithm hopelessly intractable meant demonstrate upper bound 
theorem target hypothesis pick target accuracy confidence level 
suppose splittable 
appropriate choice probability algorithm draw unlabeled points queries return hypothesis error 
theorem possible derive label complexity bounds fine tuned specific target hypothesis 
time extremely loose attempt optimize logarithmic factors 
examples simple boundaries line returning example hw hw threshold function hw 
suppose underlying distribution simplicity ll assume density discussion easily generalized 
distance measure induces hw hw hw hw assuming 
pick accuracy consider finite set edges hwi hw 
loss generality wi nondecreasing order edge length greater wi 
pick wn 
easy see wn eliminate half edges splittable 
echoes simple fact active learning just binary search 
intervals line case consider identical earlier example linear separators results carry example constant factors 
hypotheses correspond intervals real line ha ha 
assume density 
distance measure induces ha ha denotes symmetric difference 
simple class hypotheses easier active learn 
hypotheses amenable active learning 
divide real line disjoint intervals probability mass hi denote hypotheses value corresponding intervals 
zero concept 
hi satisfy conditions corollary star shaped configuration forces value active learning doesn help choosing 
hypotheses amenable active learning 
bad hypotheses ones intervals small probability mass ll see larger concepts bad particular interval mass splittable 
pick ha 
consider set edges endpoints ha length 
lengths denote probability masses 
concept ha precisely interval lie outer box contain inner box inner box empty 
edge ha ha length single interval union intervals total length lies inner outer boxes 
pick random distribution restricted space boxes 
space mass occupied 
separates ha ha probability 
look expected number edges split probability edges split 
splits 
summarize hypothesis ha ha denote probability mass interval 
set splittable 
short version space efficient active learning possible 
initial phase getting managed random sampling labels bad large 
linear separators uniform distribution encouraging positive result active learning date learning homogeneous origin linear separators data drawn uniformly surface unit sphere splitting indices case bring immediately theorem splittable 
related open problems lot related model points queried synthetically constructed chosen unlabeled data 
expanded role model substantially different intuitions carry instance corollary generalizes notion teaching dimension 
discussed 
technique useful active learning look unlabeled data place bets certain target hypotheses instance ones large margin 
insight nicely formulated specific active learning orthogonal search issues considered 
positive examples random data point intersects version space chance splitting 
permits naive active learning strategy suggested just pick random point label sure 
kinds problems prototypical cases intelligent querying needed 

grateful yoav freund introducing field peter bartlett john langford adam kalai claire helpful discussions anonymous nips reviewers detailed perceptive comments 
angluin 
queries revisited 
alt 

blum 
pac style model learning labeled unlabeled data 
eighteenth annual conference learning theory 
cohn atlas ladner 
improving generalization active learning 
machine learning 
dasgupta 
analysis greedy active learning strategy 
nips 
dasgupta 
full version www cs ucsd edu dasgupta papers sample ps 
dasgupta kalai 
analysis perceptron active learning 
eighteenth annual conference learning theory 
freund seung shamir tishby 
selective sampling query committee algorithm 
machine learning journal 
goldman kearns 
complexity teaching 
journal computer system sciences 
haussler 
decision theoretic generalizations pac model neural net learning applications 
information computation 
shawe taylor bartlett williamson anthony 
structural risk minimization data dependent hierarchies 
ieee transactions information theory 
