scalable inference hierarchical generative models thomas dean department computer science brown university providence ri borrowing insights computational neuroscience family inference algorithms class generative statistical models specifically designed run commonly available distributed computing hardware 
class generative models roughly architecture visual cortex shares structural computational characteristics 
addition describing variants basic algorithm preliminary experimental results demonstrating pattern recognition capabilities approach characteristics approximations algorithms produce 
lee mumford propose generative model visual cortex combines top bottom inference employs invariant compositional representations encoding retrieving patterns supports associative recall pattern completion sequence prediction functions 
model cast graphical statistical model providing clear semantics terms joint distribution set random variables representing features target pattern space 
inference algorithms graphical models appear task learning parameters absorbing evidence scale primate visual cortex 
propose remedy situation 
shows regions temporal cortex inferotemporal cortex illustrated left stack increasingly visual features associated processing units right arranged cortical sheet 
depicts postulated interaction regions implementing top bottom communication 
bottom communications combine primitive features ones top communications exploit expectations generated prior experience 
derives associating cortical region random variable encoding knowledge probabilistic relationships features hierarchy features 
information propagates hierarchy marginal probabilities associated variables updated reflect evidence bottommost level 
represent cortical regions observed data input 
chain rule simplifying assumption sequence variable independent depiction regions temporal cortex hierarchical bayesian model proposed lee mumford 
activity ith region influenced bottom feed forward data xi top probabilistic priors xi xi representing feedback region 
variables immediate neighbors sequence write equation relating regions resulting graphical model bayesian network chain variables primate cortex consists approximately sparsely connected neurons organized columnar structures composed roughly cells 
assign variable model columnar structure network consisting variables formidable challenge inference algorithm 
generalized pyramid graphs clearly cortex structure significantly complicated indicated simple schematic shown 
model hierarchical arrangement regions structure regions employ class acyclic graphs called pyramid graphs term borrowed characterize class bayesian networks loopy belief propagation works reasonably 
generalized pyramid graph directed acyclic graph nodes partitioned levels level designating bottommost input level level designating topmost root level 
directed edge connects node level node level inter level edges node level intra level edges 
child level unique parent level node level fi set children level fi said receptive field adjacent nodes level respective receptive fields fi fj may may overlap 
nodes level arranged square grid 
nodes level grid adjacent grid diagonal directions aligned grid axes 
nodes grid adjacent may may adjacent nodes comprising receptive field arranged square sub grid grid level containing receptive field 
receptive fields nodes grid adjacent parent level overlap share nodes child level 
possible receptive fields nodes grid adjacent level overlap child level example case overlap level greater half width receptive fields level 
overlap level specified integer determine receptive fields share nodes depending arrangement nodes associated receptive fields parent level 
depicts generalized pyramid graphs discuss length section 
pyramidal bayesian networks graphical model compact representation joint probability distribution set random variables conditional dependencies completely characterized graph random variable associated vertex 
bayesian networks bns markov random fields mrfs examples graphical models 
pyramid graph bayes networks referred text graphical model nodes observed assigned values subsequent inference remaining variables hidden 
graphical model parameterized assigning node graph conditional probability density cpd case directed models bayesian networks potential function case undirected markov random fields 
generalized pyramid graphs represent dependency structure class graphical models called pyramidal bayesian networks 
level corresponds input level level evidence added bayesian network consists terminal nodes bayesian network 
level homogeneous meaning nodes level cpds inhomogeneous meaning parameters differ established separately 
case homogeneous levels parameters nodes level tied expedite inference 
limit families allowable densities just gaussians tabular cpds methods described extended broader class continuous densities nonparametric belief propagation methods 
level mix discrete tabular continuous gaussian nodes 
component problems subnets associated simple level pyramid graph bayes network subnet graph associated decomposition pyramid graph bayes network shown hierarchical expectation refinement section describe algorithm called hierarchical expectation refinement operates decomposing problem learning large pyramidal bayesian network pbn set tractable component learning problems 
problem estimating parameters pbn decomposed subproblems level 
subproblems subdivided smaller component learning problems corresponding graph fragments create small bayesian networks referred subnets solved isolation 
learning proceeds level level bottom starting input level 
learning level proceed simultaneously parallel subnets associated level 
simplicity presentation assume nodes bn discrete corresponding cpds represented conditional probability tables cpts 
example shown 
depicts level pbn showing decomposition layer subnets 
dashed boxes enclose variables involved component learning problems associated layer 
edges extend outside dashed box eliminated purposes parameter estimation isolating relatively small tractable subgraph characterized joint probability distribution learning component labeled 
estimate parameters subnet expectation maximization training set assignments variables set variables hidden 
estimated parameters quantify pbn throw away rest 
generalizing suppose want learn parameters node level having learned parameters nodes levels 

recall node level parents level 
define extended family node level parents children parents level learn parameters associated node level parameters parents construct component bn set nodes corresponding extended family component bn nodes level observed level hidden 
initialize cpts variables component bn random distributions dirichlet priors collect sufficiently large set samples hidden nodes component set samples training set learning component parameters 
assign parameters cpt pbn cpt component bn 
straightforward version learning algorithm learn parameters variables level proceeding level 
learning parameters variables level training set assignments variables level correspond inputs 
case assignments required estimating parameters variables level training data acquired sampling posterior belief function bel vector observed variables component learning problem level assignment observed variables input level pbn 
hand written digit recognition experiments described section learn parameters root penultimate levels unsupervised manner 
subnets corresponding penultimate level typically trained classifier supervised manner supplied training labels 
number obvious optimizations 
creating separate component problem node level compute maximal sets collection sets corresponding extended families node 
maximal sets construct set component bns 
estimate parameters component bn learned parameters quantify variables extended family subset maximal set associated component bn reducing total number component problems considerably 
homogeneous level construct single component bn effectively ties parameters nodes level 
algorithms inference 
exact inference general graphical models np hard 
fast algorithms exist perform exact inference graphical models consisting hundreds thousands nodes 
exact inference implementation lauritzen spiegelhalter junction tree algorithm referred jtree 
approximate inference loopy belief propagation implemented belief propagation algorithms pearl original message passing algorithm pearl variant aji mceliece gdl generalized distributive law algorithm exhibits better convergence networks 
alternative jtree pearl exploit structure induced component bns perform approximate inference 
depicts graph subnet graph vertices subnets si edge si sj just case si hidden node observed node sj 
subnet graph perform inference propagating samples input level root follows soon subnet observed nodes instantiated compute subnet posterior distribution map assignment hidden variables 
assignment propagated subnet parent subnets subnet graph 
refer algorithm subnet 
implementation evaluation approach described previous section designed implemented compute cluster mpi message passing interface code handle communication processes responsible component learning problems 
haven realized parallel implementation developed non parallel prototype matlab kevin murphy bayes net toolbox available download www cs brown 
edu projects cortex prototype tar gz includes code required replicate experiments discussed section 
tested ideas problem recognizing handwritten digits 
yann lecun taken subset images national institute science technology nist database handwritten digits size normalized digit centered fixed sized image produce data set consisting training images digits produced set writers test images produced additional set writers disjoint set 
lecun data set available yann lecun com exdb mnist 
experiments nist hand written digit data set consider small perform required inference directly global model full pbn compare performance global model performance various decompositions global model 
inference plays different roles inference required generate samples required local model subnet parameter estimation sufficient number samples obtained inference required estimate parameters local models model parameters learned inference required evaluation purposes 
global model inference local model parameters update parameters global model 
investigate factors governing performance different algorithms roles inference required 
instance generate samples perform evaluation exact inference algorithm applied global model update parameters global model local models 
experiment provides insight local models perform learning generative models 
substitute approximate inference algorithm scale size global model evaluate impact approximation performance 
preliminary experimental results pbn designed characteristics mind create network exhibits features consider important architecture multiple levels overlapping receptive fields intra level edges network small possible perform exact inference global model 
pbn shown summarized follows width overlap intra refer respectively width receptive fields number rows columns overlap intra level edges respect level level width overlap intra size subnets na total input level maps directly bit images digits comprising data set 
largest subnet nodes 
total number parameters 
models follow maximum number parameters single cpd corresponding random variable possible states parents possible states 
input level homogeneous consisting conditional gaussian nodes implementing mixture gaussians classifier applied image patch 
levels inhomogeneous nodes discrete 
describing experiment specify number patches train input level patch number images training levels train number images evaluate performance test percentage training images correctly classified train percentage testing images correctly classified test cpu hours required training evaluation hour 
addition identify inference algorithm roles mentioned specifying triple alg alg alg jtree pearl subnet na alg alg serve shorthand alg alg alg alg alg respectively jtree na jtree serves benchmark smaller networks 
results representative experiments emphasize implications subnet estimate parameters global model algorithm patch train test train test hour subnet jtree subnet jtree subnet jtree jtree na jtree jtree na jtree substitute loopy belief propagation exact inference get mixed results due fact pearl failed converge training testing examples occasionally failed converge smaller set algorithm patch train test train test hour subnet pearl subnet na subnet sampling evaluation observe pattern performance training set falls subnet just sampling jtree evaluation restored subnet roles algorithm patch train test train test hour subnet jtree subnet jtree subnet jtree subnet subnet subnet pbn shown shown eliminate intra level edges level order evaluate impact intra level edges 
pbn far fewer parameters experimental model versus 
level width overlap intra size subnets na total get reduction performance exact inference sampling evaluation algorithm patch train test train test hour subnet jtree subnet jtree jtree na jtree jtree na jtree pearl converge cases 
running times pearl matlab relatively poor performance problems easily vectorized algorithms desirable implemented exploit parallelism 
algorithm patch train test train test hour subnet pearl subnet subnet subnet na pbn shown designed test impact adding additional level 
resulting network subnets largest nodes 
total number parameters level width overlap intra size subnets na total network large algorithms performed previous reasonable considers ratio training examples total number parameters 
experiments networks size wait parallel implementation reduce training evaluation time hour case training testing examples 
algorithm patch train test train test hour subnet subnet pbn shown designed reduce total number parameters somewhat smaller receptive fields encouraging localized features lower levels 
uses receptive fields overlap rows columns lowest level 
number subnets previous network parameters total level width overlap intra size subnets na total performance respectable experiments run far reduction performance training data performance test examples 
experiments involving subnet performance training data goes slightly testing performance improves 
algorithm patch train test train test hour subnet subnet subnet performs inference purely bottom fashion algorithms subnet pearl propagate information hierarchy 
obvious alternative perform variant belief propagation operates directly subnet graph 
subnets serve algorithmic variants couple nearby features serve diagnostic causal inference 
test idea implemented instance generalized belief propagation framework described referred gbp 
gbp belief propagation carefully orchestrated passes subnet graph 
start pass subnets input level incorporate new evidence propagate marginal distributions shared variables level subnets adjacent subnet graph 
marginals computed posterior distribution incorporates new evidence marginalize variables intersection adjacent subnets 
evidence continues propagate subnet graph reaches highest level subnet parameters learned 
second pass marginals propagated back input level 
third pass essentially pass completes process distributing new evidence subnet graph 
get idea additional passes impact performance compare gbp subnet algorithm benchmark jtree na jtree 
table shows results set experiments network shown note gbp significantly outperforms subnet compares favorably jtree na jtree 
algorithm patch train test train test hour gbp gbp subnet subnet jtree na jtree jtree na jtree provide results set experiments network intra level edges shown case difference performance gbp subnet significant manifest marked reduction accuracy training data compared jtree na jtree 
algorithm patch train test train test hour gbp gbp subnet subnet jtree na jtree jtree na jtree compare performance gbp subnet network shown don explanation gbp performs subnet training data outperforms subnet testing data behavior observed similar experiments involving network 
algorithm patch train test train test hour gbp subnet related essentially implementing expectation maximization distributed highly structured model hierarchically arranged levels topologies embed plane nearly 
expect perceptual learning problems particular way decompose parameter estimation problem related hinton weight sharing parameter tying techniques avoid potential problems due early permanently assigning parameters lower layers 
originally attracted idea level level parameter estimation reading george hawkins hierarchical bayesian model invariant pattern recognition visual cortex 
algorithm described node bayesian network collects instantiations children uses derive state space associated random variable estimate parameters children cpds 
states comprise state space node correspond perturbed versions exemplars frequently appearing vectors instantiations children 
network singly connected class graphs pearl belief propagation algorithm exact efficient scales linearly diameter underlying graph 
hinton osindero teh hybrid model combining boltzmann machines directed acyclic bayesian networks improvement generalization hinton method contrastive divergence 
authors greedy level level approach learning generative model similar proposed lee mumford 
subnet medium grain parallelizable subnet running separate processor case fewer processors subnets strategy depicted communication fixed sharing hidden observed random variables specified subnet graph 
local computations yield parallel implementation subnets allocated processors roughly columnar arrangement subnets adjacent subnet graph tend run processor 
reasonably approximation global joint distribution promising 
preliminary results suggest inference intra level edges potentially problematic developing hybrid model inter level edges directed intra level edges undirected inference performed semi synchronous manner 
model related model described hinton osindero bao propagation carried levels simultaneously followed propagation adjacent levels simultaneously repeating cycle quiescence 
srinivas aji robert mceliece 
generalized distributive law 
ieee transactions information theory 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
george jeff hawkins 
hierarchical bayesian model invariant pattern recognition visual cortex 
proceedings international joint conference neural networks 
ieee 
hinton 
training products experts minimizing contrastive divergence 
neural computation 
geoffrey hinton simon osindero bao 
learning causally linked markov random fields 
robert cowell zoubin ghahramani editors proceedings th international workshop artificial intelligence statistics january hotel pages 
society artificial intelligence statistics 
geoffrey hinton simon osindero yee teh 
fast learning algorithm deep belief nets 
submitted neural computation 
isard 
real valued graphical models computer vision 
proceedings ieee computer society conference computer vision pattern recognition volume pages 
steffen lauritzen david spiegelhalter 
local computations probabilities graphical structures application expert systems 
journal royal statistical society 
tai sing lee david mumford 
hierarchical bayesian inference visual cortex 
journal optical society america july 
kevin murphy 
bayes net toolbox matlab 
computing science statistics 
kevin murphy yair weiss michael jordan 
loopy belief propagation approximate inference empirical study 
proceedings th conference uncertainty artificial intelligence pages 
morgan kaufmann 
judea pearl 
distributed revision composite beliefs 
artificial intelligence 
sudderth freeman willsky 
nonparametric belief propagation 
proceedings ieee computer society conference computer vision pattern recognition volume pages 
jonathan yedidia william freeman yair weiss 
understanding belief propagation generalizations 
exploring artificial intelligence new millennium pages 
morgan kaufmann publishers san francisco 
daniel yellin 
algorithms subset testing finding maximal sets 
proceedings rd annual acm siam symposium discrete algorithms pages 

