learning multiple related tasks latent independent component analysis jian zhang zoubin ghahramani yiming yang school computer science gatsby computational neuroscience unit mellon university university college london pittsburgh pa london wc ar uk jian zhang zoubin yiming cs cmu edu propose probabilistic model independent component analysis learning multiple related tasks 
model task parameters assumed generated independent sources account relatedness tasks 
laplace distributions model hidden sources possible identify hidden independent components just modeling correlations 
furthermore model enjoys sparsity property parsimonious robust 
propose efficient algorithms empirical bayes method point estimation 
experimental results multi label text classification data sets show proposed approach promising 
important problem machine learning generalize multiple related tasks 
problem called multi task learning learning learn cases predicting multivariate responses 
multi task learning potential practical applications 
example newswire story predicting subject categories regional categories reported events input text problem 
mass tandem spectra sample protein mixture identifying individual proteins contained peptides example 
attention machine learning research placed effectively learn multiple tasks approaches proposed 
existing approaches share basic assumption tasks related 
general assumption beneficial learn tasks jointly borrow information learn task independently 
previous approaches roughly summarized relatedness tasks modeled iid tasks bayesian prior tasks linear mixing factors rotation plus shrinkage structured regularization kernel methods 
previous approaches basic assumption multiple tasks related 
consider case tasks task binary classification problem input space multiple simultaneous classifications text documents 
separately learn classifier parameters task ignoring relevant information classifiers 
assumption tasks related suggests different tasks related 
natural consider different statistical models related 
propose model multi task learning independent component analysis ica 
model parameters different classifiers assumed generated sparse linear combination small set basic classifiers 
coefficients sparse combination factors sources basic classifiers learned data 
multi task learning context relatedness multiple tasks explained fact share certain number hidden independent components 
controlling model complexity terms independent components able achieve better generalization capability 
furthermore distributions laplace able enjoy sparsity property model parsimonious robust terms identifying connections independent sources 
model combined popular classifiers indispensable part scalable algorithms empirical bayes method point estimation able solve high dimensional tasks 
probabilistic model convenient obtain probabilistic scores confidence helpful making statistical decisions 
discussions related section 
latent independent component analysis model propose solving multiple related tasks latent independent component analysis lica model hierarchical bayesian model traditional independent component analysis 
ica promising technique signal processing designed solve blind source separation problem goal extract independent sources observed data linear combinations unknown sources 
ica successfully applied blind source separation problem shows great potential area 
help non gaussianity higher order statistics correctly identify independent sources opposed technique factor analysis able remove correlation data due intrinsic gaussian assumption corresponding model 
order learn multiple related tasks effectively transform joint learning problem learning generative probabilistic model tasks precisely task parameters precisely explains relatedness multiple tasks latent independent components 
standard independent component analysis observed data estimate hidden sources lica observed data ica task parameters 
consequently latent need learned training data individual task 
give precise definition probabilistic model lica 
suppose represent model parameters tasks thought parameter vector th individual task 
consider generative model tasks hidden source models denotes distribution parameters linear transformation matrix noise vector graphical model latent independent component analysis usually assumed multivariate gaussian diagonal covariance matrix 
essentially assuming hidden sources responsible dependencies conditioned independent 
generally speaking member exponential families situations noise taken multivariate gaussian convenient 
graphical model equation shown upper level lower part described 
data probabilistic discriminative classifiers building block lica probabilistic model learning individual task focus classification tasks 
notation describe probabilistic discriminative classifier task notation simplicity omit task index 
suppose training input data vector binary class label goal seek probabilistic classifier prediction conditional probability 
assume discriminative function linear form easily generalized non linear functions feature mapping 
output class label thought randomly generated bernoulli distribution parameter model summarized follows denotes bernoulli distribution probability density function random variable 
changing definition random variable able specialize model variety popular learning methods 
example standard logistic distribution get logistic regression classifier standard gaussian get probit regression 
principle member belonging class classifiers plugged lica generative classifiers naive bayes 
take logistic regression basic classifier choice affect main point 
note straightforward extend framework regression tasks likelihood function solved simple efficient algorithms 
point shown graphical model training instances share input vector mainly notation simplicity restriction model 
convenient reality may able obtain task responses training instance 
learning inference lica basic idea inference algorithm lica iteratively estimate task parameters hidden sources mixing matrix noise covariance 
algorithms empirical bayes method point estimation suitable high dimensional tasks 
empirical bayes method graphical model shown example hierarchical bayesian model upper levels hierarchy model relation tasks 
empirical bayes approach learn parameters data treating variables hidden random variables 
get caused interaction assume standard parametric form zero mean unit variance remove 
goal learn point estimators obtain posterior distributions hidden variables training data 
log likelihood incomplete data calculated integrating hidden variables maximization parameters involves complicated integrals respectively 
furthermore classification tasks likelihood function typically non exponential exact calculation intractable 
step approximate solution applying em algorithm decouple series simpler steps steps follows 
step parameter th step compute distribution hidden variables 
step maximizing expected log likelihood complete data expectation taken distribution hidden variables obtained log likelihood complete data written third item depend 
simplification step summarized leads updating equations step need calculate posterior distribution parameter calculated previous step 
essentially second order moments little abuse notation ignore difference discriminative generative classifier level denote likelihood general 
algorithm variational bayes step subscript removed simplicity 
initialize standard distribution laplace distribution case 

solve bayesian logistic regression bayesian classifier 
update 
repeat steps convergence conditions satisfied 
inequality needed 
exact calculation intractable approximate belonging exponential family certain distance measure asymmetric minimized 
case apply variational bayes method applies distance measure 
central idea lower bound log likelihood jensen 
rhs equation want maximize straightforward show maximizing lower bound equivalent minimize kl divergence 
tasks decoupled conduct inference task respectively 
assume general reasonable simplifying assumption allows optimization iteratively 
details step shown algorithm 
comment things algorithm 
assume form multivariate gaussian reasonable choice especially considering fact second moments needed step 
second prior choice step significant associated data point 
particular laplace distribution lead sparse solution clear section 
take parametric form product laplace distributions unit variance known mean fixed variance intended remove issue caused interaction scales 
full covariance gaussian choice due reason caused rotations diagonal gaussian 
result argue product better product gaussians parametric form prior 
variational method bayesian logistic regression efficient algorithm variational method proposed solve step algorithm guaranteed converge known efficient problem 
gaussian prior parameter training set want obtain approximation true posterior distribution 
data point example basic idea exponential function approximate non exponential likelihood function turn bayes formula tractable 
omit task index simplify notation 
inequality step step logistic function 
maximize lower bound em algorithm formulated treating parameter hidden variable due gaussianity assumption step thought updating sufficient statistics mean covariance 
woodbury formula em iterations get efficient shot step updating involving matrix inversion due space limitation skip derivation calculated step reduced find fixed point dimensional problem solved efficiently process performed data point get final approximation 
point estimation empirical bayes method efficient medium sized problem computational cost memory requirement grow number data instances features increases 
example easily happen text image domain number features need faster methods 
obtain point estimation treating limiting case previous algorithm 
specific letting converging dirac delta function step algorithm thought finding map estimation step lasso optimization problem denotes point estimation solved numerically 
furthermore solution optimization sparse 
particularly nice property consider hidden sources association tasks significantly supported evidence 
experimental results lica model effectively tasks want learn related 
experiments apply lica model multi label text classification problems case existing text collections including popular ones reuters new rcv corpus 
individual task classify document particular category assumed multi label property implies tasks related latent sources semantic topics 
reuters choose categories categories fact categories correlated previous studies 
preprocessing get unique features words empirical bayes method stemming remove stopwords rare words words occur times 
macro individual lica reuters training set size micro individual lica rcv training set size multi label text classification results reuters rcv solve problem 
hand include topic categories rcv corpus get larger vocabulary size unique features 
bayesian inference intractable high dimensional case memory requirement store full covariance matrix 
result take point estimation approach reduces memory requirement data sets standard training test split rcv test part corpus huge documents randomly sample test set 
effectiveness learning multiple related tasks jointly best demonstrated limited resources evaluate lica varying size training set 
setting repeated times results summarized 
result individual obtained regularized logistic regression category individually 
number tasks equal reuters rcv respectively set dimension hidden source experiments 
measure preferred error rate text classification due unbalanced positive negative document ratio 
reuters collection report macro results corpus easier micro methods 
rcv collection report micro due space limitation fact observed similar trend macro values lower due large number rare categories 
furthermore achieved sparse solution point estimation method 
particular obtained non zero sources tasks rcv collection 
discussions related viewing multitask learning predicting multivariate responses breiman friedman proposed method called regression problems 
intuition apply shrinkage rotated basis original task basis information borrowed tasks 
treating tasks iid generated probability space empirical process theory applied study bounds asymptotics multiple task learning similar case standard learning 
hand general bayesian perspective treat problem learning multiple tasks learning bayesian prior task space 
despite generality principles necessary assume specific structure parametric form task space functional space usually higher infinite dimension compared input space 
model related proposed semiparametric latent factor model regression teh 
uses gaussian processes gp model regression latent factor analysis 
difference fa ica advantage gp non parametric works instance space disadvantage model training instances need shared tasks 
furthermore clear explore different task structures instance space viewpoint 
pointed earlier exploration different source models important learning related tasks prior plays important role standard learning 
proposed probabilistic framework learning multiple related tasks tries identify shared latent independent components responsible relatedness tasks 
corresponding empirical bayes method point estimation algorithms learning model 
non gaussian distributions hidden sources possible identify independent components just decorrelation particular enjoyed sparsity modeling hidden sources laplace distribution 
having sparsity property model parsimonious robust dependence latent independent sources shrunk zero significantly supported evidence data 
learning related tasks jointly able get better estimation latent independent sources achieve better generalization capability compared conventional approaches learning task done independently 
experimental results multi label text classification problems show evidence support claim 
approach assumes underlying structure task space linear subspace usually capture important information independent sources 
possible achieve better results incorporate specific domain knowledge relatedness tasks model obtain reliable estimation structure 
research consider flexible source models incorporate domain specific knowledge specify learn underlying structure 
ando zhang framework learning predicative structures multiple tasks unlabeled data 
technical rc ibm watson research center 
baxter model inductive bias learning 
artificial intelligence research 
breiman friedman predicting multivariate responses multiple linear regression 
royal stat 
society 
micchelli pontil learning multiple tasks kernel methods 
machine learning research 
ghahramani beal variational inference bayesian mixtures factor analysers 
advances neural information processing systems vol 
heskes empirical bayes learning learn 
proc 
th icml 
jaakkola jordan variational approach bayesian logistic regression models extensions 
proc 
sixth int 
workshop ai statistics 
koller sahami hierarchically classifying documents words 
proc 
th icml 
roberts 
editors 
independent component analysis principles practice cambridge university press 
teh seeger jordan semiparametric latent factor models 
ghahramani cowell editors workshop artificial intelligence statistics 
thrun pratt 
editors 
learning learn kluwer academic publishers 
