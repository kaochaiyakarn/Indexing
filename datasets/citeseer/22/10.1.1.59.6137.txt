face recognition weighted locally linear embedding nathan christian john tsotsos department computer science center vision research york university toronto ontario canada tsotsos cs yorku ca approach recognizing faces varying appearances considers relative probability occurrence appearance 
propose demonstrate extending dimensionality reduction locally linear embedding lle model local shape manifold neighboring nodes graph probability associated node considered 
approach implemented software evaluated yale database face images 
recognition rates compared non weighted lle principal component analysis pca setting weighted lle achieves superior performance 
keywords face recognition non linear dimensionality reduction locally linear embedding 
face detection recognition extremely active area computer vision resulting large number publications evidenced meta reviews 
face recognition characterized high level pattern recognition problem humans skilled machines presents considerable challenge 
addition authentication recognition efforts attempt utilize face recognition improve user interfaces 
set training images face recognition system needs independently train recognize person new image 
useful system needs capture image detail enable reliable recognition minimal sensitivity variations test image pose face lighting 
data tends large scalability critical 

related research due overwhelming number approaches proposed literature extensive survey face recognition techniques scope 
restrict discussion approaches dimensionality reduction 
kirby sirovich proposed low dimensional reconstruction technique face images maximizes variance optimally assumption data distribution follows gaussian cloud keeping significant components eigenfaces karhunen loeve expansion principal component analysis pca see 
technique extended face recognition turk pentland dimensionality reduction reduce complexity comparing input image images stored database database storage requirements 
technique tremendous impact area machine vision general remains popular dimensionality reduction techniques object recognition day 
murase nayar clever extension pca represent different appearances object continuous manifold low dimensional space 
technique projects image low dimensional space identify object 
object recognized projected new coordinate system defined specifically object 
murase nayar system produces impressive results sensitive slight changes object shape lighting conditions 
dimensionality reduction techniques shown outperform pca settings ica de correlates higher order statistics image lda supervised technique seeks orthonormal projection maximizes inter class vs intra class distances 
pca remains popular due simplicity efficiency applicability general case 
inherent disadvantages pca squares estimation technique pca extremely sensitive outliers 
issue increasing evidence image space face space particular non linear making application pca face recognition sub optimal ica lda linear reduction techniques suffer disadvantage 
significant effort devoted pursuit appropriate dimensionality reduction technique effectively model image space 
techniques developed consider non linear structure manifold particularly locally linear embedding lle see better discriminators face images classification purposes suggesting problem face recognition local manifold structure carries discriminating power global euclidean structure 
pitfall techniques local structure data point learned neighbors 
words techniques require large training set available 
promising results produced non linear approaches come cost higher computational complexity pca 
schemes proposed extend pca address issues missing data robustness outliers 
aj propose technique extend pca include weights images training set individual pixels image 
weights recognition test face computation reduced space 
powerful idea key element approach 

contributions apparent effectiveness non linear non global dimensionality reduction potential weights motivate section prompted investigate generalization non linear technique process images variable probabilities 
main contributions research follows certain appearance variations modelled variance standard image decaying probability occurrence test image diverges standard image 
second unified framework allows computing low dimensional mapping known locally linear embedding channels learning recognize images variable probabilities 

outline proposes technique face recognition variable probabilities locally linear embedding lle 
composed main sections 
section provided motivation considering probabilities face appearances dimensionality reduction local features 
section describe algorithmic technical aspects approach general terms 
section presents empirical results realization algorithm 
section provides summary section points directions research 

technical approach approach ideas variation face appearance produce range face images varying probabilities occurrence 
shown space face images highly non linear 
approaches demonstrate superior face recognition rates pursuit locally linear dimensionality reduction scheme justified 
non linear dimensionality reduction techniques lle may enhanced introducing image weights represent image probability occurrence 
ideas discussed greater detail 

weighted images various generalizations pca including weighted pca statistics decades 
aj applied weighted pca image recognition types weights weights individual pixels spatial weights account parts image unreliable unimportant weights images refer temporal weights 
main motivation idea images subject object reliable 
idea maximize weighted variance low dimensional space order achieve lower reconstruction error certain target images pixels 
possible applications image weights tuning authentication system achieve lower error rates individuals high levels classification 
inclusion weights motivated different argument 
normally attempt standardize input images minimize variations 
example variation location reduced image alignment projecting sliding window image reduced space looking minimum distance projection hyperplane 
variations difficult mitigate variation due scale orientation rotation respect camera optical axis pose relative camera facial expression occlusion lighting conditions presence absence features glasses articles clothing hats 
reason face recognition systems typically trained multiple images individual 
systems treat training images equally implicitly assuming probability occurrence uniformly distributed 
clearly assumption flawed 
example consider plane rotation faces appear upright 
absence specific knowledge probability distribution face images respect rotation convenient assume gaussian distribution highly non uniform 

locally linear dimensionality reduction quest non linear techniques dimensionality reduction driven various factors necessarily relating machine vision 
roweis illustrates inadequacy pca modelling artificial examples swiss roll 
pca fails generate feasible mapping due reliance euclidean distances geodesic distances 
demonstrated pca extreme sensitivity outliers examples ironic axes selected 
addition contrived examples works demonstrated space face images lies non linear manifold 
method appears generate particularly results face recognition locally linear embedding lle 
lle computes low dimensional embedding adjacency points original space maintained low dimensional space 
words local arrangement points preserved 
addition learning local structure manifold technique promotes robustness outlier affects neighbors 
hand lle heavily dependent sufficient sampling manifold learn shape 
manifold sampled point neighbors lie linear hyperplane describes patch manifold 
point lle finds coefficients neighbors best describe linear combination generates lowest reconstruction error 
sets coefficients computed lle computes mapping low dimensional space point approximated coefficients minimizing reconstruction error 
various flavors lle exist different formulations define local neighborhood 
simplest form identifies nearest neighbors point euclidean sense 
slightly complex formulation looks neighbors fall ball radius 
roweis uses method simpler computationally expensive 
loss generality implemented approach nearest neighbor model extending variable image weights 
extension radius ball formulation fairly simple 

image weights lle aj 
introduce methods image pixel weights pca 
em algorithm allows online processing images batch algorithm training data available upfront 
batch method introduces weights places computation eigenfaces calculation distance projected test image recognition step 
compute weighted eigenfaces input image xi normalization subtracting average face multiplied image weight follows xi wi xi 
xi adjusted image eigenvectors covariance matrix computed image weights factored average face 
similarly spatial weights introduced follows xij xij 

xi image adjusted spatial weights weight pixel ultimately weighted input image xi eigenfaces derived computed follows xij ws xij 

aim introduce image weights lle fashion achieve similar results 
limit research time nearest neighbors flavor lle 
lle algorithm consists steps 
identify nearest neighbors 
step generally inexpensive 
implementation employs brute force algorithm complexity dn size data original dimensionality nearest neighbors computed nlogn time trees ball trees 

compute wij weights best reconstruct data point xi neighbors 
computational complexity step 

compute low dimensional vectors yi best reconstruct weights computed step 
step runs dn time reduced dimensionality 
extension images associated weights represent probability occurrence reliability 
local shape manifold point learned neighbors need factor neighbors weights order reduce effect euclidean neighbors low weights 
need examine steps algorithm closer 
nearest neighbors point xn identified local gram matrix computed 
ci xn xi xn xj neighbor 

note gram matrix symmetric definite defines difference vectors xi xj isometry 
optimal weights minimize reconstruction error easily computed lagrange multipliers equivalently solving cwi wi normalizing wi wij 
euclidean distance point neighbors plays role selection neighbors formation gram matrix 
clearly need adjust distance incorporate neighbor probability occurrence weight 
pi probability occurrence point xi 
compute adjusted distance dij point neighbor ij ij consequence determining point neighbors step computation gram matrix step 
identifying point nearest neighbors search points smallest value ij adjusted gram matrix point computed follows pi xn xi xn xj neighbor 

solution wi yields best weights adjusted probability occurrence embedding step computed adjusted weights 
issue considered selecting appropriate value allows fair comparison standard training set enlarged introducing additional variations 
ultimately defines size neighborhood learn local shape manifold 
adding training data introducing variations generated rotation increases sample density aids learning manifold effectiveness lle greatly depends size training set 
euclidean neighbors low weights play relatively small role construction embedding choosing weights need considered 
factor needs examined similarity degree variation introduced new data 
consider extreme case training set simply doubled duplication equal weights 
clearly case appropriate neighbors excluding point twin 
basic intuition choosing appropriate value leave rigorous complete derivation research 

empirical assessment designed series performance measurements test effectiveness proposed approach 
standard database face images augmented variations faces generated plane rotation various small angles 
gaussian distribution probabilities assigned rotation angle 
rationale selecting rotation mutator rotation easily corrected maximizing projection sliding window commonly done correct location 
faces generally appear upright subject position camera geometry allow small variations 

testing methodology testing yale face database consists images subjects recorded different lighting conditions showing variety facial expressions 
keeping testing methodology applied roweis saul cropped aligned images final size 
reduce computational cost idea proposed niyogi performed pca preprocessing step images reducing largest principal components effectively keeping information 
tested recognition rates achieved pca lle standard yale database upright face images 
lle empirical evidence proved choice 
image database created variations performing plane rotation effectively increasing size database images 
tested extended data set lle intuition provided section weighted lle image weights set 
indicates rank ordered set angles 
recognition rates measured techniques leave strategy fold cross validation image removed database create training set size classified 

results compared recognition error rates achieved methods various dimensionality settings 
demonstrates weighted lle algorithm realizes recognition clustering images individual low dimensional space 
plot markers represent face images projected dimensions different marker individual 
clusters different appearances individual apparent providing insight weighted lle recognition power 
results testing summarized plots error rate face recognition function dimension low dimensional embedding space 
recognition error rate recognition error rates dimensionality reduction pca std 
faces lle std 
faces lle rotations weighted lle rotations embedding dimensionality 
recognition errors obtained experiments different dimension reduction techniques face recognition database 

projection images weighted lle dimensions 
marker type represents different individual 
note formation clusters algorithm 
observations apparent chart 
testing lle consistently outperformed pca 
roweis tests pca achieves better recognition rates lle higher dimensions results entirely odds clarify 
surprisingly lle performance improved larger training set introduced 
note weighted lle achieved superior weighted recognition rates compared non weighted version 
aside computational cost lle weighted non weighted experiments greatly depended size training set 
target dimensionality neighborhood size affected execution time moderately 
difficulty appearance vision techniques computational complexity heavily depends size training data allowing variable probability occurrence training images feasible 

discussion results encouraging confirm hypothesis setting images training set varying probabilities resembling test image weighted lle achieves superior recognition results 
interesting note embedding dimensionality considered experiments weighted lle pro duced lowest recognition error large margin 
weighted lle reached especially low recognition errors embedding spaces small dimensionality 
tests lle performed consistently better pca 
roweis experiments pca beat lle higher dimension settings observed improvement pca recognition rates dimensionality embedding space grows saw improvement lle 
roweis experiments crossover occurs 
tested values reached crossover point 
observation larger training set non weighted lle appears improve dimensionality increases weighted lle recognition rates practically constant 
may suggest existence upper bound recognition rate due poor modelling face manifold areas 
sets include rotation due closer competition neighbors 
may possibly alleviated better parameter tuning 

summary novel approach face recognition 
observation face recognition systems generally improve reliability multiple training images subject noting training images treated equally algorithms proposed extending locally linear embedding procedure weighting scheme 
extension weights associated images represent probability occurrence 
lle algorithm recovers local structure face manifold neighbors face shown impact neighboring faces low weights may reduced 
effectiveness approach verified experiments yale face database 
demonstrated compared pca standard lle weighted lle algorithm performs better embedding dimensionality considered experiments weighted lle produced lowest weighted recognition error rates error rates lower non weighted lle 

noted order realize results chose values empirically 
currently definitive method exists choose optimal values lle non linear dimensionality reduction techniques 
may study response weighted non weighted lle different values mentioned earlier research covers nearest neighbors paradigm 
may useful introduce weights complex due variable neighborhood size fixed radius model 
problem left research rigorous derivation appropriate value variable image probability model 
additionally feel better understanding needed factors contribute assignment modes variation principal axes reduced space 
idea weighting extended assign different weights individual pixels image information deemed significant non linear reduction algorithms isomap 
acknowledgments authors helpful comments reviewing 
belhumeur hespanha kriegman 
eigenfaces vs fisherfaces recognition class specific linear tion 
ieee trans 
pattern anal 
machine intelli 
de la torre black 
robust principal component analysis computer vision 
proc 
iccv pages 
draper bartlett beveridge 
recog faces pca ica 
comput 
vis 
image 
yan hu niyogi 
zhang 
face recog nition 
ieee trans 
pattern anal 
ma chine intelli 
low 
face detection survey 
comput 
vis 
image 
jolliffe 
principal component analysis 
springer verlag new york 
kirby sirovich 
low dimensional procedure characterization human faces 
optical society america 
mart nez kak 
pca versus lda 
ieee trans 
pat tern anal 
machine intelli 
murase nayar 
visual learning recognition objects appearance 
int 
comput 
vision 
roweis saul 
nonlinear dimensionality reduction locally linear embedding 
science 
shashua levin avidan 
manifold pursuit new approach appearance recognition 
icpr vol ume pages 
aj bischof leonardis 
robust pca gorithm building representations panoramic images 
proc 
eccv volume iv pages 
springer verlag 
tenenbaum de silva langford 
global ric framework nonlinear dimensionality reduction 
sci ence 
turk pentland 
eigenfaces recognition 
cognitive neuro science 

yang 
face recognition extended isomap 
proc 
icip pages 

yang kriegman ahuja 
detecting faces images survey 
ieee trans 
pattern anal 
machine 
zhao chellappa phillips rosenfeld 
face recognition literature survey 
acm comput 
surv 
