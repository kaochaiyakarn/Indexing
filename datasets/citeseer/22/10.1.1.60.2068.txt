university california san diego scheduling task parallel applications rapid turnaround desktop grids committee charge dissertation submitted partial satisfaction requirements degree doctor philosophy computer science engineering derrick kondo professor henri casanova chairman professor andrew chien chairman professor phillip bourne professor larry carter professor rich wolski copyright derrick kondo rights reserved 
dissertation derrick kondo approved ac quality form publication university california san diego chair chair table contents signature page 
table contents 
vita publications fields study 
vii list figures 
viii list tables 
xi 
xii 
desktop grids past 
prospects challenges 
goal motivation approach 
contributions 
ii desktop grid system design implementation state art 
background 
system anatomy physiology 

client level 

application resource management level 

worker level 
worker daemon 
worker sandbox 

design trade offs centralization 
scalability 
fault tolerance 
iii resource characterization 
ideal resource trace 
related resource measurements modelling 

host availability 

host load cpu utilization 

process lifetimes 
trace method 
trace data sets 

sdsc trace 

deug lri traces 

ucb trace 
characterization exec availability 

number hosts available time 

temporal structure availability 

temporal structure unavailability 

task failure rates 

correlation availability hosts 

correlation availability host clock rates 
characterization cpu availability 

aggregate cpu availability 

host cpu availability 
example applying characterization results cluster equivalence 
system performance model 

cluster equivalence 
summary 
iv resource management methods models metrics 

models instantiations 

platform model instantiation 

application model instantiation 
proposed approaches 
measuring analyzing performance 

performance metrics 

method performance analysis 
computing optimal makespan 

problem statement 

single availability interval single host 
scheduling algorithm 
proof optimality 

multiple availability intervals single host 

multiple availability intervals multiple hosts 

optimal makespan checkpointing enabled 
resource selection 
resource prioritization 

heuristics 

results discussion 
resource exclusion 

excluding resources clock rate 

makespan predictions 
evaluation different desktop grids 
related 
summary 
vi task replication 

measuring analyzing performance 

performance metrics 

method performance analysis 
proactive replication heuristics 

results discussion 
reactive replication heuristics 

results discussion 
hybrid replication heuristics 

feasibility predicting probability task completion 

probabilistic model task completion 

rep prob heuristic 

results discussion 

evaluating benefits rep prob 
estimating application performance 
related 

task replication 

checkpointing 
summary 
vii scheduler prototype 
overview xtremweb scheduling system 
excl pred heuristic design implementation 

task priority queue 

makespan predictor 

summary contributions 

defining iqr factor 
iqr sensitivity 
additional resource selection exclusion results discussion 
additional task replication results discussion 
proactive replication 
reactive replication 
hybrid replication 
bibliography 
vita computer science stanford university computer science engineering university california san diego publications kondo chien casanova 
resource management rapid application turnaround enterprise desktop grids proceedings acm conference high performance computing networking sc november pittsburgh pennsylvania 
kondo casanova 
computing optimal makespan jobs identical scheduled volatile hosts 
technical report cs dept computer science engineering university california san diego july 
kondo brooks casanova chien 
characterizing evaluating desktop grids study 
proceedings international parallel distributed processing symposium may 
kondo casanova wing berman 
models scheduling mechanisms global computing applications 
proceeding international parallel distributed processing symposium april fort lauderdale florida 
joseph whirl kondo altman calculation relative geometry ribosome directed radical probing data 
rna 

fields study major field computer science studies parallel distributed computing professor henri casanova major field computer science studies computational biology professor russ altman vii list figures ii common anatomy desktop grid systems 
ii cpu availability task execution 
iii distribution small gaps min 
iii host clock rate distribution platform 
iii number hosts available week platform 
iii 
iii cumulative distribution length availability intervals terms time business hours non business hours 
iii cumulative distribution length availability intervals normalized total duration availability terms time business hours non business hours ucb platform 
iii cumulative distribution length availability intervals terms operations business hours non business hours 
iii unavailability intervals terms hours 
iii task failure rates business hours 
iii correlation availability 
iii percentage time cpu availability threshold hosts business hours non business hours 
iii cpu availability host sdsc platform 
iii cpu availability host deug platform 
iii cpu availability host lri platform 
iii cpu availability host ucb platform 
iii model application rate entire sdsc desktop grid number operations seconds versus task size number minutes dedicated cpu time ghz host 
iii cluster equivalence desktop grid cpu function application task size 
lines shown resources weekdays weekends 
iii cumulative percentage total platform computational power sdsc hosts sorted decreasing effectively delivered computational power hosts clock rates 
iv cumulative task completion vs time 
iv scheduling model 
iv cumulative clock rate distributions real simulated platform 
iv laggers application tasks 
iv intg helper function scheduling algorithm 
iv scheduling algorithm single availability interval 
iv example task execution higher lower job 
jobs arrive time 
case task scheduled immediately overhead incurred 
case scheduler waits period scheduling task 
viii iv example task execution higher lower middle job 
iv scheduling algorithm multiple availability intervals 
iv scheduling algorithm multiple availability intervals multiple hosts subintervals denoted double arrows availability interval 
length subinterval shown subinterval lengths differ seconds 
performance resource prioritization heuristics sdsc grid 
complementary cdf prediction error expected operations time interval 
number tasks scheduled left axis hosts available right axis 
performance heuristics thresholds sdsc grid 
heuristic performance sdsc grid 
cause laggers iqr factor sdsc grid 
fcfs 
pri cr 
excl 
excl pred 
length task completion quartiles sdsc grid 
optimal 
fcfs 
pri cr 
excl 
excl pred 
heuristic performance gimps grid 
heuristic performance lri wisc grid 
vi performance heuristics combined replication sdsc grid 
vi waste heuristics proactive replication sdsc grid 
vi performance reactive replication heuristics sdsc grid 
vi waste reactive replication heuristics sdsc grid 
vi probability task completion day task lengths 
vi cdf prediction errors probability task completion day minute tasks dedicated ghz host vi finite automata task execution 
vi timeline task completion 
vi performance rep prob sdsc grid 
vi waste rep prob sdsc grid 
vi cause laggers iqr factor sdsc grid 
fcfs 
pri cr 
excl 
excl pred 
excl pred 
rep prob 
vi cdf task failure rates host 
vi performance difference excl pred transformed ucb lri platforms 
vi performance checkpointing heuristics sdsc grid 
vi length task completion quartiles sdsc grid 
optimal 
fcfs 
pri cr 
excl 
excl pred excl pred 
rep prob 
cause laggers iqr factor sdsc grid 
fcfs 
pri cr 
excl 
excl pred 
excl pred 
rep prob 
ix cause laggers iqr factor sdsc grid 
fcfs 
pri cr 
excl 
excl pred 
excl pred 
rep prob 
performance resource selection heuristics deug grid 
performance resource selection heuristics lri grid 
performance resource selection heuristics ucb grid 
performance proactive replication heuristics deug grid 
performance proactive replication heuristics lri grid 
performance proactive replication heuristics ucb grid 
waste proactive replication heuristics excl pred dup time excl dup time spd 
waste proactive replication heuristics deug grid 
waste proactive replication heuristics lri grid 
waste proactive replication heuristics ucb grid 
performance proactive replication heuristics varying replication level sdsc grid 
performance reactive replication heuristics deug grid 
performance reactive replication heuristics lri grid 
performance reactive replication heuristics ucb grid 
waste reactive replication heuristics deug grid 
waste reactive replication heuristics lri grid 
waste reactive replication heuristics ucb grid 
performance hybrid replication heuristic deug grid 
performance hybrid replication heuristic lri grid 
performance hybrid replication heuristic ucb grid 
waste hybrid replication heuristic deug grid 
waste hybrid replication heuristic lri grid 
waste hybrid replication heuristic ucb grid 
list tables characteristics desktop grid applications 
iii characteristics desktop grid applications 
deriv 
denotes derivable iii correlation host clock rate machine characteristics business hours sdsc trace 
iii correlation host clock rate failure rate business hours 
task size term minutes dedicated ghz host 
iv qualitative platform descriptions 
vi mean performance difference relative excl pred dup increasing number replicas task 
vi mean performance difference waste difference excl pred dup excl pred 
vi mean performance waste difference excl pred rep prob 
vi makespan statistics excl pred sdsc platform 
lower confidence intervals mean 
mean standard deviation median units seconds 
vi summary replication heuristics 
makespan statistics deug platform 
lower confidence intervals mean 
mean standard deviation median units seconds 
makespan statistics lri platform 
lower confidence intervals mean 
mean standard deviation median units seconds 
makespan statistics ucb platform 
lower confidence intervals mean 
mean standard deviation median units seconds 
xi dissertation scheduling task parallel applications rapid turnaround desktop grids derrick kondo doctor philosophy computer science engineering university california san diego professors henri casanova andrew chien chairs early largest distributed computing systems world desktop grids idle cycles mainly desktop pc support large scale computation 
despite enormous computing power offered systems range applications largely limited task parallel compute bound high throughput applications 
limitation mainly heterogeneity volatility underlying resources shared desktop users 
focuses broadening applications desktop grids particular focus development scheduling heuristics enable rapid turnaround short lived applications 
contributions dissertation follows 
mea sure characterize real enterprise desktop grid systems characterization essential accurate modelling simulation 
second characterization design scheduling heuristics enable rapid application turnaround 
heuristics scheduling techniques resource prioritization resource sion task replication 
find best heuristic uses relatively static resource information prioritization exclusion reactive task replication achieve formance factor optimal 
third implement best heuristic real desktop grid system demonstrate feasibility 
xii chapter late largest distributed computing systems world desktop grids aggregate idle cpu cycles desktop pc support large scale computations 
main motivation desktop grids platforms offer high computational power low cost 
reuse existing infrastructure resources systems staff machine hardware support large computational demands 
numerous studies shown desktops cpu availability desktop pc getting expensive prevalent savings infrastructure costs idle cycles desktop pc high factor :10.1.1.22.4887
virtually desktop grid applications run wide area environments task parallel compute bound high throughput 
application task parallel consists tasks independent 
compute bound application high computation communication ratio 
high throughput application tasks number hosts 
desktop grids applications span wide range scientific domains cluding computational biology climate modelling physics cryptography 
desktop grids enable applications utilize computing power provided hundreds thousands hosts relatively little cost allowed applications explore enormous parameter spaces run simulations high levels detail impossible 
instance folding home prediction home projects resulted nu published discoveries understanding protein folding structure prediction 
set hosts scattered internet participate desktop grid projects incredibly diverse terms usage patterns hardware configurations network connectivity 
example home machines little hours month averages minutes day machines enter prise environments power entire day 
configurations hosts participating seti home desktop grid project spans operating systems including version variants cpu types including family variants 
network connectivity hosts ranges dial cable dsl mbps ethernet 
diversity resources developing software infrastructure harnessing idle cycles challenging endeavor 
desktop grids past soon computers networked notion idle cycles desktop pc arose 
section describe desktop grid systems evolved early desktop grid systems implemented deployed 
outline design system discuss strengths weaknesses 
xerox worm earliest desktop grid systems basic resource management security schemes mechanisms limited resource 
worm spread machines xerox parc sequentially scanning list resource addresses 
address worm segment send probe corresponding host response indicate availability 
host idle worm segment replicate execution new host 
execution worm segment avoided disk accesses entirely limit 
applications run included telephone alarm clock image display ethernet network testing 
worm grow worm control mechanism special kill packet broadcasted kill worm entirely 
principle xerox worm little modification modern day internet number malicious worms blaster worm similar spreading mechanisms 
main challenge controllability manageability worm ized approaches desktop grid computing focus ongoing research :10.1.1.10.7529
mid java applet systems javelin han allowed java applications ran secure sandboxed environment harvest idle cycles computers distributed internet 
users browsers download execute tasks form portable java applet 
addition portability java applets executed sandbox browsers reduced risk harmful code downloaded executed host machine 
despite security mechanisms place mechanisms limited running applet 
applet able consume cpu memory resources restriction limiting may impossible requires inspection system performance counters applets allowed 
security mechanisms java applets restrictive desktop grid applications example applets read write local file system 
academic systems tested relatively hosts robustness scalability systems proven 
furthermore systems lacked tools manageability essential large systems example way ensure java runtime environment supported browser date 
time mid authors argued networks shared workstations built commodity components similar better performance massively parallel processor machines mpp fraction cost 
enormous volume commodity components produced re duced production costs dramatically gordon bell rule stated doubling volume reduces unit cost 
engineering lag time developing specialized applications operating systems hardware mpp network workstations attractive alternative 
november ity clusters list top supercomputers 
plethora research gone supporting high performance computing research area ranged cooperative file caching implement ing raid set workstations 
terms harnessing idle computing desktops compute intensive tasks condor distributed batch system relevant 
inception condor extensively desktop grid systems enterprise settings 
condor pools exist containing total hosts pool containing hundreds hosts 
condor pool computer science department university wisconsin machines 
system supports remote checkpointing process migration network data encryption recovery faults component system 
numerous operating systems supported including windows limited functionality unix variants installation require superuser privileges features supported case 
condor user submits application submission daemon 
execution daemon runs resource responsible managing task execution checkpointing terminating task user activity 
condor scheduler matchmaker determines resources suitable application vice versa requirements application machines clock rates ghz specified requirements resources task run night specified 
contact bind task specific resource 
condor designed primarily local area environments running wide area environments hosts private networks firewalls problematic 
particular condor uses udp tcp communication deployment firewalls congested networks difficult 
nascent efforts address problem new methods added stable release september 
late growth internet exploded distributed computing projects sought exploit potential computing power offered tens millions hosts 
largest known project seti home runs embarrassingly parallel astronomy application currently utilizes sec active desktops 
seti home worker daemon runs participating host requests tasks central ized server 
worker daemon ensures task runs host idle 
software centralized server consists server hosts fire walls download update tasks database server stored location inputs outputs various statistics participants 
implementation seti home application specific tools managing system 
impact seti home project proving people significant numbers willing donate cycles desktops large scale computing projects 
remarkable social phenomenon seti home listed web top contributors project terms completed tasks soon teams composed enthusiastic desktop users formed sought gain higher status rankings 
success seti home spurred numerous projects academic industrial endeavors developing multi application desktop grid software 
early academic desktop grid infrastructures xtremweb implemented 
commercial companies entropia united de vices founded companies developed industrial grade desktop grid software professionally tested supported purpose deploying task parallel applications :10.1.1.12.8273
systems tools large scale system manage ment support user authentication data encryption 
prospects challenges commodity compute storage network technologies technology improve costly pervasive desktop grids increasingly attractive running large scale applications 
april purchase dell dimension ghz pentium processor mhz front side bus mb sdram mhz gb ultra ata hard drive rpm mbps network interface 
buy dell tm port gigabit ethernet switch 
university purchase desktops switches excluding costs installation maintenance space power 
cpu availability shared desktop environments average free disk space resulting platform aggregate computing power close terabytes disk space 
desktop grids high return investment 
believe desktops enterprise settings especially useful contribute significant fraction cumulative computing power internet desktop grids 
desktop grid researchers reported ratio useful hosts non useful hosts participating desktop grid projects low 
enterprise desktops relatively high availability usually constant network connectivity 
desktop grid companies entropia united devices separate products target enterprise environments exclusively :10.1.1.12.8273
desktop grids restricted enterprise environments attractive reasons 
hosts limited number system administrative domains 
software configuration hosts operating system version software libraries similar simplify process software infrastructure application deployment developers need software application portable combination operating system operating system version programming library 
second security terms ensuring application executable data tampered issue 
presumably desktop users university malicious attempting thwart computation 
certainly preclude accidental harm application reduce risk occurrence 
enterprise desktop grids attractive exists wide dispar ity structural complexity applications runnable mpp current desktop grid applications 
internet desktop grid applications task parallel compute bound 
table obtained shows list typical applications run enterprise desktop grids 
third column table server bandwidth required support workers forth column maximum number workers mbits sec 
virtually desktop grid applications deployed internet resemble docking application shown table 
ap application task run time characteristics task data size server bandwidth workers docking min 
mbyte mbits sec small data med run min 
mbyte mbits sec blast min 
mbyte mbits sec large data large run min 
mbyte mbits sec maximum workers mbits sec table characteristics desktop grid applications plications compute bound task parallel task sizes order kilobytes megabytes run times order minutes hours 
higher capacity networks enterprise environments allow applications higher communication computation ratios run desktop grids shown lower rows table 
majority applications run desktop grids high throughput applications tasks hosts available 
reason applications deployed desktop grids task parallel com pute bound high throughput hosts volatile heterogeneous 
hosts volatile sense cpu availability desktop grid application may fluctuate dramatically time host shared user owner machine 
host shared way user owner activities keyboard mouse activity processes get higher priority desktop grid task host reserved block time 
hosts wide range clock rates application deployment complicated 
goal motivation approach goal thesis broaden range applications utilize desktop grids 
particular focus designing scheduling heuristics enable rapid application turnaround enterprise desktop grids 
rapid application turnaround mean turnaround order minutes hours versus days months typical high throughput applications run desktop grids 
experience discrete event simulation suggests users desire turnaround time windows hours minutes length example having results lunch hour morning 
true especially industry results required short term deadlines 
indicated need fast turnaround respect biological docking simulations 
simulations especially simulations explore range parameters organized hundreds thousands independent tasks task consists data input sizes order kilobytes task takes order minutes hours run 
applications mpp workloads day length indicating short jobs uncommon 
applications consisting independent tasks soft real time requirements commonly area interactive scientific visualization 
example application requires rapid turnaround line parallel 
tomography construction models projections common electron microscopy tomography create images biological specimens 
electron takes images specimen model ideally refreshed series projections incorporating additional formation obtained new projections 
refresh view new model redirect attention different area specimen correct configuration microscope 
interactively viewing model set projection allows converge correct model quickly turn reduces chance damage sample excessive exposure election beam 
authors determine line tomography amenable grid computing environments include networks workstations develop scheduling heuristics supporting soft deadline application 
particular tomography application embarrassingly parallel projection de composed independent slices distributed set resources processing 
slice order kilobytes megabytes size typically hun thousands slices projection depending size projection 
ideally processing time single projection done user acquiring image microscope typically takes minutes 
line parallel tomography potentially executed desktop grids effective heuristics meeting application relatively stringent time demands 
thesis develop heuristics allow applications requiring rapid turnaround utilize desktop grids effectively focusing particularly enterprise environments 
approach develop characterization volatility heterogeneity real enterprise desktop grids 
characterization influence design scheduling heuristics 
heuristics scheduling techniques resource prioritization resource exclusion task replication 
large difference effective compute rate hosts desktop grid doing resource prioritization cause tasks assigned best hosts 
worst hosts significantly impede application execution excluding hosts may remove bottleneck 
examine various criteria exclude hosts run application tasks 
replicating task multiple hosts reduce chance task fails slows application execution 
method drawback wasting cpu cycles problem desktop grid application 
investigate issues pertaining replication including task replicate host replicate 
contributions ment crux dissertation summarized thesis state scheduling heuristics resource prioritization exclusion reactive task replication techniques relatively static information resources result tremendous performance gains task parallel compute bound applications needing rapid turnaround contributions thesis follows 
accurate measurement characterization desktop grids 
simple novel method measuring availability resources desktop grid platforms 
method records availability ex real application 
characterize temporal structure cpu availability platform individual resources identifying important similarities differences 
measurement characterization useful creating generative predictive explanatory models driving desktop grid simulations shaping design scheduling heuristics 

effective resource management heuristics rapid application turnaround 
desktop grid characterization design heuristics schedul ing techniques resource prioritization resource exclusion task cation 
evaluate heuristics trace driven simulations representa tive desktop grid configurations 
find ranking desktop resources clock rates account availability history sur effective practice 
main result heuristic uses appropriate combination resource prioritization resource exclusion task replication achieves performance factor optimal 

scheduler prototype scheduling applications implement scheduler prototype scheduling application requiring rapid turned 
implementation proves feasibility heuristics real settings 
thesis structured follows 
chapter ii give describe state art desktop grid systems 
chapter iii describe measurement characterization desktop grid system 
detail method measurements method differs studies 
chapter iv outline design evaluation scheduling heuristics rapid application turnaround describing simulation models general scheduling techniques performance metrics 
chapter describe scheduling heuristics prioritization exclusion effectively resource selection quantify performance optimal schedule achievable omniscient scheduler 
best resource selection techniques task failures continue impede application execution chapter vi investigate methods masking task failures means task replication 
examine issues replicate host replicate 
implement best heuristic demonstrate feasibility describe implementation chapter vii 
chapter viii summarize impact thesis 
chapter ii desktop grid system design implementation state art ii background desktop grid system consists large set network connected computa tional storage resources harvested unused purpose large scale computations 
computational resources usually shared users owners machines demand priority desktop grid applications 
result resources availability set machines guaranteed period time 
resources volatile due user activity machine hardware failures network failures example factors turn prevent tasks running completion 
addition volatile resources usually heterogeneous terms clock rate memory disk size speed network connectivity characteristics 
terminology related components desktop grids defined follow 
term client refer user application submission 
utilize desktop grid client submits application consists set tasks server 
scheduler server assigns tasks available worker daemon manages task execution runs host 
term host resource synonymously 
ideal desktop grid system characteristics 
scalability throughput system increase proportionally number resources 

fault tolerance system tolerant server failure example data server crashes worker failure example user shutting machine 
traditionally term failure refers defect hardware software 
term failure broadly include causes task failure including failure host hardware worker software keyboard mouse activity causes worker kill running task 

security machine including data hardware processes pro misbehaving desktop grid application 
conversely application executable input output data may proprietary protected user inspection corruption 

increasingly human resources costly computing resources 
systems provide tools installing updating workers eas ily tools managing applications resources monitoring progress 

unobtrusiveness desktop grid application shares system user user processes priority client worker detects user activity task suspended temporarily activity task killed restarted host available 

usability integration application desktop grid system transparent possible cases complexity legacy program fact source code proprietary available difficult modify code desktop grid system 
ii system anatomy physiology currently exist number academic industrial desktop grid systems harvest idle cycles desktop pc internet environments enterprise environments 
describe systems achieve fail achieve design goals described previous section 
systems share features architec tural design organization give overview anatomy physiology current systems identifying commonalities important differences client application resource management worker levels see ii reflects logical organization various components desktop grid system 
note physical organization may different shown ii 
example components client level reside host worker 
client level user submits application desktop grid tools controlling application execution monitoring status 
application resource management level application scheduled workers information applications workers stored 
worker level worker ensures application task executes transparently respect user processes hosts 
overview give procedural outline submission exe cution desktop grid application noting action fits parentheses respect ii 
user application submit authenticates desktop grid server client level application resource management level 
optional step application input data database protein sequences partitioned units organized batches tasks 
client level 
task batches generated client manager application sent application manager 
application submitted client manager control monitor application 
client level application resource management level 
application manager assigns application scheduler completion 
application resource management level 
scheduler assigns workers application worker constraints scheduling heuristic 
application resource management level worker level 
available worker computes task returns result scheduler relays application manager application completed 
worker level application resource management level 
application manager tallies results returns application client manager post processing necessary 
client level detail various components level shown ii involved procedure application submission management exe cution 
relevant inject discussion details particular systems entropia united devices xtremweb cur rently large projects incorporate hundreds thousands resources :10.1.1.12.8273
entropia united devices commercial companies offer desktop grid software professionally developed tested supported 
companies separate products tailored enterprise internet environments 
discussion ref erences entropia united devices frameworks refer software designed internet environments 
xtremweb open source internet desktop grid frameworks 
xtremweb system academic project developed university paris sud hundreds machines projects 
system deployed hundreds hosts currently support seti home project large projects 
ii client level order user submit application desktop grid system user register application binary application manager sending executable specifying access permissions 
application input data client level application resource management level worker level application client manager application manager scheduler worker daemon worker application sandbox database ii common anatomy desktop grid systems stored database flat files example partitioned formatted tasks 
systems xtremweb entropia united devices nimrod provide tools packaged part client manager creating tasks range set parameters 
client manager provides command line interface user submit tasks application manager 
option offered entropia united devices systems application manager api submit tasks programmatically 
application submitted client manager monitor progress application control execution 
systems entropia xtremweb provide functionality client manager web browser 
ii application resource management level application binary submitted application manager creates corresponding entry application table relational database record path corresponding binary permissions accessing information application constraints resources application tasks scheduled minimum cpu speed memory size 
set tasks submitted ap plication manager creates entry task table database record application task corresponds paths corresponding input files server 
application manager responsible supplying tasks application scheduler resource selection binding 
scheduler receives request worker scheduling decision information cpu speed memory size disk space network speed worker stored worker table database resource constraints application 
scheduler packages application binary data inputs sends inputs back worker response 
schedulers current systems assign tasks resources come server fcfs order tailored high throughput jobs 
entropia system uses multi level priority queue task assignment 
schedulers passive sense push tasks workers scheduler wait worker connection server able assign task 
due fact hosts internet including enterprises protected firewalls block incoming connections firewalls usually allow kind outgoing connections connection worker server initiated worker 
worker successfully completes task returns task application manager records completion results database table storing time completion user completed task credits user host completed task 
ii worker level ii worker daemon host worker daemon runs background control communi cation server task execution host monitoring machine activity 
worker particular recruitment policy determine task execute task suspended terminated 
recruitment policy consists cpu threshold suspension time waiting time 
cpu threshold percentage total cpu determining machine con sidered idle 
example condor machine considered idle current cpu cpu threshold default 
suspension time refers duration task suspended host non idle 
typical value suspension time minutes 
host non idle suspension time expires task terminated 
busy host available worker waits fixed period time quiescence starting task period time called waiting time 
condor default waiting time minutes 
ii shows example effect recruitment policy cpu avail ability 
task initially uses cpu 
user key board mouse activity task gets suspended 
various causes task termination enforce unobtrusiveness include user level activity mouse keyboard activity cpu processes disk accesses machine failures reboot shut crash 
activity suspension time expires task resumes execution completes 
worker uploads result server downloads new task time indicated interval labelled gap 
task begins execution gets suspended eventually killed due user activity usually task progress lost systems system level sup port checkpointing 
host available task execution waiting time expires task restarts shortly execution host loaded processes cpu utilization threshold task continues executing receiving slice cpu time 
addition controlling execution desktop grid application worker daemons xtremweb entropia periodically poll server indicate current state worker example running task waiting machine idle host worker 
task assigned worker worker stops sending heartbeats server worker assumed failed task reassigned worker 
ii worker sandbox ii cpu availability task execution 
ensure protection underlying host task executing systems provide form sandboxed environment 
particular entropia provides virtual machine sandbox guards machine errant worker processes 
virtual machine user level program simulates windows kernel worker application runs thread virtual machine 
application runs system call virtual machine catches system call presumably call analogous ptrace linux executes simulated environment 
virtual machine configured map application virtual file accesses file accesses actual machine 
example applications changes windows registry potentially obtrusive host 
entropia virtual machine shadow registry installation directory writes preventing modifications actual registry 
benefits virtual machine 
virtual machine enables fine grain control network memory disk computing resources application order limit 
entropia virtual machine simplifies application integration desktop grid system allowing propri windows executable run worker changes legacy source code recompiling link special libraries 
entropia xtremweb research group investigated user level sandbox intercepts system calls application intercepted system call runs security check ensure call valid allowing execution 
specifically xtremweb deploys method ptrace allow parent process retain control child specific operations executed system calls 
child process system call execution paused parent process inspect parameters call allowing execution 
child system call fails parent check parent kill child process 
drawback sandboxing techniques overhead context switches child parent processes 
applications significant io perform poorly systems 
alternatively xtremweb group considered kernel level sandbox technique kernel patch installed adds hooks particular kernel functions 
superuser insert module implements hooks define specific security policy 
advantage method context switches necessary 
xtremweb considered technique required root privileges method implemented 
protect host machine misbehaving application workers security mechanisms protect application including data user 
deter inspection application executable data encrypted multiple keys examination difficult modifications detectable 
ii design trade offs centralization spectrum desktop grid completely centralized client manager components application resource manage ment layer located single machine 
host desktop grid completely autonomous little knowledge hosts ap plications system 
desktop grid systems centralized identify trade offs centralized versus decentralized design respect system goals outlined section ii focusing particularly scalability server fault tolerance task result verification worker software manageability 
ii scalability focus aspects resource management application data management 
important parts resource management monitoring re sources determine dynamic information cpu network activity resource selection 
systems nws hierarchical approach amenable incorporation desktop grid systems allow scalable monitoring resources 
regarding resource selection exist centralized systems 
example system execute expressive resource queries including ranking clustering large set attributes millions resources order sec modest machine 
particular implementation relational database store hierarchical structure set resources xml database improve performance 
authors showed decentralized resource management specifically monitoring selection advantageous performance wise authors strategically placing node server clusters support resource discovery results performance comparable decentralized approaches distributed hash tables dht 
terms ease implementa tion experience suggests resource selection greatly simplified global view resources system lost fully decentralized system 
time efforts decentralize resource monitoring discovery achieve scalability fault tolerance sword guard 
general approach sword dht distributing data resources related queries set hosts 
example store data host cpu availability host may store values host may store values 
queries form attribute value mapped unique keys routed host containing corresponding data 
advantage approach tolerate host failures dht automatically restructure needed 
approach guard create gossiping protocol distance vectors resource information propagates automatically node neighbors 
protocol designed scalable withstand host failures 
benefits decentralized resource management relative centralized management debatable limiting aspects centralized design application data management particular storage distribution 
authors show application medium input sizes mb low execution times minutes requires significant bandwidth mbps medium number workers 
applications higher data input sizes distributing data inputs centralized server infeasible mandates decentralized approaches peer peer methods described 
example chord protocol provides fast method locate datum stored set volatile hosts wide area 
particular chord distributed hash table primitive supports data lookups log messages number hosts system 
hosts dht organized logical overlay maps unique id corresponding host position overlay 
node contains routing table indicates neighbors closer datum 
datum unique identifier mapped closest node logical overlay 
methods locating datum set volatile hosts linking computation data addressing issues locality open problem 
ii fault tolerance centralization cause server single point failure 
avoid failure argue replicating server components application resource management level reduce probability failure significantly 
example seti home server including web data servers nonfunctional times 
causes failure included hardware upgrades updates database database software raid cards failing electrical storms repairs power outages full disks database failures rearranging hardware 
assuming server fails rate times year server replicated servers independent failure rates probability servers fail approximately decades 
point setting extra servers server farm versus totally decentralized solution systems xtremweb support reduce chance failure near zero 
failure occur effect small internet desktop grid applications high throughput outage hours significant applications stringent time requirements 
systems xtremweb mechanisms graceful recovery 
example data server fails workers finish computing tasks server comes back worker upload requests 
reduce storm requests xtremweb force exponential backoff workers server overloaded 
regarding task result verification result returned worker erroneous 
particular authors significant computation errors differences caused hardware malfunctions incorrect software modifications malicious attacks differences floating point hardware libraries compilers 
error rates scientific application charmm deployed inter net desktop grid respectively 
task replication means detection correction 
multiple copies unit sent different workers 
results returned compared result appears computed credible worker assumed correct 
worker computed bad result prevent worker effecting application 
blacklisting worker centralized system trivial fully decentralized system require notification node hosting components application resource management level order prevent participating computation 
regarding manageability entropia xtremweb united devices provide form command line tools web interfaces single administrator manage applications monitor progress install workers send updates 
effectively manage applications decentralized environment open area research 
summary certainly potential beneficial centralization significant challenges overcome costs decentralization outweigh benefits 
currently research efforts developing decentralized desktop grid system cluster computing fly system organic grid 
authors propose cluster computing fly system uses distributed hash table techniques locating available resources 
authors describe prototype organic grid fully decentralized system mobile agents 
chapter iii resource characterization measurement characterization desktop grids useful reasons 
data performance evaluation entire system subsets individual hosts 
example determine aggregate compute power entire desktop grid time 
second data develop pre generative explanatory models 
example predictive model formed predict availability host available pe time 
generative model data generate host clock rate distribution availability intervals simulation platform 
showing precise fit data model model help explain certain trends shown data 
third measurements drive simulation experiments 
fourth characterization influence design decisions resource management heuristics 
discuss chapter measurement tech nique obtaining traces real desktop grids statistical characterization system individual hosts 
iii ideal resource trace design evaluation scheduling heuristics requires accurate characterization desktop grid system 
accurate characterization involves obtain ing detailed availability traces underlying resources 
term availability different meanings different contexts clearly defined problem hand :10.1.1.13.1523
characterization data sets distinguished types availability 
host availability 
binary value indicates host reachable corresponds definition availability :10.1.1.160.7346:10.1.1.22.4887:10.1.1.11.4677
causes host unavailability include power failure machine shutoff reboot crash 

task execution availability 
binary value indicates task execute host worker recruitment policy 
refer task execution availability exec availability short 
causes exec unavailability include prolonged user keyboard mouse activity user compute bound process 

cpu availability 
percentage value quantifies fraction cpu exploited desktop grid application corresponds definition 
factors affect cpu availability include system user level compute intensive processes 
host unavailability implies exec unavailability implies cpu ity 
clearly host unavailable due shutdown machine new task may execution executing task fail 
period task execution unavailability due keyboard mouse activity desktop grid worker execution task causing fail disallow task execution result task execution unavailability task observe zero cpu availability 
cpu unavailability imply exec unavailability 
example task suspended zero cpu availability task resume continue execution host available terms task execution 
similarly exec unavailability imply host unavailability 
example task terminate due user mouse keyboard activity host 
definitions availability ideal trace availability characteristics 
trace log cpu availability terms cpu time real application receive executing host 

trace record exec availability particular failures occur 
find temporal structure availability intervals 
call inter val time consecutive periods exec unavailability availability interval 

trace determine cause failures mouse user activity machine reboot crash 
enable statistical modeling purpose prediction example particular type failure 

trace capture system overheads 
example desktop grid workers run virtual machines may overheads terms start system calls memory costs 
iii related resource measurements mod plethora done measurement char host cpu availability main deficiencies re lated research 
traces capture causes task failures users mouse keyboard activity inferring task failures temporal characteristics availability traces difficult 
second traces may reflect idiosyncrasies os showing true cpu contention running task 
section highlight trace methods studies explain statistical models founded trace data inapplicable desktop grids 
table iii summarizes methods representative studies 
iii host availability traces obtained log host availability time 
authors designed sensor periodically records machine uptime proc file system sensor monitor machines student lab 
authors periodically rpc calls rpc runs part network file system nfs hosts connected internet see row corresponding data set long table iii 
response call indicated host missing response indicated failure 
prober runs overnet peer peer file sharing system looking host id machine corresponding id available responds probe machines monitored fashion :10.1.1.13.1523
authors determine availability periodically probing ip addresses gnutella system :10.1.1.160.7346:10.1.1.11.4677
traces record host availability purpose modeling desktop grids problematic hard relate cpu cycles usable desktop grid application 
factors affect application running time desktop grid include host availability cpu load user activity 
traces indicate uptime dubious performance modeling desktop grids driving simulations 
iii host load cpu utilization numerous data sets containing traces host load cpu utilization groups workstations 
host load usually measured moving average number processes ready queue maintained operating system scheduler cpu utilization measured cpu time clock cycles time interval received process 
host load correlated cpu utilization discuss type studies section 
cpu availability traces described obtained unix tools ps scan proc filesystem monitor processes 
particular author ps measure cpu availability workstations period months monitored workstations period months see row corresponding data set condor ta ble iii 
post processed data determine machine unavailability intervals 
host considered unavailable cpu utilization user processes went 
assumed waiting time minute month period traces minutes second month trace period 
similarly authors measured host load periodically exponential moving average number processes ready queue recorded kernel see row corresponding data set dinda table iii 
study machines week period august 
contrast previous studies unix systems measured cpu availability windows nt machines days periodically inspecting windows performance counters determine fractions cycles processes idle process 
trace data sets mentioned record various events cause application task failures keyboard mouse activity data sets immune os idiosyncrasies 
example unix process schedulers linux kernel particular give long running processes low priority 
long running process running cpu sensor determine cpu completely unavailable 
desktop grid task running cpu task received sizable chunk cpu time 
furthermore processes may fixed low priority 
cause task failures inferred data doing trivial may accurate 
iii process lifetimes authors conduct empirical study process lifetimes pro pose function fits measured distribution lifetimes 
model determine process migrated 
inferring temporal struc ture availability model process lifetimes difficult clear determine starting point process time relationship 
study monitor keyboard mouse activity significantly impacts availability intervals addition cpu load 
waiting suspend host exec true thresh time time cpu old years ity ity old 
ity 
characteristics cpu method user base hosts data set os trace dates mix rpc calls hosts long unix month internet rpc frontend interactive batch hosts compute servers desktops hosts moving load average week dinda digital unix cluster faculty min min load average ps workstations months total condor bsd unix system programmers graduate students secretaries conference rooms administrations research staff cluster students min min real measurement tasks hosts sdsc windows month total sec real measurement hosts xtremweb linux month tasks min deriv 
ee cs grad students user level daemon dec workstations days ucb ultrix iii trace method gather traces submitting measurement tasks desktop grid system perceived executed real tasks 
tasks perform computation periodically write computation rates file 
method requires desktop grid application running allows measure exactly compute power real compute bound application able exploit 
measurement technique differs previously methods measurement tasks consume cpu cycles real application 
measurement period keep desktop grid system fully loaded requests cpu bound fixed time length tasks minutes length 
desktop grid worker running host ensured tasks interfere desktop user tasks suspended terminated necessary resource owners unaware measurement activities 
task fixed time length consists infinite loop performs mix integer floating point operations 
dedicated ghz pentium processor perform operations second 
seconds task evaluates able achieve seconds writes measurement file 
files retrieved desktop grid system assembled construct time series cpu availability terms number operations available desktop grid application second interval 
windows version measurement task implement timing task window multimedia timer 
timer implemented spawning high priority thread sets kernel timer blocks 
thread wakes executes callback routine posting number operations completed seconds windows application message queue sleeps 
note message queue sufficiently large preclude queue minutes measurements task 
frequency kernel interrupts check timer expiration set initializing timer 
tried various fre millisecond second noticed little difference total operations logged time period 
timer resolution ms assuming overhead timer negligible 
regardless overhead timer constant time intervals number operations time interval equally affected 
implement computational aspect task iteratively performing integer floating calculations integer array double array 
intra loop dependencies added prevent compiler optimization 
size array bytes respectively small fit cache excluded costs memory accesses traces 
linux version measurement task implemented similar manner 
main advantage obtaining traces fashion application experiences host cpu availability exactly real desktop grid application 
method susceptible os idiosyncrasies logging done cpu bound task running host 
approach captures various causes task failures including limited mouse keyboard activity operating system failures hardware failures resulting trace reflects temporal structure availability intervals caused failures 
method takes account overhead limitations policies accessing resources desktop grid infrastructure 
measurement method weaknesses method certainly 
weakness compared ideal trace data set identify specific causes failures distinguish failures caused user activity versus power failures example 
turn stochastic failure prediction models difficult derive source failure skew distribution source 
types failures subsumed traces contrast studies omit types desktop failures described earlier sections 
tasks executed means desktop grid worker particular recruitment policy 
means trace may biased particular worker settings specific deployment 
knowledge settings straightforward infer reliably points trace bias occurs possible remove bias 
removing bias post process traces cpu recruitment policy determine corresponding cpu availability 
possible collect data trace method desktop grid recruitment policy simulate desktop grid different recruitment policy set traces minor adjustments 
iii trace data sets previously described method collected data sets desktop grids 
desktop grids consisted desktop pc san diego super computer center sdsc ran commercial entropia desktop grid software :10.1.1.12.8273
refer data collected sdsc environment sdsc trace 
desktop grid consisted desktop pc university paris south ran open source xtremweb desktop grid software 
xtremweb desktop grid incorporated machines different environments 
environment cluster computer science research group running parallel applications benchmarks refer data set collected cluster lri trace 
second environment consisted desktop pc classrooms year undergraduates refer data set deug trace 
obtained traces described measured different trace method refer data set ucb trace 
describe method section iii advantages data sets versus highlighted table iii labelled data sets sdsc xtremweb ucb table 
traces obtained measurements contain gaps 
expected desktop resources unavailable variety reasons rebooting powering hosts local processes cpu desktop grid worker detecting mouse keyboard activity user actively pausing worker 
observe large fraction gaps clustered minute range 
figures iii iii plot distribution small gaps entropia desktop grid sdsc xtremweb desktop grid university paris sud respectively 
average small gap length seconds entropia grid seconds xtremweb grid 
number gaps gap length sec sdsc number gaps gap length sec xtremweb iii distribution small gaps min 
careful examination traces short gaps occur exclusively termination task new task host 
conclude small gaps correspond actual exec unavailability due delay desktop grid system starting new task 
sdsc grid majority gaps spread approximate range seconds see iii 
sources overhead include various system costs receiving scheduling sending task actual built limitation prevents system sending tasks resources quickly 
entropia server enforces delay time receives request worker time sends task worker 
limit damaging effect black hole problem worker correctly execute tasks repeatedly frequently sends requests tasks server 
artificial task sending delay result black hole problem applications thousands tasks completed instantly erroneously 
xtremweb grid majority gaps seconds seconds length 
xtremweb worker available execute task sends request server 
server busy task execute worker told request certain period time seconds expired 
explains bimodal distribution gaps length xtremweb system 
small availability gaps observed entropia xtremweb grids experienced tasks real application tasks 
consequently eliminated gaps minutes traces performing linear interpolation 
specifically interpolate gaps minutes length method 
number operations measured subinterval length ends just gap begins 
number operations measured subinterval length begins just gap ends 
seconds length measurements usually seconds 
gap length interpolated number operations available gap 
calculate weighted average rate operations completed immediately gap rate operations longer subintervals carries weight interpolation usually subintervals immediately preceding gap seconds length interpolated rate just average rates gap 
small portion gaps larger minutes may attributed server delay means post processed traces may slightly optimistic 
note interpolation average small gap length performance models describe section iii account server delay 
real application gaps may larger due transfer input data files necessary task execution 
transfer cost added average small gap length easily included performance model developed section iii weakness interpolating relatively small gaps effect masks short failures minutes length 
failures due fast machine reboots example overlooked interpolation method 
iii sdsc trace data set collected entropia tm desktop grid software system deployed sdsc cumulative period month hosts 
conducted measurements distinct time periods total approximatively days measurements 
characterization simulation experiments longest continuous period trace measurements week period 
hosts secretaries public hosts avail able sdsc conference rooms system administrators remain ing sdsc staff scientists researchers 
hosts class network clients having mbit sec connection having mbit sec connection 
hosts desktop resources run different flavors windows tm entropia server running dual processor xeon mhz machine gb ram 
validate measurements monitoring application sdsc grid isolated small set machines accessed system counters deter mine cpu utilization process monitoring task running host controlled entropia worker daemon 
particular continuously sending monitoring tasks entropia system windows management instrumentation wmi remotely access system counters machines sdsc determine clock ticks devoted process running host 
limited set machines accessed hours september needed superuser privileges wmi 
note method moni machines went network record machine system counter readings measurements delayed due network congestion 
compared wmi measurements task measurements availability non availability intervals recorded monitoring tasks corresponded times task appeared list processes wmi 
cpu avail ability measured monitoring task closely matched cpu utilization measured wmi queries 
experiments hosts effectively running entropia worker hosts users presumably disabled worker obtained measurements hosts 
clock rates ranged mhz ghz average ghz 
iii shows cumulative distribution function cdf clock rates 
curve continuous instance host clock rate ghz ghz 
curve skewed instance hosts clock rates mhz mhz represents clock rate range 
interesting feature entropia system virtual machine vm insulate application tasks resources 
vm technology critical security protection issues possible fine grain control executing task terms resources uses limiting cpu memory disk usage restricting threads processes design principle application host resources possible interfering local processes 
benefit allows application cpus possible values 
cpu availability measure 
note measurements easily post processed evaluate desktop grid system allows application tasks run host say available cpu 
weakness measurement method sdsc data set resolution traces limited length task 
entropia task terminated task output lost result trace data lost time 
consequently unavailability intervals observed data set pessimistic task length statistical analysis may suffer periodicity 
believe data set modelling simulation cross validate findings data sets limitation measurement method removed 
weakness interpolation gaps may hidden short failures reboots 
sdsc system administrations recorded reboots applying windows patches entire month trace period 
particular server reboot pm reboot desktops pm reboot desktops am possible reboot desktops user prompted server reboot pm reboot machines pm reboot machines simultaneous 
lower bound believe hosts rebooted infrequently usually single desktop allocated user sense desktops dedicated systems 
phenomenon described undergraduates sitting desktop rebooting machines clean system remote users causing high load occur 
entropia system sdsc shared users take measurements days time 
traces consecutive days versus weeks months keep track long term monthly churn machines lose long term temporal structure availability 
fraction sdsc deug lri ucb clock rate mhz iii host clock rate distribution platform iii deug lri traces second data set collected xtremweb desktop grid software continuously month period total hosts university paris sud 
particular xtremweb deployed cluster denoted lri total hosts classroom denoted deug hosts respectively 
lri cluster xtremweb research group researchers performance evaluation running scientific applications 
deug classroom hosts year students 
typically classroom hosts turned 
classroom hosts turned weekends class weekends 
xtremweb worker modified keep output running task failed return partial output server 
removes bias due failure fixed sized task period availability logged task identical observed real desktop grid application 
compared clock distribution hosts sdsc platform hosts deug lri platforms relatively homogeneous clock rates see iii 
large mode clock rate distribution deug platform occurs ghz median hosts clock rates ghz 
clock rates deug platform range ghz ghz 
lri platform mode clock rate distribution occurs ghz median hosts clock rates speed 
range clock rates ghz ghz 
iii ucb trace obtained older data set reported different measurement method 
traces collected daemon logged cpu keyboard mouse activity seconds day period hosts 
hosts graduate students ee cs department uc berkeley 
largest continuously measured period 
traces post processed reflect availability hosts desktop grid application desktop grid settings 
host considered available task execution cpu average past minute keyboard mouse activity time 
recruitment period minute busy host considered available minute activity 
task suspension disabled task running immediately fail indication user activity 
clock rates hosts ucb platform identical extremely slow speeds 
order traces usable simulations experiments transform clock rates hosts clock rate ghz see iii modest reasonable value relative clock rates platforms close clock rate host lri platform 
reason ucb data set usable desktop grid characterization measurement method took account primary factors affecting cpu availability keyboard mouse activity cpu availability 
mentioned previously method determining cpu availability may accurate application level method submitting real tasks desktop grid system 
desktop grid settings relatively strict host considered busy cpu believe result post processing accurate 
weakness data set years old host usage patterns changed time 
data set show fact characteristics desktop grids remained constant years 
note ucb trace previously existing data set tracked user mouse keyboard activity usable desktop grid simulations 
iii characterization exec availability section characterize detail exec availability discussion term availability denote exec availability noted 
report discuss aggregate statistics hosts platform relevant describe host statistics 
iii number hosts available time observed total number hosts available time determine times week day machines volatile 
useful determining periods interest testing various scheduling heuristics 
figures iii iii iii iii show number available hosts week period sdsc deug lri ucb traces respectively 
date shown corresponds sunday series dates proceeds saturday 
date shown corresponds am particular day 
number hosts axis represents number hosts single availability interval hour range sdsc deug lri platforms minute range ucb platform unavailability intervals platform tended smaller rest platforms 
exception lri trace observe diurnal cycle volatility general weekday business hours 
business hours variance number machines time relatively high non business hours number relatively stable 
case ucb sdsc trace number machines usually decreases business hours deug trace number machines increase decrease 
difference trends explained culturally 
machines enterprise environments tend powered day fluctuation number hosts usually downward fluctuations 
contrast europe machines powered business hours save power reduce fire hazards example result fluctuations upward 
students staff scientists form majority user base sdsc ucb believe cause volatility business hours primarily keyboard mouse activity user keyboard short compilations programming code long computations run clusters supercomputers sdsc ucb 
supported observations similar cpu availability studies 
deug platform volatility due machines powered addition interactive desktop activity 
number hosts lri trace see iii follow diurnal cycle 
trend explained user base cluster computer science researchers submit long running batch jobs cluster 
result hosts tend unavailable groups time reflected large drop host number 
little interactive cluster total number hosts available total number hosts available jan sep jan sep jan sep jan sep sep time sdsc jan jan time deug sep jan sep jan sep iii number hosts available week platform 
total number hosts available total number hosts available jan jan feb jan feb jan feb jan jan time lri mar mar time ucb jan mar iii jan mar mar number hosts available week platform cont 
cumulative percentage number hosts time remains relatively constant 
lri cluster exception handful nodes possibly front nodes turned weekends explains drop sunday saturday 
refer daily time period set hosts volatile business hours 
close examination number hosts time determine times delimiting business hours sdsc deug ucb platforms am pm am pm am pm respectively 
regarding lri platform distinction non business hours business hours 
iii temporal structure availability successful completion task directly related size ity intervals intervals consecutive periods unavailability 
show distributions various types availability intervals platform characterize volatility 
sdsc mean deug mean sdsc lri mean deug ucb mean lri ucb interval length hours business hours 
cumulative percentage sdsc mean deug mean ucb mean sdsc deug ucb interval length hours non business hours 
iii cumulative distribution length availability intervals terms time business hours non business hours 
iii show length availability intervals terms hours hosts platform 
figures iii iii show intervals business hours non business hours respectively business hours platform defined section iii 
case host available continuously entire business hour non business hour period truncate intervals respective period 
plot availability intervals lri non business hours weekend machines turned 
comparing interval lengths business hours non business hours observe lengths tend longer weekends times longer 
business hours observe ucb platform tends shortest lengths availability minutes deug sdsc relatively longer lengths availability half hour hours respectively 
lri platform far exhibits longest lengths hours 
ucb platform shortest length cpu thresh old relatively low 
authors observed system daemons cause load potentially increase frequency availability interruptions 
addition ucb platform interactively students keyboard mouse activity cause momentary short bursts cpu activity 
hosts deug sdsc platforms interactively relatively short 
surmise long intervals lri platform result cluster workload consists periods high activity followed low activity 
cluster nodes tend available longer periods time 
summary lower cpu threshold shorter availability intervals 
availability intervals tend shorter interactive environments intervals tend longer business hours non business hours 
figures iii iii cdf corresponding ucb trace business hours appear quite similar cdf non business hours number hosts shown iii varies considerably business hours versus non business hours 
reason discrepancy cdf weight distribution total sum availability 
example consider data sets element availability length time cumulative percentage business hours non business hours interval length hours iii cumulative distribution length availability intervals normalized total duration availability terms time business hours non business hours ucb platform 
units sets 
cdf range appear quite similar fact platform derived considerably stable availability interval time units 
iii show cumulative distribution interval length weighted total sum availability 
see larger portion availability intervals business hours smaller comparison intervals non business hours 
data interesting applications require hosts reachable period time content distribution confirm extend relevant problem scheduling compute intensive tasks 
perspective compute bound application ghz host available hours average cpu availability attractive say ghz host available hour average cpu availability 
contrast iii plots cumulative distribution availability intervals business hours non business hours terms number operations performed 
showing availability interval durations axis shows number operations performed interval cumulative percentage computed measured cpu availability 
quantifies directly performance application factoring heterogeneity hosts 
major trends data expected hosts cpus frequently available business hours non business hours 
empirical data enables quantify task failure rates develop performance model describe section iii 
sdsc mean ops deug mean ops lri mean ops ucb mean ops sdsc deug lri ucb interval length number operations interval length business hours 
cumulative percentage sdsc mean ops deug mean ops ucb mean ops sdsc deug ucb interval length number operations interval length non business hours 
iii cumulative distribution length availability intervals terms operations business hours non business hours 
iii temporal structure unavailability scheduling application useful know long host typically unavailable unable execute task 
hosts identical avail ability interval lengths prefer host smaller unavailability intervals 
availability unavailability interval data predict host high chance completing task certain time example 
iii shows cdf length unavailability intervals terms hours business hours non business hours platform 
note platform may exhibit heavy tailed distribution necessarily mean platform generally available 
describe cpu availability section iii cumulative observe distinct trends platform 
sdsc platform notice unavailability intervals longer business hours non business hours 
explained fact weekends patches installed backups done short unavailability intervals result done batch 
second deug platform unavailability intervals tend shorter business hours min versus non business hours hours 
explanation machines turned night weekends resulting long periods unavailability 
fact non business hours deug unavailability intervals half hour length explained machines interactively non business hours 
third lri platform notice unavailability intervals hour length 
due fact jobs submitted clusters mpp tend quite short length 
lastly ucb platform cdf unavailability intervals appears similar 
believe platform user base students number machines business hours versus non business hours pattern students machines keyboard mouse running short processes resulting nearly identical distributions 
sdsc mean lri mean sdsc deug mean deug ucb mean lri ucb interval length hours cumulative sdsc mean deug mean ucb mean sdsc deug ucb interval length hours iii unavailability intervals terms hours fraction tasks failed iii task failure rates sdsc deug lri ucb sdsc deug lri ucb task size minutes ghz machine aggregate cumulative fraction sdsc deug lri ucb failure rate host minute tasks iii task failure rates business hours characterization temporal structure resource availability possible derive expected task failure rate probability host unavailable task completes distribution number operations performed periods unavailability data shown iii random incidence 
calculate failure rate choose hundreds thousands random points periods exec availability traces 
point determine task size number operations run completion host available task execution 
count trial success count trial failure 
count tasks started periods exec unavailability assume worker case connected scheduler scheduling task point possible 
iii shows expected task failure rates computed business hours platform 
squares line platform superimposed dotted magenta line correlation coefficient shown 
illustration purposes axis shows task sizes number operations execution time dedicated ghz host minutes hours 
maximum task size minutes chosen significant number task executions simulated business hours 
expected task failure rate strongly dependent task lengths 
weekends show similar linear trends albeit failure rates lower 
platforms failure rates lowest highest lri sdsc deug ucb agrees ordering platform shown iii 
appears platforms task failure rate increases task size increase linear lowest correlation coefficient indicating exists strong linear relationship task size failure rate 
squares fit define closed form model aggregate performance attainable high throughput application corresponding desktop grid see section iii 
note task failure rate larger tasks eventually plateau approaches 
iii shows aggregate task failure rate system fig ure iii shows cumulative distribution failure rate host platform particular task size minutes dedicated ghz host 
heavier tail volatile hosts platform 
distributions appears quite skewed 
majority hosts relatively stable 
example deug platform hosts failure rates 
ucb platform skewed hosts failure rates 
fact hosts relatively low failure rates affect scheduling tremendously discuss effect chapter surprisingly iii sdsc lower task failure rates ucb iii sdsc larger fraction hosts failure rates compared ucb 
discrepancy explained fact ucb larger fraction hosts failure rates sdsc averaging sdsc lower failure rates 
iii correlation availability hosts assumption fault tolerance research large scale systems resource failure rates modelled independent identical probability distributions 
number analytical studies desktop grids assume exec availability simplify probability calculations 
validity assumptions respect exec availability desktop grid systems 
studied independence exec availability hosts ing correlation exec availability pairs hosts 
specifically compared availability pair hosts adding machines available machines unavailable subtracting host available 
method authors study correlation availability thousands hosts microsoft 
iii shows cumulative fraction hosts pairs partic ular correlation coefficient 
line labelled trace legend indicates correlation calculated traces 
line labelled min indicates minimum pos sible correlation percent time machine available likewise line labelled max 
iii corresponding deug platform point means possible host pairing percent time host availability matched time hosts availability mis matched 
difference points means fraction host pairings matching possible hosts 
difference points indicates host pairings mismatching availability 
point means host pairings availability mismatched time matched 
see platforms host pairings positive correlation indicates host available unavailable host available unavailable respectively 
figures iii iii show correlation due fact hosts usually available combined fact time hosts cpu availability higher reflected closely trace line follows random line 
host available unavailable time available unavailable randomly 
result correlation observed traces fact occur randomly hosts high availability 
turn gives strong evidence completely sufficient exec availabilities hosts sdsc ucb platforms independent 
believe high likelihood independence exec availability sdsc deug traces primarily due user base hosts 
mentioned previously user base sdsc consisted primarily research scientists administrators believe windows hosts primarily word processing surfing inter net tasks directly affect availability hosts 
similarly ucb platform believe students host primarily short tions evident short availability intervals 
primary factors causing host unavailability user processes keyboard mouse activity independent machine desktop environments observe exec availability desktop grids significantly correlated 
hand figures iii iii show different trends line corresponding trace differs significantly respect correlation line corresponding random correlation 
weak correlation deug trace due particular configuration classroom machines 
machines wake lan enabled ethernet adapters allowed machines turned remotely 
system administrators configured machines wake hour turned user 
machines turned machines awakened time resulting weak correlation availability 
believe wake lan configuration specific configuration machines deug general machine availability independent desktop grids keyboard mouse activity high 
hosts lri platform shows significant correlation relative random 
behavior expected batch jobs submitted cluster tend consume large number nodes simultaneously consequently nodes unavailable desktop grid task execution times 
independence result supported host availability study performed microsoft reported 
study authors monitored fraction host pairings fraction host pairings min trace random max correlation coefficient sdsc min trace random max correlation coefficient lri fraction host pairings fraction host pairings min trace random max correlation coefficient min trace random max iii correlation availability 
deug correlation coefficient ucb desktops machines week period correlation host availability matched random correlation 
host unavailability implies exec unavailability measurements subsume host unavailability addition primary factors cause exec unavailability cpu load keyboard mouse activity 
show exec availability hosts significantly correlated effects cpu load keyboard mouse activity addition host availability 
difference study microsoft study mi study analyzed correlation machines separate desktop machines departmental groups hosts administrators software development groups management example 
possible correlation group hidden 
contrast groups host analyzed relatively homogeneous user base result stronger sense rule possibility correlation hosts heterogeneous user bases 
studies host availability fact shown correlation host fail ures due power outages network switches 
failures clearly affect application execution believe types failures dominant cause task failures desktop grid applications 
common cause task failures due high cpu load keyboard mouse activity study directly takes account factors affect exec availability contrast studies 
host availability may correlated correlation significantly weakened major causes exec unavailability 
evidence host independence simplify reliability analysis platforms 
particular result simplify calculation probability multiple host failures describe chapter vi 
iii correlation availability host clock rates hypothesized host clock rate indicator host performance 
intuitively host speed correlated number machine characteristics 
example faster host clock rate faster complete task lower failure rate 
faster host clock rate clock rate mean availability interval length time log clock rate mean availability interval length time clock rate time unavailable log clock rate time unavailable clock rate mean availability interval length ops log clock rate mean availability interval length ops clock rate task failure rate minute task log clock rate task failure rate minute task clock rate complete min task min log clock rate complete min task min table iii correlation host clock rate machine characteristics business hours sdsc trace user particular host available host 
surprisingly host speed correlated factors believed 
table iii shows correlation clock rate various measures host availability sdsc trace 
clock rates platforms roughly uniform calculate correlation 
clock rates increase exponentially time compute correlation log clock rates factors 
compute correlation clock rate mean time availability interval capture relationship clock rate temporal structure availability case host small availability intervals available time 
compute correlation clock rate percent time host unavailable find little correlation clock rate mean length cpu availability intervals terms time see rows table iii percent time host unavailable see rows 
explain fact desktops time intermittent brief tasks example word processing machines relatively low clock rates high unavailability example due frequent mouse keyboard activity results availability similar faster hosts 
majority desktops distributed office rooms 
desktop users choice choosing faster desktop 
task size failure rate table iii correlation host clock rate failure rate business hours 
task size term minutes dedicated ghz host 
matters application time interval opera tions interval affects task failure rate related clock rate host 
compute correlation clock rate cpu avail ability terms mean operations interval failure rate minute task 
weak positive correlation clock rate mean number operations interval see rows weak negative correlation clock rate failure rate see rows 
possibility strong correlation weakened randomness user activity 
factors independent clock rate hosts faster clock rates tend operations availability interval increasing chance task complete interval 
furthermore rows table iii see relationship clock rate rate successful task completion certain amount time 
particular rows show fairly strong positive correlation clock rate probability task completes minutes 
size task minutes executed dedicated ghz 
computed correlation task sizes similar correlation coefficients 
clearly task completes certain amount time related clock rate 
relationship slightly weakened due randomness exec unavailability unavailability cause task executing relatively fast host fail 
implication correlation shown rows scheduling heuristic host clock rates task assignment may effective 
correlation host clock rate task failure rate increase task size hosts high failure rates close 
short tasks low mean failure rate near zero hosts naturally low correlation 
task size increases failure rate correlated clock rates general faster hosts able finish tasks sooner 
table iii shows correlation coefficient clock rate failure rate different task sizes 
weak negative correlation host speed task failure rate increases general task size expected 
believe weak correlation partly due randomness keyboard mouse activity machine 
consequence result respect scheduling larger task size important schedule tasks hosts faster clock rates 
recomputed correlation hosts failure rates greater effort remove hosts exceptionally available find significant changes correlation coefficients 
removing certain hosts correlation calculation left relatively hosts remaining clear data meaningful calculation 
iii characterization cpu availability temporal structure availability directly impacts task execution useful observe cpu availability entire system understanding temporal structure availability affects system performance cpu availability availability interval fluctuate 
aggregate cpu availability puts availability unavailability intervals perspective ing effect types intervals compute power system statistics availability intervals unavailability intervals necessarily reflect characteristics 
iii aggregate cpu availability estimate computational power number cycles de desktop grid application aggregate measure cpu availability 
time threshold sdsc deug lri ucb availability threshold business hours time threshold sdsc deug ucb availability threshold non business hours iii percentage time cpu availability threshold hosts business hours non business hours 
data point measurements hosts computed cpu availability threshold business hours non business hours platform 
figures iii iii plot frequency cpu availability threshold threshold values data point means time cpu availability 
instance graphs show cpu availability sdsc platform time business hours time non business hours 
general cpu availability tends higher business hours non business hours 
example business hours sdsc deug platforms zero cpu availability time business hours hosts platforms cpu availability greater equal zero 
business hours observe sdsc deug initial drop threshold 
believe cpu unavailability indicated drop primarily due exec unavailability cause user activity process hosts system 
causes cpu unavailability include suspension desktop grid task brief bursts cpu unavailability long cause load machine go cpu threshold 
exceptions ucb lri platforms shows drop 
ucb platform level worker cpu threshold relatively stringent resulting common brief unavailability intervals little impact cpu availability system 
reason lri plot relatively virtually constant cluster lightly loaded host cpu available time 
initial drop threshold curves remain constant respective cpu threshold reached 
artifact worker cpu threshold determine terminate task 
reason compute bound task little constant cpu availability processes users running system 
cpu usage host user goes worker cpu threshold task terminated resulting virtually constant line threshold worker cpu threshold 
note case ucb assume host cpu completely available unavailable valid worker cpu threshold 
reason curves completely constant range possibly system processes briefly cpu long increase moving average host load cause desktop grid task terminated 
threshold worker cpu threshold curves downward slope 
reason slope occurs system processes cpu running simultaneously 
aver age sdsc platform cpu completely unavailable time weekdays cases weekends 
note curves rela tively flat cpu availability denoting hosts rarely exhibit availabilities range 
studies obtained similar data aggregate cpu availability desktop grids 
characterizations possible obtain coarse estimates power desktop grid difficult related directly desktop grid application hope achieve 
particular understanding host availability patterns statistical properties duration time intervals application host characterization power host delivers time intervals key obtaining quantitative measures utility platform applications 
develop characterization chapter 
iii host cpu availability aggregate cpu availability statistics reflect availability system possible hosts available 
show cpu availability host reveal potential imbalance 
figures iii iii iii iii show cpu availability host 
vertical bar corresponds cpu availability particular host hosts sorted clock rate axis 
bar sub bars correspond percent time host cpu availability fell particular range 
figures iii iii observe heavy imbalance terms cpu unavailability 
appears cpu availability increase host clock rate slowest hosts cpu unavailability greater equal time 
iii corresponds lri platform see system relatively underutilized hosts lri cluster cpu unavailability greater 
iii corresponds ucb platform observe system underutilized hosts cpu unavailability greater time believe result system low cpu threshold 
summary cpu availability show strong correlation clock rate table iii amount unavailability strongly dependent user base worker criteria idleness 
implication result simple probabilistic models desktop grid systems described assume hosts constant frequencies unavailability constant failure rates insufficient modelling complex systems 
time availability range nodes sorted clock rate iii cpu availability host sdsc platform 
time availability range nodes sorted clock rate iii cpu availability host deug platform 
time availability range nodes sorted clock rate iii cpu availability host lri platform 
time availability range nodes sorted clock rate iii cpu availability host ucb platform 
iii example applying characterization results cluster equivalence results measurement characterization study numerous uses desktop grid modelling simulation 
section give example characterization derive performance model desktop grid system quantifying negative impact heterogeneous volatile resources system throughput 
measure impact utility cluster equivalence metric node desktop grid equivalent node dedicated cluster 
focus analysis sdsc platform deug lri ucb platforms hosts models utility metrics applicable platforms 
iii system performance model section iii task failure rate strongly correlated task size linear function task size model task failure rate 
section task failure rate function performance model desktop grid model determine grid cluster equivalence high throughput application 
particular propose model application expected rate number useful operations performed time units uniform task size number operations task expressed minutes dedicated ghz host follows 
measurements determine average overhead task scheduled resource due desktop grid server see sec tion iii 
method describe section iii compute task failure rate function task size 
estimate average com pute rate operations second host desktop grid computing average delivered operations second host availability traces average hosts 
number operations seconds application hosts desktop grid sg iii effective compute rate accounting failures sg overhead scheduling task 
instantiate model data obtained desktop grid platforms determine corresponding rate 
example compute sdsc grid hosts 
average overhead scheduling task characterized seconds sdsc platform see sec tion iii 
squares fit weekday task failure rate calculated method described section iii sdsc data 
average weekday rate account unavailability host load host operations second 
substituting equation iii obtain closed form expression sdsc platform iii iii plots rate range task sizes executed sdsc grid weekdays plots rate weekends 
weekdays task sizes minutes host progress increases rapidly task size compensates fixed overhead 
task size increases host progress decreases penalty additional task failures wastes cpu cycles 
trend exhibited weekends longer availability intervals enable compute rates improve task sizes minutes length 
weekdays weekends trade overhead failures produces optimal task size minutes respectively 
note number minutes task execution require dedicated ghz host effective execution times experienced sdsc entropia grid range approximately times longer times shorter 
rate ops sec weekday weekend task size minutes dedicated ghz host iii model application rate entire sdsc desktop grid number operations seconds versus task size number minutes dedicated cpu time ghz host 
iii cluster equivalence characterize impact resource volatility desktop grid usable performance cluster equivalence utility metric introduced :10.1.1.22.4887
desktop environment corresponding temporal cpu availability fraction dedicated cluster cpu desktop cpu worth application 
information establish desktop grid size dedicated cluster performance equivalent 
precisely host desktop grid nodes dedicated cluster comparable cpu clock rates required platforms equal utility define cluster equivalence ratio desktop grid 
objective quantify performance impact resource volatility normalize assuming cpu clock rate node cluster equal mean cpu clock rate desktop grid clear desktop grid measurements cluster equivalence ratio depends application structure characteristics 
consider task parallel applications various task sizes 
higher task size lower numerous industrial interactions committee members suggest true companies 
cluster equivalence ratio cluster equivalence ratio application subject failures see iii 
cluster equivalence weekdays weekends subjob size minutes mhz machine sdsc desktop grid equivalent number cluster nodes cluster equivalence ratio cluster equivalence weekdays weekends subjob size minutes mhz machine sdsc desktop grid iii cluster equivalence desktop grid cpu function application task size 
lines shown resources weekdays weekends 
compute cluster equivalence range application task sizes shown iii 
curves essentially scaled versions fig ure iii 
data points graph determine effective cluster cpu sdsc desktop grid delivers 
example operation tasks approximately minutes ghz cpu performance node sdsc entropia desktop grid equivalent node cluster weekends node cluster weekdays 
comparison iii shows cluster equivalence metric computed subset desktop grid excludes machines case machines produced year clock rates higher ghz 
mean clock rate subset hosts approximately mhz 
observe trends similar seen iii 
fact average relative difference cluster equivalence ratios entire desktop grid subset task sizes approximately 
fact cluster equivalence metric relatively consistent different equivalent number cluster nodes cumulative total power delivered power clock rates top sorted hosts iii cumulative percentage total platform computational power sdsc hosts sorted decreasing effectively delivered computational power hosts clock rates 
subsets desktop grid explained iii 
plots cumulative percentage operations delivered subset entire platform corresponding increasing percentage sorted hosts 
words data point graph means hosts useful hosts deliver compute operations entire platform 
hosts sorted number delivered operations seconds computed measurements corresponding clock rate seen curves iii 
see curves strikingly similar 
indicates average availability patterns hosts platform measurement period uncorrelated host clock rates shown chapter iii 
turn explains cluster equivalence metric consistent platform subset containing older machines 
interestingly find curves iii linear moderately skewed compared dotted line 
instance useful hosts deliver compute power 
similarly useful hosts deliver approximately compute power 
note skew high justify small fraction resources 
iii summary chapter described simple trace method measured cpu avail ability way reflects cpu availability experienced real application 
method gather data sets platforms distinct clock rate distributions user types 
addition obtained fourth data set ered earlier authors 
derived useful statistics regarding exec availability cpu availability system individual hosts 
findings characterization study summarized follows 
volatile platform strictest host recruitment policy availability intervals tend minutes length greater mean length platforms interactive users hours 
showed section iii relatively short task lengths minutes utilize availability intervals significantly harm aggregate achievable performance application server enforced task assignment overhead 
second task failure rates system correlated task size task failure rate approximated linear function task size 
turn allowed construct closed form performance model system describe section iii third observed platforms interactive users exec availability tends independent hosts 
independence affected significantly configuration hosts example wake lan enabled ethernet adapters cause correlated availability hosts 
platforms run batch jobs availability significantly correlated 
fourth availability interval lengths terms time correlated clock rates percentage time host unavailable 
means hosts faster clock rates necessarily 
interval lengths terms operations task failure rates correlated clock rates 
indicates selecting resources clock rates may beneficial 
studied cpu availability resources 
cause recruitment policies worker cpu availability hosts zero 
regarding cpu availability host wide varia tion availability host host especially platforms interactive users 
result platforms hosts identical clock rates significant heterogeneity terms performance host respect application 
chapter iv resource management methods models metrics iv previous chapter measured characterized desktop grids virtually statistics computed influence design implementation scheduling heuristics discuss remaining chapters 
chapter outline scheduling techniques heuristics 
addition describe platform application models instantiations simulation method performance metrics evaluate scheduling heuristics 
consider problem scheduling applications application re source management level enterprise desktop grid consists volatile hosts lan 
lan university companies entropia united devices specifically targeted lan platform supporting desktop grid applications 
enterprise desktop grids attractive platform large scale computation hosts usually better connectivity mbps ethernet example relatively volatility heterogeneity desktop grids span entire internet 
compared dedicated clusters enterprise grids volatile heterogeneous platforms main challenge develop fault tolerant scalable efficient scheduling heuristics 
evaluate heuristics enterprise environments traces enterprise desktop grids design heuristics applicable internet environments 
number consequences respect platform application models describe section iv commonly scheduling method desktop grid systems come serve fcfs 
desktop grids seti home fold ing home home typically run high throughput jobs performance metric aggregate rate system weeks months 
highest aggregate rate achieved assigning tasks hosts fcfs manner start wind phases application executions negligible compared steady state phase 
number tasks far greater number hosts scheduler allocate tasks resources possible issue resource selection 
desktop grids commonly high throughput jobs desktop grids attractive platform supporting rapid application turnaround order minutes hours 
discussed chapter numerous applications computational biology graphics interactive scientific visualization require rapid turnaround 
addition applications mpp workloads day length applications workload require relatively rapid turnaround day time 
supporting rapid turnaround applications volatile desktop grids chal number reasons 
resources heterogeneous terms clock rates memory disks sizes network connectivity example 
distribution clock rates typical desktop grids spans order magnitude 
assuming system capability task preemption application equally sized tasks number tasks roughly number hosts potentially suffer severe load imbalance tasks allocated slow hosts near application completion delaying application completion hosts sit needlessly idle 
second resources shared desktop user owner way subset machines reserved block time 
dedicated access set machines unplanned interruptions task execution scheduling difficult resources completely dedicated 
particular desktop user activities priority desktop grid application desktop volatile result fluctuating cpu host availability result frequent task failures 
example task takes minutes run dedicated ghz host failure rate calculated means random incidence trace data set collected desktop grid sdsc show section iii 
system checkpointing support failures near application execution result poor application performance failed task restarted scratch turn delays application completion 
causes volatility significant effects applications require rapid turnaround 
iv show cumulative number tasks completed time observed trace driven simulations server schedules tasks hosts fcfs manner 
results obtained application tasks run platform hosts task execute minutes dedicated ghz processor simulation platform driven sdsc trace see section iv detailed description simulation methodol ogy 
curves initial hump system reaches steady state throughput increases roughly linearly 
cumulative throughput reaches plateau accounts increasingly large fraction application makespan number tasks decreases 
application tasks tasks completed minutes application finish minutes passed identical makespan larger application tasks 
main causes plateau 
cause task failures occur near completion application 
task fails started scratch occurs near application execution delay application completion 
second cause tasks assigned slow hosts 
task assigned slow host fcfs scheduler task preemption replication capabilities forced wait slow host completes result 
number tasks gets large compared number hosts platform plateau significant justifying fcfs strategy 
applications relatively small number tasks resource selection improve performance short lived applications significantly 
cumulative number tasks completed tasks tasks tasks time minutes iv cumulative task completion vs time 
design various resource management methods address scheduling problem describe section 
iv models instantiations order design evaluate scheduling heuristics create models desktop grid systems targeted applications capture relevant char 
models enable computationally tractable simulations number heuristics large range applications platforms 
instantiate models desktop grid traces described chapter iii 
implement discrete event simulator application platform models drive simulator traces described chapter iii collected real desktop grid platforms representative grid configurations 
simulation studying resource selection desktop grids direct experimentation allow controlled repeatable experiments 
true desktop grid fashion iv scheduling model simulations deployed xtremweb desktop grid system 
describe system application models instantiations section ing client application resource management worker levels described chapter ii shown ii 
iv platform model instantiation application resource management level assume scheduler maintains queue tasks scheduled ready queue available workers see iv 
workers available notify server scheduler server places workers corresponding task requests ready queue 
resource selection scheduler examines ready queue determine possible choices task assignment 
hosts volatile heterogeneous size host ready queue changes dramatically application execution workers assigned tasks removed ready queue workers different speeds avail ability complete tasks notify server 
host ready queue usually small subset workers workers notify server available task execution 
worker level assume worker running host cally sends heartbeat server indicates state task 
assume worker sends heartbeat minute indicate task running failed done xtremweb system 
assume system provides remote checkpointing abilities 
internet desktop grids systems lack remote checkpointing 
reason significant number machines high connected dial modems transferring large core dumps mb size wide area quickly feasible 
task save state persistently disk task terminated task revert previous state idle period losing little past computation 
consequently chapter vi consider case task checkpoint state local disk 
assume server cancel task scheduled worker 
reason resource access limited firewalls usually configured block incoming connections precluding incoming rpc allow outgoing connections restricted set ports port 
heuristics preempt task assigned workers initiative request tasks server 
platform model deviates significantly traditional grid scheduling mod els 
scheduling model grid scheduling research collection tasks ready assigned pool resources 
grid scheduler devise plan determines applications including data placed specific resources minimize application makespan 
desktop grid environ ments devising static plan task assignment may futile tasks pushed workers 
pool resources select vary dynamically time depending workers available 
plan devised set resources chosen may available time assignment 
platform model desktop grids mentioned chap ter iii sdsc deug lri ucb platforms 
instantiated plat form model hosts desktop grid availability defined corresponding traces 
sdsc platform hosts re platform significantly fewer hosts 
compensate aggregated host traces different days approximately host traces platform 
aggregation full days traces drive simulations 
shown number studies including hosts weekday business hours exhibit higher variable load peak hours weekday nights weekends 
simulations performed traces captured business hours varied depending platform 
am pm sdsc am pm deug day lri am pm ucb 
heuristics perform relatively peak hours host performance predictable performance difference lessened 
diversity desktop configurations compare performance heuristics configurations representative internet multi cluster desktop grids 
access types desktop grids unable gather traces types platforms 
desktop grid projects publicly report clock rates participating hosts 
real traces transform clock rates hosts sdsc grid reflect distribution clock rates particular platform transform cpu availability trace corresponding host accordingly 
example internet desktop grids utilize machines enterprise home settings usually slow hosts fast hosts host speed distribution left heavy 
host cpu statistics collected gimps internet wide project determine distribution clock rates ranged mhz ghz 
projects folding home home show similar distributions 
desktop grids focused resources multiple labs 
reports xtremweb student lab lri ghz machines condor cluster wisc mhz machines mhz machines 
configuration specified model multi cluster scenario 
plot cumulative clock rate distribution functions additional platform scenarios iv 
distributions ran simulations sdsc desktop grid traces transforming host clock speeds accordingly 
justification clock rate transformations availability fraction sdsc deug lri ucb clock rate mhz real fraction clock rate mhz simulated lri wisc gimps iv cumulative clock rate distributions real simulated platform platform range clock rates volatility sdsc high med deug low med lri low low ucb low high gimps high med lri wisc bimodal med table iv qualitative platform descriptions 
interval size independent host clock rate chapter iii 
desktop grid similar user base sdsc researchers administrative assistants am pm expect hosts similar availability interval lengths regardless cpu speed 
summary table iv shows type platforms evaluate heuristics 
clock rates host volatility primary sources poor application performance explore real hypothetical platforms wide range clock rates volatility levels representative real desktop grid systems 
particular examine cases platform medium volatility wide range clock rates low range clock rates wide range volatility levels 
iv application model instantiation majority desktop grid applications high throughput ingly parallel 
applications tasks relative number hosts dependencies communication tasks 
effort broaden set applications desktop grids study schedul ing applications stringent time demands require turnaround order minutes hours versus days months number tasks order number hosts 
experience sprite developers characterization availability chapter iii numerous interactions industrial companies committee members suggest desktop grids enterprise underutilized scenario number resources order magnitude comparable number tasks uncommon :10.1.1.14.7130
investigate techniques scheduling applications consist independent identically sized tasks volatile hosts order appli cations contain tasks correspond roughly half equal double number hosts respectively long medium short plateaus cumulative number tasks completed application execution seen iv 
addition vary lengths tasks affects failure rate application tasks 
experiment tasks exhibit minutes execution time dedicated ghz host 
task sizes corresponding failure rate scheduled set resources business hours 
described chapter iii determined failure rate task size random incidence entire trace period 
collected traces chose thousands random points start execution task noted task run completion meet host failure 
task failure rate increases linearly task size minimum minute task minute task maximum minute task 
maximum task size minutes chosen significant number applications complete scheduled business hours single weekday 
assume tasks compute bound focus applications small data input output sizes order kilobytes megabytes fast networks enterprise environments take account network effects completion time application 
iv proposed approaches consider general approaches resource management resource prioritization way resource selection sort hosts ready queue criteria clock rate number cycles delivered past assign tasks hosts 
prioritization effect number tasks left execute greater number hosts ready queue 
fewer tasks execute ready hosts typically application execution prioritization simple way avoiding bad hosts 
resource exclusion fixed threshold simple way select resources exclude hosts run application tasks 
filtering simple criterion hosts clock rates threshold 
distribution resource clock rates skewed slowest hosts significantly impede application completion excluding potentially remove bottleneck 
fixed threshold unintentionally exclude relatively slow hosts contributed application completion address deficiency approach 
resource exclusion makespan prediction sophisticated resource exclusion strategy consists removing hosts complete task assigned expected application completion time 
words may possible obtain estimate application reasonably complete host push application execution estimate 
advantage method compared blindly excluding resources fixed threshold sensitive distribution clock rates 
relatively slow hosts contribute application completion excluded unnecessarily contrast previous method uses fixed threshold 
task replication best resource selection method task failures near application execution inevitable 
deal failures examine effect replicating multiple instances particular task assigning different hosts replicating task may increase chance task instance complete 
study approaches order listed 
ap proaches listed increasing order respect complexity costs evident chapters 
approach examine wide range scheduling heuristics vary complexity quality information resources static dynamic information 
heuristic identify costs benefits increasing heuristic complexity quality information resources 
best heuristic particular ap proach study augment heuristic addressing weaknesses subsequent approach 
inductive method heuristic design allows examine manageable number heuristics 
stage heuristic design evaluate compare heuristics simulation detail section 
iv measuring analyzing performance experiment particular number tasks task size simulated competing resource management heuristics applications starting different times business hours 
ran experiment starting times averaged results obtain statistically significant results 
section discuss evaluate performance application scheduled particular heuristic automatically analyzed performance heuristic 
iv performance metrics application makespan metric compare results achieved different scheduling heuristics wish compare execution time achieved oracle full knowledge host availabilities 
oracle works follows 
determines time host complete task looking availability traces scheduling task soon host available 
selects host completes task repeats process tasks completed 
greedy algorithm results optimal schedule compare performance heuristics ratio makespan particular heuristic optimal makespan achieved oracle 
optimality greedy algorithm easy see intuitively 
prove formally section chapter analysis results algorithm 
note upcoming chapters focuses minimizing execution elapsed time makespan single par application trying optimize performance multiple competing applications 
heuristics develop schedule single application provide key elements designing effective multi application scheduling strategies doing appropriate space sharing applications selecting resources application deciding task duplication level application 
iv method performance analysis determine cause poor performing heuristic visually inspect execution trace subset applications scheduled heuristic 
host graph time starts task 
task completes graph completion time fails plot time failure 
analyzing performance visual inspection application execution trace possible due high number thousands applications executed simulation 
supplement visually analysis application execution traces develop simple automated approach determine causes poor performance 
discussed section iv delays task completion near applica tion execution result plateau task completion rate poor performance refer tasks completed extraordinarily late application execution lag gers 
visual inspection numerous application execution traces hypothesize causes hosts relatively low clock rates task failures occur near application execution 
confirm hypothesis manner 
automated method find laggers application ex coordinated fcfs scheduler 
automatically classify cause slow host clock rate task failure 
find high percentage laggers caused low host clock rates task failures giving strong evidence hypothesis 
confirming hypothesis automated method determine impact slow host clock rates task failures application execution scheduling heuristics 
describe automated method detail 
determine cause poor performing applications automatically mine simulation logs determining number laggers classifying particular cause low clock rate task failure 
classify completed tasks laggers determining interquartile range iqr task completion 
iqr defined range lower quartile th percentile upper quartile th percentile task completion times excluding task executions fail complete 
method defined finding sample quantiles quantile actual data point 
method biased methods take average data points 
multiply iqr certain factor term iqr factor add result upper quartile give threshold 
task completed threshold classified 
particular assuming iqr factor exists threshold lower bound task completion rate slowdown interquartile quartile tasks laggers signify dramatic decrease task completion rate 
iv shows cumulative throughput application tasks execution progresses 
third quartiles task completions labelled showing iqr 
iqr factor shows threshold respect third quartile 
tasks finish execution threshold considered laggers 
rationale iqr define threshold task completion rate iqr close approximation optimal 
application execution enters steady state iqr available host assigned cumulative number tasks completed st qr 
iqr rd qr 
threshold rd qr 
iqr time minutes iv laggers application tasks 
task 
task completion rate iqr guaranteed optimal follows trivially proof optimal scheduling algorithm discuss section iv alternative find standard deviation application makespans define threshold factor standard deviation 
nature laggers tend relatively extreme outliers terms task completion times standard deviation sensitive outliers extreme cause standard deviation quite high threshold standard deviation result false negatives 
approach quantiles affected extreme laggers 
alternative method classify completed tasks application laggers 
case optimal application execution method classify tasks laggers yield relatively high false positives 
question related analysis choose suitable iqr factor clearly extremely low iqr factor yield false positives extremely high iqr factor true laggers limiting analysis insignificant number laggers 
determine set possible iqr factors conducted simple sensitivity analysis number laggers determined iqr factor 
lowest possible iqr factor steady state maximum task completion rate usually occurs iqr 
maximum iqr factor iqr factors greater near zero laggers application 
range iqr factors number laggers decreases gradually factor increased 
choose intermediate iqr factor values significantly change distribution laggers see appendix 
find set laggers application determine cause follows 
determine cause task failure near application completion look tasks completed th percentile task fails point conclude failure cause 
determine cause slow clock rate clock rate slowest host corresponding optimal application execution 
application begins execution particular time run optimal execution omniscient scheduler find slowest host execution 
clock rate slowest host classify assigned slow host 
advantage method classification confirms faster host scheduling heuristic looking application optimal execution 
advantage method determines clock rate threshold instance application execution 
relatively fast hosts optimal application execution resulting clock rate threshold tend higher 
comparison heuristics consider number laggers weak correlation correlation coefficient usually number laggers makespan fcfs scheduling method relatively low iqr factor 
low iqr factor ensure laggers counted 
weak correlation cause application waiting completion task scheduled extremely slow host rest tasks completed shown application tasks iv 
case number laggers relatively small effect application makespan sole tremendous 
reason consider number laggers weak correlation mean application makespan mean number laggers set scheduling heuristics heuristic results lower mean makespan fact laggers different heuristic results higher mean makespan 
reason iqr metric relative total makespan application mean makespan particular heuristic decreases iqr decreases turn lowers threshold defined th quantile iqr iqr factor lower threshold raise chance host complete task threshold clock rate distribution cpu availability hosts platform remains fixed 
supplement analysis show absolute measure length times intervals delimited second third fourth quartiles task completion times 
absolute iqr heuristics example iqr resulting fcfs scheduling method iqr fcfs higher times higher general iqr tend similar iqr heuristics result false negatives 
define relative iqr heuristic 
iv computing optimal makespan prove greedy algorithm proposed section iv results optimal makespan jobs identical independent hosts scheduled volatile hosts 
optimal algorithm consisted starting task host soon possible availability interval assigning tasks host soon previous task host completes 
task fails due availability interval reached necessary operations delivered task task restarted scratch availability interval 
greedy fashion availability intervals hosts filled infinite number tasks tasks packed tightly possible 
tasks sorted increasing completion time pick tasks 
assignment tasks hosts corresponds optimal schedule 
schedule intuitively optimal may wonder inserting delays tasks beneficial order match overhead periods length periods host cpus exhibit low fact availability 
sections give formal description algorithm formal proofs optimality single availability interval single hosts multiple availability intervals single hosts multiple availability intervals multiple hosts general case 
posteriori proof straightforward algorithm optimality worth proving formally heavily chapters 
approach follows 
defining problem formally section iv show optimality single availability interval single host sec tion iv 
section iv show optimality multiple availability inter vals separated failures single host 
section iv show optimality multiple availability intervals multiple hosts 
section iv consider variation problem allows task checkpointing 
iv problem statement consider job consists tasks consider hosts variable cpu availability described traces previous chapter possibly dif ferent maximum amount operations delivered time unit 
denote instantaneous number operations delivered instant ii depicts availability trace continuous fact discrete step function 
denote dt number operations delivered host desk top grid application time time provided interval fully contained availability interval 
tasks identical computational cost independent 
denote task size number operations overhead seconds scheduling task incurred computation 
observed overhead practice explained chapter iii 
denote fm instantaneous number operations delivered time host fully known trace see previous section 
recall practice function fm known report focus developing optimal schedule achieved omniscient algorithm host availability hosts 
scheduling problem assign tasks hosts job makespan minimized 
iv single availability interval single host focus optimally scheduling tasks single availability interval 
formalize algorithm prove optimality 
iv scheduling algorithm define helper function intg takes arguments input number operations overhead seconds overhead task start time upper bound task finish time cpu availability function corresponds single availability interval 
intg returns time task size started time host cpu availability described function incurring overhead overhead start computing complete complete time 
assumed time lies inside single availability interval defined function words function intg returns exists time dx exist 
show implementation intg pseudo code iv 
note pseudo code shows discrete implementation loop increments value local variable assuming step size trace step function second 
intuitively function intg scheduling algorithm see task fit inside availability interval started interval 
greedy algorithm iv computes schedule described informally section iv 
takes parameters consider number tasks scheduled overhead starting task task size number operations cpu availability function absolute start times host availability interval algorithm intg overhead sum sum sum sum return return iv intg helper function scheduling algorithm 
array stores time task begins filled array stores time task completes filled returns number tasks scheduled availability interval tasks 
pseudo code easy see schedules tasks availability interval task scheduled immediately previous task completes 
duration task execution computed call intg helper function 
iv proof optimality tt denote times task begins execution schedule computed algorithm 
note just availability interval 
task execution times counting overhead ti ti ei consider schedule obtained algorithm call lay start task early possible 
words algorithm adds time delay wi starting task times tasks start execution task execution times counting overhead schedule 
prove schedule better schedule guar schedule optimal availability interval 
algorithm intg time task completes time task scheduled return return iv scheduling algorithm single availability interval 
proposition schedules tasks optimally 
prove induction 
base case assume hold meaning situation depicted iv 
convenience denote completion times task schedules respectively assumption iv example task execution higher lower job 
jobs arrive time 
case task scheduled immediately overhead incurred 
case scheduler waits period scheduling task 
schedule write dt just means execution task consumes exactly number operations needed 
write dt dt dt dt 
note second integral right hand side equation equal corresponds full computation task schedule 
second note third integral strictly positive 
equal zero number operations delivered host task interval zero meaning useful computation performed interval schedule 
schedule completion time fact lower equal agree hypothesis 
result dt contradiction 
conclude holds 
inductive case assume holds prove 
execution timeline schedule depicted iv tj lower equal due 
base case cj denote completion times task schedules 
iv example task execution higher lower middle job 
suppose cj schedule write cj tj dt just means operations delivered application task execution 
split integral follows cj tj dt wj tj dt wj cj dt dt 
integral valid wj tj due property 
argument base case integral right hand side equation strictly positive cj cj 
second integral equal number operations delivered task execution schedule 
obtain cj tj contradiction 
cj property holds completes proof induction 
iv multiple availability intervals single host section consider scheduling tasks multiple intervals avail ability start times denoted ai bi 
loss generality ignore availability intervals single task complete bi dt consider infinite number availability intervals ai host number large accommodate tasks 
scheduling algorithm seen iv takes parameters number tasks scheduled overhead starting task task size number operations cpu availability function array stores start times tasks filled array stores completion times tasks filled 
property schedules tasks optimally 
prove true induction 
base case true 
equivalent running opt 
availability interval know leads optimal schedule section iv 
tasks schedule tasks scheduled availability interval tasks finish availability interval task scheduled interval scheduled second 
case 
equivalent 
interval result optimal proved previous section 
case algorithm ai bi concat concat iv scheduling algorithm multiple availability intervals 
task scheduled second interval 
results optimal schedule earliest second task execute finish second interval 
inductive case assume true 

gives optimal schedule tasks 
schedule task place interval th task possible place interval 
case results optimal schedule schedule th task optimally interval resulting makespan th tasks optimal 
case results optimal schedule know 
optimal second availability interval 
true 
optimal schedule 
follows true computes iv multiple availability intervals multiple hosts algorithm scheduling tasks multiple availability intervals multiple hosts optimal seen iv takes parameters number tasks scheduled overhead starting task task size number operations arrival time job matrix stores start times tasks scheduled hosts row computed call matrix stores completion times tasks scheduled hosts row computed call returns total makespan 
assume functions describing cpu avail abilities host fm known 
optimal uses local variable array stores index completed task host 
define argmin operator classical way series say xi argmin mini xi 
algorithm optimal schedule tasks host determine task completion time fi select tasks completed return iv scheduling algorithm multiple availability intervals multiple hosts 
schedules task optimally optimal 
se tasks complete resulting optimal schedule 
iv optimal makespan checkpointing enabled section consider scenario desktop grid system able support local task checkpointing restart 
assume task encounters failure restarted machine began execution consider process migration 
greedy algorithm accounts checkpointing proof optimality similar described previous sections give high level description new optimal scheduling algorithm proof sketch optimality 
discussion checkpointing define new parameters overhead terms time checkpointing task overhead terms time restarting task checkpoint 
number operations completed checkpoint performed 
changes account checkpointing 
checkpointing enabled view intervals failures periods cpu availability 
host trace treated single continuous availability interval schedule tasks 
task scheduled begins execution checkpointing overhead incurred operations completed 
execution task encounters failure progress checkpoint lost overhead incurred restart task checkpoint 
reduce problem scheduling tasks checkpointing enabled problem scheduling task checkpointing follows 
consider single task scheduled availability interval task execution encounter failures 
scheduled incurs overhead operations complete overhead incurred due checkpointing 
treat task subtasks size size 
subtask scheduled overhead subtask scheduled overhead failures encountered task execution achieves optimal schedule argument similar section iv true schedule multiple tasks treated batches subtasks availability interval 
failure encountered task execution progress checkpointing lost overhead incurred immediately failure task restarted checkpoint 
argument prove section iv scheduled subtasks previous availability interval optimally starting subtask availability interval incurring overhead execution gives optimal schedule 
replace optimal failures viewed cpu availability resulting algorithm achieves optimal schedule hosts 
shown greedy algorithm full knowledge host cpu availabilities achieves optimal makespan scheduling job identical independent tasks volatile desktop grid 
best knowledge previous dealt case cpu availability fluctuates time taken account host heterogeneity failures 
note algorithm achieves optimal makespan necessarily achieve optimal execution time delaying task allow encounter periods higher cpu availability 
interesting extension consider multiple job scenario minimizing execution time versus makespan beneficial system performance 
chapter resource selection investigate various heuristics resource selection involves deciding resources resource exclude 
regarding issue focus resource prioritization techniques hosts 
regarding issue study resource excluding techniques filter bad hosts impede completion application execution entirely 
evaluate heuristics sdsc grid contains volatile hosts exhibit wide range clock rates 
report results heuristics run platforms applicable interesting 
resource prioritization heuristics examine methods resource prioritization different levels information hosts virtually information comprehensive historical statistics derived traces host evaluate method trace driven simulation 
pri cr method hosts server ready queue prioritized clock rates 
similar pri cr pri cr wait sorts hosts clock rates scheduler waits fixed period minutes assigning tasks hosts 
rationale collecting pool ready hosts making task assignments improve host selection 
scheduler stops waiting ratio ready hosts tasks threshold resource selection executed immediately large pool resources exists queue 
threshold ratio experiments 
experimented values fixed waiting period ratio obtained similar results 
contrast pri cr pri cr wait static information hosts method pri history uses dynamic information history host past performance predict performance 
specifically host scheduler calculates expected operations availability interval operations executed host failures previous weekday trace 
particular availability interval task may execution interval task higher probability completing longer interval shorter 
longer intervals weighted shorter ones calculating expected operations interval 
take account considering possible subinterval starting points second increments availability interval 
availability interval results subintervals seconds availability interval interval stopping point see 
subintervals denoted double arrows availability interval 
length subinterval shown subinterval lengths differ seconds 
expected operations interval determine priority queues host placed 
expected number operations intervals greater equal number operations application task average task execute completion host placed higher priority queues 
host put low priority queue corresponds hosts task expected run completion 
queue hosts prioritized expected operations interval divided expected operations second result hosts queue prioritized speed 
higher priority queue lists hosts task expected complete faster hosts terms operations interval higher priority 
lower priority queue lists hosts task expected complete faster hosts higher priority 
scheduling pri history check higher priority queue select host highest priority fastest expected speed 
higher priority queue empty pri history check lower priority queue select host highest priority fastest expected speed 
results discussion sdsc platform shows mean makespan heuristics pri cr pri history pri cr wait mean makespan fcfs heuristic normalized mean optimal execution time applications tasks lengths minutes dedicated ghz host 
bold dotted line represents normalized mean makespan optimal algorithm 
recall averages obtained distinct experiments 
explain performance heuristics visual analysis particular application execution traces automated method described section iv give additional concrete evidence 
analysis shown focuses fcfs pri cr find best resource prioritization heuristic 
show heuristics discuss section shows classification laggers caused slow hosts task failures heuristic application size 
height bar corresponds mean number laggers application particular number tasks task size 
particular bar height sub bar represents number laggers caused slow hosts task failures 
find poor performance fcfs predominately caused slow hosts heuristics achieve better performance eliminating slow hosts discuss 
reduction laggers caused slow hosts corresponds reduction laggers caused task failures showed task failure rate correlated host clock rate 
effect eliminating laggers application makespan shown fig ure 
height bar corresponds mean makespan applications particular task size number 
particular bar sub bars represent length quartile task completion times 
observe length second third quartiles heuristic approximately equal optimal task completed steady state near optimal rate 
appears quartiles missing optimal algorithm fact quartiles small visible 
difference makespans primarily due length th quartile turn caused reduction laggers resulting task execution slow hosts 
discuss prioritization heuristic eliminate laggers 
general trend shown larger number tasks application closer achieved makespans optimal expected larger number tasks resource selection critical performance greedy method approaches optimal 
focusing applications tasks notice prior heuristics perform badly fcfs 
reason fcfs performs hosts appear earliest queue tend high task com rates clock rates negatively correlated task failure rates 
reason pri cr pri history perform similarly fcfs clock rate expected number operations interval weakly correlated task completion rate shown chapter iii prioritized hosts ready queue similar order fcfs 
reflected similar number proportion laggers caused slow hosts task failures shown 
pri cr wait average makespan relative optimal number tasks application fcfs pri cr pri history pri cr wait task length minutes dedicated ghz host performance resource prioritization heuristics sdsc grid 
performs poorly small minutes tasks improves surpasses pri cr 
initial waiting period minutes costly task min ap plication takes minutes complete optimal case 
task size increases application execution time penalty incurred waiting host requests lessened hosts request queue application submitted pri cr wait performs identically pri cr better 
provides additional insights pri cr wait largely 
shows number available hosts number tasks scheduled time typical execution 
initially hosts available tasks execution immediately drops hosts tasks available host gets assigned task 
see usually case far tasks schedule ready hosts far ready hosts tasks schedule 
scenario pri cr wait performs exactly pri cr 
case waiting give algorithm choice selecting resources 
noticed different trends number tasks application roughly half number hosts 
reason fcfs performs poorly initially tasks schedule hosts fcfs chooses hosts randomly slow hosts chosen causes reduction application performance 
contrast pri cr exclude slowest resources slow hosts excluded computation shown 
surprisingly pri history performs poorly compared pri cr uses static dynamic information 
availability interval size terms time terms operations stationary weekdays expected operations second poor predictor performance certain hosts 
determined host prediction error day follows 
host calculated mean number operations interval weekday business hours 
took absolute value difference host mean particular day 
show complementary cumulative distribution function prediction error expected time interval host 
plots fraction prediction errors greater length time 
see predictions errors minutes length 
average mean prediction error minutes length median error minutes 
applications hour length high prediction error problematic 
show complementary cdf prediction error expected ops interval host 
plots fraction prediction errors greater quantity operations delivered interval 
find prediction errors equivalent minutes dedicated ghz host average mean prediction error minutes length median error minutes 
high prediction error significant applications hour length pri history tend hosts high expected operations interval 
similarly authors host mean performance long durations reflect dynamism cpu availability poor predictor 
compared prediction error compute rate host estimated expected operations time length interval 
hosts usually completely idle shown section iii rate predicted correctly 
attribute poor performance pri history poor operations interval predictions cause hosts put wrong priority queues 
fraction mean min std dev min median min prediction error time interval minutes expected ops interval fraction mean min std dev min median min prediction error ops interval min 
ghz expected time interval complementary cdf prediction error expected operations time interval summary number tasks greater equal number hosts little benefit prioritization fcfs fastest available hosts naturally requests tasks 
waiting collect pool available hosts improve resource selection delays task assignment 
application execution usually far tasks hosts far fewer tasks hosts case waiting beneficial 
number tasks number hosts pri cr works better pri history expected number operations interval tends unpredictable certain hosts 
pri cr works slowest hosts excluded computation prioritization resulting exclusion improve performance 
average pri cr times better fcfs applications tasks sdsc platform 
see pri cr outperforms fcfs consistently re source prioritization leads performance far optimal factor application minute tasks 
looking task schedules detail noticed slowest hosts significantly limited performance address issue heuristics described section 
number tasks scheduled number tasks number ready hosts time minutes number tasks scheduled left axis hosts available right axis 
resource exclusion prevent slower hosts delaying application completion developed heuristics exclude hosts computation variety criteria 
heuristics host clock rates obtain lower bounds task completion time seen expected operations time interval predictor performance 
resource exclusion heuristics prioritize re sources clock rates previous section pri cr performed best prioritization heuristics 
excluding resources clock rate group heuristics excludes hosts clock rates lower mean clock rate hosts ghz sdsc platform minus factor standard deviation clock rates mhz sdsc platform entire duration computation 
heuristics excl excl excl excl exclude hosts threshold standard number ready hosts deviations mean clock rate respectively 
shows performance heuristics sdsc platform see cases exclusion heuristics improves performance relative pri cr 
cases minimum makespan occurs threshold excl effectively eliminates laggers caused slow hosts see 
makespan increases higher lower thresholds useful hosts useless hosts excluded computation 
usually excl excludes hosts removes useless hosts excludes useful ones exception application tasks equal roughly half number hosts 
particular desktop grid platform excluding hosts speeds mean leave slightly half hosts filtering case hurt performance 
excl excludes hosts remaining useless hosts hurt application makespan 
average makespan relative optimal number tasks application fcfs pri cr excl excl excl excl task length minutes dedicated ghz host performance heuristics thresholds sdsc grid resource exclusion significantly beneficial 
experiments performs average times better fcfs sdsc desktop grid 
sdsc platform excl particular threshold yields best performance average excl performs better pri cr applications tasks respectively hosts slow clock rates eliminated computation 
platforms different clock rate distributions fixed threshold may adequate 
shows performance excl compared fcfs multi cluster lri wisc platform 
applications tasks see negative effect fixed threshold excl resulting performance worse fcfs 
excl excludes hosts mhz clock rates contribute significant fraction platform compute power 
larger applications scheduled excl exhibit worse performance fcfs 
section propose strategies makespan predictor filter hosts way sensitive clock rate distribution compare excl different desktop grid configurations 
makespan predictions avoid pitfalls fixed threshold particular clock rate standard deviation mean case excl develop heuristic scheduler uses sensitive criteria eliminating hosts 
specifically heuristic predicts application makespan excludes resources complete task projected completion time 
rationale definition slow host vary application size number tasks completed runtime distribution clock rates 
large applications tasks relative number hosts hosts long delay application completion small applications fewer tasks hosts small subset hosts complete task application projected makespan 
predict makespan compute average operations completed second host account host load availability traces computing average hosts call average 
number hosts desktop grid assume platform hosts speed estimate optimal execution time entire application tasks size operations wr 
rationale prediction method optimal schedule encounter task failures 
host unavailability cpu speed main factors influencing application execution time factors accounted addition account granularity tasks completed 
assess quality predictor wr compared optimal execution time predicted time tasks minutes size applications tasks 
average error experiments maximum 
satisfactory accuracy prediction explained fact total computational power grid remains relatively constant individual resources may availability intervals unpredictable lengths 
show computed number operations delivered weekday business hours minute increments aggregated hosts 
coefficient variation operations available minute interval 
relatively low variation aggregate computational power accurate predictions wr possible 
heuristic excl pred uses makespan prediction adaptively changes prediction application execution progresses 
particular heuristic starts makespan computed wr tasks com recomputes projected makespan 
choose recompute prediction tasks completed reasons 
extreme static predic tion computed prone errors due resource availability variations 
extreme recomputing prediction second beneficial create moving target slide prediction back factor tasks completed 
application near completion predicted completion time early risk hosts get excluded 
tasks remaining time pred pred predicted application completion time mean clock rate hosts excl pred heuristic reverts pri cr time 
ensures excl pred switches pri cr clear hosts complete task predicted completion time 
note heuristic waited time pred versus pred switching pri cr result poor resource utilization seen early simulations hosts available excluded time pred 
waiting time pred making task assignments pri cr cause hosts sit needlessly idle 
evaluation different desktop grids tested evaluated heuristics simulation desktop grid platforms described section iii focus discussion platforms remarkable results sdsc gimps lri wisc platforms report results platforms appendix particular heuristics clock rate information resource selection exclusion heuristics executed platforms contained hosts relatively similar clock rates usually similar results deug lri platforms exception ucb platform see appendix 
average makespan relative optimal number tasks application fcfs cr excl excl pred task length minutes dedicated ghz host heuristic performance sdsc grid shows sdsc grid pri cr performs nearly excl pred excl applications tasks performs worse excl applications tasks 
performance pri cr depends greatly number tasks application number causes slow hosts excluded computation 
application tasks slow hosts get excluded pri cr relatively see 
application tasks pri cr assigns tasks slow hosts impede application completion 
application tasks tasks hosts kept busy computation slow tasks complete 
contrast pri cr exclusion heuristics perform relatively application sizes 
shows excl pred usually performs excl machines sdsc clear advantage excl pred particular distribution clock rates sdsc desktop grid excl appears particular threshold yields best performance 
heuristics excl eliminates highest percentage laggers caused slow hosts reduc tion percent laggers caused slow hosts high 
excl pred slightly laggers caused slow hosts excl aggressive filtering hosts excl 
consequently excl pred performs poorly excl application minute tasks 
close inspection traces laggers handful rela tively slow hosts finish execution past projected makespan task failures slow hosts occurring near application 
application tasks delay hidden tasks keep hosts busy slow hosts finish task execution 
application tasks rela tively slow unstable hosts get filtered fewer tasks hosts heuristic prioritizes resources clock rate 
reasoning sdsc platform explain excl outperforms excl pred gimps desktop grid see sdsc grid left heavy distribution resource clock rates 
gimps resources applications scheduled fcfs pri cr finish weekday business hours period application completion times greater hours extremely slow resources 
slow hosts especially internet desktop grids having left heavy distribution clock rates detrimental performance fcfs pri cr 
excl performs best sdsc gimps desktop grids tasks application min tasks min tasks min tasks slow host failed task laggers tasks application min tasks min tasks min tasks tasks application laggers min tasks min tasks min tasks laggers cause laggers iqr factor sdsc grid 
fcfs 
pri cr 
excl 
excl pred tasks application min tasks min tasks min tasks st nd rd th duration sec tasks application min tasks min tasks min tasks tasks application duration sec min tasks min tasks min tasks duration sec length task completion quartiles sdsc grid 
optimal 
fcfs 
pri cr 
excl 
excl pred average makespan relative optimal number tasks application fcfs cr excl excl pred task length minutes dedicated ghz host heuristic performance gimps grid threshold excl inadequate different desktop grid platforms filtering criteria adaptiveness excl pred advantageous sce 
particular excl pred performs outperforms excl multi cluster lri wisc platform 
application tasks see excl pred outperforms excl case lri wisc 
excl lri wisc desktop grid excludes mhz hosts con tribute significantly platform computing power 
general longer steady state phase application better excl pred performs re spect excl excl excludes useful resources utilized excl pred 
explains excl pred performs better excl applications tasks larger task sizes 
pri cr excl pred clearly pri cr effective platforms especially left heavy distribution clock rates 
makespan prediction prevent unnecessary exclusion useful resources 
method conservative elimination hosts especially shorter applications 
average makespan relative optimal number tasks application fcfs cr excl excl pred task length minutes dedicated ghz host heuristic performance lri wisc grid related emergence grid platforms resource selection heterogeneous shared dynamic systems focus intense investigation 
desk top grids compared traditional grid systems incorporating mainly set clusters mpp heterogeneous volatile reflected results chapter iii 
consequently platforms models grid scheduling inadequate desktop grids 
example inadequacy typical model resource availability 
discussed chapter iii availability models host cpu availability described accurately reflect availability resource perceived desktop grid application 
scheduling heuristics designed evaluated models inapplicable desktop grid environments 
example describes system scheduling soft real time tasks statistical predictors host load 
system presents user confidence intervals running time task 
confidence intervals formed time series analysis historical information host load 
assumes homogeneous environment disregards task failures caused user activity system specifically target desktop grid environments 
effectiveness system desktop grids questionable 
example studies problem scheduling tasks computational grid purpose online tomography 
application consist multiple independent tasks scheduled quasi real time shared network workstations mpp authors formalize scheduling problem constrained optimization problem 
scheduling heuristics construct plan constraints user requirement feedback certain time characteristics application data input size 
scheduling model considers fact host loaded model consider task failures 
high task failure rates desktop grid systems heuristics executed desktop grids suffer poor performance 
described relevant terms desktop grid schedul ing 
author investigates problem scheduling multiple independent compute bound applications soft deadline constraints condor desktop grid sys tem 
application study consists single task 
issue addressed prioritize multiple applications having soft deadlines highest number deadlines met 
author uses approaches 
approach schedule application closest deadline 
approach deter mine task complete deadline history host availability previous day randomly choose task predicted complete deadline 
author finds combined approach scheduling task expected complete closest deadline best method 
platform model study considers shared volatile hosts platform model assumes hosts identical clock rates platform supports check pointing 
study determine impact relatively slow hosts task failures execution set tasks likewise author study effect resource prioritization clock rates resource exclusion 
summary chapter investigated resource selection techniques improving application makespan resource prioritization resource exclusion 
resource prioritization improve application performance improvement varied greatly number tasks application applica tion consisted tasks tasks inevitably assigned slow hosts limited performance 
number tasks equal greater number hosts little benefit prioritization fcfs 
capable hosts tended request tasks fcfs performed prioritization heuristics studied 
waiting pool host re quests collect performing resource selection delayed application execution 
number tasks number hosts prioritization resulting exclusion poor hosts improved performance 
pri cr average performed times better fcfs applications tasks 
static clock rate information useful relatively dynamic information length availability intervals mean availability interval length poor predictor host performance 
studied heuristics eliminate slow hosts application execution 
exclusion heuristics fixed threshold respect platform mean clock rate filter hosts adaptive threshold application predicated makespan 
fixed threshold exclusion heuristics achieved high performance gains excl best performing fixed threshold heuristic sdsc platform performed times better fcfs sdsc grid 
exclusion fixed threshold degrade performance depending distribution host speeds 
studied heuristic excluded resources predicted makespan 
cally heuristic excl pred makespan prediction excluded hosts complete task predicted makespan 
sdsc gimps platforms excl pred proved conservative exclusion re sources performed times worse 
multi cluster lri wisc excl pred performed times better especially longer applications slower hosts incorporated excl pred excluded excl 
see chapter excl pred combined task replication performs best close best platforms 
chapter vi task replication vi previous chapter explored range heuristics determined host schedule task 
best resource selection method performance degradation due task failures possible 
relatively long quartile task completion times best performing heuristic compared quartile optimal minutes times shorter indicates room improvement 
chapter augment resource selection exclusion heuristics described previously task replication techniques dealing failures 
define task replication assignment multiple task instances particular task set hosts task applications unit task instance corresponding application executable data inputs assigned host 
refer task instance created original replicated task instances replicas 
assigning multiple task instances hosts probability tasks failing reduced 
replication means adapting dynamic host arrivals desktop grid systems support process migration example case task assigned relatively slow host fast host arrives shortly task replicated fast host opposed migrated accelerate task completion 
task replication plausible technique coping task failures delays reasons 
abundance resources available com pared amount completed 
point seti home project participants actual tasks distribute scheduler began replicating tasks just keep participants busy 
sprite project authors noted idle hosts limited lack applications lack hosts :10.1.1.14.7130
personal communication committee mem bers suggests desktop grids enterprises underutilized 
little contention resources applications replication plausible option 
second task replication relatively easier implement deploy check pointing process migration replication requires modification appli cation hosts operating system 
little modification schedulers desktop grid systems support task replication simple bookkeeping details task instance need added see chapter vii 
contrast imple mentation system level checkpointing process migration requires integration kernel highly specific kernel version possible considering wide range operating systems versions hosts enterprise internet desktop grids 
remote checkpointing requires servers store checkpoints process migration involves moving entire state application different hosts 
considering hosts memory sizes mb common relatively low data transfer speeds capable internet remote checkpointing process migration internet desk top grids may practical feasible especially applications require rapid application turnaround 
order replicate tasks effectively investigate issues 
task replicate host replicate 
task instance running fast stable host replicating task different host lower clock rate availability clearly improve performance 
study different methods choosing task replicate host schedule replica 

replicate 
clearly task throughput tends decrease inversely amount replication 
reason simply task instances particular task assigned effective amount increases factor throughput reduced factor extreme task replicated extreme task replicated available hosts 
determine performance improvement waste various levels replication 
regarding issue replicate application execution heuristics replicate hosts tasks 
applications number tasks larger number hosts steady state phase replicating steady state phase usually improve makespan delay task completion 
fact length time quartiles application execution close optimal supports claim replication unnecessary phase see 
replication point number available hosts greater number tasks remaining scheduling tasks surplus hosts way reducing chance replicated task delay execution task 
replicating anytime sooner cause host redundant unscheduled tasks hosts cause delay application completion 
examine replication issues respect broad approaches task replication proactive reactive hybrid approaches 
proactive replication multiple instances task created initially assigned hosts available 
proactive replication techniques aggressive sense repli cation done delay application completion time occurred 
contrast reactive replication heuristics replicate task task completion delayed execution delaying completion sense heuristics reactive 
develop heuristic uses hybrid approach replicating tasks high risk delaying application completion currently delaying completion heuristic uses proactive reactive replication techniques 
vi measuring analyzing performance vi performance metrics similar chapter continue makespan relative optimal performance metric 
addition waste percent tasks replicated including fail quantify expense wasting cpu cycles 
replication heuristic high waste problematic entire desktop grid loaded multiple applications competing resources 
note reason consider heuristics replication previous chapter replication option high resource contention multiple applications system 
vi method performance analysis general techniques analysis chapter analysis laggers take account replication follows 
define task instance executable data particular task assigned host 
replication involves assigning multiple instances task different host 
task replication task instances complete threshold complete threshold 
address scenario classify task instances task laggers completion times task instances task fall threshold 
way instances task completed threshold instances task completed threshold excluded analysis 
task instance classified consider instances corresponding task analysis order assess task instance completed late 
vi proactive replication heuristics augment heuristics pri cr excl excl pred described chapter replication refer new heuristics pri cr dup excl dup excl pred dup respectively 
scheduling application heuristics create instances original replicas task place priority queue 
replicas scheduled queue number hosts available greater number tasks schedule 
tasks prioritized clock rate host original task instance assigned 
task instances assigned slower hosts replicated 
heuristics pri cr dup excl dup excl pred dup differ set hosts considered task assignment described chapter heuristics discussed prioritize tasks clock rate host original task instance assigned 
study criteria selecting task schedule 
excl pred dup time similar excl pred dup original task instances assigned farthest past assigned original task instances assigned farthest past failed stuck slow hosts 
excl pred dup time spd prioritizes tasks time task instance assigned plus shortest possible completion time task task size divided host maximum compute rate 
hosts available time expect hosts complete tasks shortest possible time heuristic replicates tasks take longer execution delayed 
heuristics create replica task 
study effect application performance varying number times task replicated 
vary number replicas created excl pred dup heuristics excl pred dup excl pred dup excl pred dup respectively 
vi results discussion addition replication heuristic invariably improves performance significantly average see vi sdsc platform 
somewhat surprisingly performance replication heuristics similar regardless set hosts excluded 
attribute fact replication done near application far hosts tasks hosts fast stable 
replication done set relatively fast hosts heuristic hosts clock rates greater ghz average makespan relative optimal number tasks application fcfs cr cr dup excl excl dup excl pred excl pred dup task length minutes dedicated ghz host vi performance heuristics combined replication sdsc grid 
number tasks heuristic excl pred dup excl pred dup excl pred dup table vi mean performance difference relative excl pred dup increasing number replicas task 
excluding slow resources point 
find task replicated replicating improve performance 
table vi shows mean performance difference excl pred pred excl pred dup excl pred dup excl pred dup applications tasks 
maximum mean improvement relative excl pred dup heuristics 
lack performance im provement partly due fact replicating task dramatically decreases probability failure fast hosts available near ap plication execution 
creating replicas significantly reduce probability failure 
replicas created large fraction hosts doing redundant preventing useful done degrading performance 
explains performance degradation shown table vi excl pred dup excl pred dup heuristics 
trends described sdsc platforms match trends deug lri ucb platform summarize shows appendix performance improvement resulting replication deug platform improvement sdsc platform deug host clock rates relatively homogeneous compared sdsc host clock rates 
little improvement lri platform hosts stable homogeneous clock rates 
replication ucb platform results high benefits hosts volatile replication reduces chance failure 
conclude proactive replication useful wide range host clock rates hosts volatile 
waste terms percent tasks replicated number tasks application pri cr dup excl dup excl pred dup task length minutes dedicated ghz host vi waste heuristics proactive replication sdsc grid 
despite performance improvement creating single replica waste resources significant see vi average high 
loaded desktop grids especially waste unacceptable result dramatic decrease system throughput 
develop heuristics reactive replication reduce level waste section 
vi reactive replication heuristics considered heuristics place replicas task queue initially soon original task scheduled resulted high waste 
effort improve efficiency consider heuristics discriminate deciding tasks replicated 
modify excl pred heuristic evaluate certain criteria task placing replica queue effectively delaying task replication 
excl pred similar excl pred delays creation replicas predicted application completion time passes 
original task instance scheduled associate task instance predicted application completion time 
completion time determined makespan predictor described section uses average effective compute rate host predict application complete 
predicted completion time time value time task instance completed create replica place queue 
heuristic optimistic sense creates replica determines original task instance failed complete predicted application completion time replicating earlier 
rationale replicate tasks scheduled fast reliable hosts replicate determined execution task instance delaying application completion task instance execution goes past predicted completion time 
heuristic effectively replicates close completion time application 
heuristic consider excl pred spd replicates aggressively excl pred aggressively excl pred dup 
excl pred spd creates replica minimum task completion time expired task size host clock rate seconds expired 
reasoning hosts unloaded time host usually completely available execute task 
case task instance completed expected execution time task execution suspended multiple times host slightly loaded heuristic assumes task execution delay application completion places replica queue 
vi results discussion performance excl pred excl pred spd similar aggressive replication heuristics section vi see vi despite replicating tasks application execution replicating tasks mean difference average makespan excl pred dup excl pred close zero 
due fact heuristics replicate task instance determined executing task instance delay application completion 
task instance replicated usually fast stable host complete task instance quickly reliably 
discuss detail section vi 
average makespan relative optimal number tasks application fcfs excl pred dup excl pred excl pred spd task length minutes dedicated ghz host vi performance reactive replication heuristics sdsc grid 
performance excl pred excl pred spd re markable heuristics significantly outperform excl pred dup waste sdsc platform 
cases sdsc platform platform metric sdsc deug lri ucb makespan waste table vi mean performance difference waste difference excl pred dup excl pred 
excl pred achieves lowest waste heuristics shown vi wasteful excl pred spd average 
attribute efficiency excl pred makespan predictor forces sched wait long possible significantly delaying application execution replicating task 
results show reactive replication achieve high performance gains relatively low resource waste 
waste terms percent tasks replicated number tasks application excl pred dup excl pred excl pred spd task length minutes dedicated ghz host vi waste reactive replication heuristics sdsc grid 
table vi summarizes mean makespan difference mean difference waste excl pred dup excl pred platforms 
positive means excl pred better excl pred dup 
terms mean makespan excl pred performs worse excl pred dup deug lri platforms respectively 
excl pred performs slightly worse platforms wasteful average wasteful deug lri platforms 
partly excl pred adjust volatility platforms 
excl pred replicate task delays execution lri scenario waste excl pred excl pred dup 
opposite sce platform volatile excl pred replicate tasks 
find excl pred performs worse average excl pred dup tasks relatively high chance failing ucb platform 
excl pred replicates task instance timed relatively high probability replica fail benefits excl pred relatively volatile ucb compared platforms 
contrast excl pred excl pred dup replicates tasks imme soon original task instance assigned host versus waiting predicted application completion time smaller chance task instances fail delay application completion 
time waste excl pred significantly excl pred dup average 
summary find excl pred general performs similar excl pred dup average platforms causing waste average platforms 
excl pred replicates task instance delay application completion replica scheduled relatively fast stable host 
exception ucb platform resources volatile replicating task soon original task instance assigned results faster task completion timeouts case excl pred dup performs better excl pred 
vi hybrid replication heuristics previous sections designed evaluated proactive reactive replication heuristics replicate tasks proactively reactively effort reduce probability task failure near application execution 
section investigate hybrid approach replication replicates proactively tasks high chance failing replicating reactively tasks completed predicted completion time 
clearly just combining proactive replication heuristic excl pred dup reactive replication heuristic excl pred beneficial excl pred achieved similar performance excl pred dup far waste platforms excl pred dup wasteful indiscriminately replicated tasks order assigned slowest hosts 
contrast refined method determining task replicate replicate hybrid heuristic 
approach probability task completion previous day predict probability task completion day 
predicted probabilities replicate tasks predicted probabilities task completion go threshold 
describe heuristic detail 
rep prob heuristic uses history host availability formed decisions regarding replication 
specifically heuristic prioritizes host predicted probability completing task projected application completion time 
random incidence discussed section iii previous day host traces determine predicted probability task completion 
projected application completion time determined makespan predictor excl pred described section 
rep prob prioritizes task probability tion predicated makespan set hosts assigned task lowest probability completion replicated host highest probability 
regarding task instances create heuristic create single replica excl pred dup heuristic 
task instances scheduled slow unreliable hosts probability task completion remain low task require replicas 
rep prob uses probability completion estimate task replicas create order ensure probability task completion greater threshold 
vi feasibility predicting probability task completion evaluate feasibility approach examine stationarity probability task size completes day day 
vi shows probability task completion day tasks minutes length platforms 
graphs show probabilities business days week staring monday 
see platform probability task completion relatively constant deviates previous day 
provides evidence predicted values may sufficiently close actual 
calculate prediction error probability task completion host day 
figures vi vi vi vi show cdf prediction errors sdsc deug lri ucb platforms 
find platforms prediction errors 
results combined evidence host independence shown section iii optimistic compute probability task completion accurately 
vi probabilistic model task completion create accurate probabilistic model created simple deterministic finite automata dfa understand clarify various states task execution see vi 
note concept availability refers exec availability 
task begins execution state 
host fails task complete task fails state wait host available task execution state 
host available long task complete task completes state 
model apparent geometric distribution model probability task completes certain number attempts possible 
geometric distribution assume attempt complete task instance host independent attempts host 
particular probability task completion computed pa rameters probability task completion probability task completion jan sep jan sep sep day sdsc jan day lri sep jan sep min min min jan min min min probability task completion probability task completion jan mar jan mar jan day deug min min min mar day ucb jan mar min min min jan mar vi probability task completion day task lengths 
cumulative fraction cumulative fraction min min min prediction error task completion rate day sdsc prediction error task completion rate day lri min min min cumulative fraction cumulative fraction min min min prediction error task completion rate day deug min min min prediction error task completion rate day ucb vi cdf prediction errors probability task completion day minute tasks dedicated ghz host task fails host available host fails task completion task begins execution host available long task complete task completes vi finite automata task execution 
time current time trial trial time task failure trial trial length unavailability vi timeline task completion 
hn set heterogeneous volatile hosts 
wt set tasks application scheduled 
ci completion time task wi 
ci completion time task instance task wi 
ri number instances task wi 
desired completion time application determined makespan predictor example 
execution time task particular host 
starting current time number attempts trials possible particular host complete task time length time failed attempt particular host probability task completion computed random incidence discussed section iii particular host 
parameters defined particular host hm written lhm respectively brevity omit subscripts discussion 
task complete time executed particular host hm attempt complete task occur time number attempts task completion time required failed attempt see vi 
dfa vi time task executing failure just entering state state plus time host available length unavailability interval incurred going state back state 
ideally modelled probability distribution task time failure length unavailability intervals 
constructing joint probability distribution difficult day worth historical data results sparse probability distribution multiple dimensions 
simplification calculate expected time task failure compute random incidence plus expected length unavailability particular host derive traces 
probability task instance task wi completes time estimated ci vi sums probability task completes th attempt 
section iii gave evidence exec availability independent hosts shown section iii certain platforms 
assuming exec availability independent hosts probability particular task wi completes time estimated ci ci ci ci ci ci ri vi probability application completes time estimated maxi ci ct vi probability completion host desired completion time determine amount replication needed achieve minimum probability threshold 
clearly particular time application execution may hosts replicate order achieve threshold 
heuristic rep prob best effort replicating task lowest probability completion host highest remaining hosts left task instances assigned highest task priority ensure instance task assigned replicating 
equation vi estimate probability application completion theory practice impossible achieve high amount replication number hosts required 
shown simple back envelope calculation determine number instances task required achieve probability bound 
assume application consists tasks scheduled sdsc grid desired probability application completion maxi ci 
achieving threshold requires task completed probability ci ln assuming task completed equal probability 
task instance fails probability realistic number shown section iii require task instances task totalling task instances application tasks 
hosts sdsc platform computing task instances possible relatively small application 
furthermore waste extremely high reduce effective system throughput considerably 
confirmed simulation range application sizes tasks task sizes minutes dedicated ghz host task replicated application rarely completes predicated makespan 
trying achieve probability threshold application rep prob best effort achieve probability threshold task equation vi 
vi rep prob heuristic procedural outline rep prob heuristic 
predict application completion time makespan predictor described section 
prioritize tasks probability task completion time estimated equation vi 
unassigned tasks highest priority 
tasks timed second highest priority 

prioritize hosts probability completing task time 
tasks remaining queue assign instance task lowest probability completion host highest probability 
assign timeout task 
task completed time task second highest possible priority corresponding timed tasks 
recompute task probability completion 
remove task queue probability completion hypothesize rep prob outperform excl pred 
rep prob takes account host clock rate host volatility deciding task replicate host replicate 
rep prob aggressively replicates tasks low probability completion soon original task instance assigned turn reduces chance tasks scheduled volatile hosts delay application completion 
contrast excl pred replicates task completed predicted makespan replica assigned host clock rate disregarding host volatility 
tasks initially assigned volatile replicas scheduled late application execution may result delays application completion 
replicas may assigned volatile hosts hosts may relatively fast clock rates 
vi results discussion vi shows results sdsc platform application size table vi shows performance rep prob relative excl pred platforms 
positive value table means rep prob performed better excl pred 
figures platforms shown appendix 
surprisingly rep prob perform better excl pred sdsc deug platforms 
platform rep prob perform significantly better excl pred performance difference average 
tested range thresholds threshold adequate terms improving application performance 
average makespan relative optimal number tasks application fifo excl pred dup excl pred rep prob task length minutes dedicated ghz host vi performance rep prob sdsc grid 
waste terms percent tasks replicated number tasks application excl pred dup excl pred rep prob task length minutes dedicated ghz host vi waste rep prob sdsc grid 
platform metric sdsc deug lri ucb makespan waste table vi mean performance waste difference excl pred rep prob 
performance excl pred similar rep prob rea sons 
strong correlation probability completion partic ular time clock rate shown section iii 
excl pred replicates tasks hosts highest clock rates hosts tend highest probability completion projected completion time 
second large fraction hosts platform relatively stable 
figures vi vi vi show cdf task failure rates host platform 
example fifteen minute task fraction hosts failure rates sdsc deug lri ucb platforms respectively 
small fraction tasks timeout replica scheduled relatively stable host especially excl pred choose host fastest clock rate correlated probability task completion resulting probability task dramatically lowered 
example timed task chance failure replica scheduled host chance completion probability task fail mere 
fact excl pred dup excl pred dup excl pred dup improve performance sdsc platform supports claim see section vi 
comparing number laggers caused task failures excl pred rep prob see little improvement number laggers rep prob heuristic 
vi shows number laggers applications scheduled excl pred rep prob heuristics see number laggers caused failures usually similar average rep prob laggers excl pred 
see vi number laggers excl pred rep prob exceeds number laggers corre sponding excl applications tasks minutes length time mean makespans excl pred rep prob better mean makespan excl average 
discrepancy due fact iqr excl pred rep prob shorter excl higher number task instances classified laggers 
comparing number laggers heuristic look vi shows mean makespans heuristic gain perspective 
significant reduction number laggers rep prob heuristic mean makespans resulting excl pred rep prob similar benefits rep prob dubious 
third un availability host respect correlated platforms probability task completion computed lower bound 
fact availability hosts deug correlated shown section iii may reason excl pred outperforms rep prob particular platform 
rep prob wastes significantly resources excl pred gain performance see table vi 
rep prob naturally replicates excl pred heuristic replicates tasks low probabilities completion 
reason result sig performance improvement mispredictions probability task completion 
significant fraction predictions may actual value discussed section vi misprediction leaves task assigned volatile host unreplicated costly application 
assumption series attempts complete task instance particular host independent may valid observing traces short availability interval followed short availability interval 
rep prob perform slightly better excl pred ucb platform 
hosts ucb platform clock rates excl pred prioritizes hosts clock rates excl pred distinguish stable host volatile 
rep prob hand prioritize hosts predicted probability completion advantage case 
performance improvement limited large fraction hosts ucb platform relatively stable 
vi evaluating benefits rep prob achilles heel excl pred fact sorts clock rates certainly construct pathological cases excl pred perform poorly rep prob 
example imagine scenario half hosts extremely volatile half extremely stable slightly lower clock rates volatile hosts 
case excl pred tend schedule tasks hosts faster albeit slightly faster clock rates volatile result tasks tend fail delay application completion 
rep prob hand take account host volatility schedule tasks stable hosts 
investigate issue construct new platform half consists volatile hosts ucb platform 
clock rates ucb hosts transformed tasks application min tasks min tasks min tasks slow host failed task laggers tasks application min tasks min tasks min tasks tasks application laggers min tasks min tasks min tasks laggers vi cause laggers iqr factor sdsc grid 
fcfs 
pri cr 
excl 
excl pred 
excl pred 
rep prob 
cumulative fraction sdsc deug lri ucb failure rate min 
task cumulative fraction sdsc deug lri ucb failure rate cumulative fraction min 
task sdsc deug lri ucb failure rate min 
task vi cdf task failure rates host 
follow normal distribution mean mhz standard deviation mhz 
half new platform consists stable hosts lri cluster relatively homogeneous terms host clock rates 
create set platforms transform clock rates hosts lri platform 
clearly clock rates stable lri hosts relatively low better schedule tasks volatile ucb hosts excl pred perform better rep prob 
clock rates stable lri hosts higher ucb hosts better schedule tasks stable fast lri hosts excl pred outperform rep prob 
clock rates lri hosts slightly clock rates ucb hosts rep prob chance outperforming excl pred 
specifically transform clock rates lri hosts relative mean clock rate ucb hosts mhz refer resulting platforms ucb lri ucb lri ucb lri ucb lri ucb lri ucb lri respectively 
run excl pred rep prob platform determine heuristic performs 
observe rep prob performs better excl pred limited set platforms range relative mean ucb clock rate see vi 
limited improvement relatively small set hypothetical scenarios rep prob rarely outperform excl pred performance difference slight 
practice real platforms find rep prob performs better excl pred causing waste average platforms 
general believe excl pred usually outperform perform rep prob possibility performing slightly worse 
vi estimating application performance estimates bounds application makespan useful users submit ting applications 
results application simulations give estimates performance difference percent percent deviation mean clock rate ucb hosts min 
tasks min 
tasks min 
tasks clock rate mode transformed lri hosts platform vi performance difference excl pred transformed ucb lri platforms makespan best heuristic excl pred provide lower confidence inter vals application executed platform 
table vi shows mean makespan excl pred sdsc plat form lower confidence intervals relative mean makespan standard deviation median 
sdsc platform lower confidence interval application makespan remarkably tight away mean task sizes numbers 
mean lower confidence intervals deug lri ucb gimps lri wisc platforms respectively respective means 
means lower confidence interval table vi get reasonably accu rate prediction makespan mean 
lower confidence significantly wider 
statistics empirical simulation data shown table vi user get estimate long application take execute sdsc platform variance expect 
example application tasks minutes length dedicated ghz host take minutes complete sdsc platform scheduled excl pred heuristic 
estimate take minutes longer confidence predicted mean 
makespan statistics task number mean std 
dev 
median min 
tasks makespan statistics task number mean std 
dev 
median min 
tasks makespan statistics task number mean std 
dev 
median min 
tasks table vi makespan statistics excl pred sdsc platform 
lower confidence intervals mean 
mean standard deviation median units seconds 
vi related vi task replication authors similar probabilistic model described section vi analyze various replication issues 
platform model study similar resources shared task preemption disallowed checkpointing supported 
application models similar model tightly coupled applications loosely coupled application consisted task parallel components barrier synchronization 
authors assume probability task completion follows geometric distribution 
despite similarities platform application models number important differences study 
results discrete time model unit time length task task began execution time fails task started time assumption ensure trial evenly spaced computing time task completion simplified 
assumption problematic places unrealistic constraint time required restart task execution 
particular case task failure model assumes expected time failure plus expected period unavailability equal task length entirely dependent task length rare improbable occurrence second difference study platform model assumes homogeneous environment study consider effect hosts different speeds replicating 
examines analytically costs executing task parallel ap plications desktop grid environments 
model assumes machine un available fixed number time units unit completed 
estimates execution time lower bounds 
believe assumption restrictive especially size availability intervals correlated time short availability interval cause task failure followed short availability interval 
studies task replication focused detecting errors ensuring correctness 
types security methods deployed current systems ad hoc fail proof 
example seti home recomputes tasks indicated positive signal dedicated machine prevent false positives 
example described author develops methods give probabilistic guarantees result correctness credibility metric worker 
results built dubious unsupported assumptions probabilities task result error rates 
numerous sources error hardware software malfunction malicious attacks creating probabilistic models error rates may possible 
vi checkpointing task checkpointing means dealing task failures task state stored periodically local disk remote checkpointing server event failure occurs application restarted checkpoint 
combination checkpointing process migration deal cpu unavailability better host available moving pro cess machine 
discussed earlier section vi remote checkpointing process migration infeasible internet environments application consume hundreds megabytes memory bandwidth internet limited 
heuristics evaluated traces gathered solely enterprise environments heuristics designed platform application models discussed section iv function internet environments 
design ing heuristics assume process migration capabilities longer applicable internet environments 
investigate effect local checkpointing application makespan 
specif ically assume excl pred heuristic replication enabled local checkpointing capabilities refer heuristic excl pred 
enable optimal scheduler checkpointing abilities refer resulting algorithm optimal 
assume checkpointing heuristic checkpoints half minutes cost checkpointing seconds 
assume cost restarting task checkpoint occurred seconds 
tried range values frequency cost checkpointing restart costs trends 
makespan sec excl pred dup excl pred optimal optimal task size min dedicated ghz vi performance checkpointing heuristics sdsc grid 
vi shows mean makespan applications tasks sizes ranging minutes executed sdsc platform 
executed applications sizes complete business hours 
addition plotting performance checkpoint enabled heuristics excl pred optimal plot performance excl pred dup optimal performance resulting optimal schedule comparison 
note optimal schedule platform checkpointing capabilities determined optimal different optimal schedule platform checkpointing enabled determined optimal 
example extremely long task optimal algorithm may able complete task optimal able series availability intervals little progress task execution lost host fails 
find excl pred performs times worse excl pred dup 
optimal performs slightly worse optimal task sizes ranging minutes task sizes larger minutes optimal outperforms optimal slightly 
poor performance excl pred due fact task reassigned assigned slow host host unavailable task execution 
host unavailable task execution typically unavailable long periods time relative execution time application 
particular mean length unavailability intervals sdsc lri deug ucb platforms minutes respectively 
result task exe cution delayed amount time required host available execution applications require rapid turnaround tal 
optimal performs nearly optimal omniscient scheduler avoid periods exec unavailability performs slightly worse tasks minutes length overheads involved checkpointing 
task sizes greater minutes optimal outperforms optimal checkpointing enabled costs restarting task scratch due exec unavailability higher overheads checkpointing 
local checkpointing possible find benefits limited short lived applications relatively long lengths unavailability intervals real desktop grid environments 
vi summary studied variety approaches improving performance means replication 
proactive reactive hybrid approaches approach examined issues task replicate host replicate replicate see table vi 
reactive replication strategy uses timeouts execution time task goes past predicted makespan surprisingly superior aggressive replication heuristics heuristics dynamic historical informa tion predict task completion rates 
explained fact large portion host platform stable clock rates correlation heuristic host task replicas pri cr dup clock rate clock rate excl dup excl pred dup excl pred dup clock rate clock rate excl pred dup clock rate clock rate excl pred dup clock rate clock rate excl pred clock rate timeout predicated makespan excl pred spd clock rate timeout clock rate rep prob ci ci table vi summary replication heuristics 
strongly task completion rates 
combining fact usually hosts relative number tasks near application execution shown excl pred demonstrates best performance terms reducing makespan waste 
vi shows mean makespan excl pred rep prob sdsc grid addition best performing heuristics examined previous chapters 
find sdsc grid fourth quartile excl pred average times shorter fourth quartile excl pred excl pred performs better excl pred factor average 
compared optimal schedule excl pred performs factor sdsc deug lri platforms factor ucb platform 
addition achieving best close best performance excl pred re sults close waste replication heuristics achieving mean waste sdsc deug lri ucb platforms re spectively large performance benefits reactive replication achieved little waste 
tasks application min tasks min tasks min tasks st nd rd th duration sec tasks application min tasks min tasks min tasks tasks application duration sec min tasks min tasks min tasks duration sec vi length task completion quartiles sdsc grid 
optimal 
fcfs 
pri cr 
excl 
excl pred excl pred 
rep prob 
chapter vii scheduler prototype chapter describe implementation best performing heuristic excl pred show scheduling model feasible real system 
implementation excl pred integrated open source xtremweb desktop grid software 
vii overview xtremweb scheduling system architecture xtremweb system matches general architecture desktop grid systems described section ii describe detail components xtremweb system reside application resource management level modify components scheduler 
application submitted application manager periodically selects subset tasks task pool distributes scheduler set sched 
scheduler responsible completion tasks 
workers request scheduler typically java rmi methods communication ssl tcp udp supported 
default scheduler xtremweb schedules tasks hosts fcfs fashion schedules tasks hosts order arrived 
completion worker return result scheduler stores result server disk records task completion results database 
vii excl pred heuristic design implemen tation replace fcfs scheduler xtremweb excl pred sched 
involves number changes xtremweb system describe 
vii task priority queue potential hazard replication replicas delay original task instances executed 
example suppose instances particular task replicated high number times placed queue 
suppose task instances different task placed queue instances task 
task instances assigned order placed queue second task starve workers kept busy executing replicas task 
reduce chance task starvation scheduler uses level queue refer higher level queue primary queue lower level queue secondary queue 
application submitted client instance task placed primary queue 
excl pred heuristic timeout associated original task instance scheduled host 
time expires task replica placed secondary queue 
doing task assignment scheduler schedule tasks primary queue secondary queue effort ensure instance task scheduled replicas 
keep number replicas growing rapidly original task instances allowed time 
original task instance fails new corresponding instance placed primary queue replica fails done 
task instance priority queues implemented fixed sized lists xtremweb scheduler lists act buffer database tasks workers requesting task instances 
periodically primary priority queue filled original task instances instantiated tasks database 
task state thread periodically checks state task instances priority queue 
particular state task state thread causes appropriate action taken 
example implement timeout mechanism set timeout task instance primary queue 
periodically thread checks state tasks task timed places replica secondary queue 
vii makespan predictor excl pred depends makespan prediction formula described section predict application makespan 
requires having predicted aggregate operations completed second 
rate determined submitting real measurement tasks consisting number operations task workers short duration 
instance xtremweb records start completion time task instance 
number operations task separate thread scheduler responsible computing daily average hosts 
counting number tasks completed application thread rough estimate application completion main scheduling thread estimate excl pred heuristic 
aggregate operations second remains relatively constant time see section estimate accurate days 
chapter viii viii summary contributions desktop grids attractive platform executing large computations cause offer high return investment idle capacity existing computing infrastructure 
projects wide range scientific domains utilized computing power offered hundreds thousands desktop pc applications projects high throughput task parallel com pute bound 
dissertation studied schedule application requires rapid turnaround effort broaden types applications executable desktop grid environments 
contributions measurement characterization real enterprise desktop grids 
char real desktop grid platforms accurate measurement technique captured performance exactly perceived real application 
measurement data characterized temporal structure availability platform individual hosts 
measurement data character ization drive simulations basis forming predicative explanatory generative models 
respect modelling number pertinent statistics 
instance task failure rate correlated task length availability correlated host clock rates 
resource prioritization exclusion heuristics 
characterization develop novel effective resource prioritization resource exclusion heuristics scheduling short lived applications 
static clock rate information prioritize hosts improve performance performance prioritization depends number tasks application relative number hosts tasks assigned poor hosts 
adapted prioritization heuristic exclude poor hosts application execution 
fixed threshold filter hosts beneficial application performance dependent distribution clock rates 
lessen dependence developed heuristic predicted makespan eliminate hosts application execution 
heuristic sensitive clock rate distribution aggressive exclusion smaller applications performed slightly poorly 
benefit predicted makespan eliminate hosts obvious combined heuristic task replication 
task replication heuristics 
studied proactive reactive hybrid replication techniques combining task replication best resource exclusion prioritization heuristics 
heuristic uses makespan predictor reactive replication means timeouts effective practice makespan predictor essential eliminating poor hosts setting timeouts task waste relatively low 
reason timeouts effective platforms large portion relatively stable hosts 
volatility negatively correlated clock rates best replication heuristic prioritizes tasks hosts clock rates probability failure reduced dramatically replicating task 
surprisingly heuristic achieves similar performance relatively waste compared heuristics replicate aggressively dynamic information resources 
best heuristic performs factor optimal 
scheduler prototype 
show feasibility heuristic implementing scheduler prototype real desktop grid system 
heuristic incorporated real open source desktop grid system xtremweb 
believe scheduler improve performance short lived applications 
viii number ways extended terms measurement characterization types applications studied characterization internet desktop grids 
designed heuristics applicable effective internet environments 
traces internet desktop grids prove heuristics effectiveness internet environments 
collection internet desktop grid trace data currently conducted recovery oriented computing group berkeley 
data able evaluate heuristics internet desktop grids 
characterization memory network connectivity 
clearly applications resources addition cpu 
extension characterization exec availability useful characterize resource usage data memory allocation network traffic 
improve accuracy platform model 
scheduling applications dependencies 
interesting class appli cations class dependencies tasks 
interesting characterization data study costs benefits running applications task de 
fact hosts desktop grid environments appear independent simplify performance modelling applications 
believe probabilistic model task completion described chapter vi aid analysis scheduling applications task dependencies 
scheduling multiple applications desktop grid 
desktop grid application luxury entire platform exclusively 
useful investigate scenario multiple applications competing set resources 
costs desktop resources users submit applications quite low applications large time may users require rapid application turnaround 
balance system throughput response time promoting fairness users interesting research direction 
performance excl pred dependent existence stable hosts platform scenario multiple application hosts applications rep prob heuristic may fact prove beneficial compared excl pred 
ends believe thesis helpful stepping stone desktop grid research 
appendix defining iqr factor iqr sensitivity tasks application min tasks min tasks min tasks slow host failed task laggers tasks application min tasks min tasks min tasks laggers tasks application min tasks min tasks min tasks laggers cause laggers iqr factor sdsc grid 
fcfs 
pri cr 
excl 
excl pred 
excl pred 
rep prob 
tasks application min tasks min tasks min tasks slow host failed task tasks application laggers min tasks min tasks min tasks tasks application laggers min tasks min tasks min tasks laggers cause laggers iqr factor sdsc grid 
fcfs 
pri cr 
excl 
excl pred 
excl pred 
rep prob 
appendix additional resource selection exclusion results discussion average makespan relative optimal number tasks application fcfs cr excl excl pred task length minutes dedicated ghz host performance resource selection heuristics deug grid ucb platform host clock rates identical 
excl exclude hosts results corresponding excl shown 
excl pred cr perform worse fcfs mainly prioritize resources average makespan relative optimal number tasks application fcfs cr excl excl pred task length minutes dedicated ghz host performance resource selection heuristics lri grid average makespan relative optimal number tasks application fcfs cr excl excl pred task length minutes dedicated ghz host performance resource selection heuristics ucb grid clock rates fcfs prioritizes resources time arrival queue 
fcfs tend assign tasks resources available longest tended longer probability task completion pri cr assigned tasks hosts randomly 
fcfs outperforms pri cr particular find platforms pri cr outperforms performs fcfs 
appendix additional task replication results discussion proactive replication average makespan relative optimal number tasks application fcfs pri cr pri cr dup excl excl dup excl pred excl pred dup task length minutes dedicated ghz host performance proactive replication heuristics deug grid 
similar replication heuristics described previous section excl pred dup time excl pred dup time spd wasteful re sources see 
reason tasks replicated regardless average makespan relative optimal number tasks application fcfs pri cr pri cr dup excl excl dup excl pred excl pred dup task length minutes dedicated ghz host performance proactive replication heuristics lri grid 
average makespan relative optimal number tasks application fcfs pri cr pri cr dup excl pred excl pred dup task length minutes dedicated ghz host performance proactive replication heuristics ucb grid 
speed reliability host original task instance assigned 
waste terms percent tasks replicated number tasks application optimal cr dup excl dup excl pred dup excl pred dup time excl pred dup time spd task length minutes dedicated ghz host waste proactive replication heuristics excl pred dup time excl dup time spd 
waste heuristic excl dup shown hosts ucb clock rates heuristic excluded hosts 
reactive replication hybrid replication waste terms percent tasks replicated number tasks application pri cr dup excl dup excl pred dup task length minutes dedicated ghz host waste proactive replication heuristics deug grid 
waste terms percent tasks replicated number tasks application pri cr dup excl dup excl pred dup task length minutes dedicated ghz host waste proactive replication heuristics lri grid 
waste terms percent tasks replicated number tasks application pri cr dup excl pred dup task length minutes dedicated ghz host waste proactive replication heuristics ucb grid 
average makespan relative optimal number tasks application optimal excl pred dup excl pred dup time excl pred dup time spd excl pred dup excl pred dup time excl pred dup time spd excl pred dup excl pred dup time excl pred dup time spd excl pred dup excl pred dup time excl pred dup time spd task length minutes dedicated ghz host performance proactive replication heuristics varying replication level sdsc grid 
average makespan relative optimal number tasks application fifo excl pred dup excl pred task length minutes dedicated ghz host performance reactive replication heuristics deug grid 
average makespan relative optimal number tasks application fcfs excl pred dup excl pred task length minutes dedicated ghz host performance reactive replication heuristics lri grid 
average makespan relative optimal number tasks application fcfs excl pred dup excl pred task length minutes dedicated ghz host performance reactive replication heuristics ucb grid 
waste terms percent tasks replicated number tasks application excl pred dup excl pred task length minutes dedicated ghz host waste reactive replication heuristics deug grid 
waste terms percent tasks replicated number tasks application excl pred dup excl pred task length minutes dedicated ghz host waste reactive replication heuristics lri grid 
waste terms percent tasks replicated number tasks application excl pred dup excl pred task length minutes dedicated ghz host waste reactive replication heuristics ucb grid 
average makespan relative optimal number tasks application fifo excl pred dup excl pred rep prob task length minutes dedicated ghz host performance hybrid replication heuristic deug grid 
average makespan relative optimal number tasks application fcfs excl pred dup excl pred rep prob task length minutes dedicated ghz host performance hybrid replication heuristic lri grid 
average makespan relative optimal number tasks application fcfs excl pred dup excl pred rep prob task length minutes dedicated ghz host performance hybrid replication heuristic ucb grid 
waste terms percent tasks replicated number tasks application excl pred dup excl pred rep prob task length minutes dedicated ghz host waste hybrid replication heuristic deug grid 
waste terms percent tasks replicated number tasks application excl pred dup excl pred rep prob task length minutes dedicated ghz host waste hybrid replication heuristic lri grid 
waste terms percent tasks replicated number tasks application excl pred dup excl pred rep prob task length minutes dedicated ghz host waste hybrid replication heuristic ucb grid 
makespan statistics task number mean std 
dev 
median min 
tasks makespan statistics task number mean std 
dev 
median min 
tasks makespan statistics task number mean std 
dev 
median min 
tasks table makespan statistics deug platform 
lower confidence intervals mean 
mean standard deviation median units seconds 
makespan statistics task number mean std 
dev 
median min 
tasks makespan statistics task number mean std 
dev 
median min 
tasks makespan statistics task number mean std 
dev 
median min 
tasks table makespan statistics lri platform 
lower confidence intervals mean 
mean standard deviation median units seconds 
makespan statistics task number mean std 
dev 
median min 
tasks makespan statistics task number mean std 
dev 
median min 
tasks makespan statistics task number mean std 
dev 
median min 
tasks table makespan statistics ucb platform 
lower confidence intervals mean 
mean standard deviation median units seconds 
bibliography cell computing 
www net 
net 
net 
distributed net 
www distributed net 
home 
einstein phys edu 
lhc home 
web cern ch 
ucla internet report surveying digital 
technical report ucla center communication policy january 
abramson giddy foster 
high performance parametric modeling nimrod killer application global grid proceedings international parallel distributed processing symposium may 
acharya saltz :10.1.1.22.4887
utility exploiting idle workstations parallel computation 
proceedings acm sigmetrics international conference measurement modeling computer systems pages 
amir wool 
evaluating quorum systems internet 
th symposium fault tolerant computing ftcs june 
anderson 
personal communication april 
anderson culler patterson 
case networks workstations 
ieee micro 
arpaci dusseau vahdat liu anderson patterson 
interaction parallel sequential workloads network workstations 
proceedings sigmetrics pages may 
baker koenig 
home pc portrait 
technical report pc data reston va 
berman wolski schopf shao 
application level scheduling distributed heterogeneous networks 
proc 
supercomputing pittsburgh 
bhagwan savage voelker :10.1.1.13.1523
understanding availability 
proceedings iptps 
bhagwan yu chung cheng stefan savage geoffrey voelker 
total recall system support automated availability management 
nsdi pages 
resource measurement 
roc cs berkeley edu projects 
bolosky douceur ely theimer 
feasibility serverless distributed file system deployed existing set desktop pcs 
proceedings sigmetrics 
herault performance evaluation sandboxing techniques peer peer computing 
technical report lri paris sud university february 
wolski 
quantifying machine availability networked desktop grid systems 
technical report cs dept computer science engineering university california santa barbara november 
calder chien wang yang 
entropia virtual machine desktop grids 
technical report cs university california san october 
calder chien wang yang 
entropia virtual machine desktop grids 
proceedings acm usenix conference virtual execution environments vee june 
compute cancer project 
www org 
christiansen schauser wu 
javelin internet parallel computing java 
proceedings sixth acm sigplan symposium principles practice parallel programming 
casanova berman 
heuristics scheduling parameter sweep applications grid environments 
proceedings th heterogeneous computing workshop hcw pages may 
casanova berman wolski 
apples parameter sweep template user level middleware grid 
proceedings supercomputing sc nov 
baumgartner 
organic grid self organizing computation peer peer network 
proceedings international conference autonomic computing may 
chien calder bhatia :10.1.1.12.8273
entropia architecture performance enterprise desktop grid system 
journal parallel distributed computing 
chien 
personal communication december 
chu levine 
availability locality measurements peerto peer file systems 
proceedings scalability traffic control ip networks july 
condor statistics 
www cs wisc edu condor map 
dinda 
statistical properties host load 
scientific programming 
dinda 
prediction real time scheduling advisor 
proceedings international parallel distributed processing symposium ipdps april 
dinda 
online prediction running time tasks 
cluster computing july 
dinda hallaron 
evaluation linear models host load prediction 
proceedings eighth ieee international symposium high performance distributed computing page 
fred douglis john ousterhout :10.1.1.14.7130
transparent process migration design alternatives sprite implementation 
software practice experience 
eri 
xtremweb generic global computing system 
proceedings ieee international symposium cluster computing grid ccgrid may 
fight aids home project 
www org 
berkeley open infrastructure network computing 

berkeley edu 
foster kesselman editors 
grid blueprint new computing infrastructure chapter chapter medical data federation biomedical informatics research network 
morgan kaufmann nd edition 
ian foster carl kesselman editors 
grid blueprint new computing infrastructure 
morgan kaufmann publishers san francisco usa 
james frey todd tannenbaum miron livny ian foster steven tuecke 
condor computation management agent multi institutional grids 
cluster computing 
leutenegger 
improving speedup response times replicating parallel programs snow 
proceedings th workshop job scheduling strategies parallel processing june 
great internet prime search gimps 
www mersenne org 
harchol balter downey 
exploiting process lifetime distributions dynamic load balancing 
proceedings acm sigmetrics international conference measurement modeling computer systems pages 
hsu 
personal communication march 

worm programs early experience distributed computation 
communications acm 
kee logothetis huang casanova andrew chien 
efficient resource description high quality selection virtual grids 
ieee conference cluster computing grid ccgrid 
larson snow shirts pande 
folding home genome home distributed computing tackle previously intractable problems computational biology 
computational genomics 
leutenegger sun 
distributed computing feasibility non dedicated homogeneous distributed system 
proc 
sc portland oregon 
li 
improving performance computational replication large scale computational grid 
proc 
ieee international symposium cluster computing grid ccgrid may 
litzkow livny mutka 
condor hunter idle workstations 
proceedings th international conference distributed computing systems icdcs 
lo zhou zappala liu zhao 
cluster computing fly scheduling idle cycles internet 
rd international workshop peer peer systems iptps feb 
neri thain livny 
xtremweb condor sharing resources internet connected condor pool 
proceedings ieee international symposium cluster computing grid ccgrid workshop global computing personal devices may 
nri cordier 
auger xtremweb monte carlo computation global computing platform 
proceedings computing high energy nuclear physics chep march 
long muir golding 
longitudinal survey internet host reliability 
th symposium reliable distributed systems pages 
lopez dinda lowekamp hallaron 
preliminary report design framework distributed visualization 
proceedings international conference parallel distributed processing techniques applications pdpta pages las vegas nv june 
feitelson 
workload parallel supercomputers modeling characteristics rigid jobs 
parallel distributed comput 
editors 
statistics engineering science 
prentice hall 
editor 
mitchell claudia science engineering indicators 
technical report national science board washington usa 
mutka 
considering deadline constraints allocating shared capacity private workstations 
int 
journal computer simulation 
mutka livny 
available capacity privately owned workstation environment performance evaluation july 
schopf editors 
grid resource management chapter 
kluwer press 
larry carter jeanne ferrante 
guard gossip autonomous resource detection 
ipdps 
wolski 
model checkpoint scheduling volatile resource environments 
technical report cs dept computer science engineering university california santa barbara 
olson 
personal communication april 
oppenheimer albrecht patterson vahdat 
distributed resource discovery planetlab sword 
proceedings acm usenix workshop real large distributed systems worlds december 
oppenheimer albrecht patterson vahdat 
design implementation tradeoffs wide area resource discovery 
th ieee symposium high performance distributed computing hpdc july 
pande 
personal communication december 
pruyne livny 
worldwide flock load sharing workstation clusters journal generations computer systems 
reuters 
worldwide pc seen rising slightly 
news cnet com investor news html february 
sean rhea patrick eaton dennis geels hakim weatherspoon ben zhao john kubiatowicz 
pond oceanstore prototype 
fast 
ryu 
exploiting idle cycles networks workstations 
phd thesis 

sabotage tolerance mechanisms volunteer computing systems 
proceedings ieee international symposium cluster computing grid may 

building studying web volunteer computing systems java 
generation computer systems 
saroiu gummadi gribble 
measurement study peer peer file sharing systems 
mmcn january 
serial section electron tomography method dimensional reconstruction large structures 
soto ge young sj tj lamont bo mh 
neuroimage june 
current statistics 
berkeley edu stats html 
technical news report 
ssl berkeley edu tech news html 
casanova berman 
tunable line parallel tomography 
proceedings supercomputing denver colorado nov 
son livny 
recovering internet symmetry distributed computing 
proceedings rd international symposium cluster computing grid tokyo japan may 
pande 
empirical force field assessment interplay backbone term scaling 
computational chemistry 
pande 
exploring helix coil transition atom equilibrium ensemble simulations 
biophysical journal 
spence harris 
distributed resource discovery open platform 
th ieee international symposium high performance distributed computing hpdc june 
ion stoica robert morris david karger frans kaashoek hari balakrishnan 
chord scalable peer peer lookup service internet applications 
proceedings acm sigcomm conference san diego california august 
sullivan bowyer cobb anderson 
new major seti project project data personal computers 
proc 
fifth intl 
conf 

charles brooks iii 
predictor home protein structure prediction supercomputer public resource computing 
ipdps 
david anderson pietro charles brooks iii 
homogeneous redundancy technique ensure integrity molecular simulation results public computing 
ipdps 
top list 
www top org sublist stats index php list type submit 
united devices www ud com 
vijay pande 
private communication 
wolski spring hayes 
predicting cpu availability time shared unix systems 
th ieee high performance distributed computing conference hpdc august 
wolski spring hayes 
network weather service distributed resource performance forecasting service metacomputing 
journal generation computing systems 
wyckoff johnson jeong 
finding idle periods networks workstations 
technical report cs dept computer science new york university march 
pande 
structural correspondence alpha helix random flight chain resolves unfolded proteins native properties 
nature structural biology 

