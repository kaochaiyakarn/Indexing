technical report university toronto csrg october density propagation continuous temporal chains generative discriminative models cristian sminchisescu allan jepson department computer science artificial intelligence lab university toronto jepson cs toronto edu www cs toronto edu jepson analyze non linear non gaussian temporal chain models dynamical systems having continuous hidden states non linear non gaussian dynamics observation models 
setting study discriminative generative models describe underlying independence assumptions give propagation rules filtering smoothing 
despite different graphical model structure independences motivation similar models infer dynamically varying hidden state sequences observations 
setting common solution inverse problems artificial intelligence computer vision speech control theory 
see companion papers demonstrations discriminative generative models human motion reconstruction monocular video applications 
keywords generative models discriminative models non linear systems variational approximation mixture models 
consider non linear non gaussian continous chain model dynamical system having temporal state prior observation model observations dynamics define conditional state distribution previous state current observation density reasons clear shortly 
model joint state estimated time series observations encoded wish compute quantities slice filtered conditionals smoothed conditionals filtered density written reasons tractability assume order markov property simplifies similarly smoothed distribution computed terms joint joint filtered distribution joint derived different choices density types generative discriminative models 
generative models generative continuous chain model non linear dynamical system 
generative continuous chain model non linear dynamical system described graphical model fig 

model observations conditional independent states different derivations density propagation generative models appeared 
filtering density filtering rule proof transforms generative density propagation conditionals proof follows directly uses bayes rule invert recursively propagate long time series approaches state equilibrium distribution conditional dynamics may approximately precomputed 
computation filtered posterior involves division distributions may complicate matters discussed 
note forcing discriminative style distribution note working wouldn model additional dependency structure graphical model implies contrary discriminative model case obvious meaning conditional predicting node parents 
interesting check propagation rules expressed terms distribution 
filtered density derived proof despite avoid modeling current observation previous state generative observation conditional 
smoothing joint distribution factorizes proof properties structure graphical model simplifying obtain smoothing proof follows directly 
discriminative models discriminative continuous chain model 
discriminative continuous chain model graphical structure shown fig 

properties easily verified visually fig 
bayes ball algorithm model observations marginally independent 
notice different generative chain model observations conditionally independent states 
filtering density propagation rule proof line note working appears natural ask derive filtering recursions separate conditionals discriminative chains explaining away effects prevent simple factorization separate observation dynamic state conditionals 
marginally independent conditionally dependent observing conditioning partial derivations possible satisfactory alternative derivation gives smoothing proof discriminative models longer windows observations model state conditioned window observations past 
simplicity running time indexes starting assume loss generality past observations available estimation starts 
filtering proof follows directly time longer range observation dependency account smoothing proof follows directly similarly 
mixture gaussian density propagation section give formulas density propagation case relevant distributions temporal prior local state conditionals represented mixtures 
cases treated similarly give formulas density propagation generative models separate dynamic observation discriminative style state conditionals complex compute 
focus gaussian component resulting distributions involved 
principle triples say upper bound number components distribution assume non linear regressor functions 
case may necessary statistical linearization order account uncertainty linear approximation matrices done unscented transform 
integral representing predicted distribution single mixture components gaussian gaussian numerator filtered distribution product gaussians product gaussians gaussian normalized normalization factor say dimensionality state denominator filtered distribution derived similarly 
distributions numerator components denominator components respectively seek component distribution parameterized approximates ratio equivalent minimizing divergence optimized gradient descent find 
binder murphy russell 
space efficient inference dynamic probabilistic networks 
international joint conference uncertainty artificial intelligence 
gordon salmond smith 
novel approach non linear non gaussian state estimation 
iee proc 

isard 
real valued graphical models computer vision 
ieee international conference computer vision pattern recognition 
isard blake 
condensation conditional density propagation visual tracking 
international journal computer vision 
jordan editor 
learning graphical models 
mit press 
kitagawa 
monte carlo filter smoother non gaussian nonlinear state space models 
comput 
graph 
statist 
merwe doucet freitas wan 
unscented particle filter 
technical report cued infeng tr cambridge university department engineering may 
sminchisescu jepson 
generative modeling continuous non linearly embedded visual inference 
international conference machine learning pages banff 
sminchisescu jepson 
variational mixture smoothing non linear dynamical systems 
ieee international conference computer vision pattern recognition volume pages washington 
sminchisescu li metaxas 
human motion reconstruction bayesian mixtures experts 
probabilistic discriminative approach 
technical report csrg university toronto october 
sudderth freeman 
non parametric belief propagation 
ieee international conference computer vision pattern recognition 

