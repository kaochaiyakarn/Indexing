boosting classifiers drifting concepts martin scholz ralf klinkenberg artificial intelligence group university dortmund dortmund germany scholz klinkenberg ls cs uni dortmund de www ai cs uni dortmund de 
proposes boosting method train classifier ensemble data streams 
naturally adapts concept drift allows quantify drift terms base learners 
algorithm empirically shown outperform learning algorithms ignore concept drift 
performs worse advanced adaptive time window example selection strategies store data suited mining massive streams 
machine learning methods applied problems data collected extended period time 
realworld applications introduces problem distribution underlying data change time 
knowledge discovery data mining time changing data streams concept drift handling data streams important topics machine learning community gained increased attention illustrated numerous conference tracks workshops topics specially dedicated journal issues see 
example companies collect increasing amount data sales figures customer data find patterns customer behavior predict sales 
customer behavior tends change time model underlying successful predictions adapted accordingly 
problem occurs information filtering adaptive classification documents respect particular user interest 
amount online information communication growing rapidly increasing need automatic information filtering 
information filtering techniques build personalized news filters learn news reading preferences user filter mail guide user search world wide web 
interest user concept underlying classification texts changes time 
filtering system able adapt concept changes 
machine learning approaches handling type concept drift shown outperform static approaches ignoring 
formalizing concept drift problem sec 
previous approaches handling reviewed sec 

sec 
discusses related ensemble methods data streams introduces boosting algorithm data streams naturally adapts concept drift sec 
approach evaluated real world datasets different simulated concept drift scenarios economic dataset exhibiting real concept drift 
sec 
summarizes results provides outlook 
concept drift problem definition study problem concept drift pattern recognition problem framework 
example consists feature vector label indicating classification 
data arrives time batches 
loss generality batches assumed equal size containing examples 
batch batch batch batch denotes th example batch batch data independently identically distributed respect distribution pi 
depending amount type concept drift preliminary short version ecml pkdd workshop knowledge discovery data streams 
fig 
example sliding time window stream text documents texts classified assumed arrive batches 
time point classification model learned data window consisting batches 
performance estimated available batch 
example distribution pi pi batches differ 
learner aims sequentially predict labels batch minimize cumulated number prediction errors 
batch learner subset training examples batches predict labels batch 
fig 
illustrates example application information filtering incoming text documents form stream documents classified interesting regard interest particular user 
documents assumed arrive batches correct classification labels documents batches known system 
sliding time window consisting batches batches train classifier applied classify documents newly arrived batch labels known system 
received labeled data batch assumed closely resemble current true concept 
adaptive time window approaches automatically optimizing size sliding time window dependent current amount drift model selection approaches adaptive amount drift labeled batch estimating performance prediction error learned models newly received data see sec 
sec 
details 
related concept drift machine learning changing drifting concepts handled time windows fixed adaptive size training data weighting data parts hypothesis age utility classification task 
approach weighting examples information filtering incremental relevance feedback approaches 
windows fixed size choice window size compromise fast adaptability small window generalization phases concept change large window 
fixed window size strong assumptions quickly concept changes 
basic idea adaptive window management adjust window size current extent concept drift 
heuristics adapt different speed amount drift involve parameters difficult tune 
task learning drifting time varying concepts studied computational learning theory 
learning changing concept infeasible restrictions imposed type admissible concept changes drifting concepts provably efficiently learnable certain concept classes rate extent drift limited particular ways 
helmbold long assume possibly permanent slow concept drift define extent drift probability subsequent concepts disagree randomly drawn example 
results include upper bound extend function randomly jumping values zero predicted learner accuracy 
drift maximally tolerable learner algorithms learn concepts drift certain constant extent drift 
furthermore show sufficient learner see fixed number examples 
window certain minimal fixed size allows learn concepts extent drift appropriately limited 
helmbold long restrict extend drift kuh petsche rivest determine maximal rate drift acceptable learner maximally acceptable frequency concept changes implies lower bound size fixed window time varying concept learnable similar lower bound helmbold long 
practice usually guaranteed application hand obeys restrictions reader electronic news may change interests arbitrarily radically 
furthermore large time window sizes theoretical results hold impractical 
application oriented approaches rely far smaller windows fixed size window adjustment heuristics allow far smaller window sizes usually perform better fixed larger windows see 
heuristics intuitive particular application domain usually require tuning parameters transferable domains lack proper theoretical foundation 
drifting concepts learned effectively efficiently little parameterization error minimization framework adaptive time windows example weighting selection 
framework support vector machines svms special properties allow efficient reliable error estimation single training run 
methods framework maintain adaptive time window training data select representative training examples weight training examples 
key idea automatically adjust window size example selection example weighting respectively estimated generalization error minimized 
approaches require complicated parameterization theoretically founded effective efficient practice 
described detail sec 
sec 
experiment reported comparative purposes 
related concept drift approaches ensemble learners described sec 

adaptive time windows adaptive time window approach experiments reported state art method comparisons automatically adjusts window size current extent concept drift selecting window size minimizes expected classification error new examples 
approach follows idea shown fig 
sec 

sliding time window consisting batches train classifier newly arriving unclassified examples 
point time new batch examples best window size determined new classifier learned examples window 
candidate time window sizes minimizing expected error chosen training classifier candidate window size estimating error rate resulting classifiers received labeled data examples batch assumed closely resemble current true concept 
simple concept drift scenario single abrupt concept shift batch fig 
illustrates selected best expected window size point time current batch long concept remains stable batch batch time window grows 
drift occurs batch old data longer representative new concept dropped window size reduced batch 
target concept remains stable time window keeps growing 
window adjustments error estimation minimization complicated domain dependent parameter tuning necessary 
fig 
adaptive time window sliding time window grows phases stable target concept batches cut abrupt concept shift batch 
batch selection previously described adaptive time window approach select set connected consecutive batches training set point time batch selection strategy selects batches sufficiently matching labeled batch individually flexible 
point time batch second state art method handling concept drift comparisons experiment section trains classifier batch newest labeled data available applies classifier data previous batch 
batch classifier produces error times high higher produced classifier training batch discarded training set data considered close current target concept 
batches batch form final training set final classifier new batch learned 
fig 
shows batches selected training final classifier point time current batch drift scenario target concept starts concept abruptly shifts concept batch batch abruptly shifts back concept 
illustration demonstrates batch selection strategy selects discards batches fitting current target concept individually example re fig 
batch selection batches fitting current target concept selected individually 
old data time concept change second change current target concept change 
adaptive time window approach batch selection strategy error minimization heuristics 
experiments reported previous factor acceptable error range simply set optimization parameter performed data sets 
results adaptive time window batch selection approaches reported svms base learner frameworks performance availability efficient error estimators 
performance classification learner estimated cross validation leave error estimation frameworks restricted svms base learner applied classification learner 
adapting ensemble methods drifting streams section presents novel ensemble method data streams appealing properties presence concept drift 
subsection motivates ensembles methods streaming data briefly reviews existing approaches 
idea ensemble method sketched subsection 
ensemble algorithm adapted streaming scenario concept drift 
ensemble methods data stream mining years algorithms specifically tailored mining data streams proposed 
goals algorithms vary depending assumed scenarios 
apart able cope concept drift scalability important issues 
large datasets induction classifiers decision trees efficiently possible sampling strategy hoeffding bounds vfdt algorithm efficiently induces decision tree constant time 
extended version algorithm updates tree time window fixed length allows compensate concept drift certain degree 
combining trees ensemble classifiers techniques bagging boosting shown significantly improve predictions datasets 
ensemble algorithms corresponding online variants data streams suggested see 
sea algorithm induces ensemble decision trees data streams explicitly addresses concept drift 
splits data batches fits decision tree batch 
predict label base models combined unweighted majority vote similar bagging 
soon number base models exceeds user specified constant models discarded heuristic approach 
authors report increase classification performance compared single decision tree learner state ensemble recovers concept drifts 
recovery time approach unnecessarily long exchanges model iteration uses confidence weights 
interesting approach unweighted base learners similar random forests 
exploits example selection include useful older data training set 
examples earlier batches included predicted correctly latest model assumed optimal 
cross validation experiments may cause learner discard old examples rely new model learned scratch 
allows adapt quickly sudden drift possible sea 
author points heuristic selection strategy 
disadvantages required assumption fixed marginal distribution fixed probability observe specific instance regardless label high computational costs different base learner 
theoretical analysis suggests weighted base learners preferable alternative domains concept drift 
analyzed algorithm steadily updates weights experts base models ensemble adds new expert time ensemble misclassifies example 
new experts start learn scratch weight reflects loss suffered ensemble current example 
experts continuously trained new examples 
main results reported theoretical interest original version requires maintain unreasonably large number experts efficient variants rely heuristics 
practically oriented addresses learning data streams similar fashion described 
approaches trains weighted fixed size committee incremental decision trees updated new example arrives 
point time performance base models estimated window fixed size 
poorly performing models discarded replaced new decision tree trained subsequently read examples 
disadvantage fixed number base classifier approach induce ensembles diverse base models boosting algorithms appropriately re weighting examples 
approach leads redundant ensembles examples weighted individually different base learners parts training sets identical 
updating incremental decision trees simultaneously may turn expensive concept drifts 
expected outperform adapting time window approaches implicitly maintains heuristically derived weights examples 
approach reads training data batches trains classifier batch 
stationary phases drift classifier trained single batch 
consequently large batch sizes required 
performance classifier estimated just classified batch iteration inverse estimated error rate weight model 
authors prove estimates precise weighting scheme outperforms single classifier trained data concept drift 
result surprising especially compared boosting improvement accuracy expected additional base model exact estimates provided 
approach weight examples done boosting procedures 
consequence introducing diversity ensembles possible applying heuristics pruning procedure aspect similar sea 
sec 
extends efficient boosting procedure sec 
streams 
trades predictive performance versus scalability 
online algorithm reads examples aggregated batches decides batch add new expert ensemble 
sea similar algorithms base models ensemble combined weighted majority vote 
subsequent models trained re weighting examples new batch new base classifier model assigned weight depends performance performance remaining ensemble 
adaptation concept drift works continuously re estimating weights ensemble members similar procedure weight fit residuals weighted ensemble previous base models 
aspect similar logistic regression predictions base models acting constructed features 
ensemble generation knowledge sampling motivation subsequently knowledge sampling example weighting technique fig 
illustrates main idea simplified concept drift scenario 
underlying assumption concept drift examples sampled mixture distribution thought weighted combination pure distributions characterizing target concepts drift 
initial target concept simply referred concept 
examples sampled corresponding stationary distribution dotted vertical line 
learning algorithm may simply induce model data predict concept 
drift starts concept overlaps concept model show decreasing accuracy 
please note intermediate batches label best described probabilistic combination different concepts 
pure concepts label depends features deterministically perfect model concepts derived terms bayes optimal decision rule 
simplicity assume concepts correctly trained model concept drift starts 
situation point time batch bayes optimal classifier predicts concept 
concluded assumed generative model conflict concepts concept correct 
point knowing concept useless making predictions shown thick dotted vertical line fig 

point optimal purely predict concept 
just clear induce appropriate model seeing batches sampled pure corresponding distribution 
batches available learner dotted vertical line second line predict concept 
main motivations boosting algorithm accurate model con fig 
continuous concept drift starting pure concept pure concept 
target distribution probabilistic mixture 
optimal predict concept dotted line concept 
cept allows decompose mixture distribution concept drift 
possible construct sample respect concept soon drift starts dotted line 
look ahead strategy inherently different approaches discussed sec 
allows adapt drift quickly 
main reason exploits information encoded stream 
please note thick final dotted lines model concept useless helps purify subsequent batches subtracting deprecated concept 
resulting model probabilistic ensemble classifier bayes optimal decision rule explicitly applied crisp predictions required 
algorithm introduced sampling strategy suggested 
patterns discovered iteratively 
pattern iteration extends user prior knowledge 
iteration sampling procedure produces training sets orthogonal combined probability estimate corresponding prior knowledge 
aspect close boosting classifiers 
idea removing prior knowledge biased sampling formulated terms constraints 
formally step defines new distribution close original function possible orthogonal estimates produced available prior knowledge 
technically step realized introducing example weights 
result switching distributions evaluation metrics model candidates applied kinds training sets blinded regarding parts data concluded prior knowledge 
accounted iteration unexpected component model 
scope kind prior knowledge base models yielded preceding iterations 
instance space nominal class attribute examples expected sampled initial distribution ir denote base model hypothesis space predicting class 
constraint new distribution constructed longer support knowledge encoded hypothesis means respect observation fixed label independent possible predictions px px eq 
hold hypothesis allow derive information true label conditional distribution prediction second constraint probability observing specific class probability specific prediction change sufficient possible remove correlation true label predicted label px px px px eq 
ensures class skew change result implicitly altered cost model misclassifying exam ples 
eq 
avoids skew marginal distribution unnecessarily 
partition sharing predicted label true class new distribution defined proportionally initial having hypothesis prior knowledge instances partition indistinguishable 
changes conditional probabilities partition prefer instance despite equivalence respect available prior knowledge 
translates constraint px px constraints induce unique target distribution 
definition eases notation 
definition 
lift hypothesis predicted class true class label defined lift px px px similar precision lift measures correlation specific prediction specified true label 
value larger indicates positive correlation 
theorem 
initial distribution hypothesis constraints equivalent pd pd lift proof 
theorem defines new distribution sample hypothesis prior knowledge 
assuming single hypothesis restrictive possible directly incorporate new base model single ensemble classifier 
classical learning scenario concept drift 
initialize uniform distribution example set 
xm ym 
user number iterations call di find accurate model hi 
compute lift hi applying definition 
di xj yj di xj yj lift xj yj xj yj 
output 
hn lift values 
predict eq 

fig 
algorithm kbs knowledge sampling theorem directly applied iteratively base model hi selected distribution di 
distribution di defined applying theorem di hi 
corresponding knowledgebased sampling algorithm kbs tailored learning streams depicted fig 

boosts weak base learners empirically shown competitive adaboost logitboost 
inverse re weighting strategy allows approximately reconstruct original distribution combination single hypotheses 
formula estimates odds hn sequence hypotheses result separate iteration learning lift hi di hi lift hi di hi allows compute estimates conditional probabilities 
re weighting scheme kbs base classifiers rank models contribution accuracy kbs samples model accuracy overlapping correlated models respect new distribution reduced degree overlap 
constraint lift subset common prediction 
model predictive accuracy acc rewritten acc lift linear combination corresponding lift values covered subsets 
examples re weighted respect model base classifier favors models independent contributions 
similar boosting approaches example weights anticipate expectation previously trained models 
motivated especially useful handling smooth concept drifts 
sudden drifts require quick detection way rapidly adjust working hypothesis smooth drifts better collect information new target concept period time 
especially preceding concept identified accurately point time drift starts removing knowledge current concept data allows decompose mixture distributions required 
kbs strategy learn drifting concepts data streams original kbs algorithm fig 
assumes complete training set available main memory 
step adopt data streams read classify examples iteratively 
subsequent learning steps re weighting strategy kbs allows compute example weights efficiently 
data assumed arrive batches large train initial version base classifier 
sizes training sets effectively model determined dynamically algorithm 
processing new batch yields ensemble variants 
variant appends current batch cache training iteration refines latest base model accordingly 
second variant adds new model trained latest batch 
ensemble variant performing better batch kept 
initialize empty ensemble 
stream 
read batch ek iteration 
predict ek current ensemble eq 


read true labels ek 

alternative ensemble exists compare accuracy wrt 
ek 
better ensemble discard worse ensemble 
discarded ek shrink cache batch 

initialize uniform distribution ek 


apply hi predictions ek 
recompute lift ek def 

update lifts hi 
di xj yj di xj yj lift xj yj 
call ek get new model 
compute lift def 


add model lifts ensemble 
batch ek alternative ensemble 

ek extend cache batch clone discard base model repeat steps ek fig 
algorithm kbs stream xj yj ek 
strategy serves purposes 
stationary distributions new model trained empirical evidence increases accuracy resulting ensemble 
generally happen learning curve latest model leveled see data set suited boosting 
second sudden concept drift concept shift occurs estimation procedure instantly suggests add new model help overcome drift 
second step adopting kbs data streams foresee re computation phase base model performances updated respect current distribution 
fact believe main advantage weighted ensembles concept drift scenario 
stationary distributions weights vary marginally smoothly drifting scenarios systematically shifted allow quantify interpret drift terms previously patterns models 
discussed sec 

sudden drifts pose problem automatically result radically reduced weights previously trained models high weights subsequently trained models parameters re estimated new data 
response time drifts short 
streaming variant kbs closely coupled accurate kbs boosting algorithm predictive performance expected outperform single base models datasets 
pruning ensembles efficiently addressed weight re computation model reach fixed minimum advantage random guessing latest batch discarded 
natural common pruning strategy boosting algorithms weka implementation adaboost 
algorithm depicted fig 

loops stream ends 
lines apply current ensemble new batch knowing correct labels 
lines check continuing training latest model latest batch outperforms adding new model trained batch better ensembles kept 
lines recompute lift parameters base models 
models iteratively applied new batch weights adjusted similarly learning phase 
lines train variants ensemble extending cache updating newest model appropriately adds new model trained newest data 
degree freedom left line algorithm may classify new batch performance unknown time 
experiments variants implemented 
uses ensemble models trained larger batches generally reliable 
second variant uses hold set batch decide ensemble pseudocode assume incremental base learner trains new models cached data 
incremental base learners cache required 

alternatively perform reliable costly cross validation experiments apply estimator support vector machines efficient validation approach suggested 
experiments errors due hold estimation negligible compared systematic batch delay distributions training application 
reason currently address kind validation 
incremental base learners latest batch needs stored 
runtime dominated adjusting model data applying base models 
avoids combinatorial explosion memory requirements advanced time windowing batch selection techniques respectively see sec 

incremental variants exist popular learning algorithms particular decision trees support vector machines 
quantifying concept drift appropriate combination base classifiers allows increase predictive accuracy achieved average single classifier 
disadvantage results lose interpretability certain extent 
principle similar argument applies context concept drift interesting see proposed technique allows extract different kind information setting allows track kind drift underlying data stream analyzing weights individual base learners 
please recall methods continuously retrain models kbs algorithm freezes models latest 
weights frozen models re estimated continuously applying chronological order current batch weighting examples accordingly estimating lift values models weighted examples 
exactly procedure training phase exception reported sec 

extreme case batch size chosen small algorithm 
significant deviation initial model weights indicates corresponding change underlying distribution 
advantage lifts performance estimates corresponding weight vectors clear semantics different points time comparable additional artificial normalization 
weight latest model comparable different iterations continuously refined boosting procedure 
model weights re estimated chronological order effects remaining ensemble 
idea drift quantification illustrated scenario sketched fig 
discussed sec 
motivation knowledge sampling approach overcome concept drift 
simplicity assume base learner support vector machine continuously improves additional training data benefit boosting 
stationary distribution drift kbs algorithm fits single model training data 
model capture deterministic relation features label 
may perform differently predicting positive negative label lifts vary just marginally batch batch underlying distribution change 
drift starts significant change distribution model perform worse lift ratios positive negative slowly approach 
interestingly re weighted batch suddenly allows fit separate model model higher estimated accuracy 
model frozen estimates continuously updated 
drift takes batches new model reflect minor effects data 
small lifts second model estimated re weighted batch cause unbalanced ensemble models having higher impact 
second model refined drift reflects increasingly important aspect data model loses accuracy batch batch 
consequently lift ratios importance model decreased kbs algorithm favor second 
model concept drift weight model weight model fig 
idealistic change model weights time 
solid line depicts drift initial new target concept 
dotted lines show base learner weights reflect presence represented target concepts 
useless drift significant advantage random guessing lift discarded automatically 
case different target concepts overlap 
fig 
depicts ideal change ensemble weights time described scenario 
drift starts base model assumed accurate consequently receives high weight 
weight continuously changed estimates current accuracy 
weight decreases importance weight model increases 
ideal situation accuracy reflected maximum dotted lines optimal respect bayes rule 
sec 
reports corresponding results experiments real world data 
experiments experimental setup evaluation scheme order evaluate kbs learning approach drifting concepts compared adaptive time window approach batch selection strategy simple non adaptive data management approaches 
full memory learner generates classification model previously seen examples forget old examples 
memory learner induces hypothesis batch 
corresponds window fixed size batch 
window fixed size time window fixed size batches training data 
adaptive window window adjustment algorithm adapts window size current concept drift situation see sec 

batch selection batches producing error twice estimated error newest batch applied model learned newest batch selected final training set 
examples see sec 

performance classifiers measured prediction error 
results reported sec 
simulated concept drift scenarios real world data averaged runs different random ordering examples stream 
results reported sec 
single run examples taken real order artificial concept drift simulated imposed real concept drift real world data set 
experiments conducted machine learning environment yale svm implementation learners weka toolbox support vector machine smo svm decision tree learner meta learner adaboost provided weka 
evaluation simulated concept drifts trec data set experiments performed information filtering domain typical application area machine learning methods able handle drifting concepts 
text documents represented attribute value vectors bag words model relevance topic batch 
scenario scenario scenario fig 
relevance topic concept time concept change scenarios respectively 
relevance second relevant topic concept relevance topic 
distinct word corresponds feature value ltc tf idf weight word document 
experiments subset documents data set text retrieval conference trec 
real world business news texts assigned categories considered 
concept change scenarios simulated experimental set 
texts randomly ordered stream split batches equal size containing documents 
scenarios document considered relevant certain point time matches interest simulated user time 
user interest changes topics documents remaining topics relevant 
fig 
shows probability relevant document category batch scenarios implies probability second relevant topic 
scenario abrupt concept shift second topic batch 
scenario full fixed adaptive batch kbs kbs memory memory size size selection stream hold 


table 
error time window example selection methods vs kbs 
user interest changes slowly batch batch 
scenario simulates abrupt concept shift user interest second topic batch back batch 
tab 
compares results static adaptive time window batch selection approaches scenarios terms prediction error variants kbs 
results averaged runs different random orderings examples 
cases learning algorithm support vector machine svm linear kernel 
kbs algorithm manages adapt kinds concept drift 
tracking ensembles revealed distributions current model continuously refined 
concept shift scenario new model trained old model received significantly lower weight 
discarded helped identify topics irrelevant 
hold set helped identify better ensembles reliably classification time 
scenario models trained 
ensemble accurately adopted drift classification time systematic batch delay hold estimate misleading 
scenario full memory approach competitive batch selection scenario 
hold set kbs applies model iteration long concept shift 
delay increases error rate 
problem circumvented hold set 
essence kbs algorithm performed domain outperformed computationally expensive approaches 
scenario batch selection method clearly superior probably method able error rate batch adaptive window memory fixed size full memory fig 
trec data scenario error rate time non adaptive methods versus adaptive time window approach 
concatenate data second concept shift single training set 
tab 
lists error rates different learning strategies averaged time batches repeated runs experiments figures show error rates different learning strategies time batch averaged runs 
fig 
compares non adaptive methods adaptive time window approach concept drift scenario learning available labeled data ignoring possible concept drift may happened full memory leads generalization consequently low error rates long concept drift occurs 
soon concept drift occurs error rate goes slowly decreases old data longer representative current target concept part training set hinders effective learning 
opposite approach storing old data labeled batch memory allows maximally fast adap error rate batch adaptive window batch selection kbs stream kbs hold fig 
trec data scenario error rate time adaptive time window batch selection techniques versus kbs variants 
tation concept drift correspondingly quick recovery error rate 
baseline error second simple strategy phases concept drift comparatively high twice high strategies averaged error rate listed tab 
favorable 
sliding time window fixed size means compromise extremes acceptable baseline error better recovery speed full memory method comparison extremes shows performance static window approach compromise trade adaptability phases concept drift low error rate stable phases leaves lot potential improvements adaptive strategies 
described behavior non adaptive methods explains high error rates tab 
motivates adaptive approaches handling concept drift 
error rate batch adaptive window batch selection kbs stream kbs hold fig 
trec data scenario error rate time adaptive time window batch selection techniques versus kbs variants 
adaptive window approach able combine generalization performance full memory method stable phases concept drift keeping representative data possible fast adaptability memory method dropping misleading old data immediately drift occurs 
adaptive time window manages combine advantages static extremes adapting current extent drift 
fig 
compares adaptive time window batch selection strategies variants kbs concept drift scenario adaptive time window batch selection kbs variants achieve low baseline error rates adapt quickly concept drift 
hold set allows kbs quicker adapt drift consequently quicker recovery error rate 
concept drift scenario fig 
shows variants kbs exhibit similar behavior terms error rate time adaptive time window batch selection strate error rate batch adaptive window batch selection kbs stream kbs hold fig 
trec data scenario error rate time adaptive time window batch selection techniques versus kbs variants 
gies far low base line error adaptability drift concerned 
concept drift scenario depicted fig 
applies kbs uses hold set kbs adapt quickly hold set 
scenario re occurring target concept batch selection strategy advantage able re old data previous concept drifts quickly slightly outperforms approaches 
scenario scenarios kbs performs competitively better shown behavior time plots average error rate tab 

evaluation simulated drifts satellite image data second set experiments satellite image dataset uci library real world dataset classification 
contains known drift time simulated concept drifts technique described sec 
data adaboost kbs kbs fixed memory full memory full memory stream hold 


table 
averaged prediction errors satellite image dataset 
set randomly ordered stream split batches equal size examples batch grey soil damp grey soil classes selected relevant 
drift scenarios sec 
simulated selected classes corresponded selected topics trec experiments 
results averaged runs different random orderings examples 
decision trees typical base learner ensemble methods chose algorithm weka toolbox base learner experiments data set 
compared kbs non adaptive fixed size window batches full memory strategies 
addition running stand run adaboost top 
runs default settings learners 
results listed tab 

experiments trec data results kbs improved hold set 
scenario tackled full memory approach exploited adaboost 
scenarios kbs better fixed size window learner better full memory approach 
handling real drift economic real world data predicting phases business cycles third evaluation domain task economics real world data exhibiting real concept drift 
quarterly data describes west german business cycles 
examples described indicator variables 
task predict current phase business cycle west german economy 
accordance findings theis full fixed adaptive batch kbs kbs memory memory size size selection stream hold batches batches table 
prediction error business cycle data 
phases description business data described 
experiments compare performance kbs data stream algorithm previously reported results number batches 
timely order examples quarters preserved artificial concept drift simulated 
results single run averaged runs previous sections examples taken real order artificial concept drift imposed real concept drift real world data set demonstrated previous experiments 
approaches compared support vector machines svms base learners 
results evaluations shown tab 

column fixed time window approach lists results fixed size performed best 
fact approach performs may due cyclic nature domain 
size generally known advance shown fixed window sizes leads significant drops performance 
results batches shows kbs perform batch consists dozen examples 
reason possible get reliable probability estimates small data sets 
algorithm cache older data cases reasonable choose larger batch sizes 
just batches examples improves situation kbs performs similar fixed size adaptive size batch selection approach 
hold set turns surprisingly effective larger batches 
result provides evidence kbs able adapt classifier ensembles different kinds concept drift real world datasets 
empirical drift quantification final experiments article investigate claims sec 
realistic practice 
examples extracted real experiments trec data illustrate kbs base model weights allow characterize kind intensity concept drifts practice 
sophisticated methods pruning model evaluation learning applied order falsify results 
experiments lift values models making boolean predictions reduced single weight model 
model estimates odds ratios described eq 
transformed classifier form sign offset weight model weights wn ir 
weight vectors re weighted figures order ease comparison model impacts iteration 
transformation works considering lift prior ratios results weights model offset term ii transforming corresponding bayes optimal decision function applying logarithm results linear model iii centering weights model shifting offsets constant model independent term single weight suffices 
fig 
shows weights base classifiers kbs application time 
algorithm applied trec dataset simulated concept shift scenario support vector machine linear kernel base learner 
base models trained period time weights adjusted 
performance initial model directly estimated unweighted batch 
model weights upper bounded artificially ease visualization 
concept shift occurs middle model refined extending training set batch batch 
way model reaches high confidence varies bit due estimates small batches examples 
batch sampled new distribution decreases weight initial classifier rapidly 
classifier frozen kbs introduces second classifier refined iterations 
model turns useful negative weight indicates opposite initial target concept correlated new target concept 
precise weights models vary bit converge 
refining second model examples longer improve accuracy point kbs estimates freeze second model introduces third 
step allows increase expressiveness underlying model language promising 
second experiment provides realistic counterpart motivating example slow concept drift scenario sec 

fig 
shows weights involved base models change time 
just outlier model directly removed ensemble kbs algorithm induction removed order overload 
initial model reaches high weight stationary phase reflects highly confident predictions 
confidence decreases rapidly drift batches sampled new stationary target distribution initial model discarded learner batch 
new models introduced drift quickly lose weight target concept diminishes 
please recall kbs re weights batches sampled pure target distribution new concept 
sense early batches drift considered higher noise level ones explains decreasing weights 
learner fits classifier couple consecutive batches drift 
reaching new stationary distribution weights intermediate models converge contain fixed amount information new target concept 
final model weights base learners ensemble initial base model model shift final base model current batch fig 
base model weights kbs ensemble simulated scenario trec data 
model refined frozen just continuously re weighted respect latest batch 
concept shift occurs weight initial model drops drastically batch 
induced drift ends point previous model weights converged 
curves simple ideal case sketched earlier example illustrates weights base learners identify kind degree concept drift underlying data stream 
expect higher robustness sketched quantification property batch size estimating base model performances increases 
presents new ensemble method learning data streams 
iteration base models induced re weighted continuously considering latest batch examples 
ensemble methods proposed strategy adapts early quickly different kinds concept drift 
algorithm low computational costs 
empirically weights base learners ensemble initial base model model frozen batch model frozen batch final base model current batch fig 
base model weights kbs ensemble trained simulated concept drift scenario trec data 
shown competitive outperform sophisticated adaptive window batch selection strategies 
advantage allows track kind extent concept drift 
interesting directions evaluations precise robust strategies estimate model weights comparisons different pruning techniques 
addition alternatives refine latest model add new ensemble variants learning scratch batches continuously evaluated parallel 
allow replace complex ensemble single equally performing base model stationary phases 
precise model weights gained cross validation similar techniques drift quantification property kbs extended predict kind drift expected near 
approaches linearly extrapolating observed drift compared complex ones meta learning 
comparison proposed techniques classifier induction presence concept drift standardized simulated drift scenarios desirable 
supported deutsche forschungsgemeinschaft dfg collaborative research center reduction complexity multivariate data structures sfb university dortmund germany 

jesus aguilar ruiz paul cohen 
acm symposium applied computing sac special track data streams 
haddad andrea omicini roger wainwright lorie editors proceedings acm symposium applied computing sac nicosia cyprus march 
www informatik uni trier de ley db conf sac sac html 

jesus aguilar ruiz francisco ferrer 
acm symposium applied computing sac special track data streams 
proceedings acm symposium applied computing sac 

jesus aguilar ruiz joao gama editors 
second international workshop knowledge discovery data streams porto portugal october th 
conjunction ecml pkdd th european conference machine learning ecml th european conference principles practice knowledge discovery databases pkdd 
www pt 

james allan 
incremental relevance feedback information filtering 
frei editor proc 
th annual acm sigir conf 
research development information retrieval sigir rich swiss august pages new york ny usa 
acm press 

marko balabanovic 
adaptive web page recommendation service 
johnson editor proc 
int conf 
autonomous agents pages new york ny usa 
acm press 

blake merz 
uci repository machine learning databases 
dept information computer sciences university california irvine uci irvine ca usa 
www ics uci edu mlearn mlrepository html 

leo breiman 
bagging predictors 
machine learning 

leo breiman 
random forests 
machine learning 

william cohen 
learning rules classify mail 
proceedings aaai spring symposium machine learning information access stanford ca usa 
aaai press 

pedro domingos geoff hulten 
mining high speed data streams 
proceedings th acm sigkdd international conference knowledge discovery data mining kdd pages 

wei fan 
systematic data selection mine concept drifting data streams 
proceedings th acm sigkdd international conference knowledge discovery data mining kdd pages seattle wa usa 
acm press 

simon fischer ralf klinkenberg ingo oliver 
yale learning environment tutorial 
technical report ci collaborative research center university dortmund dortmund germany june 
issn 
yale sf net 
yoav freund robert schapire 
decision theoretic generalization line learning application boosting 
journal computer system sciences 

johannes rnkranz peter flach 
roc rule learning better understanding covering algorithms 
machine learning 

joao gama jesus aguilar ruiz editors 
international workshop knowledge discovery data streams pisa italy september th 
conjunction ecml pkdd th european conference machine learning ecml th european conference principles practice knowledge discovery databases pkdd 
www lsi es aguilar ecml 

heinz josef nch 
classification west german business cycles 
technical report collaborative research center reduction complexity multivariate data sfb university dortmund dortmund germany 

david helmbold philip long 
tracking drifting concepts random examples 
leslie valiant manfred warmuth editors proceedings fourth annual workshop computational learning theory colt pages san mateo ca usa 
morgan kaufmann 

david helmbold philip long 
tracking drifting concepts minimizing disagreements 
machine learning 

geoff hulten laurie spencer pedro domingos 
mining time changing data streams 
proceedings th acm sigkdd international conference knowledge discovery data mining kdd pages 

thorsten joachims 
estimating generalization performance svm efficiently 
pat langley editor proceedings international conference machine learning pages san francisco ca usa 
morgan kaufman 

thorsten joachims dayne freitag tom mitchell 
webwatcher tour guide world wide web 
proceedings international joint conference artificial intelligence ijcai volume pages 
morgan kaufmann 

george john pat langley 
static versus dynamic sampling data mining 
proceedings second international conference knowledge discovery databases data mining 

ralf klinkenberg 
zum bei sich ver 
master thesis computer science department university dortmund dortmund germany february 
www ai cs uni dortmund de klinkenberg ps gz 

ralf klinkenberg 
predicting phases business cycles concept drift 
andreas hotho gerd stumme editors der gi workshop lernen wissen proceedings workshop week teaching learning knowledge adaptivity national german computer science society gi annual workshop machine learning pages karlsruhe germany october 
km aifb uni karlsruhe de ws final klinkenberg pdf 

ralf klinkenberg 
learning drifting concepts example selection vs example weighting 
intelligent data analysis ida special issue incremental learning systems capable dealing concept drift may 

ralf klinkenberg thorsten joachims 
detecting concept drift support vector machines 
pat langley editor proceedings seventeenth international conference machine learning icml pages san francisco ca usa 
morgan kaufmann 
www ai cs de klinkenberg joachims ps gz 

ralf klinkenberg ingrid renz 
adaptive information filtering learning presence concept drifts 
sahami craven joachims mccallum editors workshop notes icml aaai workshop learning text categorization pages menlo park ca usa 
aaai press 
www ai cs de klinkenberg renz ps gz 

ralf klinkenberg stefan ping 
concept drift importance examples 
rgen franke ingrid renz editors text mining theoretical aspects applications pages 
physica verlag berlin germany 

jeremy marcus 
additive expert ensembles cope concept drift 
proceedings nd international conference machine learning icml pages new york ny usa 
acm press 

miroslav kubat joao gama paul utgoff 
intelligent data analysis ida journal special issue incremental learning systems capable dealing concept drift vol 


kuh petsche rivest 
learning time varying concepts 
advances neural information processing systems volume pages san mateo ca usa 
morgan kaufmann 

gerhard 
und zur data mining 
master thesis fachbereich informatik universit ulm germany june 

ken lang 
newsweeder learning filter netnews 
proceedings twelfth international conference machine learning icml pages san francisco ca usa 
morgan kaufmann 

carsten 
dynamic neural classification 
master thesis fachbereich informatik universit braunschweig germany october 

herbert lee clyde 
lossless online bayesian bagging 
journal machine learning research february 

tom mitchell rich caruana dayne freitag john mcdermott david zabowski 
experience learning personal assistant 
communications acm cacm july 

morik stefan ping 
multistrategy approach classification phases business cycles 
elomaa heikki mannila hannu toivonen editors machine learning ecml volume lecture notes artificial intelligence pages berlin 
springer 

charles taylor editors 
workshop notes dynamically changing domains theory revision context dependence issues th european conf 
machine learning ecml prague czech republic april 

stuart russell 
online bagging boosting 
eighth international workshop artificial intelligence statistics key west florida usa 

oliver ralf klinkenberg simon fischer ingo sven 
yale machine learning environment 
ralf klinkenberg stefan ping andreas nicola henze christian herzog ralf molitor olaf schr der editors der gi workshop lernen wissen number des informatik universit dortmund pages dortmund germany october 
issn 
yale sf net 

stefan ping 
manual 
universit dortmund lehrstuhl informatik viii 
www ai cs uni dortmund de software 

stefan ping 
incremental learning support vector machines 
nick cercone lin wu editors proceedings ieee international conference data mining icdm pages 
ieee 

salton buckley 
term weighting approaches automatic text retrieval 
information processing management 

martin scholz 
comparing knowledge sampling boosting 
technical report collaborative research center reduction complexity multivariate data structures sfb university dortmund dortmund germany 

martin scholz 
knowledge sampling subgroup discovery 
morik jean francois arno siebes editors local pattern detection volume lnai lecture notes artificial intelligence pages 
springer 

martin scholz 
sampling sequential subgroup mining 
grossman bayardo bennett vaidya editors proceedings th acm sigkdd international conference knowledge discovery data mining kdd pages chicago illinois usa august 
acm press 

martin scholz ralf klinkenberg 
ensemble classifier drifting concepts 
gama aguilar ruiz editors proceedings second international workshop knowledge discovery data streams pages porto portugal october 
conjunction ecml pkdd th european conference machine learning ecml th european conference principles practice knowledge discovery databases pkdd 
www pt 

kenneth stanley 
learning concept drift committee decision trees 
technical report ai department computer sciences university texas austin austin tx usa 

nick street kim 
streaming ensemble algorithm sea large scale classification 
proceedings th acm sigkdd international conference knowledge discovery data mining kdd pages 

charles taylor carsten 
structural change classification 
taylor editors workshop notes dynamically changing domains theory revision context dependence issues th european conf 
machine learning ecml prague czech republic pages april 

theis claus 
clustering techniques detection business cycles 
technical report collaborative research center reduction complexity multivariate data structures sfb university dortmund dortmund germany 

utgoff 
incremental induction decision trees 
machine learning 

georg 
einsatz eines zur eines pers 
master thesis computer science department university dortmund dortmund germany may 

wang wei fan philip yu jiawei han 
mining concept drifting data streams ensemble classifiers 
getoor senator domingos faloutsos editors proceedings th acm sigkdd international conference knowledge discovery data mining kdd pages washington dc usa 
acm press 

gerhard widmer miroslav kubat 
learning presence concept drift hidden contexts 
machine learning 

ian witten eibe frank 
data mining practical machine learning tools techniques java implementations 
morgan kaufmann 

