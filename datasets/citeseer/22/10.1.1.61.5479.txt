journal machine learning research submitted revised published walk sums belief propagation gaussian graphical models dmitry dmm mit edu jason johnson mit edu alan willsky willsky mit edu department electrical engineering computer science massachusetts institute technology cambridge ma usa editor michael jordan new framework walks graph analysis inference gaussian graphical models 
key idea decompose correlation pair variables sum walks variables graph 
weight walk product partial correlation coefficients 
representation holds large class gaussian graphical models call walk summable 
give precise characterization class models relate classes including diagonally dominant attractive pairwise normalizable 
provide walk sum interpretation gaussian belief propagation trees approximate method loopy belief propagation graphs cycles 
walk sum perspective leads better understanding gaussian belief propagation stronger results convergence loopy graphs 
keywords gaussian graphical models walk sum analysis convergence loopy belief propagation 
consider multivariate gaussian distributions defined undirected graphs referred gauss markov random fields 
nodes graph denote random variables edges capture statistical dependency structure model 
family gauss markov models defined graph naturally represented information form gaussian density 
key parameter information form information matrix inverse covariance matrix 
information matrix sparse reflecting structure defining graph diagonal elements diagonal elements corresponding edges graph non zero 
model consider problem computing mean variance variable determining marginal densities mode 
principle obtained inverting information matrix complexity computation cubic number variables 
efficient recursive calculations possible graphs sparse 
elaborates earlier brief publication johnson willsky presents subsequent developments 
research supported air force office scientific research fa fa army research office nf 
opinions findings recommendations expressed publication authors necessarily reflect views air force army 
dmitry jason johnson alan willsky 
johnson willsky structure example chains trees graphs thin junction trees 
models belief propagation bp junction tree variants efficiently compute marginals pearl cowell 
large scale models complex graphs example models arising oceanography tomography junction tree approach computationally prohibitive 
iterative methods numerical linear algebra varga compute marginal means 
order efficiently compute means variances approximate methods loopy belief propagation lbp needed pearl yedidia freeman weiss weiss freeman van roy 
important motivation lbp emphasized example van roy distributed nature important applications sensor networks 
lbp shown provide approximate solutions problems guaranteed general may fail converge 
prior van roy analyzed gaussian lbp graph 
special case established variances converge means follow linear system convergence variances means converge correct 
weiss freeman analyzed lbp computation tree perspective give sufficient condition equivalent diagonal dominance information matrix convergence showed correctness means convergence 
wainwright 
introduced tree reparameterization view belief propagation gaussian case showed correctness means convergence 
convergence forms lbp analyzed 
kappen unfortunately sufficient conditions directly applicable gaussian case 
develop walk sum formulation computation means variances correlations sums certain sets weighted walks graph 
walk sum formulation applies wide class gauss markov models call walk summable 
characterize class models show contains extends easy classes models including models trees attractive non frustrated diagonally dominant models 
show equivalence walk summability fundamental notion pairwise inference walk summable models reduced inference attractive model certain extended graph 
walk sum formulation develop new interpretation bp trees lbp general 
interpretation able extend previously known sufficient conditions convergence lbp class walk summable models 
sufficient condition stronger weiss freeman class diagonally dominant models strict subset class pairwise normalizable models 
results explain find examples lbp converge 
reason presumed 
give new explanation terms walk sums lbp converges correct means correct variances 
reason lbp captures walks needed compute means computes subset walks needed variances 

submitting aware related decomposition non gaussian classical spin systems statistical physics developed 

similarly decomposition connected neumann series expansion matrix inverse addition products edge weights weight walk includes complicated multi dimensional integral 

interesting decomposition covariance gaussian models terms path sums proposed jones west 
markedly different approach paths walks cross edge multiple times weight path hard calculate opposed walk weights 
walk sums gaussian graphical models general walk summability necessary lbp convergence 
provide tighter essentially necessary condition convergence lbp variances weaker form walk summability defined lbp computation tree 
provides deeper insight lbp fail converge lbp computation tree posed suggests connections jordan 
related concurrent johnson 
van roy shown convergence consensus propagation algorithm uses pairwise normalized model 
demonstrate equivalence pairwise walk summability suggests connection results 
van roy concurrent walk sum analysis lbp assuming pairwise consider initializations algorithm 
critical condition walk summability 
section introduce gaussian graphical models describe exact bp tree structured graphs approximate bp loopy graphs connection gaussian elimination 
section describe walk framework inference define walk summable models explore connections walk summable models subclasses gaussian models 
walk sum interpretation lbp conditions convergence section 
discuss non models tighter conditions lbp convergence section 
directions discussed section 
detailed proofs omitted main body appear appendices 

preliminaries section give brief background gaussian graphical models section gaussian elimination relation belief propagation section 
gaussian graphical models gaussian graphical model defined undirected graph set nodes vertices set edges set unordered pairs collection jointly gaussian random variables xi 
probability density exp xt jx symmetric positive definite matrix sparse respect graph ji 
condition necessary defines valid normalizable probability density 
information form gaussian density 
call information matrix potential vector 
related standard gaussian parameterization terms mean covariance follows class densities precisely family non degenerate gaussian distributions markov respect graph speed subset nodes separates 
choose particular initialization lbp 
fixing initialization restrict class models applications results apply 
instance application considered van roy handled framework simple reparameterization 
johnson willsky subsets corresponding subsets random variables xa xc conditionally independent xb 
particular define neighborhood node set neighbors 
conditioned variable xi independent rest variables graph 
partial correlation coefficient variables xi measures conditional correlation values variables xk 
computed normalizing diagonal entries information matrix lauritzen ri cov xi xj var xi var ji 
observe relation sparsity conditional independence variables 
agreement hammersley clifford theorem hammersley clifford gaussian models may factor probability distribution terms node edge potential functions xi xi xj xi exp aix xi xj exp xi bi ai bi add jx aix xi bi xi xi 
choice decomposition ai bi unique diagonal elements jii split various ways ai bi diagonal elements copied directly corresponding bi possible find decomposition ai bi 
call models decomposition exists pairwise normalizable 
analysis limited pairwise normalizable models 
decomposition ai jii bi ji ji exists leads node edge potentials xi exp xi xj exp jx 
note decomposition easily converted decomposition local operations required elements read adding overlapping matrices 
illustrate framework prototypical estimation problem 
suppose wish estimate unknown signal image noisy observations commonly prior model image processing thin membrane model exp 
precise negative logarithms usually referred potentials statistical mechanics literature 
abuse terminology slightly convenience 

example model valid model decomposition single pairwise positive definite factors exists 
verified posing appropriate semidefinite feasibility problem discuss walk summability 
walk sums gaussian graphical models xi specifies nearest neighbors image 
model described sparse information matrix jii ji consider local observations yi xi 
distribution interest markov respect graph modified information parameters 
instance gaussian distributed measurement noise zero mean covariance 
exp jx ht xt introducing local observations changes potential vector diagonal information matrix loss generality subsequent discussion assume observations absorbed belief propagation gaussian elimination important inference problem graphical model computing marginals pi xi obtained integrating variables xi node problem solved efficiently graphs trees form variable elimination known belief propagation provides approximate method general graphs 
belief propagation trees principle marginal node computed recursively eliminating variables just desired node remains 
belief propagation trees interpreted efficient form variable elimination 
computing marginal variable independently compute sharing results intermediate computations 
ultimately node receive information neighbors message mi neighbor represents result eliminating variables subtree rooted node including neighbors see 
messages variable elimination steps corresponding subtrees rooted neighbors node set fixed point equations relate messages tree mi xi xj xi mk xi dxi 
fixed point messages marginals obtained combining messages node pi xi xi mk xi normalizing result 
equations solved finite number steps variety message schedules including schedule corresponds roughly sequential variable elimination pass leaf nodes common root node followed reverse pass back leaf nodes fully parallel schedule node begins sending non informative messages mi initially set followed iterative computation tree 
trees message schedule terminate correct values finite number steps equal diameter tree case fully parallel iteration 

important problem computation max marginals pi xi obtaining maximizing respect variables useful determine mode argmax 
gaussian models equivalent inference problems marginals proportional max marginals mean equal mode 
johnson willsky discussed variety ways information matrix decomposed edge node potential functions decomposition leads bp iterations different detail 
development simple decomposition directly terms elements gaussian models expressed information form variable elimination marginalization corresponds gaussian elimination 
example wish eliminate single variable obtain marginal formulas yielding information parameterization marginal ju ju ju ij ii ji hu hu ju ij ii hi 
ju hu specify marginal density xu ju hu submatrix subvector information parameters full graph 
messages gaussian models parameterized information form mi exp ji jx hi jx fixed point equations stated terms information parameters 
steps 
step corresponds preparing message sent node node collecting information neighbors jii jk hi hk 
second step produces information quantities propagated node ji jji jji hi jji 
equations solved various message schedules ranging leaf gaussian elimination back substitution fully parallel iteration starting noninformative messages ji hi set zero 
fixed point solution obtained computation marginal node obtained combining messages local information ji jii jk hi hi hk easily inverted recover marginal mean variance hi pii general performing gaussian elimination corresponds upto permutation computing ldl factorization information matrix ldl lower triangular diagonal permutation matrix corresponding particular choice elimination order 
factorization exists non singular 
trees elimination order chosen step procedure node eliminated leaf node remaining subtree 
node elimination step corresponds message upward pass leaf root leaf form 
common decomposition pairwise normalizable models selects ai bi kumar weiss freeman van roy 

connection gaussian elimination belief propagation noted kumar information form 
walk sums gaussian graphical models mi message mi passed node node captures effect eliminating subtree rooted illustration bp message passing trees 
gaussian bp 
particular dii nodes parent node eliminated dii ji variable corresponding root tree 
clear dii positive definite 
conclude models trees positive definite equivalent quantities ji positive condition indicate saying bp tree posed 
performing gaussian bp trees serves simple test validity model 
importance notion apparent shortly 
loopy belief propagation message passing formulas derived tree models applied models defined graphs cycles longer corresponds precisely variable elimination graph 
approximation method called loopy belief propagation lbp proposed pearl 
course case cycles graph iterative message scheduling forms defined 
precise message schedule specifies messages corresponding directed edges updated step messages updated xi xj xi xi dxi updated iteration serial versions message updated iteration 
application messages parameterized reduces iterative application equations 
denote information parameters step messages 
example fully parallel case messages initialize lbp non informative zero values information parameters messages 
known lbp may may converge 
converge general yield correct values marginal distributions 
gaussian case known weiss freeman van roy lbp converges yields correct mean values general incorrect values variances 
considerable analyzing convergence lbp general particular story far complete 
major contribution analysis provides new insights lbp gaussian models brings story steps closer completion 
key component analysis insightful interpretation lbp terms called computation tree yedidia weiss freeman jordan captures structure lbp computations 
basic idea message 
undirected edge messages mi direction 
johnson willsky graph gauss markov model nodes edge weights partial correlations shown 
parallel lbp message passing scheme 
show iterations messages link form computation tree node subtree associated message indicated dotted outline 
illustrate equivalent gauss markov tree model edge weights copied marginal root node computed lbp iterations 
marginal estimate associated computation trees pedigree 
initially trees just single nodes 
message computation tree summarize computed constructed joining trees neighbors common root node adding additional edge form marginal estimate computed computation tree rooted formed joining trees neighbors common root 
node edge original graph may replicated times computation tree manner preserves local neighborhood structure 
potential functions assigned nodes edges copying corresponding nodes edges original loopy graphical model 
manner obtain markov tree model marginal root node precisely computed lbp 
case fully parallel form lbp leads collection balanced computation trees assuming leaf nodes having uniform depth illustrated 
construction applies message schedules difference resulting computation trees may grow non uniform manner 
walk walk sums gaussian graphical models sum analysis lbp section relies computation trees applies general message passing schedules 
mentioned bp trees corresponds performing gaussian elimination posed positive definite 
lbp gaussian models corresponds gaussian elimination computation tree information matrix corresponding unfolding illustrated involving replication information parameters original loopy graphical model 
consequently lbp posed yielding non negative variances stage iteration model computation tree valid information matrix computation tree positive definite 
importantly case matrix original graph positive definite 
analysis things point clear analysis situations lbp converges fails converge 

walk summable gaussian models describe walk sum framework gaussian inference 
convenient assume normalized model rescaling variables jii zero diagonal diagonal elements equal partial correlation coefficients ri 
label edge graph partial correlations ri edge weights see figures 
walk summability walk length graph sequence wl nodes wk step walk wk wk corresponds edge graph wk wk walks may visit nodes cross edges multiple times 
denote length walk define weight walk product edge weights walk wk allow zero length self walks node define 
connection walks gaussian inference decompose covariance matrix neumann power series matrix inverse 
spectral radius maximum absolute value eigenvalues power series converges 
th element expressed sum weights walks go length denoted ri wl rw 

neumann series holds unnormalized case diagonal part weight walk defined kwk wk wk analysis extends unnormalized case 

note greater 
occurs eigenvalue 
models walk summable analysis section section applies 
johnson willsky equality holds terms correspond walks graph non zero contributions terms partial correlation coefficients wk zero 
set walks length finite sum weights walks defined 
define walk sums arbitrary countable sets walks 
care taken walk sums countably walks may may converge convergence may depend order summation 
motivates definition say gaussian distribution walk summable ws unordered sum walks denoted defined converges value possible summation order 
appealing basic results analysis rudin unordered sum defined converges absolutely converges 
take closer look walk summability introduce additional notation 
matrix element wise absolute value ai 
notation element wise comparisons comparisons positive definite ordering 
version perron frobenius theorem horn johnson varga non negative matrices occasions perron frobenius theorem exists non negative eigenvector eigenvalue 
graph connected ri edges strictly positive apart non negative eigenvectors addition monotonicity properties spectral radius ii 
equivalent conditions walk summability proposition walk summability conditions equivalent walk summability converges 
ii converges 
iii 
iv 
proof appears appendix uses absolute convergence rearrange walks order increasing length perron frobenius theorem part iv 
condition stronger 
sufficient convergence walks ordered increasing length walk summability enables convergence answer arbitrary order summation 
note iv implies model walk summable replace negative partial correlation coefficients absolute values defined model information matrix 
note condition iv relates walk summability walk sums gaussian graphical models example graphs cycle chord 
cycle 
called matrices linear algebra horn johnson varga 
immediate corollary identify important subclass walk summable models corollary attractive models valid model non negative partial correlations 
walk summable 
superclass attractive models set non frustrated models 
model non frustrated contain frustrated cycles cycles odd number negative edge weights 
show appendix proof corollary model non frustrated negate variables model attractive subclass walk summable models inclusion strict frustrated models walk summable see example corollary non frustrated models valid 
non frustrated 
example 
illustrate small gaussian graphical models 
models information matrix normalized unit diagonal partial correlations indicated 
consider cycle chord 
model frustrated due opposing sign partial correlations increasing worsens frustration 
model valid walk summable example min 
interval model valid walk summable example min 
note opposed 
model stops diagonally dominant walk summability strictly larger set extends 
summarize various critical points model model diagram 
additional useful implications walk summability proof appendix proposition ws necessary conditions implied walk summability 
possibly non symmetric matrix matrix eigenvalues matrix aii mi ai positive real parts 
symmetric matrices equivalent positive definite 
iv matrix 

result referred 

addition proving exists sign similarity proof gives algorithm checks model frustrated determines subset variables negate model non frustrated 
diag 
dominant valid johnson willsky diag 
dominant valid critical regions example models 
cycle chord 
cycle 

ii 
iii implication ii shows walk summability sufficient condition validity model 
iii shows relevance walk sums inference walk sums inference show walk summable models means variances correspond walk sums certain sets walks 
proposition ws inference walk summable covariance walk sums pi 
means walk sums reweighted value start walk sum walks node arbitrary starting node denotes starting node walk proof 
fact 
pi walk sums gaussian graphical models single walk 
weight 
self return walks 

set walks 

illustration walk sums means variances 
walk sum notation provide compact notation walk sets walk sums 
general set walks define walk sum reweighted walk sum hw denotes initial node walk adopt convention 
denotes set walks having property denote associated walk sums simply 

instance denotes set walks corresponding walk sum 
denotes set walks node corresponding reweighted walk sum 
notation pi 
illustration walk sums connection inference appears list walks walk sums cycle graph 
walk sum algebra show walk sums required inference walk summable models significantly simplified exploiting recursive structure walks 
simple algebraic properties walk sums 
lemmas assume model walk summable 
lemma wk subsets wk disjoint 
wk 
proof 
sum partition theorem absolutely convergent series wk 
lemma wk wk wk wk 
johnson willsky proof 
empty set 
wk wk 
lemma wk wk lim wk wk lim wn step obtain result 
walks un vm un walk begins walk ends define product walks uv un vm 
countable sets walks walk ends node walk node 
define product set uv uv 
say valid decomposition uv unique pair uv lemma valid decomposition 
uv 
proof 
individual walks evident uv 
note uv uv sets uv uv mutually disjoint 
lemma uv uv uv uv uv uv note set self return walks node walks node self return walks include walks return times 
set walks non zero length visit point 
call single revisit self return walks node set self return walks return exactly times generated product copies denoted 
obtain self return walks 
similarly recall denotes set walks node denote set walks non zero length node visit previously call single visit walks 
walks obtained valid decomposition 
decompose means variances terms single visit single revisit section analyze bp 
proposition 
pii hi walk sums gaussian graphical models frustrated model defined negative edge 
corresponding attractive model defined proof 
note decomposition products single revisit walks valid decomposition 
lemma lemma pii walk summability model implies convergence geometric series 
lastly decomposition implies hi hi correspondence attractive models shown attractive models walk summable 
interestingly turns inference walk summable model reduced inference corresponding attractive model defined graph twice nodes 
basic idea separate walks positive negative weights 
specifically defined follows 
node define corresponding nodes set 
edge ri define edges set partial correlations edges equal ri edge ri define edges set partial correlations ri see illustration 
max ri max ri 
expressed difference non negative matrices 
construction defines unit diagonal information matrix note defines valid attractive model 
proposition walk summable 
proof relies perron frobenius theorem appendix max hi max hi 
define information form model valid attractive model non negative node johnson willsky potentials 
performing inference respect augmented model obtain mean vector covariance matrix calculations obtain moments original walk summable model proposition 
proof appears appendix proposition shows estimation walk summable models may reduced inference attractive model walk sums sums positive weights 
essence accomplished summing walks positive negative weights separately difference possible walk summable models 
pairwise simplify presentation assume graph contain isolated nodes node incident edges 
say information matrix pairwise normalizable pn represent form je je symmetric positive definite matrix 
notation je means je zero padded matrix principal submatrix je 
je 
pairwise implies node covered positive definite submatrix je 
jpn denote set pairwise normalizable information matrices requiring unit diagonal normalization 
set nice convexity properties 
recall set convex implies cone implies 
cone pointed 
proposition convexity pn models set jpn convex pointed cone 
proof appendix establish fundamental result proposition ws pn walk summable pairwise normalizable 
proof appears appendix equivalent result derived independently linear algebra literature boman 
establish symmetric matrices positive diagonals equivalent ws part iv proposition equivalent matrices factor width pn models 
result pn ws established earlier johnson 
proof ws pn uses perron frobenius theorem boman 
generalized diagonal dominance property matrices 
equivalence pairwise gives insight set walk summable models 
example set unit diagonal matrices walk summable convex intersection jpn affine space 
set walk summable matrices sparse respect particular graph entries restricted convex 
important class models diagonally dominant information matrix holds ji jii 

alternative definition pairwise existence decomposition ci je je 
graphs isolated nodes definitions equivalent 
walk sums gaussian graphical models ti illustration subtree notation ti proposition diagonally dominant models pairwise normalizable walk summable 
constructive proof appendix converse hold models diagonally dominant 
instance example cycle chord shown model diagonally dominant walk summable pairwise normalizable 

walk sum interpretation belief propagation section concepts machinery walk sums analyze belief propagation 
models trees show valid models walk summable 
models show exact walk sums infinite sets walks means variances computed efficiently recursive fashion 
show walk sum computations map exactly belief propagation updates 
results computation tree interpretation lbp recursions provide foundation analysis loopy belief propagation section 
walk sums bp trees analysis bp property proposition trees walk summable tree structured models valid trees walk summable 
trees max 
proof 
proof special case proof corollary 
trees non frustrated cycles frustrated cycles walk summable 
negating variables model attractive change eigenvalues 
proposition shows walk sums means variances defined treestructured models reordered arbitrary ways affecting convergence 
rely fact heavily subsequent sections 
results identify walk sum variance mean computations bp update equations 
ingredients results decompositions variance mean walk sums terms sums walks subtrees decomposition terms single revisit single visit walks provided proposition 
ri johnson willsky walk sum variance calculation look computation variance node equal self return walk sum 
computed directly single revisit walk sum proposition 
walk sum decomposed sums disjoint subsets walks corresponds single revisit self return walks exit node specific neighbors say particular illustrated single revisit self return walks correspond walks live subtree ti notation ti set single revisit walks restricted stay subtree ti see ti single revisit self return walk lives ti leave return node single edge steps execute possibly multiple revisit self return walk node constrained pass node live subtree indicated 
ti show walk sums variances pj efficiently calculated walk sum analog belief propagation 
result proposition consider valid tree model ji ji quantities defined gaussian bp equations 
see appendix proof 
walk sum mean calculation extend analysis calculate means trees 
mean reweighted walk sum walks start node 
walk ends node expressed single visit walk node followed self return walk node term corresponds length zero walk starts ends node done variances single visit walks node partitioned single visit walks reach node neighbors say node prior step edge reside subtree ti ri ti 
proposition consider valid tree model hi hi quantity defined gaussian bp equation 
proof appears appendix lbp walk summable models walk sums gaussian graphical models subsection lbp computation tree show lbp includes walks means subset walks variances 
allows prove lbp convergence walk summable models 
contrast non models lbp may may converge fact variances may converge means may 
see section analyzed examining walk summability validity computation tree walk summability original model 
discussed running lbp number iterations yields identical calculations particular node exact inference calculations corresponding computation tree rooted node notation nth computation tree node ti full computation tree assign label root node 
denotes variance root node nth computation tree rooted node lbp variance estimate node steps equal 
similarly lbp estimate mean steps lbp 
mentioned definition computation trees depend message schedule lbp specifies subset messages updated iteration say message schedule proper message updated infinitely directed edge graph exists clearly fully parallel form proper message updated iteration 
serial forms iteratively cycle directed edges graph proper 
convergence analysis section presumes proper message schedule 
ensures convergence walk sums independent order summation choice particular message schedule unimportant convergence analysis 
result proven appendix lemma walks ti correspondence finite length walks walks ti root node 
particular walk large 
corresponding walk recall compute mean need gather walk sums walks start just shown lbp gathers walks computation tree grows infinity 
story variances different 
true variance pii walk sum self return walks start walks start may map walks start root node replica root node root 
walks captured lbp variance estimate 
walks variance estimate self return walks start root node 
recall computation tree representation computations seen root node tree computation node replica node corresponds lbp computation node johnson willsky computation tree 
consider 
walk self return walk original graph self return walk computation tree shown 
lbp variances capture self return walks original graph self return walks computation tree example walk self return walk figures 
call walks backtracking 
lemma self return walks ti lbp variance estimate node sum backtracking self return walks subset self return walks needed calculate correct variance 
note back tracking walks variances positive weights edge walk traversed number times 
lbp step computation tree grows new back tracking walks included variance estimates grow monotonically 
shown walks lbp gathers computation tree 
convergence corresponding walk sums remains analyzed 
walk summable models answer simple lemma computation trees ws models ws walk summable model com walk summable valid 
putation trees intuitively walks computation tree subsets walks converge 
implies computation trees walk summable valid 
argument precise shorter formal proof monotonicity spectral radius appears appendix observations show convergence lbp models 
proposition convergence lbp walk summable models model graph walk summable lbp posed means converge true means lbp variances converge walk sums backtracking self return walks node 
proof 
bt denote back tracking self return walks node lemmas note computation trees nw bt nw 
node nested 
lemma obtain result lim lim bt bt lim lim 
monotonically increasing variance estimates characteristic particular initialization lbp potential decomposition uninformative initial messages 
uses potential decomposition variances monotonically decreasing 
walk sums gaussian graphical models lbp converges lbp converge lbp variances vs iteration 
rn vs iteration 
corollary lbp converges attractive non frustrated diagonally dominant models 
attractive non frustrated models lbp variance estimates equal true variances missing non backtracking walks positive weights 
weiss freeman gaussian lbp analyzed pairwise normalizable models 
show convergence case diagonally dominant models correctness means case convergence 
class walk summable models strictly larger class diagonally dominant models sufficient condition stronger 
show lbp variances omit terms needed correct variances 
terms correspond correlations root replicas computation tree 
framework correlation subset non backtracking self return walks computation tree particular replica root 
example 
consider model 
summarize various critical points model 
model walk summable lbp converges small interval model walk summable lbp converges larger lbp converge 
apply lbp model plot lbp variance estimates node vs iteration number 
lbp converges walk summable case 
converges soon fails converge increase 
note series converges lbp converge 
sufficient lbp convergence showing importance stricter walk summability condition 
lbp non models condition proposition necessary sufficient certain special classes models example trees single cycles sufficient generally johnson willsky example lbp may converge non models 
extend analysis develop tighter condition convergence lbp variances weaker form walk summability defined respect computation trees 
shown proposition trees walk summability validity equivalent 
condition essentially corresponds validity computation tree 
note model valid positive definite walk summable finite computation trees may invalid indefinite 
turns primary reason belief propagation fail converge 
walk summability original graph implies walk summability validity computation trees 
model walk summable computation tree may may valid 
characterize walk summability computation trees follows 
computation tree rooted node define information matrix identity matrix 
nth computation tree nth normalized valid due fact trees 
interested validity finite computation trees consider quantity limn 
lemma guarantees existence limit lemma sequence monotonically increasing bounded 
limn exists equal sup 
proof fold graphs introduce appendix proof appears appendix limit lemma defined respect particular root node message schedule 
lemma shows connected graphs long message schedule proper matter 
lemma connected graphs proper message schedule limn independent limit change proper message schedule 
independence results fact large computation trees rooted different nodes overlap significantly 
technical details proof appear appendix lemma suppress dependence root node notation simplify matters 
limit turns critical convergence lbp variances proposition lbp validity variance convergence finite computation trees valid lbp variances converge walk sums back tracking self return walks 
ii computation tree eventually invalid lbp ill posed 
proof 
limn sequence monotonically increasing exists implies computation trees walk summable variances monotonically increase weights back tracking walks positive see discussion lemma 
max min max maximum eigenvalue matrix bound maximum entry matrix ii max 
variances monotonically increasing bounded converge 
ii limn exists means computation trees invalid variance estimates nodes negative 
walk sums gaussian graphical models discussed section lbp computation tree valid information parameters computed lbp iterations strictly positive easily detected lbp computation tree invalid 
case continuing run lbp meaningful lead division zero computation tree singular negative variances positive definite 
recall limit invariant message order lemma 
proposition convergence lbp variances likewise invariant message order possibly 
limit bounded walk summability sufficient condition posedness computation tree 
bound tight general trees single cycles 
related phenomenon limit spectral radius finite computation trees spectral radius infinite computation tree leaf nodes 
see 
analysis related discrepancy 
means non ws models case walk sums lbp variances converge absolutely see proof proposition walk sums means 
reason lbp computes subset self return walks variances captures walks means 
series lbp computes means corresponding particular ordering walks may converge 
known van roy variances converge updates means follow linear system 
consider fixed lbp messages means hi follow linear system update 
parallel message schedule express matrix vector convergence system depends spectral radius 
difficult analyze matrix depends converged values lbp variances 
improve convergence means damp message updates modifying follows ji 
observed experiments cases variances converge obtain convergence means damping bp messages 
tried damping updates messages variances converge appears independent damping 
apparently validity computation tree essential convergence means variances damped versions gaussian lbp 
example 
illustrate proposition simple example 
consider node cycle model 
plot rn vs lower curve observe limn rn lbp converges 
upper curve model defined node cycle valid limn rn lbp ill posed converge 
mentioned non models series lbp computes means absolutely convergent may diverge variances converge 
cycle chord example region variances converge means diverge lbp means conv 
lbp vars converge valid johnson willsky lbp means converge lbp vars converge valid critical regions example models 
cycle chord 
cycle 
number iterations converge true variance lbp estimate error cycle chord example 
convergence divergence means near lbp mean critical point 
variance near lbp variance critical point top number iterations variances converge bottom true variance lbp estimate error node 
narrow parallel message schedule critical point means slightly higher serial schedule 
show mean estimates vs iteration number sides lbp mean critical point 
case means converge slowly definitely diverge 
spectral radius linear system mean updates cases respectively 
divergent example eigenvalues real components maximum real component 
damping force eigenvalues enter unit circle damped linear system means converge 
illustrate near lbp variance critical point lbp estimates difficult obtain quality deteriorates dramatically 
consider graph walk sums gaussian graphical models valid models lbp posed non frustrated trees diagonally dominant attractive venn diagram summarizing various subclasses gaussian models 
approaches critical point convergence variances 
picture shows number iterations error lbp variance estimates explode near critical point 
show variance node similar behavior occurs node 
summarize critical points models 
walk sum interpretation inference gaussian graphical models holds wide class models call walk summable 
shown walk summability encompasses classes models considered easy inference trees attractive non frustrated diagonally dominant models includes models outside classes 
venn diagram summarizing relations sets appears 
shown equivalence walk summability pairwise 
established walk summable models lbp guaranteed converge means variances convergence means correct variances capture walk sums back tracking walks 
walk summability valid positive definite models trees develop complete picture lbp non models relating variance convergence validity lbp computation tree 
variety directions results extended 
involves developing improved walk sum algorithms gather walks lbp yield better variance estimates 
results lines involving vectors variables node factor graph versions lbp group larger sets variables publication 
direction apply walk sum analysis algorithms gaussian inference example chandrasekaran applying walk sums better understand embedded trees algorithm sudderth 
current limited gaussian models walk sums arise power series expansion matrix inverse 
related expansions correlations terms walks investigated models 
fisher developed approximation pairwise correlations ising models self avoiding walks 

walk sums non gaussian classical quantum spin systems weights walks involve complicated multi dimensional integrals 
useful develop ways compute approximate self avoiding non gaussian walk sums efficiently extend walk sum perspective inference broader class models 
appendix detailed proofs johnson willsky proof proposition proof ii 
examine convergence matrix series ii element wise 
note absolute walk sum walks length finite number walks sum defined 
holds properties absolute convergence order sum wish converges 
order walks length group terms walks equal lengths group finite number terms obtain series converges proof ii 
show convergence sum sufficient test convergence convenient ordering walks 
shown corresponds particular ordering walks converges ii 
walk sums converge absolutely 
proof ii iii 
standard result matrix analysis varga 
proof iii iv 
note eigenvalue eigenvalue rx 
min max 
perron frobenius theorem max non negative 
min min 
proof corollary show non frustrated model exists diagonal dii signature matrix drd eigenvalues drd drd similarity transform preserves eigenvalues matrix 
follows implies walk summability proposition iv 
describe construct signature similarity attractive models 
show split vertices sets negating model attractive 
find spanning tree graph pick node assign node unique path product edge weights path positive assign model non frustrated edges positive edges positive edges negative 
seen constructing cycle goes crosses edge close 
paths positive weight order cycle positive weight step positive weight 
cases similar 
diagonal dii dii drd rv drd proof proposition proof ws 
ws equivalent proposition 


walk sums gaussian graphical models proof ii 
holds min max 
max 
min max 
proof iii 
standard result matrix analysis 
proof proposition assume connected apply proof connected component spectral radii maxima respective connected components 
prove 
perron frobenius theorem exists positive vector rx 
rx eigenvalue positive eigenvector suppose connected 
perron frobenius theorem unique positive eigenvector eigenvalue equal 
ws ws 
disconnected block diagonal matrix copies relabeling nodes 
proof proposition partition walk sums sums odd walks number negative edges crossed walk 
walk odd 
graph defined walk walk odd 
pi odd pi pi second part proposition follows similar logic 
classify walk hw odd hw 
note setting effect walks hw walks hw 
consequently walk ends odd walk ends 
odd proof proposition take pairwise normalizable 
take positive 
pairwise normalizable simply weighted combinations je matrices 
setting shows jpn cone setting shows convexity 
cone pointed subset cone semidefinite matrices pointed 
johnson willsky proof proposition proof pn ws 
evident matrix positive definite 
furthermore reversing sign partial correlation coefficient edge simply negates diagonal element je change value je 
negative coefficients positive resulting model pairwise normalizable positive definite 
proposition iv walk summable 
proof ws pn 
walk summable model construct pairwise normalized representation information matrix 
may assume graph connected may apply construction connected component graph 
perron frobenius theorem exists positive eigenvector rx 
construct representation je set je ri xi ri defined division zero positive 
verify je 
evident diagonal elements edge matrices sum check diagonal elements sum je ii xi ri ri xi ri rx xi xi verify je positive definite 
matrix positive diagonal determinant 
ri ri xi ri xi 
inequality follows walk summability 
je 
proof proposition ai jii ji 
note ai follows diagonal dominance 
deg denote degree node je edge set je ji ai deg elements je set zero 
note ji ji ji deg je ii ji ai ai deg ji jii je positive diagonal elements determinant det je 
je 
pairwise normalizable 
walk sums gaussian graphical models proof proposition calculate walk sum multiple revisit self return walks single revisit counterpart ti 
ti decompose single revisit walks subtree terms possible step walk ti tk 
able represent walk sum ti ti terms walk sums tk smaller subtrees tk basis recursive calculation equations look strikingly similar belief propagation updates 
combining section ji jii jk evident recursive walk sum equations mapped exactly belief propagation updates 
normalized models jii 
message update ji variance estimate subtree ti proof proposition multiple revisit walk written terms single visit walks ti hi ti ti 

remaining term decomposed subtrees walk lives recursion ti tk 
ri hi 
compare gaussian bp updates combine section hi ji hi hk bp updates means mapped exactly recursive walk sum updates hi johnson willsky proof lemma note walk ends root node corresponding walk ends reason neighbors node correspond subset neighbors step wk wk walk corresponding step show walk wl contained wl consider parallel message schedule computation tree wl grows uniformly 
walk ends wl length walk wl ends root 
intuition message schedules step walk appear eventually proper message schedule formal proof somewhat technical 
unwrap walk tree tw rooted wl way start wl walk traverse walk reverse 
add edge wl wl tw 
suppose node wk tw step wk wk 
wk neighbor wk tw set current node tw wk 
create new node wk add edge tw 
clear loops procedure tw tree 
show proper message schedule tw part computation tree wl pick leaf edge tw 
proper exist edge appears root holds subsequent steps 
remove tw pick leaf edge 
proper exist remove tw continue similarly 
point nk eliminating new edge ik jk tw eliminated subtree tw extending ik jk belong nk ik jk continue just root tw remains step com putation tree wl created splicing tw contains tw contains walk edges coming root proof lemma result comes immediate corollary proposition states partial correlation matrix 
ws models result follows 
proof lemma fact sequence bounded nontrivial fact proven appendix fold graph construction 
prove monotonicity note trees 
note variables computation tree zero pad spectral radius 
holds establishing monotonicity 
size change element wise 
follows proof lemma denote nth computation tree proper message schedule rooted node simple extension lemma nth computation tree rooted message schedule 
take node replica node exists walk sums gaussian graphical models illustration graph fold graph message schedule 
proof parallels lemma tree finite number edges induction adding edge time 
consider message schedule 
lemma limn exists 
pick holds pick replica node side 
property previous paragraph exists 
similarly exists 
follows zero pad matrices size 

holds limn appendix fold graphs proof boundedness 
consider arbitrary graph 
suppose pairwise mrf defined self potentials xi vi pairwise potentials xi xj vi vj construct family fold graphs follows 
create disconnected copies gk nodes edges 
nodes edges gk labeled way ones potentials copied corresponding nodes edges gk 

pick pair graphs gk gl choose edge vi vj flip corresponding edges gk gl edges 
pairwise potentials adjusted accordingly 

repeat step arbitrary number times different pair graphs gk different edge illustration procedure appears 
original graph cycle chord 
create fold graph flipping edges 
apply fold graph construction gaussian mrf models 
suppose model information parameters suppose normalized 
fold graph information matrix unit diagonal construction 
johnson willsky nth computation tree original graph corresponding information matrix unit diagonal 
identity matrices appropriate dimensions 
lemma spectral radii fold graph 
proof 
suppose connected apply proof connected component spectral radius maximum spectral radii connected components 
perron frobenius theorem exists vector rx create fold vector copying entry xi corresponding entries positive holds local neighborhoods 
non negative matrix positive eigenvector achieves spectral radius perron frobenius theorem 

construction fold graph parallels computation tree fold graph locally equivalent computation tree leaf nodes locally equivalent show computation tree contained gk large 
lemma fold graphs computation trees consider computation tree ing graph exists fold graph gk contains 
correspond subgraph large proof 
provide simple construction fold graph making attempt minimize vn en 
node vn corresponds node create fold graph gk making copy gv node vn 
edge en computation tree edge flip nodes graphs gu gv correspond operation defined edges edge meet 
procedure creates contains map subgraph 
preceding lemmas prove bound spectral radii matrices computation tree proposition bound computation tree 
proof 
consider computation tree recall lemma construct fold graph tree 
subgraph 
zero padding size holds lemma 
walk sums gaussian graphical models boman chen parekh toledo 
factor width symmetric matrices 
linear algebra applications 
frohlich sokal 
random walk representation classical spin systems correlation inequalities 
communications mathematical physics 
chandrasekaran johnson willsky 
walk sum analysis convergence embedded subgraph algorithms 
preparation 
cowell dawid lauritzen spiegelhalter 
probabilistic networks expert systems 
springer verlag 
fisher 
critical temperatures anisotropic ising lattices ii general upper bounds 
physical review 

analysis springer verlag 
hammersley clifford 
markov fields finite graphs lattices 
unpublished manuscript 
liu strang 
laplacian eigenvalues growing 
conf 
math 
theory networks systems 
horn johnson 
matrix analysis 
cambridge university press 
horn johnson 
topics matrix analysis 
cambridge university press 
fisher iii willsky 
loopy belief propagation convergence effects message errors 
journal machine learning research may 
johnson 
walk summable gauss markov random fields 
unpublished manuscript available www mit edu people december 
johnson willsky 
walk sum interpretation analysis gaussian belief propagation 
advances neural information processing systems 
jones west 
covariance decomposition undirected gaussian graphical models 
biometrika 
mcdonald 
sign patterns require positive eigenvalues 
linear multilinear algebra 
lauritzen 
graphical models 
oxford statistical science series 
oxford university press 
van roy 
consensus propagation 
advances neural information processing systems 
van roy 
convergence min sum message passing algorithm quadratic optimization 
technical report march 
johnson willsky kappen 
sufficient conditions convergence loopy belief propagation 
proc 
uncertainty artificial intelligence 
pearl 
probabilistic inference intelligent systems 
morgan kaufmann 
kumar 
extended message passing algorithm inference loopy gaussian graphical models 
ad hoc networks 
rudin 
principles mathematical analysis 
mcgraw hill rd edition 
van roy 
analysis belief propagation turbo decoding graph gaussian densities 
ieee trans 
information theory 
speed 
gaussian markov distributions finite graphs 
annals statistics 
sudderth wainwright willsky 
embedded trees estimation gaussian processes graphs cycles 
ieee trans 
signal processing november 
jordan 
loopy belief propagation gibbs measures 
uncertainty artificial intelligence 
varga 
matrix iterative analysis 
springer verlag 
wainwright jaakkola willsky 
tree reparameterization framework analysis sum product related algorithms 
ieee trans 
information theory 
weiss freeman 
correctness belief propagation gaussian graphical models arbitrary topology 
neural computation 
yedidia freeman weiss 
understanding belief propagation generalizations 
exploring ai new millennium 

