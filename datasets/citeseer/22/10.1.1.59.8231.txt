submitted special issue ieee tnn temporal coding identification control dynamical systems self organizing map barreto member ieee ara jo introduce general modeling technique called vector quantized temporal associative memory vqtam uses kohonen self organizing map som alternative mlp rbf neural models dynamical system identification control 
demonstrate estimation errors decrease som training proceeds allowing vqtam scheme understood self supervised gradient error reduction method 
performance proposed approach evaluated variety complex tasks time series prediction ii identification siso mimo systems iii nonlinear predictive control 
tasks simulation results produced som accurate produced mlp network better produced rbf network 
som shown sensitive weight initialization mlp networks 
conclude discussing main properties vqtam relationships established methods dynamical system identification 
suggest directions 
keywords self organizing maps time delays temporal associative memory function approximation time series prediction predictive control 
dynamical system identification discipline interested building mathematical models nonlinear systems starting experimental time series data measurements observations 
typically certain linear nonlinear model structure contains unknown parameters chosen user 
general parameters computed errors estimated predicted actual outputs system minimized order capture dynamics system close possible 
resulting model tool analysis simulation prediction monitoring diagnosis control system design 
artificial neural network ann models successfully applied identification control variety nonlinear dynamical systems chemical economical biological technological processes 
achievements mainly due number theoretic empirical studies showing supervised feedforward architectures multilayer perceptron mlp radial basis function rbf networks approximate arbitrarily continuous input output mapping see surveys 
propose system identification technique uses self organizing map som barreto department teleinformatics engineering federal university ce brazil 
mail br 
ara jo center informatics federal university pe brazil 
mail cin br 
function approximation usual supervised ones mlp rbf 
technique called vector quantized temporal associative memory vqtam shows som successfully approximate dynamical input output mappings minor modifications original algorithm 
som unsupervised neural algorithm designed build representation neighborhood spatial relationships vectors unlabelled data set 
neurons som put output layer threedimensional arrays 
neuron weight vector wi dimension input vector network weights trained competitive cooperative scheme weight vectors winning neuron neighbors output array updated presentation input vector 
usually trained som clustering data visualization purposes 
som previously applied learn static inputoutput mappings usually represented current output depends solely current input 
interested systems modeled nonlinear discrete time difference equation 
ny 
nu ny nu memory orders dynamical model 
stated system output time depends sense defined nonlinear map past ny output values past nu values input situations desirable approximate inverse mapping nonlinear plant 
ny 
nu system identification goal obtain estimates available time series data 
som unsupervised networks able learn dynamical mappings type short term memory stm mechanism 
som capable temporarily storing direct inverse control term replaced signal usually assumed available time submitted special issue ieee tnn temporal coding past information system input output vectors 
stm models delay lines leaky integrators reaction diffusion mechanisms feedback loops incorporated som allow approximate dynamical mapping inverse 
order draw parallel standard system identification approaches limit describe vqtam approach terms time delays stm mechanisms 
remainder organized follows 
section ii introduces vqtam technique approximate forward inverse mappings 
section iii deals convergence analysis vqtam showing som learning rule builds self supervised error reduction scheme function approximation 
section iv instantiate vqtam approach variety complex dynamical modeling control tasks time series prediction identification siso mimo systems nonlinear predictive control 
section computer simulations illustrate approximation capability vqtam technique comparing obtained results produced mlp rbf networks linear models 
section vi discusses main features vqtam relationships previous unsupervised approaches dynamical system identification control 
concluded section vii 
ii 
temporal associative memory som pointed introductory section mlp rbf neural networks usual choices model structures identification nonlinear dynamical mappings 
goal devise strategy enables som approximate input output mappings 
purpose describe general procedure input vector som augmented comprises parts 
part denoted carries data input dynamic mapping learned 
second part denoted contains data concerning desired output mapping 
weight vector neuron wi dimension increased accordingly 
changes written wi win wout respectively portions weight vector store information inputs outputs mapping studied 
depending variables chosen build vectors som algorithm learn forward inverse mapping plant system 
instance approximate forward dynamics definitions apply 
ny 
nu interest inverse identification defines 
ny 
nu training stage winning neuron time step determined arg min xin updating weights learning rate gaussian neighborhood function exp ri ri ri ri respectively locations neurons output array 
parameters decay exponentially fast time denote initial values final ones training iterations 
trained som network obtain esti mates output learned mapping directly output portion weight vector follows forward case inverse case cases winning neuron 
estimation process continues steps entirely new series built estimated values 
note learning rules follow usual formulation som rule acts input second acts output mapping learned 
underlying idea training proceeds som algorithm learns associate topology preserving way outputs mapping corresponding inputs 
technique referred vector quantized temporal associative memory vqtam 
vqtam promptly understood generalization demanding task modeling dynamical input output mappings 
worth emphasizing difference vqtam strategy usual supervised approach 
mlp rbf networks vector network input network output compute explicitly error signal guides submitted special issue ieee tnn temporal coding supervised ann mlp rbf learning algorithm unsupervised ann som learning algorithm fig 

differences supervised self supervised approach system identification 
learning 
vqtam scheme allows unsupervised neural networks som learn associate correlate inputs outputs mapping explicit computation error signal 
section show associative nature vqtam computes indirectly error signal 
property allows understand approach self supervised gradient error reduction method 
iii 
analysis convergence section analysis convergence process som algorithm vqtam approach 
specifically want know additional learning rule allows som learn input output mapping 
analysis adaptation purposes current carried 
goal verify estimation approximation error really decreases som training proceeds converging eventually stable state 
sake clarity assume mapping approximated source information available time series measured inputoutput data 
means vqtam approach som provides approximation unknown mapping 
case som computes estimate unforeseen vector follows explicit representation winning neuron emphasizes associative nature vq tam approach element responsible linking input output portions learned mapping 
able define absolute value estimation error definition demonstrate vqtam scheme equivalent implicit learning procedure 
main features som algorithm cooperation winning neuron neighbors neighborhood function 
loss generality assume neighborhood function time invariant 
current analysis valid convergence phase som training ordering phase iterations algorithm 
account cooperation neurons learning process network convergence evaluated average expected squared prediction error dx assumed expectation value taken infinite number stochastic samples means joint probability density function data estimator formed dx volume differential space formed 
probability density unknown sequence finite reality shall resort robbins monro stochastic approximation technique mini mization find optimal value parameter wout method stochastic samples functional computed sequences follows underlying idea decrease function new step descending direction negative gra respect current parameter vector wout 
recursive formula parameter vector wout scalar defines step size satisfies 
considering partial derivative wout submitted special issue ieee tnn temporal coding recursive equation written equation exactly learning rule 
starting arbitrary initial values sequence wout converge optimal vector corre sponding local minimum dimensional som particular steady state happens global minimum 
iv 
case studies section provide instantiations vqtam method different modeling control problems time series prediction ii identification siso single input single output systems iii predictive control siso systems iv identification mimo multi input multi output systems 
time series prediction consider scalar time series denoted described special case nonlinear regressive model order ny follows 
ny nonlinear mapping arguments random variable included take account modeling uncertainties noise 
stationary time series commonly assumed drawn gaussian white noise process 
expected structure nonlinear mapping unknown thing available set observables 
total length time series 
task construct physical model time series data 
propose vqtam approach som build step ahead predictors 
purpose past ny samples ny comprises network input vector output vector defined time step sample 

ny winning neuron determined basis input vector weight vectors updated 
prediction test phase winning neuron input vector step ahead estimate time series time step recovered wout prediction error residual defined difference true value sample time series estimated value variance residuals estimate variance white noise estimate evaluate accuracy model means normalized root mean squared error nrmse nrmse variance original time series length sequence residuals 
efficient linear prediction models vqtam essence som algorithm vector quantization 
outputs produced assume discrete values limited number neurons network 
shown simulations obtain small prediction errors number neurons large greater units 
efficient approach train som described just neurons units order compact representation time series encoded weight vectors network 
training completed linear autoregressive model fitted weight vectors som original input vectors 
technique detailed 
linear autoregressive model order ny simply ar ny represented ny ajy aj coefficients model 
geometric terms fitting ar model time series equivalent fit ny dimensional hyperplane data ny basic technique compute coefficients known squares method prediction vector regression matrix 
vqtam entities constructed weight vectors win win win ny follows win win win win win 
ny ny win win ny number neurons som 
write fact predicting scalar sample time series 
avoid confusion denote technique model refer plain vqtam simply som model 
submitted special issue ieee tnn temporal coding main advantages model respect som model fold smaller prediction error hyperplane described interpolates weight vectors som ii reduction computational cost prediction task fewer neurons needed get small prediction error 
uses ny weight vectors similar current input neurons neighborhood winning neuron vqtam build local ar predictors manner similar 
local models suitable model nonstationary time series detailed discussion subject outside scope 
identification siso systems univariate time series prediction assumed observations single variable 
section generalization task called identification siso systems involving univariate time series input variable output variable 
goal estimate output plant current previous values input output variables 
mathematically 
ny 
nu equation equivalent dimensions input output variables adapted scalar case 
application vq tam method siso system identification obeys exactly replacing vectors scalars 
training prediction follow accordingly 
worth emphasizing efficient linear prediction approach described subsection iv equally applied identification siso systems reducing number neurons needed get smaller prediction errors 
predictive control som models applied system identification fair evaluate performances model control problems nonlinear predictive control npc 
field neural system identification field dominated supervised neural models mlp rbf networks 
particularly evident npc strategy 
works evaluated self organizing neural networks npc 
designed strategies highly modified versions som limiting general applicability approaches 
vqtam emphasizes viability som algorithm slightly modified input output desired mapping input network system identification control keeping training procedure exactly original algorithm 
control signal optimization algorithm plant feedforward model output fig 

general architecture model predictive controller 
predictive control strategy comprises basically components system plant feedforward model plant optimization algorithm 
model estimate outputs plant optimizer computes sequence control actions guides system output desired behavior 
cost function minimized optimization algorithm nu signal setpoint output estimated output variation control signal define respectively minimum maximum prediction horizon nu control horizon constant penalizing large variations control signal 
known levenberg marquardt algorithm optimization functional 
simplify optimization process adopt nu control action time step computed 
main advantage predictive control strategy relies ability compute optimal control action account estimates values system output time horizon 
anticipatory scheme results smoother transient effects caused sudden changes setpoint provides ability performing advance corrective actions external disturbances 
efficient vqtam approach npc strategies identification scheme discussed subsection iv modified order enable model predict system output single step 
purpose vectors built follows 
ny 
nu 
approach results larger prediction errors imposes network harder requirement estimating values time steps 
considerably stable submitted special issue ieee tnn temporal coding usual recursive approach plant outputs estimated feeding predicted value back network input 
general running recursive mode network needs reset avoid instabilities caused propagation estimation time 
usual search winning neurons obeys weight updating governed 
training completed scheme depicted outputs model computed follows summary steps som npc strategy step time step define signal build vector 
step compute output estimates 
step choose optimization algorithm minimize nu order find optimal control signal plant output close possible signal 
step apply control signal step control plant 
steps repeated signal available 
identification mimo systems previously said vqtam equations stated section ii refers general hard deal case multiple inputs multiple outputs mimo systems 
mimo systems inputs outputs time vectors components refer specific variable feature measured observed time 
assess learning ability vqtam approach regard mimo systems chosen task modeling reproduction complex robot trajectories 
choice allows performance comparison som previous self organizing approaches goal task estimate joint angles puma robot arm execution trajectory may contain repeated states 
purpose cartesian positions robot effector assumed input variables joint angles output variables 
direct substitution write version adapted task robot trajectory modeling reproduction follows 

nx vectors easily defined 

nx winning neuron weights updated 
important note formulation allows som model simultaneously inverse kinematics relationships temporal aspects trajectory 
training completed estimate joint angles robot trajectory follows estimate controlling robot arm 
simulations simulations aim compare performance vqtam standard approaches case studies discussed previous section 
standard approaches mean mlp rbf networks linear regression methods 
particularly interested understanding influence parameters som final prediction error model order ny number neurons number training epochs size training data set 
influence neighborhood function studied 
time series prediction modeling prediction chaotic time series relatively new research topic dating back 
interesting point field noise free conditions chaotic system shows apparently random behavior may modeled techniques nonlinear deterministic system identification 
perfect modeling dynamical behavior system noise free case short term predictions possible due extreme sensitivity chaotic systems uncertainties initial conditions 
time series considered obtained known mackey glass differential equation delay td dy dt ay td td time series sample time step td system approaches stable equilibrium point td enters limit cycle td chaotic td successive period doubling bifurcations td 
adopting td generated time series length scaled 
simple euler method approximate derivative dy dt 
cross validation purposes samples train remaining samples test models 
simulation evaluates influence model order ny prediction error computed test data 
submitted special issue ieee tnn temporal coding nrmse model order amplitude time fig 

prediction error nrmse versus memory order ny 
sequence estimated samples mackey glass 
nrmse size training set number epochs number epochs size training set fig 

evolution prediction error number epochs size training set som increases 
dimensional som neurons trained epochs 
epoch consists presentation entire training sequence minus ny samples 
training parameters set prediction errors computed ny time steps minimum value ny 
note absence memory ny generates highest errors deep memory ny imply performance 
mackey glass time series best result obtained ny 
shows samples estimated som ny 
second simulation concerned influence number training epochs size training set final prediction error 
tests performed ny fixed number epochs number training vectors increased increments 
size training set prediction error test set computed 
test gives idea information training data provided som order perform test set 
fixed size training set number epochs varied increments 
roughly speaking test reveals fast som acquires knowledge data perform test data 
results tests shown 
easily infer need train som epochs training set length order small prediction errors 
values higher ones improve considerably network prediction performance 
simulation evaluates sensitivity som number neurons model help reduce number 
test training set length train dimensional som epochs ny 
number neurons varied increments 
number neurons prediction errors generated som model computed 
results shown 
noted prediction errors som continue decay exponentially number neurons increases error curve decays faster stabilizing value just neurons 
neurons error obtained som high nrmse due quantization effect process resulting plain som algorithm 
interpolated output provided model smoothes estimated time series reducing prediction error considerably 
say generalization ability better som 
samples mackey glass time series generated som algorithms neurons shown 
simulations compare som methods mlp rbf standard linear ar models 
coefficients ar model computed submitted special issue ieee tnn temporal coding nrmse som ar som number neurons amplitude som original time fig 

evolution prediction error mackey series increasing number neurons 
estimated samples mackey glass time series generated som models neurons 
maximum likelihood method 
algorithms adopted ny 
dimensional som neurons trained epochs model neurons trained epochs 
experimentation data best configuration mlp network units hidden layer unit output layer 
transfer function neurons hidden output layers hyperbolic tangent function 
mlp trained backpropagation learning algorithm adaptive learning rate momentum 
values learning rate momentum factor set 
best configuration rbf network gaussian basis functions centers computed means algorithm output neuron weights computed ordinary squares 
radii spread gaussian kernels set 
number input units mlp rbf equal dimension table shows final prediction errors generated algorithms mackey glass time series known time series 
worth noting som model produced prediction errors order magnitude produced mlp rbf networks 
best performance time series 
expected neural models performed better linear 
comparisons som mlp rbf networks 
siso system identification shows measured values valve position input variable oil pressure output variable hydraulic actuator 
worth noting oil pressure time series shows behavior caused mechanical resonances 
som mlp rbf networks nonlinear models approximate forward inverse mappings hydraulic actuator 
assumption uncorrelated gaussian noise maximum likelihood approach equivalent squares method 
table minimum prediction errors nrmse som mlp rbf ar models chaotic time series 
model mackey glass lorenz laser som mlp rbf ar forward identification task neural nets compared usual linear autoregressive model exogenous inputs arx ny nu ai bj coefficients model estimated value plant output time step 
coefficients computed applying squares method input data directly weights som see subsection iv 
model accuracy evaluated root mean square error rmse rmse depending mapping learned length estimated series 
data models preprocessing stage 
total number inputoutput samples samples train submitted special issue ieee tnn temporal coding valve opening time oil pressure time fig 

measured values valve position left oil pressure right 
neural networks compute coefficients linear arx model remaining samples validate models 
training epoch defined presentation training samples 
simulations assumed ny nu suggested 
dimensional som neurons dimensional space dim dim simulated 
weights randomly initialized adjusted epochs training parameters best result generated som model validation rmse evaluation influence number training epochs approximation performance shown 
error decays fast initially stabilizing rmse epochs 
linear arx model provided higher error rmse 
mlp network input units dim hidden layer neurons output neuron 
neurons hidden layer hyperbolic tangent transfer functions output neuron linear transfer function 
mlp network trained backpropagation algorithm momentum 
values learning rate momentum factor set respectively 
training stopped rmse maximum number training epochs reached 
rbf network input units intermediate layer neurons gaussian basis function output neuron 
rbf design specific regression problems number neurons intermediate layer number training samples gaussian kernel centered training vector 
intermediate output weights just target values output simply weighted average target ranges random weight initialization tested significant influence final configuration weights 
oil pressure rmse time epochs fig 

simulation som upper model test data evolution estimation error training epochs lower 
solid line simulated signal 
dotted line true oil pressure 
values training cases close input case 
parameters need tuned radii gaussian kernels 
parameter rbf units deterministically varied evaluate effect accuracy approximation 
evaluate influence neighborhood function final approximation results simulated plain winner take wta network 
equivalent training som 
results submitted special issue ieee tnn temporal coding som mlp rbf fig 

results obtained som left mlp middle rbf right inverse identification hydraulic actuator 
table ii rmse mlp som wta rbf networks forward inverse identification hydraulic actuator 
forward identification inverse identification rmse min max mean var min max mean var mlp som wta rbf networks forward inverse identification hydraulic actuator shown table ii 
rmse values shown som wta mlp networks averaged training runs 
rbf radius varied minimum rmse maximum rmse increments 
note mlp network provides best results general 
som algorithm turn produces better results rbf network approximately number neurons 
minimum rmse rbf network occurs radius increasing approximate linear fashion maximum radius 
interesting result som sensitive weight initialization mlp network seen th th columns table ii 
sensitivity measured standard deviation rmse values generated training runs 
furthermore som adapt line new incoming data retraining entire network standard mlp case 
difficulty designing mlp network occurrence overtraining 
som network essence type vector quantization algorithm som network trained precise approximation statistical sense probability density training data 
training time learning som stabilizes rmse substantial reduction rmse occurs 
results wta network illustrate neighborhood cooperation crucial performance vqtam approach 
best results produced som mlp networks inverse identification hydraulic actuator shown 
specific cases estimation errors rmse som rmse mlp 
worth noting portions estimated time series errors high values 
occurs inverse mapping may unique issue inherently harder approximate forward mapping 
predictive control simulation considered nonlinear system model second order differential equation dt dy dt denotes input output 
total input output pairs train som generated solving pulse random binary signal samples training shown 
remaining samples testing model 
linear model plant coefficients computed subsection iv training model neurons epochs 
estimated output time series test shown 
worth noting relatively small number neurons approximation results quite due interpolated outputs 
having plant model possible design predictive controller 
known levenberg marquardt algorithm chosen optimization step due kind input signal widely system identification force system operate different dynamical ranges modes 
submitted special issue ieee tnn temporal coding input time series output time series time fig 

input signal corresponding output time series siso system identification task 
testing som identification nonlinear plant open circles denote true values output 
fig 

vqtam predictive controller 
upper plant output time series thicker line signal 
lower resulting optimal control sequence 
fast convergence speed 
parameters set follows 
joint simulation plant model optimizer shown signal 
noted tracking signal accomplished desired smooth transients abrupt change setpoint 
mimo system identification simulations generated data pairs matlab toolbox robotics moving robot effector path 
joint angles certain time steps recorded resulting cartesian positions computed means forward kinematics functions available toolbox 
main goal study ability som reproduce learned trajectory accuracy ambiguity caused presence repeated states 
som trained vqtam ap proach performance compared proposed competitive temporal hebbian cth network see appendix 
cth unsupervised algorithm designed store reproduce robot trajectories 
works storing states trajectory competitive feedforward weights temporal order encoded lateral hebbian weights 
temporal context units allow resolution ambiguities trajectory reproduction 
terms learning cth stores trajectory states directly competitive weights need training exemplar learning som needs recycling trajectory data epochs order get condensed representation input vectors prototype learning 
trajectory train networks contains states 
som neurons cth exactly units 
trial error memory orders set nx 
training parameters som 
cth amax 
table iii mean squared error mse som cth networks reproduction trajectory 
joint trajectory world trajectory cth som table iii shows resulting mse values networks asked reproduce learned trajectory 
order evaluate generalization ability networks initial states different training 
table note som performs better cth network matter error measured joint cartesian submitted special issue ieee tnn temporal coding fig 

upper row angular trajectories joint generated som cth networks reproduction trajectory 
solid line denotes desired trajectory asterisks estimated values 
lower row corresponding cartesian trajectories effector generated som cth networks open circles denote desired values 
world domain 
result somewhat predictable prototype learning som fewer neurons provides better generalization learning cth 
figures show values estimated som cth networks respectively joint robot arm reproduction trajectory 
corresponding cartesian values shown figures 
expected trajectories retrieved som approach closer desired values ones retrieved cth network 
vi 
discussion section discuss advantages vqtam method respect previous applications som algorithm identification control dynamical systems 
som 
issue addressed section concerns choice som algorithm competitive neural model implement vqtam approach 
firstly som chosen simple algorithm wide applicability neural network community 
furthermore number theoretic developments available account computational properties som 
principle vqtam method efficiently topology preserving competitive neural model topology representing network growing neural gas illustrated section som performs better competitive networks preserve topology data wta cth models 
topology preservation ability som direct result neighborhood function learning 
temporal associative memory associative nature vqtam differs standard approaches variants hopfield bam networks see aspects commonly conventional approaches pre wired weights non adaptive 
changes dynamics input output mapping learned automatically 
vqtam adaptive method associations learned time 
hopfield bam models build synaptic memory matrix directly input output data correlation hebbian learning rule vq tam builds memory weight prototype vec tors wout 
entire set weight vec tors forms compact representation input output pairs approach reduces compu submitted special issue ieee tnn temporal coding tational costs algorithm memory required store represent information 
competitive learning process vqtam leads prototype vectors representing centroids quantized regions inputoutput mapping 
prototype vectors seen filtered versions input output pairs moving average process takes place 
vqtam robust noise standard methods 
fact property inherited som network 
function approximation clustering techniques proposed function approximation 
system identification instance broader research field som seen clustering method fair discuss vqtam light previous 
objective function approximation oriented clustering proposed increase density prototypes weight vectors input areas target function presents higher variability response just regions input examples done clustering techniques 
authors argue goal adequate function approximation problems helps reduce prediction error means minimizing output variance explained model 
exactly originally done vqtam approach simpler way described 
due influence neighborhood function weight updating weight vectors converge steady state long learning rate annealed 
high density regions input data space better clustered low density areas input vector comprises ny past values input signal higher variability signal amplitudes higher chance data clusters separated data clusters detected network 
viewpoint som naturally suited function approximation tasks 
global versus local modeling som models learn input output mapping globally sense model structure 
authors proposed som build linear model structures responsible approximating localized region system state space improving approximation ability som 
approaches known local lin ear mapping llm similar vqtam neuron som stores input weight win corresponding output weight wout 
additionally neuron stores local gradient jacobian matrix ai input output pair calculated learning phase 
gradient information produce order expansion output space rep output weight wout leading improved som theory property called magnification factor meaning inverse point density weight vectors 
estimate output ai xin llm method linear interpolation technique smoothes quantized output original vq tam 
method applied robotics system identification time series prediction 
discussed subsection iv simpler computationally lighter approach build local models smooth output model weight vectors winners compute coefficients standard ar model see variant method 
need neuron store jacobian matrix 
general local modeling som produces better results global modeling nonstationary signals 
local modeling demands higher computational efforts global modeling issue may important real time control 
effective performance comparison different approaches local modeling topic research 
unsupervised error learning section iii demonstrated vqtam method self supervised gradient error reduction method 
general term self supervised denote unsupervised methods modified applied supervised problems 
methods named vector associative memory vam proposed describe self organizing neural circuits control planned arm movements visually guided reaching 
purpose suffices say neural circuits vam comprise dimensional topographic maps fixed topographic ordering imposed initially 
means topographic maps develop course learning required prior knowledge structure set input data network process 
fixed topographic maps provide automatic increase resolution representation movements trained frequently 
vqtam approach cooperation neighboring neurons plays special role convergence algorithm 
assuming linear neuron output yi xk th input neuron plain hebbian rule written wij xj yi learning rate 
modified hebbian rule developed addition usual number inputs modifiable weights neuron teaching signal established fixed negative synaptic weight 
case learning rule wij xj yi 
approach equivalent vqtam desired output introduced input pathway see order allow computation error signal guide weight adjustments 
vqtam general modified hebbian rule aspects 
weight vector connecting desired output neuron adjustable gives flexibility submitted special issue ieee tnn temporal coding method extract relevant information data 
second discussed previously cooperative learning procedure implemented neighborhood function essential accuracy function approximation 
taken account modified hebbian learning rule 
time series prediction discussed application domain major number contributions involving som algorithm 
methods representational power som build prediction models 
example temporal variants som help construction local linear ar models associated neuron network 
training network entire set training vectors local ar model neuron built solely subset input vectors neuron winner training stage 
subset input data weight vectors som compute coefficients ar model 
method described involves selection winning neurons direct explicit minimization prediction error 
equivalent vqtam criterion find winning neuron training involves output vectors wout vectors xin win 
write follows arg min sight equation intuitive original ultimate goal prediction minimum possible error time step 
vector desired outputs available prediction test stage limiting application method 
final approach equivalent vqtam time series prediction 
works developed independently vqtam proposed general framework wider range applicability 
vii 
identification control dynamical systems neural networks research fields completely dominated supervised learning paradigms 
unsupervised neural networks studied feasible alternative standard mlp rbf networks 
goal ambitious sense proposes common framework som approximate nonlinear input output mappings arising complex dynamical problems 
simulations shown illustrated potential vqtam technique 
demonstrated estimation errors decrease asymptotically som training proceeds allowing vqtam scheme understood self supervised gradient error reduction method 
performance proposed approach evaluated variety complex dynamical modeling tasks time series prediction ii identification siso mimo systems iii nonlinear predictive control 
tasks simulation results produced som accurate produced mlp network better produced rbf network 
furthermore verified som sensitive weight initialization mlp networks 
model performed better mlp models 
compared som produces smaller prediction errors considerably lower number neurons 
number training epochs needed train smaller required som 
discussed main properties vqtam relationships unsupervised methods dynamical system identification control 
pointed previous paragraph focused potentialities vqtam applications studied depth 
need done empirical theoretical terms order effectively consolidate proposed technique feasible alternative supervised paradigms 
issues certainly deserves attention residual analysis computation confidence intervals estimates generated som models comparison obtained supervised learning methods robustness noise 
performance comparison different approaches local input output modeling topic research 
authors cnpq dcr financial support 
appendix description cth algorithm input vector time comprises parts 
sensorimotor vector second part fixed context usually set initial state trajectory 
kept unchanged current sequence reached 
third part time varying context stm mechanism implemented tapped delay lines ct 
suitable length time window determined trial error 
time step current state vector compared feedforward weight vector terms euclidean distance follows ws fixed context distance df cf wf time varying context distance dt ct wt computed 
ds find winners current competition df dt submitted special issue ieee tnn temporal coding solve ambiguities trajectory reproduction 
output neurons ranked number output neurons 
index th neuron closest fi choice function defined follows di ds ri fi ri ds weighting factor ri called responsibility function 
consider neurons 
winners current competition represents current state vector context 
activation decay maximum value amax minimum amin follows amax amin amax max amax amin user defined ai function ri updated ri ri ai called exclusion parameter ri weight vectors wi adjusted ai wf ai cf wt ai ct learning rate wi initialized random numbers 
lateral weights mi mi mi 
mim mi encode temporal order trajectory states simple hebbian learning rule associate winner previous competition winner current competition mir mir ai ar gain parameter mir output values computed follows mir ar yi df yt dt 
function chosen dg du 
output values set yi control signal delivered robot computed weight vector neuron highest value yi arg maxi yi 
ljung system identification theory user prentice hall englewood cliffs nj nd edition 
chen billings nonlinear system identification neural networks international journal control vol 
pp 

narendra parthasarathy identification control dynamic systems neural networks ieee transactions neural networks vol 
pp 

hunt neural networks control systems survey automatica vol 
pp 

sj berg zhang ljung benveniste 
nonlinear blackbox modeling system identification unified overview automatica vol 
pp 

haykin principe making sense complex world neural networks dynamically model chaotic events sea clutter ieee signal processing magazine vol 
pp 

ravn hansen neural networks modelling control dynamic systems springer verlag 
narendra lewis special issue neural networks feedback control automatica vol 

pinkus approximation theory mlp model neural networks acta numerica vol 
pp 

schilling carroll jr approximation nonlinear systems radial basis function neural networks ieee transactions neural networks vol 
pp 

kohonen self organizing maps springer verlag berlin heidelberg nd extended edition 
barreto araujo ritter self organizing maps modeling control robotic manipulators journal intelligent robotic systems vol 
pp 

walter ritter rapid learning parametrized self organizing maps neurocomputing vol 
pp 

grumbach learning associations selforganization lasso model neurocomputing vol 
pp 

wang arbib complex temporal sequence learning short term memory ieee proceedings vol 
pp 


wang temporal pattern processing handbook brain theory neural networks pp 

mit press cambridge ma nd edition 
barreto ara jo time self organizing maps overview models international journal computer research vol 
pp 

principe principles networks self organization space time neural networks vol 
pp 

robbins monro stochastic approximation method annals mathematical statistics vol 
pp 

cottrell theoretical aspects som algorithm neurocomputing vol 
pp 

principe lefebvre neural adaptive systems fundamentals simulations john wiley sons 
principe wang local dynamic modeling self organizing maps applications nonlinear system identification control proceedings ieee vol 
pp 

tong non linear time series dynamical system approach oxford university press oxford uk 
balakrishnan weil literature survey mathematical computer modelling vol 
pp 

mills adaptive modelbased control neural networks international journal control vol 
pp 

generalized predictive control neural networks neural processing letters vol 
pp 

submitted special issue ieee tnn temporal coding evans williams development performance neural network predictive controller control engineering practice vol 
pp 

vega neural predictive control application highly non linear system engineering applications artificial intelligence vol 
pp 

lazar neural predictive controller non linear systems mathematics computers simulation vol 
pp 

nonlinear controller self organizing maps proc 
ieee international conference systems man cybernetics vancouver ca pp 

delgado control nonlinear systems self organising neural network neural computing applications vol 
pp 

sherali shetty nonlinear programming theory algorithms wiley nd edition 
ara jo barreto context temporal sequence processing self organizing approach application robotics ieee transactions neural networks vol 
pp 

schreiber nonlinear time series analysis cambridge university press 
mackey glass oscillations chaos physiological control systems science vol 
pp 

box jenkins time series analysis forecasting control holden day san francisco 
lorenz deterministic flow journal atmospheric science vol 
pp 

weigend editors time series prediction forecasting understanding past addison wesley 
specht generalized regression neural network ieee transactions neural networks vol 
pp 

corke robotics toolbox matlab ieee robotics automation vol 
pp 

barreto ara jo cker ritter distributed robotic control system temporal selforganizing neural network ieee transactions systems man cybernetics vol 
pp 

martinetz schulten topology representing networks neural networks vol 
pp 

fritzke growing cell structures self organizing network unsupervised supervised learning neural networks vol 
pp 

wang multi associative neural networks application learning retrieving complex spatio temporal sequences ieee transactions systems man cybernetics vol 
pp 

gonz lez rojas ortega prieto new clustering technique function approximation ieee transactions neural networks vol 
pp 

martinetz ritter schulten threedimensional neural net learning visuomotor coordination robot arm ieee transactions neural networks vol 
pp 

function approximation continuous self organizing maps proc 
icsc symposium neural computation nc berlin germany icsc academic press 
ramon extended self organizing maps local linear mappings function approximation system identification proc 
workshop self organizing map espoo finland pp 

martinetz schulten neural gas network vector quantization application timeseries prediction ieee transactions neural networks vol 
pp 

walter ritter schulten non linear prediction self organizing maps proc 
international joint conference neural networks ijcnn san diego ca pp 

grossberg vector associative maps unsupervised real time error learning control movement trajectories neural networks vol 
pp 

modified model hebbian synapse role motor learning human movement science vol 
pp 

lampinen oja self organizing maps spatial temporal ar models proc 
th scandinavian conference image analysis helsinki finland pp 

som local models time series prediction proc 
workshop self organizing map espoo finland pp 

kaski time series prediction recurrent som local linear models international journal knowledge intelligent engineering systems vol 
pp 

lendasse lee verleysen forecasting electricity consumption nonlinear projection selforganizing maps neurocomputing vol 
pp 

barreto ara jo self organizing network application prediction chaotic time series proc 
international joint conference neural networks ijcnn washington dc vol 
pp 

