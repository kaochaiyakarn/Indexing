learning relational navigation policies alexandru kristian kersting christian wolfram burgard luc de raedt machine learning lab autonomous intelligent systems group institute computer science university freiburg georges koehler building freiburg germany kersting burgard informatik uni freiburg de navigation fundamental tasks mobile robot 
majority path planning approaches designed entirely solve problem scratch current goal configurations robot 
approaches yield highly efficient plans computed policies typically transfer similar tasks 
propose learn relational decision trees navigation strategies example paths 
relational abstraction interesting important properties 
allows mobile robot generalize navigation plans specific examples provided users exploration 
second navigation policy learned environment transferred unknown environments 
experiments real robots real environment simulated runs demonstrate usefulness approach 
various tasks delivery guidance rescue mobile service robots need plan actions 
past vast majority approaches computing navigation paths dealt solving navigation problem robot faces operation scratch 
approaches yield highly efficient paths typically take account solutions similar problems 
addition navigation plans easily communicated humans hard instruct robot typical navigation behaviors 
consider problem learning navigation plans mobile robots set trajectories configuration space robot 
key idea utilize labels assigned individual places environment generalize sequences labels corresponding places traversed robot performing task 
problem planning trajectories mobile robots studied intensively past capability effectively planning motions eminently necessary definition robot accomplishes tasks moving real world 
different types planning problems coarsely classified information provided robot 
classical path planning problem situation robot perfect knowledge environment starting point goal position 
complex problems emerge robot possesses partial knowledge 
example location target unknown robot search target 
situations environment unknown target location known lrta popular algorithms guide robot goal location 
consider complex situation location target point priori 
situation example occurs robot find entrance hall hotel large office building 
problems different algorithms including depth search uninformed lrta see koenig comprehensive comparison proposed 
situations actions robot non deterministic 
approaches markov decision processes mdps proposed 
mdps provide sound theoretical framework deal uncertainty related robot motor perceptive actions planning plan execution phases 
techniques provide highly elegant efficient solutions corresponding problems ability improve performance learning past experience similar tasks entrance halls office buildings 
approach alleviates situation adopting techniques relational reinforcement learning reinforcement learning relational representation learn general search preferences navigation problems 
precisely technique starts set specific example navigation plans computed solving relational mdp obtained helpful teacher assumed set labels assigned position configuration space robot 
labels obtained robustly analyzing sensor measurements temporal evolution see 
observed labels form relational state descriptions relational markov decision process rmdp 
apply relational learning techniques generating abstractions 
result obtain relational decision tree expresses preferences navigation actions 
preferences robot generate navigation actions 
experiments show navigation algorithm deal noise observed labels gracefully degrades random search noise level increases 
past years relational representations machine learning ai received lot attention see 
known name statistical relational learning srl 
appears appropriate time apply srl techniques robotics 
advantages srl approach threefold 
learned navigation preferences directly transfered alternative instances vertical passage room emergency exit horizontal passage main entrance fig 

map hotel robot room supposed find way main entrance hotel 
navigation problem searching exit building 
additionally directly transferred environment way enable robot efficiently carry similar navigation tasks unknown environments 
approach applied settings example plans generated robot provided human guarantees optimal 
setting called behavioral cloning learning imitation 
experiments show cases approach beneficial 
organized follows 
briefly review mdps 
show learn navigation policies framework guide search mobile robot 
section iv reports experiments carried real robot simulation shows approach leads efficient navigation plans 
vertical passage ii 
markov decision processes running example hotel world task robot find way unknown hotel 
consider hotel 
mobile robot room 
time robot go room say 
action probabilistic probability action succeeds robot probability action fails robot stays 
natural formalism encode hotel world markov decision processes mdps 
mdps tuples set states 
set actions goto transition model reward model 
set actions applicable state denoted 
transition state caused action occurs probability reward received entering 
defines proper probability distribution states actions 
deterministic policy specifies action executed agent state policy discount factor state value function represents value state policy expected rewards 
policy optimal optimal value function denoted standard techniques exactly solving mdps value iteration vi 
vi algorithm assumes state space represented table stated follows starting value function states iteratively update value state vt max vt get 
vi guaranteed converge limit traditional mdps vi expressed equation essentially propositional state represented separate proposition 
severely limited expressiveness really capture structure underlying class problems 
consequence hard generalize policies domains similar properties 
instance propositional policies hotel directly applied hotels 
iii 
learning relational navigation policies relational mdps see combine relational logic mdps 
possible generalize policies cases hotels may possess varying number objects rooms relations connections 
relational logic hotel world elegantly represented relational logic 
reconsider hotel rooms ro ro 
ro connected con con con con con con symmetric facts 
types rooms horizontal vertical passages 
furthermore emergency exits entrance halls main 
time robot room actions robot take 
typical action going room horizontal passage goto 
probability actions fail robot stays current room 
task robot find way hotel enter main 
relational logic expressions form 
tm relation symbol followed bracketed tuple terms ti called atoms 
term variable constant 
follow convention variables start upper case constants lower case character 
main idea underlying replace propositional symbols mdps states 
state conjunction logical atoms logical query represents set states 
consider state con ro ro 
summarizes situations robot room connected room 
instance example con ro ro exists substitution substitution assignment terms variables 
xn tn xi variables ti terms 
term atom conjunction called ground contains variables 
conjunctions implicitly assumed existentially quantified 
conjunction said subsumed conjunction denoted exists substitution relational markov decision processes notions relational logic briefly review key ingredients actions rewards 
details refer 
action rule atom representing name arguments action state denoting preconditions state represents successful outcome value probability action succeeds 
semantics action current state subsumed action result probability probability action fails stay illustration consider con ro ro goto con ro ro describes robot going room horizontal passage probability 
applied state action goto yield con ro ro probability probability 
reward model specifies rewards generated entering states 
specified finite list value rules form state state assigns maximal value matching value rules value 
rule matches consider true 
assigns true value rule assures states assigned value 
simple reward model expressive hotel world basically shortest path problem goal reach main 
main entered process ends 
episodic tasks encoded called absorbing states specified set queries main 
example absorbing addition integrity constraints employed exclude impossible states cf 

relational navigation policies discuss compute navigation policies 
key observation rmdp induces traditional mdp obtained starting initial ground state applying transition new ground states computed 
existence optimal policy resulting ground mdp guaranteed 
hotel world navigation pattern goto ro con ro sake simplicity consider actions succeed fail cause costs 
general cases straightforward 
goto background knowledge con goto rr ro goto goto goto fig 

relational decision tree representing relational navigation policy hotel world 
states robot go horizontal passage room 
course policies extensional propositional sense specify ground state separately action execute 
learn policy intentionally specifies action take state set ground states abstraction 
formally relational navigation policy finite set relational navigation rules form action state 
instance relational navigation rule goto con ro ro generalizes rooms learn relational navigation policy start set traces ti ground situation action sequences lead goal state 
specific situation action sequences optimal instance obtained computing optimal policy fully known map environment instance provided human shows robot proceed particular initial goal goal state 
case correspond situation model known second corresponds model free case allows learn imitation perform called behavioral cloning 
key idea trace ti describes situation action sequence instance leaving hotel 
precisely ti consists ground navigation rules 
rule describes interpretation con ro ro simple enumeration ground facts robot needs know rooms connections rooms types rooms room horizontal passage vertical passage elevator order take associated optimal actions goto 
task induce relational navigation policy situation action pairs abstraction experience provided agent 
realized learning interpretations settings studied field inductive logic programming relational programs induced interpretation class pairs 
standard approach employ generalization relational decision trees 
relational decision tree see binary decision tree node contains conjunction ro 
node captures logical test succeeds fails applied particular state 
succeeds left subtree considered right 
nodes may share variables ancestor nodes con 
test performed node consists conjunction conjunctions succeeding path root node instance ro con 
leafs represent action taken state consisting conjunctions path leaf 
instance tree essentially encodes relational navigation rule leftmost path suggests take action goto state induce tree essentially employ quinlan known scheme information gain splitting criterion details see 
summarize approach works follows observe number successful ground state action sequences induce relational navigation policy form relational decision tree experience 
resulting navigation policy typically experiments uses local information environment need completely known 
akin explanation learning ebl subsequent successful problem solving session proof constructed explains success 
proof generalized description states solved way 
state space problems investigating proofs correspond showing sequence actions achieves goal ebl corresponds goal regression state action sequence 
surprising ebl generalization algorithm prodigy system learn general control rules specific examples problem solving episodes 
dietterich flann combined idea reinforcement learning associating generalized state descriptions values obtained value iteration 
subsequently boutilier kersting generalized dietterich flann approach relational domains 
weld suggested approximate value function inducing relational regression tree observed traces 
unfortunately relational description states share value increasingly complex states get farther farther goal number states covered state reduces dramatically 
results large number value rules 
observed case early ebl systems called utility problem 
avoid problem approach works directly state action sequences inductively generalizes relational policy trees 
time advantage contrast ebl model required allows apply techniques behavioral cloning 
particularly interesting case focus experiments concerned learning optimal state action sequences 
generated model rmdp fully known 
obtain optimal fig 

map real office environment experiments robot carried 
robot room node labels observable black 
labels topology environment unknown gray 
navigation policies learned different real buildings 
state action sequence ground rmdp compute optimal navigation policy resulting mdp mdp solver 
approach face utility problem typically learn compact policies approaches approximating value function 
iv 
experiments algorithm evaluated experiments carried real pioneer dx robot equipped sick laser range finders simulation 
goal experiments demonstrate navigation plans effectively control mobile robot reach target location unknown environments 
implementation details current system system spudd solve mdps generate example navigation plans 
execute navigation plans robot simulation runs robot navigation toolkit carmen 
assume robot identify type place current location type place door leads 
require information free noise experiments demonstrates 
assumptions perform forward search guided learned navigation policy 
start state determine action perform evaluating relational decision tree current state 
perform action observe state repeat process 
relational navigation policies deterministic system needs choose equally actions 
choose possibilities put ones chosen list alts 
robot encounters loop dead calculates shortest path current state state alts chooses shortest distance current location 
case alts empty put state connected visited state alts 
navigation office environment experiments designed demonstrate approach results effective navigation behaviors realworld scenarios 
experiments carried real mobile robot typical office environment see 
task robot find entrance fig 

application policy finding entrance hall building 
robot leaves room enters corridor left image 
samples randomly moves corridor middle image 
step different experiment chose upper corridor directly right image 
hall building navigation policy learned abstracting optimal trajectories calculated floor plans buildings 
actual map environment unknown robot just labels neighboring rooms observed 
experiments described robot started upper seminar room labeled navigation policy action robot leave room enter corridor labeled situation carrying action part environment observed robot far depicted left image 
point navigation policy outputs equally alternatives corridor hallway experiment chose turn right enter corridor labeled place labeled corridor robot decided return choose alternative corridor adjacent corridor proceeded area labeled corresponds entrance hall 
resulting trajectory robot depicted middle image 
rightmost image shows resulting trajectory case optimal corridor sampled directly robot simulation experiments quantitatively evaluate performance approach compared optimal paths real time search methods interleave planning local searches plan execution 
popular real time search method robot navigation unknown terrain uninformed lrta maximum lookahead 
ran simulation experiments maps real buildings ones depicted 
outlines buildings manually generated annotated topological map calculating paths 
evaluate performance randomly chose starting locations 
starting locations indicated yellow gray labels 
goal robot tasks find exit building indicated fig 

maps second set experiments 
upper maps learning lower testing 

average optimal plan length mean standard deviation 
method achieved uninformed lrt performed steps reach exit 
approach required substantially fewer steps uninformed lrta 
note experiments count room visited step 
lrta performed case superior approach 
illustrates approach substantially increases efficiency resulting navigation plans 
time plans steps longer optimal plans 
additional experiments reported sampled test revealed improvement obtained policy search significant level 
behavioral cloning important aspect approach training instances need optimal paths 
generated manually sketching possible trajectories 
final experiment described carried analyze degradation performance case system learn sub optimal training instances 
evaluate performed experiment maps hotels hotel different areas 
leave cross validation tested performance approach compares optimal policy real time search algorithm 
general policy normalized success rate leave cross validation policy sub optimal training uninformed lrta steps allowed find goal normalized success rate noise experiment strategy strategy uninformed lrta noise level fig 

normalized success rates see text different maximal steps allowed reach goal left 
normalized success rates varying level noise observed labels right 
learned maps evaluated left 
performed restarts started randomly areas 
shows normalized success rate different approaches defined li number runs length optimal path li length path th run indicates goal reached steps 
differences significant sampled test 
approach substantially better uninformed lrta 
additionally policy learned suboptimal hand drawn trajectories worse policy learned optimal trajectories 
note policy abstracted hand drawn trajectories yields better paths manually generated paths times longer optimal ones generated algorithm showed overhead 
observation noise robots perception world perfect 
algorithm able cope noisy label observations gracefully degrades uninformed lrta noise level increases 
implemented strategies dealing situations belief place labels revised 
strategy returns previous place inconsistent place label detected strategy stays new room updates faulty label information continues navigating 
right diagram shows navigation performance changes varying level observation noise 
results obtained simulated runs hotels 
noise level specifies probability label observed different 
seen diagram strategy outperforms strategy noise levels performance smoothly degrades uninformed lrta noise level approaches 
presents new approach generating navigation policies relational learning 
key idea learn relational decision tree sequences places traversed robot carries task 
resulting tree guide search robot similar tasks 
advantages approach relational abstraction allows generalize previously planned paths transfer policies tasks previously unseen environments 
algorithm evaluated experiments real robots simulation runs 
results demonstrate learned policies highly efficient outperform uninformed lrta maximum lookahead 
additionally experiment learn trajectories sketched examples provided users 
lynch hutchinson kantor burgard kavraki thrun principles robot motion theory algorithms implementations 
mit press 
planning algorithms 
cambridge univ press 
latombe robot motion planning 
kluwer acad 
publ 
stentz focused algorithm real time replanning proc 
ijcai 
korf real time heuristic search artificial intelligence vol 
pp 

koenig agent centered search ai magazine vol 
pp 

bellman dynamic programming 
princeton univ press 
koenig simmons xavier robot navigation architecture partially observable markov decision process models artificial intelligence mobile robotics case studies successful robot systems 
mit press pp 

zeroski de raedt driessens relational reinforcement learning machine learning vol 
pp 

weld solving relational mdps order machine learning proc 
workshop planning uncertainty incomplete information 
mart nez burgard semantic place classification indoor environments mobile robots boosting proc 
aaai pp 

burgard supervised learning places range data adaboost proc 
icra 
de raedt dietterich getoor muggleton eds working notes dagstuhl seminar probabilistic logical relational learning synthesis 
sutton barto reinforcement learning 
mit press 
kersting van de raedt bellman goes relational proc 
icml pp 

muggleton de raedt inductive logic programming theory methods logic progr vol 
pp 

blockeel de raedt top induction order logical decision trees artificial intelligence vol 
pp 

quinlan programs machine learning morgan kaufmann series machine learning 
explanation learning survey programs perspectives acm comp 
surveys vol 
pp 

lent laird learning procedural knowledge observation proc 
international conference knowledge capture cap canada pp 

minton carbonell knoblock kuokka etzioni gil explanation learning problem solving perspective artificial intelligence vol 
pp 

dietterich flann explanation learning reinforcement learning unified view machine learning vol 
pp 

boutilier reiter price symbolic dynamic programming order mdp proc 
ijcai pp 

hoey st aubin hu boutilier spudd stochastic planning decision diagrams proc 
uai 
montemerlo roy thrun perspectives standardization mobile robot programming proc 
iros 
