hybrid adaboost algorithm integrate binding site predictions yi sun mark robinson rod adams paul kaye alistair rust neil davey university hertfordshire college lane hertfordshire ab robinson adams kaye davey ac uk institute systems biology currently best algorithms transcription factor binding site prediction severely limited accuracy 
reason believe predictions different classes algorithms conjunction improve quality predictions 
previous applied single layer networks rules sets support vector machines predictions key real valued algorithms 
furthermore window consecutive results input vector order neighbouring results 
improve classification result aid hybrid adaboost algorithm working dataset windowed inputs 
proposed algorithm apply weighted majority voting 
data points classified easily weighted majority voting classified adaboost algorithm 
find method outperforms original individual algorithms classifiers previously 
address problem identifying transcription factor binding sites sequences dna 
different algorithms current search binding sites 
produce high rate false positive predictions 
problem addressed reduce false positive predictions means classification techniques taken field machine learning 
integrated results different base algorithms identifying binding sites non linear classification techniques 
improve classification results employed windowed inputs fixed number consecutive results input vector neighbouring results 
data classes labeled binding sites non binding sites non binding sites 
sampling techniques working traditional neural network single layer networks rules sets rules contemporary classification algorithm support vector machines svm 
focus dataset windowed inputs 
apply adaboost algorithm data 
propose novel hybrid adaboost algorithm improve classification results 
north th street seattle wa usa org problem domain section 
section iii describes datasets 
briefly introduce adaboost algorithm propose hybrid adaboost algorithm section iv 
set common metrics assessing classifier performance covered section section vi briefly introduces experiments section vii gives experimental results 
ends section viii 
ii 
problem domain exciting active areas research biology currently understanding fine resolution gene expression achieved molecular level 
clear highly non trivial problem 
mapping coding region gene protein product straightforward relatively understood mapping gene expression profile information contained non coding region simple understood 
estimated human genome dna part full regulatory instructions 
cis regulatory component consists dna encodes site protein dna interaction gene regulatory system conversely trans regulatory component consists protein binds cis regulatory dna sequence 
cis regulatory elements form nodes connecting genes regulatory networks controlling important biological phenomena essential focus research field 
lines research directly benefit effective means elucidating cis regulatory logic genes include embryology cancer pharmaceutical industry 
known mechanisms gene regulation act directly transcriptional sequence level example genes known play integral roles 
set regulatory interactions class dna binding proteins known transcription factors short sequences dna bound proteins virtue dimensional conformation 
transcription factors bind number different related sequences 
base substitution cis regulatory element commonly simply modify intensity protein dna interaction 
flexibility ensures cis regulatory elements networks form connecting nodes fairly robust various mutations 
unfortunately complicates problem predicting cis regulatory elements random background non coding dna sequences 
current state art algorithms transcription factor binding site prediction spite advances severely limited accuracy 
show large sample annotated yeast promoter sequences selection key algorithms unable reduce false positive predictions annotated binding sites recovered 
algorithms represent wide variety approaches problem transcription factor binding site prediction regular expression searches pwm scanning statistical analysis regulation evolutionary comparisons 
reason believe predictions different classes algorithms complementary integrated improve quality predictions 
described take results afore mention algorithms combine different feature vectors shown section 
investigate integrated classification results algorithms produce better classifications algorithm 
iii 
description data data possible binding positions prediction result algorithms 
algorithms categorised higher order groups single sequence algorithms algorithms comparative algorithm evolutionary algorithm 
label information contains best information able gather location known binding sites sequences 
denote prediction binding site location denote predictions binding site location 
base algorithms prediction result binary real valued see 
data consists ary real vectors associated binary value 
divide dataset training set test set training set test set 
data repeated vectors label repeated items different labels inconsistent items 
obviously unhelpful repeated inconsistent items training test set removed 
data drawn sequence dna nucleotides label near locations relevant label particular location 
training test data windowing vectors shown 
locations side giving window size consequent input vector size 
gene sequence known binding sites key algorithm results fig 

dataset columns possible binding prediction binary real value 
algorithms give prediction sequence position column shown 
key algorithm results labels windows label label label fig 

window size set study 
middle label continuous prediction sites label new windowed inputs 
length windowed input show results including better svm 
focus datasets windowed inputs 
iv 
hybrid adaboost algorithm adaboost algorithm adaboost algorithm produces sequence weak classifiers collectively form strong classifiers 
idea firstly weak classifier 
data points poorly classified frequency increased new dataset train second weak classifier 
process iterates specified number iterations 
final strong classifier linear combination weak classifiers 
weak classifier single layer neural network 
see table description adaboost process 
adaboost algorithm number interesting properties 
shown training error strong classifier approaches zero exponentially number iterations 
suggested generalisation performance adaboost algorithm related margin separation examples achieve large margin rapidly 
hybrid adaboost algorithm propose new classification procedure hybrid adaboost algorithm 
idea identify data points base algorithms find difficult 
adaboost algorithm applied points 
weighted majority voting identify difficult data points 
base algorithm votes confidence measured probability pattern positive computed training set single inputs follows number true positive examples number positive predictions denote summed confidence base algorithm give positive prediction negative 
threshold picked 
data points values close going classify 
point deemed difficult collected difficult sub set 
data points determine predicted binding site 
unfortunately naive method 
main reason dataset imbalanced means positive predictions seen effects confidence levels sums increase effect positive predictions multiplier 
condition determine data point difficult shown follows method applied variant 
complete description hybrid adaboost algorithm shown table ii 
practice cross validation choose see section vi 
classifier performance apparent problem domain imbalanced dataset classification accuracy rate sufficient table adaboost algorithm inputs data points number iterations initialise weights train classifier training set respect obtain hypothesis calculate weighted training error set update weights normalization constant terminate set final strong classifier table ii hybrid adaboost algorithm inputs data points multiplier threshold variant weighted majority voting calculating confidences training dataset test data point computes sum positive confidences sum negative confidences put point new dataset classify signed confidence value 
sampling sampling applied training set windowed inputs see section vi strong classifier generated adaboost algorithm see section iv training set resulting strong classifier classify difficult data points 
standard performance measure 
evaluate classifiers apply common performance metrics recall precision score calculated understand performance classification algorithm minority class 
confusion matrix see table iii computed test results tn number true negative examples fp false positive examples fn false negative examples tp true positive examples common performance metrics defined follows table iii confusion matrix tn fp fn tp recall tp tp fn precision tp tp fp score accuracy recall precision recall precision tp tn tp fn tn fp fp fp rate fp tn vi 
experiments classification techniques previous support vector machine svm 
svm experiments completed libsvm available url www csie ntu edu tw cjlin libsvm 
sampling techniques imbalanced dataset learning dataset binding positions vectors imbalanced dataset 
dataset imbalanced supervised classification algorithms expected predict majority class non binding site category 
various methods dealing imbalanced data 
concentrate data method sampling majority class negative examples sampling minority class positive examples 
combine sampling sampling methods experiments 
sampling randomly selected subset data points majority class 
sampling case complex 
author addresses important issue class imbalance problem problem minority class contains small sub clusters 
indicates simply sampling replacements may significantly improve minority class recognition 
overcome problem apply synthetic minority oversampling technique proposed 
pattern minority class search nearest neighbours minority class euclidean distance 
dataset mixed continuous binary features follow suggestion binary features differ pattern nearest neighbours median standard deviations continuous features minority class included euclidean distance 
new pattern belonging minority class generated follows continuous features difference feature pattern nearest neighbour taken multiplied random number added corresponding feature pattern binary features majority voting principle element nearest neighbours feature vector space employed 
take nearest neighbours double number items minority class 
actual ratio minority majority class determined sampling rate majority class 
previous experience set final ratio half works 
parameter settings data build independent validation set evaluate model user chosen parameters obtained cross validation 
training set divided equal subsets validation set possible sets 
classifier range reasonable parameter settings selected 
parameter setting validated validation sets having previously trained validations taken performance classifier parameter setting 
parameter setting best performance classifier subsequent experiments 
example hybrid adaboost algorithm parameters weighted majority training data 
mean performance voting part 
different combinations evaluated 
selected range vii 
results results shown table iv best base algorithm highest score 
compared table iv common performance metrics possible binding sites windowed inputs 
classifier recall precision score accuracy fp rate best alg 
svm adaboost hybrid best base algorithm classifiers increase precision decrease fp rate 
seen hybrid adaboost algorithm clearly best classifier outperforms terms score 
score improves comparing best base algorithm svm respectively 
shows recall value hybrid adaboost algorithm nearly best base algorithm svm adaboost algorithm conservative predicting binding sites greater accuracy 
viii 
significant result hybrid adaboost algorithm considerably improve binding site prediction decreasing false positive predictions time improving score 
fact able reduce false positive predictions best base algorithm whilst maintaining number true positive predictions improving score new algorithm presents notable improvement algorithms tried 
instance considered svm place variant alternate hybrid adaboost algorithm 
version performed better ordinary adaboost algorithm original hybrid algorithm 
investigate searching method find suitable ratio minority majority classes give better results ii algorithm technologies cope imbalanced dataset iii considering wider range supervised meta classifiers ensemble learning algorithms 
important avenue explore examine biological significance results currently working visualisation tool 
apostolico bock lonardi xu efficient detection unusual words journal computational biology vol 
davidson hardwiring development organization function genomic regulatory systems development 
bailey elkan fitting mixture model expectation maximization discover motifs biopolymers proceedings second international conference intelligent systems molecular biology aaai press 
blanchette tompa program designed phylogenetic nucleic acids research vol 

relationship recall precision journal american society information science vol 
pp 

chawla bowyer hall kegelmeyer smote synthetic minority sampling technique journal artificial intelligence research 
vol 
pp 

crick genetic code sci am pp 

fawcett rule sets maximize roc performance proceedings ieee international conference data mining icdm los alamitos ca pp ieee computer society 
hughes tavazoie church computational identification cis regulatory elements associated groups functionally related genes saccharomyces cerevisiae journal molecular biology mar 
freund schapire decision theoretic generalization line learning application boosting journal computer system sciences 
wu chang class boundary alignment imbalanced dataset learning 
workshop learning imbalanced datasets ii icml washington dc 
japkowicz class imbalances focusing right workshop learning imbalanced datasets ii icml washington dc 
joshi kumar agarwal evaluating boosting algorithms classify rare classes comparison improvements ieee international conference data mining san jose ca 
markstein markstein markstein keys lee richardson levine decoding noncoding regulatory genomes proceeding st ieee computer society bioinformatics conference csb stanford ca usa august 
computational detection genomic cis regulatory modules applied body patterning early drosophila embryo bmc bioinformatics 
schapire freund bartlett lee boosting margin new explanation effectiveness voting methods 
annals statistics october 
pf smola learning kernels support vector machines regularization optimization mit press 
sun robinson adams rust davey real valued meta classifiers integrate binding site predictions proceedings ijcnn 
marchal de moor moreau gibbs sampling method detect represented motifs upstream regions genes proceedings recomb 
wu lin weng probability estimates multiclass classification pairwise coupling journal machine learning research pp 

www mrc ac uk software 
family caltech edu index html 
