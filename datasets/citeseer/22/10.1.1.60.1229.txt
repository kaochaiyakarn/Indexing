unified theory state abstraction mdps li thomas walsh michael littman state abstraction state aggregation extensively studied fields artificial intelligence operations research 
working ground state space decision maker usually finds solutions state space faster treating groups states unit ignoring irrelevant state information 
number abstractions proposed studied reinforcement learning planning literatures positive negative results known 
provide unified treatment state abstraction markov decision processes 
study particular abstraction schemes proposed past different forms analyze usability planning learning 
state abstraction state aggregation widely studied artificial intelligence operations research technique accelerating decision making 
abstraction thought process maps ground representation original description problem representation compact easier 
words abstraction allows decision maker distinguish relevant information irrelevant information 
computational perspective state abstraction technique making learning planning algorithms practical large real world problems 
focus state abstraction markov decision processes different types abstraction proposed including bisimulation homomorphism utile department computer science rutgers university piscataway nj cs rutgers edu tion policy irrelevance 
diversity choices natural questions arise unified definition state abstraction 
relationships abstractions 
solution mdp relate ground mdp 
guarantees 
select abstraction schemes 
different types abstractions developed independently unified treatment available 
results estimating bounding approximation errors state abstraction seek answer qualitative questions information lost abstraction applied optimal policy preserved fact lacking qualitative insights state abstractions lead negative results state abstraction 
example mccallum observed aggregating states form state abstraction impossible find optimal policy value iteration learning gordon reported chattering phenomenon sarsa combined improperly constrained state abstraction 
major contribution general treatment state abstraction unifies previous works topic 
providing immediately available algorithms solving concrete problems focus abstraction theory including formal definitions relationships qualitative properties preservation optimality learnability 
rest organized follows 
section presents notation reviews previous 
background prior section studies state abstraction formally examines particular types abstraction detail 
section discusses concrete examples illustrates abstractions affect size state space 
give concluding remarks mention research directions section 
background prior focus sequential decision making problems concrete model markov decision processes mdps 
mdp described tuple finite set states finite set actions transition function ss denoting nextstate distribution action state bounded reward function ra denoting expected immediate reward gained action state discount factor 
policy mapping states actions policy define function expected cumulative reward received executing state similarly state action value function expected cumulative reward received action state 
reinforcement learning agent attempts learn optimal policy value functions denoted respectively 
wellknown max max full model mdp set standard algorithms exists finding optimal value function optimal policy including linear programming value iteration policy iteration 
transition reward functions unknown decision maker gather information interacting environment 
fact agent able compute optimal policy interaction 
previous problem defining state abstraction rules focus previous 
introduced algorithm stochastic dynamic programming built aggregation trees factored setting create model states transition reward functions fixed policy grouped 
identified givan special case form abstraction bisimulation states transition reward functions aggregated 
flat representation mdp partitioning accomplished polynomial time 
partitioning provides compact representation ground mdp bisimulation strict criterion aggregation 
adaptations proposed 
ravindran examined state aggregation homomorphisms model strict action matching 
options mechanism encapsulating abstraction information :10.1.1.25.1865
note vehicle independently type abstraction performed 
givan analyzed approximate bisimulation drops exact equivalence requirement favor bound method producing boundedly accurate mdps :10.1.1.42.6651
similarly ferns proposed bisimulation metrics statistical semi metrics determine similarity states transition functions 
measure combined difference reward functions decide aggregation 
note criterion comes close comparing actual value functions states idea return shortly 
algorithms aggregate states iterations model planning algorithms value iteration policy iteration 
approach reminiscent previously proposed adaptive aggregation strategy groups states runs value iteration bellman residuals 
algorithm unique criterion allowance states aggregated abstraction step 
dietterich maxq hierarchy effectively aggregates states subtask reward transition functions policy consistent hierarchy :10.1.1.32.8206
mc proposed strict criterion aggregating states optimal action similar values actions statistical test 
mccallum allowed aggregation occur system learning mdp 
statistics abstraction online feature earlier works including algorithm aggregated states reward values action 
online aggregation strategy statistical tests provided jong stone groups states policy state abstraction theory abstraction mecha criterion exactness mdp nism 
notes bisimulation model equivalence exact strictest measure homomorphisms model equivalence exact match accounts spacial relations ing actions symmetry approximate tion model similarity flexible bounded builds bisimulation metrics model similarity statistically tested error bounds deducible maxq model equivalence exact integrated maxq hi hierarchically policies consistent stochastic dynamic pro equivalent models exact covered bisimulation gramming policy algorithm equivalent rewards statistically feature relevance values tested independent utile distinction equivalent best actions statistically aggregation occurs online similar values tested policy irrelevance equivalent best actions statistically may yield optimal policy tested ground mdp adaptive aggregation similar bellman residuals bounded states dis aggregated dynamically abstraction discovery require mdp model :10.1.1.32.8206:10.1.1.42.6651
table selected previous strategies state aggregation irrelevance states aggregated optimal action 
mdps learn policy larger mdp may yield optimal policies may prevent algorithms converging 
summary properties aforementioned table 
table orders algorithms roughly strictest coarsest abstractions 
noted coarser abstractions methods providing better potential computational efficiency generalization looser performance loss bounds value function 
methods overlap considerably endeavor formalize unified theory abstraction better understand similarities differences 
state abstraction theory adopt viewpoint giunchiglia walsh argue abstraction general mapping problem representation new representation preserving properties 
focus preservation properties needed agent decisions lead optimal behavior 
previous abstraction definitions applied insight 
example bisimulation essentially type abstraction preserves step model mdp transition reward functions policy irrelevance types abstraction attempt preserve optimal actions 
methods cited attempt fuzzy conservation properties statistical tests notions bounding interest developing formalism choose focus abstraction schemes states aggregated exact equality parameters abstraction scheme 
general definition state abstraction definition state abstraction mdps give novel 
definition able study formal properties abstraction subsequent sections 
definition ground mdp version define abstraction function state corresponding ground state inverse image set ground states correspond abstraction function 
note assumptions partitions ground state space guarantee defined weighting function needed 
definitions hand define transition reward state abstraction theory functions mdp follows ss 
easy verify defined nextstate distribution checking sum ss ss 
intuitively measures extent state contributes state 
rest mention necessary valid weighting function 
consider policies mdp translate policies ground mdp 
ground states treated identically natural translate policies rule value functions mdp defined straightforward way denoted respectively 
topology abstraction space subsection concerned question abstractions relate 
denote abstraction space set abstractions mdp need definition 
definition suppose say finer denoted iff states implies 
addition strictly finer denoted 
may say strictly coarser denoted 
say comparable 
obvious definitions relation satisfies antisymmetry transitivity 
arrive theorem 
theorem finer relation partial ordering 
words depict topology abstraction space graph obtain directed acyclic graph dag self loop node edge points abstraction abstraction 
finest representation ground representation denoted identity mapping extreme coarsest representation null representation consisting single state 
types abstraction abstractions mdp possible ways partition state space 
abstractions appear equally important 
useful abstraction preserve information critical solving original mdp 
subsection see pieces important information mdps lead respective abstractions defined 
definition mdp states define types abstraction arbitrary fixed weighting function 

model irrelevance abstraction model action state model model implies ra model model 

irrelevance abstraction policy action implies 

irrelevance abstraction action implies 

irrelevance abstraction class action optimal states class implies maxa maxa 
similar definitions possible state value functions focus state action value functions key role popular planning learning algorithms 
appear important definition play important role affecting planning learning efficiency 
state abstraction theory 
irrelevance abstraction class action optimal states class implies maxa maxa 
intuitively model preserves step model bisimulation preserves stateaction value function policies preserves optimal state action value function stochastic dynamic programming factored representations algorithm preserves optimal action value utile distinction attempts preserve optimal action 
properties abstractions previous subsection introduced collection state abstractions 
study properties formally provides insights previous findings 
question address abstractions relate 
theorem states form chain partial order 
furthermore exist examples showing equal general 
theorem mdp model 
consequence abstractions special case finer abstractions 
example instance instance irrelevance abstraction irrelevance abstraction 
ily model observation helpful consider learning planning problems prove property abstraction property automatically applies finer abstractions 
second question related planning 
specifically mdp may standard dynamic programming algorithms value iteration policy iteration solve obtaining optimal policy abstractions guaranteed optimal ground mdp 
results theorem abstractions model optimal policy optimal note examples coarsest possible implementations respective abstractions including listed ground mdp 
exists examples optimal policy abstraction suboptimal ground mdp see 
proof sketch able show operator state action value functions mdp contraction mapping optimal value function fixed point 
optimality follows fact optimal actions values preserved abstraction 
consider learning problem agent estimates optimal value function experience 
abstraction case qlearning requires modification 
observing transition st rt st agent backup state action space st rt max st appropriately decaying step size parameter 
standard tools stochastic approximation theory obtain results learning theorem assume state action pair visited infinitely step size parameters decay appropriately 
converges optimal state action value function ground mdp 
resulting optimal policy optimal ground mdp 

learning abstractions model 
learning abstraction necessarily converge 
behavior policy fixed learning converges respect weighting greedy policy optimal ground mdp may predict optimal values suboptimal actions ground mdp 

learning abstraction converge action value function greedy policy suboptimal ground mdp 
note policy search methods may find optimal policy case 
proof sketch part theorem follows earlier convergence results theorem conditions theorem satisfied 
second part assume state abstraction theory visited infinitely fixed behavior policy followed learner run steady state occupation distribution weighting function associated proportion fixed limit 
resulting mdp defined stationary 
convergence learning follows previous analyses conditions 
behavior policy fixed empirically observed chattering phenomenon similar gordon observed illustrated 
third part theorem comes existing examples shown 
details 
negative learning results described stem similar causes introduces pitfall non convergence convergence suboptimal policy 
investigate cases examples 
possibility qlearning fail converge 
example mdp abstraction sample run learning periodically changing policy provided figures 
suboptimal actions state action pairs susceptible 
removing action aggregate state provides example 
gordon observed behavior combination sarsa 
think learning backups behavior possible 
chattering phenomena caused learning algorithm partial observability introduced abstraction function generally function approximator 
suppose combine state action pairs different values say making value state 
weights state occupation probabilities changing policy example weights may come stationary distribution causing learning chatter 
cause chatter similar weighted sum hold strictly case 
guarantee qlearning convergence fixed policy 
knowledge time behavior catalogued learning abstraction scheme guaranteed preserve optimal actions 
note results hold just learning learning algorithm trajectory sampling combined non stationary policy 
introduces new liability learning possible chattering optimal action values 
possible learning converge values indicate optimal action suboptimal shortcoming policy irrelevance noted previously 
crux failure combination states lead value aggregate state state aggregated state higher value 
corresponds optimal action define values state action pairs recursively bellman equation 
step case optimal action suboptimal note concern limited step backup case state action pairs receive backup states path leading 
notice suboptimal action selection im possible abstraction scheme 
contrast model free reinforcement learning learning model learning prioritized sweeping agent builds empirical model approximate mdp interaction experience solves model 
context state abstraction agent builds empirical model solves translated back ground policy 
class learning algorithms results theorem abstractions model empirical model built experience converges true model infinite experience weighting function fixed 
furthermore model reinforcement learning converges optimal value function greedy policy optimal ground mdp 
optimality guaranteed general 
proof sketch weighting function fixed mdp defined regardless policy collect empirical data empirical model converges asymptotically true mdp samples 
abstractions model optimal policy empirical mdp converges optimal policy true mdp turn optimal ground mdp state abstraction theory value action values aggregate state timestep example problem applied learning may fail converge 
solid dashed lines represent actions respectively 
corresponding graph shows value function aggregated middle state actions 
policy changed steps action action 
probability random action selected 
notice instability chattering suboptimal action value 
mdp subjected yields optimal policy suboptimal examples modifications mdp investigated gordon 
theorem 
abstraction empirical model inherently flawed planning algorithm may fail find optimal policy ground mdp 
necessary conditions locally checkable abstraction coarsest abstraction function listed theorem sufficient convergence model setting convergence policy optimal ground mdp may speculate conditions defining necessary guarantee 
back open question posed mccallum asking utile distinction instantiation necessary sufficient condition preserving optimal policy 
limited sense find answer 
define set locally checkable abstractions lc step reward transition class step reachable states optimal function values candidate states decide states aggregated 
notice lc contains abstraction functions defined definition 
investigating class come theorem lc coarsest possible abstraction guaranteed model reinforcement learning algorithms conditions theorem learning fixed policy converge value function greedy policy optimal ground mdp 
theorem follows fact optimal action values changed construct malicious example spirit new value causes state trajectory leading aggregated state promote sub optimal action optimality lower bound coarseness stands necessary class reasoning transition reward function states aggregated ensure optimality ensuring preservation optimal action qvalue 
summary introduced general definition state abstraction derive formal properties natural types abstraction 
number choices abstractions important question shall prefer 
seen analysis information problem mdp lost coarser coarser abstractions 
example model gives opportunity recover essentially entire model possible represent learn optimal state action value function 
coarser ceases guarantee learnability value function suboptimal actions allow planning value iteration 
chain optimal planning generally lost optimal policy representable 
coarser abstraction results larger reduction state space turn translates efficiency solving problem 
tradeoff minimizing information loss maximizing state space reduction selecting abstractions 
section examine examples shed light state space reduction achieved abstractions 
case studies consider concrete widely studied examples study different types abstraction affect size state space 
open question find irrelevance abstractions efficiently enumerating possible policies give results 
note size state space sizes state spaces model 
problem taxi domain taxi navigate grid world pick passenger deliver destination :10.1.1.32.8206
states ground state space actions 
agent charged reward action final successfully delivering passenger destination 
problem known coffee domain versions studied literature 
consider simplified scenario robot wandering locations lab office coffee room mail room hallway 
goal fold tidy lab deliver coffee mail user sitting office 
total ground states robot able perform actions actions allowed states 
receives reward single action rewarded coffee mail delivered lab tidy 
reward functions obtain versions problem 
problem called variations studied names 
problem agent binary vector bits 
goal reach zero vector flipping bits illegal flip bit 
moment actions choose flip highest bit flip lowest bit flip random bit results immediate reward 
bit flip say bit highest bit bits reset bits unchanged higher bits set lower bits remain unchanged 
optimal action states flip highest bit optimal value state negative number state 
sizes coarsest state space type abstraction problems shown table 
note corresponds abstraction coarsest studied abstraction size greater 
results implications 
model appear provide great opportunity state space reduction compared 
taxi domain example state space large ground state space 
second provide greatest opportunity state space reduction 
shown previous theorems guarantee dynamic programming qlearning converge optimal policy value function 
observations combined considering theorem imply abstraction may useful type abstraction possesses guaranteed optimality leads significant state space reduction 
introduced theory state abstraction markov decision processes 
particular gave formal definition state abstraction domains model taxi coffee coffee table sizes state spaces 
analyzed properties general abstractions specific abstractions 
equipped theory insights better understanding obtained 
provided beginnings formal foundation classification state abstraction hope spur efforts formal analysis state abstraction novel powerful algorithms 
important problem address discovery problem 
model interaction experience agent discover specific abstraction 
efficient discovery algorithms exist abstractions bisimulation obvious efficiently discover types abstraction example exhaustively computing value functions 
givan voiced concerns regarding abstraction measures functions 
significant approximate tests value function irrelevance notions 
concede discovery problem abstractions open problem believe avenue research merits exploration particularly results regarding 
emphasize discovering abstractions important solving sets related mdps 
example abstractions form knowledge transferred set related tasks 
set basic results abstraction theory 
fact interesting extensions 
abstractions definition exact equivalence certain quantities 
definition stringent practice especially stochastic domains discounted mdps 
possible extension relax exactness allow form approximate soft abstraction 
second extend definition abstraction action homomorphisms 
third important see state abstraction theory hierarchical reinforcement learning 
fourth theoretical interest investigating operations abstractions intersection composition 
mention connection state abstraction factored representations state space partitioning continuous mdps 
gratefully acknowledge support national science foundation iis darpa hr nasa 
carlos provided solution taxi problem 
nick jong helpful discussions policy irrelevance 
james bean john birge robert smith 
aggregation dynamic programming 
operations research 
dimitri bertsekas david casta 
adaptive aggregation methods infinite horizon dynamic programming 
ieee transactions automatic control 
craig boutilier richard dearden goldszmidt 
stochastic dynamic programming factored representations 
artificial intelligence 
david chapman leslie pack kaelbling 
input generalization delayed reinforcement learning algorithm performance comparisons 
ijcai pages 
thomas dean robert givan sonia leach :10.1.1.42.6651
model reduction techniques computing approximately optimal solutions markov decision processes 
uai pages 
thomas dietterich :10.1.1.32.8206
hierarchical reinforcement learning maxq value function decomposition 
journal artificial intelligence research 
eyal dar mansour 
approximate equivalence markov decision processes 
colt pages 
norm ferns prakash panangaden precup 
metrics finite markov decision processes 
proceedings twentieth conference uncertainty artificial intelligence uai pages 
alison gibbs francis edward su 
choosing bounding probability metrics 
international statistical review 
fausto giunchiglia toby walsh 
theory abstraction 
artificial intelligence 
robert givan thomas dean matthew greig 
equivalence notions model minimization markov decision processes 
artificial intelligence 
geoffrey gordon 
chattering sarsa 
technical report cmu learning lab 
tommi jaakkola michael jordan satinder singh 
convergence stochastic iterative dynamic programming algorithms 
neural computation 
nicholas jong peter stone 
state abstraction discovery irrelevant state variables 
ijcai 
li michael littman 
lazy approximation solving continuous mdps 
aaai pages 
michael littman carlos alexander strehl 
hierarchical approach efficient reinforcement learning 
proceedings icml workshop rich representations reinforcement learning 
michael littman ri 
generalized reinforcement learning model convergence applications 
icml pages 
andrew mccallum 
reinforcement learning selective perception hidden state 
phd thesis university rochester rochester ny 
andrew moore christopher atkeson 
prioritized sweeping reinforcement learning data time 
machine learning 
andrew moore christopher atkeson 
parti game algorithm variable resolution reinforcement learning multidimensional state spaces 
machine learning 
ronald parr stuart russell 
reinforcement learning hierarchies machines 
advances neural information processing systems pages 
martin puterman 
markov decision processes discrete stochastic dynamic programming 
wiley interscience new york 
ravindran andrew barto 
smdp homomorphisms algebraic approach abstraction semi markov decision processes 
ijcai pages 
herbert robbins sutton monro 
stochastic approximation method 
annals mathematical statistics 
david rogers robert richard wong james evans 
aggregation disaggregation techniques methodology optimization 
operations research 
richard sutton 
generalization reinforcement learning successful examples sparse coarse coding 
advances neural information processing systems pages 
richard sutton andrew barto 
reinforcement learning 
mit press cambridge ma march 
richard sutton david mcallester satinder singh mansour 
policy gradient methods reinforcement learning function approximation 
advances neural information processing systems pages 
richard sutton precup satinder singh :10.1.1.25.1865
mdps semi mdps framework temporal abstraction reinforcement learning 
artificial intelligence 
benjamin van roy 
performance loss bounds approximate value iteration state aggregation 
mathematics operations research 
appear 
christopher watkins 
learning delayed rewards 
phd thesis king college university cambridge uk 
