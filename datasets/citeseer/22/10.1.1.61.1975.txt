augmented pac model semi supervised learning pac model semi supervised learning maria avrim blum standard pac learning model proven useful theoretical framework thinking problem supervised learning 
tend capture assumptions underlying semi supervised learning methods 
chapter describe augmented version pac model designed semisupervised learning mind help think problem learning labeled unlabeled data different approaches taken 
model provides unified framework analyzing unlabeled data help discuss sample complexity algorithmic issues 
model viewed extension standard pac model addition concept class proposes compatibility function type compatibility believes target concept underlying distribution data 
example believes target cut low density region space self consistent way training 
belief explicitly represented model 
unlabeled data potentially helpful setting allows estimate compatibility space hypotheses reduce size search space set hypotheses assumptions priori reasonable respect distribution 
proposing model analyze sample complexity issues setting type data expect need order learn basic quantities numbers depend 
provide examples sample complexity bounds uniform convergence cover algorithms algorithmic results 
augmented pac model semi supervised learning seen previous chapters growing interest unlabeled data labeled data machine learning number different approaches developed 
assumptions methods quite distinct captured standard theoretical models 
difficulty theoretical point view standard discriminative learning models really capture unlabeled data help 
particular pac model purposefully complete disconnect data distribution target function learned valiant blumer kearns vazirani 
prior belief belongs class known fully function possible 
instance perfectly natural common talk problem learning concept class dnf formulas linial intersection halfspaces baum blum kannan vempala uniform distribution clearly case unlabeled data useless just generate 
learning unknown distribution standard pac setting unlabeled data help somewhat allowing distribution specific sample complexity bounds fully capture power unlabeled data practice 
generative model settings easily talk theoretically unlabeled data castelli cover 
results typically strong assumptions essentially imply natural distinction unlabeled data distribution 
instance typical generative model setting assume positive examples generated gaussian negative examples generated gaussian 
case unlabeled data principle recover gaussians need labeled data tell gaussian positive negative 
strong assumption real world settings 
model allow distribution data documents want classify number plausible distinctions want 
addition general framework model different uses unlabeled data 
chapter pac style framework bridges positions believe help think ways 
castelli cover assume gaussians particular assume distributions distinguishable perspective issue 

fact generative model setting practical side goes direction see nigam nigam 
discuss connections generative models section 
main idea unlabeled data typically including approaches discussed chapters 
framework extends pac model way allows express form target function considering relationships hopes target function underlying distribution possess 
analyze sample complexity issues setting type data expect need order learn give examples algorithmic results model 
specifically idea proposed model augment pac notion concept class set functions linear separators decision trees notion compatibility function data distribution hope target function satisfy 
talking learning concept class talk learning concept class compatibility notion example suppose believe exist linear separator furthermore data happens cluster separator probably slice middle clusters 
want compatibility notion penalizes functions fact slice clusters 
framework extent unlabeled data helps depends quantities extent true target function satisfies assumption second extent distribution allows assumption rule alternative hypotheses 
instance data cluster functions equally satisfy compatibility notion assumption ends helping 
bayesian perspective think pac model setting prior just functions function underlying distribution relate 
model formal need ensure degree compatibility estimated finite sample 
require compatibility notion function compatibility function data distribution ex 
degree incompatibility think kind unlabeled error rate measures priori unreasonable believe proposed hypothesis 
instance example margin style compatibility define increasing function distance separator case unlabeled error rate measure probability mass close proposed separator 
training example views underlying belief true target decomposed functions view examples 
case define 
compatibility hypothesis underlying distribution pr 
setup allows analyze ability finite unlabeled sample reduce dependence labeled examples function compatibility target function correct assumption various measures helpfulness distribution 
particular model find unlabeled data help distinct ways 
augmented pac model semi supervised learning ways unlabeled data help formal framework target function highly compatible unlabeled data estimate compatibility principle reduce size search space just estimated compatibility high 
instance helpful set functions smaller entire set providing estimate unlabeled data allow refined distribution specific notion hypothesis space size annealed vc entropy devroye rademacher complexities bartlett mendelson size smallest cover itai vc dimension blumer kearns vazirani 
fact natural cases find sense unlabeled data reduces size search space best described distribution specific measures 
distribution especially nice may find set compatible small cover elements cover far apart 
case assume target function fully compatible may able learn fewer labeled examples needed just verify hypothesis 
effectively committing target generative models 
framework allows address issue unlabeled data expect need 
roughly vcdim form standard pac sample complexity bounds bound number unlabeled examples need 
technically set vc dimension care set defined complexity depends complexity complexity notion compatibility see section 
consequence model target function data distribution behaved respect compatibility notion sample size bounds get labeled data substantially beat hope achieve pure labeled data bounds illustrate number examples chapter 
section formally introduce mean notion compatibility illustrate number examples including margins training 
assume examples labeled unlabeled come fixed unknown distribution instance space labeled unknown target function standard pac model concept class hypothesis space set functions instance space assumption realizable case target function belongs class hypothesis true error rate defined err prx 
hypotheses formal framework legal notion compatibility margins distance respect defined dd prx 
err denote empirical error rate labeled sample denote empirical distance unlabeled sample 
define notion compatibility mapping hypothesis distribution indicating compatible order estimable finite sample require compatibility expectation individual examples 
imagine general notions property 
specifically define definition legal notion compatibility function overloading notation define ex 
sample define empirical average sample 
allow compatibility functions tuples examples case unlabeled sample complexity bounds simply increase factor settings known advance transductive learning see section drop requirement entirely allow notion compatibility legal 
definition compatibility notion incompatibility 
call unlabeled error rate errunl clear context 
sample errunl denote empirical average need notation set functions incompatibility value 
definition threshold define cd errunl 
cd similarly sample define cs errunl give examples illustrate framework example 
suppose examples points rd class linear separators 
natural belief setting data separated target function separate positive negative examples reasonable margin 
assumption transductive svm see joachims chapter book 
case front define farther distance hyperplane defined 
incompatibility probability mass distance 
define smooth function distance separator want commit specific advance 
contrast defining compatibility hypothesis largest probability mass exactly zero distance separator fit model augmented pac model semi supervised learning training written expectation individual examples definition distinguish zero exponentially close zero small sample unlabeled data 
example 
training blum mitchell assume examples come pairs goal learn pair functions instance goal classify web pages represent words page words attached links pointing page pages 
hope underlies training parts example consistent allows training algorithm bootstrap unlabeled data 
example iterative training uses small amount labeled data get initial information link words advisor points page page probably faculty member home page finds unlabeled example half confident link says advisor uses label example training hypothesis half 
approach variants variety learning problems including named entity classification collins singer text classification nigam ghani ghani natural language processing pierce cardie large scale document classification park zhang visual detectors levin 
mentioned section assumptions underlying training fit naturally framework 
particular define incompatibility hypothesis distribution pr 
example 
transductive graph methods set unlabeled examples connected graph interpretation edge believe endpoints edge label 
labeled vertices various graph methods attempt infer graph labels remaining points 
willing view distribution methods edges uniform distribution unweighted training define incompatibility hypothesis probability mass edges cut motivates various cut algorithms 
instance require boolean mincut method blum chawla finds compatible hypothesis consistent labeled data allow fractional define algorithm zhu finds compatible consistent hypothesis 
wish view distribution edges distribution vertices broaden definition allow function pairs examples 
fact mentioned perfect knowledge setting allow compatibility function legal 
discuss connections graph methods section 
example 
special case training suppose examples pairs points class linear separators believe points 
discussion regarding training see chapter book 
sample complexity results pair side target function 
version training require 
motivation want pairwise information example want features data point 
instance word sense disambiguation problem studied yarowsky goal determine dictionary definitions intended target word piece text plant linear separator indicate tree factory 
local context word viewed graph cuts placing rd edges correspond completely different type information belief word appears twice document probably sense times 
setting compatibility function example having concept class possible functions reduce just linear separators 
agreement example 
related setting training considered examples single points pair hypothesis spaces generally tuple ck goal find pair hypotheses low error labeled data agree distribution 
instance data sufficiently separated expect exist linear separator decision tree assumption reduce need labeled data 
case define compatibility prx similar notion 
sample complexity results sample complexity bounds fall framework showing unlabeled data suitable compatibility notion reduce need labeled examples 
basic structure results follows 
unlabeled data function measure complexity possibly uniformly estimate true compatibilities functions empirical compatibilities sample 
quantity give preference ordering functions reduce set functions compatibility larger true target function bounds number labeled examples needed learning 
specific bounds differ terms exact complexity measures issues stratification realizability provide examples illustrating certain complexity measures significantly powerful 
particular cover bounds section provide especially bounds training graph settings 
augmented pac model semi supervised learning interpretation uniform convergence bounds uniform convergence bounds section give tighter cover bounds apply algorithms particular form 
clarity case finite hypothesis spaces measure size set functions just number functions set 
discuss issues arise considering infinite hypothesis spaces appropriate measure size set compatible functions need account complexity compatibility notion 
note standard pac model typically talks realizable case assume agnostic case see kearns vazirani 
setting additional issue unlabeled error rate priori assumption target function unlabeled error low aim occam style bound stream labeled examples halt sufficient justify hypothesis produced 
finite hypothesis spaces give bound doubly realizable case 
theorem see mu unlabeled examples ml labeled examples mu ln ln ml ln cd ln probability err errunl err 
proof probability hypothesis errunl errunl mu value mu 
union bound number unlabeled examples sufficient ensure probability hypotheses cd errunl 
number labeled examples similarly ensures probability true error empirical error yielding theorem 
target function perfectly correct compatible theorem gives sufficient conditions number examples needed ensure algorithm optimizes quantities observed data fact achieve pac guarantee 
emphasize say algorithm efficiently learns pair able achieve pac guarantee time sample sizes polynomial bounds theorem 
think theorem bounding number labeled examples need function helpfulness distribution respect notion compatibility 
context helpful distribution cd small need labeled data identify sample complexity results interpretation function 
get similar bound situation target function fully compatible theorem see mu unlabeled examples ml labeled examples mu ln ln ml ln cd ln probability err errunl err furthermore errunl errunl 
particular implies errunl err high probability optimizes err errunl err 
proof theorem apply hoeffding bounds see devroye unlabeled error rates 
give simple occam luckiness type bound setting 
sample define ln cs errunl 
description length nats sort hypotheses empirical compatibility output index ordering 
similarly define ln cd errunl 
upper bound description length sort hypotheses approximation true compatibility 
get bound follows theorem set unlabeled data ml labeled examples probability satisfying err ml ln err 
furthermore ln ln probability satisfy 
point theorem algorithm observable quantities determine confident 
furthermore unlabeled data observable quantities worse learning slightly compatible function infinite size unlabeled sample 
note non distribution dependent ordering hypotheses inducing description length desc compatibility assumptions turn wrong desc 
case unlabeled data hurting helping 
infinite hypothesis spaces reduce notation assume rest chapter prx 
sample complexity results easily extended general case 
augmented pac model semi supervised learning interpretation infinite hypothesis spaces issue arises order achieve uniform convergence unlabeled error rates set complexity care define 
instance suppose examples just points line fa fa iff 
case vcdim 
imagine compatibility function fa depends complicated relationship real numbers case vcdim larger need unlabeled examples estimate compatibility second issue need appropriate measure size set surviving functions 
vc dimension tends choice instance consider case example margins data concentrated separated blobs set compatible separators large vc dimension entire class similar respect better consider distribution dependent complexity measures annealed vc entropy rademacher averages 
introduce notation 
specifically denote expected number splits points drawn concepts fixed denote uniform distribution expected number splits points drawn concepts get bounds follows theorem unlabeled sample size cdim mu labeled sample size ml log log log log cd ml expected number splits ml points drawn concepts unlabeled error rate sufficient probability err errunl err furthermore errunl errunl 
analog theorem infinite case 
particular implies err errunl high probability optimizes err errunl err 
proof sketch standard vc bounds devroye vapnik number unlabeled examples sufficient ensure probability estimate prx 
implies estimate unlabeled error rate errunl set hypotheses errunl contained cd 
bound number labeled examples follows devroye shown expected number partitions sample complexity results interpretation maximum standard vc proof 
bound ensures probability functions cd true labeled error empirical labeled error 
give bound specify number labeled examples function unlabeled sample useful imagine learning algorithm performing calculations unlabeled data deciding labeled examples purchase 
theorem unlabeled sample size max cdim cdim log log sufficient label ml examples drawn uniformly random ml log log cs ml probability err errunl err 
furthermore errunl errunl 
proof standard vc bounds form theorem imply number labeled examples ml sufficient guarantee theorem err replaced err error respect replaced 
number unlabeled examples ensure probability err err 
combining statements yields theorem 
err errunl high probability optimizes err errunl err 
assume errunl cs cs 
notice case example worst case distributions essentially recover standard margin sample complexity bounds 
particular cs contains separators split margin greater maximum number ways splitting ml points margin 
distribution nice bounds better may fewer ways splitting margin 
instance case separated blobs discussed large just 
give stratified version theorem follows theorem unlabeled sample size max cdim cdim log log sufficient probability simultaneously true label mk examples drawn uniformly random augmented pac model semi supervised learning mk log log cs mk err errunl err 
theorem analog theorem essentially justifies stratification estimated unlabeled error rates 
imagine having data dependent bounds labeled unlabeled data doing double stratification respect labeled unlabeled error rates 
particular derive bound follows theorem unlabeled sample size max cdim cdim log log sufficient probability simultaneously true label mk examples drawn uniformly random mk log log cs mk err errunl err 
similarly derive tight bounds rademacher averages 
different versions statements stronger bounds see blum 
cover bounds bounds previous section uniform convergence provide guarantees algorithm optimizes observed data 
section consider stronger bounds covers obtained algorithms behave specific way unlabeled examples choose representative set compatible hypotheses labeled sample choose 
bounds covers exist classical pac setting framework bounds algorithms type especially natural convenient 
recall set cover respect close prx 
illustrate produce stronger bounds consider setting example graph algorithms graph consists cliques vertices connected edges particular number edges connecting cliques small compared 
suppose target function labels cliques positive negative define compatibility sample complexity results examples cover bounds beat uniform convergence bounds hypothesis fraction edges cut target function unlabeled error rate 
set sl ml labeled examples highly compatible hypothesis consistent sl just separates positive points sl entire rest graph number edges cut nml hypothesis clearly high true error unbalanced 
uniform convergence 
hand set functions unlabeled error rate small cover particular partition cuts edges close positive function negative function target function complement target function cover bounds act concept class functions require constant number labeled examples 
case cover bounds beat uniform convergence bounds imagine examples pairs points class linear separators compatibility determined points side separator case example 
suppose simplicity target function just splits hypercube coordinate distribution uniform pairs having coordinate target fully compatible 
hard show polynomially unlabeled examples su log labeled examples sl high probability exist high error functions consistent sl compatible su 
uniform convergence 
contrast cover size set functions compatible su constant cover bounds allow learning just constant number labeled examples 
particular give cover bound follows 
theorem upper bound errunl size minimum cover cd mu unlabeled examples ml labeled examples cdim mu log log ml ln 
effectively cover bounds allow rule hypothesis say just separates positive points sl rest graph noting hypothesis close respect negative hypothesis hypothesis high labeled error rate 

proof set variables appear positive example sl appear negative example sl 
draw sl variable chance belonging high probability size consider hypothesis corresponding conjunction variables correctly classifies examples sl classifies example su negative example su chance satisfying variable size su means compatible su consistent sl true error high 
augmented pac model semi supervised learning probability identify hypothesis close proof sketch unlabeled sample su define follows labeling su consistent choose hypothesis errunl smallest hypotheses corresponding labeling 
obtain eliminating hypotheses property errunl 
apply greedy procedure obtain gs follows initialize 
gi argmin errunl 

unlabeled data determine hi crossing hi hypotheses property gi 

set increase goto 
bound mu sufficient ensure probability cover implies probability cover cd 
possible show probability cover cd size idea greedily creating cover respect distribution su creating cover respect cover cd respect furthermore doing functions greedy cover procedure cd respect optimal cover cd 
learn labeled data empirical risk minimization 
standard bounds see instance itai number labeled examples ensure probability empirical optimum hypothesis true error 
implies probability find hypothesis error 
interesting case unlabeled data helps substantially consider training setting target fully compatible satisfies conditional independence label property 
shown blum mitchell boost weak hypothesis unlabeled data setting assuming labeled data produce weak hypothesis 
related sample complexity results dasgupta 
show unlabeled data fact learn just single labeled example 
specifically possible show concept classes theorem assume err errunl satisfies independence label 
mu unlabeled examples ml labeled examples find hypothesis probability error provided mu cdim cdim ln ln sample complexity results cover bounds training ml log proof sketch convenience show bound assume simplicity setting example general case handled similarly just requires notation 
characterize hypotheses true unlabeled error rate 
recall pr concreteness assume predicts 
consider errunl define pr pr define pij pr 
clearly err 
errunl pr independence label get 
implies compatible hypothesis types 
close exactly err 

close opposite exactly err 

predicts negative exactly 

predicts positive exactly 
consider constant positive function constant negative function 
unlabeled sample su sufficient ensure probability hypothesis zero estimated unlabeled error true unlabeled error 
previous analysis kinds hypotheses consistent unlabeled data close close complement close close 
furthermore compatible unlabeled data 
check exists hypothesis errunl df df 
hypothesis exists know opposite close case know close set functions opposite close labeled data pick output lemma 
lemma consider subset containing opposite hypotheses property close ml log labeled examples sufficient probability concept close fact lower empirical error 
proof easy calculation ml log ml ml ml 
particular reducing poly reduce number labeled augmented pac model semi supervised learning algorithmic results examples needed ml 
fact result extended case considered merely satisfy constant expansion 
example illustrates data especially behaved respect compatibility notion bounds labeled data extremely 
section show case linear separators independence label give efficient algorithms achieving bounds theorem terms labeled examples polynomial time algorithm 
note bounds rely heavily assumption target fully compatible 
assumption hope belief need additional labeled examples just validate hypothesis produced 
section give examples efficient algorithms model 
simple case give simple example illustrate bounds section give polynomial time algorithm takes advantage 
instance space vars set variables set class monotone disjunctions vars set variables suppose say example compatible function vars vars vars vars 
strong notion margin says essence variable positive indicator negative indicator example contain positive negative indicators 
setup give simple learning algorithm pair 
unlabeled data construct graph vertices variable putting edge vertices example unlabeled sample vars 
labeled data label components 
target function fully compatible component get multiple labels component get multiple labels halt failure 
produce hypothesis vars union positively labeled components 
fully compatible unlabeled data zero error labeled data theorem sizes data sets bounds high probability hypothesis produced error 
notice want view algorithm purchasing labeled data simply examine graph count number connected components request ln ln labeled examples 
cs 
proof high probability cd purchasing number labeled examples theorem statement 
interesting see difference helpful non helpful algorithmic results need assume independence algorithmic results distribution problem 
especially non helpful distribution uniform distribution examples vars components 
case unlabeled data help needs labeled examples distribution non uniform lower bounds ehrenfeucht 
hand helpful distribution high probability number components small case features appearing independently label 
training linear separators consider case training hypothesis class class linear separators 
simplicity focus case example target function linear separator rd example pair points assumed side separator example line segment cross target hyperplane 
show results extended general setting 
previous example natural approach try solve consistency problem set labeled unlabeled data goal find separator consistent labeled examples compatible unlabeled ones gets labeled data correct doesn cut edges 
unfortunately consistency problem np hard graph embedded rd distinguished points np hard find linear separator cuts minimum number edges minimum 
reason additional assumption points example drawn independently label 
single distribution rd probability points drawn restricted positive side target function probability drawn restricted negative side target function 
note sample complexity results section extend weaker assumptions distributional expansion introduced need true independence algorithmic results 
blum mitchell positive algorithmic results training halves example drawn independently label assuming underlying function learnable statistical query algorithms true linear separators blum labeled data produce weakly useful hypothesis defined halves 
give improvement result showing run algorithm blum mitchell single labeled example obtaining efficient algorithm model 
worth noticing process simplify results blum 
detailed description statistical query model see kearns kearns vazirani 
augmented pac model semi supervised learning somewhat 
analysis need definition 
weakly useful predictor function inverse polynomial input size pr pr 
equivalent usual notion weak hypothesis see kearns vazirani target function balanced requires hypothesis give information target function unbalanced see blum mitchell 
theorem polynomial time algorithm number bits example learn linear separator assumptions polynomial number unlabeled examples single labeled example 
proof sketch assume convenience target separator passes origin denote separator 
assume convenience prd target function overwhelmingly positive overwhelmingly negative easy case arguments complicated 
define margin point distance separating plane equivalently cosine angle drawing large unlabeled sample xi xi denote sj set xi 
describe algorithm working fixed unlabeled sample just need apply standard vc dimension arguments get desired result 
step perform transformation ensure reasonable poly fraction margin poly outlier removal lemma blum vempala 
outlier removal lemma states algorithmically remove fraction ensure remainder vector maxx poly ex number bits needed describe input points 
reduce dimensionality necessary get rid vectors quantity zero 
determine linear transformation described blum transformed space unit length 
maximum bounded guarantees poly fraction points poly margin respect separating hyperplane 
avoid cumbersome notation rest discussion drop simply denote points separator transformed space 
distribution originally reasonable probability mass 
reader willing allow running time polynomial margin data set part argument needed 
algorithmic results reasonable margin identity anyway 
second step argue random halfspace poly chance weak predictor 
blum uses perceptron algorithm get weak learning need simpler labeled data 
specifically consider point angle imagine draw random subject half property 
prf 
poly fraction points poly margin implies prf prf poly 
means poly probability mass functions fact predictors 
final step algorithm follows 
observation pick random plug bootstrapping theorem blum mitchell unlabeled pairs noisy label feeding result sq algorithm repeating process poly times 
high probability random weakly useful predictor steps low error hypothesis 
rest runs algorithm guarantees 
observe 
function small err small errunl 
secondly assumption independence label shown theorem functions low unlabeled error rate functions close close close positive function close negative function 
simply examine hypotheses produced procedure pick low unlabeled error rate far positive negative functions close just draw single labeled example determine case 
easily extend algorithm standard training setting different follows repeat procedure symmetric way order find pair functions just try combinations pairs functions find small unlabeled error rate close positive negative 
labeled example produce low error hypothesis part example functions pair 
augmented pac model semi supervised learning related models discussion minimum cut normalized graph cuts constraints transductive analog model talk transductive analog inductive model incorporates existing transductive methods learning labeled unlabeled data 
transductive setting assumes unlabeled sample random small subset labeled goal predict rest order unlabeled examples express relationship hope target function distribution compatibility notion 
case compatibility hypothesis completely determined known need require compatibility expectation unlabeled examples 
setup sample complexity point view care labeled data need algorithmically need find highly compatible hypothesis low error labeled data 
presenting general theorems focus modeling aspect give examples context graph semi supervised algorithms binary classification 
methods usually assumes weighted graph defined priori encodes prior knowledge 
denote weighted adjacency matrix cs set binary functions minimum cut suppose cs define incompatibility weight cut determined implicit notion compatibility considered blum chawla algorithmically goal find compatible hypothesis gets labeled data correct solved efficiently network flow 
sample complexity point view number labeled examples need proportional vc dimension class hypotheses compatible target function known see kleinberg kleinberg number edges cut size global minimum cut graph 
note randomized mincut algorithm considered blum extension basic mincut approach viewed motivated pac bayes sample complexity analysis problem 
normalized cut consider normalized cut setting joachims cs define size weight cut determined fpos number points predicts negative size positive respectively 
cs define incompatibility fpos note implicit compatibility function joachims algorithmically goal find highly compatible hypothesis gets labeled data correct 
unfortunately corresponding optimization problem case np hard 
approximate solutions considered leading different semi supervised learning algorithms 
instance related models discussion gaussian random field harmonic function generative models fit model joachims considers spectral relaxation leads sgt algorithm relaxation semi definite programming considered bie cristianini 
harmonic function model algorithms introduced zhu zhu follows 
consider probabilistic prediction function defined incompatibility wi ft lf un normalized laplacian similarly model algorithm introduced zhou noticing incompatibility lf normalized laplacian generally graph kernel methods viewed framework consider incompatibility kf kernel derived graph see instance zhu 
connections generative models interesting consider generative models fit model 
mentioned section typical assumption generative setting mixture probability density function see instance venkatesh castelli cover 
means labeled examples generated mechanism label drawn distribution classes corresponding random feature vector drawn class conditional density py 
assumption typically mixture identifiable 
identifiability ensures bayes optimal decision border deduced known construct estimate bayes border 
essentially decision border estimated small labeled sample suffices learn high confidence small error appropriate class labels associated disjoint regions generated estimate bayes decision border 
see incorporate setting model consider illustration setting venkatesh assume class conditional densities dimensional gaussians unit covariance unknown mean vectors rd algorithm unknown parameter vector estimated unlabeled data maximum likelihood estimate determines hypothesis linear separator passes point orthogonal vector decision regions separated hyperplane labeled majority labeled examples region 
setting natural notion compatibility consider expected log likelihood function expectation taken respect unknown distribution specified 
detailed discussion see chapter book 
augmented pac model semi supervised learning relationship luckiness framework 
specifically identify legal hypothesis set parameters determine define ex log 
venkatesh show unlabeled sample large hypotheses specified parameters close property empirical compatibilities close true compatibilities 
implies observations gaussian mixtures maximum likelihood estimate close permutations 
motivates compatibility function model 
generally deal parametric families setting compatibility notion need impose certain constraints distributions allowed order ensure compatibility defined expected log likelihood bounded 
mentioned section kind generative setting really extreme model 
assumption distribution generates data really mixture implies knew distribution possible concepts left unlabeled data extremely useful 
connections luckiness framework worth noticing strong connection approach luckiness framework see shawe taylor mendelson philips 
cases idea define ordering hypotheses depends data hope lucky find functions compatible target 
main differences 
luckiness framework designed supervised learning uses labeled data estimating compatibility learning difficult task result bounds labeled data significantly better 
instance example described section non degenerate distribution dataset pairs probability completely shattered fully compatible hypotheses luckiness framework help 
contrast larger unlabeled sample potentially reduce space compatible functions quite significantly learn labeled examples depending distribution see section 
secondly luckiness framework talks compatibility hypothesis sample define compatibility respect distribution 
allows talk amount unlabeled data needed estimate true compatibility 
number differences technical level definitions 
easy availability unlabeled data settings growing interest methods try data expensive labeled data learning 
substantial disagreement related models discussion clear consensus unlabeled data helps 
chapter provided pac style model semi supervised learning captures ways unlabeled data typically provides general framework thinking issue 
high level main implication analysis unlabeled data useful notion compatibility target function low unlabeled error rate distribution helpful sense hypotheses low unlabeled error rate unlabeled data estimate unlabeled error rates 
consequence model target function data distribution behaved respect compatibility notion sample size bounds get labeled data substantially beat hope achieve pure labeled data bounds illustrated number examples chapter 
abu mostafa 
machines learn hints 
scientific american 
agrawala 
learning probabilistic teacher 
ieee transactions information theory 

blum 
augmented pac model semi supervised learning 
manuscript 

blum yang 
training expansion bridging theory practice 
nips 
bartlett mendelson 
rademacher gaussian complexities risk bounds structural results 
journal machine learning research pages 
baum 
polynomial time algorithms learning neural nets 
proceedings third annual workshop computational learning theory pages 
itai 
learnability respect fixed distribution 
theoretical computer science 
de bie cristianini 
convex transduction normalized cut 
manuscript 
blum chawla 
learning labeled unlabeled data graph 
proceedings eighteenth international conference machine learning icml pages 
blum kannan 
learning intersection halfspaces uniform distribution 
journal computer systems sciences 
blum mitchell 
combining labeled unlabeled data training 
proceedings eleventh annual conference computational learning theory pages 
blum frieze kannan vempala 
polynomial time algorithm learning noisy linear threshold functions 
algorithmica 
blum lafferty reddy 
semi supervised learning randomized 
icml 
blumer ehrenfeucht haussler warmuth 
learnability vapnik chervonenkis dimension 
journal acm 
lugosi massart 
sharp concentration inequality applications 
random structures algorithms 
bousquet lugosi 
theory classification survey advances 
manuscript 
castelli cover 
exponential value labeled samples 
pattern recognition letters 
castelli cover 
relative value labeled unlabeled samples pattern recognition unknown mixing parameter 
ieee transactions information theory 
collins singer 
unsupervised models named entity classification 
proceedings joint sigdat conference empirical methods natural language processing large corpora pages 
dasgupta littman mcallester 
pac generalization bounds training 
dietterich becker ghahramani editors advances neural information processing systems cambridge ma 
mit press 
devroye lugosi 
probabilistic theory pattern recognition 
springer verlag 
vempala 
optimal outlier removal high dimensional spaces 
proceedings rd acm symposium theory computing 
ehrenfeucht haussler kearns valiant 
general lower bound number examples needed learning 
inf 
comput 

personal communication 

learning recognize patterns teacher 
ieee transactions information theory 
ghani 
combining labeled unlabeled data text classification large number categories 
proceedings ieee international conference data mining 
joachims 
transductive learning spectral graph partitioning 
proceedings international conference machine learning icml 
joachims 
transductive inference text classification support vector machines 
proceedings sixteenth international conference machine learning icml pages bled slovenia 
morgan kaufmann 
kearns 
efficient noise tolerant learning statistical queries 
journal acm jacm pages 
kearns vazirani 
computational learning theory 
mit press 
kleinberg 
detecting network failure 
proceedings st ieee symposium foundations computer science pages 
kleinberg sandler 
network failure detection graph connectivity 
proceedings st ieee symposium foundations computer science pages 
donnell 
learning intersections thresholds halfspaces 
proceedings rd symposium foundations computer science pages 

rademacher penalties structural risk minimization 
ieee trans 
inform 
theory pages 

value agreement new boosting algorithm 
colt pages 
levin viola freund 
unsupervised improvement visual detectors cotraining 
proceedings ieee international conference computer vision iccv pages nice france 
ieee 
linial mansour nisan 
constant depth circuits fourier transform learnability 
proceedings thirtieth annual symposium foundations computer science pages research triangle park north carolina october 
mendelson philips 
random subclass bounds 
proceedings th annual conference computational learning theory colt 
nigam 
unlabeled data improve text classification 
technical report doctoral cmu cs carnegie mellon university 
nigam ghani 
analyzing effectiveness applicability training 
proc 
acm cikm int 
conf 
information knowledge management pages 
nigam mccallum thrun mitchell 
text classification labeled unlabeled documents em 
machine learning 
park zhang 
large scale unstructured document classification unlabeled data syntactic information 
pakdd lncs vol 
pages 
springer 
pierce cardie 
limitations training natural language learning large datasets 
proc 
conference empirical methods nlp pages 
venkatesh 
learning mixture labeled unlabeled examples parametric side information 
proceedings eighth annual conference computational learning theory pages 

probability error adaptive pattern recognition machines 
ieee transactions information theory 
shawe taylor bartlett williamson anthony 
structural risk minimization data dependent hierarchies 
ieee transactions information theory 
valiant 
theory learnable 
commun 
acm 
vapnik 
statistical learning theory 
john wiley sons 
vempala 
random sampling algorithm learning intersection half spaces 
proceedings th symposium foundations computer science pages 

learning dnf uniform distribution quasi polynomial time 
colt pages 
yarowsky 
unsupervised word sense disambiguation rivaling supervised methods 
meeting association computational linguistics pages 
zhou bousquet lal weston 
learning local global consistency 
nips 
zhu ghahramani lafferty 
semi supervised learning gaussian fields harmonic functions 
twentieth international conference machine learning pages washington dc usa 
aaai press 
zhu ghahramani lafferty 
active learning semi supervised learning gaussian fields harmonic functions 
icml workshop continuum labeled unlabeled data machine learning pages washington dc usa 
zhu ghahramani lafferty 
semi supervised learning gaussian fields gaussian processes 
technical report carnegie mellon university 
