computational model cerebral cortex current understanding primate cerebral cortex neocortex particular posterior sensory association cortex matured point possible develop family graphical models capture structure scale power neocortex purposes associative recall sequence prediction pattern completion functions 
implementing models readily available computing clusters grasp labs provide scientists opportunity experiment hard wired connection schemes structure learning algorithms inspired animal learning developmental studies 
neural circuits involving structures external neocortex thalamic nuclei understood availability computational model test hypotheses accelerate understanding circuits 
furthermore existence cortical substrate facilitate understanding brain enable researchers combine lessons learned biology state theart graphical model machine learning techniques design hybrid systems combine best biological traditional computing approaches 
decade researchers significant progress understanding structure function cerebral cortex associated regions human primate brain 
multi cell recordings imaging techniques fmri enabled scientists build foundational hubel wiesel 
experimental studies yielded insights concerning low level processing higher order cognitive functions see example review articles issue science encoding retrieving episodic memory miyashita role medial frontal cortex cognitive control 
argue current theoretical models neocortex sufficiently rich predictive power detailed specification warrant concerted effort copyright american association artificial intelligence www aaai org 
rights reserved 
thomas dean department computer science brown university providence ri cs brown edu implement subject computational experiment 
particular argue neocortex ideal candidate implementation relatively homogeneous structure state knowledge concerning function circuitry powerful inferential capability 
substantial progress refining models individual neurons mcculloch pitts contention aggregate model cortical circuitry necessary instantiate model approaches scale complexity human cortex 
scale complexity instantiated model useful basis experimental study potential component general hybrid computational architectures 
adopt bayesian graphical models approach jordan expressive modeling capability potential integration inferential components graphical models 
researchers developed mathematical models neocortex accord experimental results provide directions implementation anderson george hawkins lee mumford rao ballard zemel 
generalize lee mumford model hierarchical bayesian inference visual cortex borrow anderson sutton network networks model ideas learning connectivity large graphical models 
challenge design build composite generative graphical model combines top bottom hierarchical inference employs invariant compositional representations encoding retrieving patterns supports associative recall pattern completion sequence prediction functions 
graphical models neocortex early mumford proposed computational theory neocortex 
building subsequently described grenander pattern theory grenander potentially model brain terms generative model feedback resolve ambiguities 
dayan helmholtz machine provided approximation realizing approach feedback implementing priors 
feedback helmholtz machine utilized support learning integral part inference 
rao regions visual cortex shown left stack depicting hierarchical organization regions higher stack capture general visual features lower stack capture specific typically transient features 
alternative depiction right underscores fact regions physically arranged patches planar cortical sheet 
ballard predictive coding kalman filter model employs feedback inference approach limited due reliance linear models 
lee mumford model hierarchical inference visual cortex provides significant step forward advantage confluence ideas statistics neuroscience cognitive computational science 
specifically propose generative model visual cortex hierarchical bayesian inference model emphasizes role feedback meets head challenge coping combinatorial explosion conflicting interpretations 
remainder section lee mumford model emphasizing hierarchical structure pattern feedback required facilitate simultaneous top bottom inference 
shows regions visual cortex arranged left hierarchy increasingly visual features associated processing units right schematically appear distributed cortical sheet 
depicts postulated interaction regions implementing top bottom communication 
bottom communications combine primitive features ones top communications exploit expectations generated prior experience 
basic feed forward feedback circuitry depicted highly schematic form 
feed forward projections originate layer terminate layer feedback projections originate layers terminate layer 
exact circuitry complicated appear cortex fully capable implementing information flows sort shown 
lee mumford provide bayesian account hierarchical inference implied 
applied vision bayes rule enables combine previous experience form prior probability xb hidden variable xb context background knowledge imaging model xo xb relating observations xo variables xo xb xo xb xb typically assume imaging model depend background knowledge allowing 
cross section cortical sheet depicting columnar structure layers cerebral cortex 
triangles layers represent pyramidal cells circles layer cells triangles layers infra granular pyramidal cells 
black dots represent dendrites 
cells left schematically represent region consisting columns level hierarchy features communicating second region shown right higher level hierarchy braitenberg 
rewrite equation xo xb xo xb xb xo xb xo xb normalizing factor required xo xb proper probability distribution 
case early vision xo denotes output lateral geniculate nucleus lgn denotes features computed xb denotes information contextual background available time inference 
computes values finding posteriori estimate maximizes xo xb 
lee mumford extend simple model account processing multiple regions temporal cortex including inferotemporal cortex 
assume regions responsible computing features different levels abstraction 
chain rule simplifying assumption sequence xo xv xv xv xit variable independent variables immediate neighbors sequence write equation relating regions xo xv xv xv xit xo xv xv xv xv xv xv xit xit resulting undirected graphical model markov random field mrf chain variables xo xv xv xv xit follows xv xo xv xv xit xo xv xv xv xv xo xv xv xit xv xv xv xv xv xo xv xv xit xv xv xv xit 
schematic hierarchical bayesian framework proposed lee mumford 
regions visual cortex linked markov chain 
activity ith region influenced bottom feed forward data xi top probabilistic priors xi xi representing feedback region 
markov property plays important computational role allowing units depend immediate neighbors markov chain 
graphical models need potentials indicating preferred pairs values directly linked variables xv xo xv xv xit xo xv xv xv zo xv xo xv xv xit xv xv xv xv zv xv xo xv xv xit xv xv xv xit zv zi xi xj required normalization 
potentials learned experience essential model 
roth black taken ideas sparse coding image patches applied homogeneous markov random fields obtain translation invariant models local image statistics 
exploiting products distributions model welling hinton osindero contrastive divergence hinton learn gibbs distribution rich set potentials defined learned filters 
contrast previous approaches pre determined set filters translation invariant learning method produces filters distribution parameters properly account spatial correlations data 
resulting prior trained generic database natural images exploited bayesian inference method requires spatial prior 
demonstrate denoising algorithm remarkably simple lines matlab code achieves performance close best special purpose wavelet denoising algorithms 
advantage wavelet methods lies generality prior applicability different vision problems 
roth black learning algorithm batch method markov random field model hierarchical 
contrast george hawkins simple line algorithm learns parameters conditional probability density functions hierarchical level tree structured model level level starting lowest level 
demonstrate translation invariant recognition capabilities model simple line drawings 
results roth black george hawkins provide evidence possible learn parameters lee mumford model 
derives modeling cortical regions local experts encoding knowledge probabilistic relationships features hierarchy features 
expert seeks maximize probability computed features referred beliefs combining bottom feed forward feature selections top feedback expectations priors 
information propagates hierarchy top bottom messages change reflect combined expertise hierarchy 
loopy belief propagation murphy weiss jordan related turbo decoding berrou glavieux pearl belief propagation algorithm 
particularly appropriate inference networks sort illustrated 
loopy belief propagation local messages propagate hierarchy system moves global equilibrium state 
particle filtering candidate inference lee mumford hierarchical bayesian model particle filtering generalizes kalman filter dispenses restrictive requirements gaussian noise linear dynamics 
particle filtering operates pearl algorithm regions corresponding random variables graphical model communicate message passing 
communication asynchronous providing computationally simple framework hypothesized communication regions visual cortex 
combining belief propagation particle filtering lee mumford anticipated algorithms approximate inference non parametric belief propagation isard sudderth 
inferring single preferred feature value xi ith region cortical hierarchy particle filtering generates set values region 
values constitute sample distribution associated ith region serve proxy full distribution avoiding problem keeping track potentially exponential number possible states system 
lee mumford relate algorithmic behavior particle filtering ideas zemel population coding zemel 
basic organization linked regions illustrated forward backward algorithm serves inference involving hierarchically arranged visual features serves equally processing time series data sort arising language sequences data sensory modalities 
variants cortical model outlined represent hierarchical hidden markov models fine singer tishby factored markov decision processes boutilier dean hanks hierarchical reinforcement learning models barto mahadevan 
architectural issues biggest challenges implementing applying model arises wiring cortical regions corresponding variables sets variables graphical model 
imagine starting large un time abstraction sensory input output mappings architecture illustrating spatial layout columns responsible sequence prediction responsible inferring high level compositional features lower level features 
differentiated markov random field task hopeless 
start roughly planar topology regular neighborhood structure variety strategies institute non local connections 
mentioned earlier cortex consists layered sheet uniform cellular structure 
identified called columns corresponding groups local cells running perpendicular cortical surface see 
special issue journal cerebral cortex devoted cortical columns writes basic unit cortical operation 
containing order neurons 
measures order transverse diameter separated adjacent vertical cell sparse zones vary size different cortical areas grouped cortical columns formed binding common input short range horizontal connections take cortical column basic computational module anderson sutton gross structure neocortex consists dense mat inter columnar connections outermost layer cortex web connections base columns 
inter columnar connectivity relatively sparse order connections spanning approximately neurons evidence sporns suggest induced inter columnar connection graph exhibits properties small world graph newman watts strogatz particular low diameter length longest shortest path separating pair vertices graph enabling low latency communication cortical columns 
simple local connectivity sort suggested anderson sutton network networks model provides straightforward architecture allows simultaneously handle temporal sequences sensor input support hierarchy increasingly compositional features suggested lee mumford visual cortex model 
illustrates sensor sequences hierarchies representing multiple modalities simply replicating architecture shown 
increasingly features handled just local connections 
architecture allow implement hidden markov models easily 
course elbow pipe shown labeled sensory input output mappings masks additional dimensions give rise need longer connections 
mapped visual input spatial relationships preserve sensor modalities similar representational requirements 
imagine separate patch cortex local connections sensor modality effector modality 
assuming obvious replication structure architecture shown illustrates need simply local connections 
different modalities linked longer range connections allowing notice correlations neural activity corresponding sensors effectors occurring time 
implementing multi modal architecture may useful take hint nature selectively emphasize different regions computational analog blood flow cellular atp depletion 
danger thinking body unlimited energy reserves perform global energy mediated local optimizations economics blood flow energy conversion may important role learning avoiding fitting diffuse hard handle models 
primate brain consumes energy organ afford activate small fraction total complement neurons cognitive task active cortical structures tend clustered uniformly distributed entire cortex 
faced new task learn existing healthy structure easily adapted generalized cover brain tends opt underutilized cortical structures 
principles task require fraction total processing units new tasks assigned underutilized spatially separate units serve direct wiring time impose constraint avoid fitting analogous minimum description length mdl priors statistical machine learning 
representational issues lee mumford model addresses basic structure hierarchical inference visual cortex provides little detail concerning information represented different levels hierarchy 
continue underestimate importance designing preferably learning hierarchical representations solving problems involving perceptual inference image speech text processing particular 
cognitive scientists linguists neural modeling experts machine vision researchers generally agree need find way exploit compositional structure language natural scenes order avoid searching combinatorial space possible interpretations chomsky grenander 
may somewhat optimistic appears consensus opinion emerging regards compositional structure representational solution geman potter chi jackendoff 
implementing cortical model lines described directly address representation problem agreed substrate existence fast scalable implementations serve science ways 
obviously building standard foundation promotes sharing 
interesting experiments involve combining theories regarding different modalities require combining representations experts different areas experiments facilitated having initially developed representations separately shared framework 
second reason believe size matter secrets brain representational components consist new primitive features composition rules suitable pattern language require large number components span gap raw input concepts computation required exhibit basic competence easily swamp capabilities standard workstations 
third architecture sure evolve existence agreed standard focus effort synthetic biologically inspired representations contributions discouraged ad hoc embedded shared framework 
evidence suggest brain extensive invariant representations particularly visual system retinal circuits largely contrast invariant cells exhibit invariance respect position size receptive fields sensitive wide range poses 
invariants important enabling visual cortex capture essential characteristics ignoring irrelevant details fukushima riesenhuber poggio invariants play role processing sensory modalities 
unfortunately invariant representations mask differences reducing selectivity geman 
point designing compositional hierarchical representations increase selectivity reduce search combining top expectations bottom inference 
managing tradeoff invariance selectivity just representational challenges faced building artificial cortex 
implementation issues terms implementing cortex scale architecture immediate progress processors running nodes myrinet gigabit interconnect computing clusters existing interconnect technologies support functional units unit accounting neurons aggregate arranged hierarchically columns virtual connections allowing upwards unit unit communications millisecond 
mpi codes accelerating local message passing provide reasonable performance wide range experiments see johansson lansner discussion implementing cortex sized artificial neural networks clustered computers 
somewhat smaller large networks fit smaller clusters 
discounting technical challenges involved implementing high performance cortex sized models challenges increasingly simple manage continuing improvements performance affordable hardware 
neocortex represents set capabilities level robustness artificial intelligence researchers long 
understanding neural basis inference cortex advanced point computation representation center stage computer scientists contribute draw inspiration neural modeling 
choice mathematical tools exploits fact bayesian principles graphical models common neural modeling community due providing clear semantics expressive medium capturing neural function multiple levels detail 
time opportune creating computational framework statistical techniques modeling neocortex tools implementing cortex scale models available hardware 
framework serve role developing biologically inspired computational architectures bayesian reformulation quick medical model shwe pushing computer aided medical diagnosis simultaneously advancing research learning inference algorithms large graphical models 
anderson sutton 
compute faster understand better 
behavior methods instruments computers 
anderson 
arithmetic parallel computer perception versus logic 
brain mind 
barto mahadevan 
advances hierarchical reinforcement learning 
discrete event dynamic systems theory applications 
berrou glavieux 
near shannon limit error correcting coding turbo codes 
proceedings international conference communications 
boutilier dean hanks 
decision theoretic planning structural assumptions computational leverage 
journal artificial intelligence research 
braitenberg 
anatomy cortex 
berlin springer verlag 
chomsky 
knowledge language nature origin 
new york ny praeger 
dayan hinton neal zemel 
helmholtz machine 
neural computation 
fine singer tishby 
hierarchical hidden markov model analysis applications 
machine learning 
fukushima 
neocognitron self organizing neural network model mechanism pattern recognition unaffected shift position 
biological 
geman potter chi 
composition systems 
quarterly applied mathematics lx 
geman 
invited talk cvpr invariance selectivity ventral visual pathway 
george hawkins 
hierarchical bayesian model invariant pattern recognition visual cortex 
proceedings international joint conference neural networks 
ieee 
grenander 
general pattern theory mathematical study regular structures 
new york ny oxford university press 
hinton 
training products experts minimizing contrastive divergence 
neural computation 
isard 
real valued graphical models computer vision 
proceedings ieee computer society conference computer vision pattern recognition volume 
jackendoff 
foundations language brain meaning grammar evolution 
oxford uk oxford university press 
johansson lansner 
mapping cluster computers 
technical report na department numerical analysis computing science royal institute technology stockholm sweden 
jordan ed 

learning graphical models 
cambridge ma mit press 
lee mumford 
hierarchical bayesian inference visual cortex 
journal optical society america 
miyashita 
cognitive memory cellular network top control 
science 

special issue computation cortical columns 
cerebral cortex 
mumford 
computational architecture neocortex role cortical loop 
biological cybernetics 
mumford 
computational architecture neocortex ii role cortico cortical loops 
biological cybernetics 
mumford 
neuronal architectures problems 
large scale neuronal theories brain 
mit press 

mumford 
pattern theory mathematics perception 
proceedings international congress mathematicians volume iii 
murphy weiss jordan 
propagation approximate inference empirical study 
proceedings th conference uncertainty artificial intelligence 
morgan kaufmann 
newman watts strogatz 
random graph models social networks 
proceedings national academy science 
pearl 
probabilistic reasoning intelligent systems networks plausible inference 
san francisco california morgan kaufmann 
rao ballard 
dynamic model visual recognition predicts neural response properties visual cortex 
neural computation 
nieuwenhuis 
role medial frontal cortex cognitive control 
science 
riesenhuber poggio 
hierarchical models object recognition cortex 
nature neuroscience 
roth black 
fields experts framework learning image priors applications 
proceedings ieee conference computer vision pattern recognition 
ieee 
shwe middleton heckerman henrion horvitz lehmann cooper 
probabilistic diagnosis reformulation internist qmr knowledge base probabilistic model inference algorithms 
methods information medicine 
sporns 
small world cerebral cortex 

sudderth freeman willsky 
nonparametric belief propagation 
proceedings ieee computer society conference computer vision pattern recognition volume 
welling hinton osindero 
learning sparse topographic representations products student distributions 
becker obermayer eds advances neural information processing systems 
cambridge ma mit press 

zemel 
cortical belief networks 
hecht ed theories cerebral cortex 
new york ny springer verlag 
