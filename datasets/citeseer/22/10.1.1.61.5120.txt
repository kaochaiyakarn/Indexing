journal machine learning research submitted published online passive aggressive algorithms crammer crammer cis upenn edu ofer dekel cs huji ac il joseph keshet cs huji ac il shai shalev shwartz cs huji ac il yoram singer school computer science engineering hebrew university jerusalem israel singer cs huji ac il editor manfred warmuth family margin online learning algorithms various prediction tasks 
particular derive analyze algorithms binary multiclass categorization regression prediction sequence prediction 
update steps different algorithms analytical solutions simple constrained optimization problems 
unified view allows prove worst case loss bounds different algorithms various decision problems single lemma 
bounds cumulative loss algorithms relative smallest loss attained fixed hypothesis applicable realizable unrealizable settings 
demonstrate merits proposed algorithms series experiments synthetic real data sets 

describe analyze online learning tasks algorithmic prism 
introduce simple online algorithm call passive aggressive pa online binary classification see herbster 
propose alternative modifications pa algorithm improve algorithm ability cope noise 
provide unified analysis variants 
building unified view show generalize binary setting various learning tasks ranging regression sequence prediction 
setting focus online learning 
online setting learning algorithm observes instances sequential manner 
observation algorithm predicts outcome 
outcome simple decision case binary classification problems complex string large alphabet 
algorithm prediction receives feedback indicating correct outcome 
online algorithm may modify prediction mechanism presumably improving chances making accurate prediction subsequent rounds 
online algorithms typically simple implement analysis provides tight bounds performance see instance kivinen warmuth 

current affiliation department computer information science university pennsylvania walnut street philadelphia pa usa 

current affiliation google view ca usa 
crammer ofer dekel joseph keshet shai shalev shwartz yoram singer 
crammer dekel keshet shalev shwartz singer learning algorithms hypotheses set linear predictors 
class may restrictive pioneering vapnik colleagues demonstrates mercer kernels employ highly non linear predictors entertain formal properties simplicity linear predictors 
concreteness presentation analysis confined linear case referred primal version vapnik cristianini shawe taylor sch lkopf smola 
constructions linear kernel machines paradigm builds notion margin 
binary classification setting discuss 
setting instance represented vector prediction mechanism hyperplane divides instance space half spaces 
margin example proportional distance instance hyperplane 
pa algorithm utilizes margin modify current classifier 
update classifier performed solving constrained optimization problem new classifier remain close possible current achieving unit margin example 
forcing unit margin turn aggressive presence noise 
describe versions algorithm cast tradeoff desired margin proximity current classifier 
formalism motivated warmuth colleagues deriving online algorithms see instance kivinen warmuth 
furthermore analogous optimization problem arises support vector machines svm classification vapnik 
core construction viewed finding support vector machine single example replacing norm constraint svm proximity constraint current classifier 
benefit approach fold 
get closed form solution classifier 
second able provide unified analysis cumulative loss various online algorithms solve different decision problems 
specifically derive analyze versions regression problems prediction multiclass problems sequence prediction tasks 
analysis realm relative loss bounds 
framework cumulative loss suffered online algorithm compared loss suffered fixed hypothesis may chosen hindsight 
proof techniques surprisingly simple proofs fairly short easy follow 
build numerous previous results views 
mere idea deriving update result constrained optimization problem compromising opposing terms largely advocated littlestone warmuth kivinen colleagues littlestone kivinen warmuth 
online margin prediction algorithms quite prevalent 
roots papers date back perceptron algorithm rosenblatt 
modern examples include algorithm li long gentile alma algorithm gentile mira algorithm crammer singer norma algorithm kivinen 
mira algorithm closely related specifically mira algorithm binary classification identical basic pa algorithm 
mira designed separable binary multiclass problems algorithms apply nonseparable problems 
furthermore loss bounds derived crammer singer inferior general bounds derived 
norma algorithm shares similar view classification problems 
projecting current hypothesis set constraints induced example norma update rule stochastic gradient approach kivinen 
online learning algorithms herbster probably closest online passive aggressive algorithms 
herbster describes analyzes projection algorithm mira essentially basic pa algorithm separable case 
surpass mira herbster algorithm providing bounds separable nonseparable settings unified analysis 
mentioned extend algorithmic framework analysis complex decision problems 
organized follows 
sec 
formally introduce binary classification problem section derive variants online learning algorithm setting 
variants algorithm analyzed sec 

show modify algorithms solve regression problems sec 
prediction problems sec 

shift gears discuss analyze complex decision problems 
specifically sec 
describe generalization algorithms multiclass problems extend algorithms cope sequence prediction problems sec 

describe experimental results binary multiclass problems sec 
conclude discussion directions sec 


problem setting mentioned describes analyzes online learning tasks algorithmic prism 
binary classification serves main building block remainder 
online binary classification takes place sequence rounds 
round algorithm observes instance predicts label 
prediction true label revealed algorithm suffers instantaneous loss reflects degree prediction wrong 
round algorithm uses newly obtained instance label pair improve prediction rule rounds come 
denote instance algorithm round xt concreteness assume vector rn assume xt associated unique label yt 
refer instance label pair xt yt example 
algorithms discussed predictions classification function maintain internal memory update round round 
restrict discussion classification functions vector weights rn take form sign 
magnitude interpreted degree confidence prediction 
task algorithm incrementally learn weight vector denote wt weight vector algorithm round refer term yt wt xt signed margin attained round margin positive number sign wt xt yt algorithm correct prediction 
satisfied positive margin value additionally algorithm predict high confidence 
algorithm goal achieve margin possible 
rounds algorithm attains margin suffers instantaneous loss 
loss defined hinge loss function 
margin exceeds loss equals zero 
equals difference margin value 
note passing choice margin threshold loss suffered arbitrary 
sec 
generalize hinge loss function context regression problems letting threshold user defined parameter 
abbreviate loss crammer dekel keshet shalev shwartz singer suffered round wt xt yt algorithms shown attain small cumulative squared loss sequence examples 
words prove different bounds length sequence 
notice prediction mistake bound cumulative squared loss bounds number prediction mistakes sequence examples 

binary classification algorithms previous section described general setting binary classification 
obtain concrete algorithm determine initialize weight vector define update rule modify weight vector round 
section variants online learning algorithm binary classification 
pseudo code variants fig 

vector initialized variants variant employs different update rule 
focus simplest round sets new weight vector wt solution constrained optimization problem wt argmin wt xt yt 
geometrically wt set projection wt half space vectors attain hinge loss zero current example 
resulting algorithm passive zero wt wt 
contrast rounds loss positive algorithm aggressively forces wt satisfy constraint wt xt yt regardless step size required 
name algorithm passive aggressive pa short 
motivation update stems helmbold 
helmbold formalized trade amount progress round amount information retained previous rounds 
hand update requires wt correctly classify current example sufficiently high margin progress 
hand wt stay close possible wt retaining information learned previous rounds 
solution optimization problem eq 
simple closed form solution wt wt xt 
show update derived standard tools convex analysis see instance boyd vandenberghe 
wt satisfies constraint eq 
clearly optimal solution 
concentrate case 
define lagrangian optimization problem eq 
wt yt xt lagrange multiplier 
optimization problem eq 
convex objective function single feasible affine constraint 
sufficient conditions slater condition hold finding problem optimum equivalent satisfying karush tucker online passive aggressive algorithms input aggressiveness parameter initialize receive instance xt predict yt sign wt xt receive correct label yt suffer loss max yt wt xt update 
set xt min xt 
update wt wt xt pa pa pa ii variants passive aggressive algorithm binary classification 
conditions boyd vandenberghe 
setting partial derivatives ofl respect elements zero gives wl wt wt 
plugging back eq 
get xt yt wt xt derivative ofl respect setting zero get xt yt wt xt yt wt xt xt assumed yt xt 
summary state unified update case case setting xt discussed pa algorithm employs aggressive update strategy modifying weight vector needed satisfy constraint imposed current example 
certain real life situations strategy may result undesirable consequences 
consider instance common phenomenon label noise 
mislabeled example may cause pa algorithm drastically change weight vector wrong direction 
single mislabeled example lead prediction mistakes subsequent rounds 
cope problems variations pa update employ update strategies 
adopt technique previously derive soft margin classifiers vapnik introduce non negative slack variable optimization problem defined eq 

variable introduced different ways 
consider update objective function scales linearly crammer dekel keshet shalev shwartz singer wt argmin wt xt yt 
positive parameter controls influence slack term objective function 
specifically show larger values imply aggressive update step refer aggressiveness parameter algorithm 
term algorithm results update pa alternatively objective function scale quadratically resulting constrained optimization problem wt argmin wt xt yt 
note constraint appears eq 
longer necessary non negative 
term algorithm results update pa ii pa positive parameter governs degree update pa ii aggressive 
updates pa pa ii share simple closed form wt wt min xt pa xt pa ii 
detailed derivation pa pa ii updates provided appendix worth noting pa ii update equivalent increasing dimension xt setting xn setting remaining new coordinates zero simple pa update 
technique previously derive noise tolerant online algorithms simon freund schapire :10.1.1.48.8200
observation explicitly lead tighter analysis 
restricted discussion linear predictors form sign 
easily generalize algorithms section mercer kernels 
simply note pa variants wt wt xt xi xt 
inner product right hand side replaced general mercer kernel xi xt changing derivation 
additionally formal analysis section holds kernel operator 

analysis section prove relative loss bounds variants pa algorithm previous section 
specifically theorems section relate cumulative squared loss attained algorithms sequence examples loss attained arbitrary online passive aggressive algorithms fixed classification function form sign sequence 
previously mentioned cumulative squared hinge loss upper bounds number prediction mistakes 
bounds essentially prove sequence examples algorithms worse best fixed predictor chosen hindsight 
simplify presentation abbreviations 
denote instantaneous loss suffered algorithm round addition denote loss suffered arbitrary fixed predictor comparing performance 
formally arbitrary vector define wt xt yt xt yt 
technical lemma facilitates proofs section 
lemma handy derive loss mistake bounds variants pa algorithm previous section 
lemma xt yt sequence examples xt yt defined pa variants fig 

notation eq 
bound holds xt proof define wt wt prove lemma summing bounding sum 
note telescopic sum collapses wt wt wt facts defined zero vector wt non negative upper bound right hand side conclude 
turn bounding 
minimum margin requirement violated round 
focus rounds 
definition wt wt yt txt write wt wt wt wt yt txt wt wt wt xt xt wt xt xt 
crammer dekel keshet shalev shwartz singer assumed yt wt xt alternatively yt wt xt addition definition hinge loss implies yt xt yt xt facts back eq 
gives xt xt 
summing comparing lower bound eq 
upper bound eq 
proves lemma 
prove loss bound pa algorithm separable case 
bound previously herbster analogous classic mistake bound perceptron algorithm due 
assume exists yt xt 
loss generality assume scaled yt xt attains loss zero examples sequence 
vector disposal prove bound cumulative squared loss pa theorem xt yt sequence examples xt yt xt assume exists vector cumulative squared loss pa sequence examples bounded proof lemma implies xt 
definition pa algorithm left hand side gives xt fact xt get multiplying sides inequality gives desired bound 
remaining bounds prove section depend separability assumption 
contrast assumptions thm 
vector appears theorems arbitrary vector necessarily perfect separator 
theorems bounds cumulative squared loss attained pa algorithm special case online passive aggressive algorithms instances input sequence normalized xt 
assumption somewhat restrictive case practical applications classification instances normalized 
instance certain kernel operators gaussian kernel imply input instances unit norm 
see example cristianini shawe taylor 
theorem xt yt sequence examples xt yt xt vector cumulative squared loss pa sequence examples bounded proof special case xt equal 
lemma gives cauchy schwartz inequality upper bound right hand side inequality denoting lt ut get largest value lt inequality satisfied larger values inequality holds equality 
obtain upper bound lt need find largest root second degree polynomial ut lt ut fact conclude lt ut 
square sides inequality plugging definitions lt ut eq 
gives desired bound 
turn analysis pa theorem provide loss bound mistake bound pa algorithm 
prove direct bound number times yt sign wt xt proxy 
theorem xt yt sequence examples xt rn yt xt vector rn number prediction mistakes pa sequence examples bounded max aggressiveness parameter provided pa fig 
crammer dekel keshet shalev shwartz singer proof pa prediction mistake round 
assumption xt definition min xt conclude prediction mistake occurs holds min denote number prediction mistakes entire sequence 
non negative holds min 
definition know xt plugging inequalities lemma gives combining eq 
eq 
conclude min 
theorem follows multiplying sides max 
turn analysis pa ii proof theorem lemma 
theorem xt yt sequence examples xt yt xt vector holds cumulative squared loss pa ii sequence examples bounded aggressiveness parameter provided pa ii fig 
proof recall lemma states xt defining subtract non negative term summand right hand side inequality get xt xt xt online passive aggressive algorithms plugging definition obtain lower bound xt definition xt rewrite xt replacing xt upper bound rearranging terms gives desired bound 
conclude section brief comparison bounds previously published bounds perceptron algorithm 
mentioned bound thm 
equal bound perceptron separable case 
thm 
bounds cumulative squared hinge loss pa bound number prediction mistakes 
gentile proved mistake bound perceptron nonseparable case compared mistake bound pa thm 

notation thm 
gentile bounds number mistakes perceptron price slightly loosening bound inequality get simpler bound bound thm 
bound inferior gentile factor 
loss bound pa ii thm 
compared bound freund schapire perceptron algorithm 
notation defined thm 
freund schapire bound number incorrect predictions perceptron easily verified bound pa ii algorithm thm 
exactly equals bound freund schapire set 
optimal choice bound cumulative squared hinge loss pa ii bound freund schapire number mistakes 

regression crammer dekel keshet shalev shwartz singer section show algorithms described sec 
modified deal online regression problems 
regression setting instance xt associated real target value yt online algorithm tries predict 
round algorithm receives instance xt predicts target value yt internal regression function 
focus class linear regression functions yt wt xt wt incrementally learned vector 
making prediction algorithm true target value yt suffers instantaneous loss 
insensitive hinge loss function positive parameter controls sensitivity prediction mistakes 
loss zero predicted target deviates true target grows linearly yt yt 
round algorithm uses wt example xt yt generate new weight vector wt extend prediction round 
describe various pa algorithms sec 
adapted learn regression problems 
case classification initialize 
round pa regression algorithm sets new weight vector wt argmin wt xt yt binary classification setting gave pa update geometric interpretation projecting wt linear half space defined constraint xt yt 
regression problems set zt half space hyper slab width 
geometrically pa algorithm regression projects wt hyper slab round 
shorthand wt xt yt update eq 
closed form solution similar classification pa algorithm previous section wt wt sign yt yt txt xt obtain pa pa ii variants online regression introducing slack variable optimization problem eq 
classification eq 
eq 

closed form solution updates comes wt wt sign yt yt txt defined eq 

derivations closed form updates identical classification problem sec 

turn analysis pa regression algorithms described 
show analysis sec 
classification algorithms holds regression counterparts 
suffices show lemma holds regression problems 
obtaining regression version lemma regression versions thm 
thm 
follow immediate corollaries 
lemma xt yt arbitrary sequence examples xt yt defined pa variants regression problems 
notation eq 
bound holds xt online passive aggressive algorithms proof proof lemma follows lemma subtleties discussed detail proof omitted 
definition wt wt argument lemma implies focus attention bounding rounds 
recursive definition wt rewrite wt wt sign yt yt txt sign yt yt wt xt xt add subtract term sign yt yt right hand side get bound sign yt yt wt xt yt sign yt yt xt yt xt 
wt xt yt sign yt yt wt xt yt wt xt yt 
need consider case wt xt yt rewrite bound eq 
sign yt yt xt yt xt know sign yt yt xt yt xt yt xt yt 
enables bound xt xt 
summing comparing upper bound discussed proof proves lemma 

prediction section pa algorithms prediction problem 
task involves predicting sequence vectors yt rn prediction fundamentally different classification regression algorithm predictions observing external input instance xt 
specifically algorithm maintains memory vector wt rn simply predicts element sequence wt 
extending prediction element sequence revealed instantaneous loss suffered 
measure loss insensitive loss function 
regression setting positive user defined parameter 
prediction true sequence element loss suffered 
loss proportional euclidean crammer dekel keshet shalev shwartz singer distance prediction true vector 
round wt updated order potentially accurate prediction element sequence fall 
equivalently think prediction task finding center point vectors sequence fall radius section discuss generalization problem radius determined algorithm 
initialize 
pa algorithm define update prediction algorithm wt argmin wt yt geometrically wt set projection wt ball radius yt 
show closed form solution optimization problem turns wt wt yt 
wt yt wt yt rewrite equation express wt wt wt yt wt yt wt problem kkt conditions sufficient necessary optimality 
prove eq 
minimizer eq 
verifying kkt conditions hold 
lagrangian eq 
wt yt lagrange multiplier 
differentiating respect elements setting partial derivatives zero get kkt condition stating optimum satisfy equality yt wl wt 
yt addition optimal solution satisfy conditions yt 
clearly 
show wt optimum eq 
suffices prove wt satisfies eq 
eq 

equalities trivially hold assume 
plugging values wt right hand side eq 
gives note wt wt wt yt wt wt yt wt yt yt wt yt wt wt yt 
wt yt yt wt yt wt yt wt yt yt wt wt yt wt yt wt yt wt yt wt yt 
combining eq 
eq 
get online passive aggressive algorithms wt wt eq 
holds wt 
similarly wt yt wt yt wt yt eq 
holds 
summary shown kkt optimality conditions hold wt eq 
gives desired closed form update 
obtain versions pa pa ii add slack variable optimization problem eq 
way eq 
eq 
classification algorithms 
update pa defined wt argmin update pa ii wt argmin wt yt wt yt 
closed form updates derived technique deriving pa update 
final outcome pa pa ii share form update eq 
set min pa pa ii 
extend analysis pa variants sec 
case prediction 
proving version lemma 
proving lemma discuss additional technical difficulty needs addressed thm 
thm 
carry smoothly case 
lemma yt arbitrary sequence vectors yt defined pa variants prediction 
notation eq 
bound holds proof prove lemma way lemma 
definition wt wt fact stated eq 
crammer dekel keshet shalev shwartz singer focus attention bounding rounds 
recursive definition wt rewrite wt yt wt yt wt yt wt wt yt wt wt yt wt yt wt wt yt wt add subtract yt term wt get wt yt yt yt wt wt yt wt yt yt yt wt wt yt cauchy schwartz inequality term yt yt wt bound wt yt yt add subtract right hand side get wt yt yt dealing case holds wt yt 
definition yt 
facts get summing inequality comparing result upper bound eq 
gives bound stated lemma 
mentioned remains technical obstacle stands way applying thm 
thm 
case 
difficulty stems fact xt defined term appears theorems 
issue easily resolved setting xt case arbitrary vector unit length xt 
technical modification enables write xt pa algorithm classification case 
similarly defined classification case pa pa ii thm 
thm 
applied verbatim pa algorithms 
learning radius predictor derivation simplifying assumption radius predictor fixed online algorithm move center show learning parallel harder learning 
simple reduction argument 
technical reasons require upper bound denote specified ahead time online passive aggressive algorithms arbitrarily large appear analysis 
typically think far greater conceivable value 
goal incrementally find wt wt yt possible 
additionally stay relatively small extremely large value solve problem trivial way 
reducing problem different problem radius fixed yt adding additional dimension problem learn machinery developed problems 
reduction stems observation eq 
written equivalently wt yt 
concatenate yt increasing dimension considered th coordinate wt equivalent eq 
simply wt yt problem reduced fixed radius problem radius set initialized equivalent initializing 
round extracted wt wt defined convex combination wt yt equals zero wt bounded decrease round round 
means radius defined increase radius initialized zero learned parameters algorithm natural tendency favor small radii 
denote center fixed predictor denote radius 
reduction described enables prove loss bounds similar sec 
replaced 
multiclass problems address complex decision problems 
adapt binary classification algorithms described sec 
task multiclass multilabel classification 
setting instance associated set labels yt 
concreteness assume different possible labels denote set possible labels byy 
instance xt set relevant labels yt subset ofy say label relevant instance xt yt 
setting discussed text categorization applications see instance schapire singer xt represents document yt set topics relevant document chosen predefined collection topics 
special case single relevant topic instance typically referred multiclass single label classification multiclass categorization short 
discussed adaptation pa variants multiclass multilabel settings encompasses single label setting special case 
previous sections algorithm receives instances sequential manner xt belongs instance receiving instance algorithm outputs score labels 
algorithm prediction vector element vector corresponds score assigned respective label 
form prediction referred label ranking 
predicting label ranking general crammer dekel keshet shalev shwartz singer flexible predicting set relevant labels yt 
special purpose learning algorithms adaboost schapire singer adaptations support vector machines crammer singer devised task label ranking 
describe reduction online label ranking online binary classification deems label ranking simple binary prediction 
note case multiclass single label classification prediction algorithm simply set label highest score 
pair labels score assigned algorithm label greater score assigned label say label ranked higher label goal algorithm rank relevant label irrelevant label 
assume provided set features feature mapping reals 
denote vector formed concatenating outputs features feature applied pair 
label ranking function discussed section parameterized weight vector round prediction algorithm dimensional vector wt xt wt xt motivate construction example domain text categorization 
describe variant term frequency inverse document frequency tf idf representation documents rocchio salton buckley 
feature corresponds different word denoted 
corpus documents potential topic feature defined tf log df tf number times appears df number times appears documents labeled value grows proportion frequency document frequent word topics context important point feature label dependent 
making prediction ranking labels algorithm receives correct set relevant labels yt 
define margin attained algorithm round example xt yt wt xt yt min yt wt xt max wt xt 
yt definition generalizes definition margin binary classification employed single label multilabel learning algorithms support vector machines vapnik weston watkins elisseeff weston crammer singer 
words margin difference score lowest ranked relevant label score highest ranked irrelevant label 
margin positive relevant labels ranked higher irrelevant labels 
spirit binary classification satisfied mere positive margin require margin prediction 
receiving yt suffer instantaneous loss defined hinge loss function mc 
online passive aggressive algorithms previous sections abbreviation mc wt xt yt irrelevant label ranked higher relevant label attains value greater 
upper bounds number multiclass prediction mistakes rounds way updating weight vector wt mimic derivation pa algorithm binary classification defined sec 
set wt argmin wt mc xt yt 
satisfying single constraint optimization problem equivalent satisfying set linear constraints yt yt xt xt 
attempting satisfy yt yt constraints focus single constraint violated wt 
show sequel prove cumulative loss bound simplified version update 
note satisfying constraints simultaneously leads online algorithm crammer singer 
online update involved computationally expensive analysis covers realizable case 
formally rt denote lowest ranked relevant label st denote highest ranked irrelevant label round rt argmin yt wt xt st argmax wt xt 
yt single constraint choose satisfy xt rt xt st wt set solution simplified constrained optimization problem wt argmin wt xt rt xt st 
apparent benefit simplification lies fact eq 
closed form solution 
draw connection multilabel setting binary classification think vector xt rt xt st virtual instance binary classification problem label 
reduction mind eq 
equivalent eq 

closed form solution eq 
wt wt xt rt xt st 
xt rt xt st 
essentially neglecting labels step multiclass update obtain multiclass cumulative loss bounds 
key observation analysis wt xt yt wt xt rt xt st mc remind reader right hand side equation binary classification loss defined eq 

equivalence definitions convert thm 
bound crammer dekel keshet shalev shwartz singer multiclass pa algorithm 
need cast assumption holds xt rt xt st bound immediately converted bound norm feature set xt rt xt st xt rt xt st 
norm mapping xt bounded xt rt xt st 
particular assume xt obtain corollary 
corollary xt yt sequence examples xt yt 
mapping xt assume exists vector xt yt cumulative squared loss attained multiclass multilabel pa algorithm bounded similarly obtain multiclass versions pa pa ii update rule eq 
setting min xt rt xt st xt rt xt st respectively 
analysis pa pa ii thms 
carries binary case multilabel case way 
multi prototype classification discussion assumed feature vector label dependent single weight vector form ranking function 
applications multiclass classification setup somewhat unnatural 
times single natural representation instance multiple feature representations individual class 
example optical character recognition problems ocr instance gray scale image character goal output content image 
example difficult find set label dependent features 
common construction settings assume instance vector rn associate different weight vector referred prototype labels vapnik weston watkins crammer singer 
multiclass predictor parameterized wr rn output predictor defined xt xt distinguish setting previous refer setting multi prototype multiclass setting previous single prototype multiclass setting 
describe reduction multi prototype setting single prototype enables multiclass algorithms discussed multi prototype setting 
obtain desired reduction define feature vector representation induced instance label pair 
define dimensional vector composed blocks size blocks th block set zero vector th block set applying single prototype multiclass algorithm problem produces weight vector wt kn online round 
analogous construction vector wt composed online passive aggressive algorithms input cost function initialize receive instance xt rn predict yt wt xt receive correct label yt define yt wt xt wt xt yt yt define yt pb qt yt ml suffer loss wt xt qt wt xt yt yt qt set xt yt xt qt update wt wt xt yt xt qt prediction pb max loss ml passive aggressive updates multiclass problems 
blocks size denote block construction get wt xt xt 
equipped construction verbatim single prototype algorithm proxy multi prototype variant 
round find pair indices rt st corresponds largest violation margin constraints rt argmin yt st argmax yt wt xt argmin yt xt wt xt argmax yt xt 
unraveling single prototype notion margin casting multi prototype get loss multi prototype case amounts xt yt rt wt xt st xt rt xt st xt 
furthermore applying reduction update scheme get resulting update rt wrt txt st wst txt 
pa algorithm value ratio loss eq 
squared norm xt rt xt st 
construction vector blocks elements zeros blocks equal xt xt 
non zero blocks non overlapping get xt rt xt st xt xt xt due reduction get multi prototype versions thm 
thm 

crammer dekel keshet shalev shwartz singer 
cost sensitive multiclass classification cost sensitive multiclass classification variant multiclass single label classification setting discussed previous section 
instance xt associated single correct label yt prediction extended online algorithm simply yt argmax wt xt 
prediction mistake occurs yt yt cost sensitive setting different mistakes incur different levels cost 
specifically pair labels cost associated predicting correct label cost function defined user takes non negative values 
assume goal algorithm minimize cumulative cost suffered sequence examples minimize yt yt 
multiclass pa algorithms discussed adapted task incorporating cost function online update 
recall began derivation multiclass pa update defining set margin constraints eq 
round focused attention satisfying constraints 
repeat idea incorporating cost function margin constraints 
specifically online round constraints hold yt wt xt yt wt xt yt 
reason square root function equation justified shortly 
mentioned online update focuses single constraint constraints eq 

describe analyze different ways choose single constraint lead different online updates cost sensitive classification 
update techniques called prediction update max loss update 
pseudo code updates fig 

share identical analysis may similar update possesses unique qualities 
discuss significance update section 
prediction update focuses single constraint eq 
corresponds predicted label yt 
concretely update sets wt solution optimization problem wt argmin wt wt xt yt wt xt yt yt yt yt defined eq 

update closely resembles multiclass update eq 

define cost sensitive loss prediction update pb 
note loss equals zero correct prediction yt yt 
hand prediction mistake occurred means wt ranked yt higher yt yt yt wt xt yt wt xt yt yt yt pb wt xt yt 
online passive aggressive algorithms previous sections prove upper bound cumulative squared loss attained algorithm pb wt xt yt cumulative squared loss turn bounds yt yt quantity trying minimize 
explains rationale choice margin constraints eq 

update eq 
closed form solution wt wt xt yt xt yt pb wt xt yt xt yt xt yt 
obtain cost sensitive versions pa pa ii setting pb wt xt yt min xt yt xt yt pa pb wt xt yt xt yt xt yt pa ii cases user defined parameter 
second cost sensitive update max loss update focuses satisfying single constraint eq 

yt label defined yt argmax wt xt wt xt yt yt 
yt loss maximizing label 
suffer greatest loss round predict yt 
max loss update focuses single constraint eq 
corresponds yt 
note online algorithm continues predict label yt yt influences online update 
concretely max loss update sets wt solution optimization problem wt argmin wt wt xt yt wt xt yt yt yt update eq 
closed form solution eq 
eq 
yt replaced yt 
define loss max loss update ml defined eq 

note attains maximal loss labels follows pb wt xt yt ml wt xt yt 
inequality eq 
conclude ml upper bound yt yt 
note worthy difference pb ml ml wt xt yt eq 
holds yt case pb 
prediction max loss updates previously discussed dekel 
context hierarchical classification 
predefined hierarchy label set induce cost function 
basic online algorithm update max loss update mentioned context batch learning setting 
crammer dekel keshet shalev shwartz singer dekel 
evaluated techniques empirically highly effective speech recognition text classification tasks 
turning analysis cost sensitive algorithms follow strategy analysis regression algorithms 
proving cost sensitive version lemma prediction max loss updates 
lemma xt yt arbitrary sequence examples xt yt arbitrary vector defined eq 
eq 
pb wt xt yt xt yt xt yt ml xt yt defined eq 
eq 
yt replaced yt ml wt xt yt xt yt xt yt ml xt yt proof prove statement lemma involves prediction update rule 
proof second statement identical yt replaced yt pb wt xt yt replaced ml wt xt yt 
proof lemma definition wt wt fact 
focus attention bounding 
recursive definition wt rewrite wt wt xt yt xt yt wt xt yt xt yt xt yt xt yt 
definition ml xt yt equals max xt xt yt yt ml xt yt maximum clearly greater xt yt xt yt yt yt 
written xt yt xt yt yt yt ml xt yt 
plugging back eq 
get xt yt xt yt yt yt ml xt yt xt yt xt yt 
online passive aggressive algorithms rearranging terms definition pb get wt xt yt xt yt yt yt pb wt xt yt 
enables rewrite eq 
yt yt pb wt xt yt yt yt ml xt yt xt yt xt yt pb wt xt yt xt yt xt yt ml xt yt summing comparing lower bound upper bound provided eq 
gives desired bound 
lemma obtain cost sensitive versions thms 
prediction max loss updates 
proof theorems remains essentially cosmetic change required xt replaced xt yt xt yt xt yt xt yt theorems proofs 
provides cumulative cost bounds pa pa ii cost sensitive algorithms 
analyzing cost sensitive version pa requires slightly delicate adaptation thm 

brevity prove theorem max loss variant algorithm note proof prediction variant essentially identical 
simplifying assumptions assume xt yt xt yt upper bounded 
second assume aggressiveness parameter pa algorithm upper bound square root cost function 
theorem xt yt sequence examples xt yt xt yt xt yt cost function aggressiveness parameter provided pa algorithm yt yt vector cumulative cost obtained max loss cost sensitive version pa sequence bounded yt yt ml xt yt 
proof abbreviate yt yt ml wt xt yt proof 
round discussed section 
defined min xt yt xt yt due assumption xt yt xt yt get min 
combining facts gives min assumption know summing get bound 
crammer dekel keshet shalev shwartz singer definition know ml xt yt ml xt yt xt yt xt yt plugging inequalities second statement lemma gives combining eq 
eq 
proves theorem 
ml xt yt 
concludes analysis cost sensitive pa algorithms 
wrap section discussion significant differences prediction max loss variants cost sensitive algorithms 
variants utilize prediction function output predicted label yt variant follows different update strategy evaluated respect different loss function 
loss function evaluate prediction variant function yt yt loss function evaluate max loss update essentially ignores yt 
respect prediction loss natural 
hand analysis prediction variant lacks aesthetics analysis 
analysis max loss algorithm uses ml evaluate performance algorithm performance analysis prediction algorithm uses pb evaluate algorithm ml evaluate prediction relative bound extent comparing apples oranges algorithm evaluated loss function 
summary algorithms suffer theoretical disadvantage theoretically superior 
turn attention important algorithmic difference update strategies 
prediction update great advantage max loss update cost function play role determining single constraint update focuses 
cases significantly speed running time required online update 
example section exploit property devising algorithms complex problem sequence prediction 
reading section note max loss update sequence prediction place prediction update 
significant difference cost sensitive updates 

learning structured output useful application large margin methods learning structured output 
setting set possible labels endowed predefined structure 
typically set labels large structure plays key role constructing efficient learning inference procedures 
notable examples structured label sets graphs particular trees sequences collins altun taskar tsochantaridis 
overview cost sensitive learning algorithms described previous section adapted structured output settings 
concreteness focus adaptation sequence prediction 
derivation easily mapped settings learning structured output 
sequence prediction problems provided predefined 
input instance associated label sequence simplicity assume output sequence fixed length round learning algorithm receives instance xt predicts output sequence yt predicting algorithm receives online passive aggressive algorithms correct sequence yt associated xt 
cost sensitive case learning algorithm provided cost function 
value represents cost associated predicting assume equals zero apart requirement may computable function 
sequence prediction algorithms assume decomposable 
specifically common construction taskar tsochantaridis achieved defining yi non negative local cost contrast revert general cost function pairs sequences 
multiclass settings discussed assume exists set features takes input instance sequence outputs real number 
denote vector features evaluated equipped left task finding yt argmax wt xt online round 
yt hand pa update string prediction identical prediction update described previous section 
obtaining yt general case may require evaluations wt xt 
problem intractable large 
impose restrictions feature representation enable find yt efficiently 
possible restriction feature representation assume feature takes form xt xt yi yi xt computable functions 
construction analogous imposing order markovian structure output sequence 
form paves way efficient inference solving eq 
dynamic programming procedure 
similar richer structures dynamic bayes nets imposed long solution eq 
computed efficiently 
note passing similar representation efficiently computable feature sets proposed altun taskar tsochantaridis 
analysis cost sensitive pa updates carries verbatim sequence prediction setting 
algorithm learning structured outputs successfully applied task music score alignment shalev shwartz 

experiments section experimental results demonstrate different aspects pa algorithms accompanying analysis 
sec 
start experiments synthetic data examine robustness algorithms noise 
sec 
investigate effect aggressiveness parameter performance pa pa ii algorithms 
sec 
compare multiclass versions pa algorithms online algorithms multiclass problems crammer singer natural data sets 
synthetic data set experiments generated follows 
label chosen uniformly random 
positive labeled examples instances chosen randomly sampling dimensional gaussian mean diagonal covariance matrix error error crammer dekel keshet shalev shwartz singer pa pa pa ii error optimal linear classifier pa pa pa ii error optimal linear classifier loss loss pa pa pa ii error optimal linear classifier pa pa pa ii error optimal linear classifier average error left average loss right pa pa pa ii function error optimal fixed linear classifier presence instance noise top label noise bottom 
diagonal 
similarly negative labeled examples instances sampled gaussian mean covariance matrix positive labeled examples 
validate results repeated experiment times repetition generated random examples 
results reported averaged repetitions 
robustness noise experiments examine robustness algorithms instance noise label noise 
examine instance noise contaminated instance random vector sampled zero mean gaussian covariance matrix varied 
set parameter pa pa ii 
ran pa pa pa ii resulting sequence examples 
evaluate results brute force numerical method find optimal fixed linear classifier linear classifier fewest classification mistakes entire sequence examples 
define average error online learning algorithm input sequence number prediction mistakes algorithm sequence normalized length sequence 
similarly define average loss online learning algorithm sequence 
plots top fig 
depict average error average loss pa variants function average error optimal linear classifier 
plots underscore error error online passive aggressive algorithms log log loss loss log log average error left average loss right pa top pa ii bottom function log different levels label noise probability interesting phenomena 
note low levels noise pa variants similar number errors 
bounds sec 
suggest noise level increases pa pa ii outperform basic pa algorithm 
clear graphs expectations met pa pa ii outperform basic pa algorithm noise level high 
experiment pa pa ii performed equally levels noise 
second experiment left instances intact flipped label probability set different values 
previous experiment set pa pa ii results depicted bottom fig 

apparent graphs behavior observed previous experiment repeated 
effect second set experiments examine effect aggressiveness parameter performance pa pa ii flipped label instance synthetic data set probability time set 
ran pa pa ii resulting sequence examples different values parameter average error average loss algorithms function parameter depicted fig 
error crammer dekel keshet shalev shwartz singer error average error pa left pa ii right function number online rounds different values seen graphs value parameter significantly effects results algorithms 
graphs explained loss bounds thm 
thm 

concreteness focus loss bound pa ii algorithm thm 

bound cumulative loss algorithm comprised terms depends squared norm competitor second depends cumulative squared loss competitor 
parameter divides term multiplies second term 
small bound dominated term large bound dominated second term 
label noise applied data effects second term expect small values loss pa pa ii high regardless noise level 
hand increase value difference different noise levels apparent 
general rule thumb small data noisy 
far length sequence examples online algorithms fixed 
discuss effect performance algorithms function sequence length 
generated synthetic data set consisting examples label noise probability 
ran pa pa ii algorithms data set 
online round calculated average error attained far 
results fig 

pa pa ii setting small number leads slow progress rate online update changes online hypothesis small amount 
hand large error rate decreases faster price inferior performance 
multiclass experiments experiment demonstrates efficiency pa algorithms multiclass problems 
experiment performed standard multiclass data sets usps mnist data sets handwritten digits 
compared multiclass versions pa pa pa ii online error multiclass perceptron mira pa online passive aggressive algorithms error multiclass mira pa number prediction mistakes different multiclass online algorithms function online round index usps left mnist right data sets 
multiclass algorithms described crammer singer 
specifically crammer singer multiclass versions perceptron algorithm new margin online multiclass algorithm named mira 
preprocessing step shifted scaled instances data set mean equals zero average squared euclidean norm 
mercer kernels algorithms replaced standard dot product polynomial kernel xi xj xi usps data set mnist data set 
kernel parameters set arbitrarily previous experience data sets different algorithms 
set parameter pa pa ii note similar results hold 
parameter mira set crammer singer 
plots fig 
depict number online prediction mistakes data sets different algorithms pa uniform update version multiclass perceptron mira 
performance pa pa ii virtually indistinguishable pa reason uniform update version multiclass perceptron 
apparent pa mira outperform perceptron 
addition performance pa comparable mira slight advantage 
online update mira requires solving complex optimization problem update pa simple closed form expression faster easier implement 

discussion described online algorithmic framework solving numerous prediction problems ranging classification sequence prediction 
derived loss bounds algorithms thms 

proofs bounds single lemma lemma 
possible extensions 
conducted research applications pa algorithmic framework learning margin suffix crammer dekel keshet shalev shwartz singer trees dekel pseudo metrics shalev shwartz hierarchical classification dekel segmentation sequences shalev shwartz 
focus online settings online algorithms serve building blocks construction performing batch algorithms 
online batch conversions proposed algorithms important research direction 
update taken algorithms aggressive sense small loss forces update hypothesis 
kernels property results examples representing learned predictor 
memory requirements imposed kernels quite demanding 
currently pursuing extensions pa framework operate realm bounded memory constraints 
acknowledgments research funded israeli science foundation number 
carried hebrew university jerusalem 
appendix derivation pa pa ii updates sec 
update occurs equals zero 
derive updates defining lagrangian respective optimization problem satisfying kkt conditions 
lagrangian pa optimization problem wt yt xt wt yt xt lagrange multipliers 
find minimum lagrangian respect unconstrained primal variables 
previously discussed pa update differentiating lagrangian respect elements setting partial derivatives zero gives eq 
write wt 
note minimum term respect zero 
approach 
need maximize dual rule case pose constraint dual variables 
kkt conditions confine non negative conclude discuss possible cases xt plugging eq 
back eq 
return lagrangian original pa algorithm see eq 

point repeat derivation original pa update get xt case xt condition rewritten xt yt wt xt 
know constraint eq 
hold optimum yt xt 
explicit form eq 
rewrite constraint yt wt xt xt 
online passive aggressive algorithms combining inequality inequality eq 
gives xt xt 
earlier obtain 
turning kkt complementarity condition know optimum 
having concluded strictly positive get equal zero 
plugging eq 
gives summing kkt conditions show case xt optimal select folding possible cases single equation define min xt 
update pa update pa clipped turning update pa ii recall leads deal rounds 
lagrangian optimization problem eq 
equals wt yt xt lagrange multiplier 
differentiating lagrangian respect elements setting partial derivatives zero gives eq 
write wt 
differentiating lagrangian respect setting partial derivative zero results expressing replacing eq 
wt rewrite lagrangian xt setting derivative zero gives xt yt wt xt yt wt xt pa pa give definition holds cases yt wt xt xt 

relaxation method linear inequalities 
canadian journal mathematics 
altun tsochantaridis hofmann 
hidden markov support vector machines 
proceedings twentieth international conference machine learning 
boyd vandenberghe 
convex optimization 
cambridge university press 
crammer dekel keshet shalev shwartz singer collins 
discriminative reranking natural language parsing 
machine learning proceedings seventeenth international conference 
crammer singer 
algorithmic implementation multiclass kernel vector machines 
machine learning research 
crammer singer 
new family online algorithms category ranking 
machine learning research 
crammer singer 
ultraconservative online algorithms multiclass problems 
machine learning research 
cristianini shawe taylor 
support vector machines 
cambridge university press 
dekel keshet singer 
large margin hierarchical classification 
proceedings international conference machine learning 
dekel shalev shwartz singer 
power selective memory self bounded learning prediction suffix trees 
advances neural information processing systems 
elisseeff weston 
kernel method multi labeled classification 
advances neural information processing systems 
freund schapire 
large margin classification perceptron algorithm 
machine learning 
gentile 
new approximate maximal margin classification algorithm 
journal machine learning research 
gentile 
robustness norm algorithms 
machine learning 
helmbold kivinen warmuth 
relative loss bounds single neurons 
ieee transactions neural networks 
herbster 
learning additive models online fast evaluating kernels 
proceedings fourteenth annual conference computational learning theory pages 
kivinen smola williamson 
online learning kernels 
ieee transactions signal processing 
kivinen warmuth 
exponentiated gradient versus gradient descent linear predictors 
information computation january 
simon 
noise free noise tolerant line batch learning 
proceedings eighth annual conference computational learning theory pages 
li long 
relaxed online maximum margin algorithm 
machine learning 
online passive aggressive algorithms littlestone 
mistake bounds logarithmic linear threshold learning algorithms 
phd thesis santa cruz march 

convergence proofs perceptrons 
proceedings symposium mathematical theory automata volume xii pages 
rocchio 
relevance feedback information retrieval 
gerard salton editor smart retrieval system experiments automatic document processing pages 
prentice hall englewood cliffs nj 
rosenblatt 
perceptron probabilistic model information storage organization brain 
psychological review 
reprinted neurocomputing mit press 
salton buckley 
term weighting approaches automatic text retrieval 
information processing management 
schapire singer 
improved boosting algorithms confidence rated predictions 
proceedings eleventh annual conference computational learning theory pages 
appear machine learning 
schapire singer 
boostexter boosting system text categorization 
machine learning 
sch lkopf smola 
learning kernels support vector machines regularization optimization 
mit press 
shalev shwartz keshet singer 
learning align polyphonic music 
proceedings th international conference music information retrieval 
www cs huji ac il 
shalev shwartz singer ng 
online batch learning pseudo metrics 
proceedings international conference machine learning 
taskar guestrin koller 
max margin markov networks 
advances neural information processing systems 
tsochantaridis hofmann joachims altun 
support vector machine learning interdependent structured output spaces 
proceedings international conference machine learning 
vapnik 
statistical learning theory 
wiley 
weston watkins 
support vector machines multi class pattern recognition 
proceedings seventh european symposium artificial neural networks april 

