dataflow complement superscalar mihai budiu pedro seth copen goldstein microsoft com cs cmu edu seth cs cmu edu microsoft research silicon valley carnegie mellon university resurgence interest dataflow architectures potential exploiting parallelism low overhead 
analyze performance class static dataflow machines integer media control intensive programs explain dataflow machine unlimited resources outperform superscalar processor general purpose codes assumption machines take time execute basic operations 
compare program specific dataflow machine unlimited parallelism superscalar processor running program 
dataflow machines provide performance data parallel programs show dataflow machine take advantage available parallelism 
dynamic critical path investigate mechanisms superscalar processors provide performance advantage impact dataflow model 

renewed interest dataflow architectures part sparked underlying elegance model motivated changes continued technology scaling 
increased silicon resources desire avoid global wires complexity global clock distribution diminishing returns longer pipelines complexity monolithic superscalar structures suggest designs simpler rely local signals contain instances type functional unit 
effectively distributed architectures exploit instruction level parallelism ilp open question addressing 
addresses question analyzing architecture extreme space possible solutions 
call architecture ash acronym application specific hardware 
ash functional units shared instructions 
ash optimized communication computation units shared need share communication channels data value uses dedicated directional high speed link 
cash compiler translate automatically program hardware static dataflow machine synthesized collection asynchronous circuits 
circuits feature completely distributed datapath register file control logic global signals notion clock 
programs run ash little power en ergy efficiencies orders magnitude better superscalar processors performance worse run superscalar 
understand performance problems dataflow machines developed new tool allows visualize analyze dynamic critical path programs executed ash 
analysis shows ash circuits certain bottlenecks inherent dataflow model bottlenecks superscalar overcomes specific mechanisms traditionally considered part dataflow model 
completeness start section describing compilation methodology 
section employ timing accurate simulation measure performance programs superscalar dataflow machines observing dataflow exploitation unlimited parallelism offer performance advantage 
comparison employ simple normalizing assumption arithmetic operations take time machines 
section describe fully automatic tool visualize dynamic critical path dataflow execution 
tool section identify performance bottlenecks analyzing representative hot functions 
show various architectural features impact performance dataflow machines compare superscalar microarchitectural mechanisms 
ability modern speculate branches speculate loads perform dynamic register renaming dynamic loop unrolling issue instruction multiple times execute procedure calls arguments computed instrumental high performance 
implementing mechanisms dataflow machine requires extensions basic model 
fact mechanisms require monolithic structures global wires attributes contrary distributed nature dataflow exploited proposals 

compiling hardware section describes cash translates programs hardware dataflow machines 
cash represents input program pegasus dataflow intermediate representation ir :10.1.1.1.5122
output cash hardware dataflow machine directly executes input program 
program represented directed graph nodes operations edges indicate value flow 
cash generate drawings program representation dot language generate figures 
briefly describe programs represented control data data data mu switch din eta dout control flow operations mu switch eta 
dotted lines represent boolean values 
flow graphs translated pegasus 
translation accomplished steps control flow graph divided acyclic regions 
current implementation uses hyperblocks acyclic regions trying grow hyperblocks large possible 
region separately converted predicated data flow representation 
second phase translation hyperblocks creating representation procedure 
hyperblocks dataflow 
hyperblock transformed straight line code predication techniques similar 
hyperblocks pegasus uses multiplexor mux nodes ssa nodes 
speculation introduced predicate promotion instructions side effects executed unconditionally hyperblock entered 
predication speculation core constructs pegasus 
translating control flow constructs dataflow reducing criticality control dependences :10.1.1.24.4184
effectively increase ilp 
operations lenient implementations produce output inputs known 
boolean generate false result soon input known false 
steering dataflow mu switch eta 
pegasus uses special computational primitives steer data hyperblocks 
shows graphic representation 
mu operations data inputs data output 
moment data inputs available 
input copied output immediately 
pegasus uses mus entry points hyperblock 
mu nodes second output index input received data purpose drive switch nodes 
switch nodes serve purpose mu nodes having data inputs data output 
control input indicates data inputs expected fire 
switches similar muxes sense control input selects data inputs forward 
muxes expect data input time 
cash generates special value called current execution point abbreviated crt corresponds somewhat traditional program counter indicates currently executing hyperblock 
invariant maintained exists exactly single instance crt value program point 
crt value steered mu nodes crt mu nodes provide control switch node inputs 
mu switch nodes steer data entry hyperblock eta nodes steer data hyperblock 
eta inputs data predicate output 
eta forwards data put predicate true predicate false data consumed forwarded 
memory accesses represented explicit load store nodes 
operations side effects predicate input predicate false operation executed 
compiler adds dependence edges called token edges explicitly synchronize operations side effects may commute 
operations memory side effects load store call return token input 
token edges explicitly encode data flow memory 
operation memory sideeffects collect tokens potentially conflicting predecessors store set loads 
combine operator purpose 
combine multiple token inputs single token output generates output receives inputs 
assume memory operations connected single load store queue lsq interfaces dataflow machine conventional memory hierarchy 
implement aggressive protocol memory access memory operation received token reserves slot lsq 
additional inputs reach operation address predicate data stores updates reserved slot 
lsq performs dynamic disambiguation incomplete information 
soon lsq slot reservation succeeds operation issues token output enabling successors reserve slots 
bandwidth lsq storage space lsq token part computation propagate ahead scalar computation 
note scheme similar operation lsq superscalar processor 
synthesis 
hardware back synthesizes node distinct stage having single output register edge channel connecting source destination pipe stages 
channel composed data bus data ready signal acknowledgment signal consumer indicate ready receive value 
run time edge graph holds value empty 
required inputs operation available computing 
computation consumes input values output empty latch newly produced output 
behavior corresponds static dataflow machine edge hold value single register 
example 
shows function uses induction variable accumulate sum squares ofi 
right program pegasus representation consists hyperblocks 
hyperblock receives crt arguments 
comparison node labeled eta nodes steer values hyperblock 
hyperblock represents loop contains switch nodes loop carried values sum mu node crt labeled drives switch nodes 
back edges hyperblock denote loop carried values example edges hyperblock 
hyperblock function epilog containing merge nodes return value crt return 
int squares int int sum sum return sum crt sum sum crt sum program pegasus representation hyperblock shown numbered rectangle 
dotted lines represent predicates 
dashed lines represent flow crt 

performance comparison compare ash superscalar examining programs executions timing accurate simulators 
parameters interpret comparison limit study 
experimental setup 
assumptions arithmetic operations latencies computational fabrics muxes switches mus boolean operations latencies proportional log number inputs eta takes time addition memory operations ash opposed superscalar incur additional cost network arbitration propagation load store queue lsq ash latency memory access network cycle independent program size memory hierarchy models identical load store queue lsq level cache hierarchy 
latency cpu cycle latency cycles models 
study similar lsq fabrics 
lsq input output ports 
model queueing lsq ports 
latency cycles memory latency cycles 
assumptions clearly optimistic ash dealing programs 
particular complexity memory access network interconnection medium procedure calls returns assumed constant 
best case latency networks grow size compiled program assuming dimensional layout 
superscalar way order simplescalar simulation shelf simulator wide issue stage pipeline pisa instruction set gcc optimization flags generating mips binaries 
dataflow machine simulated high level simulator automatically generated cash 
cash configured compile maximum optimizations performs extensive loop unrolling 
crt switch crt sum ret sum speed percents mediabench specint benchmarks executed various dataflow machine configurations 
superscalar way order processor 
negative values indicate slower execution dataflow 
bar shows non lenient execution performance section 
second bar correspond baseline data flow machine performance 
bar shows effect perfect eta predicate prediction section section 
performance measurements 
data benchmarks successfully compiled simulated infrastructure 
naively expect ash faster benefits unlimited parallelism lack computational resource constraints dynamic scheduling overhead instruction fetch decode dispatch 
programs show performance improvements 
rest devoted explaining results 
crt eta mu sum crt crt 
dynamic critical path understand results characterize program execution dynamic critical path 
path simultaneously function program dependences runtime execution path function input data hardware resources dynamic scheduling 
despite apparent simplicity concept practical methodology proposed extract analyze dynamic critical paths superscalar processors 
key insight critical events arrival events 
event enables data latched 
events correspond signal transitions edges dataflow graphs 
arrival event input reach operation 
lenient operations see section arrival event input enables computation output 
circuit experiences backlog arrival event may acknowledgment signal enables computation proceed 
acknowledgment signal stall signal superscalar pipeline 
typical execution multiple critical events may correspond hardware structure input operation may critical times 
way summarize critical path histogram edges edge labeled number occurrences 
datapath hardware shared ash critical path maps naturally program operations 
enabled completely automate computation critical path 
implemented utility captures post processes complete execution trace produced simulator 
tools filters arrival events operation 
arrival events trace reversed continuous chain arrival events computed starting operation executed 
chain dynamic critical path 
tool generates drawing specified function critical path edges highlighted proportionally frequency occurrence 
rest rely tool understand performance differences superscalar processor ash 

insights critical path section critical path investigate impact performance various microarchitectural compiler mechanisms 
discuss hot functions exhibit poor performance ash behavior representative program fragments 

outer loop pipelining branch prediction addition reducing pipeline bubbles instrumental creating loop parallelism 
shows code fragment mediabench program performance ash superscalar due lack branch prediction 
branches involved shallow nested loops backwards branch taken loop iterates twice branch taken backwards branch taken 
machine instructions outer loop body processor window large internal int transpose int mat int rows int cols register int modulus rows cols register int swap pos current pos swap val current pos current pos modulus current pos swap pos current pos swap pos swap pos cols modulus swap pos current pos current pos swap pos swap val mat swap pos mat swap pos mat current pos mat current pos swap val internal int transpose function epic benchmark mediabench exhibits poor performance ash 
hold iterations flight 
innermost loop contains tight long latency true loop carried data dependence swap pos new value apparently accelerated form value prediction 
branch prediction processor effectively issue second iteration outermost loop completing previous 
iterations outer effectively pipelined 
critical resource loop single division unit pipelined takes cycles complete modulus 
throughput function averages cycles outer loop iteration cpu 
cost operations loop multiply branches memory accesses hidden division 
ash important part critical path shown bold lines 
omitted nodes critical path 
edges higher frequency 
performance code hampered explicit sequencing control hyperblocks 
particular new iteration outer loop initiated long inner loop terminated 
predicates controlling loops operations critical path 
puts cycle multiplier critical path 
extra operations path sum cycles corresponds observed slowdown 
inner loop value computed innermost loop immediately decide loop 
outer loop comparison computed earlier inputs known 
value pos unchanged loop propagated inner loop iterations completed 

bypassing control equivalent hyperblocks hyperblocks surrounding inner loop control equivalent computation condition outer loop lifted performed parallel inner loop 
optimization called bypass invariant value crt crt current pos current pos cols current pos original critical path alternates innermost cycle outer loop 
mu node crt bottom hyperblock removed optimizer single input 
cols crt crt crt crt crt crt cols cols crt crt current pos current pos current pos critical path bypassing 
computation outer loop predicate longer critical path performed parallel inner loop 
dynamic critical path int transpose 
sample program fragment corresponding critical path strict lenient execution 
strict execution employed multiplier critical path lenient implementation mux multiplier critical result computations 
current pos forwarded directly crossing inner loop 
bypassing allows static data flow machine exploit control independence graph property expressed regular isas aid improving program performance :10.1.1.24.4184
observation original motivations multiscalar processors employ multiple threads purpose 
performing optimization changes critical path shown 
performance code fragment improves removed inexpensive operations 
critical path outer loop solely composed machinery required control global flow computation 

lenient execution form speculative execution employed pegasus executes forward branches simultaneously alleviates impact branches may plagued problem paths known literature predicated execution illustrated mux waits inputs generate result critical path entire construct includes longest critical paths inputs 
solve problem 
muxes implemented soon selector true corresponding data available mux generates output 
result dynamic critical path non speculative implementation 
example multiplication affect critical path 
bar illustrates ash performance absence lenient execution 

control dependences predicates branch prediction decreases pipeline bubbles increases parallelism eliminate control dependences shows 
example ash times slower processor 
shows essential part optimized code generated cash critical path highlighted 
case previous function predicate deciding loop iterate thing computed depends loaded value 
critical path contains load load predicate computation indicates load needs executed 
shows critical path value eta predicate perfectly predicted 
assumption load executed speculatively critical path goes load predicate prevents load issued early memory 
crt void init processor void int sfu bits sfu bits break sfu regs sfu bits reset loop beq exit lw bne loop exit source code 
gcc mips assembly inner loop 
crt processor specint benchmark ksim 
crt sfu bits sfu bits offset sfu bits offset crt crt sfu bits sfu bits offset sfu bits offset original critical path path perfect eta predicate prediction 
crt crt sfu bits sfu bits offset sfu bits offset crt crt sfu bits sfu bits offset sfu bits offset critical path speculative loads 
path predicate prediction speculative loads 
frequent edges form tight loop load address load ack edge load address 
dynamic critical path processor 
load fetches bytes word shown 

issuing loads speculatively superscalar code shown 
code con modified lsq connected dataflow machine allowing initiate load reaches head queue speculatively waiting confirmation actual predicate value true 
change critical path contains address computation shown 
problem address computation iteration speculatively needs know iteration initiated starting computation 
tains loop carried dependence involving instructions control true true control latency loop ought cycles 
branch prediction correctly guesses outcome branches control dependences vanish critical path remaining circular dependence 
wide processor branch prediction execute loop cycle iteration 
going back ash combining load speculation eta predicate prediction critical path shown 
critical path contains acknowledgment edge load address computation 
technically limiting dependence anti dependence output register load consecutive iterations 
dependence stems fact load single output register acknowledge address input successfully latched result 
despite hitting cache latency load cycles takes cycle just get lsq cycles hit cycle return result 
loop iteration latency bounded latency longest operation loop case load 
superscalar processor removes load anti dependence register renaming issued load writes result different physical register 
multiple instances load operation flight 
memory system supports pipelined load operations sustain high throughput loop 
proves dynamic dataflow model allows multiple values graph edge approximated superscalar computational resources strictly powerful static model employed dataflow machines 
address static dataflow model ways pipelining load operation allow multiple simultaneous accesses initiated operation 
solution requires hardware load able buffer handle order replies memory system multiple outstanding load requests reorder buffer simpler fifo buffer memory system returns replies order 
loop unrolling transforms load multiple loads parallel 
latency computation bounded latency load limit applies code containing multiple iterations original loop throughput increased 
option discussed 

dynamic loop unrolling statically unroll processor loop perfect branch prediction performance ash approaches equal superscalar processor 
reason inner loop executes different numbers iterations different times iterates times iterates 
static unrolling helps number iterations large 
exacerbated fact cash unrolling inner loops creates large loop body single hyperblock instructions executed due predicate promotion speculative execution 
loop single iteration extra code unrolling pure overhead 
superscalar loop compiler unrolling performs dynamic unrolling loop run time inside instruction window guided branch predictor 
overhead pays prediction fails 
int unused arg int unused int return int main return unused arg expensive sample synthetic code exhibiting reduced performance due strictness call return instructions 

call strictness program shows superscalar mechanisms provide advantage implementation procedure calls decoupling control transfer argument forwarding order execution 
example function unused arg ignores argument computed expensive expression 
current version ash call instruction single circuit element strict implementation waits arguments available transferring control callee 
critical path contains expensive computation 
superscalar passes call arguments registers call instruction just branch 
net effect call instruction lenient control may passed procedure arguments computed 
example procedure may return expensive computation terminated result unused 
similar problem plagues current ash implementation return instruction strict arguments value returned memory token crt 
contrast processor just writes return value register branches back caller 
return branch may complete earlier actual register write back 
caller ignores returned value frequent occurrence functions returned value computation may vanish critical path assuming stall processor resources order commit stage 
simplest solution problem dataflow machine applicable inline callee 
considering call return implementations allow lenient invocations 

synchronization overhead feature ash detrimental impact performance run time cost steering data flow 
machinery required superscalar adds significant overhead execution cost 
roughly correspond conditional branches superscalar incur propagation delay 
contrasts superscalar relies branch prediction significantly reduce cost conditional branching 
superscalar microarchitectures may rely branch folding execute branches truly zero cost 
bar illustrates performance gains assuming incur delay performance improvements noticeable 
speed percents mediabench specint benchmarks executed various dataflow machine configurations 
baseline data flow machine 
second bar assume respectively eta combine cost 
bar shows impact eliminating cost controlflow operations 
ash control flow graph join points require extra machinery switch mu mux nodes 
nodes correspond roughly labels assembly language cost logarithmic number inputs superscalar labels essentially free 
bar corresponds ash performance assumption operations incur zero delay 
performance improvements demonstrate significant amount extra cost associated control flow join points ash 
note proposed processor architectures supporting predication may inject multiplexor micro operations incur run time cost 
true conditional move instructions 
combine operator enforce runtime memory dependences contributes critical path 
cost combine logarithmic number inputs 
combine operators large fan arise ambiguous store large set loads incur substantial run time overhead 
second bar correspond achievable performance assuming combines zero cost 
believe fundamental issue tension parallelism synchronization overhead manifested level individual instructions 
dataflow replaces global information program counter branch predictor explicit communication synchronization 
steering data eta mux mu switch nodes requires coordination multiple nodes broadcast branch condition 
cost operations low important overhead tight loops ones exemplified section 
distributed architectures mit raw pay broadcasting branch conditions remote computations 
reasons executing control intensive code single raw tile faster parallelizing code multiple tiles 

related interpret results limit study amount ilp available programs 
studies exploring interplay architectural features compilers ilp carried course years :10.1.1.24.4184:10.1.1.115.6325
study behavior large programs timing accurate simulation superscalar processor static dataflow machine memory bandwidth modeled 
discuss distinguishing aspects 
memory disambiguation dependence analysis memory dependences known stumbling block exposing ilp 
trace studies significant increase available parallelism relying perfect memory disambiguation :10.1.1.115.6325
study relies realistic compiler memory dependence analysis coupled run time disambiguation queue 
control dependences exploiting control dependences allows larger amount ilp exposed relying speculation 
trace studies quantified impact exploiting control dependences increase available parallelism significant gains 
ambitious approach enable early execution instructions control equivalent basic blocks 
rotenberg studied effectiveness approach context superscalar processor exploit technique context data flow machine chen yong proposed superscalar microarchitecture exploits control independence 
speculating hard predict branches executes control equivalent instructions executed regardless branch outcome 
execution instructions control dependent branch proceed hard predict branch resolved 
branch prediction branch coupled speculative execution technique reduce impact control flow evaluate impact branch prediction perfect branch prediction dataflow 
instruction window size ilp static data flow machine exploits parallelism identified compiler studies parallelism extracted dynamically limited size window 
dataflow computation publications compare model computation similar dataflow architecture wide superscalar reached quite different implying dataflow model execution potential high parallelism integer type benchmarks 
assumes model execution closer dynamic dataflow uses significantly different memory access protocol ensure coherence 
shows performance differences easily ascribed single aspect architecture detailed evaluation need carried compare models fair way 
trips project studies hybrid form execution dynamic dataflow superscalar processors wide instruction word drives dataflow limited size fabric 
attempts accurately models cost communication lattice functional units 
exploiting ilp control intensive programs quite difficult type architecture 
numbers show sustained ipc specint perfect memory dependence prediction monolithic memory disambiguation engine obtain benefits 
implementing distributed memory disambiguation mechanism may incur additional synchronization costs corresponding combine operators 
large amount dataflow computation vast majority parallel functional languages fair comparison implementation 

performance results analysis critical code segments conclude model dataflow execution fundamental limitations exploitation ilp control dependences limit ilp code :10.1.1.24.4184
known predication employ aggressively completely eliminate unpredictable branches propagates unpredictability branches 
implementing generic prediction scheme branch prediction value prediction dataflow model hindered difficulty building mechanism squashing computation wrong paths 
register renaming mechanism superscalar provides mechanism free 
interesting proposal distributed dataflow implementation speculation 
assuming control flow speculation squashed quality prediction requires global information level branch prediction memory dependence prediction 
implementation may naturally map distributed nature ash dataflow machines 
ability register renaming remove operation different iterations crucial efficiently executing tight loops 
distributed nature computation dataflow machine requires remote accesses lsq accesses non local expensive monolithic system 
cost explicit join operations representation mu switch mux combine essentially free processor substantial 
cost synchronization replaces global signals monolithic architectures 
study increased admiration capability superscalar processors exploiting ilp pipeline parallelism virtualization limited set resources computational units registers bandwidth internal external interconnection networks 
free lunch price paid effectiveness exploiting ilp power consumption die pentium excluding caches devoted support structures trying keep functional units busy 
dataflow model lies extreme lacks helper structures provide excellent perfor mance code ample data parallelism may times slower superscalar control intensive code 
show ash uses times power perform tasks 
believe place models computation 
conclude control intensive code relatively low ilp superscalar performance excellent compared dataflow machine 
performance wise dataflow winning solution programs exhibiting large amount parallelism execute high performance low power 
research needed designing balanced hybrid inherit strengths models computation 
international technology roadmap semiconductors 
public net files sia roadmap design pdf 
august hwu framework balancing control flow predication 
international symposium computer architecture isca december 
bailey 
clocking design analysis mhz alpha microprocessor 
ieee journal solid state circuits november 
budiu 
spatial computation 
phd thesis carnegie mellon university computer science department december 
technical report cmu cs 
budiu goldstein 
compiling application specific hardware 
international conference field programmable logic applications fpl pages september 
budiu spatial computation 
international conference architectural support programming languages operating systems asplos october 
burger austin 
simplescalar tool set version 
computer architecture news volume pages 
acm sigarch june 
carter simon path analysis renaming predicated instruction scheduling 
international journal parallel programming special issue 

chen 
su impact synchronization granularity parallel systems 
international symposium computer architecture isca pages 

cher 
microarchitecture exploiting control flow independence 
micro pages 
ieee computer society press december 
scalable selective edge architectures 
international conference architectural support programming languages operating systems asplos pages 

branch folding microprocessor reducing branch delay zero 
press editor international symposium computer architecture isca pages 
acm press 
fields bod slack maximizing performance technological constraints 
international symposium computer architecture isca pages 
fields rubin focusing processor policies critical path prediction 
international symposium computer architecture isca 
gansner north 
open graph visualization system applications software engineering 
software practice experience 
www research att com sw tools graphviz 
ho mai wires 
ieee april 
johnson pingali 
dependence program analysis 
pldi pages june 
selective eager execution architecture 
international symposium computer architecture isca pages june 
kuck muraoka number operations simultaneously executable fortran programs resulting speedup 
ieee transactions computers toc 
lam wilson :10.1.1.24.4184
limits control flow parallelism 
international symposium computer architecture isca 
lee barua space time scheduling parallelism raw machine 
international conference architectural support programming languages operating systems asplos pages 
mahlke comparison full partial predicated execution support ilp processors 
international symposium computer architecture isca pages 
acm may 
mahlke lin effective compiler support predicated execution hyperblock 
international symposium computer architecture isca pages dec 
nicolau fisher 
measuring parallelism available long instruction word architectures 
ieee transactions computers toc november 
palacharla jouppi complexity effective superscalar processors 
international symposium computer architecture isca pages june 
green limits instruction level parallelism spec applications 
workshop interaction compilers computer architectures october 
rauchwerger measuring limits parallelism characterizing vulnerability resource constraints 
ieee acm international symposium microarchitecture micro pages 
ieee computer society press 
lightweight distributed selective re execution implications value speculation 
value prediction workshop june 
riseman foster 
inhibition potential parallelism conditional jumps 
ieee transactions computers toc december 
rotenberg jacobson study control independence superscalar processors 
international symposium high performance computer architecture hpca 
nagarajan technology scalable architecture fast clocks high ilp 
workshop interaction compilers computer architecture january 
nagarajan exploiting ilp tlp dlp trips architecture 
international symposium computer architecture isca pages 
acm press 
simon calder incorporating predicate information branch predictors 
workshop epic architectures compiler technology december 
smith sohi 
microarchitecture superscalar processors 
ieee 
smith johnson limits multiple instruction issue 
international conference architectural support programming languages operating systems asp los pages 
acm press 
sohi breach multiscalar processors 
international symposium computer architecture isca volume 
acm may 

increasing processor performance implementing deeper pipelines 
international symposium computer architecture isca 
sutherland 
turing award lecture 
communications acm june 
swanson 
ieee acm international symposium microarchitecture micro december 
taylor lee evaluation raw microprocessor exposed wire delay architecture ilp streams 
international symposium computer architecture isca june 
theobald gao effects resource limitations program parallelism 
advanced compilers architectures parallel systems acaps technical memo january 
flynn 
detection parallel execution independent instructions 
ieee transactions computers toc october 
scalable processor high ipc 
journal instruction level parallelism august 
veen 
dataflow machine architecture 
acm computing surveys 
budiu asynchronous dataflow circuits 
international workshop logic june 
wall 
limits instruction level parallelism 
technical report digital wrl research report digital western research laboratory november 
wang wang register renaming scheduling dynamic execution predicated code 
international symposium high performance computer architecture hpca january 

yeh patt 
comprehensive instruction fetch mechanism processor supporting speculative execution 
ieee acm international symposium microarchitecture mi cro pages december 

yeh patt 
comparison dynamic branch predictors levels branch history 
international symposium computer architecture isca pages 
