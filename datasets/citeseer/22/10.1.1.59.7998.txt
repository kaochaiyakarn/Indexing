machine learning springer science business media manufactured netherlands 
discriminative bayesian network classifiers logistic regression roos roos hiit fi hannes complex systems computation group helsinki institute information technology box fi finland peter gr centrum voor wiskunde en informatica box nl gb amsterdam netherlands petri ki henry tirri complex systems computation group helsinki institute information technology box fi finland editors pedro larra aga jose lozano jose pe aki 
discriminative learning parameters naive bayes model known equivalent logistic regression problem 
show fact holds general bayesian network models long corresponding network structure satisfies certain graph theoretic property 
property holds naive bayes complex structures tree augmented naive bayes tan mixed diagnostic discriminative structures 
results imply networks satisfying property conditional likelihood local maxima global maximum simple local optimization methods 
show property hold general conditional likelihood local non global maxima 
illustrate theoretical results empirical experiments local optimization conditional naive bayes model 
furthermore provide heuristic strategy pruning number parameters relevant features models 
data sets obtain results heavily pruned submodels containing fewer parameters original naive bayes model 
keywords bayesian classifiers bayesian networks discriminative learning logistic regression 
bayesian network models widely discriminative prediction tasks classification 
parameters models determined unsupervised methods maximization joint likelihood friedman geiger goldszmidt 
years recognized theoretically experimentally situations better matching discriminative supervised learning algorithm conditional likelihood maximization friedman geiger goldszmidt greiner grove schuurmans ng jordan kontkanen ki tirri 
show network structure satisfies certain simple graphtheoretic condition corresponding conditional likelihood maximization problem equivalent logistic regression certain statistics data different network roos structures leading different statistics 
establish connection relatively new methods bayesian network classifier learning established statistical method logistic regression long history thoroughly studied past mclachlan 
main implications connection condition mentioned holds network structure corresponding conditional likelihood bayesian network model local optima global optimum local optimization methods 
implications additional results summarized 
section reviews bayesian network models bayesian network classifiers discriminative learning bayesian network models conditional joint likelihood 
consider network structure directed acyclic graph dag tuple discrete valued random variables xm 
section define main concept conditional bayesian network model set conditional distributions class variable feature variables xm induced bayesian network model structure achieved non zero parameters 
define network structure corresponding canonical form facilitates comparison logistic regression 
graph simply markov blanket class variable arcs added parents fully connected 
show models mb mb identical set joint distributions 
xm corresponding may coincide set conditional distributions xm graphs 
section reviews multiple logistic regression model 
provide reparameterization conditional bayesian network models mb parameters new parameterization correspond logarithms parameters standard bayesian network parameterization 
way conditional bayesian network model mapped logistic regression model 
cases parameters logistic regression model allowed vary freely 
words bayesian network model corresponds subset logistic regression model full logistic regression model 
established theorem 
section main result theorem provides general condition network structure bayesian network model mapped full logistic regression model freely varying parameters 
condition simple requires corresponding canonical structure perfect dag meaning nodes moral nodes 
satisfied example naive bayes model naive bayes model tan complicated models class node parents 
conditional log likelihood logistic regression models concave function parameters 
new parameterization conditional log likelihood concave function parameters condition allowed vary freely convex set rk implies find global maximum conditional likelihood surface simple local optimization techniques hillclimbing 
result leaves open possibility network structures conditional likelihood surface local non global maxima 
superfluous condition 
second result theorem shows bayesian network classifiers logistic regression case simple network structures canonical version perfect conditional likelihood exhibit local non global maxima 
section discusses various issues arising implementing local optimization methods finding maximum conditional likelihood bayesian network models structure satisfies property 
involve choosing appropriate parameterization dealing model selection parameter feature pruning missing data flat regions conditional likelihood surface handle introducing bayesian parameter priors 
turns standard parameterization bayesian network conditional likelihood may non concave bayesian network corresponds logistic regression model 
optimization performed logistic parameterization likelihood concave 
introduce algorithms experiments including heuristic algorithm pruning conditional naive bayes model 
section reports results experiments 
main findings pruning strategy leads considerably simpler models competitive naive bayes conditional naive bayes full set features terms predictive performance 
viewing bayesian network models subsets logistic regression models suggested earlier papers heckerman meek ng jordan greiner zhou 
concavity log likelihood surface logistic regression known result 
main contribution supply condition bayesian network models correspond logistic regression completely freely varying parameters 
guarantee local maxima likelihood surface 
direct consequence result show conditional likelihood instance tree augmented naive bayes tan model local non global maxima 

bayesian network classifiers start notation basic properties bayesian networks 
information see pearl lauritzen 

preliminaries notation consider discrete random vector 
xm variable xi takes values ni directed acyclic graph dag factorizes xi pai pai xm parent set variable xi model usually parameterized vectors components form xi pa defined xi pa xi xi pai pa roos pai configuration set values parents pai xi 
want emphasize pai determined complete data vector 
xm write pai denote configuration pai vector data vector xm need consider modified vector forthe replaced entries remain 
write pai configuration pai xm 
interested predicting class variable xl conditioned xi loss generality may assume class variable children xm instance called naive bayes model children class variable independent value 
bayesian network model corresponding set distributions satisfying conditional independencies encoded conditional distributions class variable variables written xm 
bayesian network classifiers xm xm pa xi pai pa xi pai 
bayesian network model probabilistic prediction classification 
probabilistic prediction mean game query vector xm output conditional distribution xm class variable 
logarithmic loss function log loss incur ln xm units loss actual outcome 
successively predict class variable outcomes query vectors xm xm logarithmic loss function just minus conditional log likelihood query vectors standard statistical measure 
classification mean scenario query vector output single value consider outcome 
loss loss zero guess correct 
bayesian network model corresponding bayesian network classifier friedman geiger goldszmidt uses prediction classification 
parameter vector log loss output conditional distribution loss choose guess class value maximizing 
distribution indexed parameter vector distribution generating data vectors cases bayes optimal choice 
order predict classify new instances training data need fix method infer parameter values training data 
commonly method maximize likelihood equivalently log likelihood training data 
complete data matrix full log likelihood ll parameters bayesian network classifiers logistic regression ll ln xm ln pa 
derivative wrt 
shows complete data maximum achieved setting xi pa xi pa xi pa pa xi pa pa respectively numbers data vectors xi xi pai pa pai pa case data contain missing values closed form solution iterative algorithms expectation maximization algorithm dempster laird rubin mclachlan krishnan local search methods gradient ascent russell thiesson applied 
closed case bernardo smith case data generating distribution represented bayesian network maximizing full likelihood consistent method estimating joint distribution joint distribution maximum likelihood ml parameters converges growing sample size generating distribution 
conditional distributions class variable variables maximum likelihood parameters converge true conditional distribution 
consequence maximum likelihood plug classifier obtained plugging converges bayes optimal classifier 
holds loss logarithmic loss 

discriminative parameter learning context predicting class variable variables full loglikelihood natural objective function take account discriminative supervised nature prediction task 
focused version conditional log likelihood defined cll ln xm xm 
important note appearing terms pa xm class variable incoming arcs 
case pa maximizing full likelihood maximize conditional likelihood 
due normalization possible values parameters maximizing conditional likelihood 
fact closed form solution known friedman geiger goldszmidt 
maximizing conditional likelihood consistent method estimating conditional distributions class variable closed case just maximizing full likelihood 
crucial difference methods case roos generating distribution model class independence assumptions inherent bayesian network structure violated 
open case clear plug predictor guaranteed converge bayes optimal classifier 
probabilistic prediction log loss maximizing conditional likelihood converges best possible distribution model class minimizes expected conditional log loss 
see distributions xm define conditional kullback leibler divergence ex ln expectation taken respect kullback leibler divergence gives expected additional log loss best possible distribution see appendix friedman geiger goldszmidt 
proposition shows converges probability distribution minimizing expected conditional log loss proposition 
data distribution full support 
xm vectors components xi 
ni 
probability large exists maximizing conditional log likelihood 
sequence maximizing distribution converges probability distribution closest conditional kullback leibler divergence 
follows theorem greiner zhou gives addition rate convergence term sample size 
maximizing full likelihood open case may converge best possible distribution 
appendix give simple concrete case example probability ordinary ml estimator converges parameter vector conditional ml estimator converges vector cond ln cond ln 
classification situation clear cut log loss prediction 
empirical theoretical evidence maximizing conditional likelihood leads better classification see friedman geiger goldszmidt ng jordan greiner zhou 
theoretically argue follows 
suppose data distribution necessarily distribution say conditionally correct xm 
xm 
xm occur positive probability 
suppose contains cond conditionally correct 
cond distribution optimal classification bayes classifier cond achieves minimum expected classification error expectation distribution consequence proposition large limit conditional bayesian network classifiers logistic regression ml estimator converges cond asymptotically bayes classifier conditional ml estimator optimal 
note holds cond unconditionally incorrect sense marginal distributions xm xm cond different 
contrast maximizing unconditional likelihood distribution ml estimator converges guaranteed lead optimal classification rule contains distribution fully correct stronger condition 
large training samples conditional ml estimator shown optimal classification weaker conditions unconditional ml estimator 
suggests course prove large training samples conditional ml estimators classification performance unconditional ml estimators 

conditional bayesian network models define conditional model set conditional distributions represented network equipped strictly positive parameter set set functions 
xm distributions form 
model contain notion joint distribution terms xi pai xi class variable undefined interested 
heckerman meek call models bayesian regression classification brc models 
conditional models useful properties shared unconditional counterparts 
example different network structures corresponding conditional models equivalent 
xi pa standing nodes class variable children cancel terms pa pa relevant parameters conditional likelihood form xi pa xi ni pa configuration xi parents pai 
terms graphical structure expressed lemmas 
lemma pearl 
bayesian network conditional distribution variable xi depends variables nodes markov blanket xi 
lemma buntine 
bayesian network mb corresponding conditional model 
node xi parents values implies xi parents class variable bayesian network created deleting arcs xi represents conditional probability model mb mb note node xi parameter xi pa cancels value xi may influence conditional probability xi parent child 
case xi parameters pa cancel 
restrict attention markov blanket just class roos 
networks equivalent terms conditional distributions variable disease shaded variables 
reproduced buntine 
node children 
lemma slightly rephrased buntine illustrated implies assume parents class variable fully connected greatly simplifies comparison logistic regression models 
definition 
arbitrary bayesian network structure define corresponding canonical structure classification structure constructed restricting markov blanket adding arcs needed parents fully connected 
network structures mb mb equivalent 
follows lemmas 
graph graph 

bayesian network classifiers logistic regression think conditional model obtained bayesian network predictor combines information observed variables update distribution target class variable 
view bayesian network discriminative generative model dawid heckerman meek ng jordan jebara 
order view concrete introduce logistic regression models extensively studied statistics see mclachlan 
section see conditional bayesian network model may viewed subset logistic regression model 

logistic regression models random variable possible values yk bea real valued random vector 
multiple logistic regression model dependent variable bayesian network classifiers logistic regression 
logistic regression model 
covariates 
yk defined set conditional distributions exp xi yi exp yi parameter vector components form 

allowed take values rn graphical representation model intermediate nodes correspond linear combinations xi yi covariates converted predicted class probabilities pi normalized exponential softmax bishop function 
values class variable 
covariates 
components gradient vector partial derivatives log likelihood ln ys indicator function value argument true 
hessian matrix second derivatives entries ln 
theorem known statistics see mclachlan 
theorem 
hessian matrix negative semidefinite 
theorem direct consequence fact logistic regression models exponential families see mclachlan barndorff nielsen 
roos model extended outcomes assumption defining loglikelihood ln 
data set entries gradient vector hessian matrix sums terms respectively 
theorem hessian data vector negative semidefinite logarithm concave 
log likelihood sum concave functions concave corollary 
log likelihood concave function parameters 
corollary 
concavity fact parameter vector varies freely convex set guarantees local non global maxima log likelihood surface logistic regression model 
conditions global maximum exists maximum likelihood estimators diverge discussed mclachlan 
possible solution cases maximum exists assign prior model parameters maximize conditional posterior likelihood see section 
prior resolve problems optimization caused known fact parameterization logistic model log likelihood surface strictly concave 

logistic representation bayesian networks order create logistic model corresponding bayesian network structure introduce new set covariates derived original variables 
parent configurations pa set pa pa 
denote parameters associated covariates pa define pa pai parent set xi exclusion class variable 
fori xi ni pa dom pa set xi pa xi xi pa pa 
denote parameters associated covariates xi pa example 
consider bayesian network 
covariates type correspond combinations values age occupation climate 
covariates bayesian network classifiers logistic regression type correspond combinations values age symptoms 
logistic model obtained network identical 
convenience loss generality indexing form pa yi 
notations written function variables xm xm exp pa pa pa pa exp ni xi pa xi pa ni xi pa xi pa xi pa xi pa indicator variables take value zero equation simplifies xm exp pa exp xi pa pa xi pa 

conditional model mb set conditional distributions represented logistic regression model corresponding turns logistic regression conditional model mb closely related corresponding conditional bn model mb theorem shows conditional distributions representable bayesian network mapped distributions logistic model 
theorem 
set conditional distributions represented bayesian model network structure strictly positive parameters conditional model defined logistic regression model covariates 
proof arbitrary parameter vector 
theorem equivalent parameter vector logistic regression model models represent conditional distributions 
set pa ln pa xi pa ln xi pai pai combination pa 
plugging gives conditional distributions 
data define conditional log likelihood logistic model ln xm roos 
note properties 
particular corollaries apply local maxima log likelihood surface logistic regression model global conditional maximum likelihood parameters obtained training data prediction data 
addition discussed heckerman meek perform model selection competing model structures bayesian information criterion bic schwarz approximations minimum description length mdl criterion rissanen 
heckerman meek state general conditional bayesian network models may difficult determine global maximum gradient methods 
locate local maxima 
corollary shows network structure models equivalent mb mb find global maximum conditional likelihood logistic model local optimization method 
crucial problem determine exact condition equivalence holds 

theoretical results preceding sections gave logistic representation bayesian networks showed conditional distributions class variable represented logistic model 
show general converse statement true means conditional models equivalent 
give condition network structure conditional models equivalent 

condition network structure shown previous section parameters follows distribution mb mb theorem 
suggests doing reverse transformation exp pa xi pa exp xi pa show distributions parameters logistic model free cases violate sum constraint rn get transformation parameters ni xi 
pai 
parameters valid xi pa bayesian network parameters 
note simply renormalizing xi 
change resulting distributions 
parameterization logistic model may case distribution indexed parameters mb turns network structures corresponding mb distribution mb expressed parameter vector mapping gives valid bayesian network parameters 
case mb mb bayesian network classifiers logistic regression 
simple bayesian network class variable denoted satisfies condition canonical form connected making structure perfect network remains unchanged canonical transformation remains imperfect 
main result case canonical version definition perfect definition 
definition lauritzen 
directed graph nodes having common child connected called perfect 
example 
consider bayesian networks depicted 
perfect 
canonical version added arc 
perfect 
network perfect changing conditional model theorem shows conditional likelihood surface local maxima implying case examples network structures perfect naive bayes nb naive bayes tan models friedman geiger goldszmidt 
proof straightforward omitted 
generalization children class variable allowed form tree structures see 
implies class moral node common child node directly connected 
moral may violated exemplified 
condition automatically satisfied incoming arcs diagnostic models discussed kontkanen ki tirri 
bayesian network structures condition hold add arrows 
tree augmented naive bayes tan model perfect graph network canonical form tan perfect 
roos arrive structure condition hold instance add arrow 
subset larger model condition holds 

main result main result stating conditional models bayesian network corresponding logistic regression model respectively equivalent perfect theorem 
canonical version perfect proof proposition proposition lauritzen 
perfect dag 
distribution admits factorization form respect factorizes set cliques non negative functions depend variables clique recall clique fully connected subset nodes 
set cliques appearing contains maximal non maximal cliques consisting single nodes 
proof theorem 
need show arbitrary parameter vector logistic model bayesian network parameters index distribution logistic model 
xi pa parameters obtained define normalizing constant pa xi pa xi pai define consider joint distribution pz defined pz xi pai 
note product xi pai may sum data vectors introducing normalizing constant ensure resulting pz defines probability distribution 
xm distribution induces conditionals logistic model parameter vector 
bayesian network classifiers logistic regression function non negative function set xi pai clique assumption 
factorization form proposition implies pz admits factorization form usual bayesian network factorization 
bayesian network parameters satisfy sum constraint represent distribution pz 
particular parameters give conditional distributions logistic model 
proof constructive explicitly give bayesian network parameters give conditional distributions logistic model 
constructive proof 

omitted proof shorter easier understand clarifies connection 
corollary stating conditional log likelihood concave theorem shows suffices ensure conditional likelihood surface local non global maxima 
implies example conditional likelihood surface tan models local maxima 
global maximum local optimization techniques 
case perfect 
second result theorem proven appendix says case local maxima theorem 
exist network structures canonical form perfect conditional likelihood local non global maxima 
theorem implies mb mb network structures infact stronger statement structures logistic model indexes conditional distributions mb proof stronger statement contradiction mb coincided logistic regression model conditional likelihood surface mb local maxima contradiction 
superfluous condition 
may ask necessary condition having mb mb logistic model mb possibly different network structure plan address intriguing open question 

technical issues point tools order build logistic regression model equivalent bayesian classifier underlying network structure canonical version perfect 
parameters may determined hill climbing local optimization method conditional log likelihood maximized 
results prediction method cases outperforms corresponding bayesian classifier ordinary maximum likelihood parameters 
practice find number questions answered 
address crucial technical details outline algorithms implemented 
roos 
standard logistic parameterization 
mapping bayesian parameters logistic model proof theorem continuous follows calculus mb mb maxima concave conditional likelihood logistic parameterization global connected maxima standard bayesian parameterization 
may standard parameterization optimize conditional log likelihood locally obtain global maximum 
demonstrated example log likelihood surface function unpleasant properties concave general worse wrinkles mean convex subsets parameter space likelihood surface exhibit local non global maxima 
suggests computationally preferable optimize logistic parameterization original bayesian network parameterization 
example 
consider bayesian network class variable child variables take values 
training data 
set parameters follows 
shows conditional log likelihood cll data function 
note log likelihood peaks twice straight line contradicting concavity 

conditional log likelihood conditional bayesian network example peaks twice line defined bayesian network classifiers logistic regression 
continuous predictor variables bayesian classifiers built difficulty handling continuous data conditional probability distributions represented tables form xi pa see section 
opposed logistic regression models natural way handling discrete data 
shortcoming easily introducing covariate pa instantiation pa parent set variable xi seen section 
experiments discretized continuous features training data entropy method fayyad irani 
way methods compare discrete input 
note logistic models flexible exploited advantages 
fed original continuous values just difficulty combining information discrete attributes handled mixed features died age alive 

possible discretize fly generate covariates form xi xi xi beneficial discriminative model 
combination original continuous value feature discretized version introducing piece wise log linear function model 
leave pursuit ideas objective research 

model selection usually provided bayesian network structure data sample choose model structure 

hard problem modelling joint data distribution buntine heckerman ki 
finding conditional model harder joint modelling tools modelling joint distribution longer 
example methods cross validation prequential validation computationally highly demanding candidate network re optimize model parameters 
optimizing conditional log likelihood single model quite feasible reasonable size data optimization computationally demanding task model selection 
joint data likelihood criterion easier may yield poor results improvements achieved heuristic semi supervised methods mix supervised unsupervised learning keogh pazzani kontkanen cowell shen madden grossman domingos 
reasons take different approach 
start naive bayes classifier simplest model account data entries 
predictions form extended independence 
naive bayes models set pa empty nodes xi may denote pa xi xi pa 
algorithm conditional naive bayes cnb 
second algorithm pruned naive bayes pnb submodel cnb parameter selection scheme described 
cnb pnb models parameterized logistic regression fashion 
roos stress scope experiments limited 
implementing fully supervised computationally feasible model selection method severe restrictions range network structures challenging open research topic 

missing data standard classification methods including bayesian network classifiers logistic regression designed case training data vectors complete sense missing values 
real world data missing data feature values appear rule exception 
need explicitly deal problem 
general way handling issue treat missing legitimate value features xi value missing data vectors ki 
model larger may result overfitting worse classification performance small samples 
assumed patterns data provide information class values 
mathematically expressed follows assume true data generating distribution satisfies class values vectors ith element missing xi missing xi xi sum ordinary values xi excluding value missing 
equation trivially extended multiple missing values assumption typically wrong leads acceptable results practice 
related notion missing completely random see little rubin strictly stronger requirement requires xi missing gives information joint distribution xm require hold class variable 
proper way implement integrate missing entries 
parameter learning search problem np complete 
adjusted learning method achieve approximation effectively ignoring skipping parameters corresponding missing information inference prediction 
precisely introduce constraints form xi xi estimate terms xi training data 
result models respect approximation logarithmic version log xi missing xi logp xi bayesian network classifiers logistic regression way skipping parameters corresponding missing values results logarithmically unbiased predictive distributions judged basis experiments reported section approximation practice 

priors practical applications sample typically include zero frequencies xi xi 
case conditional log likelihood cll maximum diverge 
problem arise subtle situations see example 

avoid problems introducing bayesian prior distribution conditional model bernardo smith 
kontkanen 
shown ordinary unsupervised naive bayes danger fitting training data usually small sample sizes prediction performance greatly improved imposing prior parameters 
conditional model cnb inclined worse fitting unsupervised naive bayes ng jordan hold case 
impose strictly concave prior goes zero absolute value parameter approaches infinity 
choose closest find uniform informative prior usual parameterization parameters take values zero 
parameters xi back space probability distributions normalized exponentials define prior probability density proportional product entries resulting distributions 
conditional log likelihood cll optimize conditional posterior gr cll cll ln 
prior yields strict cll sum concave strictly concave function guarantees unique maximum 

algorithms fill missing details algorithms explain actual optimization performed 
conditional naive bayes algorithm maximizes cll component wise binary search convergence 
compute second derivatives takes computation time benefit convergence speed obtained sophisticated methods newton raphson conjugate gradient ascent lost 
minka suggests compares number algorithms task 
case simplest coordinate wise line search suffice 
second method pruned naive bayes classifier pnb aims preventing overfitting 
prune full naive bayes model cnb maximizing objective roos additional freedom exclude parameters model 
ignored way parameter corresponding data entry missing cll ln exp exp ln defined set parameters apply vector xi missing 
note zero valued parameters effect conditional likelihood corresponding prior term zero parameter chosen part model associated cost 
defines natural threshold parameter xi improve conditional log likelihood ln xi ln removed model deteriorate log posterior 
pnb algorithm quite simple 
start full cnb model eliminate parameters time improvement achieved 
speed process choose parameter dropped cll maximized remaining parameters 
re optimize choosing parameter drop 
parameter left exclusion model yields direct gain choose parameter causing loss 
re optimizing system objective improved undo step algorithm terminates 

empirical results compare methods conventional naive bayes 
experimental methodology resembles madden friedman geiger goldszmidt 
split data set disjunct train test sets random training set contains original data set test set remaining times independently previous splits 
report average log loss loss times standard deviation splits 
mentioned section discretize continuous features entropy method fayyad irani 
done training data random split results possibly different discretizations 
data vectors missing entries included tests 
test bed took data sets uci machine learning repository blake merz contain missing data 
tables marked asterisk 
table lists data sets sizes number parameters different algorithms 
nb cnb models parameterized differently obviously contain number parameters 
variance number due individual discretizations training set 
note drastic pruning performed pnb 
list log scores achieved algorithms nb cnb pnb table 
comparison report results default predictor class node independent nb algorithm equipped uniform prior parameters 
default gives clue hard learn bayesian network classifiers logistic regression table 
data sets numbers parameters models learned 
data set size classes nb cnb pnb balance scale bc wisconsin congr 
voting crx ecoli glass ident 
hd cleveland hd hungarian hd switzerland hd va hepatitis iris mushrooms pima diabetes tic tac toe waveform wine rec 
predictors hard beat default predictor may information features class 
hand high variance default may indicate great effect random splits hard prediction task 
winning scores typeset boldface 
terms log loss discriminative models cnb pnb clearly outperform standard naive bayes 
cases standard naive bayes slightly better outperformed supervised methods greater margin 
case especially large data sets mushrooms waveform independence assumptions naive bayes model badly violated balance scale congressional voting tic tac toe 
behavior natural reported greiner zhou 

figures illustrate numerical results 
observe pnb algorithm chooses parameter candidates performance comparable cnb see 
addition pnb robust tends yield better results cnb assumed fit cnb loses nb default losing little cases cnb better performance 
note regardless suboptimal search method pnb stable terms variance performance different data splits 
parameters pruned logistic model achieves results 
interestingly data set hard learn roos 
pairwise comparison algorithms nb cnb nb pnb cnb pnb terms log loss misclassification percentage uci data sets test bed 
bayesian network classifiers logistic regression table 
predictive accuracies respect logarithmic loss 
data set default nb cnb pnb balance scale bc wisconsin congr 
voting crx ecoli glass ident 
hd cleveland hd hungarian hd switzerland hd va hepatitis iris mushrooms pima diabetes tic tac toe waveform wine rec 
class pnb constantly chooses parameters leads better performance full model 
hand data set tic tac toe pnb chooses large fraction available parameters behaving different cnb splits 
table counterpart table reporting results test runs terms loss 
figures compare classification errors algorithms 
naive bayes relatively better loss note wins smaller margins loses data sets 
logistic models cnb pnb quite comparable terms classification accuracy 

focus discriminative learning models sample data goal determine model parameters maximizing conditional supervised likelihood commonly joint unsupervised likelihood 
theoretical part showed bayesian network models satisfying simple graphtheoretic condition problem equivalent logistic regression problem 
bayesian network structures satisfying condition include naive bayes model naive bayes model condition allows non trivial network structures 
remains open problem condition necessary roos table 
percentages correct predictions 
data set default nb cnb pnb balance scale bc wisconsin congr 
voting crx ecoli glass ident 
hd cleveland hd hungarian hd switzerland hd va hepatitis iris mushrooms pima diabetes tic tac toe waveform wine rec 
bayesian networks violating condition represented logistic regression model 
empirical part exploited theoretical results obtained experimented discriminative models 
model conditional version naive bayes model parameters optimized respect conditional likelihood 
second model added heuristic procedure selecting set parameters relevant features 
cases theoretical results offer parameterization conditional likelihood global maximum finding maximizing discriminative parameters principle easy computationally demanding parameters maximizing joint likelihood 
empirical results contrasted obtained standard naive bayes classifier 
results demonstrate discriminative models typically give better predictive accuracy respect logarithmic loss 
parameter pruning algorithm introduced yields models simpler naive bayes classifier discriminative version significant decrease accuracy 
fact interesting bayesian network structures conditional likelihood function global maximum practically important means discriminative parameters local optimization methods 
need apply computationally elaborate techniques finding parameters 
furthermore result suggests wishes naive bayes classifier straw man method bayesian network classifiers logistic regression alternative approaches compared case equally better straw man method offered supervised version naive bayes model 
hand results may implications respect model selection problem supervised domains model selection criteria typically contain data likelihood factors criterion cf 
example bayesian information criterion bic schwarz approximations minimum description length mdl criterion rissanen 
natural assume conditional likelihood plays important role supervised versions model selection criteria 
aspect addressed formally 
appendix proofs proof sketch theorem rightmost network structure 
data 
interested predicting value 
parameter defining distribution conditional predictions ignore 
remaining parameters notation 
idea proof 
empirical distribution data highly perfectly dependent value 
perfect dependence represented distributions mb network structure implies conditioned independent 
parameters correspond contexts occur fact exploited setting represent distributions 
distributions represent dependence converge maximum conditional likelihood 
setting represent distributions converge maximum conditional likelihood 
part proof formalize argument show data non connected suprema conditional likelihood 
part ii sketch argument extended allow non global maxima suprema 
roos 
function 
part conditional log likelihood written cll ln xy 
xy illustrates function atx 
parameter appears 
fixed maximize term separately 
apply lemma defined 
follows lemma supremum log likelihood fixed ln ln achieves maximum value ln 
furthermore lemma shows log likelihood approaches supremum 
item ii lemma suprema separated areas log likelihood smaller suprema local connected 
part ii 
conclude proof need address issues local suprema give conditional log likelihood ln suprema maxima achieved 
roughly sketch extend argument deal issues 
concerning fix consider sample consisting data vectors repetitions repetitions repetitions repetitions 
lemma way find conditional log likelihood local suprema connected 
suprema equal 
concerning sample extra barrier data vectors added 
corresponding conditional log likelihood cll 
parameters bayesian network classifiers logistic regression 
hand continuous finite soc achieve maximum maxima hand large influence barrier data vectors individual point negligible 
lemma item iii facts exploited show large maxima equal connected 
omit details 
lemma 
define defined 

varying global supremum satisfies sup sup ln 
ii local suprema lim lim ln 
iii restricted sup ln sup sup ln proof differentiating twice wrt 
gives xy positive function achieves maximum value 
points differentiating wrt 
yields xy 
xy case derivative negative second case derivative positive increases monotonically increases monotonically 
denoting lim lim ln roos implies orl 
item follows 
inspecting derivative wrt see exists depending increases monotonically 
increases monotonically neighborhood local supremum 
proof neighborhood analogous concluding item ii 
equation item iii follows calculating noting result ln independent second equation follows showed second derivative wrt positive fixed sup max 
supremum expression achieved equal 
third equation proved similarly 
example 
order see conditional predictions result maximum likelihood parameters data set consider proof theorem parameter configuration cond small 
conditional probability close conditional probability 
contrast unconditional maximum likelihood parameters conditional probability 
suppose data distribution puts uniform probability data vectors 
law large numbers probability unconditional ml parameters sample converge conditional ml parameters achieve maxima cond infinitesimally close arguments proof theorem show ln cond ln denoting conditional kl divergence defined 
example shows exist data generating distributions conditional ml far superior unconditional ml 
acknowledgments authors anonymous reviewers useful comments 
supported part academy finland projects minos prima ist programme european community pascal network excellence ist 
publication reflects authors views 
notes 
avoid zero parameters introducing priors parameters see section 
markov blanket node xi consists parents xi children xi parents children xi 
bayesian network classifiers logistic regression 
addition done introducing cycles lauritzen usually different ways equivalent purposes 

noted section case maximum conditional likelihood parameters may determined analytically 
barndorff nielsen 

information exponential families statistical theory 
new york ny john wiley sons 
bernardo smith 

bayesian theory 
new york ny john wiley sons 
bishop 

neural networks pattern recognition 
oxford uk oxford university press 
blake merz 

uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
university california irvine department information computer science 
buntine 

operations learning graphical models 
journal artificial intelligence research 
cowell 

searching optimal classifiers bayesian networks 
jaakkola richardson eds proceedings international workshop artificial intelligence statistics 
pp 

san francisco ca morgan kaufmann 
dawid 

properties diagnostic data distributions 
biometrics 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
fayyad irani 

multi interval discretization continuous valued attributes classification learning 
bajcsy ed proceedings thirteenth international joint conference artificial intelligence pp 

san francisco ca morgan kaufmann 
friedman geiger goldszmidt 

bayesian network classifiers 
machine learning 
greiner grove schuurmans 

learning bayesian nets perform 
geiger shenoy eds proceedings thirteenth annual conference uncertainty artificial intelligence pp 

san francisco ca morgan kaufmann 
greiner zhou 

structural extension logistic regression discriminant parameter learning belief net classifiers 
dechter kearns sutton eds proceedings eighteenth national conference artificial intelligence pp 

cambridge ma mit press 
grossman domingos 

learning bayesian network classifiers maximizing conditional likelihood 
brodley ed proceedings international conference machine learning pp 

madison wi 
gr kontkanen ki roos tirri 

supervised posterior distributions 
seventh valencia international meeting bayesian statistics spain 
heckerman 

tutorial learning bayesian networks 
technical report msr tr microsoft research redmond wa 
heckerman meek 

embedded bayesian network classifiers 
technical report msr tr microsoft research redmond wa 
heckerman meek 

models selection criteria regression classification 
geiger shenoy eds proceedings thirteenth annual conference uncertainty artificial intelligence pp 

san francisco ca morgan kaufmann 
jebara 

machine learning discriminative generative 
boston ma kluwer academic publishers 
keogh pazzani 

learning augmented bayesian classifiers comparison distribution classification approaches 
heckerman whittaker eds proceedings seventh international workshop artificial intelligence statistics pp 

san francisco ca morgan kaufmann 
kontkanen ki tirri 

supervised selection bayesian networks 
laskey prade eds proceedings fifteenth international conference uncertainty artificial intelligence pp 

san francisco ca morgan kaufmann publishers 
roos kontkanen ki tirri gr 

predictive distributions bayesian networks 
statistics computing 
kontkanen ki tirri 

classifier learning supervised marginal likelihood 
breese koller eds proceedings seventeenth conference uncertainty artificial intelligence pp 

san francisco ca morgan kaufmann 
lauritzen 

graphical models 
oxford uk oxford university press 
little rubin 

statistical analysis missing data 
new york ny john wiley sons 
madden 

performance bayesian network classifiers constructed different techniques 
working notes ecml pkdd workshop probabilistic graphical models classification pp 

mclachlan 

discriminant analysis statistical pattern recognition 
new york ny john wiley sons 
mclachlan krishnan 

em algorithm extensions 
new york ny john wiley sons 
minka 

algorithms maximum likelihood logistic regression 
technical report carnegie mellon university department statistics 
revised sept 
ki tirri 

course web tool bayesian causal data analysis 
international journal artificial intelligence tools 
ng jordan 

discriminative vs generative classifiers comparison logistic regression naive bayes 
dietterich becker ghahramani eds advances neural information processing systems pp 

cambridge ma mit press 
pearl 

evidential reasoning stochastic simulation causal models 
artificial intelligence 
pearl 

probabilistic reasoning intelligent systems networks plausible inference 
san mateo ca morgan kaufmann 
shen ng mccallum 

classification hybrid generative discriminative models 
thrun saul sch lkopf eds advances neural information processing systems 
cambridge ma mit press 
rissanen 

fisher information stochastic complexity 
ieee transactions information theory 
russell binder koller 

local learning probabilistic networks hidden variables 
thrun mitchell eds proceedings fourteenth international joint conference artificial intelligence pp 

san francisco ca morgan kaufmann publishers 
schwarz 

estimating dimension model 
annals statistics 
shen su greiner cheng 

discriminative parameter learning general bayesian network classifiers 
proceedings fifteenth ieee international conference tools artificial intelligence pp 

los alamitos ca ieee computer society press 
thiesson 

accelerated quantification bayesian networks incomplete data 
fayyad uthurusamy eds proceedings international conference knowledge discovery data mining pp 

cambridge ma mit press 
gr roos ki tirri 

supervised naive bayes parameters 
ala kaski eds proceedings tenth finnish artificial intelligence conference pp 

oulu finland finnish artificial intelligence society 
gr roos ki tirri 

discriminative learning bayesian network parameters easy 
gottlob walsh eds proceedings eighteenth international joint conference artificial intelligence pp 

san francisco ca morgan kaufmann publishers 
received march revised november accepted november 
