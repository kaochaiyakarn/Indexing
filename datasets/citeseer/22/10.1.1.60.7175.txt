metric adaptation optimal feature classification learning vector quantization applied environment detection villmann university leipzig cl str 
leipzig villmann informatik uni leipzig de hammer university ck department mathematics computer science ck hammer informatik uni de 
gmbh leipzig stra fms de deals concept relevance learning learning vector quantization 
approaches considered generalized learning vector quantization soft learning vector quantization 
shown relevance learning included methods obtaining similar structured learning rules prototype learning relevance factor adaptation 
show power case image classification natural environment images 
performance tool suitable classification tasks robotics 
robotics navigation robots natural environments difficult task 
robot determine actual position control faithfully respective action dependence location working task 
accurate knowledge location strongly required 
purpose knowledge positions known objects may supposes fast recognition objects robot sensory informations 
large number sensors may exist discover environment different ways different sensors different importance specific recognition tasks 
detection object sensory informations may done classification system 
naturally respective classifications adapted advance supervised training 
exist lot classification systems neural networks play important role robustness adaptivity 
learning vector quantizers constitute algorithms neural network family mainly influenced standard algorithms lvq 
lvq introduced kohonen 
intuitive prototype classification algorithms 

mars robot spirit lot different sensors tasks navigation 
modifications developed improve standard algorithms ensure instance faster convergence better adaptation receptive fields optimum bayesian decision adaptation complex data structures name just 
critic point instability algorithm 
various approaches deal development efficient cost functions minimized types soft computing extensions unsupervised models supervised learning 
models unsupervised supervised vector quantization investigate application non standard metrics relevance learning 
modifications particularly suited high dimensional heterogeneous data dealt hyperspectral images data bioinformatics mentioned robotics 
focus problems concerning lvq classification systems short review approaches providing modifications obtain cost function minimized second deals weighting sensory channels better classification 
concept taken metric adaptation called relevance learning 
particular review approaches overcome problem cost functions learning vector quantization consider light relevance learning 
algorithm supervised relevance neural gas srng generalized relevance learning vector quantization approach extended neighborhood cooperation known neural gas ng 
cost function derived generalized learning vector quantization glvq including terms due relevance learning approach 
cost function modified srng cost function unsupervised ng pay attention neighborhood learning supervised case 
second method investigated proposed soft learning vector quantization 
show certain conditions relevance learning applied leads similar update relevance weights srng 
structured follows shortly review approaches srng 
derive relevance learning rules show similarity srng 
section apply relevance learning classifiers classify images natural environments show performance relevance learning approach 
learning vector quantization cost function lvq supervised neural gas standard lvq hand side heuristic approach reduce classification error supervised learning 
adaptation dynamic minimize continuous cost function shows instabilities 
hand result lvq crucially depends initialization prototypes commonly initialized classes random representatives training set 
algorithms iterative local learning rules easily stuck local optima 
glvq avoids numerical instabilities lvq due stochastic gradient descent cost function optimizes margin 
crucially dependent initialization 
overcome drawback combination glvq ng proposed way cost function minimized trough learning rule 
cost function leads training similar ng simple glvq respectively depending choice parameter cost function 
training parameter varied neighborhood cooperation assures distribution prototypes data set separation classes accounted training 
shortly respective formal descriptions 
clarify notations cv lbe label input set labels classes nl 
letv dv finite set inputs lvq uses fixed number prototypes weight vectors codebook vectors class 
wr set codebook vectors cr class label furthermore wc wr cr subset prototypes assigned class task vector quantization realized map winner take rule stimulus vector mapped neuron ws pointer closest stimulus vector argmin wr arbitrary differentiable similarity measure may depend parameter vector 
moment take fixed 
neuron called winner best matching unit 
subset input space mapped particular neuron forms masked receptive field neuron forming voronoi tesselation 
class information weight vector boundaries generate decision boundaries classes 
training algorithm adapt prototypes class corresponding codebook represent class accurately possible 
means set points class cv uc union receptive fields corresponding prototypes differ wr wc little possible 
consider generalized learning vector quantization approach glvq 
main idea introduce cost function learning rule gives gradient descent 
time assess number misclassifications prototype classification 
exp logistic function 
glvq minimizes cost function cost similarity measure non negative real valued function variables contrast distance measure necessarily fulfill triangle inequality symmetric 
naturally distance measure similarity measure 
stochastic gradient descent squared distance input vector nearest codebook vector labeled cr cv andd squared distance best matching prototype labeled cr cr 
shown usage function yields robust behavior lvq 
learning rule glvq obtained derivatives cost function 
wr wr wr wr obtains weight updates wr wr learning rates 
shown learning rules valid case continuous data distribution 
original unsupervised neural gas ng adapts unlabeled wi prototypes data set cost function minimized local costs cost ng wi ei dv neighborhood function known ng exp ki ki yields number wj prototypes relation wj wi valid ki winner rank 
normalization constant depending neighborhood range cardinality learning rule reads minimizing cost function 
initialization prototypes longer crucial ng involved neighborhood cooperation 
mentioned supervised neural gas sng constitutes combination glvq ng 
wc wr cr subset prototypes assigned class land cardinality 
assume data vectors vi 
pointed neighborhood learning input label applied subset respective cost function sng cost wr wc vi wc ci exp defined glvq wr 
neighborhood sure prototypes spread faithfully data respective classes 
note lim vanishing neighborhood sng optimal sense margin analysis 
neighborhood range large typically training prototypes class share responsibilities input 
neighborhood cooperation involved initialization prototypes longer crucial 
training vi ci example wr wci prototypes closest wrong wr prototype adapted 
get update rules wr wr wc wci ci vi wci ci wr include neighborhood cooperation wrong prototypes 
shown yields instabilities learning 
margin analysis algorithm important asses level confidence classifier respect decision 
example sample margin defined distance input decision boundary 
natural choice margin maximized learning vector quantization causes numerical instabilities 
alternative definition gives hypothesis margin margin distance classifier move changing way labels sample data 
sample margin hypothesis margin 
hypothesis margin prototype classifier fact glvq sng vanishing neighborhood maximizes cost function closely related hypothesis margin taken maximum margin algorithm 
margin provides upper bound generalization error classifier higher margin lower generalization error 
detailed analysis refer 
soft learning vector quantization soft learning vector quantization published approach overcome numerical instabilities lvq lvq algorithm 
defines cost function optimized gradient approach 
cost function suitable chosen probability density functions 
introduce detail 
strongly follow description 
probability density data cv label input set labels classes nl 
cv defines input set described mixture model component mixture homogeneous generates data points belong class cv cj 
wr set codebook vectors 
cr class label wr wr cr 
furthermore subset prototypes assigned class land complement 
complement single class probability density data point nl cj probability data points generated jth component mixture conditional probability particular data vector generated jth component 
case lvq related prototypes wr identify conditional probability prototype wr 
probability density data point label cv mixture model correct classification analogy cv cr cv cv cr ccv probability density data point generated mixture model incorrect classes 
respective likelihood functions vk ccv maximized minimized respectively obtain mixture model 
achieved maximizing likelihood ratio logarithmic value cv cost log log vk vk vk vk log vk computationally easier handle 
expectation log cost function regard true distribution cv written terms kullback leibler divergences log kl cv cv dkl cv cv log dv kullback leibler divergence 
minimization dkl equivalent maximize divergence cv cost true density data distribution cv performed incorrect classes minimize divergence cv data distribution performed correct class 
cv cv maximizing divergence dkl cv may cause instabilities kullback leibler divergence bounded 
gaussian distributions increases increasing center distance 
introduce window rule lvq 
alternatively modify cost function log lr log vk vk vk vk vk relation 
ratio bounded stochastic learning done gradient ascent cost function cost vk cv vk divergent behavior prevented 
obtain learning rule theory stochastic approximation wr wr log cv conditional probabilities normalized exponential kernels gradient exp wr cost yields update rule 
relations wr log cv cv cv wr wr wr wr exp wr cr cv exp wr cv cr cv cr posterior probability input assigned mixture component assumption generated correct class exp wr exp wr posterior probability input assigned mixture component components 
cv taken soft assignments prototypes belonging class prototypes respectively 
yields learning rule 
choice function wr parameter specified 
context lvq learning wr describe relation weight vectors data adequate manner 
relevance learning glvq sng consider relevance learning introduced learning vector quantizers study influence parameter vector distance measure defined 
words interested adaptation distance measure dependence minimize cost function looking relevance parameters 
adaptation step parameters added usual weight vector adaptation 
assume 
learning relevance glvq supervised neural gas relevance learning glvq obtained derivatives cost function parameters 
followed renormalization 
learning rate 
refer algorithm glvq relevance 
sng relevance learning yields srng weight update extended wr wc vi ci learning rate renormalization demanded 
refer algorithm supervised relevance neural gas srng 
update rule valid continuous case 
learning rate approaches magnitude smaller learning rates weight vectors 
weight vector adaptation takes place quasi stationary environment respect slowly changing metric 
margin optimization takes place level parameter get optimization margin including relevance weighting parameters 
learning relevance introduce components relevance learning similar approach srng refer approach 
assume wr wr differentiable non negative similarity measure parametrized parameter vector partial derivatives wr require 
gradient cost func tion respect parameters determines stochastic gradient ascent optimization log cv log cv cv remember cv take mixture approach cr cv cv account 
term rewritten cv cv cv second term reformulated cr cv kernel approach obtain cv cr cv exp cr cv wr exp wr exp wr exp summarizing update rule parameters cr cv wr cv cr cv wr wr 
definitions soft assignments cv 
value respective learning parameter 
relevance learning scaled euclidean distances srng scaled euclidean distances consider case wi squared scaled euclidean distance 
immediately get glvq update wr wr wr respectively diagonal matrix entries relevance parameter update reads dv wr case sng srng get updates case scaled euclidean distance wr wr vi ci wr vi wr wc diagonal matrix entries wr wc vi ci ci wr wr wr wr gaussian mixtures scaled euclidean distances specify general approaches exponential kernel 
choice generalized gaussian kernels const 

cv number codebook vectors codebooks priori probability 
posterior probabilities soft assignments simplified cv exp cr cv exp wr exp exp respectively 
update rule prototypes obtained cv exp exp wr exp wr exp wr exp wr cv cr diagonal matrix entries ii relevance factor update derived exp wr wr exp exp cv cr cr cv exp wr wr cr cv exp lvq glvq learning test table 
accuracy results image classification different lvq approaches 
application relevance learning detection natural environments show possibilities relevance learning environment detection classification robotic applied database images natural environments 
database obtained uci repository machine learning databases image segmentation database ftp ftp ics uci edu pub machinelearning databases image data evaluating learning valid experiments delve project delve toronto www cs toronto edu delve 
instances regions drawn randomly database outdoor images cement foliage grass path sky window 
images hand segmented create classification pixel 
data set consists data vectors data dimension 
vector components give details regions contrast color intensity excess saturation 
randomly split data learning test data 
tested algorithm weight vector class 
performance algorithm compared standard learning vector quantizer lvq glvq 
see glvq improves accuracy classification standard lvq 
outperforms glvq 
mentioned eliminated dimensions respective relevance weights set zero 
shown concept relevance learning known generalized learning vector quantization glvq applied published method soft learning vector quantization referred 
derive update rule shows high analogy soft variant supervised relevance neural gas srng 
particular srng correct classifying prototypes best matching wrong prototype updated 
simulations shown updating prototypes sng srng causes numerical instabilities 
considering update relevance factors second term focusses prototypes srng best matching incorrect prototype taken account relevance learning 
shown optimal margin classifier support vector machines svm 
generalization bounds derived show aims structural risk minimization comparable svm 
svm computational costs lower 
srng offers principled alternative classification tasks 
particular abilities online long life learning interesting classification tasks robotic 
hammer von relevance determination learning vector quantization 
th european symposium artificial neural networks 
esann 
proceedings 
facto belgium pages 
crammer gilad bachrach tishby 
margin analysis lvq algorithm 
proc 
nips www cs cmu edu groups nips nips nips index html 
hammer villmann 
generalization ability networks 
cker zur mathematik university ck germany 
hammer villmann 
prototype recognition splice sites 
editor computational intelligence paradigms appear 
springer verlag 
hammer villmann 
generalized relevance learning vector quantization 
neural networks 
kaski sinkkonen 
topography preserving latent variable model learning metrics 
yin slack editors advances self organizing maps pages 
springer london 
kaski sinkkonen 
bankruptcy analysis self organizing maps learning metrics 
ieee transactions neural networks july 
kohonen 
self organizing maps volume springer series information sciences 
springer berlin heidelberg 
second extended edition 
kohonen kaski rvi 
adaptive subspace self organizing map assom 
international workshop self organizing maps pages helsinki 
kushner clark 
stochastic methods constrained unconstrained systems 
springer verlag new york 
martinetz schulten 
neural gas network vector quantization application time series prediction 
ieee trans 
neural networks 
pfurtscheller 
automated feature selection distinction sensitive learning vector quantizer 
neurocomputing 
ritter 
self organizing maps non euclidean spaces 
oja kaski editors kohonen maps pages 
elsevier amsterdam 
sato yamada 
analysis convergence generalized lvq 
niklasson bod ziemke editors proceedings icann th international conference artificial neural networks volume pages 
springer london 
sato yamada 
formulation learning vector quantization new misclassification measure 
ina jain venkatesh andb lovell editors proceedings 
th internat 
conf 
pattern recognition volume pages 
ieee computer society los alamitos ca usa 
sato yamada 
generalized learning vector quantization 
tesauro touretzky leen eds adv 
neural information processing systems vol 
pages 
mit press 
seo bode obermayer 
soft nearest prototype classification 
ieee transaction neural networks 
seo obermayer 
soft learning vector quantization 
neural computation 
kohonen 
self organizing maps learning vector quantization feature sequences 
neural processing letters 
villmann hammer 
supervised neural gas learning vector quantization 
kim martinetz editors proc 
th german workshop artificial life pages 
infix ios press berlin 
villmann mer nyi hammer 
neural maps remote sensing image analysis 
neural networks 
