propose family learning algorithms new form regularization allows exploit geometry marginal distribution 
focus semisupervised framework incorporates labeled unlabeled data generalpurpose learner 
transductive graph learning algorithms standard methods including support vector machines regularized squares obtained special cases 
utilize properties reproducing kernel hilbert spaces prove new representer theorems provide theoretical basis algorithms 
result contrast purely graph approaches obtain natural sample extension novel examples able handle transductive truly semi supervised settings 
experimental evidence suggesting semisupervised algorithms able unlabeled data effectively 
absence labeled examples framework gives rise regularized form spectral clustering sample extension 
problem learning labeled unlabeled data semi supervised transductive learning attracted considerable attention years cf 
:10.1.1.19.3957:10.1.1.20.9305
consider problem new framework data dependent regularization 
framework exploits geometry probability distribution generates data incorporates additional regularization term 
consider detail special case probability distribution supported manifold regularization mikhail belkin niyogi niyogi cs uchicago edu department computer science university chicago submanifold ambient space 
general framework propose specific families algorithms laplacian regularized squares laplacian support vector machines 
natural extensions rls svm respectively 
addition proposed transductive methods seen special cases general approach 
solution semi supervised case expressed expansion labeled unlabeled data points 
building solid theoretical foundation obtain natural solution problem sample extension see examples unlabeled obtain new regularized version spectral clustering 
general framework brings distinct concepts received independent attention machine learning regularization reproducing kernel hilbert spaces technology spectral graph theory geometric viewpoint manifold learning algorithms 
semi supervised learning framework recall standard statistical framework learning examples probability distribution training examples generated 
labeled examples pairs drawn unlabeled examples simply drawn marginal distribution hope knowledge marginal exploited better function learning classification regression tasks 
shows unlabeled data radically alter prior belief appropriate choice classification functions 
identifiable relation unlabeled data prior beliefs tween conditional knowledge 
specific assumption connection marginal conditional 
assume points close intrinsic geometry conditional distributions similar 
words conditional probability distribution varies smoothly geodesics intrinsic geometry utilize geometric ideas extend established framework function learning 
number popular algorithms svm ridge regression splines radial basis functions may broadly interpreted regularization algorithms different empirical cost functions complexity measures appropriately chosen reproducing kernel hilbert space rkhs 
mercer kernel associated rkhs functions corresponding norm set labeled examples standard framework estimates unknown function minimizing loss function squared loss rls soft margin loss func tion svm 
penalizing rkhs norm imposes smoothness conditions possible solutions 
classical representer theorem states solution minimization problem exists written problem reduced optimizing finite dimensional space coefficients algorithmic basis svm regularized squares regression classification schemes 
goal extend framework incorporating additional information geometric structure marginal ensure solution smooth respect ambient space marginal distribution achieve introduce additional regularizer reflect intrinsic structure controls complexity function ambient space controls complexity function intrinsic geometry setup prove representer theorem appropriate penalty term theorem 
assume penalty term sufficiently smooth respect rkhs norm solution optimization problem eqn exists admits representation support marginal proof theorem runs pages omitted lack space 
see details including exact statement smoothness conditions 
applications know attempt get empirical estimates note order get empirical esti mates sufficient unlabeled examples 
case particular interest support compact submanifold optimization problem case natural choice comes term may approximated basis labeled unlabeled data graph laplacian 
set labeled examples set unlabeled exam ples consider optimiza tion problem graph laplacian edge weights data adjacency graph 
diagonal matrix normalizing coefficient natural scale factor empirical estimate laplace operator 
sparse adjacency graph may replaced simple version representer theorem shows minimizer expansion terms labeled unlabeled examples key algorithms 
theorem 
minimizer optimization problem admits expansion terms labeled unlabeled examples 
proof variation standard orthogonality argument omit lack space 
remarks natural choices exist 
examples heat kernel ii iterated laplacian iii kernels geodesic coordinates 
kernels geodesic analogs similar kernels euclidean space 
note restricted denoted kernel defined associated rkhs functions suggest restricted reasonable choice turns minimizer corresponding optimization problem get yielding solution standard regularization different algorithms solutions optimization problem posed eqn 
fix notation assume labeled examples unlabeled examples interchangeably de note kernel function gram matrix 
laplacian regularized squares laplacian regularized squares algorithm solves eqn squared loss function solution form objective function reduced convex differentiable function dimensional expansion coefficient vector minimizer gram matrix labeled unlabeled points dimensional label vector diagonal matrix diagonal entries rest 
note eqn gives zero coefficients unlabeled data 
coefficients labeled data exactly standard rls 
laplacian support vector machines laplacian svms solve optimization problem eqn 
soft margin loss function defined slack variables standard lagrange multiplier techniques deriving svms arrive quadratic program dual variables subject contraints diagonal matrix gram matrix labeled unlabeled data data adjacency graph laplacian matrix labeled example 
obtain optimal expansion coefficient vector solve linear system solving quadratic program note svm qp eqns give zero expansion coefficients unlabeled data 
expansion coefficients labeled data matrix standard svm case 
laplacian svms easily implemented standard svm software packages solving linear systems 
manifold regularization algorithms connections table 
graph regularization label propagation see 
manifold regularization algorithms input labeled examples output estimated function unlabeled examples step construct data adjacency graph nodes nearest neighbors 
choose edge weights binary weights heat kernel weights step choose kernel function compute gram matrix step compute graph laplacian matrix diagonal matrix step choose step compute eqn squared loss laplacian rls eqns svm qp solver soft margin loss laplacian svm 
step output function connections algorithms manifold regularization standard regularization rls svm sample extension graph regularization rls svm sample extension label propagation rls svm hard margin rls svm related section survey various approaches semisupervised transductive learning highlight connections manifold regularization algorithms 
transductive svm tsvm optimization principle proposes joint optimization svm objective function binary valued labels unlabeled data functions rkhs 
parameters control relative hinge loss labeled unlabeled sets 
joint optimization implemented inductive svm label unlabeled data iteratively solving svm quadratic programs step switching labels improve objective function 
procedure susceptible local minima requires unknown possibly large number label switches converging 
note tsvm inspired transductive inference provide sample extension 
semi supervised svms vm vm incorporate unlabeled data including minimum choices labels unlabeled example 
formulated mixed integer program linear svms intractable large amounts unlabeled data 
presentation algorithm restricted linear case 
measure regularization conceptual framework closest approach 
authors consider gradient regularizer penalizes variations function high density regions low density regions leading optimization principle density marginal distribution authors observe straightforward find kernel arbitrary densities associated rkhs norm absence representer theorem authors propose perform minimization linear space generated span fixed set basis functions chosen apriori 
worth noting uses gradient ambient space penalty functional associated gradient submanifold 
situation data truly lies near submanifold difference significant smoothness normal direction data manifold irrelevant classification regression 
algorithm demonstrate formance improvements real world experiments 
graph approaches see variety graph methods proposed transductive inference 
methods provide sample extension 
nearest neighbor labeling test examples proposed unlabeled examples labeled transductive learning 
test points approximately represented linear combination training unlabeled points feature space induced kernel 
note sample extensions semisupervised learning 
graph regularization label propagation see 
manifold regularization provides natural sample extensions graph approaches 
connections summarized table page 
methods different paradigms unlabeled data include cotraining bayesian techniques 
experiments performed experiments synthetic dataset real world classification problems arising visual speech recognition 
comparisons inductive methods svm rls 
compare transductive svm survey related algorithms section 
experiments constructed adjacency graphs nearest neighbors 
software datasets experiments available manifold cs uchicago edu manifold regularization 
detailed experimental results 
synthetic data moons dataset moons dataset shown 
best decision surfaces wide range parameter settings shown svm transductive svm laplacian svm 
dataset contains ples labeled example class 
demonstrates tsvm fails find optimal solution 
laplacian svm decision boundary intuitively satisfying 
shows increasing intrinsic regularization allows effective unlabeled data constructing classifiers 
handwritten digit recognition exam set experiments applied laplacian svm laplacian algorithms binary classification problems arise pairwise classification handwritten digits 
images digit usps training set preprocessed pca dimensions taken form training set randomly labeled 
remaining images formed test set 
polynomial kernels degree set inductive methods experiments reported 
manifold regularization chose split weight ratio observations reported section hold consistently wide choice parameters 
compare error rates laplacian algorithms svm tsvm precision recall breakeven points roc curves averaged random choices labeled examples binary classification problems 
comments manifold regularization results significant improvements inductive classification rls svm compares significantly outperforms tsvm classification problems 
note tsvm solves multiple quadratic programs size labeled unlabeled sets solves single qp size labeled set followed linear system 
resulted substantially faster training times experiment 
scatter plots performance test unlabeled data sets confirm sample extension 
laplacian algorithms significantly stable respect choice labeled data inductive methods tsvm shown scatter plot standard deviation error rates 
plot performance function number labeled examples 
spoken letter recognition experiment performed isolet database letters english alphabet spoken isolation available uci machine learning repository 
data set contains utterances subjects spoke name letter english alphabet twice 
speakers grouped sets speakers referred isolet isolet 
purposes experiment chose train speakers isolet forming training set examples test isolet con taining examples utterance missing database due poor recording 
considered task classifying letters english alphabet experimental set meant simulate real world situation consid ered binary classification problems corresponding splits training data utterances moons dataset best decision surfaces rbf kernels svm tsvm laplacian svm 
labeled points shown color points unlabeled 
svm transductive svm laplacian svm moons dataset laplacian svm increasing intrinsic regularization 
svm laplacian svm laplacian svm usps experiment error rates precision recall breakeven points binary classification problems error rates test rls vs rls classification problems sample extension unlabeled error rates test svm vs svm classification problems sample extension unlabeled error rates svm tsvm std dev tsvm vs tsvm classification problems std deviation error rates std dev usps experiment mean error rate precision recall breakeven points function number labeled points test set unlabeled set average error rate rls vs rls rls number labeled examples average error rate svm vs svm svm number labeled examples isolet experiment error rates precision recall breakeven points binary classification problems error rate unlabeled set error rates test set rls vs rls labeled speaker rls vs rls labeled speaker speaker labeled rest left unlabeled 
test set composed entirely new speakers forming separate group isolet 
chose train rbf kernels width best value settings respect fold cross validation error rates fully supervised problem standard svm 
svm set best value settings respect mean error rates splits 
laplacian rls laplacian svm set compare algorithms 
comments significant performance improvements inductive methods tsvm predictions unlabeled speakers come group labeled speaker choices labeled error rates unlabeled set error rates test set svm vs tsvm vs svm tsvm labeled speaker svm vs tsvm vs svm tsvm labeled speaker speaker 
isolet comprises separate group speakers performance improvements smaller consistent choice labeled speaker 
expected appears systematic bias affects algorithms favor group speakers 
details see 
regularized spectral clustering data representation training examples unlabeled optimization problem framework expressed eqn reduces clustering objective function regularization parameter controls complexity clustering function 
avoid degenerate solutions need impose additional conditions cf 

easily seen version representer theorem holds minimizer form substituting back eqn 
come optimization problem moons dataset regularized clustering vector ones corresponding gram matrix 
letting projection subspace orthogonal obtains solution constrained quadratic problem generalized eigenvalue problem final solution eigenvector corresponding smallest eigenvalue 
framework clustering sketched provides regularized form spectral clustering controls smoothness resulting function ambient space 
obtain natural extension clustering points original data set 
shows results method dimensional clustering problem 
multiple eigenvectors system eqn 
obtain natural regularized sample extension laplacian eigenmaps 
leads new method dimensionality reduction data representation 
study approach direction research 
acknowledgments 
grateful marc steve smale peter bickel intellectual support nsf funding financial support 
acknowledge toyota technological institute support 
belkin niyogi manifold structure partially labeled classification nips 
belkin niyogi laplacian eigenmaps dimensionality reduction data representation neural computation june belkin niyogi regression regularization large graphs colt 
belkin niyogi manifold regularization geometric framework learning examples technical report univ chicago department computer science tr 
available www cs uchicago edu research publications techreports tr bennett semi supervised support vector machines nips bengio le roux efficient non parametric function induction semi supervised learning technical report university montreal 
blum chawla learning labeled unlabeled data graph icml 
blum mitchell combining labeled unlabeled data training colt bousquet chapelle hein measure regularization nips chapelle weston cluster kernels semi supervised learning nips 
joachims transductive inference text classification support vector machines icml 
smola kondor kernels regularization graphs colt kw 
burges vapnik extracting support data task kdd 
seeger learning labeled unlabeled data tech report 
edinburgh university martin szummer tommi jaakkola partially labeled classification markov random walks nips 
vapnik statistical learning theory wiley interscience 
zhou bousquet lal weston learning local global consistency nips 
zhu lafferty ghahramani semi supervised learning gaussian fields harmonic functions icml 
