ensemble classifier drifting concepts martin scholz ralf klinkenberg university dortmund dortmund germany scholz klinkenberg ls cs uni dortmund de www www ai cs uni dortmund de 
proposes boosting method train classifier ensemble data streams 
naturally adapts concept drift allows quantify drift terms base learners 
algorithm empirically shown outperform learning algorithms ignore concept drift 
performs worse advanced adaptive time window example selection strategies store data suited mining massive streams 
machine learning methods applied problems data collected extended period time 
real world applications introduces problem distribution underlying data change time 
knowledge discovery data mining time changing data streams concept drift handling data streams important topics machine learning community gained increased attention illustrated numerous conference tracks workshops topics specially dedicated journal issues see 
example companies collect increasing amount data sales figures customer data find patterns customer behavior predict sales 
customer behavior tends change time model underlying successful predictions adapted accordingly 
problem occurs information filtering adaptive classification documents respect particular user interest 
amount online information communication growing rapidly increasing need automatic information filtering 
information filtering techniques build personalized news filters learn news reading preferences user 
interest user concept underlying classification texts changes time 
filtering system able adapt concept changes 
machine learning approaches handling type concept drift shown outperform static approaches ignoring 
formalizing concept drift problem sec 
previous approaches handling reviewed sec 

sec 
discusses related ensemble methods data streams introduces boosting algorithm data streams naturally adapts concept drift 
sec 
approach evaluated real world datasets different simulated concept drift scenarios economic dataset exhibiting real concept drift 
concept drift problem definition study problem concept drift pattern recognition problem framework 
example consists feature vector label indicating classification 
data arrives time batches 
loss generality batches assumed equal size containing examples 
batch batch batch batch denotes th example batch batch data independently identically distributed respect distribution pi 
depending amount type concept drift example distribution pi pi batches differ 
learner aims sequentially predict labels batch minimize cumulated number prediction errors 
batch learner subset training examples batches predict labels batch 
related concept drift machine learning changing concepts handled time windows fixed adaptive size training data weighting data parts hypothesis age utility classification task 
approach weighting examples information filtering incremental relevance feedback approaches 
windows fixed size choice window size compromise fast adaptivity small window generalization phases concept change large window 
fixed window size strong assumptions quickly concept changes 
heuristics adapt different speed amount drift involve parameters difficult tune 
basic idea adaptive window management adjust window size current extent concept drift 
drifting concepts learned effectively efficiently little parameterization error minimization framework adaptive time windows example weighting selection 
framework support vector machines svms special properties allow efficient reliable error estimation single training run 
methods framework maintain adaptive time window training data select representative training examples weight training examples 
key idea automatically adjust window size example selection example weighting respectively estimated generalization error minimized 
approaches require complicated parameterization theoretically founded effective efficient practice 
adapting ensemble methods drifting streams ensemble methods data stream mining years algorithms specifically tailored mining data streams proposed 
apart able cope concept drift scalability important issues 
large datasets induction classifiers decision trees efficiently possible sampling strategy hoeffding bounds vfdt algorithm efficiently induces decision tree constant time 
combining trees ensemble classifiers techniques bagging boosting shown significantly increase predictive accuracy datasets 
corresponding variants data streams suggested :10.1.1.32.8889
sea algorithm induces ensemble decision trees data streams explicitly addresses concept drift 
splits data batches fits decision tree batch 
predict label base models combined unweighted majority vote similar bagging 
models discarded heuristic 
authors report increase classification performance compared single decision tree learner state ensemble recovers concept drifts 
recovery time approach unnecessarily long exchanges model iteration uses confidence weights 
theoretical analysis suggests weighted base learners preferable alternative domains concept drift 
analyzed algorithm steadily updates weights experts base models ensemble adds new expert time ensemble misclassifies example 
new experts start learn scratch weight reflects loss suffered ensemble current example 
sec 
extends efficient boosting procedure sec 
streams 
trades predictive performance versus scalability 
online algorithm reads examples aggregated batches decides batch add new expert ensemble 
sea base models ensemble combined weighted majority vote 
subsequent models trained reweighting examples new batch new base classifier model assigned weight depends performance performance remaining ensemble 
continuous re estimation adapts weights ensemble members concept drift 
ensemble generation knowledge sampling algorithm introduced sampling strategy suggested 
patterns discovered iteratively 
pattern iteration extends user prior knowledge 
iteration sampling procedure produces training sets orthogonal combined probability estimate corresponding prior knowledge 
aspect close boosting classifiers 
idea removing prior knowledge biased sampling formulated terms constraints 
formally step defines new distribution close original function possible orthogonal estimates produced available prior knowledge 
accounted iteration unexpected component model 
technically step realized introducing example weights 
kind prior knowledge base models yielded preceding iterations 
instance space nominal class attribute examples expected sampled initial distribution ir denote base model hypothesis space predicting class 
constraint new distribution constructed longer support knowledge encoded hypothesis predictions true label independent considering px px eq 
hold hypothesis allow derive information true label conditional distribution prediction 
second constraint probability observing specific class probability specific prediction change sufficient possible remove correlation true label predicted label px px px px eq 
ensures class skew change result implicitly altered cost model misclassifying examples 
eq 
avoids skew marginal distribution unnecessarily 
partition sharing predicted label true class new distribution defined proportionally initial having hypothesis prior knowledge instances partition indistinguishable 
changes conditional probabilities partition prefer instance despite equivalence respect available prior knowledge 
translates constraint px px constraints induce unique target distribution 
definition eases notation 
definition 
lift hypothesis predicted class true class label defined lift px px px similar precision lift measures correlation specific prediction specified true label 
value larger indicates positive correlation 

denote uniform distribution example set 
xm ym 
user number iterations call di find accurate model hi 
compute lift hi applying definition 
di xj yj di xj yj lift hi xj yj xj yj 
output 
hn lift values 
predict eq 

fig 
algorithm kbs knowledge sampling theorem 
initial distribution hypothesis constraints equivalent pd pd lift proof 
theorem defines new distribution sample hypothesis prior knowledge 
assuming single hypothesis restrictive possible directly incorporate new base model single ensemble classifier 
theorem applied iteratively base model hi selected distribution di 
distribution di defined applying theorem di hi 
corresponding algorithm kbs depicted fig 

boosts weak base learners empirically shown competitive adaboost logitboost 
inverse reweighting strategy allows approximately reconstruct original distribution combination single hypotheses 
formula estimates odds hn sequence hypotheses result separate iteration learning lift hi di hi lift hi di hi allows compute estimates conditional probabilities reweighting scheme kbs base classifiers rank models contribution accuracy kbs samples model accuracy overlapping correlated models respect new distribution reduced degree overlap 
constraint lift subset common prediction 
predictive accuracy linear combination corresponding lift values 
examples reweighted respect base classifier favors models independent contributions 
similar boosting approaches example weights anticipate expectation previously trained models 
especially useful handling smooth concept initialize empty ensemble 
stream 
read batch ek iteration 
predict ek current ensemble eq 


read true labels ek 

alternative ensemble exists compare accuracy wrt 
ek 
better ensemble discard worse ensemble 
discarded ek shrink cache 

initialize uniform distribution ek 


apply hi predictions ek 
recompute lift hi ek def 

update lifts hi 
di xj yj di xj yj lift xj yj 
call ek get new model 
compute lift def 


add model lifts ensemble 
batch ek alternative ensemble 

ek extend batch clone discard base model repeat steps ek fig 
algorithm kbs stream xj yj ek 
drifts sudden drifts require quick detection way rapidly adjust working hypothesis smooth drifts better collect information new target concept period time 
especially preceeding concept identified accurately point time drift starts removing knowledge current concept data learner focus new concept 
kbs strategy learn drifting concepts data streams original kbs algorithm fig 
assumes complete training set available main memory 
step adopt data streams read classify examples iteratively 
subsequent learning steps reweighting strategy kbs allows compute example weights efficiently 
data assumed arrive batches large train base classifiers 
sizes training sets effectively model determined dynamically algorithm 
processing new batch yields ensemble variants 
variant appends current batch batch training iteration refines latest base model accordingly 
second variant adds new model trained latest batch 
ensemble variant performing better batch kept 
strategy serves purposes 
stationary distributions new model trained empirical evidence increases accuracy resulting ensemble 
generally happen learning curve latest model leveled see data set suited boosting 
second sudden concept drift concept shift occurs estimation procedure instantly suggests add new model help overcome drift 
second step adopting kbs data streams foresee re computation phase base model performances updated respect current distribution 
fact believe main advantage weighted ensembles concept drift scenario 
stationary distributions weights vary marginally smoothly drifting scenarios systematically shifted allow quantify interpret drift terms previously patterns models 
sudden drifts pose problem automatically result radically reduced weights previously trained models high weights subsequently trained models parameters re estimated new data 
response time drifts short 
streaming variant kbs closely coupled accurate kbs boosting algorithm predictive performance expected outperform single base models datasets 
pruning ensembles efficiently addressed weight re computation model reach fixed minimum advantage random guessing latest batch discarded 
algorithm depicted fig 

loops stream ends 
lines apply current ensemble new batch knowing correct labels 
lines check continuing training latest model latest batch outperforms adding new model trained batch better ensembles kept 
lines recompute lift parameters base models 
models iteratively applied new batch weights adjusted similarly learning phase 
lines train variants ensemble extending batch updating newest model appropriately adds new model trained newest data 
degree freedom left line algorithm may classify new batch performance unknown time 
experiments variants implemented 
uses ensemble models trained larger batches generally reliable 
second variant uses hold set batch decide ensemble 
incremental base learners latest batch needs stored 
runtime dominated adjusting model data applying base models 
avoids memory requirements combinatorial explosion advanced techniques see sec 

pseudocode assume incremental base learner trains new models cached data 
incremental base learners cache required 
relevance topic scenario scenario scenario batch fig 
relevance topic concept concept change scenarios relevance second relevant topic concept relevance topic 
experiments experimental setup evaluation scheme order evaluate kbs learning approach drifting concepts compared adaptive time window approach batch selection strategy simple non adaptive data management approaches 
full memory learner generates classification model previously seen examples forget old examples 
memory learner induces hypothesis batch 
corresponds window fixed size batch 
window fixed size time window fixed size batches training data 
adaptive window window adjustment algorithm adapts window size current concept drift situation see sec 

batch selection batches producing error twice estimated error newest batch applied model learned newest batch receive weight selected final training set 
examples setting weights zero 
performance classifiers measured prediction error 
results reported sec 
averaged runs 
experiments conducted machine learning environment yale svm implementation learners weka toolbox 
evaluation simulated concept drift trec data experiments performed information filtering domain typical application area learning drifting concepts 
text documents represented full fixed adaptive batch kbs kbs memory memory size size selection stream hold 


table 
error time window example selection methods vs kbs 
attribute value vectors bag words model distinct word corresponds feature value ltc tf idf weight word document 
experiments subset documents data set text retrieval conference trec 
text assigned categories considered 
concept change scenarios simulated 
texts randomly split batches equal size containing documents 
scenarios document considered relevant certain point time matches interest simulated user time 
user interest changes topics documents remaining topics relevant 
fig 
shows probability relevant document category batch scenarios implies probability second relevant topic 
scenario concept shift second topic batch 
scenario user interest changes slowly batch batch 
scenario simulates abrupt concept shift user interest second topic batch back batch 
tab 
compares results static adaptive time window batch selection approaches scenarios terms prediction error variants kbs 
cases learning algorithm support vector machine linear kernel 
kbs algorithm manages adapt kinds concept drift 
tracking ensembles revealed distributions current model continuously refined 
concept shift scenario new model trained old model received significantly lower weight 
discarded helped identify topics irrelevant 
hold set helped identify better ensembles reliably classification time 
scenario models trained 
ensemble accurately adopted drift classification time systematic batch delay hold estimate misleading 
scenario full memory approach competitive batch selection scenario 
hold set kbs applies model iteration long concept shift 
delay increases error rate 
problem circumvented hold set 
essence kbs algorithm performed domain outperformed computationally expensive approaches 
scenario batch selection method clearly superior probably method able concatenate data second concept shift single training set 
adaboost kbs kbs fixed memory full memory full memory stream hold 


table 
averaged prediction errors satellite image dataset 
experiments satellite image data second set experiments satellite image dataset uci library real world dataset classification 
contains known drift time simulated drift technique described sec 
data set split batches random examples batch grey soil damp grey soil classes selected relevant 
drift scenarios sec 
simulated classes corresponded topics trec experiments 
decision trees typical base learner ensemble methods chose algorithm weka toolbox 
compared kbs nonadaptive fixed size batches full memory strategies 
additionally running stand tried run adaboost top 
learners default settings 
results listed table 
experiment results kbs improved hold set 
scenario tackled full memory approach exploited adaboost 
scenarios kbs better fixed size learner better full memory approach 
predicting phases business cycles third evaluation domain task economics real world data 
quarterly data describes west german business cycles 
examples described indicator variables 
task predict current phase business cycle 
accordance findings theis phases description business data described 
experiments compare performance kbs data stream algorithm previously reported results number batches 
timely order examples quarters preserved artificial concept drift simulated 
results evaluations shown tab 

column fixed time window approach lists results fixed size performed best 
fact approach performs may due cyclic nature domain 
size generally known advance shown fixed window sizes leads significant drops performance 
results batches shows kbs perform batch consists dozen examples 
reason possible get reliable probability estimates small data sets 
algorithm cache older data cases reasonable choose larger batch full fixed adaptive batch kbs kbs memory memory size size selection stream hold batches batches table 
prediction error business cycle data 
sizes 
just batches examples improves situation kbs performs similar fixed size adaptive size batch selection approach 
hold set turns surprisingly effective larger batches 
result provides evidence kbs able adapt classifier ensembles different kinds concept drift real world datasets 
presents new ensemble method learning data streams 
iteration base models induced reweighted continuously considering latest batch examples 
ensemble methods proposed strategy adapts quickly different kinds concept drift 
algorithm low computational costs 
empirically shown competitive outperform sophisticated adaptive window batch selection strategies 

kubat gama utgoff intelligent data analysis ida journal special issue incremental learning systems capable dealing concept drift vol 

aguilar ruiz cohen symposium applied computing sac special track data streams 

lang newsweeder learning filter netnews 
proc 
th int 
conf 
machine learning 
klinkenberg joachims detecting concept drift support vector machines 
proc 
th int 
conf 
machine learning 
klinkenberg predicting phases business cycles concept drift 
der gi workshop 
mitchell caruana freitag mcdermott zabowski experience learning personal assistant 
communications acm 
widmer kubat learning presence concept drift hidden contexts 
machine learning 
klinkenberg renz adaptive information filtering learning presence concept drifts 
workshop notes icml aaai workshop learning text categorization aaai press 
hulten spencer domingos mining time changing data streams 
th acm int 
conf 
knowledge discovery data mining 
taylor structural change classification 
workshop notes dynamically changing domains theory revision context dependence issues ecml 

allan incremental relevance feedback information filtering 
acm sigir conf 
research development information retrieval 
balabanovic adaptive web page recommendation service 
proc 
st int 
conf 
autonomous agents acm press 
klinkenberg ping concept drift importance examples 
franke renz eds text mining theoretical aspects applications 
physica verlag 
klinkenberg learning drifting concepts example selection vs example weighting 

joachims estimating generalization performance svm efficiently 
proc 
th int 
conf 
machine learning 
domingos hulten mining high speed data streams 
proc 
th acm sigkdd int 
conf 
knowledge discovery data mining 

breiman bagging predictors 
machine learning 
freund schapire decision theoretic generalization line learning application boosting 
journal computer system sciences 
russell online bagging boosting 
th int 
workshop artificial intelligence statistics 

lee clyde lossless online bayesian bagging 
journal machine learning research 
street kim streaming ensemble algorithm sea large scale classification 
proc 
th acm sigkdd int 
conf 
knowledge discovery data mining 

additive expert ensembles cope concept drift 
nd int 
conf 
machine learning 
scholz knowledge sampling subgroup discovery 
morik siebes eds local pattern detection 
springer 
rnkranz flach roc rule learning better understanding covering algorithms 
machine learning 
scholz sampling sequential subgroup mining 
proc 
th acm sigkdd int 
conf 
knowledge discovery databases 
scholz comparing knowledge sampling boosting 
technical report sfb universit dortmund germany 
john langley static versus dynamic sampling data mining 
second int 
conf 
knowledge discovery databases data mining 

fischer klinkenberg yale learning environment tutorial 
technical report ci collaborative research center university dortmund germany 
yale sf net 
ping manual 
universit dortmund lehrstuhl informatik viii 
www ai cs uni dortmund de software 
witten frank data mining practical machine learning tools techniques java implementations 
morgan kaufmann 
salton buckley term weighting approaches automatic text retrieval 
information processing management 
blake merz uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
theis clustering techniques detection business cycles 
sfb technical report universit dortmund germany 
morik multistrategy approach classification phases business cycles 
european conf 
machine learning 
