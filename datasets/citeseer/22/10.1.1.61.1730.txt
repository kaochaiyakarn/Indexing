pac style model learning labeled unlabeled data maria avrim blum computer science department carnegie mellon university avrim cs cmu edu 
growing interest practice unlabeled data labeled data machine learning number different approaches developed 
assumptions methods quite distinct captured standard theoretical models 
describe pac style framework model assumptions analyze sample complexity issues setting type data expect need order learn basic quantities numbers depend 
model viewed extension standard pac model addition concept class proposes type compatibility believes target concept underlying distribution 
view unlabeled data helpful allows estimate compatibility space hypotheses reduce size search space assumptions apriori reasonable respect distribution 
discuss number technical issues arise context provide sample complexity bounds uniform convergence cover algorithms 
consider algorithmic issues give efficient algorithm special case training 
substantial interest unlabeled data labeled data machine learning 
motivation unlabeled data cheaper plentiful labeled data useful information extracted reduces need labeled examples significant benefit 
number techniques developed doing experimental results variety different learning problems 
include label propagation word sense disambiguation training classifying web pages parsing improving visual detectors document classification transductive svm em text classification graph methods :10.1.1.1.5684:10.1.1.122.539:10.1.1.20.9305:10.1.1.20.9305
difficulty theoretical point view standard discriminative learning models really capture unlabeled data help 
particular pac model complete disconnect data distribution target function learned 
prior belief belongs class known fully function possible 
instance perfectly natural common talk problem learning concept class uniform distribution clearly case unlabeled data useless just generate 
learning unknown distribution standard pac setting unlabeled data help somewhat allowing distribution specific sample complexity bounds fully capture power unlabeled data practice 
generative model settings easily talk theoretically unlabeled data 
results typically strong assumptions essentially imply natural distinction unlabeled data distribution 
instance typical setting assume positive examples generated gaussian negative examples generated gaussian 
case unlabeled data recover gaussians need labeled data tell gaussian positive negative 
strong assumption real world settings 
model allow distribution data documents want classify number plausible distinctions want 
addition general framework model different uses unlabeled data 
goal provide pac style framework bridges positions captures ways unlabeled data typically 
extend pac model way allows express relationships hopes target function underlying distribution possess going far done generative models 
analyze issues setting type data expect need order learn give algorithmic results 
idea proposed model augment notion concept class notion compatibility target function data distribution 
talking learning concept class talk learning concept class compatibility notion furthermore require degree compatibility estimated finite sample 
specifically require function compatibility ex :10.1.1.61.1897
degree incompatibility think kind unlabeled error rate measures priori unreasonable believe proposed hypothesis 
example example margins suppose examples points class linear separators 
natural belief setting data separated target function separate positive negative examples reasonable margin castelli cover assume gaussians particular assume distributions distinguishable perspective issue 
:10.1.1.20.9305
case define farther distance hyperplane defined 
incompatibility probability mass distance 
define smooth function distance separator want commit specific advance 
contrast defining compatibility hypothesis largest probability mass exactly zero distance separator fit model written expectation individual examples distinguish zero exponentially close zero small sample 
example training training assume examples come pairs goal learn pair functions instance goal classify web pages represent words page words attached links pointing page pages 
hope underlies training parts example consistent allows training algorithm bootstrap unlabeled data 
case naturally define incompatibility hypothesis pr 
example linear separator graph cuts special case example suppose examples pairs points class linear separators believe points pair side target function training requiring 
define incompatibility probability mass examples 
thing problem interesting view examples edges view data graph embedded set labeled unlabeled data view objective finding linear separator minimum cut 
setup allows analyze ability finite unlabeled sample reduce need labeled data function compatibility target function various measures helpfulness distribution 
particular model find unlabeled data help distinct ways 
target function highly compatible unlabeled data estimate compatibility principle example iterative training uses small amount labeled data get initial information link words advisor points page page probably faculty member home page finds unlabeled example half confident link says advisor uses label example training hypothesis half 
motivating example consider problem word sense disambiguation text surrounding target word plant want determine dictionary definition intended tree factory 
yarowsky uses fact word appears twice document probably sense times :10.1.1.122.539
reduce size search space just estimated compatibility high 
providing estimate unlabeled data allow refined distribution specific notion hypothesis space size annealed vc entropy size smallest cover vc dimension 
fact natural cases find sense unlabeled data reduces size search space best described distribution specific measures 
distribution especially nice may find set compatible small cover elements cover far apart 
case assume target function fully compatible may able learn fewer labeled examples needed just verify hypothesis 
framework allows address issue unlabeled data expect need 
roughly vcdim form standard pac sample complexity bounds bound number unlabeled examples need 
technically set vc dimension care set defined complexity depends complexity complexity notion compatibility see section 
relationship luckiness framework 
strong connection approach luckiness framework :10.1.1.33.8995
cases idea define ordering hypotheses depends data hope lucky find functions compatible target 
main differences 
luckiness framework uses labeled data estimating compatibility learning difficult task result bounds labeled data significantly better 
instance example non degenerate distribution dataset pairs probability completely shattered fully compatible hypotheses luckiness framework help 
contrast larger unlabeled sample potentially reduce space compatible functions quite significantly depending distribution see section 
secondly luckiness framework talks compatibility hypothesis sample define compatibility respect distribution 
allows talk amount unlabeled data needed estimate true compatibility 
number differences technical level definitions 
outline results 
describing formal framework section give simplest version sample complexity bounds case finite hypothesis spaces 
section give uniform convergence bounds infinite hypothesis spaces 
achieve tighter bounds section consider cover size give bounds hold algorithms unlabeled data choose small set representative hypotheses compatible close choose representatives labeled data 
section give algorithmic results 
particularly simple illustration give main algorithmic result efficient algorithm learning linear separators training model just single labeled example assumption distribution satisfies independence label 
process simplify noisy halfspace learning algorithm somewhat 
formal framework assume examples labeled unlabeled come fixed unknown distribution instance space labeled unknown target function standard pac model concept class hypothesis space set functions instance space assumption realizable case target function belongs class hypothesis true error rate defined err prx 
hypotheses distance respect defined dd prx 
err denote empirical error rate labeled sample denote empirical distance unlabeled sample 
define notion compatibility mapping hypothesis distribution indicating compatible order estimable finite sample require compatibility expectation individual examples :10.1.1.61.1897
specifically define definition 
legal notion compatibility function overloading notation define ex :10.1.1.61.1897
sample define empirical average sample 
definition 
compatibility notion incompatibility 
call unlabeled error rate errunl clear context 
sample errunl denote empirical average need notation set functions incompatibility value 
definition 
threshold define cd errunl 
cd similarly sample define cs errunl finite hypothesis spaces illustrate unlabeled data suitable compatibility notion reduce need labeled examples 
case imagine general notions property 
finite hypothesis spaces measure size set functions just number functions 
standard pac model typically talks realizable case assume agnostic case 
setting additional issue unlabeled error rate priori assumption target function unlabeled error low aim occam style bound stream labeled examples halt sufficient justify hypothesis produced 
give bound doubly realizable case 
theorem 
see mu unlabeled examples ml labeled examples mu ln ln ml ln cd ln probability err errunl err 
proof 
notice probability hypothesis errunl errunl mu value mu 
union bound number unlabeled examples sufficient ensure probability hypotheses cd errunl 
number labeled examples similarly ensures probability true error empirical error yielding theorem 
target function perfectly correct compatible theorem gives sufficient conditions number examples needed ensure algorithm optimizes quantities observed data fact achieve pac guarantee 
emphasize say algorithm efficiently learns pair able achieve pac guarantee time sample sizes polynomial bounds theorem 
think theorem bounding number labeled examples need function helpfulness distribution respect notion compatibility 
context helpful distribution cd small need labeled data identify function 
get similar bound situation target function fully compatible theorem 
see mu unlabeled examples ml labeled examples mu ln ln ml ln cd ln probability err errunl err furthermore errunl errunl :10.1.1.61.1897
particular implies errunl err high probability optimizes err errunl err 
proof 
theorem apply hoeffding bounds unlabeled error rates 
give simple occam luckiness type bound setting 
sample define ln cs errunl 
description length nats sort hypotheses empirical compatibility output index ordering 
similarly define ln cd errunl 
upper bound description length sort hypotheses approximation true compatibility 
theorem 
set unlabeled data ml labeled examples probability satisfying err ml ln err 
furthermore ln ln probability satisfy 
point theorem algorithm observable quantities determine confident furthermore unlabeled data observable quantities worse learning slightly compatible function infinite size unlabeled sample 
infinite hypothesis spaces uniform convergence bounds reduce notation assume rest prx 
sample complexity results easily extended case :10.1.1.61.1897
infinite hypothesis spaces issue arises order achieve uniform convergence unlabeled error rates set complexity care define 
instance suppose examples just points line ha ha iff 
case vcdim 
imagine compatibility function ha depends complicated relationship real numbers case vcdim larger need unlabeled examples estimate compatibility second issue need appropriate measure size set surviving functions 
vc dimension tends choice instance consider case example margins data concentrated separated blobs set compatible separators large vc dimension entire class similar respect consider expected number splits sample size drawn logarithm annealed vc entropy exhibits better behavior 
specifically denote expected number splits points drawn concepts fixed denote uniform distribution expected number splits points drawn concepts get bound follows theorem 
unlabeled sample size cdim mu labeled sample size ml log log log log cd ml sufficient probability err errunl err furthermore errunl errunl 
analog theorem infinite case 
particular implies err errunl high probability optimizes err errunl err 
proof sketch standard vc bounds number unlabeled examples sufficient ensure probability estimate prx 
implies estimate unlabeled error rate errunl set hypotheses errunl contained cd 
bound number labeled examples follows shown expected number partitions maximum standard vc proof 
bound ensures probability functions cd true labeled error empirical labeled error 
give bound specify number labeled examples function unlabeled sample useful imagine learning algorithm performing calculations unlabeled data deciding labeled examples purchase 
theorem 
unlabeled sample size max cdim cdim log log sufficient label ml examples drawn uniformly random ml log log cs ml probability err errunl err 
furthermore errunl errunl 
proof 
standard vc bounds form theorem imply number labeled examples ml sufficient guarantee theorem err replaced err error respect replaced 
number unlabeled examples ensure probability err err 
combining statements yields theorem 
err errunl high probability optimizes err errunl err 
assume errunl cs cs 
notice case example worst case distributions essentially recover standard margin sample complexity bounds 
particular cs contains separators split margin greater maximum number ways splitting ml points margin 
distribution nice bounds better may fewer ways splitting margin 
instance case separated blobs discussed large just 
mention give versions bounds complexity measures rademacher averages 
cover bounds bounds previous section uniform convergence provide guarantees algorithm optimizes observed data 
section consider stronger bounds covers obtained algorithms behave specific way unlabeled examples choose representative set compatible hypotheses labeled sample choose 
bounds covers exist classical pac setting framework bounds algorithms type especially natural convenient 
recall set cover respect close prx 
illustrate produce stronger bounds imagine examples pairs points class linear separators compatibility determined points side separator case example 
suppose simplicity target function just splits hypercube coordinate distribution uniform pairs having coordinate target fully compatible 
hard show polynomially unlabeled examples log labeled examples high probability exist high error functions consistent compatible uniform convergence 
contrast cover size set functions compatible constant cover bounds allow learning just constant number labeled examples 
proof set variables appear positive example appear negative example draw variable chance belonging high probability size consider hypothesis corresponding conjunction variables correctly classifies examples whp classifies example negative example chance satisfying variable size means compatible consistent true error high 
theorem 
upper bound errunl size minimum cover cd mu unlabeled examples ml labeled examples cdim mu log log ml ln probability identify hypothesis close proof sketch unlabeled sample define follows labeling consistent choose hypothesis errunl smallest hypotheses corresponding labeling 
obtain eliminating hypotheses property errunl 
apply greedy procedure obtain gs follows initialize 
gi argmin errunl 

unlabeled data determine hi crossing hy property gi 
set increase goto 
bound mu sufficient ensure probability cover implies probability cover cd 
possible show probability cover cd size idea greedily creating cover respect distribution creating cover respect cover cd respect furthermore doing functions greedy cover procedure cd respect optimal cover cd 
learn labeled data empirical risk minimization 
standard bounds number labeled examples ensure probability empirical optimum hypothesis true error 
implies probability find hypothesis error 
interesting case unlabeled data helps substantially consider training setting target fully compatible satisfies independence label property 
shown boost weak hypothesis unlabeled data setting assuming labeled data produce weak hypothesis 
show unlabeled data fact learn just single labeled example 
specifically possible show concept classes theorem 
assume err errunl satisfies independence label 
mu unlabeled examples ml labeled examples find hypothesis probability error provided mu cdim cdim ln ln ml log 
particular reducing poly reduce number labeled examples needed ml 
fact argument extended case considered merely satisfy constant expansion 
section give efficient algorithm case class linear separators requires true independence label 
algorithmic results simple computational example give simple example illustrate bounds section give polynomial time algorithm takes advantage 
instance space vars set variables set class monotone disjunctions vars set variables suppose say example compatible function vars vars vars vars 
strong notion margin says essence variable positive indicator negative indicator example contain positive negative indicators 
setup give simple efficient learning algorithm pair 
unlabeled data construct graph vertices variable putting edge vertices example unlabeled sample vars 
labeled data label components 
target function fully compatible component get multiple labels get multiple labels halt failure 
produce hypothesis vars union positively labeled components 
fully compatible unlabeled data zero error labeled data theorem sizes datasets bounds high probability hypothesis produced error 
notice want view algorithm purchasing labeled data simply examine graph count number connected components request ln ln labeled examples 
cs 
proof high probability cd purchasing number labeled examples theorem statement 
interesting see difference helpful distribution problem 
especially non helpful distribution uniform distribution examples vars components 
case unlabeled data help needs labeled examples distribution non uniform vc dimension lower bounds 
hand helpful distribution high probability number components small case features appearing independently label 
training linear separators consider case training hypothesis class class linear separators 
simplicity focus case example target function linear separator example pair points assumed side separator example line segment cross target plane 
previous example natural approach try solve consistency problem set labeled unlabeled data goal find separator consistent labeled examples compatible unlabeled ones 
unfortunately consistency problem np hard graph embedded distinguished points np hard find linear separator cuts minimum number edges minimum 
reason additional assumption points example drawn independently label 
single distribution probability points drawn iid restricted positive side target function probability drawn iid restricted negative side target function 
blum mitchell positive algorithmic results training halves example drawn independently label assuming underlying function learnable statistical query algorithms true linear separators labeled data produce weakly useful hypothesis halves 
key contribution show run algorithm single labeled example 
process simplify results somewhat 
theorem 
polynomial time algorithm number bits example learn linear separator assumptions polynomially unlabeled examples single labeled example 
proof sketch assume convenience target separator passes origin denote separator 
assume convenience target function overwhelmingly positive overwhelmingly negative easy case arguments complicated 
define margin point distance separating plane equivalently cosine angle drawing large unlabeled sample xi xi denote sj set xi 
describe algorithm working fixed unlabeled sample just need apply standard vc dimension arguments get desired result 
step perform transformation weakly useful predictor hypothesis pr pr equivalent usual notion weak hypothesis target function balanced requires hypothesis give information target function unbalanced 
ensure reasonable poly fraction margin poly outlier removal lemma 
outlier removal lemma states algorithmically remove fraction ensure remainder vector maxx poly ex number bits needed describe input points 
reduce dimensionality necessary get rid vectors quantity zero 
determine linear transformation described transformed space unit length 
maximum bounded guarantees poly fraction points poly margin respect separating hyperplane 
avoid cumbersome notation rest discussion drop simply denote points separator transformed space 
distribution originally reasonable probability mass reasonable margin identity anyway 
second step argue random halfspace poly chance weak predictor 
uses perceptron algorithm get weak learning need simpler labeled data 
specifically consider point angle imagine draw random subject half property 

poly fraction points poly margin implies poly 
means poly probability mass functions fact predictors 
final step algorithm follows 
observation pick random plug bootstrapping theorem unlabeled pairs xi xi noisy label xi feeding result sq algorithm repeating process poly times 
high probability random weakly useful predictor steps low error hypothesis 
rest runs algorithm guarantees 
observe 
function small err small errunl 
secondly assumption independence label shown theorem functions low unlabeled error rate functions close close close positive function close negative function 
simply examine hypotheses produced procedure reader willing allow running time polynomial margin data set part argument needed 
exactly case generative models start 
pick low unlabeled error rate far positive negative functions close just draw single labeled example determine case 
easily extend algorithm standard training different follows repeat procedure symmetric way order find pair functions just try combinations pairs compatible functions find small unlabeled error rate close positive negative functions constant number labeled examples produce low error hypothesis part example functions pair 
provided pac style model incorporates labeled unlabeled data number sample complexity bounds 
intent model capture ways unlabeled data typically provide framework thinking unlabeled data help 
main implication analysis unlabeled data useful notion compatibility target function low unlabeled error rate distribution helpful sense hypotheses low unlabeled error rate unlabeled data estimate unlabeled error rates 
best cover bounds apply strategies unlabeled data select small set reasonable rules labeled data select algorithms section 
interesting consider relates algorithms original training algorithm labeled data unlabeled data bootstrap 
open problem generally better understand space efficient algorithms context 
particular positive algorithmic results fairly simple pairs difficult efficiently full unlabeled data additional assumptions distribution 
specific open problem exist efficient algorithms simple problem section allow irrelevant variables 
assume set variables partitioned groups positive example vars vars negative example vars vars allow 

santosh vempala number useful discussions 
supported part nsf iis ccr 

blum yang 
training expansion bridging theory practice 
nips 

itai 
learnability respect fixed distribution 
theoretical computer science 

blum chawla 
learning labeled unlabeled data graph 
proc 
icml pages 

blum frieze kannan vempala 
polynomial time algorithm learning noisy linear threshold functions 
algorithmica 

blum mitchell 
combining labeled unlabeled data cotraining 
proc 
th annual conf 
computational learning theory pages 

blumer ehrenfeucht haussler warmuth 
learnability vapnik chervonenkis dimension 
journal acm 

bousquet lugosi 
theory classification survey advances 
manuscript 

lugosi massart 
sharp concentration inequality applications 
random structures algorithms 

castelli cover 
exponential value labeled samples 
pattern recognition letters 

castelli cover 
relative value labeled unlabeled samples pattern recognition unknown mixing parameter 
ieee transactions information theory 

devroye lugosi 
probabilistic theory pattern recognition 
springer verlag 

vempala 
optimal outlier removal high dimensional spaces 
proceedings rd acm symposium theory computing 

ehrenfeucht haussler kearns valiant 
general lower bound number examples needed learning 
inf 
comput 


personal communication 

hwa osborne sarkar steedman 
corrected training statistical parsers 
icml workshop continuum labeled unlabeled data machine learning data mining washington 

joachims 
transductive inference text classification support vector machines 
proc 
icml pages 

levin viola freund 
unsupervised improvement visual detectors training 
proc 
th int 
conf 
computer vision pages 

nigam mccallum thrun mitchell 
text classification labeled unlabeled documents em 
mach 
learning 


park 
zhang 
trained support vector machines large scale unstructured document classification unlabeled data syntactic information 
information processing management 

shawe taylor bartlett williamson anthony 
structural risk minimization data dependent hierarchies 
ieee transactions information theory 

valiant 
theory learnable 
commun 
acm 

vapnik 
statistical learning theory 
john wiley sons 

yarowsky 
unsupervised word sense disambiguation rivaling supervised methods 
meeting association computational linguistics pages 

zhu ghahramani lafferty 
semi supervised learning gaussian fields harmonic functions 
proc 
icml pages 
