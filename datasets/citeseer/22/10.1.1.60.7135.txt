predicting probabilities supervised learning alexandru niculescu cs cornell edu rich caruana caruana cs cornell edu department computer science cornell university ithaca ny examine relationship predictions different learning algorithms true posterior probabilities 
show maximum margin methods boosted trees boosted stumps push probability mass away yielding characteristic sigmoid shaped distortion predicted probabilities 
models naive bayes unrealistic independence assumptions push probabilities 
models neural nets bagged trees biases predict calibrated probabilities 
experiment ways correcting biased probabilities predicted learning methods platt scaling isotonic regression 
qualitatively examine kinds distortions calibration methods suitable quantitatively examine data need effective 
empirical results show calibration boosted trees random forests svms predict best probabilities 

applications important predict calibrated probabilities accuracy area roc curve sufficient 
examines probabilities predicted supervised learning algorithms svms neural nets decision trees memory learning bagged trees random forests boosted trees boosted stumps naive bayes logistic regression 
show maximum margin methods svms boosted trees boosted stumps tend push predicted probabilities away 
hurts quality probabilities predict yields characteristic sigmoid shaped distortion predicted probabilities 
methods naive bayes opposite bias tend push predictions closer 
learning methods appearing proceedings nd international conference machine learning bonn germany 
copyright author owner 
bagged trees neural nets little bias predict calibrated probabilities 
examining distortion lack characteristic learning method experiment calibration methods correcting distortions 
platt scaling method transforming svm outputs posterior probabilities platt isotonic regression method zadrozny elkan calibrate predictions boosted naive bayes svm decision tree models platt scaling effective distortion predicted probabilities sigmoid shaped 
isotonic regression powerful calibration method correct monotonic distortion 
unfortunately extra power comes price 
learning curve analysis shows isotonic regression prone overfitting performs worse platt scaling data scarce 
examine probabilities predicted learning method method predictions calibrated 
experiments classification problems suggest random forests neural nets bagged decision trees best learning methods predicting calibrated probabilities prior calibration calibration best methods boosted trees random forests svms 

calibration methods section describe methods mapping model predictions posterior probabilities platt calibration isotonic regression 
unfortunately methods designed binary classification trivial extend multiclass problems 
way deal multiclass problems transform binary problems calibrate binary models recombine predictions zadrozny elkan 

platt calibration platt proposed transforming svm predictions posterior probabilities passing sigmoid 
see section sigmoid transformation justified boosted trees boosted stumps 
output learning method 
get calibrated probabilities pass output sigmoid predicting probabilities supervised learning exp af parameters fitted maximum likelihood estimation fitting training set fi yi 
gradient descent find solution argmin pi yi log pi pi exp afi questions arise sigmoid train set come 
avoid overfitting training set 
data set train model want calibrate introduce unwanted bias 
example model learns discriminate train set perfectly orders negative examples positive examples sigmoid transformation output just function 
need independent calibration set order get posterior probabilities 
draw back set model parameter selection 
avoid overfitting sigmoid train set model 
positive examples negative examples train set training example platt calibration uses target values respectively detailed treatment justification particular target values see platt 

isotonic regression sigmoid transformation works learning methods appropriate 
zadrozny elkan successfully general method isotonic regression robertson calibrate predictions svms naive bayes boosted naive bayes decision trees 
method general restriction mapping function isotonic monotonically increasing 
predictions fi model true targets yi basic assumption isotonic regression yi fi table 
pav algorithm algorithm 
pav algorithm estimating posterior probabilities uncalibrated model predictions 
input training set fi yi sorted fi initialize mi yi wi mk mi set wk wk wi set mk wk mk wi mi wk replace mk mi mk output stepwise const 
function mi fi fj isotonic monotonically increasing function 
train set fi yi isotonic regression problem finding isotonic function yi fi algorithm finds stepwise constant solution isotonic regression problem pair adjacent pav algorithm ayer table 
case platt calibration model training set xi yi get training set xi yi isotonic regression introduce unwanted bias 
independent validation set train isotonic function 

data sets compare algorithms binary classification problems 
adult cov type letter uci repository blake merz 
cov type converted binary problem treating largest class positive rest negative 
converted letter boolean ways 
letter treats letter positive remaining letters negative yielding unbalanced problem 
letter uses letters positives negatives yielding difficult balanced problem 
hs data set difficult class positive class 
slac problem stanford linear accelerator 
medis mg medical data sets 
data sets summarized table 
table 
description problems problem attr train size test size adult cov type letter letter medis mg slac hs 
qualitative analysis predictions section qualitatively examine calibration different learning algorithms 
algorithm variations parameter settings train different models 
example train models decision tree styles neural nets sizes svms kernels training apply platt scaling isotonic regression calibrate models 
model trained random sample cases calibrated independent samples cases 
figures section select problem learning algorithm model best calibration scaling 
real problems true conditional probabilities known model calibration visualized reliability diagrams degroot fienberg 
prediction space discretized bins 
cases predicted value fall bin second bin bin mean predicted value plotted true fraction positive cases 
model calibrated points fall near diagonal line 
examine predictions boosted trees 
shows histograms predicted values top row reliability diagrams middle bottom rows boosted trees test problems large test sets training calibration 
interesting aspect reliability plots display sigmoidal shape problems motivating sigmoid transform predictions calibrated probabilities 
reliability plots middle row show sigmoids fitted platt method 
reliability plots bottom show function fitted isotonic regression 
examining histograms predicted values top row note values predicted boosted trees lie central region predictions approaching 
exception letter highly skewed data set positive class 
problem predicted values approach careful examination histogram shows problem sharp drop number cases predicted probability near 
shifting predictions center histogram causes sigmoid shaped reliability plots boosted trees 
show calibration transforms predictions plot histograms reliability diagrams problems boosting overfits adult problem best performance achieved iterations boosting 
boosting allowed continue iterations display sigmoidal shape adult figures 
predicting probabilities supervised learning boosted trees platt calibration isotonic regression 
figures show calibration undoes shift probability mass caused boosting calibration cases predicted probabilities near 
reliability diagrams closer diagonal shape characteristic boosted tree predictions gone 
problem transforming predictions platt scaling isotonic regression yields significant improvement predicted probabilities leading lower squared error log loss 
difference isotonic regression platt scaling apparent histograms isotonic regression generates piecewise constant function histograms coarse histograms generated platt scaling smoother 
see niculescu caruana thorough analysis boosting point view predicting calibrated probabilities 
shows prediction histograms learning methods slac problem calibration calibration platt method 
reliability diagrams showing fitted functions platt method isotonic regression shown 
boosted stumps svms exhibit distinctive sigmoid shaped reliability plots second third rows respectively 
boosted stumps svms exhibit similar behavior problems 
case boosted trees sigmoidal shape reliability plots occurs concentration mass center histograms predicted values boosted stumps extreme 
interesting note learning methods exhibit behavior maximum margin methods 
sigmoid shaped reliability plot results predictions pushed away appears characteristic max margin methods 
shows histograms predicted values reliability plots neural nets tells different story 
reliability plots closely follow diagonal line indicating neural nets calibrated need post training calibration 
cov type problem appears benefit little calibration 
problems calibration methods appear striving approximate diagonal line task isn natural 
scaling hurt neural net calibration little 
sigmoids trained platt method trouble fitting tails properly effectively pushing predictions away seen histograms 
histograms uncalibrated neural nets look similar histograms boosted trees platt scaling giving confidence histograms reflect underlying struc min 
svm predictions scaled min max fraction positives fraction positives fraction positives fraction positives predicting probabilities supervised learning cov type adult letter letter medis slac hs mean predicted value mean predicted value mean predicted value mean predicted value mean predicted value mean predicted value mean predicted value 
histograms predicted values reliability diagrams boosted decision trees 
cov type adult letter letter medis slac hs mean predicted value mean predicted value mean predicted value mean predicted value mean predicted value mean predicted value mean predicted value 
histograms predicted values reliability diagrams boosted trees calibrated platt method 
cov type adult letter letter medis slac hs mean predicted value mean predicted value mean predicted value mean predicted value mean predicted value mean predicted value mean predicted value mg mean predicted value mg mean predicted value mg mean predicted value 
histograms predicted values reliability diagrams boosted trees calibrated isotonic regression 
ture problems 
example conclude letter hs problems available features defined classes small number cases gray region slac problem classes high overlap significant uncertainty cases 
interesting note neural networks single sigmoid output unit viewed linear classifier span hidden units sigmoid output predictions 
respect neural nets similar svms boosted trees calibrated platt method 
examining histograms reliability diagrams logistic regression bagged trees shows behave similar neural nets 
learning algorithms calibrated initially post calibration help problems 
bagged trees helped little post calibration medis letter problems 
surprising logistic regression pre fraction positives fraction positives fraction positives fraction positives fraction positives fraction positives fraction positives predicting probabilities supervised learning cov type adult letter letter medis slac hs mean predicted value mean predicted value mean predicted value mean predicted value mean predicted value mean predicted value 
histograms predicted values reliability diagrams neural nets 
mean predicted value cov type adult letter letter medis slac hs mean predicted value mean predicted value mean predicted value mean predicted value mean predicted value mean predicted value mean predicted value 
histograms predicted values reliability diagrams neural nets calibrated platt method 
dicts calibrated probabilities interesting bagging decision trees yields calibrated models 
bagged trees calibrated deduce regular decision trees calibrated average sense decision trees trained different samples data predictions averaged average calibrated 
unfortunately single decision tree high variance variance affects calibration 
platt scaling able deal high variance isotonic regression help fix problems created variance 
rows show histograms calibration reliability diagrams logistic regression bagged trees decision trees slac problem 
random forests clear cut 
rf models calibrated problems poorly calibrated letter calibrated hs cov type medis letter 
interesting problems rfs exhibit lesser extent behavior max margin methods predicted values slightly pushed middle histogram reliability plots show sigmoidal shape mg mean predicted value mg mean predicted value accentuated letter problems cov type medis hs 
methods bagging random forests average predictions base set models difficulty making predictions near variance underlying base models bias predictions near zero away values 
predictions restricted interval errors caused variance tend sided near zero 
example model predict case way bagging achieve bagged trees predict zero 
add noise trees bagging averaging noise cause trees predict values larger case moving average prediction bagged ensemble away 
observe effect strongly random forests base level trees trained random forests relatively high variance due feature subsetting 
post calibration help mitigate problem 
naive bayes unrealistic assumption attributes conditionally independent class tends push predicted values 
opposite behavior max margin methods cre fraction positives fraction positives fraction positives bst dt bst stmp svm ann bag dt dt rf knn nb 
histograms reliability diagrams slac 
ates reliability plots inverted sigmoid shape 
platt scaling helping improve calibration clear sigmoid right transformation calibrate naive bayes models 
isotonic regression better choice calibrate models 
predicting probabilities supervised learning returning see histograms predicted values calibration column different models display wide variation 
max margin methods svm boosted trees boosted stumps predicted values center histograms causing sigmoidal shape reliability plots 
platt scaling isotonic regression effective fitting sigmoidal shape 
calibration prediction histograms extend tails near predicted values 
methods calibrated neural nets bagged trees random forests logistic regression calibration platt scaling moves probability mass away 
clear looking reliability diagrams methods sigmoid difficulty fitting predictions tails calibrated methods 
examines probability histograms calibration clear histograms similar platt scaling 
calibration significantly reduces differences probabilities predicted different models 
course calibration unable fully correct predictions inferior models decision trees naive bayes 

learning curve analysis section learning curve analysis calibration methods platt scaling isotonic regression 
goal determine effective calibration methods amount data available calibration varies 
analysis models section vary size calibration set cases cases factors 
measure calibration performance examine squared error models 
plots show average squared error test problems 
problem perform trials 
error bars shown plots narrow may difficult see 
calibration learning curves shown learning methods decision trees left 
nearly horizontal lines graphs show squared error prior calibration 
lines perfectly horizontal test sets change data moved calibration sets 
plot shows squared error calibration platt method isotonic regression size calibration set varies small large 
calibration set small cases platt scaling outperforms isotonic regression learning methods 
happens isotonic regression constrained bst dt predicting probabilities supervised learning unscaled platt isotonic rf calibration set size calibration set size calibration set size bag dt svm knn unscaled platt isotonic unscaled platt isotonic unscaled platt isotonic calibration set size calibration set size calibration set size bst stmp nb unscaled platt isotonic unscaled platt isotonic unscaled platt isotonic calibration set size unscaled platt isotonic calibration set size ann unscaled platt isotonic calibration set size 
learning curves platt scaling isotonic regression averages problems 
platt scaling easier overfit calibration set small 
platt method overfitting control built see section 
size calibration set increases learning curves platt scaling isotonic regression join cross 
points calibration set isotonic regression yields performance better platt scaling 
learning methods calibrated predictions neural nets bagged trees logistic regression platt scaling isotonic regression yields improvement performance calibration set large 
methods calibration beneficial hurts performance calibration sets small 
max margin methods boosted trees boosted stumps svms calibration provides improvement calibration set small 
section saw sigmoid match boosted trees boosted stumps svms 
expected methods platt scaling performs better isotonic regression small medium sized calibration cases virtually indistinguishable larger calibration sets 
expected calibration improves performance naive bayes models calibration set sizes isotonic regression outperforming platt scaling data 
rest models knn rf dt shown post calibration helps calibration sets large 

empirical comparison learning algorithm train different models different parameter settings calibrate squared error log loss bst dt bst dt uncalibrated isotonic regression platt scaling svm rf uncalibrated isotonic regression platt scaling rf svm ann bag dt bag dt ann predicting probabilities supervised learning knn knn bst stmp bst stmp 
performance learning algorithms dt dt model isotonic regression platt scaling 
models trained samples calibrated independent samples 
data set learning algorithm calibration method select model best performance points calibration report performance large final test set 
shows squared error top log loss bottom learning method calibration 
bar averages trials problems 
error bars representing standard deviation means shown 
probabilities predicted learning methods boosted trees svms boosted stumps naive bayes dramatically improved calibration 
calibration help bagged hurts neural nets 
calibration best models random forests bagged trees neural nets 
calibration boosted trees random forests svms predict best probabilities 

examined probabilities predicted different learning methods 
maximum margin methods boosting svms yield characteristic distortions predictions 
methods naive nb nb bayes predictions opposite distortion 
methods neural nets bagged trees predict probabilities 
examined effectiveness platt scaling isotonic regression calibrating predictions different learning methods 
platt scaling effective data small isotonic regression powerful sufficient data prevent overfitting 
calibration models predict best probabilities boosted trees random forests svms uncalibrated bagged trees uncalibrated neural nets 
acknowledgments zadrozny elkan isotonic regression code young stanford linear accelerator slac data goddard space center help indian pines data 
supported nsf award 
ayer brunk ewing reid silverman 

empirical distribution function sampling incomplete information 
annals mathematical statistics 
blake merz 

uci repository machine learning databases 
degroot fienberg 

comparison evaluation forecasters 
statistician 
johnson 

support vector machine classifiers applied data 
proc 
eighth jpl airborne geoscience workshop 
niculescu caruana 

obtaining calibrated probabilities boosting 
proc 
th conference uncertainty artificial intelligence uai 
press 
platt 

probabilistic outputs support vector machines comparison regularized likelihood methods 
advances large margin classifiers pp 

robertson wright 

order restricted statistical inference 
new york john wiley sons 
zadrozny elkan 

obtaining calibrated probability estimates decision trees naive bayesian classifiers 
icml pp 

zadrozny elkan 

transforming classifier scores accurate multiclass probability estimates 
kdd pp 

