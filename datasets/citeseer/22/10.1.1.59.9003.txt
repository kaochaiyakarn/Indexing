neural network model inter problem adaptive online time allocation rgen schmidhuber idsia lugano switzerland tu munich 
nchen germany juergen idsia ch 
aim meta learning techniques minimize time needed problem solving effort parameter hand tuning automating algorithm selection 
predictive model algorithm performance needed task requires long training times 
address problem online fashion running multiple algorithms parallel sequence tasks continually updating relative priorities neural model maps current state expected time solution 
model updated task actual performance algorithm 
censored sampling allows train model effectively need additional exploration task solution 
preliminary experiment new inter problem technique learns outperform previously proposed intra problem heuristic 
problem statement typical machine learning scenario involves possibly inexperienced practitioner trying cope set problems solved principle element set available algorithms 
users solve dilemmas trial error blindly applying rule thumb steadily growing area meta learning research devoted automating process 
apart notable exceptions see adopt notation terminology commented bibliography existing techniques amount selection single candidate solver algorithm recommendation small subset available algorithms run parallel priority algorithm portfolio selection 
approach usually requires long training phase prohibitive algorithms hand computationally expensive assumes algorithm runtimes predicted offline problem features exhibit large fluctuations 
complex cases difficulty problems precisely predicted priori robust approach run candidate solvers parallel adapting priorities online actual performance 
termed adaptive online time allocation distinguish intra problem prediction algorithm performance heuristic priori knowledge algorithm behavior inter problem time allocation strategy learned collecting experience sequence tasks 
inter problem approach training parametric model algorithm runtimes give example model allocate time online comparing performance simple intra problem heuristic 
parametric model inter problem consider finite algorithm set containing algorithms ai applied solution problem running time allocation procedure 
ti time spent ai xi feature vector possibly including information current problem algorithm ai kind values parameters current state di hi hi set collected samples pairs ihi historic experience set relative entire order allocate machine time efficiently map pair hi time left ai reaches solution 
allowed learn mapping solving sequence related tasks successful algorithm ai solved problem time hi posteriori evaluate correct hi pair hi 
tentative experiment led poor results values targets learn regression pairs residual time values 
main problem approach values choose targets unsuccessful algorithms 
assigning heuristically penalize high values algorithms stopped point solving task give incorrectly low values algorithms solve obtaining exact targets running algorithms increase overhead 
alternative inspired censored sampling lifetime distribution estimation consists learning parametric model xi ti conditional probability density function pdf residual time 
see model trained imagine continue time allocation algorithm solves current task having successful algorithms ai indices hi correct targets evaluated 
assuming outcome independent experi ment including ease notation unknown pdf write likelihood hi li hi hi unsuccessful algorithms final time value hi recorded hi lower bound unknown possibly infinite time solve problem obtain likelihood integrate hi hi conditional cumulative distribution func tion cdf corresponding search value maximizes hi bayesian approach maximize posterior 
note cases logarithm quantities maximized terms dropped 
prevent overfitting force model realistic shape known parametric lifetime model weibull distribution pdf express dependency parameters 
example outputs feed forward neural network trained backpropagation minimizing negative logarithm derivatives easily obtainable fashion commonly modelling conditional distributions see par 
time allocation perspective advantage approach allows learn unsuccessful algorithms suffering trade accuracy learned model time spent learning 
example application estimated model correct time allocation task trivial allocate resources expected fastest algorithm lower expected run time periodically re checking algorithm selected current states xi 
practice predictive power model depends current task compares ones solved far trusting completely risky 
preliminary experiments adopted time allocation technique similar slicing machine time small intervals sharing elements distribution pa pi updated step current model re trained task history collected far follows problem solved update current current xi xi update pa pi run ai time pi update xi update update maximizing model extreme value distribution logarithms time values parameters outputs feedforward neural network separate hidden layers units weights obtained minimizing negative logarithm bayesian posterior obtained sect 
current history validation set cauchy distribution prior 
cycle time allocation current expected time solution evaluated ai xi values ranked ascending order current time slice allocated proportionally log log ri ri current rank ai total number tasks index current task 
way distribution time uniform task model untrained tends task sequence sharing pattern expected fastest solver gets half current time slice second gets quarter 
ran preliminary tests algorithm set set simple generational genetic algorithms differing population size mutation rate genome length crossover operator uniform point rate cases 
applied solvers sequence artificial deceptive problems trap described consisting copies bit trap function bit block bitstring length nm gives fitness contribution bits bits 
generated sequence different problems varying genome length size deceptive block 
problems sorted genome length block size resulting sequence roughly sorted difficulty see table 
feature vector included problem features genome length block size algorithm parameters current best average fitness values variation current trend time spent increment total inputs 
compared inter problem intra problem competitive heuristically estimated simple linear extrapolation learning curve 
show significant improvement greatly reduces computation time respect brute force approach 
purpose show parametric model algorithm performance learned allocate time efficiently requiring long training phase 
model system able learn bits priori knowledge pre wire intra problem example fact increases average fitness indicator potentially performance 
weibull distributed log extreme value distribution parameters log 
distribution logarithm residual times learn common model set tasks solution times different orders magnitude 
cumulative time fitness func 
evals unknown best nn inter intra brute force task sequence fig 

comparison method labeled nn inter sequence tasks 
shown performances priori unknown different problem random seed fastest solver set performance ideal foresight labeled unknown best estimated performance brute force approach running algorithms parallel solves problem labeled brute force leaves completes task sequence time cumulative time spent sequence tasks total time spent solving current previous tasks plotted current task index 
time measured fitness function evaluations values shown upper confidence limits calculated runs 
table 
trap problems listed block size number blocks sequence tasks model gradually reliable nn able outperform 
spite size network obtained model accurate due variety algorithms behavior different tasks discriminative rank algorithms expected runtimes 
neural network replaced parametric model learning algorithm gradient descent plan test complex mixture model order obtain accurate predictions better performances 
obtained model continuous give predictions starting algorithms ti principle adapt algorithm set current task guiding choice set promising points parameter space 

supported snf 

perspective view survey meta learning 
artif 
intell 
rev 
schmidhuber zhao wiering shifting inductive bias success story algorithm adaptive levin search incremental self improvement 
machine learning simple principles metalearning 
tr idsia 

lobo parameter genetic algorithm 
banzhaf daida eiben garzon honavar smith eds proceedings genetic evolutionary computation conference 
volume orlando florida usa morgan kaufmann 
lagoudakis littman algorithm selection reinforcement learning 
proc 
th international conf 
machine learning morgan kaufmann san francisco ca 
horvitz ruan gomes kautz selman chickering bayesian approach tackling hard computational problems 
uai proceedings th conference uncertainty artificial intelligence san francisco ca usa morgan kaufmann publishers 

schmidhuber adaptive online time allocation search algorithms 
esposito pedreschi eds machine learning ecml 
proceedings th european conference machine learning pisa italy september springer extended tech 
report available athttp www idsia ch idsia ps gz 

rnkranz brazdil soares fast subsampling estimates algorithm recommendation 
technical report tr artificial intelligence wien 
gomes selman algorithm portfolios 
artificial intelligence 
nelson applied life data analysis 
john wiley new york 
bishop neural networks pattern recognition 
oxford university press 
holland adaptation natural artificial systems 
university michigan press ann arbor 
jacobs jordan nowlan hinton adaptive mixtures local experts 
neural computation 
