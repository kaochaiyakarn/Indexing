discrete component analysis wray buntine helsinki institute information technology hiit dept computer science pl university helsinki finland wray buntine hiit fi department knowledge technologies jozef stefan institute ljubljana slovenia acm org 
article presents unified theory analysis components discrete data compares methods techniques independent component analysis non negative matrix factorisation latent dirichlet allocation 
main families algorithms discussed variational approximation gibbs sampling rao blackwellised gibbs sampling 
applications voting records united states senate reuters newswire collection 
principal component analysis pca key method statistical engineering toolbox 
century old different ways 
pca known nen lo transform hotelling transform image analysis variation latent semantic analysis lsa text analysis ddl 
kind eigen analysis manipulates data matrix 
usually applied measurements real valued data feature extraction data summarization 
lsa perform centering step subtracting mean data vector prior eigen analysis word counts document preserve matrix sparseness convert word counts real valued tf idf 
general approach data reduction 
independent component analysis ica see ways extension general approach involves estimation called latent unobservable variables 
kind estimation follows major statistical methodology deals general unsupervised methods clustering factor analysis 
general approach called latent structure analysis tit half century old 
data modelled way admits unobservable variables influence observable variables 
statistical inference reconstruct unobservable variables data jointly general characteristics unobservable variables 
theory particular assumptions model method may arrive poor results 
relatively statistical computing machine learning community aware seemingly similar approaches discrete observed data appears names 
best known community probabilistic latent semantic indexing plsi hof non negative matrix factorisation nmf ls latent dirichlet allocation lda 
variations discussed section 
refer methods jointly discrete component analysis dca article provides unifying model 
approaches assume data formed individual observations documents individuals images observation described number variables words genes pixels 
approaches attempt summarize explain similarities observations correlations variables inferring latent variables observation associating latent variables observed variables 
methods applied social sciences demographics medical informatics genotype inference text image analysis information retrieval 
far largest body applied area citation indexes genotype inference due structure program psd 
growing body text classification topic modelling see gs bpt language modelling information retrieval see bj 
guide argued section methods apply pca ica data discrete 
section unified theory analysis components discrete data compare methods related techniques section 
main families algorithms discussed section variational approximation gibbs sampling rao blackwellised gibbs sampling 
applications section voting records united states senate components subsequent classification 
views dca interpretation dca methods way approximating large sparse discrete matrices 
suppose documents different words 
document page dr cat hat sequence words 
fast went net 
said net bet bet bet net get things put bag words representation word order lost 
yields list words counts brackets bet fast get net said things went word appears original include representation zeros suppressed 
sparse vector represented vector full word space zeroes counts making non zero entries appropriate places 
matrix rows vectors non negative integers dominated zeros called large sparse discrete matrix 
bag words basic representation information retrieval 
alternative sequence words 
dca representation models act word order effects introduced incremental algorithms 
detail precise subsequent sections 
section argue various perspectives large sparse discrete data suited standard pca ica methods 
issues pca pca normally applied numerical data individual instances vectors real numbers 
practical datasets vectors integers non negative counts binary values 
example particular word negative number appearances document 
vote senator take values voting 
transform variables real numbers tf idf linear weighting affect shape distribution 
respect modelling count data linguistic applications dunning warning dun statistics assumption normal distribution invalid cases statistical text analysis enormous corpora analysis restricted common words ones interest 
fact typically ignored field 
invalid methods may seriously overestimate significance relatively rare events 
parametric statistical analysis binomial multinomial distribution extends applicability statistical methods smaller texts models normal distributions shows promise early applications method 
pca considered method gaussians justified gaussian distributions row tb 
pca justified squares distance measure properties gaussians follow distance measure 
rare events correspond points far away norm 
fundamentally different kinds large sample approximating distributions dominate discrete statistics poisson gaussian 
instance large sample binomial approximated poisson distribution integers rate events occur distribution total number events counted 
probability small gaussian ros 
illustrates showing gaussian poisson approximations binomial sample size different proportions 
plots done probability log scale errors low probability values highlighted 
clearly see problem gaussian provides rea poisson gaussian approximations binomial binomial poisson gaussian samples approximate medium values proportion small values severely underestimates low probabilities 
low probability events occur model distorted 
image analysis analogue digital converters data counts gaussian errors assumed poisson counts small 
dca avoids gaussian modelling data poisson multinomial directly 
critique general style pca comes psychology literature time justification dca gs 
griffiths steyvers argue squares distance pca methods lsa novel scale subject suggestion similarity relates distance psychological space long history shepard 
critics argued human similarity judgments satisfy properties euclidean distances symmetry triangle inequality 
tversky hutchinson pointed euclidean geometry places strong constraints number points particular point nearest neighbor sets stimuli violate constraints 
considered power law arguments pca violates associated words 
component analysis approximation data reduction approach pca seeks reduce dimensional data vector smaller dimensional vector 
done approximating full data matrix product smaller matrices representing reduced vectors called component factor score matrix representing data independent part called component factor loading matrix shown 
pca squares theory approximation eliminating lower order eigenvectors contributing components 
documents words components words jc kc 



wi wi wi li li data matrix score matrix loading matrix fig 

matrix approximation view documents words components matrix left entries matrices right entries 
represents simplification view dca methods seeking goal case matrices sparse discrete 
applying pca large sparse discrete matrices lsa word count data interpretation components desired difficult goal original method ddl 
negative values appear component matrices interpreted typical documents usual sense 
applies kinds sparse discrete data low intensity images astronomical images verb noun data language models introduced ptl instance 
cost function minimized plays important role 
dca places constraints approximating score matrix loading matrix non negative 
uses entropy distance squares distance 
components independent components independent component analysis ica developed alternative pca 
hyv nen oja ho argue pca methods merely find uncorrelated components 
ica developed way representing multivariate data truly independent components 
theory pca approximates data gaussian tb practice rarely basic formulation dimensional data vector linear invertible function independent components represented dimensional latent vector square invertible matrix 
note ica assumes notation 
plays role loading matrix 
univariate density model independent components distributed lk get likelihood formula equality fast ica algorithm ho interpreted maximum likelihood approach model likelihood formula 
sparse discrete case formulation breaks simple reason zeros equation hold discrete gradient algorithms ica justified 
get practice applying ica documents word counts turned tf idf scores 
arrive formulation suited discrete data relax equality ica expectation independent components robust relationship data score vector 
correspondence ica dca noted bj 
expectation relationship dimension dimension rectangular matrix 
basic model models number viewpoints bj 
general model 
notation words bags documents kinds data representations apply 
statistical terminology word observed variable document data vector list observed variables representing instance 
machine learning terminology word feature bag data vector document instance 
notice bag collects words change coordinates det document loses ordering 
bag represented data vector dimensional 
latent hidden unobserved vector called component scores dimensional 
term component topic factor cluster 
parameter matrix previously mentioned component loading matrix point convenient introduce symbology 
symbols summarised table introduced go 
number documents subscript indicate document dropped number different words size dictionary number components number words document number words collection vector word counts document row totals entries wj vector component counts document column totals matrix word counts component dimension entries vj vector component scores document entries lk normalised entries mk vector sequential component assignments words document entries kl 
component loading matrix dimension entries component loading vector component column dimensional parameter vectors component priors table 
summary major symbols bags sequences words 
document represented sequence words bag bagged form bag words represented vector counts 
simplest case multinomial sample size vocabulary size model bag alternatively independent discrete distributions outcomes model xl 
bag corresponds sequence order lost pj wj 
wj 
different sequences map bag likelihoods simple models differ just combinatoric term 
note likelihood methods maximum likelihood bayesian methods fitting methods instance cross validation technique likelihood black box function 
take values discrete distribution multivariate form bernoulli index sampled dimensional probability vector 
derivatives interact likelihood 
combinatoric term mapping bag sequence representations ignored safely affect fitting parameters methods irrelevant data treated bag sequence 
general property multinomial data 
consider bag words article theory applies equally sequence words representation implementation easily address cases little change algorithms just data handling routines 
general dca general formulation introduced section unsupervised version linear model applies bag words expected value mean data dot product component loading matrix latent component scores full probability bayesian modelling required give distribution non deterministic values model including model parameters latent variables 
likelihood modelling cb required give distribution data values model including observed latent variables 
core methodologies computational statistics extend 
distribution data called likelihood methodologies 
likelihood document primary way evaluating probabilistic model 
likelihood strictly probability classical statistics interpret probability probabilistic model generate document 
hand way determining document usual unusual documents low likelihood considered outliers anomalies 
trust documents low likelihoods indicate problems model 
trust model low likelihood indicates problems document 
complete formulation dca need give distributions matching constraint equation specify likelihood 
distributions needed sequence bag distributed mean formed component loading matrix advanced fitting methods gibbs sampling treat likelihood black box 
introduce latent variables expands functional form likelihood may update parts document turn 
ordering effects incurred bagging document updates different parts data done different order 
combinatoric term mapping bag sequence representations ignored algorithms effectively ordering affects 
component scores distributed full probability modelling component loading matrix distributed apriori parameters 
formulation equation called model statistical literature psd 
contrast mixture model uses related constraint latent variable representing single latent component unobserved corresponds making weighted sum probability distributions model families section introduces forms dca specific distributions sequence bag component scores fundamental model gamma poisson model gp model short 
models variations 
probability document model case latent variables known right hand side case latent variables included left hand side 
gamma poisson model general gamma poisson form dca introduced gap considered detail document data supplied form word counts 
word count word type wj 
total count wj 
document component scores indicate amount component document 
latent unobserved 
entries lk independent gamma distributed lk gamma 
affects scaling components changes shape distribution shown 
component loading matrix size entries controls partition features component 
matrix column component normalised features meaning 
column represents proportions words features component conventions gamma vary 
parameter 
conven tion revealed equation 
probability rate lk alpha alpha alpha alpha alpha alpha fig 

gamma distribution different values observed data poisson distributed wj poisson parameters gamma distributions give dimensional parameter vectors 
initially vectors treated constants estimation subject 
bayesian full probability modelling prior needed 
dirichlet prior th component prior parameters 
practice jeffreys prior 
dirichlet strong justification conjugate jeffreys prior minimax properties cb robust 
hidden latent variables component scores model parameters gamma parameters component loading matrix 
denote model gp standing gamma poisson 
full likelihood document gp composed parts 
part comes independent gamma distributions lk second part comes independent poisson distributions parameters lk likelihood likelihood exp klk lk wj exp lk wj 
conditional gamma poisson model practice fitting parameters gp dm model case go small 
situation component scores lk negligible say normalised 
maintaining negligible values allow component scores zero finite probability 
conditional gamma poisson model denoted cgp short introduces capability 
retrospect cgp sparse gp additional parameter component encourage sparsity 
cgp model extends gamma poisson model making lk zero 
general case lk independent zero probability gamma distributed probability lk gamma 
denote model cgp standing conditional gamma poisson full likelihood cgp 
full likelihood document modifying equation replaces term inside dirichlet multinomial model exp klk lk dirichlet multinomial form dca introduced mpca 
case normalised latent variables total word count modelled 
multinomial argument multinomial total count second argument vector probabilities 
denote model dm full likelihood dm 
full likelihood document wj mk choose 
wj 
model derived gamma poisson model shown section 
multivariate version variation methods allow grouping count data 
words grouped separate variable sets 
groups title words body words topics web page analysis nouns verbs adjectives text analysis 
groups treated separate discrete distributions 
possible word types document partitioned groups 
bg 
total word counts group denoted lg bg wj 
vector split vectors wg wj bg wj matrix normalised group row multivariate version dca created group wg multinomial lg mk bg bg fitting modelling methods variation related lda mpca considered detail 
advantage different kinds words multinomial distribution different kinds ignored 
version demonstrated subsequently senate voting records multinomial single vote particular senator 
related sections begins relating main approaches placing context exponential family models brief history 
correspondences various published cases dca represented terms format table 
multinomial total count possible outcomes bagged version discrete distributions possible outcomes 
table na indicates aspect model required specified methodology 
note nmf cost function table 
previously published models name bagged components nmf ls poisson na plsi hof discrete na lda discrete dirichlet mpca bun multinomial dirichlet gap poisson gamma formulation avoided defining likelihood models 
shown cost function corresponds gamma poisson parameters zero 
lda multinomial mpca replaced sequence discrete distributions choose term drops section 
plsi related lda lacks prior distribution model latent variables full probability theory weighted likelihood method hof 
plsi non bayesian version lda weighted likelihood method means accounts fitting principled manner 
lda mpca close relationship gap called gp 
parameter treated known estimated data parameter vector value aposteriori independent 
context lda mpca gap equivalent models ignoring representational issues 
lemma 
gamma poisson model section parameter constant vector entries model equivalent dirichlet multinomial model section mk lk lk addition poisson gamma proof 
consider gamma poisson model 
sum wj poisson variables distribution poisson parameter sum means 
sum poisson variables known set poisson variables multinomial distribution conditioned sum total count ros 
poisson distributions equivalent poisson lk multinomial lk parameter constant mk lk lk distributed lk distributed independently gamma 
second distribution represented multinomial note lk poisson gamma distribution produce poisson gamma distribution bs 
estimated data gap presence observed influence estimates 
case lda mpca longer effectively equivalent gap 
note canny recommends fixing estimating data 
complete set correspondences note section proven nmf corresponds maximum likelihood version gap corresponds maximum likelihood version lda mpca plsi 
notes exponential family general dca model section called exponential family distributions expected value referred dual parameter usually parameter know best 
bernoulli probability dual parameter poisson rate dual parameter gaussian mean dual parameter mean 
formulation interpreted letting exponential family dual parameter 
formulation generalises pca way linear model mn generalises linear regression 
note alternative cds exponential family distribution natural parameters 
bernoulli probability natural parameter log poisson rate natural parameter log gaussian mean natural parameter mean 
formulation generalises pca way generalised linear model mn generalises linear regression 
historical notes independent groups statistical computing machine learning community contributed development dca family methods 
original research includes grade membership gom wm probabilistic latent semantic indexing plsi hof non negative matrix factorisation nmf ls genotype inference psd latent dirichlet allocation lda gamma poisson models gap 
modifications algorithms explored multinomial pca mpca bun multiple aspect modelling ml 
clear large scale model poisson form comes ls multinomial form hof psd 
clear expression problem latent variable problem psd 
relationship lda plsi nmf poisson version lda pointed bun proven gg 
connections ica come bj 
general gamma poisson formulation final generalisation line 
related techniques statistical community traced back latent class analysis developed rich theory developed relating methods correspondence analysis statistical techniques 
component assignments words standard mixture models document collection assigned latent component 
dca family models interpreted making word document assigned latent component 
see introduce latent vector represents component assignments different words 
section done bag components sequence components representation effective change occurs basic models algorithms derived 
expand term parts treating result latent variable 
introduce dimensional discrete latent vector total count word count 
count ck gives number words document appearing th component 
posterior mean diagnostic interpretable result 
document sports news football words german words sports fan words general vocabulary words 
latent vector derived larger latent matrix size entries vj row totals wj observed data column totals ck 
vectors word appearance counts component appearance counts respectively summing rows columns matrix shown 
words components kc 
vj vj vj ck fig 

representation document contingency table 
latent matrix changes forms likelihoods development analysis algorithms easier 
section catalogues likelihood formula discussing algorithms 
gamma poisson model new latent matrix distributions underlying gamma poisson model lk gamma ck poisson lk wj vj vj multinomial ck joint likelihood document gp derived quantities represented rearrangement lck wj exp lk vj vj 
note marginalised yielding ck ck vj vj posterior mean lk ck 
ck poisson gamma 
conditional gamma poisson model likelihood follows gp case probability lk ck 
joint likelihood cgp rearrangement lck lk ck exp lk vj note marginalised yielding ck ck ck vj vj vj vj pulled constraint vj ck 
posterior mean lk ck 
dirichlet multinomial model dirichlet multinomial model similar reconstruction applies ck multinomial wj vj vj multinomial ck joint likelihood dm rearrangement 

marginalised yielding 
ck vj vj ck vj vj 

algorithms developing algorithm standard approach match optimization algorithm functional form likelihood 
bayesian statistical methodology basic approach usually step inner loop sophisticated computational statistics 
likelihoods yield easily standard em analysis 
see consider forms likelihood single document gp model consider probability latent variable observed data gp 
em analysis needs able compute 
log 
different forms likelihood seen far depending latent variables kept left hand side probability gp equation term lk wj means known simple posterior distribution gp equation term ck links latent variables prevents simple evaluation el vj required expected log probability 
gp equation term ck ck vj means known simple posterior distribution produce em algorithm separately updating turn mean formula guarantee convergence maximum posterior likelihood value apply 
spirit earlier authors point em principles apply em terminology em methods apply observed exponential family problem variational approximation algorithm kullback leibler divergence corresponds extension em algorithm gb bun 
variational approach covered 
algorithms problem follow general approaches statistical computing community 
basic approaches variational approximation gibbs sampling rao blackwellised gibbs sampling 
maximum likelihood algorithm viewed simplification algorithms 
variational approximation kullback leibler divergence approximate method applied sequential variant dirichlet multinomial version problem 
fuller treatment variational methods exponential family gb bun 
likelihood gp treated em methods latent variable leaving observed 
approach factored posterior approximation latent variables gp ql qv approximation find expectations part optimization step 
em algorithm results equality holds 
functional form approximation derived inspection recursive functional forms see bun equation ql exp qv log qv exp ql log important computation convergence approach lower bound individual document log probabilities 
naturally falls computation see bun equation 
approximation defined proportions bound log log ql qv variational approximation applies gamma poisson version dirichlet multinomial version 
gamma poisson model looking recursive functionals equation likelihood equation follows ql independent gammas component qv independent multinomials word 
general case approximation lk gamma ak bk vj 
multinomial wj nj 
uses approximation parameters ak bk gamma normalised nj multinomial 
parameters form vectors matrix respectively 
approximate posterior takes form ql qv 
approximating distributions looking recursive functionals equation extract rewrite rules parameters nj exp log lk zj ak bk log lk elk lk ak bk log lk ak log bk zj exp log lk ln digamma function defined dx available scientific libraries 
equations form step major cycle performed document 
second step re estimate model parameters posterior approximation maximising expectation log full posterior probability el ql qv log incorporates equation document prior th column model item section 
denote intermediate variables nj th document adding subscript likewise 
log probability formulas yield linear terms normalising constraints gets 
lower bound log probability equation simplification rewrites equation log log wj 
ak ak ak log lk wj log zj 
variational approximation algorithm gamma poisson version summarised 
equivalent algorithm produced words sequentially bagged 

initialise document 
uniform initialisation ak note stored 

document equations recompute update place 
concurrently compute log probability bound equation add running total 
concurrently maintain sufficient statistics total wj nj documents 
store cycle discard 
update equation normalising appropriately 

report total log probability bound repeat starting step 
fig 

variational algorithm gamma poisson complexity step uses words appearing document full step sk time complexity number words full collection 
step jk time complexity 
space complexity ik store intermediate parameters document jk store statistics 
implementation step document quite slow document word data stored disk streamed main memory complexity jk ik terms disk 
documents small instance documents sentences phrases apply 
correspondence nmf precursor gap model non negative matrix factorisation nmf ls matrix approximation paradigm kullback leibler divergence 
algorithm converted notation follows kl kl notice solution indeterminate factor multiply divide solution holds 
loss generality normalised 
lemma 
nmf equations returned normalised occur maxima gamma poisson likelihood gp 
proof 
see proven 
take solution nmf equations divide factor multiply lk factor 
equivalent solution rewrite rules lk lk wj kp klk wj lk klk kept normalised equations hold maxima likelihood gp 
left equation corresponds maxima note hessian easily shown negative indefinite right em equations likelihood 

show equivalence nmf equations prove forward direction 
take scaled solution nmf 
nmf equation lk equivalent equation lk lemma 
take nmf equation separately normalise sides 
lk term drops left equation lemma 
prove backward direction 
sufficient show nmf equations hold solution rewrite rules lemma normalised 
nmf equation lk clearly holds 
assuming rewrite rules lemma hold lk wj klk lk wj klk lk wj klk lk klk lk wj klk lk second equation nmf holds 
reorder sum apply rewrite rule note including latent variable likelihood dealing em methods achieve correct maximum likelihood solution expression gp 
practice common approximate method handling latent variable problems lead readily fitting 
dirichlet multinomial model variational approximation takes related form 
approximate posterior dirichlet vj 
multinomial wj nj 
yields style update equations equations nj exp log mk zj ak log mk mk mk log mk ak zj exp log mk equation 
lower bound individual document log probabilities log dm takes form log log ak ak log mk ak wj log zj correspondence equation readily seen 
algorithm dirichlet multinomial version related 
equations replace equations equation replaces equation initialisation ak jeffreys prior 
direct gibbs sampling styles gibbs sampling apply dca 
basic gibbs sampling proposed stephens donnelly psd 
gibbs sampling conceptually simple method 
unobserved variable problem resampled turn conditional distribution 
compute posterior distribution conditioned variables sample new value variable posterior 
instance ordering problem 

low level sampling section known distributions gamma multinomial available standard scientific libraries 
ak develop approach gamma poisson look full posterior product individual document likelihoods prior model item section 
constant terms dropped 
lc exp lk vj 
conditional distributions gibbs sampling proportional 
conditional distribution gp 
isolating terms just see conditionally gamma distributed 
likewise multinomial distributed dirichlet distributed models similar 
additional effort required arrange parameters sequencing efficient memory 
major differentiator gibbs sampling resampling latent component vector sampling schemes version table 
care required conditional gamma poisson 
ck sampling lk needs decide zero case nonzero case 
uses equation decision resorts equation non zero 
model sampling gp lk gamma ck 
cgp ck conditional gamma poisson rate pk pk pk gamma 
ck revert gamma poisson case 
dm dirichlet ck 
table 
sampling components direct gibbs single document direct gibbs algorithm general case 
gibbs scheme turns correspond variational approximation excepting sampling done maximisation expectation 
log probability words accumulated step 
terms latent variables represent reasonably unbiased estimate likelihoods 
gp rao blackwellised gibbs sampling rao blackwellisation gibbs sampling cr combines closed form updates variables gibbs sampling 
process called marginalisation 
document retrieve store sample latent component variables normalised counterpart table 
word document positive count wj component counts vector equation equation vj 
multinomial wj lk lk ff alternatively sequence components version component word sampled turn corresponding bernoulli distribution 
concurrently accumulate log probability gp cgp dm 
concurrently maintain sufficient statistics total vj documents 
store cycle discard 

dirichlet prior rows having accumulated counts document sufficient statistics posterior rows dirichlet 
sample 

report total log probability report 
fig 

major cycle gibbs algorithm dca variable elimination 
feasible lead significant improvements general case dca 
griffiths steyvers gs introduced algorithm lda easily extends gamma poisson model conditional variant little change sampling routines 
approach step consider full posterior probability see variables marginalised introducing computational complexity sampling 
gp model look posterior equation 
equations shows marginalised 
likewise marginalised instance dirichlet 
yields gamma poisson posterior 
gp constants dropped ck vj 
vj ck shown short sampling routine 
similar formula applies conditional gp case equation marginalisation term equation likewise similar formula applies dirichlet multinomial version equation ck vj 
term form jk ck drops ck known constant 
posterior distributions marginalised models gp cgp dm gibbs sampling scheme needs developed 
set vj 
sums wj forms functions equations quite nasty 
way mess convert scheme bag words model implicit sequence words model 
proceeds follows 
run words document update corresponding component assignment word 
component assignments th document dimensional vector entry takes value 
suppose th word word index jl 
step change counts 
increased decreased keeping total constant 
instance word originally component updating gibbs sampling decrease increase 
words document document 
word th document sample component assignment kl posterior kl assignments fixed 
posterior proportional denominator convenient constant sequential gp jl sequential gp jl notation sequential added right hand side combinatoric terms 
equation need dropped 
formula simplifies dramatically derived sampling schemes table 
subscript dropped assumed counts jl word index word component index resampled 
kl sampled dimensional probability vector needed 
table gives unnormalised form 
rao blackwellised gibbs algorithm 
approximately unbiased log probability recorded step 
requires value 
sufficient statistics supply current mean estimate true sampled quantity 
alternative method sample major cycle 
implementation notes due rao blackwellisation effectively re estimated sampling step model sampling proportionality gp cgp dm vj ck ck ck proportionality gp case vj ck vj ck ck 
table 
sampling kl jl rao blackwellised gibbs 
maintain sufficient statistics vj sufficient statistics component proportions 

document retrieve component assignments word recompute statistics ck vj individual component assignment word 
word word index jl component assignment kl document resample component assignment word marginalised likelihoods section 
decrement vj ck remove component assignment word 
ii 
sample kl proportionally table 
iii 
increment vj ck 
concurrently record log probability gp appropriate model 
concurrently update sufficient statistics 
fig 

major cycle rao blackwellised gibbs algorithm dca full pass documents 
effective early stages explains superiority method observed practice 
means storage slot needed store sufficient statistics direct gibbs slots needed current value plus sufficient statistics 
represents major saving memory 
sampled stage process sufficient statistics totals appearing formula gibbs estimates mcmc process 
historical notes previous algorithms placed context 
nmf straight maximum likelihood ls expressed terms kullback leibler divergence minimization optimisation jointly applies latent variables see section 
plsi annealed maximum likelihood hof best viewed terms clustering precursor hb various gibbs gibbs sampling turn full probability distribution psd gibbs sampling equivalently component assignments words sequence words representation gs lda variational approximation kullback leibler divergence significant speed 
expectation propagation ml requires ks latent variables stored prohibitive expense compared ki algorithms 
covered 
aspects estimation number algorithms needed put models regular 
component parameters treatment far assumed parameter vectors 
usual estimate parameters rest estimation tasks done 
feasible parameters shared data component vectors 
estimating number components number components usually constant assumed priori 
may helpful treat parameter random variable adapts data 
popular terms find right number components practice theory thing exist 
obtain best fitting employ cross validation assess evidence marginal likelihood model particular choice cc bj 
particular evidence posterior probability data choice parameters integrated 
new data typical model requires performing inference related particular document 
suppose instance wished estimate snippet text query matches document 
document components summarised latent variables 
new query represented gp matching quantity ideally 
unknown average 
various methods proposed ml bj 
alternative components hierarchical components suggested bj way organising large flat component space 
instance wikipedia half documents easily support discovery components 
dirichlet processes developed alternative dimensional component priors discrete model implementation effect dimensional large delete low performing components 
applications section briefly discusses applications methods 
voting data type political science data roll calls 
roll calls senate year 
vote senator recorded ways voting 
outcome roll call positive bill passed confirmed corresponding negative resolution rejected veto sustained 
outcome vote interpreted st senator associating positive outcomes negative outcomes 
application method map roll call data dca framework 
senator form words wx implies voted wx implies voted 
roll call interpreted document containing single occurrence available words 
pair words wx wx treated binomial multivariate formulation section 
priors jeffreys priors regular gibbs sampling 
special purpose models normally interpreting roll call data political science postulate model rational decision making 
senator modelled position ideal point continuous spatial model preferences 
example dimension delineates liberal conservative preference second region social issues preference 
proximities ideal points explain positive correlations votes 
ideal points senator obtained optimization instance optimal classification algorithm poo bayesian modelling 
spatial models dca interprets correlations votes membership similar 
correspond latent component variables 
course speak probability particular senator member particular bloc 
corresponding probability vector normalized assures senator member bloc average 
outcome vote member interpret membership measure influential particular bloc latent senator bloc seen casting votes roll call 
model behavior latent roll calls record behavior 
turn model membership senator particular bloc assumed constant 
related family approaches modelling relations networks blocks groups 
roll call described network individual nodes network pair nodes connected agreed 
discrete latent variables try explain existence links entities terms membership blocks hll sn 
authors prefer block model approach modelling roll call data wmm 
membership block high probability block agreements explain agreements 
bloc seen having opinion issue block explicitly 
authors extended model topics membership senator particular block depends topic issue agreement depends discussed 
topic associated words appear description issue 
visualization analyze aspects dca model applied roll call data examine membership examine actions individual issues 
approach visualization similar visualizing set probability vectors 
gray scale mirror probabilities ranging white black 
mentioned choice number 
number nuisance variable model distinctly difficult show fixed obtain negative logarithms base model likelihood 
see overwhelmingly selected far worse 
means model best describe roll call votes existence 
fewer capture nuances yield reliable probability estimates amount data 
models valid extent 
just single visualization pick best individual 
illustrate membership 
senator represented vertical bar squares indicate membership 
arranged left right binary pca approach del 
ordering attempts sort extreme moderate extreme 
shows democrat 
dca trying capture aspects text relevant distinguishing class documents 
assume classification task distinguish newspaper articles politics 
dca modelling distribution word equally important modelling distribution highly pertinent word classifying news reports 
course single example sub optimality 
methods reducing dimensionality text disregard classification problem hand compare respect classification performance 
classification experiments observe component democrat majority 
strongest component quite outcome 
component moderate democrats distinctly influential democrats majority 
component small group republican 
component republican majority influential bloc 
component republican minority influential 
component tends slightly extreme component average components clearly unambiguously sorted 
fig 

component membership ri collins outcome mccain az pa campbell gregg nh smith oh ak oh hutchison tx coleman mn fitzgerald il nv stevens ak warner va nh graham sc ne ks roberts ks nm bond mo talent mo ia nc pa hatch ut bennett ut ms allen va ga burns mt tn ok cochran ms alexander tn mcconnell ky sessions ky tx id az craig id wy ok thomas wy fig 

component membership democrats boxer ca ia md levin mi leahy vt reed ri nj clinton ny durbin il kennedy ma md nj mi kerry ma ny edwards nc murray wa graham fl hi wv wa dodd ct reid nv byrd wv sd kohl wi lieberman ct johnson sd wi inouye hi dayton mn nm nd feinstein ca nelson fl de sc conrad nd pryor ar jeffords vt de lincoln ar la mt la nelson ne outcome miller ga democrats mpca tested role feature construction tool common pca ica classification tool 
newsgroups collection described previously reuters collection employed svm light joa classifier default settings 
classification added class distinct multinomial cf 
section training data left empty test data predicted class value 
note performance accuracy svm joa clear winner 
interesting see mpca compares 
component seen generating number words document 
number component generated words plays role classification number lexemes document ordinary classification 
cases employed tf idf transformed word word counts feature values 
svm works sparse data matrices assumed component document number words component generated 
components yield classification performance competitive svm label distinguished role fitting 
may add component words default bag words hoping conjunctions words inherent component help improve classification performance 
reuters collection modapte split 
frequent categories performed binary classification 
results disclosed table major change observed adding components original set words 
performing classification components results inferior large number components 
fact components results worse components probably fitting 
regardless number components svm performance words reproduced component generated words collection 
classifying newsgroup articles categories proved successful 
employed replications fold cross validation achieved classification accuracy additional mpca components svm 
comparing confusion matrices frequent mistakes caused svm mpca svm predicting talk politics misc sci crypt errors talk religion misc predicted sci electron errors 
hand components helped better identify alt atheism talk politics misc misclassified talk religion misc fewer errors earlier 
talk politics misc talk religion misc misclassified talk politics gun fewer errors 
components successful resulting classification accuracy 
increasing number components classification accuracy gradually increases 
components needed general purpose classification 
reuters distribution test collection available david lewis professional home page currently www research att com lewis numbers percentages indicates precision recall 
table 
svm classification results svm svm mpca cat acc 
acc 
earn acq grain crude trade mpca comp 
mpca comp 
cat acc 
acc 
earn acq grain crude trade experiments conclude components may help tightly coupled categories require conjunctions words newsgroups keyword identifiable categories reuters 
judging ideas jb components help cases appearance words informative sum informativeness individual appearances word appearance word implies appearance word appear document 
article unifying framework various approaches discrete component analysis presenting model closely related ica suited sparse discrete data 
shown relationships existing approaches nmf plsi lda mpca gap 
instance nmf normalised results corresponds approximate maximum likelihood method lda gap general family models 
different algorithms available different cases gamma poisson conditional gamma poisson allowing sparse component scores dirichlet multinomial 
extends number algorithms previous developed mpca lda general gamma poisson model 
experiments mpca software show typical ghz desktop machine build models days hundreds gigabytes text 
www org models share similarities pca ica useful range feature engineering tasks machine learning pattern recognition 
rich literature extending model variety directions 
caused surprising performance algorithms availability general gibbs sampling algorithms allow sophisticated modelling 
acknowledgments wray buntine supported academy finland prose project finnish technology development center search ina box project ist programme european community superpeer semantic search engine ist ip pascal network excellence ist 
supported research agency ist programme european community sekt semantically enabled knowledge technologies ist ip 
mpca software experiments developed number authors reported code website 
experiments supported document processing environment test collections group information retrieval software sami 
authors acknowledge helpful comments referees 
girolami van 
investigating relationship language model perplexity ir precision recall measures 
sigir proceedings th annual international acm sigir conference research development retrieval pages 
bj buntine 
applying discrete pca data analysis 
uai banff canada 
bingham kab girolami 
topic identification dynamical text complexity pursuit 
neural process 
lett 
blei ng jordan 
latent dirichlet allocation 
journal machine learning research 
bpt buntine 
discrete pca web pages 
workshop statistical approaches web mining 
ecml 
bs bernardo smith 
bayesian theory 
john wiley chichester 
bun buntine 
variational extensions em multinomial pca 
th european conference machine learning ecml helsinki finland 
baeza yates ribeiro neto 
modern information retrieval 
addison wesley 
canny 
gap factor model discrete data 
sigir pages 
cb casella berger 
statistical inference 
wadsworth brooks cole belmont ca 
cb clarke barron 
jeffrey prior asymptotically favorable entropy risk 
journal statistical planning inference 
cc carlin chib 
bayesian model choice mcmc 
journal royal statistical society 
cds collins dasgupta schapire 
generalization principal component analysis exponential family 
nips 
clinton rivers 
statistical analysis roll call voting unified approach 
american political science review 
cr casella robert 
rao sampling schemes 
biometrika 
ddl deerwester dumais landauer furnas harshman 
indexing latent semantic analysis 
journal american society information science 
del de leeuw 
principal component analysis binary data applications roll call analysis 
technical report ucla department statistics 
dun dunning 
accurate methods statistics surprise coincidence 
computational linguistics 
gb ghahramani beal 
propagation algorithms variational bayesian learning 
nips pages 
gelman carlin stern rubin 
bayesian data analysis 
chapman hall 
gg gaussier goutte 
relation plsa nmf implications 
sigir proceedings th annual international acm sigir conference research development information retrieval pages 
acm press 
gs griffiths steyvers 
probabilistic approach semantic representation 
proc 
th annual conference cognitive science society 
gs griffiths steyvers 
finding scientific topics 
pnas colloquium 
hb hofmann buhmann 
pairwise data clustering deterministic annealing 
ieee transactions pattern analysis machine intelligence 
hyv rinen karhunen oja 
independent component analysis 
john wiley sons 
hll holland laskey 
stochastic steps 
social networks 
ho hyv rinen oja 
independent component analysis algorithms applications 
neural netw 
hof hofmann 
probabilistic latent semantic indexing 
research development information retrieval pages 
jb bratko 
analyzing attribute dependencies 
lavra blockeel todorovski editors pkdd volume lnai pages 
springer verlag september 
joa joachims 
text categorization support vector machines learning relevant features 
claire dellec line rouveirol editors proceedings ecml th european conference machine learning number pages chemnitz de 
springer verlag heidelberg de 
joa joachims 
making large scale svm learning practical 
sch lkopf burges smola editors advances kernel methods support vector learning 
mit press 
ls lee seung 
learning parts objects non negative matrix factorization 
nature 
lewis rose li 
rcv new benchmark collection text categorization research 
journal machine learning research 
mardia kent bibby 
multivariate analysis 
academic press 
ml minka lafferty 
expectation propagation generative aspect model 
uai edmonton 
mn mccullagh nelder 
generalized linear models 
chapman hall london second edition 
poo poole 
non parametric unfolding binary choice data 
political analysis 
psd stephens donnelly 
inference population structure genotype data 
genetics 
ptl pereira tishby lee 
distributional clustering english words 
proceedings acl june 
ros ross 
probability models 
academic press fourth edition 
row roweis 
em algorithms pca spca 
jordan kearns solla editors advances neural information processing systems volume 
mit press 
sn snijders nowicki 
estimation prediction stochastic block models graphs latent block structure 
journal classification 
tb tipping bishop 
probabilistic principal components analysis 
roy 
statistical society 
tit titterington 
aspects latent structure analysis 
volume 
van der van der ark extended study relationship correspondence analysis latent class analysis 
sociological methodology 
wm woodbury 
new procedure analysis medical classification 
methods inf med 
wmm wang mccallum 
group topic discovery relations text 
th acm sigkdd international conference knowledge discovery data mining workshop link discovery issues approaches applications pages 
yu yu tresp 
dirichlet enhanced latent semantic analysis 
saul weiss bottou editors proc 
th international workshop artificial intelligence statistics 
