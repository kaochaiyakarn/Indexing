direct value approximation factored mdps dale schuurmans department computer science university waterloo cs uwaterloo ca simple approach computing reasonable policies factored markov decision processes mdps optimal value function approximated compact linear form 
method solving single linear program approximates best linear optimal value function 
applying ecient constraint generation procedure obtain iterative solution method tackles concise linear programs 
direct linear programming approach experimentally yields significant reduction computation time approximate value policy iteration methods reducing hours seconds 
quality solutions produced linear programming weaker usually twice approximation error approximating class 
speed advantage allows larger approximation classes achieve similar error reasonable time 
markov decision processes mdps form foundation control uncertain stochastic environments reinforcement learning 
standard methods value iteration policy iteration linear programming produce optimal control policies mdps expressed explicit form policy value function state transition model represented tabular manner explicitly enumerates state space 
renders approaches impractical toy problems 
real goal achieve solution methods scale reasonably size state description size state space usually exponential nite 
basic premises solution methods scale exploiting structure mdp model structure reward function state transition model exploiting structure approximate representation optimal value function policy 
credible attempts scaling generally exploit types structure 
surprisingly dicult formulate optimization method handle large state descriptions simultaneously produce value functions policies small approximation errors errors bounded tightly 
investigate simple approach determining approximately optimal policies simple direct linear programming approach 
speci cally idea approximate optimal value function formulating single linear program exploiting structure mdp value function approximation solve linear program eciently 
preliminaries consider mdps nite state action spaces consider goal maximizing nite horizon discounted reward 
states represented vectors length simplicity assume state variables xn total number states assume small nite set actions fa mdp de ned state transition model jx speci es probability state current state action reward function speci es immediate reward obtained action state discount factor 
problem determine optimal control policy achieves maximum expected discounted reward state 
understand standard solution methods useful de ne auxiliary concepts 
policy value function ir denotes expected discounted reward achieved policy state turns satis es xed point relationship value current states expected values states backup operator operates arbitrary functions state space jx important backup operator de ned respect xed action jx action value function ir denotes expected discounted reward achieved action state policy satisfy arbitrary function states greedy policy gre respect de ned gre arg max denote optimal policy denote value function relationship max 
addition de ne gre arg max 
de nitions fundamental methods calculating formulated policy iteration start arbitrary policy iterate gre return value iteration start arbitrary function iterate kf tol 
return gre 
linear programming calculate arg min subject return gre 
methods shown produce optimal policies mdp di erent ways 
approaches share fundamental limitation scale feasibly size state descriptions 
approaches explicit representations policies value functions exponential exploiting structure scale large state spaces necessary exploit substantial structure mdp adopting form approximation optimal value function policy 
speci structural assumptions consider factored mdps linear value function approximations 
assumptions sucient permit ecient policy optimization large mdps 
combined assumptions allow approximate solutions obtained problems involving states reasonably quickly 
factored mdps spirit de ne factored mdp represented compactly additive reward function factored state transition model 
speci cally assume reward function decomposes local reward function de ned small set variables assume state transition model jx represented set dynamic bayesian networks dbns state variables action dbn de nes compact transition model directed bipartite graph connecting state variables consecutive time steps 
denote parents successor variable dbn action allow cient optimization assume parent set contains small number state variables previous time step 
model probability successor state predecessor state action product jx jx 
main bene factored representation allows large mdps encoded concisely functions jx depend small number variables represented small tables eciently combined determine jx 
unfortunately pointed factored mdp yield feasible method determining optimal policies 
main problem factored optimal value function generally compact representation optimal policy 
obtaining exact solution appears require return explicit representations 
turns factored mdp representation interacts linear value function approximations 
linear approximation central tenets scaling approximate optimal value function calculate exactly 
numerous schemes investigated approximating optimal value functions policies compact representational framework including hierarchical decompositions decision trees diagrams generalized linear functions neural networks products experts 
simplest generalized linear functions form investigate 
case consider functions form xed set basis functions denotes variables basis depends 
combining linear functions factored mdps provides opportunities feasible approximation 
rst main bene combining linear approximation factored mdps result applying backup operator linear function results compact representation action value function 
speci cally de ne rewrite ja parent variables union parent variables expresses fact factored mdp expected value component approximation depends current state variables direct parents variables mdp sparsely connected variable sets larger ability represent state action value function compact linear form immediately provides feasible implementation greedy policy gre arg max de nition gre eciently determinable turns permit feasible forms approximate policy value iteration easily implemented 
main problem factored form xed gre 
fact policy concisely represented necessarily compact form usually depends state variables jx jx product terms depend state variables 
introduce additional assumption special default action mdp actions factored transition model 

di ers 

small number state variables 
allows greedy policy gre compact form allows gre concisely represented 
ort possible formulate feasible versions approximate policy value iteration 
approximate policy iteration start default policy iterate arg min max jf gre approximate value iteration start arbitrary iterate gre arg min max jf kf tol 
expensive part iterative algorithms determining arg min max jf involves solving linear program minw subject fw linear program problematic involves exponential number constraints 
central achievement show system constraints encoded equivalent system constraints compact form 
idea construction realize searching max min linear function compact basis conducted organized fashion organized search encoded equally concise constraint system 
construction allows approximate solutions mdps state variables states generated hours approximate policy iteration 
turns approximate value iteration ective takes iterations converge fact diverge theory 
main observation solve linear programs conduct approximate iterations anyway simpler ecient approximate linear programming approach directly 
approximate linear programming rst idea simply observe factored mdp linear value approximation immediately allow directly solve linear programming approximation optimal value function min subject restricted linear form xed basis 
fact known yields linear program basis weights previously shown factored mdp equivalent linear program feasible size formulated 
results outlined easy 
show minimization objective encoded compactly components easily precomputed enumerating assignments small sets variables basis functions 
second seen exponentially constraints structured form 
speci cally represented simple basis representation allows technique encode constraint system enforces enumerating state space action 
implemented approach tested test problems 
problems directed network computer systems xn system 
systems spontaneously go probability step probability increased immediately preceding machine network 
actions default reboot machine reward state simply sum systems bonus reward system server 
considered network architectures shown transition probabilities jx parent jx parent parent parent 
discount factor 
rst basis functions considered just indicators variable plus constant basis function reported 
results network architectures shown 
approximate linear programming method labeled alp compared approximate hj qk server api time alp constraints alp api ub bellman rmax alp gen server api time alp constraints alp api ub bellman rmax alp gen experimental results timings mhz piii processor policy iteration strategy api described 
speci probabilities estimate numbers api graphs comparison meant loosely indicative general run times methods problems 
perturbing probability values signi cantly ect results implemented comparison 
implementation matlab cplex solve linear programs 
preliminary results appear support hypothesis direct linear programming ecient approximate policy iteration problems type 
advantage linear programming approach simpler program involves solving lp 
importantly direct lp approach require mdp special default action action value function directly extracted gre arg max easily recoverable discussing drawbacks note possible solve linear program eciently iteratively generating constraints needed 
possible factored mdps linear value approximations allow ecient search maximally violated constraints linear program provides ective way generating concise linear programs solved eciently formulated 
speci cally procedure exploits feasible search techniques minimizing linear functions discussed previously eciently generate small set critical constraints iteratively grown nal solution identi ed see 
numbers estimated graphs 
exact probabilities computer simulations reported assert exact comparison 
perturbed probabilities little ect performance methods tried loosely representative comparison general performance various algorithms problems 
start constraints loop compute arg min constraints constraints constraint ak solve min subject constraints min tol return 
represent greedy policy procedure rationale procedure main bottleneck previous methods generating constraints solving linear programs 
small number constraints active solution violated near solution adding violated constraints appears useful way proceed 
shows produces approximate solutions alp tiny fraction time 
extreme case produces approximate solution seconds methods take hours problem 
reason speedup explained results show numbers constraints generated method 
investigation required fully outline robustness constraint generation method 
fact guarantee greedy constraint generation scheme proposed produce feasible number constraints 
potential bene ts conservatively generating constraints needed clear 
course main drawback direct linear programming approach approximate policy iteration alp incurs larger approximation errors api 
bounding approximation error turns api alp guaranteed return best linear approximation true value function 
possible ef ciently calculate bounds approximation errors methods exploiting structure problem known result asserts max gre max case 
upper bound turn bounded quantity feasible calculate max max min min max 
upper bound error optimal value function calculated performing ecient search max shows measurable error quantity max reported ub bellman factor larger linear programming approach approximate policy iteration basis 
respect api appears inherent advantage limit exhaustive basis approaches converge optimal value 
get indication computational cost required achieve similar bound approximation error repeated experiments larger basis set included indicators pairs connected variables 
results model reported shows bound approximation error reduced substantially predictable cost sizable increase computation time 
run times appreciably smaller policy iteration methods 
paradoxically linear programming er computational advantages policy value iteration context approximation widely held inferior solution strategy explicitly represented mdps 
bertsekas 
dynamic programming optimal control volume 
athena scienti 
bertsekas tsitsiklis 
neuro dynamic programming 
athena scienti 
boutilier dearden goldszmidt 
stochastic dynamic programming factored representations 
arti cial intelligence 
boyan 
squares temporal di erence learning 
proceedings icml 
dietterich 
hierarchical reinforcement learning maxq value function decomposition 
jair 
guestrin koller parr 
max norm projection factored mdps 
proceedings ijcai 
koller parr 
computing factored value functions policies structured mdps 
proceedings ijcai 
koller parr 
policy iteration factored mdps 
proceedings uai 
martin 
large scale linear integer optimization 
kluwer 
puterman 
markov decision processes discrete dynamic programming 
wiley 
sallans hinton 
free energies represent values multiagent reinforcement learning task 
proceedings nips 
st aubin hoey boutilier 
approximating policy construction decision diagrams 
proceedings nips 
van roy 
learning value function approximation complex decision processes 
phd thesis mit eecs 
williams baird 
tight performance bounds greedy policies imperfect value functions 
technical report northeastern university 
