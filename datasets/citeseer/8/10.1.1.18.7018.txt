kdd project report error correcting codes efficient text classification large number categories ghani center automated learning discovery school computer science carnegie mellon university pittsburgh pa investigate error correcting output codes ecoc efficient text classification large number categories propose extensions improve performance ecoc 
ecoc shown perform classification tasks including text classification remains explored area ensemble learning algorithms 
explore error correcting codes short minimizing computational cost result highly accurate classifiers real world text classification problems 
results show ecoc particularly effective classification 
addition develop modifications improvements ecoc accurate intelligently assigning codewords categories learning decoding combining decisions individual classifiers order adapt different datasets 
reduce need labeled training data develop framework ecoc unlabeled data improve classification accuracy 
research impact area efficient classification documents useful web portals information filtering routing especially open domain applications number categories usually large new documents categories constantly added system needs efficient 

enormous growth line information led comparable growth need methods help users organize information 
area particular seen research activity automated learning techniques categorize text documents 
methods useful addressing problems information filtering routing clustering related documents classification documents pre defined topics keyword tagging word sense disambiguation sentence parsing 
primary application text categorization systems assign subject categories documents support information retrieval aid human indexers assigning categories 
text categorization components seeing increasing natural language processing systems data extraction 
categorization may filter documents parts documents contain extractable data incurring cost expensive natural language processing 
automated text categorization web allows savings human resources frequent updates dealing large amounts data discovery categorization new sites human intervention re categorization known sites content changes taxonomy changes 
case search engines response query search engine report relevant categories contain significant urls combining available information retrieval categorization capabilities 
machine learning techniques text classification difficult due certain characteristics domain large number input features noise large variance percentage features relevant just 
relatively moderate sized text corpus easily vocabulary tens thousands distinct words 
challenging natural language ambiguous sentence multiple meanings 
furthermore considered writing style repeatedly word particular concept synonyms 
means categorizer deal lot words having similar meanings 
natural language text contains homonyms words spelled different meanings 
bank river bank financial institution bank turn aircraft bank fire 
classifier spite word able distinguish different meanings meaning classification 
result algorithms classify text documents need lot time run large number labeled training documents 
text categorization systems attempt reproduce human categorization judgments 
common approach building text categorization system manually assign set documents categories inductive learning automatically assign categories documents 
approach save considerable human effort building text categorization system particularly replacing aiding human indexers produced large database categorized documents 
applications systems involving news current affairs web ones new documents keep coming regular intervals new categories added old categories documents modified deleted 
important applications system retrained fairly quickly training scales number categories 
nave bayes faster algorithms multiclass text classification scales linearly number categories 
algorithms performing benchmark datasets text svms built binary classification problems common approach multiclass learning svms decompose class problem binary problems build binary classifier class 
approach scales linearly number classes efficient dealing hundreds thousands categories 
method decomposes multiclass problems binary problems pairwise approach pair classes distinguished separate classifier model 
approach turns inefficient expensive class problem decomposed 
binary problems requires 
classifiers 
error correcting output codes ecoc originally developed dietterich bakiri efficient classification text documents large number categories 
application ecoc text classification new research differs traditional ecoc improved performance cost efficiency specifically focuses minimizing computational cost maximizing classification accuracy 
ecoc classification tasks including limited applications text previously focus improving classification accuracy cost increasing training time computational cost 
example berger ecoc text classification tasks reported improved performance nave bayes improvement increasing computational cost factors 
increase cost mainly due randomly generated codes extremely long 
explore ecoc increase performance time increasing efficiency system 
research different previous done text classification general ecoc particular focuses specifically text classification large number categories order hundreds categories exploring ways increasing performance ecoc short length codes 
approaches specifically short algebraic codes error correcting properties maximize accuracy minimizing code length 
assign codewords categories depending learning decoding function standard hamming distance 
develop framework ecoc enable small amount labeled training data augment large amounts unlabeled data approach tested real world text classification tasks find short errorcorrecting codes results efficient highly accurate high precision text classifiers learning decoding function adapt various datasets 
combination ecoc training labeled unlabeled data performs outperforms algorithms designed combine labeled unlabeled data 

related wide range statistical machine learning techniques applied text categorization including multivariate regression models fuhr schutze nearest neighbor classifiers yang probabilistic bayesian models koller sahami mccallum decision trees lewis ringuette neural networks schutze weigend symbolic learning apte cohen singer ensemble learning schapire singer ghani berger support vector machines dumais joachims :10.1.1.22.4864
error correcting codes shown increase accuracy decision trees neural networks non text data sets available irvine repository murphy aha artificial neural networks decision trees kong dietterich 
tried different random assignments codewords categories see significant performance differences 
schapire showed adaboost combined ecoc yield method superior ecoc uci datasets 
guruswami sahai propose method combining boosting ecoc weights individual weak learners differently schapire show method outperforms schapire adaboost oc 
ricci aha applied method combines ecoc feature selection local learners selects different subset features learn bit 
berger applies ecoc approach text classification problems contain large number categories 
knowledge study ecoc applied text classification previous 
give theoretical evidence random error correcting codes 
previous shown short random codes perform practice empirical evidence favor error correcting codes random codes 
surge combining labeled unlabeled data text learning tasks em nigam training type algorithms blum mitchell nigam ghani 
studies resulted encouraging results showing unlabeled data tremendous value studies focused problem large number categories especially training datasets binary problems 

overview ecoc previously described attempt specifically maximize performance ecoc short codes develop framework unlabeled data classifying documents taxonomy large number categories 
focus research developing improved algorithms ecoc efficient text categorization large number categories 
approach extensions ecoc range short effective codes elaborate algorithms augment limited number labeled training examples large number unlabeled data order improve performance classifier 
error correcting codes application classification problems better understood analogy dietterich bakiri 
look text classification type communications problem correct category transmitted medium channel 
channel consists words training examples learning algorithm 
due errors introduced finite training sample poor choice input features limitations invalid assumptions learning process class information distorted 
error correcting code transmitting bit separately separate run algorithm system may able recover errors 
codes correct errors possible 
ecoc works converting class supervised learning problem large number class supervised learning problems 
learning algorithm handle class learning problems applied learn problems 
thought length codewords bit codeword classifier 
class assigned unique binary string length refer strings codewords dietterich bakiri 
train classifiers predict bit string 
predicted class codeword closest codeword produced classifiers 
distance metric experiments hamming distance counts number bits codewords differ 
process mapping output string nearest codeword identical decoding step error correcting codes bose ray chaudhri 
error correcting codes traditionally correct errors transmitting data communication tasks 
idea codes add redundancy data transmitted errors occur due noise channel data correctly received 
key difference error correcting codes communication tasks opposed machine learning classification tasks communication tasks require rows code separated terms hamming distance classification tasks require columns separated 
reason row separation want codewords classes maximally far apart column separation necessary functions learned bit uncorrelated errors bit independent 
errors learner bit correlated error bit result errors multiple bits code able correct 
clear column separation necessary 
fact may ways making errors bit independent uncorrelated column separation disjoint sets features learn bit codeword independent denote concept 
utilize setting similar training algorithm proposed blum mitchell 
example case classifying web pages training data hyperlinks page page code separated terms column hamming distance independent errors data independent 
table 
ecoc algorithm number classes training phase 
create binary matrix 
class assigned row 
train base classifier learn binary functions column 
test phase 
apply classifiers test example 

combine predictions form binary string length 
classify class nearest codeword 
experimental setup experiments conducted base classifier learn bit codeword naive bayes classifier 
implementation rainbow developed andrew mccallum available www cs cmu edu mccallum bow 
feature selection done selecting words highest mutual information gain 
codes section constructed bch method uses algebraic coding theory generate codes separated hamming distance rows 
information bch codes error correcting codes literature hill peterson 
codes experiments available online www cs cmu edu ecoc 
datasets datasets experiments described section 
industry sector dataset industry sector dataset data available market guide 
www com consists web pages classified hierarchy industry sectors 
data publicly available www cs cmu edu datasets html 
take hierarchy account experiments flattened version dataset 
dataset contains total documents divided classes 
small fraction documents belongs multiple classes experiments remove documents 
tokenizing data skip mime html headers standard stoplist perform stemming 
procedure mccallum 
slightly modified version data set 
removing tokens occur corpus contains words vocabulary size 
hoovers dataset corpus web pages assembled hoovers online web resource www hoovers com obtaining list names home page urls companies web custom crawler collect web pages site breadth order examining just web pages 
sets categories available hoover online consists classes call hoovers categories hoovers label industry sector belongs oil gas sports manufacturers computer software 
categories label companies particular web pages 
reason constructed synthetic page concatenating pages crawled 
website classified category classification scheme 
populous majority class contains documents 
natural feature split available dataset randomly divide vocabulary equal parts apply training feature sets 
previously shown nigam ghani random partitioning works reasonably absence natural feature split 
documents belong classes removed dataset affecting results considerably 
jobs dataset dataset obtained whizbang 
labs consisting job titles descriptions organized level hierarchy level categories leaf categories 
classification schemes jobs classes jobs classes 
examples example consists job title corresponding job description 
consider job title job description feature sets training 
nave bayes naive bayes simple effective text classification algorithm learning labeled data 
parameterization naive bayes defines underlying generative model assumed classifier 
model class selected class prior probabilities 
generator creates word document drawing multinomial distribution words specific class 
model assumes word document generated independently class 
naive bayes forms maximum posteriori estimates class conditional probabilities word vocabulary labeled training data done counting frequency word occurs word occurrences documents class supplemented laplace smoothing avoid probabilities zero count number times word occurs document class label 
prior probabilities class calculated maximum likelihood estimation counting documents 
classification time estimated parameters applying bayes rule calculate probability class label probable class prediction 
naive bayes independence assumption states words occur independently class document 
results section describes experiments performed ecoc 
compare results performance naive bayes commonly algorithms text classification 
naive bayes classifier learn individual functions bit code 
class unique binary code length 
classify test instance test individual bit classifiers combine output compare codes class 
test instance assigned class nearest codeword terms hamming distance ties broken randomly 
error correcting codes improve classification accuracy 
shows performance ecoc approach vs single naive bayes classifier industry sector jobs jobs hoovers hoovers datasets 
vocabulary size experiments optimized selecting words highest mutual information gain class 
ecoc results higher accuracy nave bayes datasets hoovers 
noteworthy increase accuracy ecoc comes increased efficiency 
nave bayes constructs model class class industry sector dataset constructs models 
ecoc bits constructs models reduces computational cost training testing factor 
specifically short codes keep computational cost increasing classification performance 
lower performance ecoc hoovers dataset attributed fact bit code increased computational efficiency times losing accuracy 
gives information performance ecoc terms classification accuracy tell efficiency approach 
visualize gain accuracy efficiency time compares ecoc nave bayes datasets terms percent reduction error percent increase efficiency 
observe marked improvement efficiency datasets jobs bit code class problem increase efficiency 

percent reduction error percent increase efficiency ecoc compared nave bayes 
jobs increase efficiency zero re bit code class problem 

classification accuracies ecoc various datasets 
industry sector hoovers bit codes hoovers jobs bit codes jobs bit code 
industry sector hoovers hoovers jobs jobs reduction error nave bayes increase efficiency industry sector hoovers hoovers jobs jobs dataset accuracy nb ecoc ecoc improve precision nave bayes classifier 
applications text classification systems important get high precision results 
example search engines typical user looks top hits returned maximizing accuracy system important maximize precision classifier labels small proportion examples confident 
ecoc judge confident system prediction looking far predicted codeword codeword nearest class 
closer codeword nearest class confident prediction 
shows distributions hamming distance codeword nearest class 
obvious graphs distribution correctly classified examples significantly different misclassified examples 
correctly classified examples correctly matched nearest class hamming distance zero examples misclassified large hamming distance nearest class 
hd nearest class measure confidence ecoc calculated precision recall tradeoff 

distributions hamming distance codeword nearest class 
precision recall curve hoovers dataset bit ecoc correctly class ified examples ing distance nearest class misclassified examples ing distance nearest class recall nb bit ecoc recall precision bit ecoc nb figures show tradeoff hoovers industry sector datasets 
datasets ecoc results higher precision classification compared nave bayes 
nave bayes works classification known give accurate probabilistic estimates text classification tasks number features usually large multiplicative method combining evidence drives scores 
surprisingly nave bayes give high precision results curve flat recalls lower 
ecoc hand performs extremely industry sector dataset gives precision recall level 
significant point observe ecoc improves classification accuracy hoovers ecoc performs better improving precision 
nb gives maximum precision ecoc gets precision computationally efficient factor 
results suggest ecoc useful method combining multiple nb classifiers give high precision results added advantage computationally efficient standard nb models 
length codes affect performance 
table shows classification accuracies codes different lengths 
clearly observe increasing length codes increases classification accuracy 
increase accuracy directly proportional increase length code 
codes get larger accuracies start leveling observe table 
trend observed berger 
increase accuracy code length explained fact increase length codes error correcting properties enhanced 
error correcting code large row column separation 
words individual codeword separated codewords large hamming distance columns functions learned separate classifiers different learning functions cause individual classifiers mistakes multiple bits hinders error correcting code correcting 
minimum hamming distance code code error correcting code 
individual bit classifiers fewer errors resulting codeword closer correct codeword correctly decoded 
longer code separated individual codewords having larger minimum hamming distance improving error correcting ability 
seen table increase length codes reduces classification errors 
precision recall curve industry sector dataset bit ecoc table 
average classification accuracies random train test splits class industry sector dataset vocabulary size 
method nave bayes bit ecoc bit ecoc bit ecoc hoovers industry sector 
precision recall different code lengths hoovers dataset 
assume looking table keep increasing code length classification accuracy keep increasing 
case practice possible errors individual bit classifiers completely independent case 
errors dependent lot overlap data feature set learn binary functions certain code length exceeded classification accuracy starts leveling 
number training examples affect accuracy 
shows results experiments varying number training examples 
different samples train test splits taken results averaged produce graph 
see ecoc outperforms naive bayes classifier times 
previous results regarding increase classification accuracy increase code length hold longer codes give better performance experiments 
table shows average classification accuracies individual binary classifiers different sample sizes 
increase number training examples naive bayes learning binary problems shows improved performance turn improves accuracy combined classification ecoc 
improvement observed shows percent reduction error ecoc multiclass nave bayes different training sizes 
hoovers precision bit bit nb 
performance ecoc industry sector dataset varying number training examples 
table 
average classification accuracies individual bit classifiers random splits class industry sector dataset vocabulary size 
standard deviation quite small training examples class average classification accuracy individual bit classifiers 
percent decrease error nb industry sector dataset varying number training examples 
percent reduction error vary number training examples suggests ecoc provides performance considerable reduction error nb regardless size training set algorithm training data sparse 
intuitive analysis ecoc suggests increasing training examples improves performance nave bayes improves performance individual binary classifiers just nave bayes classifiers see table 
improvement accuracy ecoc nave bayes mainly error correcting ability dependent training examples class accuracy nbc bit ecoc bit ecoc bit ecoc training examples class bit ecoc bit ecoc bit ecoc code length hamming distance 
multiclass nave bayes learning classify class problem individual bit classifiers problems experimental results suggest improve performance roughly rate increase training examples keep percentage reduction error due ecoc dependent length code 
fact domains training data sparse nbc perform error correcting properties code performance increased 

choosing codewords earlier results observe ecoc provides powerful way increase accuracy collection learners 
central features approach learning algorithm code employed 
usually case learning algorithm domain dependent surprisingly code 
code purposes large column row separation 
discussion methods choosing codes published papers ecoc berger moreira 
approaches consider constructing codes standard coding theory methods bch hadamard 
constructing random codes 
constructing meaningful codes capture represent feature data set 
coding theory algebraic coding theory gives various ways constructing codes error correcting properties 
codes earlier experiments reported binary bch codes may defined constructing matrix entries belong field order converting parity check matrix binary code 
details bch codes peterson 
primary reason codes bch fixed length certain row separation guaranteed 
terms classification translates having guarantee classifiers give wrong classification classify correct class 
error correcting codes guarantee separation rows codewords mentioned earlier need separation columns errors classifiers learning bit independent possible 
column separation guaranteed bch codes exist codes hadamard codes provide guarantee 
case bch codes calculated minimum maximum average column separation codes experiments separation quite large 
reason codes coding theory computational efficiency efficient ways decoding doing linear search codewords case random codes 
may concern datasets consisting thousands classes 
random codes berger argues random codes generated picking entry matrix random 
give theoretical results bounds column row separation random codes show asymptotically length codes gets large approaches infinity probability codes separation arbitrary number approaches zero 
argue results hold theory want long codes practice high computational cost 
prefer short codes guarantee certain column row separation 
compare error correcting properties random codes constructed codes bch codes generated random codes calculated minimum maximum average row column separations 
fixed length code bch codes outperformed random codes significantly 
similarly random codes classify industry sector dataset results worse bch codes earlier experiments 
average ecoc method random codes error rate greater algebraic codes 
meaningful domain data specific codes types codes described give classification results split set classes disjoint sets priori data account 
section explores alternatives method error correcting notion binary functions learned classifiers inspired data hand 
quite logical believe functions partition classes groups similar classes easier learn random functions 
example classes football politics world politics expect function combines football class politics world politics class easier learn groups politics 
test idea industry sector dataset compare results codes earlier constructed bit code contained binary problems grouped related economic sectors 
bit code give row separation need error correcting appended bits bit bch code resulting bit code 
table 
maximum hamming distance information rows columns bit bch code bit hybrid code codewords classification accuracy industry sector dataset 
code min row hd max row hd min col hd max col hd accuracy bit bch bit hybrid expect new bit code perform better bit code new code bits constructed data hand easier learn classifier 
table gives accuracy codes observe bit code performs slightly worse bit 
look classifiers learning bit see accuracies bits added accuracies 
expected bits easier learn 
interesting point bits binary partitions induced bits don equal number documents classes groups 
example bit class contains documents class contains 
bits equal number column 
skewed binary problems easier learn suggests get better results create new binary problems property 
degenerate case class approach dietterich bakiri bit column column distinguishing particular class rest 
case expect individual classifiers perform extremely accuracy poor hamming distance codewords just error correction possible 
obvious adding bits bit bch code increase row hamming distance 
reduce column separation resulting code 
table shows hamming distance hd information codes expected row hd increased slightly column hd significantly reduced 
creating columns lot zeros ones vice versa help individual accuracies bit reduce minimum hamming distance code 
guruswami sahai describe attempts combine approaches class error correcting codes outperform approach 

semi theoretical model ecoc results previous sections conclude behavior advantages ecoc approach text classification 
saw table average accuracies individual classifiers bit code 
variation accuracies small training sample values average probability bit classified correctly calculating minimum hamming distance code experiments ecoc approach modeled binomial distribution length code number bits probability bit classified incorrectly 
probability instance classified correctly just follow binomial distribution 
table shows results calculation 
min minimum hamming distance code code minimum distance correct errors max maximum number errors code correct 
probability bit classifier classify correctly obtained experimental results 
theoretical accuracy ecoc method calculated binomial distribution 
sample calculation row table min max classifiers give wrong classification correct classification obtained 
probability classifiers classifying incorrectly classifiers classify incorrectly classifier classifies incorrectly classifiers classify incorrectly suggests expect accuracy classifier 
see expected accuracies quite close actual results experiments 
table 
calculating theoretical accuracy ecoc hmin minimum row hamming distance emax maximum number errors code correct 
bits min max min average accuracy bit theoretical accuracy 
comparison theoretical accuracies experimental results ecoc various code lengths number training examples 

extensions ecoc apart applying ecoc text classification tasks large number categories investigating performance vary conditions investigate extensions ecoc 
domain text classification hope results generalize classification task large number categories 
length code theoretical assignment codewords categories knowledge previous ecoc involved randomly assigning codewords categories 
propose method assignment adapts particular datasets lead better performance 
learn original multiclass problem standard classifier performs text classification tasks knn naive bayes learning model class generate confusion matrix training set 
matrix tells extent classifier confuses pair classes 
codewords error correcting code equidistant codewords farthest apart assigned classes confusable difficult distinguish dataset 
farthest pair codewords gets assigned confusable classes 
times go list classes may codeword assigned case reassign class different codeword higher corresponds higher 
believe approach assigning codewords categories allow generic error correcting code applied problem adapt specific problem increase performance 
intuitively classifiers errors confused classes codewords farthest apart zeros ones assigned classes allows system recover errors 
potential problem see approach classes confusable probably different classes new binary problem 
binary problem harder learn potentially lead poorer results 
hand help decoding step correcting errors resulting tradeoff accuracy individual classifiers error correcting properties code 
noticed tradeoff artificially constructing domain specific codes correspond position classes class hierarchy observed error correcting property times important difficulty individual binary problems ghani 
apply method assigning codewords categories class problem classes chosen industry sector dataset bit code shown table approach performs significantly worse assigning codewords randomly 
looking closely performance individual binary classifiers notice low accuracies assigning codewords intelligently individual binary problems harder learn turn led performance degradation 
table 
intelligently assigning codewords categories bit code class problem classes chosen industry sector dataset ecoc random codeword assignment ecoc intelligent codeword assignment decoding step ensemble learning algorithms ecoc decomposing problem decisions individual classifiers usually combined simple ways 
common ways decoding codes hamming distance map received binary string nearest codeword code 
approach assumes bits equally important ignores easy hard bit learn 
guruswami sahai showed weighting bits classification step accuracies training set gives better classification accuracy called maximum likelihood decoding 
fixed method decoding relies estimates obtained training set 
propose pose combining classification step learning problem learning algorithm learn decoding 
possible datasets learn give equal weights bits weights depend difficulty individual binary problems performance binary classifiers 
experiments layer neural network learn decoding 
bit code input output layers nn units bit 
training instance nn consisted binary codeword predicted learned binary classifiers input codeword actual categories training example desired output 
learning phase apply nn test cases map codeword output nn class nearest codeword terms hamming distance 
results classes chosen fro industry sector dataset shown table 
nn learn decoding hamming distance result significant improvement hamming distance 
results representative particular dataset experiments needed evaluate merits potentially promising approach 
table 
learning decoding function bit code class problem classes chosen industry sector dataset ecoc hamming distance decoding ecoc nn learn decoding intuition learning decoding binary problems bit code hard learn learner consistently classifies wrong gets accuracy decoding step learn classification particular bit flip classification correct increase accuracy considerably 
reason don observe experiments bits accurate nn couldn correct classification 
hypothesize intelligently assigning codewords categories described previous section binary problems harder learn nn learn decoding improve performance decrease performance observed previous section 
table 
learning decoding function bit code class problem classes chosen industry sector dataset codeword assignment random intelligent decoding hamming distance neural network test hypothesis dataset earlier find combining approaches improve performance considerably see table 
looking individual accuracies binary classifiers learning decoding step notice marked improvement accuracies corresponding bit flipping phenomena expected 
suggests construct codes error correcting properties individual binary problems may learnable decoding step correct errors just inverting individual classifications 

combining labeled unlabeled data ecoc previous sections proposed ways increase accuracy efficiency ecoc fixed amount labeled training data 
different way looking efficiency focus amount labeled training data required learning algorithm 
major difficulty supervised learning techniques text classification require large number labeled examples learn accurately 
collecting labeled examples costly labeling process done manually 
ideally prefer systems provide accurate classifications labeling examples thousands 
way reduce amount labeled data required develop algorithms learn effectively small number labeled examples augmented large number unlabeled examples 
general unlabeled examples expensive easier obtain labeled examples 
particularly true text classification tasks involving online data sources web pages email news stories huge amounts unlabeled text readily available 
collecting text done automatically feasible quickly gather large set unlabeled examples 
unlabeled data integrated supervised learning process building text classification systems significantly faster expensive 
supervised learning algorithms combine information labeled unlabeled data 
approaches include expectation maximization estimate maximum posteriori parameters generative model text classification nigam generative model built unlabeled data perform discriminative classification jaakkola haussler transductive inference support vector machines optimize performance specific test set joachims 
results shown unlabeled data significantly decrease classification error especially labeled training data sparse 
related body research uses labeled unlabeled data problem domains features naturally divide disjoint sets 
example blum mitchell algorithm classifying web pages builds classifiers words appear page words appearing hyperlinks pointing page 
datasets features naturally partition sets algorithms division fall training setting blum mitchell blum mitchell show pac guarantees learning labeled unlabeled data hold assumptions set features sufficient classification feature sets instance conditionally independent class 
published studies training type algorithms blum mitchell nigam ghani focused small binary problems clear generalize real world classification tasks large number categories 
hand error correcting output codes ecoc suited classification tasks large number categories 
earlier focused text classification problems earlier ghani berger specifically deal large number categories 
develop framework incorporate unlabeled data ecoc setup decompose multiclass problems multiple binary problems training learn individual binary classification problems 
show approach especially useful classification problems involving large number categories outperforms algorithms designed combine labeled unlabeled data text classification 
training setting training setting applies dataset natural division features 
example web pages described text web page text hyperlinks pointing web page 
traditional algorithms learn domains ignore division pool features 
algorithm uses training setting may learn separate classifiers feature sets combine predictions decrease classification error 
training algorithms labeled unlabeled data explicitly leverage split learning 
blum mitchell formalize training setting provide theoretical learning guarantees subject certain assumptions 
formalization instance described sets features 
certain assumptions blum mitchell prove training algorithms learn unlabeled data starting weak predictor 
assumption instance distribution compatible target function examples target functions feature set predict label 
example web page domain class instance identifiable hyperlink text page text 
second assumption features set instance conditionally independent features second set class instance 
assumes words web page related words incoming hyperlinks class web page somewhat unrealistic assumption practice 
argue weak initial hypothesis feature set label instances 
instances randomly distributed classifier conditional independence assumption classification noise weak hypothesis 
algorithm learn presence classification noise succeed learning labeled instances 
combining ecoc training propose new algorithm aims combining advantages ecoc offers supervised classification large number categories training combining labeled unlabeled data 
ecoc works decomposing multiclass problem multiple binary problems incorporate unlabeled data framework learning binary problems cotraining 
algorithm propose follows training phase 
problem classes create binary matrix 
class assigned row 
train trained classifiers learn binary functions column column divides dataset groups 
test phase apply single bit trained classifiers test example 

combine predictions form binary string length 
classify class nearest codeword course class problem decomposed naively binary problems training learn binary problem approach efficient ecoc reduce number models classifier constructs 
believe approach perform better nave approach conditions ecoc outperform nave bayes multiclass problem learns model class training improve single nave bayes classifier binary problem unlabeled data complication arises fulfilling condition normal binary classification problems training shown training case involves binary problems consist multiple classes 
classes bit created artificially ecoc consist real classes guarantee training learn arbitrary binary functions 
classes ecoc bit consists multiple classes original dataset 
take sample classification task consisting classes ecoc bits partitions data classes class class 
actual classes contain different number training examples possible distribution skewed 
pick initial labeled examples randomly classes guarantee example original classes 
training contain labeled example original classes confident labeling unlabeled example class 
conditions initial labeled examples cover original class target function binary partition learnable underlying classifier feature split redundant independent training algorithm utilize unlabeled data theoretically combination ecoc training result improved performance unlabeled data 
descriptions algorithms nave bayes base classifier experiments 
learn binary problems ecoc classifier training 
expectation maximization em algorithm compare proposed approach 
short description naive bayes em experiments 
expectation maximization extend supervised learning setting include unlabeled data naive bayes equations longer adequate find maximum posteriori parameter estimates 
expectationmaximization em technique find locally maximum parameter estimates 
em iterative statistical technique maximum likelihood estimation problems incomplete data dempster 
model data generation data missing values em locally maximize likelihood parameters give estimates missing values 
naive bayes generative model allows application em parameter estimation 
scenario class labels unlabeled data treated missing values 
implementation em iterative step process 
initial parameter estimates set standard naive bayes just labeled documents 
iterate steps 
step calculates probabilistically weighted class labels pr cj di unlabeled document 
step estimates new classifier parameters documents equation pr cj di continuous step 
iterate steps classifier converges 
shown technique significantly increase text classification accuracy limited amounts labeled data large amounts unlabeled data nigam 
datasets assumption correlating classes single multinomial component badly violated basic em performance suffers 
experimental results codes experiments bch codes bit codes jobs dataset bit codes hoovers dataset similar ghani combining ecoc training 
table shows results experiments comparing proposed algorithm em training 
baseline results nave bayes ecoc unlabeled data labels known 
serve upper bound performance algorithm 
codes experiments downloaded www cs cmu edu ecoc codes table 
average classification accuracies fold cross validation jobs hoovers datasets dataset nave bayes unlabeled data ecoc unlabeled data em cotraining ecoc cotraining labeled labeled labeled labeled labeled labeled labeled jobs hoovers results reported papers blum mitchell nigam ghani clear training perform give leverage unlabeled data dataset consisting large number classes 
see training em improve classification accuracy unlabeled data hoovers dataset negative effect resulted decreased accuracy 
accuracy reported em training decreasing iteration experiments stopped different times comparable 
hand proposed combination ecoc training take advantage unlabeled data better em training outperforms algorithms datasets 
worth noting ecoc outperforms nave bayes datasets pronounced number labeled examples small 
shows performance algorithm terms precision recall tradeoff 
precision recall standard evaluation measures text classification information retrieval literature 
see nave bayes em giving high precision results 
surprising resulting classifier learning em nave bayes classifier gives skewed scores test examples providing accurate probabilistic estimates 
interestingly nave bayes ecoc results high precision classification reasonable levels recall 
result encouraging enormous value applications require high precision results search engines hypertext classification systems 

precision recall tradeoff hoovers dataset algorithms training learn binary problems created ecoc 
framework incorporate unlabeled data ecoc necessary training learn individual binary functions 
theoretically learning algorithm learn binary functions labeled unlabeled examples 
section training employ algorithm named em hybrid em training learn binary problems 
em em nigam ghani iterative algorithm uses feature split similar fashion training 
feature split feature sets trains classifiers feature set 
proceeds initializing feature set naive bayes classifier labeled data 
probabilistically labels unlabeled data 
feature set classifier trains labeled data unlabeled data labels 
data process iterates classifiers converge 
predictions combined training embedded classifiers 
practice em converges quickly em experimentally run em iterations 
em algorithm thought closer match theoretical argument blum mitchell training algorithm 
essence argument initial classifier generate large sample labeled data train classifier 
em algorithm exactly learner assign labels unlabeled data second classifier learns 
contrast training algorithm learns single example time 
results em training ecoc performs better jobs dataset accuracy worse hoovers accuracy 
key difference algorithms em re labels unlabeled examples iteration training re labels example adding labeled set 
better performance em jobs dataset may due fact relabeling prevents algorithm getting stuck local minima sensitive choice initial examples 
discussion noted running experiments approach sensitive initial documents provided labeled examples 
leads believe form active learning combined method pick initial documents perform better picking random documents 
cotraining adds labeled set unlabeled examples confident 
selection criterion modified improved making directly focused classification task hand 
example adding confident examples balance confidence minimizes risk adding misclassified example measure learned example 
mccallum nigam prototypicality measure active learning setting approximately measures benefit labeling particular example 
allow training algorithms fewer labeled examples perform better 
mentioned section guarantee training learn arbitrary binary functions classes created artificially 
training labeled example original classes confident labeling unlabeled example class 
ran experiments training examples cover original classes expected results worse ones reported previous section certain number examples chosen initially class 
ways ecoc training combined training ecoc classifiers feature sets separately combining training 
interesting pursue approaches 
potential drawback approach training type algorithms need redundant independent feature sets 
experiments reported split feature sets random fashion hoovers dataset 
previous nigam ghani shown random partitions feature set result reasonable performance preliminary done developing algorithms partition standard feature set redundantly sufficient feature sets 
extend applicability proposed approach regular datasets 

summary proposed novel improvements error correcting output coding method task efficiently text classification large number categories 
improvements result algorithms efficient current ones result better classification accuracy 
showed short error correcting codes ecoc results efficient accurate high precision classifiers significantly outperform nave bayes 
showed intelligently assigning codewords categories learning decoding combining decisions individual classifiers improves performance ecoc 
experiments combining labeled unlabeled data lead believe combination ecoc training algorithms useful learning labeled unlabeled data 
shown approach outperforms training em algorithms previously shown text classification tasks 
approach performs terms accuracy provides smooth precision recall tradeoff useful applications requiring high precision results 
furthermore shown framework general algorithm learn binary functions labeled unlabeled data successfully ecoc 
research gives insight ecoc problem text classification applied domain large number categories 
fly personal possible individual quickly train classifiers giving handful labeled examples 
impact areas search engines web portals filtering systems personalization tools information retrieval systems 
proposed specifically evaluated text collections hope results generalize classification problem impact large scale classification problems 
apte damerau weiss 


language independent automated learning text categorization problems 
proceedings seventeenth annual international acm sigir conference research development information retrieval pp 

dublin ireland springer verlag 
berger 

error correcting output coding text classification 
ijcai workshop machine learning information filtering 
blum mitchell 

combining labeled unlabeled data training 
conference computational learning theory july 
bose ray chaudhri 

class error correcting binary group codes 
information control 
cohen singer 


context sensitive learning methods text categorization 
proceedings nineteenth annual international acm sigir conference research development information retrieval pp 

new york acm craven dipasquo freitag mccallum mitchell nigam slattery 

learning extract symbolic knowledge world wide web 
proceedings fifteenth national conference artificial intelligence pp 

aaai press mit press 
dietterich bakiri 

solving multiclass learning problems error correcting output codes 
journal artificial intelligence research 
freund iyer schapire singer 
efficient boosting algorithm combining preferences 
proceedings fifteenth international conference machine learning 
fuhr lustig tzeras air rule multi stage indexing system subject fields 
proceedings riao 
ghani 
error correcting text classification 
proceedings th international conference machine learning 
ghani slattery yang 

hypertext categorization hyperlink patterns meta data proceedings th international conference machine learning icml 
guruswami sahai 
boosting error correcting codes 
proceedings th annual conference computational learning theory pp 

acm 
hill 

course coding theory 
oxford university press 


codes 

koller sahami 
hierarchically classifying documents words proceedings fourteenth international conference machine learning icml 
kong dietterich 

error correcting output coding corrects bias variance 
proceedings th international conference machine learning 
pp 

morgan kaufmann 
kong dietterich 

probability estimation error correcting output coding 
iasted international conference artificial intelligence soft computing banff canada 
lewis ringuette 

comparison learning algorithms text categorization 
proceedings third annual symposium document analysis information retrieval 
lewis schapire callan papka 

training algorithms linear text classifiers 
proceedings nineteenth annual international acm sigir conference research development information retrieval pp 

moreira 
decomposition dichotomies 
proceedings fourteenth international conference machine learning 
mccallum rosenfeld mitchell ng 

improving text classification shrinkage hierarchy classes 
proceedings fifteenth international conference machine learning 
mitchell 

machine learning 
mcgraw hill publishing 
moulinier ganascia 

text categorization symbolic approach 
proceedings fifth annual symposium document analysis information retrieval 
moulinier 

learning bias issue text categorization problem 
technical report lip universite paris vi 
murphy aha 

uci repository machine learning databases machine readable data repository 
technical report university california irvine 
nigam mccallum thrun mitchell 
text classification labeled unlabeled documents em 
machine learning 
pp 

nigam ghani 
analyzing effectiveness applicability training 
proceedings ninth international conference information knowledge management cikm peterson jr 

error correcting codes 
mit press cambridge ma 


theory error correcting codes 
john wiley sons 
quinlan 

program empirical learning 
morgan kaufmann san mateo ca 
quinlan 

decision trees probabilistic classifiers 
proceedings fourth international conference machine learning 
salton 

developments automatic text retrieval 
science 
schapire singer 

boostexter system multiclass multi label text categorization unpublished 
schapire freund bartlett lee 

boosting margin new explanation effectiveness voting methods 
annals statistics 
hull pedersen comparison classifiers document representations routing problem 
proceedings th annual international acm sigir conference research development information retrieval sigir 
wiener pederson weigend 

neural network approach topic spotting 
proceedings fourth annual symposium document analysis information retrieval 
yang chute 

example mapping method text categorization information retrieval 
acm transactions information systems 
