hierarchical hidden markov model analysis applications shai fine cs huji ac il institute computer science hebrew university jerusalem israel yoram singer singer research att com labs park avenue florham park nj naftali tishby tishby cs huji ac il institute computer science hebrew university jerusalem israel 
introduce analyze demonstrate recursive hierarchical generalization widely hidden markov models name hierarchical hidden markov models hhmm 
model motivated complex multi scale structure appears natural sequences particularly language handwriting speech 
seek systematic unsupervised approach modeling structures 
standard forward backward algorithm derive efficient procedure estimating model parameters unlabeled data 
trained model automatic hierarchical parsing observation sequences 
describe applications model parameter estimation procedure 
application show construct hierarchical models natural english text 
models different levels hierarchy correspond structures different length scales text 
second application demonstrate hhmms automatically identify repeated strokes represent combination letters cursive handwriting 

hidden markov models hmms method choice modeling stochastic processes sequences applications speech handwriting recognition rabiner nag computational molecular biology krogh baldi 
hidden markov models natural language modeling see 
jelinek 
applications model topology determined advance model parameters estimated em procedure dempster known forwardbackward baum welch algorithm context baum petrie 
worked explored inference model structure stolcke omohundro 
applications difficulties due multiplicity length scales recursive nature sequences 
difficulties overcome stochastic context free grammars scfg 
parameters stochastic grammars difficult estimate typically likelihood observed sequences induced scfg varies dramatically small changes parameters model 
furthermore common algorithm parameter estimation scfgs called inside outside fine singer tishby algorithm lari young cubic time complexity length observed sequences 
hierarchical generalization hidden markov model 
primary motivation enable better modeling different stochastic levels length scales natural language speech handwriting text 
important property models ability infer correlated observations long periods observation sequence higher levels hierarchy 
show efficiently estimate model parameters estimation scheme inspired inside outside algorithm 
structure model propose fairly general allows arbitrary number activations submodels 
estimation procedure efficiently approximated computation time quadratic length observations 
long time correlations captured model keeping running time reasonable 
demonstrate applicability model estimation procedure learning multi resolution structure natural english text 
resulting models exhibit formation temporal experts different time scales punctuation marks frequent combinations letters endings phrases 
learning algorithm hierarchical hidden markov models unsupervised learning repeated strokes represent combinations letters cursive handwriting 
submodels resulting hhmms spot new occurrences letters combination unlabeled data 
organized follows section introduce describe hierarchical hidden markov model 
section derive estimation procedure parameters hierarchical hidden markov model 
section describe demonstrate applications utilize model estimation scheme 
section discuss related describe possible generalizations model conclude 
order keep presentation simple technical details deferred technical appendices 
summary symbols variables appendix 
model description hierarchical hidden markov models hhmm structured multi level stochastic processes 
hhmms generalize standard hmms making hidden states autonomous probabilistic model state hhmm 
states hhmm emit sequences single symbol 
hhmm generates sequences recursive activation substates state 
substate composed substates activate substates process recursive activations ends reach special state term production state 
production states states emit output symbols usual hmm state output mechanism output symbol emitted production state chosen probability distribution set output hierarchical hidden markov model symbols 
hidden states emit observable symbols directly called internal states 
term activation substate internal state vertical transition 
completion vertical transition may include vertical transitions lower level states control returns state originated recursive activation chain 
state transition level call horizontal transition performed 
set states vertical transitions induces tree structure root state node top hierarchy leaves production states 
simplify notation restrict analysis hhmms full underlying tree structure leaves distance root state 
analysis hhmms general structure straightforward generalization analysis 
experiments described performed general topology 
note passing hhmm represented standard single level hmm 
states hmm production states corresponding hhmm fully connected structure non zero probability moving states state 
equivalent hmm lacks multi level structure exploit applications described section 
give formal description hhmm 
sigma finite alphabet 
denote sigma set possible strings sigma 
observation sequence finite string sigma denoted delta delta delta ot state hhmm denoted dg state index hierarchy index 
hierarchy index root production states internal states need number substates 
denote number substates internal state jq clear context omit state index denote state level addition model structure topology hhmm characterized state transition probability internal states output distribution vector production states 
internal state gamma state transition probability matrix denoted ij ij jq probability making horizontal transition ith state jth substates similarly pi fp jq initial distribution vector substates probability state initially activate state turn internal state may interpreted probability making vertical transition entering substate parent state production state solely parameterized output probability vector fb oe jq probability production state output symbol oe sigma 
entire set parameters denoted dg ffa gamma pi gamma fb gg illustration hhmm arbitrary topology parameters 
fine singer tishby 
illustration hhmm levels 
gray black edges respectively denote vertical horizontal transitions 
dashed thin edges denote forced returns state level level parent state 
simplicity production states omitted 
summarize string generated starting root state choosing root substates random pi similarly internal state entered substates randomly chosen initial probability vector pi operation proceeds chosen substate recursively activates substates 
recursive operations carried production state reached point single symbol generated state output probability vector control returns state activated completion recursive string generation internal state started recursion chooses state level level state transition matrix newly chosen state starts new recursive string generation process 
level terminal state denoted actual means terminating stochastic state activation process 
terminal state reached control returns parent state hierarchy 
generation observation sequence completed control recursive activations returned root state 
assume stats reached finite number steps root state model strongly connected 

inference learning case hmms natural problems typically arise applications hhmms hierarchical hidden markov model calculating likelihood sequence hhmm parameter set find probability oj sequence generated model finding probable state sequence hhmm parameter set observation sequence find single state activation sequence generate observation sequence 
estimating parameters model structure hhmm observation sequences find probable parameter set model arg max gj 
solutions problems hhmms involved hmms due hierarchical structure multi scale properties 
instance probable state sequence observation sequence multi resolution structure state activations simple sequence indices probable states reached 
solutions problems starting simplest 
terminology say state started operation time possibly empty sub sequence delta delta delta gamma generated activated parent state symbol generated production states reached analogously say state finished operation time symbol generated production states reached control returned 
calculating likelihood sequence internal states hhmm viewed autonomous model generate substring observation substates efficient likelihood evaluation procedure recursive 
state calculate likelihood generating substring denoted 
assume moment probabilities provided root state indices states second level visited generation observation sequence ot length note state entered second level temporal position symbol generated state entire list indices denoted 
activated time step state second level activated likelihood entire sequence information oj delta delta delta gamma jq delta delta delta gamma jq delta delta delta gamma delta delta delta gamma gamma jq gamma gamma gamma gamma delta delta delta ot jq gamma gamma fine singer tishby order calculate unconditioned likelihood need sum possible switching times state indices clearly feasible exponentially combinations 
fortunately structure hhmms enables dynamic programming devise generalized version forward backward algorithm 
generalized forward probabilities ff delta defined ff gamma delta delta delta finished gamma started ff gamma probability partial observation sequence delta delta delta generated state gamma state activated gamma generation delta delta delta note operation substate gamma necessarily time delta delta delta prefix longer sequence generated gamma calculate probability sequence delta delta delta generated gamma need sum possible states level gamma delta delta delta jq gamma jq gamma ff gamma gamma likelihood observation sequence obtained summing possible starting states called root state oj jq ff definition generalized ff variables states level gamma ff gamma equivalent definition forward variable ff hmm consists level output probability vectors defined production states evaluation ff variables done recursive bottom manner ff values calculated substates internal state determine ff values summary internal state need calculate ff value possible subsequence observation sequence recursive decomposition subsequence ff values substates 
time complexity evaluating ff values states hhmm nt total number states length observation sequence 
similar manner generalized backward variable fi defined fi gamma delta delta delta started gamma finished detailed description calculation ff fi provided appendix 
finding probable state sequence probable state sequence multi scale list states state generated string delta delta delta parent state generated string delta delta delta hierarchical hidden markov model string delta delta delta subdivided substates state non overlapping subsequences 
list computed efficiently line reasoning derive ff variables replacing summation maximization 
process finds probable state sequence hmms known viterbi algorithm viterbi term modified algorithm hhmms generalized viterbi algorithm 
similar definition ff variables define ffi gamma likelihood probable hierarchical state sequence generating delta delta delta gamma entered time substate state activated control returned time interested actual hierarchical parsing sequence states maintain additional variables index probable state activated gamma activating time activated variables probable hierarchical state sequence obtained scanning lists root state production states 
breadth search scanning states listed level index top bottom 
depth search states listed activation time 
simply replaced summation maximization time complexity generalized viterbi algorithm time generalized forwardbackward nt 
pseudo code describing algorithm appendix devised heuristic finds approximation probable state sequence nt time 
heuristic assumes distributions sequences induced different states substantially different 
influence horizontal transitions finding probable state sequence negligible 
conduct approximated search ignores transition probabilities 
words treat state hhmm autonomous model ignoring influence neighboring states level maximization operation performed internal node reducing running time nt 
theoretical justification approximation experiments probable state sequence approximated search greatly resembles state sequence exact generalized viterbi algorithm see experiments described section 

estimating parameters hhmm maximum likelihood parameter estimation procedure hhmms generalization forward backward algorithms need consider stochastic vertical transitions recursively generate observations 
addition path variables ff fi correspond forward backward transitions add additional path variables correspond downward upward transitions 
variables expectation step follows fine singer tishby gamma probability performing horizontal transition substates gamma time production production gamma delta delta delta gamma 
delta delta delta ot define auxiliary variables fl fl simplify reestimation step fl gamma probability performing horizontal transition state generated 
fl calculated summing substates gamma perform horizontal transition fl gamma jq gamma gamma gamma fl gamma probability leaving state performing horizontal transition states level generation analogous fl fl calculated summing substates gamma reached single horizontal transition fl gamma jq gamma gamma path variable estimate probability vertical transition 
gamma probability state gamma entered time generated initially chose activate state gamma delta delta delta gamma gamma delta delta delta ot path variables current set parameters expectations calculated gamma gamma expected number horizontal transitions substates gamma fl gamma jq gamma gamma gamma expected number horizontal transitions state states level gamma fl gamma jq gamma gamma gamma expected number horizontal transition state states level hierarchical hidden markov model gamma expected number vertical transitions gamma jq gamma gamma gamma expected number vertical transitions gamma substates level gamma fl gamma gamma fl gamma expected number vertical transitions production state state gamma complete derivation fl fl appendix expectations calculated current parameters new set parameters re estimated follows gamma gamma jq gamma gamma gamma ij gamma jq gamma gamma gamma fl gamma gamma vk gamma vk fl gamma gamma fl gamma order find set parameters iterate expectation step calculates auxiliary path variables equ 
find new estimate parameters 
tedious fairly simple verify steps iterative procedure correspond expectation maximization steps em algorithm 
procedure guaranteed converge stationary point typically local maximum likelihood function 

applications section discuss give examples hhmms parameter estimation complex sequence modeling tasks building multi level structure english text unsupervised identification repeated strokes cursive handwriting 

multi level structure english text primary goals stochastic analysis complex sequences natural text design model captures correlations events appearing far apart sequence 
observable markov models widely tasks see instance ron 
states models constructed observable sub sequences fine singer tishby capture implicit long distance statistical correlations 
suggest alternative approach hhmms give experimental evidence approach able partly overcome difficulty 
built hhmms trained natural text consisting classical english stories 
observation alphabet included lower upper case letters blanks punctuation marks 
training approximately sentences average length characters 
trained hhmms exactly text 
hhmms follows ffl shallow hhmm consisting levels 
root state hhmm substates 
states second level substates production states 
production states level 
structure hhmm shown top part 
ffl unbalanced hhmm consisting levels 
hhmm variable number substates internal state 
structure hhmm unbalanced production states levels 
illustrations second hhmm bottom part 
applied generalized forward backward parameter estimation procedure hhmms 
training distributions strings induced substates hhmm greatly resembled distribution induced standard hmm trained data 
contrast distribution induced second hhmm substantially different revealed interesting phenomena 
distribution induced second hhmm greatly varied different substates 
sets strings probable produced states turned little overlap 
second observed multi scale behavior states 
specifically probable strings produced deep states roughly correspond phonetic units strings ing th wh ou 
going hierarchy states second third level produce strings frequent words phrases 
top hierarchy root state induced distribution corresponds sentence scale 
instance strings produced root state entire hhmm punctuation mark 
horizontal transition probabilities training unbalanced hhmm got highly peaked 
reflects strong markov dependencies states level 
distribution induced state highly concentrated strings set strings generated recursive activations deep hhmm strongly biased syntactic structures frequently appear natural texts 
demonstrate application trained hhmm submodels building block complex tasks text classification 
highly biased distributions illustrated figures 
give horizontal transition probabilities training 
list probable strings produced substate hhmms deep unbalanced hhmm bottom hierarchical hidden markov model shallow balanced hhmm top 
clear richer model developed larger variety strings include words fragments sentences 

unsupervised learning cursive handwriting singer tishby dynamic encoding scheme cursive handwriting oscillatory model handwriting proposed analyzed 
scheme performs inverse mapping continuous pen trajectories strings discrete set symbols efficiently encode cursive handwriting 
symbols named motor control commands 
motor control commands transformed back pen trajectories generative model handwriting reconstructed noise eliminated dynamic encoding scheme 
possible control command composed cartesian product form theta alphabet consists different symbols 
symbols represent quantized horizontal vertical amplitude modulation phase lags 
different roman letters map different sequences symbols 
different writing styles due existence noise human motor system cursive letter written different ways 
results different encodings represent written word 
desirable step system analyzes recognizes cursive scripts build stochastic models approximate distribution sequences correspond complete cursive pen trajectories 
motor control commands observation alphabet built hhmms corresponding different cursive words training set 
example examples word maintain estimate parameters hhmm levels 
hhmm unbalanced structure production states levels 
design topology hhmms took account additional knowledge repetitions letters combination letters written words 
structure hhmm word maintain shown 
generalized forward backward algorithm estimate parameters hhmm 
trained hhmm identify repeated strokes represent combination letters cursive handwriting 
order verify resulting hhmms learned distribution internal structure words generalized viterbi algorithm hhmms perform multi scale segmentation motor control sequences 
example result segmentation 
cursive word maintain reconstructed motor control commands shown hierarchical segmentation 
trained hhmm depth structure shown segment word 
show temporal segmentation states hierarchies hhmm 
clear viterbi algorithm assigned different states hhmm different cursive strokes 
fur fine singer tishby 
transition distribution top training unbalanced hhmm depth trained english texts 
initial distribution uniform final distribution sharply peaked different states different levels 
hierarchical hidden markov model th th st se ct th ich re pr pre re ll th en er rt ff st ou ne ng hi ha ly le pe re tha nce se sh si ou ct st pe re dir hat stat eat clt cut ir tha fo und ni tui ss st ght dir di der wish ow ca sa tha ich man may men 
probable strings generated state different hhmms 
fine singer tishby root 
hhmm cursive handwriting spotting experiments 
full model trained complete words 
submodel denoted dotted line locate occurrences letter combination ai 
thermore state consistently generate strokes represent letter combination 
instance state hierarchy responsible producing combination ai 
substates split letters ai sub strokes substate denoted generates part cursive letter second generates middle part articulated stroke connecting letters third generates letter similar phenomena observed states hhmm 
common question arises stochastic modeling sequences speech signals handwritten text natural units constitute sequences 
widely approach manually define units phonemes hierarchical hidden markov model 
hierarchical segmentation word maintain obtained viterbi algorithm hhmms 
temporal segmentation levels hhmm shown 
word reconstructed encoded dynamical representation 
spoken language letters written text drawbacks approach requires manual segmentation take account temporal interaction articulation speech consecutive units 
propose briefly demonstrate alternative approach uses substates trained hhmm provide partial answer question 
due self similar structure hhmms substate autonomous model 
substates second level hhmm described calculated probabilities induce sub sequence observation sequence 
defined simple hmm induce uniform distribution possible symbols 
simple model denoted serves null hypothesis competes submodels pulled full hhmm 
probability subsequence generated submodel compared null hypothesis assuming equal prior alternatives high values indicate occurrence letters correspond pulled model thresholding value identify locations combination letters unsegmented data 
example result letter combination spotting hhmm 
show logarithm conditional probability normalized length string oj 
probability calculated possible start locations 
submodel corresponding combination ai submodel rooted state denoted dotted line pulled hhmm constructed word maintain 
clearly occurrences letters ai correctly located 
combination letters oi word pointers received high likelihood 
ambiguities resolved refining set submodels employing higher level stochastic language model 
fine singer tishby 
spotting occurrences letters ai sentence maintained chain pointers 
submodel corresponding combination letters pulled hhmm shown built word maintain 
occurrences letters letting model compete null hypothesis induces uniform distribution possible symbols 

hierarchical hidden markov models generalization hmms provide partial answer fundamental problems arise complex sequence modeling 
hhmms able correlate structures relatively far apart observation sequences maintaining simplicity computational tractability simple markov processes 
second able handle statistical inhomogeneities common speech natural language 
maximum likelihood parameter estimation procedure viterbi probable state decoding naturally generalized richer structure 
missing component hhmms lack ability adapt topology allow self organized merging growth submodels 
natural generalization hhmms 
instance framework introduced bengio frasconi input output hmms hierarchical hmms generalized describe input output mappings strings different alphabets 
experiments hhmms described initial step better understanding hierarchical stochastic models natural complex sequences 
models factorial hidden markov models ghahramani jordan alternative parameter estimation techniques singer warmuth :10.1.1.33.721:10.1.1.16.2929
understanding connections different approaches conducting formal analysis hierarchical stochastic modeling important research direction progress 
hierarchical hidden markov model appendix generalized forward backward algorithm remind reader calculate path variables expectation step ff fi informally correspond forward backward downward upward stochastic transitions hhmm 
variables ff fi calculated bottom manner probability induced state depends substates belong tree submodel rooted variables ff fi variables calculated top manner 
simplify derivation path variables define auxiliary variables give detailed derivation variables 
definition ff gamma delta delta delta finished gamma started estimation ff gamma gamma gamma ff gamma jq gamma ff gamma gamma gamma ji gamma ff gamma gamma jq ff ff gamma gamma jq gamma ff gamma gamma ji jq ff gamma jq ff definition fi gamma delta delta delta started gamma finished estimation fi gamma gamma gamma fi gamma gamma jq gamma gamma ij fi gamma fine singer tishby fi gamma jq fi fi gamma gamma jq fi jq gamma gamma ij fi gamma jq fi gamma definition gamma delta delta delta gamma started tj estimation jq ff gamma ji gamma gamma gamma gamma gamma gamma gamma gamma jq gamma ff gamma gamma gamma ji gamma gamma gamma definition gamma finished delta delta delta ot estimation jq ij fi gamma jq gamma gamma ij fi gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma hierarchical hidden markov model definition gamma finished started delta delta delta gamma 
delta delta delta estimation ff ij fi oj ff ij oj gamma oj gamma gamma ff gamma gamma ij fi gamma gamma gamma gamma oj gamma gamma ff gamma gamma gamma gamma definition gamma started tj delta delta delta gamma gamma delta delta delta ot estimation fi oj gamma gamma gamma gamma oj fi gamma gamma gamma fine singer tishby appendix generalized viterbi algorithm remind reader pair states gamma keep variables ffl ffi gamma likelihood probable state sequence generating delta delta delta assuming solely generated recursive activation started time step state gamma ended returned gamma time step ffl gamma index probable state activated gamma state exist delta delta delta solely generated set gamma def 
ffl gamma time step probable called gamma generated entire subsequence set gamma simplify notation define functional max parameters function finite set max ff def max ff arg max ff generalized viterbi algorithm starts production states calculate ffi bottom manner follows 
production states 
initialization ffi gamma gamma gamma gamma 
recursion gamma ffi gamma gamma delta max jq gamma ffi gamma gamma gamma ji gamma internal states 
initialization ffi gamma max jq gamma ffi gamma gamma hierarchical hidden markov model 
recursion set max rjq ffi delta psi max jjq gamma ffi gamma gamma gamma ji set delta gamma max rjq gamma ffi psi find probable switching time gamma ffi gamma gamma delta max tt delta gamma psi gamma gamma delta probability probable state sequence follows gamma delta max phi ffi psi probable states sequence scanning lists starting 
fine singer tishby appendix table 
list symbols variables symbol definition section sigma finite alphabet sigma observation sequence dg hierarchy depth ith substate level pi initial substate distribution fp jq fa ij substate transition probabilities fp jq fb output probability distribution fp oe jq ffa gf pi gg hhmm set parameters ff gamma delta delta delta finished gamma started fi gamma delta delta delta started gamma finished gamma delta delta delta gamma 
delta delta delta fl gamma fl gamma jq gamma gamma gamma fl gamma fl gamma jq gamma gamma gamma delta delta delta gamma gamma delta delta delta gamma delta delta delta gamma started tj gamma finished delta delta delta ffi gamma value ffi state list transition times gamma probable generation delta delta delta gamma started gamma ended hierarchical hidden markov model acknowledgments bengio raj iyer helpful comments 
abe warmuth 
computational complexity approximating distributions probabilistic automata 
machine learning 
baldi chauvin mcclure 
hidden markov models biological primary sequence information 
proc 
nat 
acd 
sci 
usa 
baum petrie 
statistical inference probabilistic functions finite state markov chains 
annals mathematical statistics vol 

bengio frasconi 
input output hmm architecture 
tesauro touretzky leen editors advances neural information processing systems pages 
mit press cambridge ma 
cover thomas 
elements information theory 
wiley 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
ghahramani jordan 
factorial hidden markov models 
machine learning appear 
sipser 
inference minimization hidden markov chains 
proceedings seventh annual workshop computational learning theory pages 
jelinek 
robust part speech tagging hidden markov model 
ibm watson research technical report 
jelinek 
markov source modeling text generation 
technical report ibm watson research center technical report 
jelinek 
self organized language modeling speech recognition 
ibm watson research center technical report 
krogh mian haussler 
hidden markov model finds genes coli dna 
nar 
lari young 
estimation stochastic context free grammars algorithm 
computers speech language 
nag wong fallside 
script recognition hidden markov models 
proceedings international conference acoustics speech signal processing pages 
rabiner juang 
hidden markov models 
ieee assp magazine 
rabiner 
tutorial hidden markov models selected applications speech recognition 
proceedings ieee 
rissanen 
complexity strings class markov sources 
ieee transactions information theory 
ron singer tishby 
power amnesia learning probabilistic automata variable memory length 
machine learning 
singer tishby 
dynamical encoding cursive handwriting 
biological cybernetics 
singer warmuth 
training algorithms hidden markov models entropy distance functions 
mozer jordan petsche editors advances neural information processing systems pages 
mit press cambridge ma 
stolcke omohundro 
best model merging hidden markov model induction 
technical report icsi tr 
viterbi 
error bounds codes asymptotically optimal decoding algorithm 
ieee transactions information theory 
