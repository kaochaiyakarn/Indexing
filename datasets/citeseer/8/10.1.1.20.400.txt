kernel logistic regression import vector machine ji zhu department statistics stanford university stanford ca stat stanford edu trevor hastie department statistics stanford university stanford ca hastie stat stanford edu support vector machine svm known performance binary classification extension multi class classification going research issue 
propose new approach classification called import vector machine ivm built kernel logistic regression klr 
show ivm performs svm binary classification naturally generalized multi class case 
furthermore ivm provides estimate underlying probability 
similar support points svm ivm model uses fraction training data index kernel basis functions typically smaller fraction svm 
gives ivm computational advantage svm especially size training data set large 
standard classification problems set training data yn output qualitative assumes values finite set wish find rule training data new input assign class 
usually assumed training data independently identically distributed sample unknown probability distribution 
support vector machine svm works binary classification appropriate extension multi class case going research issue 
weakness svm estimates sign probability interest jx conditional probability point class propose new approach called import vector machine ivm address classification problem 
show ivm performs svm binary classification naturally generalized multi class case 
furthermore ivm provides estimate probability 
similar support points svm ivm model uses fraction training data index kernel basis functions 
call training data import points 
computational cost svm computational cost ivm number import points 
tend increase increases ivm faster svm especially large training data sets 
empirical results show number import points usually number support points 
section briefly review results svm binary classification compare kernel logistic regression klr 
section propose ivm algorithm 
section show simulation results 
section generalize ivm multi class case 
support vector machines kernel logistic regression standard svm produces non linear classification boundary original input space constructing linear boundary transformed version original input space 
dimension transformed space large infinite cases 
seemingly prohibitive computation achieved positive definite reproducing kernel gives inner product transformed space 
people noted relationship svm regularized function estimation reproducing kernel hilbert spaces rkhs 
overview 
hastie 
wahba 
fitting svm equivalent minimizing kfk hk hk hk rkhs generated kernel classification rule sign 
representer theorem kimeldorf optimal form happens sizeable fraction values zero 
consequence truncation property part criterion 
attractive property points wrong side classification boundary right side near boundary influence determining position boundary non zero 
corresponding called support points 
notice form loss penalty 
loss function yf plotted traditional loss functions 
see negative log likelihood nll binomial distribution similar shape svm 
replace yf ln yf nll binomial distribution problem klr problem 
expect fitted function performs similarly svm binary 
immediate advantages making replacement giving classification rule klr offers natural estimate probability svm estimates sign klr naturally generalized multi class case kernel multi logit regression case svm 
klr compromises hinge loss function svm longer support points property words non zero 
klr studied problem see wahba 
see green 
hastie 

yf loss binomial nll squared error support vector loss functions computational cost klr save computational cost ivm algorithm find sub model approximate full model klr 
sub model form subset training data fx xn data called import points 
advantage sub model computational cost reduced especially large training data sets performance classification 
researchers investigated techniques selecting subset lin 
divide training data clusters randomly select representative cluster smola 
greedy technique sequentially select columns kernel matrix span columns approximates span frobenius norm 
williams 
propose randomly selecting points training data nystrom method approximate eigen decomposition kernel matrix expanding results back dimensions 
methods uses output selecting subset procedure involves 
ivm algorithm uses output input select subset way resulting fit approximates full model 
import vector machine tradition logistic regression rest 
notational simplicity constant term fitted function ignored 
klr want minimize ln exp kfk hk shown equivalent finite dimensional form ln exp regressor matrix regularization matrix find set derivative respect equal method iteratively solve score equation 
shown step weighted squares step wk value kth step 
weight matrix diag mentioned section want find subset fx xn sub model approximation full model 
impossible search subset greedy forward strategy basic algorithm fx xn 
fx find minimize ln exp kf hk ln exp regressor matrix fx xn fx regularization matrix fx jsj 
argmin fx fx 
repeat steps converges 
call points import points 
revised algorithm algorithm computationally feasible step need newton raphson method find iteratively 
number import points large newton raphson computation expensive 
reduce computation approximation 
iteratively computing converges just step iteration approximation converged 
get approximation take advantage fitted result current optimal sub model jsj initial value 
step update similar score test generalized linear models glm penalty term 
updating formula allows weighted regression computed nq time 
revised step basic algorithm correspondingly augment column column row 
updating formula find 
compute 
stopping rule adding point step basic algorithm need decide converged 
natural stopping rule look regularized nll 
sequence regularized nll obtained step 
step compare pre chosen small integer example 
ratio jhk hk jhk pre chosen small number example adding new import points choosing regularization far assumed regularization parameter fixed 
practice need choose optimal 
randomly split data training set tuning set misclassification error tuning set criterion choosing 
reduce computation take advantage fact regularized nll converges faster larger 
running entire revised algorithm propose procedure combines adding import points choosing optimal start large regularization parameter 
fx xn 
run steps revised algorithm stopping criterion satisfied fx iq way compute error tuning set 
decrease smaller value 
repeat steps starting fx iq choose optimal corresponds minimum misclassification error tuning set 
simulation section simulation illustrate ivm method 
data class generated mixture gaussians hastie 

simulation results shown 
remarks support points svm close classification boundary misclassified usually large weights 
import points ivm decrease regularized nll close far classification boundary 
difference natural svm concerned classification sign ivm focuses unknown probability 
points away classification boundary contribute determining position classification boundary may contribute estimating unknown probability 
shows comparison svm ivm 
total computational cost svm computational cost ivm method number import points 

import points added regularized nll different lambda 
import points added misclassification rate different lambda 
import points added regularized nll optimal lambda radial kernel 

left middle panels illustrate choose optimal 
decreases minimum misclassification rate correspond 
right panel optimal 
stopping criterion satisfied jsj 
tend increase increases computational cost ivm smaller svm especially large training data sets 
multi class case section briefly describe generalization ivm multi class classification 
suppose classes 
write response vector component indicating class observation 
indicates response kth class indicates response th class 
th class basis multi logit written ln pm fm ln pm fm 
bayes classification rule argmax index observations index classes regularized negative log likelihood ln 
fm kfk hk yim fm kfk hk kf hk representer theorem kimeldorf 
jth element minimizes form ij svm support points 

training error test error bayes error ivm import points 

training error test error bayes error solid lines classification boundaries dotted lines bayes rule boundaries 
svm dashed lines edges margin 
ivm dashed lines lines 
ln ka ij defined way binary case ith row multi class ivm procedure similar binary case computational cost mn 
simulation multi class ivm 
data class generated mixture gaussians hastie 

multi class ivm import points 
xxxx xxxxxxxxxxxx xxxxxxxx xxxx xx xxxxxx training error test error bayes error radial kernel 
jsj 
discussed import vector machine ivm method binary multi class classification 
showed performs svm provides estimate probability 
computational cost ivm binary case mn multi class case number import points 
acknowledgments dylan small john storey rob tibshirani yan helpful comments 
ji zhu partially supported stanford graduate fellowship 
trevor hastie partially supported dms national science foundation roi ca national institutes health 
grace wahba chris williams pointing interesting important 
want anonymous nips referees helped improve 
burges 
tutorial support vector machines pattern recognition 
data mining knowledge discovery 
kluwer academic publishers boston 
volume pontil poggio 
regularization networks support vector machines 
smola bartlett scholkopf schuurmans editors advances large margin classifiers 
mit press 
green 
semi parametric generalized linear models 
proceedings nd international conference lancaster lecture notes statistics springer verlag new york 
hastie tibshirani 
generalized additive models chapman hall 
hastie tibshirani friedman elements statistical learning 
print 
lin wahba xiang gao klein klein 
smoothing spline anova models large data sets bernoulli observations randomized 
technical report department statistics university wisconsin madison wi 
kimeldorf wahba 
results spline functions 
math 
anal 
applic 

smola scholkopf 
sparse greedy matrix approximation machine learning 
proceedings seventeenth international conference machine learning 
morgan kaufmann publishers 
wahba 
support vector machine reproducing kernel hilbert spaces randomized 
technical report rr department statistics university wisconsin madison wi 
wahba gu wang chappell 
soft classification risk estimation penalized log likelihood smoothing spline analysis variance 
wolpert editor mathematics generalization 
santa fe institute studies sciences complexity 
addison wesley publisher 
williams seeger nystrom method speed kernel machines 
leen tresp editors advances neural information processing systems 
mit press 
