kernel trick distances bernhard sch olkopf microsoft research street cambridge uk bs tuebingen mpg de method described kernel trick support vector machines svms lets generalize distance algorithms operate feature spaces usually nonlinearly related input space 
done identifying class kernels represented norm distances hilbert spaces 
turns common kernel algorithms svms kernel pca really distance algorithms run class kernels 
providing useful new insight algorithms form basis conceiving new algorithms 
crucial ingredients svms called kernel trick computation dot products high dimensional feature spaces simple functions defined pairs input patterns 
trick allows formulation nonlinear variants algorithm cast terms dot products svms prominent example 
mathematical result underlying kernel trick century old fruitful machine learning community 
kernel methods led interesting generalizations learning algorithms successful real world applications 
attempts extend utility kernel trick looking problem kernels compute distances feature spaces 
underlying mathematical results mainly due schoenberg known attracted interest kernel methods community various contexts :10.1.1.21.2820
consider training data ym set possible outputs pattern recognition nonempty set domain patterns taken 
interested predicting outputs previously unseen patterns possible measure tells related training examples 
problems approach works informally want similar inputs lead similar outputs 
formalize state mean similar 
outputs similarity usually measured terms loss function 
instance case pattern recognition situation simple outputs identical different 
inputs notion similarity complex 
hinges representation patterns suitable similarity measure operating representation 
particularly simple surprisingly useful notion dis similarity derives embedding data euclidean space utilizing geometrical concepts 
instance svms similarity measured dot products angles lengths high dimensional feature space formally patterns mapped 
compared dot product avoid working potentially high dimensional space tries pick feature space dot product evaluated directly nonlinear function input space means kernel trick simply chooses kernel property exists holds true necessarily worrying actual form existence linear space facilitates number algorithmic theoretical issues 
established works mercer kernels equivalently positive definite kernels 
indices default run definition positive definite kernel symmetric function gives rise positive definite gram matrix xm ij ij called positive definite pd kernel 
particularly intuitive way construct feature map satisfying kernel proceeds nutshell follows details see 
define feature map 
denotes space functions mapping 
turn linear space forming linear combinations 
endow dot product hf gi turn hilbert space completing corresponding norm 
note particular definition dot product hk view kernel trick 
shows pd kernels thought nonlinear generalizations simplest similarity measures canonical dot product hx question arises exist generalizations simplest dissimilarity measure distance kx clearly distance feature space associated pd kernel computed kernel trick 
positive definite kernels full story exists larger class kernels generalized distances section describe 
kernels generalized distance measures start considering dot product corresponding distance measure affected translation data 
clearly kx translation invariant hx 
short calculation shows effect translation expressed terms kx kx kx 
note just hx pd kernel 
choice get similarity measure associated dissimilarity measure kx naturally leads question suggest connection holds true general cases kind nonlinear dissimilarity measure substitute right hand side ensure left hand side positive definite 
answer known result 
state need define appropriate class kernels 
definition conditionally positive definite kernel symmetric function satisfies xm called conditionally positive definite cpd kernel 
proposition connection pd cpd symmetric kernel positive definite conditionally positive definite 
proof follows directly definitions 
result generalize negative squared distance kernel cpd implies kx kx kx hx hx fact implies kernels form kx cpd pd application result proposition cpd log 
state class cpd kernels pd note trivial consequences definition know sums cpd kernels cpd ii constant cpd 
kernel form cpd cpd 
particular pd kernels cpd take pd kernel offset cpd 
examples cpd kernels cf 
:10.1.1.48.9258
return main flow argument 
proposition allows construct feature map pd kernel fix define 
due proposition positive definite 
may employ hilbert space representation cf 
satisfying substituting yields proven result 
proposition hilbert space representation cpd kernels realvalued conditionally positive definite kernel satisfying exists hilbert space real valued functions mapping drop assumption hilbert space representation reads shown semi metric metric 
show represent general symmetric kernels particular cpd kernels symmetric bilinear forms feature spaces 
generalization previously known feature space representation pd kernels comes cost longer dot product 
purposes get away 
result give intuitive understanding proposition write 
proposition essentially adds origin feature space corresponds image point feature map 
translation invariant algorithms allowed turn cpd kernel pd sense cpd kernels pd kernels 
proposition vector space representation symmetric kernels realvalued symmetric kernel exists linear space real valued functions endowed symmetric bilinear form mapping proof proof direct modification pd case 
map linearly complete image 
define 
see defined explicitly contains expansion coefficients need unique note independent similarly note independent equations show bilinear clearly symmetric 
note definition reproducing kernel feature space hilbert space functions particular rewriting suggests immediate generalization proposition practice want choose points origins feature space points preimage input space usually mean set points cf 

useful considering kernel pca 
crucial point behaviour translations identical individual points 
taken care constraint sum proposition 
asterisk denotes complex conjugated transpose 
proposition exercise symmetric matrix vector ones identity matrix satisfy 
ec ce positive definite conditionally positive definite 
proof suppose positive definite ka ka ec ec ka case cf 
terms vanish ka proving conditionally positive definite 
suppose conditionally positive definite 
map ce range orthogonal complement seen computing ce ce symmetric satisfying ce ce map ce projection 
restriction orthogonal complement definition conditional positive definiteness precisely space positive definite 
result directly implies corresponding generalization proposition proposition adding general origin symmetric kernel xm satisfy 
positive definite conditionally positive definite 
proof consider set points gram matrix xm apply proposition 
example svms kernel pca results show conditionally positive definite kernels natural choice dealing translation invariant problem svm maximization margin separation classes data independent origin position 
seen light surprising structure dual optimization problem cf 
allows cpd kernels noticed constraint projects subspace definition conditionally positive definite kernels 
ii example kernel algorithm works conditionally positive definite kernels kernel pca data centered removing dependence origin feature space 
formally follows proposition example parzen windows simplest distance classification algorithms conceivable proceeds follows 
points labelled points labelled test point compute mean squared distances classes assign mean smaller sgn distance kernel trick proposition express decision function kernel expansion input space short calculation shows sgn constant offset 
note cpd kernels 
commonly gaussian kernel nonzero constant case vanishes 
normalized gaussians kernels valid density models resulting decision boundary interpreted bayes decision parzen windows density estimates classes general cpd kernels analogy mere formal 
example toy experiment fig 
illustrate finding kernel pca carried cpd kernels 
kernel 
due centering built kernel pca cf 
example ii case equivalent linear pca 
decrease obtain increasingly nonlinear feature extractors 
note kernel parameter gets smaller weight put large distances get localized feature extractors sense regions large gradients dense sets contour lines plot get localized 
kernel pca toy dataset cpd kernel contour plots feature extractors corresponding projections principal axes feature space 
left right 
notice smaller values feature extractors increasingly nonlinear allows identification cluster structure 
described kernel trick distances feature spaces 
generalize distance algorithms feature space setting substituting suitable kernel function squared distance 
class kernels larger commonly kernel methods known positive definite kernels 
argued reflects translation invariance distance algorithms opposed genuinely dot product algorithms 
svms kernel pca translation invariant feature space really distance dot product 
argued conditionally positive definite kernels 
case svm drops optimization problem automatically case kernel pca corresponds point feature space 
contribution identifies translation invariance underlying reason enabling cpd kernels larger class kernel algorithms draws learning community attention kernel trick distances 
acknowledgments 
part done author visiting australian national university 
nello cristianini ralf herbrich sebastian mika klaus muller john shawe taylor alex smola mike tipping chris watkins bob williamson chris williams anonymous reviewer valuable input 
aizerman braverman 
theoretical foundations potential function method pattern recognition learning 
autom 
remote contr 
berg christensen 
harmonic analysis semigroups 
springer verlag new york 
boser guyon vapnik 
training algorithm optimal margin classifiers 
haussler editor proceedings th annual acm workshop computational learning theory pages pittsburgh pa july 
acm press 
girosi jones poggio 
regularization theory neural networks architectures 
neural computation 
haussler 
convolutional kernels discrete structures 
technical report ucsc crl computer science department university california santa cruz 
mercer 
functions positive negative type connection theory integral equations 
philos 
trans 
roy 
soc 
london 
schoenberg 
metric spaces positive definite functions 
trans 
amer 
math 
soc 
scholkopf burges smola 
advances kernel methods support vector learning 
mit press cambridge ma 
scholkopf smola 
muller 
nonlinear component analysis kernel eigenvalue problem 
neural computation 
smola scholkopf 
semiparametric support vector linear programming machines 
kearns solla cohn editors advances neural information processing systems pages cambridge ma 
mit press 
smola scholkopf 
muller 
connection regularization operators support vector kernels 
neural networks 
torgerson 
theory methods scaling 
wiley new york 
vapnik 
nature statistical learning theory 
springer 
wahba 
spline models observational data volume cbms nsf regional conference series applied mathematics 
siam philadelphia 
watkins 
personal communication 
