scaling multi class support vector machines inter class confusion iit bombay iitb ac support vector machines svms excel class discriminative learning problems 
outperform generative classifiers especially inaccurate generative models na bayes nb classifier 
hand generative classifiers trouble handling arbitrary number classes efficiently nb classifiers train faster svms owing extreme simplicity 
contrast svms handle multi class problems learning redundant vs classifiers class worsening performance gap 
propose new technique multi way classification exploits accuracy svms speed nb classifiers 
nb classifier quickly compute confusion matrix reduce number complexity class svms built second stage 
testing get prediction nb classifier selectively apply subset class svms 
standard benchmarks algorithm times faster svms matches exceeds accuracy 

support vector machines svms kind discriminative classifier shown superb performance classifying text data 
accurate robust quick apply test instances 
inducing linear svm training data xi yi 
xi yi involves estimating vector scalar maximize distance training point hyperplane defined written minimize subject yi xi 
distance training point optimized hyperplane called margin 
notice class labels 
holds nonlinear svms 
elegant theory large margin hyper permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
sigkdd edmonton alberta canada copyright acm 
sunita sarawagi iit bombay sunita iitb ac soumen chakrabarti iit bombay soumen cse iitb ac planes separate classes extended easily separate mutually exclusive classes 
number methods proposed reducing multi class problem collection class problems combining predictions various ways 
popular vs approach classes construct vs svm judgment class 
test instance svms say rest say 
winning svm says decision hyperplane farthest test instance competing svms 
vs technique requires classifiers built entire training data causing document processed times 
test instance evaluated svm 
apart efficiency issue unclear comparison discriminant functions different classification problems meaningful way technique reasonably practice 
technique construct svms pos sible pairs classes 
testing classifiers votes class 
winning class largest number accumulated votes 
technique number training instances classifier limited number classifiers quadratic number classes document gets processed times 
number proposed methods fall methods :10.1.1.72.7289:10.1.1.10.5265
discuss section 
vs method widely practice offer high accuracy 
approach multi class svms experiments 
regardless specific details ad hoc techniques impractical large number classes especially svms accurate quick apply fastest learners train instance problem time taken clever implementations ranges 
factors quite difficult scale svm systems large web directories open directory project dmoz org yahoo 
www yahoo com tens thousands classes millions documents 
generative classifiers contrast essentially independent number classes far training time concerned 
popular generative classifier text data na bayes nb classifier 
nb classifiers trivially scale number classes process document independent number classes 
nb classifiers train faster svms owing extreme simplicity 
hand terms accuracy linear svm decisively outperformed nb classifier owing high bias assuming attribute independence 
method tries achieve best worlds scalability nb classifiers number classes accuracy svms 
stage fast multi class nb classifier compute confusion matrix reduce number complexity class svms built second stage vs approach 
testing get prediction nb classifier selectively apply subset class svms indicated confusion matrix 
standard benchmarks algorithm times faster multi class svms superior scalability terms memory requirements training set size 
terms accuracy method better nb classifiers comparable superior svms 
proposed algorithm simple understand requires negligible coding 
substantial utility dealing large classifiers required web directories 

approach genesis approach lies class relationships derived confusion matrix easily generated fast classifier nb 
obtained held validation dataset 
newsgroups confusion matrix example show example confusion matrix built newsgroup dataset details section 
rows show actual classes columns show predicted classes 
matrix clearly shows different classes different degrees confusion classes 
classes rec sport hockey separated rest comp os ms windows misc easily confused 
mis classifications class usually limited small subset classes 
fact cases rows columns matrix rearranged manually shown reveal clusters classes confuse 
appear blocks diagonal confusion matrix 
ingly cases clusters formed classes names immediately recognized forming natural hierarchies 
confusion matrix provides domainindependent method deriving relationship 
automating re organization classes clusters similar classes technique automatically generate topic hierarchies flat set classes 
class represented row confusion matrix 
class respective row converted normalized dimensional vector denotes class confuses classes 
distance measure euclidean ln kl distance measure compute distance classes 
distances cluster classes hierarchical agglomerative clustering hac algorithm 
output hac dendrogram analyze determine clusters provide maximum inter class separation 
dendrogram scanned bottom find distances successive clusters get merged 
clip dendrogram point cluster merge distances increasing sharply 
number clusters left clipping form clusters level hierarchy 
newsgroups dataset method gives clusters similar 
newsgroups re organized confusion matrix 
hierarchical approach propose exploit clustering classes prune number complexity class classifiers needed multi class svms 
obvious approach arrange clusters level tree hierarchy train classifier internal node 
restrict nb classifiers mitchell shown feature space classifiers smoothing done accuracy flat classifier 
practice classifier deal easily separable problem independently optimized feature set lead slight improvements accuracy apart gain training testing speed 
propose combination nb svm classifiers levels 
build top level classifier discriminate top level clusters labels called level classifier 
top level classifier nb svm classifier 
svm training time smaller number classes reduced method accuracy mcsvm nb svm nb nb nb perfect svm nb svm perfect table level wise comparisons newsgroups data class svm need documents 
build multi class svms cluster classes 
total number svms second level close number classes svm significantly reduced 
classifier concentrate smaller set classes confuse 
generative classifier nb expect result better feature selection enable finer distinctions confusing classes 
svms spread negative class reduced expect separability easier better 
evaluation hierarchical approach evaluate accuracy training time solving multi class problem level hierarchy 
compare methods flat multi class nb classifiers flat multi class svms mcsvm vs approach nb classifiers levels hierarchy hier nb nb classifier followed svms hier svm 
accuracy different hierarchical methods ng training time different hierarchical methods ng notice figures training times hier nb hier svm lower mcsvm accuracy reduced 
accuracy hier nb lower 
hier svm lower accuracy mcsvm slightly better 
training time hier svm half mcsvm 
show table comparison accuracy levels separately nb svm classifiers 
show kinds accuracy documents get correctly classified correct group second absolute accuracy assuming perfect classifier 
expected classifiers individually accurate original flat classifier noted expected section 
nb accuracy combining nb accuracy leaves resultant accuracy hier nb 
nb nb individually better see compounding classification errors leaves hier nb worse 
similarly svm nb classifier hier svm accuracy 
accuracy hier svm drops slightly better worse mcsvm accuracy 
replace nb svm having accuracy accuracy hier nb hier svm improves slightly training time gets worse 
conclusive comparison decisively states hierarchical classification scheme better flat nb classifiers 
previous studies restricted number features node hierarchy tried equalize number flat hierarchical schemes 
clear attempting equalize number features classification schemes getting compromised 
main reason low accuracy hierarchical approaches compounding errors levels 
accuracy levels higher flat classifier product accuracies falls short accuracy flat classifier 
increasing levels expected worsen compounding effect 
led design new algorithm graphsvm attempts ensure stage classification process fast inaccuracy stage classifier jeopardize accuracy 
graphsvm algorithm algorithm represent class confusion general way graph may connect class class restricting confusion hierarchy disjoint groups previous approach 
previous approach start confusion matrix obtained fast multi class nb classifier 
class find set classes threshold percentage documents class gets mis classified class example confusion matrix find class alt 
atheism threshold alt atheism talk religion misc soc religion christian 
node non empty train multi class classifier distinguish classes 
classifiers constructed accurate possibly slower method svms 
testing classify document 
predicted class feed get refined prediction example test instance predicted alt atheism get prediction refined svm classes alt atheism talk religion misc soc religion christian 
prediction returned final answer test instance 
discussion algorithm graphsvm partitions classification task nb svms svms invoked small subsets classes get mis classified nb classifiers 
claim graphsvm worse provided training data representative test data 
graphsvm choose second stage svm refinements rare case svm classifier worse nb classifier validation dataset 
compared mcsvm main reason graphsvm may worse high positive values threshold decrease threshold expense increasing training time match mcsvm accuracy shown section 
cases expect strengths nb svms combine give performance better individually 
main property rely accuracy set classes class confuses nb classifier training test set 
relative distribution confusion matrix required remain unchanged provided entries previously threshold suddenly increase 
benefit graphsvm greatest misclassifications stage spread small number classes 
worst case mis classifications class uniformly distributed classes 
case algorithm reduce multi class svms 
practical datasets rarely case 

experimental evaluation text classification involves dealing tens thousands features documents 
nb classifiers proved fast scalable show moderate accuracy svm variants accurate slowest train 
aim scale expensive svms proposed graphsvm algorithm 
compare graphsvm fast nb classifiers accurate svm classifiers 
section evaluation proposed graphsvm algorithm 
compare algorithm multiclass nb multi class svms mcsvm hierarchical approach nb classifier svms hier svm 
compare algorithm accuracy training time scalability size training set 
datasets newsgroups newsgroups ng dataset collection news wire articles usenet groups 
older version dataset articles group newer version duplicates headers removed 
dataset pre processed training testing sets 
randomly chose documents training remaining testing 
corpus contained words 
features selected mutual information 
words stemmed porter stemmer html tags skipped header fields subject organization posted article ignored 
reuters reuters text categorization test collection standard text categorization benchmark 
contains classes 
chose classes training documents 
resulted training documents test documents 
xml www ai mit edu newsgroups www com resources reuters tags ignored words stemmed porter stemmer 
standard mod apte train test split 
training instances multiple class assignments considered assigned class 
ignored multi class test instances wanted see confusion classes resolved proposed algorithm 
experiments performed ghz machine mb ram running linux 
rainbow feature selection text processing experiments involving nb classifiers 
svmlight experiments involving svms 
comparison figures show accuracy training time methods ng reuters datasets 
accuracy comparison methods datasets training time comparison methods datasets ng dataset observe accuracy graphsvm slightly smaller mcsvm accuracy 
accuracy graphsvm higher previous hier svm accuracy 
reuters dataset graphsvm highest accuracy mcsvm accuracies respectively 
observe graphsvm fastest training time approaches involving svms datasets factor faster mcsvm 
expected mcsvm slowest train 
scalability number classes evaluated training time different approaches increasing number classes 
started randomly picked classes added randomly picked classes time datasets 
figures observe gap training time graphsvm mcsvm increases number classes increased 
www cs cmu edu mccallum bow svmlight joachims org training time vs number classes ng training time vs number classes reuters figures show cases graphsvm continues maintain high accuracy vis vis mcsvm 
ng dataset graphsvm maintains accuracy mcsvm reuters dataset graphsvm average better mcsvm 
notice large dip mcsvm 
reuters dataset highly skewed distribution instances class 
additionally shows total number test instances micro averaged accuracy values reported 
classes test instances 
difference graphsvm mcsvm due additional instances correctly classified graphsvm 
classes populated misclassifications seen confusion matrix larger threshold contribute edges graphsvm algorithm 
mis classifications corrected focused svms graphsvm method 
graph classes larger number instances accuracy graphsvm consistently better 
accuracy vs number classes ng scalability training set size training time size training set varied data keeping relative train test ratio constant 
observe accuracy vs number classes reuters training time graphsvm nearly linear training set sizes multi class svms training time increased super linearly training set size 
causes gap methods prominent larger datasets 
accuracy show corresponding accuracy values varying percentages training set sizes ng 
observe accuracy mcsvm increases increasing number training instances graphsvm closely tracks increase accurate 
maximum memory percentage training documents plotted maximum memory required train svm model graphsvm mcsvm approaches 
cases multiple vs svms learned size heterogeneity negative class varies largely leading different memory requirements 
mcsvm negative class contains entire dataset apart positive class greatly pruned graphsvm approach 
notice graphsvm requires fourth memory required mcsvm 
training time accuracy maximum model memory varying training set sizes ng dataset effect threshold parameter important parameter graphsvm threshold decide svms create second stage 
table show accuracy training time different values threshold 
see threshold appropriate datasets base accuracy nb classifier chosen get confusion matrix 
method threshold training accuracy time secs reuters mcsvm graphsvm graphsvm graphsvm graphsvm ng mcsvm graphsvm graphsvm graphsvm table accuracy performance threshold value kept unnecessarily high hardly graph construct assuming base classifier decent accuracy 
hand moderately accurate base classifier low threshold say densely connected graph 
case graphsvm approach mcsvm 
seen sections graphsvm accurate mcsvm reuters dataset requires training time 
scalability results reuters dataset training set size ng dataset section viz 
graphsvm highly scalable training time accuracy memory requirements compared mcsvm 
detailed results omitted space constraints 

related general framework solving multi class problems collection class problems associate coding matrix class row classifier column 
element aij denotes class serving positive class classifier denotes serving negative class denotes class participate classifier coding matrix vs columns diagonal entries 
max wins coding matrix columns column single rest entries 
error correcting output codes ecoc classifiers proposed chosen way different rows maximally separated 
testing outcome classifier treated vector compared row 
row closest returned predicted class 
ghani reports experiments number ecoc classifiers solve large multi class text classification problems nb 
report significant improvement accuracy ecoc method industry section dataset 
rennie rifkin repeated similar experiment support vector machines base classifier 
standard text benchmarks svms various ecoc classifiers provide accuracy improvement simple vs method 
platt modification testing procedure max wins algorithm reduces number kernel evaluations testing 
arrange various class classifiers dag structure order application various class svms testing 
method affect training time accuracy comparable 

described graphsvm effective framework extending discriminative classifiers svms handle data large number classes accurately efficiently 
graphsvm estimates measure affinity classes depends severity confusion classes fast classifier nb 
confusion indicates clustering classes limit number complexity svms need trained multi class categorization 
graphsvm beats accuracy multi class nb decisively builds nb classifier 
graphsvm outperforms svms training time memory requirements 
matches exceeds accuracy multi class svms 
graphsvm simple understand requires negligible coding substantial utility dealing large classifiers tens thousands classes millions instances 
acknowledgments funded research ministry information technology india 
wish anand amitabh mehta fruitful discussions 

allwein schapire singer 
reducing multiclass binary unifying approach margin classifiers 
th icml 
chakrabarti dom agrawal raghavan 
scalable feature selection classification signature generation organizing large text databases hierarchical topic taxonomies 
vldb journal 
dietterich bakiri 
solving multiclass learning problems 
jair 
dumais platt heckerman sahami 
inductive learning algorithms representations text categorization 
th cikm 
ghani 
error correcting codes text classification 
th icml 

exploiting confusion matrices automatic generation topic hierarchies scaling multi way classifiers 
technical report iit bombay 
www iitb ac aps pdf ryan rifkin jason rennie 
improving multi class text classification support vector machine ai memo aim mit 
joachims 
statistical learning model text classification svms 
sigir volume acm 
koller sahami 
hierarchically classifying documents words 
th icml 

pairwise classification support vector machines 
advances kernel methods support vector learning mit press 
mitchell 
conditions equivalence hierarchical non hierarchical bayesian classifiers 
technical note 
online www cs cmu edu tom ps platt cristianini shawe taylor 
large margin dags multiclass classification 
advances nips mit press 
porter 
algorithm suffix stripping 
program 
vapnik 
nature statistical learning theory 
springer verlag 
