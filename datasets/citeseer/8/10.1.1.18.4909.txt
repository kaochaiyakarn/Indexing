massachusetts institute technology artificial intelligence laboratory center biological computational learning memo december observations cortical mechanisms object recognition learning tomaso poggio sketches aspects hypothetical cortical architecture visual object recognition computational model 
scheme relies modules learning examples hyperbf networks basic components 
models intended precise theories biological circuitry capture class explanations call memory models mbm contains sparse population coding memory recognition codebooks prototypes 
sigmoidal units arti cial neural networks units consistent usual description cortical neurons tuned multidimensional optimal stimuli 
describe example mbm may realized terms cortical circuitry biophysical mechanisms consistent psychophysical physiological data 
number predictions testable physiological techniques 
copyright massachusetts institute technology memo describes research done center biological computational learning department brain cognitive sciences arti cial intelligence laboratory massachusetts institute technology 
research sponsored ce naval research contracts national science foundation contract asc award includes funds arpa provided hpcc program 
additional support provided north atlantic treaty organization atr audio visual perception research laboratories mitsubishi electric metal industries siemens ag 
support laboratory arti cial intelligence research arpa contract 
tomaso poggio supported helen whitaker chair mit whitaker college 
main goals vision object recognition 
may distinct routes goal goal may forms 
struggled identify particular amoeba swimming microscope slide distinguish novel visual stimuli psychophysics laboratory admit recognizing familiar face altogether di erent simpler task 
evidence lines research strongly suggests recognition tasks 
psychophysical results computational analyses suggest recognition strategies may depend type object visual task 
symmetric objects better recognized novel viewpoints asymmetric objects poggio vetter moved novel locations visual eld objects translation invariant features better recognized regan 
typical patient distinguish face car classi cation task basic level recognition recognize face marilyn monroe identi cation task subordinate level damasio 
stroke patient identify orientation line align hand imagines posting letter suggesting strongly multiple outputs visual recognition goodale 
recognition strategies diverge theories object recognition converge mechanism underlie distinct stages argue 
mechanism simple closely related template matching nearest neighbor techniques 
belongs class explanations call memory models includes memory recognition sparse population coding generalized radial basis functions networks extension hyper basis functions networks hbf poggio girosi see 
classi cation identi cation visual stimulus accomplished units 
unit broadly tuned particular template maximally excited stimulus exactly matches template responds proportionately similar stimuli 
weighted sum activities units uniquely labels novel stimulus 
successful face recognition schemes machine vision share aspects framework baron bichsel brunelli poggio turk pentland consider basic features class models implemented visual system 
aim demonstrate models conform existing physiological data physiological predictions 
speci example class rbf network 
rbf networks term rbf broad sense including generalizations pure rbf scheme grb hbf see poggio girosi 
successfully solve isolated visual tasks learning detect displacements hyperacuity resolution poggio fahle edelman learning identify gender face brunelli poggio 
discuss units rbf network realized neurons similar network implemented cortical circuitry replicated levels perform multi component recognition 
merely toy replicas neural systems viable models testable biological predictions 
main predictions memory models existence broadly tuned neurons levels visual pathway tuned single features con gurations multidimensional feature space 
types plasticity adult brain corresponding stages learning perceptual skills tasks 
stage probably involves changes tuning individual neuron responses resembles adaptation 
probably requires changes cortical circuitry speci task learned connecting neurons possibly areas 
object recognition multiple tasks multiple pathways recognizing object di cult rarely looks sighting 
consider prototypical problem recognizing speci face 
believe processing faces qualitatively different processing objects streamlined practice biological evidence supports view gross 
retinal image formed face changes observer viewpoint transformations face undergo changes location pose illumination non rigid deformations transition smile 
successful recognition system robust transformations 
outline architecture recognition system contains believe rudimentary elements robust system 
best considered protocol summary existing programs machine vision represents attempt delineate stages probably involved visual recognition humans 
scheme diagrammed dual routes recognition 
rst streamlined route recognition features extracted early stages image analysis matched directly samples database 
second potential route recognition diverges rst allow possibility database models extracted image features need processing match 
task recognizing face object consists multiple tasks fall broad categories characterize routes segmentation marking boundaries face image 
stage typically involves segmenting entire image regions correspond di erent materials surfaces subsumes gure ground segmentation prerequisite analysis marked region 
image measurements convert retinal array light intensities primal image representation computing sparse measurements array gradients center surround outputs 
result set vector measurements sparse dense set locations image 
measurements may global ones value array ltered pixel values 
classi cation basic level recognition distinguishing objects faces 
parameter values estimated preceding stage distance eyes mouth stage classi cation set features potential face animal tool 
stage requires boundaries location potential faces generally depends preceding step image segmentation may added computational cost 
identi cation subordinate level recognition matching face stored memory labeling 
stage requires form indexing database samples 
computationally implausible recognition system contains stored sample face possible views expressions possible illumination conditions possible viewing distances step general requires face transformed standard form matching stored template 
parallel direct route classi cation identi cation may exist second route call visualization route iterative sequence transformations image plane database models converges match 
stages open questions architecture discussed appendices 
outlined stages distinct implemented series route recognition 
arti cial face recognition systems tackle stages separately designed detect localize face image cluttered objects segmentation classi cation identify individual faces expected format database indexing identi cation 
arti cial recognition systems constructed achieve invariant recognition isolated transformations visualization 
examples systems recognize frontal views faces varying illuminations brunelli poggio recognize simple clip objects independently viewpoint poggio edelman identify simple objects solely color spatially varying illumination swain ballard 
biological systems arti cial systems stages may act parallel merge 
example may short cuts recognizing frequently encountered object face example 
finding face streamlined quick search resolution image patterns 
search simpli ed templates face containing anthropometric information example eyes mouth mask 
located salient features demarcate entire object belong eliminating need segment parts image 
detectors scan image presence face speci features locate face processing translation scaling 
machine vision systems implement idea translation invariant face feature detectors detectors bichsel symmetry detectors 
segmentation may occur simultaneously classi cation 
existence face detectors human visual system explain readily perceive faces simplest drawings dots lines symmetric patterns formed nature poggio detect properly con gured faces readily arbitrary inverted arrangements facial features purcell stewart 
wonder face recognition mayhave human brain rst classify image regions face non face 
notice process nding features eyes identifying face probably similar view 
set prototypical examples eyes views particular face may similar machinery rbf 
recognizing expected object speedy cient identifying unexpected 
classi cation stage features speci expected object class need measured correct classi cation require features simultaneously available 
step form template matching part templates may serve aswell templates locate classify object 
cases classi cation stage may lead unique recognition especially situational information object restricts relevant data base 
questions left hanging sketch recognition system 
biological systems matching done primal image representations outputs sparse locations sets higher level features 
computational experiments face recognition suggest strategy performs better 
exactly key features identifying localizing normalizing object speci class 
automatic way learn 
huber 
biological visual systems acquire recognition features experience edelman 
humans expectation restrict data base categorization 
psychophysical experiments suggest need higher level expectations recognize objects quickly random series images experiments familiar objects ei el tower potter pers 
comm 
memory cortical architecture recognition suggest stages face recognition generally object recognition may implemented modules intrinsic structure memory module mbm 
heart structure set neurons tuned particular value con guration feature dimensions 
take example structure hyper basis functions hbf network 
convenient choice successfully applied problems object recognition easily modi able choice closely related approximation learning techniques perceptrons 
rbf networks hbf networks approximation schemes exible radial basis functions rbf networks see poggio girosi poggio 
fundamental equation underlying rbf networks states function sum rbfs nx kx tik functions may class rbfs example gaussians 
polynomial required certain rbfs validity equation 
rbfs gaussians addition necessary improves performance network 
rbf network unit computes distance kx tk input vector center applies function distance value computes function kx tk centers corresponding data points behave templates inputs compared similarity 
atypical illustrative choice rbf gaussian kx tk exp kx tk 
limiting case narrow gaussian network effectively look table non zero signal input exactly matches center simplest recognition scheme rbf networks consider suggested poggio edelman see solve speci problem recognizing particular object novel views subordinate level task 
rbf version network center stores sample view object acts unit gaussian recognition eld view 
unit performs operation described blurred template matching 
output network activities various units combined appropriate weights learning stage 
example recognition eld measured asymmetric object training single view shown 
predicted model see poggio edelman shape surface recognition errors roughly gaussian centered training view 
particular model inputs network spatial coordinates measurements features angles lengths segments computed image 
general inputs rbf network restricted spatial coordinates include example colours con gurations segments binocular disparities features texture descriptions 
certainly biological implementation network inputs may include measurements descriptions attribute visual system may represent 
assume primate visual system recognition module may large number primitive measurements inputs taken di erent lters regarded di erent templates shape texture color forth 
restriction features directly computed image 
inputs viewer centered object centered colour viewpoint independent 
output network object centered provided su cient 
generality network permits mix information inputs relieves model constraints 
feature model renders irrelevant question object representations 
poggio edelman model clear schemes provide view invariance readily model compute pose see poggio edelman 
relevant questions explicit neurons 
mean information shape explicit neurons 
sense schemes poggio edelman model may considered plausible neurophysiological implementations models 
suggest cortical architecture recognition consists collection modules recognizable object 
certainly complex cartoon problem recognition system solve 
example cortex learn recognize objects varying illumination photometric invariance recognize objects basic subordinate level 
preliminary results real objects faces suggest hbf modules estimate expression direction illumination equally pose brunelli pers 
comm beymer pers 
comm 
distinct tasks recognition may implemented module broadly similar poggio edelman viewpoint invariance network 
expect system decomposed elementary modules similar design di erent purpose speci individual objects solving subordinate level task speci object class solving basic level task designed perform transformations feature extractions example common classes 
modules may broadly categorized object speci module designed compensate speci transformations speci object undergo 
poggio edelman network module consist units maximally tuned particular con guration object face say particular combination pose expression 
general form network may able recognize different faces hidden units tuned di erent views just face behave eigenfaces 
class speci module generalizes objects class 
example network may designed extract feature aspect class objects pose color distance 
example network designed extract pose face separate network designed extract direction illumination 
face fed input network elicit estimate pose illumination 
task speci networks solve tasks shape shading classes objects 
example generic shape shading network takes input brightness gradients image regions 
may act early stages recognition helping segment classify shapes grouped classi ed objects 
distinctions types recognition module blurred example visual system certain objects transformations 
example shape shading network develop frequently encountered type material speci class object 
working assumption apparent di erences recognition strategies di erent types objects arise fundamental di erences cortical mechanisms imbalances distribution basic modules di erent objects di erent environments 
man probably task speci modules dedicated faces shape shading modules speci familiar pieces ce furniture able recognize ling cabinet varying illumination 
suggests decomposition modules task object speci unconventional plausible idea 
transformations speci particular object may generalized transformations learned prototypes class 
example deformation caused pose face change expression age may learned set examples transformation acting prototypes class 
transformations may generalized objects sharing symmetries poggio vetter 
big question recognition system consist similar modules performing interlocking tasks modules linked hierarchy sense talk ordered stages 
constructing practical system face recognition sense rst estimate pose expression illumination generic face estimate normalize face compare single views data base additional search ne tune match may necessary 
system rst employ class speci module invariant properties faces recover say generic view analogous object centered representation feed face speci networks identi cation 
information system extracts early stages concerning illumination expression context discarded 
stage modules may decomposed arranged hierarchies may speci eyes may extract gaze angle parameter may feed module concerned pose entire face 
face recognition generic view may recovered exploiting prior information approximate bilateral symmetry faces 
general single monocular view object shading neglected contain su cient information recognition novel views 
humans certainly able recognize faces rotated degrees away frontal training just frontal view 
discussed poggio di solving problem view object generate views exploiting knowledge views prototypical objects class 
shown theoretically poggio vetter prior information generic shape constraints reduce amount information needed recognize object additional virtual views generated model views appropriate symmetry transformations 
particular bilaterally symmetric objects single non accidental model view theoretically su cient recognition novel views 
psychophysical experiments vetter poggio con rm humans better recognizing symmetric non symmetric objects 
interesting question multiple routes recognition 
obvious logically distinct steps recognition may fewer modules depending speci implementation 
shows architecture may appear classi cation visualization routes implemented hbf networks 
case database face models essentially embedded networks see poggio edelman 
course obvious alternatives architecture possible re nements extensions 
ed token architecture useful generate meaningful questions 
preceding discussion may cient performing computational experiments developing practical systems 
su cient suggesting psychophysical experiments 
course point view physiological data section provides broad support ingredients 
physiological support memory recognition architecture super cially physiological data support existence elements modules 
perrett 
perrett perrett report evidence inferotemporal cortex cells tuned individual faces face cells tuned intermediate views frontal pro le units expect class speci network designed extract pose faces 
cells support existence view centered units predicted basic poggio edelman recognition module 
young describe cells anterior respond optimally particular con gurations facial features physical prototypes 
may conceivably provide input cells described perrett person recognition units approximately view independent cells described hasselmo 
hasselmo turn correspond exactly object centred output poggio edelman model 
perrett 
report cells respond pose face regardless illumination face heavy shadow 
cells may resemble units task speci network 
superior temporal sulcus hasselmo 
nd cells sensitive head movement facial gesture independent view identity face 
cells appear class task speci 
see perrett oram detailed review relevant physiological data 
fujita tanaka reported cells respond optimally certain con gurations color shape 
may represent elements networks generalize objects classifying geometric material constitution 
signi cantly fujita tanaka report cells anterior region area te arranged columns cells respond similar con gurations color shape texture 
con guration may thought template turn encode entire object face part object lips 
column cells may respond slightly di erent versions template obtained rotations image plane example 
fujita tanaka conclude columns te may represent phoneme language objects combinations activity columns su cient encode recognizable objects 
existence columns supports notion visual system may achieve invariance transformations elementary features replicating necessary feature measurements di erent positions di erent scales di erent rotations 
section describe key aspects architecture implemented terms plausible biophysical mechanisms neurophysiological 
neural modeling memory architectures recognition section discuss detail possible neural implementations recognition system built 
main questions address constructed new object class objects learned 
mbm units constructed known biophysical mechanisms 
propose stages learning supervised unsupervised illustrate elements memorybased network correspond 
localized terms cortical structures 
mechanisms responsible 
discuss memorybased module circuitry underlie 
learning examples module simple rbf version mbm discussed section learns recognize object straightforward way 
centers xed chosen subset training examples 
parameters modi ed network learns associate view correct response target object coe cients ci weights connections center output 
full hbf network permits learning mechanisms biologically plausible allowing parameters modi ed 
hbf networks equivalent scheme approximating multivariate function nx centers coe cients unknown general fewer number data points 
norm weighted norm unknown square matrix superscript indicates transpose 
simple case diagonal diagonal elements wi assign speci weight input coordinate determining fact units measure importance feature matrix especially important cases input features di erent type relative importance unknown poggio girosi 
learning coe cients centers elements updated instruction input output examples 
see 
rbf technique similar similarly limited template matching hbf networks perform generalization template matching appropriately linearly transformed space appropriate metric 
hbf networks di erent interpretation capabilities vanilla rbf 
rbf network recognize object rotated novel orientations centers corresponding sample rotations object 
perform avariety sophisticated recognition tasks 
example 
discover basri ullman result basri ullman brunelli poggio unpublished 
strong form see poggio result states orthographic projection view visible features object may generated linear combination views 
non diagonal recognize object orthographic projection center 
provide invariance near invariance perspective projection scale rotation uniform deformations image plane requiring features invariant 
discover symmetry collinearity properties see poggio vetter 
gaussian radial basis functions special case network basis functions gaussian matrix diagonal elements wi obvious interpretation 
multidimensional gaussian basis function product dimensional gaussians scale isgiven inverse wi 
example gaussian radial function centered written kx tk kx tk tx ty andw elements diagonal matrix multidimensional center factored terms dimensional centers 
dimensional center individually tuned input centers small wi large selective give appreciable responses range values input feature centers large wi selective input accordingly greater uence response multidimensional center 
template represented center considered conjunction dimensional templates 
sense gaussian hbf network performs disjunction conjunctions conjunctions represented multidimensional centers ed weighted sum center activities forms output network 
expected physiological properties mbm units neurophysiological interpretation hbf centers key claim hbf centers tuned cortical neurons behave alike 
gaussian hbf unit maximally excited component input exactly matches component center 
unit optimally tuned stimulus value speci ed center 
units multidimensional centers tuned complex features formed conjunction simpler features described previous section 
description customary description cortical cells optimally tuned complex stimulus 
called place coding simplest universal example tuning cells roughly gaussian receptive elds peak sensitivities locations input space overlapping cell sensitivities cover space 
input space may dimensional depending cell tuned retinal coordinates stimulus orientation motion direction binocular disparity 
cells respond optimally stimulus combining appropriate values speed color logothetis pers 
comm logothetis charles 
cells respond optimally combination colour shape van essen pers 
comm 
mst cells exist optimally tuned speci motions di erent parts receptive eld different motion dimensions 
cells selective stimulus contrast 
areas cells may tuned complex stimuli changed number dimensions desimone 
gross concludes cells tend respond di erent rates variety di erent stimuli 
multidimensional units gaussian tuning biologically plausible ubiquitous cortical physiology 
claim meant imply feature dimension tuned neuron neurons feeding individually tuned dimension 
example motion selective cells mt selectivities spatial frequency temporal frequency separated 
may inappropriate consider time space independent dimensions appropriate consider velocity single dimension neuron tuned 
hand known lower levels visual system exist cells broadly tuned individually spatial frequency orientation wavelength example dimensions complex features constructed 
observe applicability describing properties cortical neurons 
particular tuned neurons behave gaussian hbf units sigmoidal units typically multilayer perceptrons mlps tuned response function cortical neurons resembles exp kx tk sigmoidal squashing function de ne vector connection weights including bias parameter 
typical sigmoidal response contrast neurons display may treated gaussian large example stimulus orientation selective cortical neuron changed optimal value direction neuron response typically decreases 
activity gaussian hbf unit decline change stimulus away optimal value sigmoid unit certain changes away optimal stimulus decrease activity example input multiplied constant 
lastly observe gaussian simplest readily interpretable rbf logical terms ultimately provide best physiological data 
general theory cortical mechanisms object recognition con ne gaussian rbfs model cortical neurons plausible 
centers fundamental property sensory world recognize object small subsets features visual non visual 
perform motor actions di erent ways 
situations sensory motor worlds redundant 
language previous section means high dimensional centers lower dimensional centers su cient perform task 
means high dimensional conjunction replaced components face may recognized eyebrows mug colour 
recognize object may templates comprising features comprising subsets features fact exemplary sets centers capable generating eyes say 
similar spirit small templates template brunelli poggio frontal face recognition brunelli poggio 
splitting recognizable world additive parts preferable reconstructing full multidimensionality system composed independently accessible parts inherently robust simultaneously dependent parts 
small loss uniqueness recognition easily set gain noise occlusion 
reduction recognizable world parts may allows understand things see see appendix 
cells 
idea sparse population coding consistent physiological evidence retinal level colors coded types photoreceptors 
young conclude neurophysiological recordings cells broadly tuned physical prototypes faces representing space cell represented surface raised feature space 
height surface point feature space response magnitude stimuli population vectors derived summing response weighted surfaces cell stimulus 
suggest importance object exposure may determine devoted recognition 
faces may representation objects simply centers 
psychophysical experiments suggest increasing number centers created extended training recognize object edelman 
dare speci prediction absolute number cells code speci object computational experiments arguments suggest minimum bound 
simulations poggio edelman suggest mbm model minimum units needed represent possible views object 
think primate visual system achieve representation fewer order 
number physiologically plausible expect actual number depend strongly reliability neurons training animal relevance represented object properties implementation 
envisage training monkey view target object may create order cells tuned view relevant cortical area generalization eld similar shown gure 
training additional view may create recruit cells tuned view 
overtraining monkey speci object result representation cortex object cells normally expected tuned views object 
results 
suggest orders magnitude cells may created stimulus selectivities existing cells altered training speci objects 
note mean imply cortical cells active presentation object 
activated critical representation 
suggest activity cells su cient discriminate distinct objects 
broadly supported young population response approximately cells approximately su cient encode particular face related observation 
activity small pool weakly correlated neurons mt su cient predict monkey behavioral response motion detection task 
hbf centers biophysical mechanisms gaussian receptive elds synthesized known receptive elds biophysical mechanisms 
simplest answer cells tuned complex probably di erent ways di erent cells may tuned di erent parts view may converge di erent prototypes representing component term prototype mind caricatures brunelli poggio features constructed hierarchy simpler cells tuned incrementally larger conjunctions elementary features 
idea standard explanation immediately formalized terms gaussian radial basis functions multidimensional gaussian function decomposed product lower dimensional gaussians marr poggio ballard mel poggio girosi 
scheme gure possible example implementation gaussian radial basis functions terms physiologically plausible mechanisms 
rst step applies situations inputs thevalue input represented location spatial array example image coordinates encoded spatial pattern 
case gaussian radial functions possibly dimensions implemented receptive elds weighted connections sensor arrays retinotopic array units activity encodes location features 
inputs input value represented continuously varying ring rate single neuron dimensional gaussian tuned cell created passing input value multiple sigmoidal functions di erent thresholds difference 
consider example problem encoding colour 
retinal level colour recorded triplet activities types cell red green blue yellow cells luminance cell 
cell signals increasing amounts red decreasing amounts green increasing ring rate 
behave gaussian tuned cell 
higher levels visual system exist cells behave units tuned particular values colour space desimone 
multidimensional tuned colour cells constructed dimensional rate coded cells 
suggest dimensional gaussian tuned cells may created mechanism selective restricted ranges colour axes 
gaussians higher dimensions synthesized products dimensional receptive elds 
important feature scheme multidimensional radial functions synthesized directly appropriately weighted connections sensor arrays need explicit computation norm exponential 
perspective computation performed gaussian receptive elds combination approximation multiplication threshold functions 
view spirit key role concept receptive eld played 
predicts sparse population coding terms low dimensional feature cells mul gaussian receptive elds somewhat similar template cells prediction tested experimentally cortical cells 
multiplication operation required previous interpretation gaussian rbfs perform conjunction gaussian receptive elds implausible biophysical point view 
performed biophysical mechanisms see koch poggio poggio 
mention possibilities 
inhibition silent type related synaptic dendritic circuitry see poggio torre torre poggio 

mechanism receptors 
logarithmic transformation followed summation followed exponentiation 
logarithmic exponential characteristic implemented appropriate ranges sigmoid pre postsynaptic voltage transduction synapses 

approximation summation thresholding suggested mel 
rst second mechanism product gure performed directly dendritic tree neuron representing corresponding radial function 
case gaussian receptive elds synthesize gaussian radial basis functions center vector ectively stored position receptive elds connections product unit 
plausible physiologically 
linear terms direct connections inputs output realized directly inputs output neuron linearly synaptic inputs 
output nonlinearity threshold sigmoid log transformation may tasks change basic form model see poggio girosi 
circuits way implement networks terms known properties neurons 
exploits equivalence mlp networks normalized inputs maruyama 
inputs normalized usual unitary input representations hbf network implemented mlp network threshold units 
problem normalizing inputs biologically plausible way 
mlp networks straightforward implementation terms linear excitation inhibition threshold mechanism spike sigmoidal nonlinearity 
implemented terms pre postsynaptic relationship presynaptic voltage postsynaptic voltage 
case implementation requires neuron sigmoidal unit network 
mel simulated speci biophysical hypothesis role cortical pyramidal cells implementing learning scheme similar hbf network 
marr proposed similar model cells neocortex learn discriminate di erent patterns 
marr model sense look table limit hbf model 
mechanisms learning reasoning hbf model mechanisms learning probably di erent occur unsupervised similar adaptation supervised probably hebb mechanisms 
rst stage learning occur site centers 
remember center represents neuron tuned particular visual stimulus example vertically oriented light bar 
coe cients represent synaptic weights connections neuron output neuron registers network response 
simple rbf scheme parameters updated learning coe cients 
constructing network centers set values equal input examples 
physiologically selecting centers correspond choosing re tuning subset neurons selectively responsive range stimulus attributes encountered task 
stage adaptation adjustment prevailing stimulus conditions 
occur unsupervised strictly depend stimuli task 
course expect centers evolution cortex 
second stage updating coe cients occur supervised depends full input output example pairs words task 
achieved simple hebb type rule gradient descent equations poggio girosi nx ig squared error correct output example actual output network 
equation says change proportional product activity output error network 
words weights synapses change depending product pre postsynaptic activity poggio girosi mel mel 
rbf case centers xed initially selected conform input examples 
hbf case centers move optimal locations learning 
movement supervised ne tuning centers stimulus selectivities 
highly biological visual system chooses distinct rbf implementations problems 
possible tuning cell selectivities occur di erent ways corresponding supervised unsupervised stages outlined 
expect types learning centers occur di erent time scales fast corresponding selecting centers pre existing set slow corresponding synthesizing new centers stimulus speci cities 
cortical locations mechanisms unsupervised supervised may di erent interesting implications data transfer learning see poggio fahle edelman 
fast unsupervised learning large reservoir centers available associated suggested mel slightly di erent context 
relevant gain non zero weight adaptive process 
alternatively mechanism similar unsupervised learning models described linsker intrator cooper 
slow supervised learning may stabilization electrically close synapses depending degree activated see mel 
scheme changes formation stabilization synapses synapse clusters synapse representing gaussian eld cortical pyramidal cell simply due correlations presynaptic activities 
suggest synthesis new centers needed learning recognize unfamiliar objects slower selecting centers existing pool 
data perceptual learning berardi poggio fahle edelman sagi indicates fact human observers rapidly learn entirely novel visual patterns suggests new centers rapidly 
reasonable conjecture updating elements matrix may slower time scale 
update schemes plausible implementation 
methods random step method girosi require calculation derivatives biologically plausible 
typical random step method network weight changes generated randomly guidance simple rules example rule size random change network improves halve size 
gaussian case basis functions synthesized product gaussian receptive elds moving centers means establishing erasing connections product unit 
similar argument learning matrix notice diagonal gaussian case parameters changed exactly gaussians spread associated receptive elds 
notice centers particular dimension suggesting learning wi may involve modi cation scale factor input arrays change dendritic spread postsynaptic neurons 
schemes real problem consists teacher input 
predictions remarks summarize highlight main predictions interpretation memory models brain 
predictions 
sparse 
general issue nervous system represents objects concepts course unresolved 
sparse coding theories propose individual cells highly speci encode individual patterns 
population theories propose distributed activity large number cells underlies perception 
models hbf type suggest small number cells groups cells centers broadly tuned may su cient represent object 
interpretation predicts sparse population coding fully distributed representations grandmother neurones 
speci cally predict activity approximately cells su cient distinguish particular object cells may active time 

viewer centered object centered cells 
model see module predicts existence viewer centered cells centers object centered cells output network 
evidence pointing direction case face cells available 
predict similar situation objects 
noted module small part architecture 
predict existence types cells pose tuned expression tuned cells 
logothetis pers 
comm 
succeeded training monkeys recognize objects human psychophysics reproduced key results edelman 
succeeded measuring generalization elds type shown gure training single view 
believe measured generalization eld corresponds group cells tuned gaussian manner view 
expect trained monkeys cells exist corresponding hidden units hbf network speci training view possibly cells responding subparts view 
conjecture critical prediction theory step creating tuned cells centers unsupervised words create centers su cient expose monkeys target views training respond speci ways 

cells tuned full views cells tuned parts 
model implies highdimensional low dimensional centers exist recognizable objects corresponding full templates template parts 
physiologically corresponds cells require object respond say face cells respond part object say mouth 

rapid synaptic plasticity 
predict formation new centers change synaptic weights may happen short time scales possibly minutes relatively early visual pathway fahle edelman 
mentioned formation new centers unsupervised synaptic changes corresponding ci coe cients supervised 
hbf modules theories brain theories brain parts networks replace computation memory 
equivalent modules interpolating look tables 
previous discussed theories type regarded modern version grandmother cell idea poggio 
proposal information processing brain performed modules similar enhanced look tables attractive reasons 
promises bring closer apparently orthogonal views immediate perception gibson representational theory marr iconic snapshots world may allow synthesis computational mechanisms equivalent vision algorithms 
idea may change signi cantly computational perspective vision tasks 
simple example consider di erent speci tasks hyperacuity employed 
proposal suggest appropriate module task somewhat similar new routine may synthesized learning brain see poggio fahle edelman 
claim common network theories multilayer perceptrons hbf networks brain explained part terms approximation modules 
case hbf networks modules considered powerful extension look tables 
mlp networks interpreted directly modi ed look tables similar extension multidimensional fourier series case normalized inputs shows similar templates 
hbf theory predicts population coding broadly tuned neurons combined linearly consequence extending look table scheme corresponding interval coding yield interpolation precisely approximation generalization 
words sparse population coding neurons tuned speci optimal stimuli direct strong predictions hbf schemes 
hidden units hbf models bear suggestive similarities usual descriptions cortical neurons tuned optimal multidimensional stimuli 
course possible hierarchy di erent networks example mlp networks may lead tuned cells similar hidden units hbf networks 
describes research done center biological information processing department brain cognitive sciences arti cial intelligence laboratory 
research dr dr poggio research cognitive neural sciences division ce naval research contract darpa arti cial neural network technology program national science foundation contract asc including funds darpa provided hpcc program 
support laboratory arti cial intelligence research onr contract 
tomaso poggio supported helen whitaker chair whitaker college massachusetts institute technology 
research supported jci serc serc iii royal society england 
architecture recognition classi cation indexing route recognition elaborate architecture recognition system introduced section 
illustrates main components architecture interlocking routes recognition 
rst route call classi cation indexing route essentially equivalent earlier proposal poggio edelman hbf network receives inputs form feature parameters classi es inputs di erent target object 
streamlined route recognition requires features extracted early stages image analysis su cient enable matching samples database 
goal may primarily basic level recognition route suit best search recognition expected object 
case identify objects subordinate level class membership known advance 
consists main stages 
image measurements rst step compute primal image representation whichis set sparse measurements image appropriate smoothed derivatives corresponding center surround directional receptive elds 
argued vector measurements considered multiple nonlinear functions di erential operators applied image sparse locations discussion linear non linear measurement matching primitives see appendix nishihara poggio 
similar procedures may involve gaussians di erent scales orientations marr poggio koenderink jets koenderink gabor lters wavelets 
regularized gradient image works 
call array array general array 
recognition frontal images faces array small su cient encode image initial size brunelli poggio 

feature detection measurements key features encoded primal measurements localized 
features may speci speci object class expected class known advance alternative class considered potential match 
step regarded performing sort template matching appropriate examples face object search templates may include eye pairs di erent size pose expression 
hbf case templates ectively correspond di erent centers matching proceed sophisticated way direct comparison 
clear step may accomplish segmentation 
features may local global may correspond eye corners mean values array ltered large set lters 

classi cation indexing parameter values estimated preceding stage features interest distance eyes mouth stage classi cation indexing database known examples 
cases may unique recognition especially situational information particular object restricts relevant data base 
classi cation done number classical schemes nearest biologically plausible networks 
open questions remain features visual system feature detection stage 
nonlocal hypothesis large set lters tuned di erent shape features ciently doing kind template matching input 
functional correlation function evaluated max correlation robust statistics correlation values see viola poggio preparation 
results may components particular lter template input vector object speci networks consisting hidden units tuned view output unit view invariant 
networks type may speci objects general object components similar precise versions biederman geons biederman 
synthesized familiarity output degree view invariance depending type number tuned cells hidden layer 
networks type tuned particular shape easily combined conjunctively represent complex shapes exploiting fundamental property additivity 
general non local scheme avoids correspondence problem components input vectors statistics taken image individual pixel values feature locations 
may absence serial mechanism eye motions attentional shifts visual system away spatial relations di erent components image detect presence say features vari ous complexity 
architecture hierarchical consisting hierarchy 
instance eye recognizing mbm network may provide inputs face recognition network combine presence possibly relative position eyes face features remember mbm network regarded disjunction conjunctions 
inputs eye recognizing networks may provided rbf networks similar eye recognizing networks inputs result ltering image basic lters large vocabulary consisting hundreds elementary templates representing vocabulary shapes type described fujita tanaka 
description perrett oram consistent scenario 
various stages hierarchy invariances may position rotation scaling similar way complex cells built simple ones 
architecture recognition visualization route recognition second potential route recognition takes necessary detour rst route ne tune matching mechanisms 
classi cation pathway stages image measurement feature detection diverges allows possibility match database measured image features directly 
processing may take place image stored examples bring registration narrow range 
main purpose loop correct deformations comparing image data base 
computational arguments breuel suggest route separate transformations applied image correct image plane deformations image plane translations scaling rotations applied database model may include rotations depth illumination changes alterations facial expression example 
system may transformations parallel multiple scales spatial resolution see van essen anderson volume nds succeeds 
general process may iterated times achieves satisfactory level 
primate visual system site transformations cortical area probably take place earlier available results properties suggest gross perrett perrett harries perrett 
main steps hypothetical second route recognition 
image measurement 
feature detection 
image recti cation feature detection stage provides information location key features stage normalize image plane translation scaling image plane rotation input 

pose estimation pose parameters illumination parameters facial expression estimated array 
computation performed mbm module learned appropriate estimation function examples objects class 

visualization models arrays data base corresponding known objects warped dimensions pose expression illumination bring register estimate obtained input image 
transformation models performed exploiting information speci object views object mayhave stored memory applying generic transformation face serious smiling learned objects class 
transformations may attempted stage match step 

veri cation indexing recti ed image compared warped data base standard representations 
open questions remain data base may organized cient means indexing 
ballard 
cortical connections parallel processing structure function 
behavioral brain sciences 
baron 
mechanisms human facial recognition 
international journal man machine studies 
basri ullman 
recognition linear combinations models 
technical report weizmann institute science 
martin bichsel 
strategies robust object recognition identi cation human faces 
phd thesis hochschule zurich 
biederman 
recognition theory human image understanding 
psychol 
review 
breuel 
geometric aspects visual object recognition 
phd thesis department brain cognitive sciences massachussetts institute technology cambridge ma 
translation invariant features object recognition 
perception 
newsome movshon 
analysis visual motion comparison neuronal psychophysical performance 
journal neuroscience 
brunelli poggio 
hyperbf networks gender classi cation 
proceedings image understanding workshop san mateo ca 
morgan kaufmann publishers edelman 
psychophysical support view interpolation theory object recognition 
proceedings national academy science 
edelman 
mapping generalization space object recognition 
invest 

vis 
science suppl 
girosi 
nondeterministic minimization algorithm 
memo arti cial intelligence laboratory massachusetts institute technology cambridge ma september 
damasio damasio 
face neural substrates memory 
annual review neuroscience 
desimone gross bruce 
stimulus selective properties inferior temporal neurons macaque 
neurosci 
edelman 
features recognition 
proc 
intl 
workshop visual form capri italy new york 
plenum press 
edelman orientation dependence recognition familiar novel views objects 
vision research 
berardi 
learning grating waveform discrimination speci city orientation spatial frequency 
vision research 

learning invariance transformation sequences 
neural computation 
fujita tanaka 
columns visual features objects monkey inferotemporal cortex 
nature 
gibson 
ecological approach visual perception 
houghton mi boston ma 
goodale milner carey 
neurological dissociation perceiving objects grasping 
nature 
gross 
representation visual stimuli inferior temporal cortex 
phil 
trans 
royal soc 
london 
hasselmo rolls 
object centered encoding neurons cortex superior temporal sulcus monkey 
experimental brain research 
huber 
projection pursuit 
annals statistics 
poggio 
computers need attention 
nature 
intrator cooper 
objective function formulation bcm theory visual cortical plasticity statistical connections stability conditions 
neural networks 
sagi 
human texture discrimination learning evidence low level neuronal plasticity adults 
perception 
tanaka 
learning new shapes changes stimulus selectivity cells inferotemporal cortex adult monkey 
supplement investigative visual science 
koch poggio 
biophysics computational systems neurons synapses membranes 
edelman gall cowan editors synaptic function pages 
wiley new york ny 
koenderink van doorn 
receptive eld families 
biological cybernetics 
linsker 
perceptual neural organization approaches network models information theory 
ann 
rev neurosci 
logothetis charles 
responses gratings de ned random dot motion 
supplement investigative visual science 
marr 
theory cerebral neocortex 
proceedings royal society london 
marr 
vision 
freeman san francisco ca 
marr poggio 
understanding computation understanding neural circuitry 
neurosciences res 
prog 
bull 
maruyama girosi poggio 
connection hbf mlp 
memo arti cial intelligence laboratory massachusetts institute technology 
mel 
murphy robot learns doing 
anderson editor neural information processing systems 
american institute physics university colorado denver 
mel 
sigma pi column model associative learning cerebral neocortex 
computational neural systems program memo california institute technology 
mel 
pattern discrimination modeled cortical neuron 
neural computation 
regan 
results translation invariance human visual system 
spatial vision 
nishihara poggio 
stereo vision robotics 
brady editor robotics research international symposium 
mit press 
perrett harries 
characteristic views visual inspection simple faceted smooth objects tetrahedra potatoes 
perception 
perrett 
visual neurones responsive faces 
trends neurosciences 
perrett oram 
neurophysiology shape processing 
image visual computing 
submitted 
perrett rolls 
visual neurones responsive faces monkey temporal cortex 
exp brain res 
perrett smith potter head milner jeeves 
visual cells temporal cortex sensitive face view gaze direction 
proc 
soc 
london 
poggio 
theory brain 
cold spring harbor symposia quantitative biology pages 
cold spring harbor laboratory press 
poggio 
object recognition prototypes view may su cient 
technical report 
poggio edelman 
network learns recognize dimensional objects 
nature 
poggio fahle edelman 
fast perceptual learning visual hyperacuity 
science may 
poggio girosi 
theory networks approximation learning 
memo arti cial intelligence laboratory massachusetts institute technology 
poggio girosi 
extension theory networks approximation learning dimensionality reduction clustering 
memo arti cial intelligence laboratory massachusetts institute technology 
poggio girosi 
networks approximation learning 
proceedings ieee september 
poggio girosi 
regularization algorithms learning equivalent networks 
science 
poggio torre 
theory synaptic interactions 
poggio editors theoretical approaches neurobiology pages 
press cambridge ma 
poggio vetter 
recognition structure model view observations prototypes object classes symmetries 
memo arti cial intelligence laboratory massachusetts institute technology 
purcell stewart 
face detection ect con guration enhances detection 
perception psychophysics 
desimone 
spectral properties neurons macaque 
journal neuroscience 
luigi 
eyes detection face recognition 
technical report 
luigi 
automatic face recognition directional derivatives 
technical report 
swain ballard 
indexing color histograms 
proceedings international conference computer vision pages osaka japan 
ieee 
torre poggio 
synaptic mechanism possibly underlying directional selectivity motion 
proc 
soc 
lond 

turk pentland 
eigenfaces recognition 
journal cognitive neuroscience 
vetter poggio object recognition symmetry virtual views 
arti cial intelligence laboratory memo massachusetts institute technology 
young 
sparse population coding faces inferotemporal cortex 
science 
classification teddy verification transformation model pose expression illumination data base image sparse measurements features shape non shape image plane transformation translation scale rotation illumination pose estimation illumination 
coarse prototypical model sketch architecture recognition hypothetical routes recognition 
single arrows represent classi cation indexing route described double arrows represent main visualization route dashed arrows alternative pathways 
rbf network approximation dimensional functions left basic hidden unit right 
components input vector compared rbf center outputs rbfs weighted summed yield function evaluated 
total number centers 

classification 
pose expression teddy image sparse measurements features shape non shape image plane transformation 
recognition pose expression sketch possibly compact 
implementation proposed recognition architecture terms 
network hyper basis functions type 
object recognition inputs image measurements values di erent lters number locations image 
network natural extension template matching scheme contains special case 
dotted lines correspond linear constant terms expansion 
output unit may contain sigmoidal transformation sum inputs see poggio girosi 
generalization eld associated single training view 
easy distinguish say tubular amoeba objects irrespective orientation recognition error rate speci objects categories increases sharply relative familiar view 
gure shows error rate amoeba objects previously seen single attitude viewpoint dependent 
means error rates subjects di erent objects plotted vs rotation depth orthogonal axes edelman edelman 
extent rotation direction center plot corresponds training attitude 
shades gray encode recognition rates increments white better black 
edelman 
note viewpoint independence achieved subject su cient number training views object 
dimensional radial gaussian implemented multiplying dimensional dimensional gaussian receptive eld functions synthesized directly appropriately weighted connections sensor arrays neural receptive elds usually thought arise 
notice implicit position stimuli sensor array number activity unit 
serve dual purpose providing required number representation activity sensor array computing gaussian function 
gaussians acting retinotopic map regarded features radial basis function represents template resulting conjunction lower dimensional features 
poggio girosi 
hbf network proposed object perspective views poggio edelman 
network attempts map view de ned text standard view arbitrarily chosen 
norm di erence output vector standard view thresholded yield answer standard view output directly binary classi cation label 
inputs accommodate input vector representing arbitrary view 
radial basis functions initially centered subset views synthesize system 
training inputs training set associated desired output standard views 
fig 
shows completely equivalent interpretation special case gaussian radial basis functions 
gaussian functions synthesized multiplying outputs dimensional gaussian receptive elds look retinotopic map object point features 
solid circles image plane represent gaussians associated rst radial basis function represents rst view object 
dotted circles represent receptive elds synthesize gaussian radial function associated view 
gaussian receptive elds values features represented implicitly activity retinotopic array product computes radial function need norms exponentials explicitly 
poggio girosi 

