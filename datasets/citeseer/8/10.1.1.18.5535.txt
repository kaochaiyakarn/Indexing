massachusetts institute technology artificial intelligence laboratory center biological computational learning department brain cognitive sciences memo january active learning statistical models david cohn zoubin ghahramani michael jordan cohn psyche mit edu zoubin psyche mit edu jordan psyche mit edu publication retrieved anonymous ftp publications ai mit edu 
learners compute statistically optimal way select data 
review techniques feedforward neural networks mackay cohn :10.1.1.33.830
show principles may select data alternative statistically learning architectures mixtures gaussians locally weighted regression 
techniques neural networks expensive approximate techniques mixtures gaussians locally weighted regression cient accurate 
copyright massachusetts institute technology report describes research done center biological computational learning arti cial intelligence laboratory massachusetts institute technology 
support center provided part national science foundation contract asc 
authors funded mcdonnell pew foundation atr human information processing laboratories siemens corporate research nsf cda ce naval research 
michael jordan nsf presidential young investigator 
aversion appears tesauro touretzky alspector eds advances neural information processing systems 
morgan kaufmann san francisco ca 
active learning background active learning problem learner ability need uence select training data 
problems great practical interest allow active learning require 
consider problem actively learning mapping set training examples xi yi xi yi learner allowed iteratively select new inputs possibly constrained set observe resulting output incorporate new examples training set 
primary question active learning choose try 
heuristics choosing intuition including choosing places don data whitehead perform poorly linden weber wehave low con dence thrun moller expect change model cohn previously data resulted learning schmidhuber 
consider may select optimally statistical viewpoint 
rst review statistical approach applied neural networks described mackay 
consider alternative learning architectures mixtures gaussians locally weighted regression 
optimal data selection neural network computationally expensive approximate nd optimal data selection statistical models cient accurate 
active learning statistical approach denote learner output input 
mean squared error output expressed sum learner bias variance 
vari ance indicates learner uncertainty esti mate goal select new example resulting example added training set integrated variance iv minimized iv dx known distribution practice compute monte carlo approximation integral evaluating number random points drawn 
selecting minimize iv requires computing new variance 
commit know see minimization performed deterministically 
learning architec explicitly denoted functions simplicity setting 
results extend easily multivariate case 
contrasts related white concerned ltering existing data set 
tures provide estimate yj current data estimate compute expectation selecting minimize expected integrated variance provides solid statistical basis choosing new examples 
example active learning neural network section review techniques optimal experiment design oed minimize estimated variance neural network fedorov mackay cohn 
assume learner training set xi yi parameter vector maximizes likelihood measure 
measure minimum sum squared residual mx yi xi estimated output variance network standard oed approach assumes normality local linearity 
assumptions allow replacing distribution yj estimated mean ance expected value new variance de ne mackay empirical results predictive power equation see cohn 
advantages minimizing criterion grounded statistics optimal assumptions 
furthermore criterion continuous di erentiable 
applicable continuous domains continuous action spaces allows hillclimbing nd best neural networks approach disadvantages 
criterion relies simpli cations strong assumptions 
computing variance estimate requires inversion jwj jwj matrix new example incorporating new examples network requires expensive retraining 
kindermann discuss approach addresses problems 
mixtures gaussians mixture gaussians model gaining popularity machine learning practitioners nowlan specht ghahramani jordan 
assumes data produced mixture gaussians gi fori em algorithm dempster nd best data conditional expectations mixture function approximation 
gaussian gi denote estimated put output means estimated ances xy conditional variance may written yjx xy denote ni possibly fractional number training examples gi takes responsibility ni mx xj xj input gi conditional expectation yi variance yi xy yjx ni expectations variances mixed prior probability responsible xji hi hi pn xjj input conditional expectation resulting mixture variance may written nx nx hi yi yjx ni contrast variance estimate computed neural network approximations 
computed ciently active learning mixture gaussians wewant select minimize mixture gaussians model estimated distribution explicit nx yj hip yj hin yi yjx hi hi 
calculation straightforward gi separately calculating expected variance new point sampled yj change hi 
new expectations combine form learner new expected variance nx yjx ni hi nx expectation computed exactly closed form ni hi ni hi hi xy ii xy hi hi hi hi hi xy xy ii yjx hi yi hi hi yi hi xy locally weighted regression hi consider forms locally weighted regression lwr kernel regression loess model cleveland 
kernel regression computes average yi data set weighted centered loess model performs linear regression points data set weighted centered kernel shape design parameter original loess model uses kernel experiments common gaussian hi xi exp xi smoothing constant 
brevity drop argument hi de ne hi 
write estimated means covariances pp hi xi pp hi yi xy hi xi yi yjx xy express conditional expectations estimated variances kernel loess xy yjx active learning locally weighted wewant select minimize lwr model estimated distribution explicit yj yjx estimate explicit 
de ning weight assigned kernel learner expected new variance kernel loess yjx expectation computed exactly closed form xy xy yjx xy experimental results describe sets experiments demonstrating predictive power query selection criteria 
rst set learners trained data noisy sine wave 
criteria described applied predict new training example selected point decrease learner variance 
predictions actual changes variance training points queried added plotted figures 
second set experiments applied techniques learning kinematics planar arm see cohn details 
illustrate problem loess algorithm 
example correlation predicted actual changes variance problem plotted 
figures demonstrate correlation may exploited guide sequential query selection 
compared loess learner selected new query minimize expected variance loess learners selected queries various heuristics 
variance minimizing learner signi cantly outperforms heuristics terms variance mse 
summary mixtures gaussians locally weighted regression statistical models er elegant representations cient learning algorithms 
neural network predicted change actual change upper portion plot indicates neural network noisy sinusoidal data 
lower portion plot indicates predicted actual changes network average estimated variance queried added training set 
changes plotted scale ts 
mixture gaussians predicted change actual change fit data correlation mixture gaussians 
shown er opportunity active learning cient statistically correct manner 
criteria derived computed cheaply problems tested demonstrate predictive power 
cleveland devlin grosse 
regression local tting 
journal econometrics 
cohn atlas ladner 
training connectionist networks queries selective sampling 
touretzky ed advances neural information processing systems morgan kaufmann 
cohn 
neural network exploration optimal experiment design 
cowan eds advances neural information processing systems 
morgan kaufmann 
kernel regression predicted change actual change fit data correlation kernel regression 
loess predicted change actual change fit data correlation loess model 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
royal statistical society series 
fedorov 
theory optimal experiments 
academic press new york 
ghahramani jordan 
supervised learning incomplete data em approach 
cowan eds advances neural information processing systems 
morgan kaufmann 
linden weber 
implementing inner drive competence re ection 
roitblat eds proc 
nd int 
conf 
simulation adaptive behavior mit press cambridge 
mackay 
information objective functions active data selection neural computation 
nowlan 
soft competitive adaptation neural network learning algorithms fitting statistical mixtures 
cmu cs school computer science carnegie mellon university pittsburgh pa kindermann 

bayesian query construction neural network models 
volume 
actual delta variance arm kinematics problem 
predicted delta variance predicted vs actual changes model variance loess arm kinematics problem 
candidate points shown model trained initial random examples 
note potential queries produce little improvement algorithm successfully identi es help 
white 
selecting concise training sets clean data 
ieee transactions neural networks 
schaal atkeson 
robot juggling implementation memory learning 
control systems magazine 
schmidhuber 
reinforcement driven information acquisition nondeterministic environments 
tech 
report fakultat fur informatik technische universitat munchen 
specht 
general regression neural network 
ieee trans 
neural networks 
thrun moller 
active exploration dynamic environments 
moody editors advances neural information processing systems 
morgan kaufmann 
whitehead 
study cooperative mechanisms faster reinforcement learning 
tr dept computer science rochester univ rochester ny 
variance bias support variance training examples variance loess learner selecting queries variance minimizing criterion discussed heuristics 
sensitivity queries output sensitive data bias queries bias minimizing criterion support queries model data support 
variance random sensitivity scale 
curves medians runs non gaussian noise 
mse random sensitivity bias support variance training examples mse loess learner selecting queries variance minimizing criterion discussed heuristics described previous gure 

