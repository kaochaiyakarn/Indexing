massachusetts institute technology artificial intelligence laboratory center biological computational learning department brain cognitive sciences memo january convergence properties em algorithm gaussian mixtures lei xu michael jordan jordan ai mit edu publication retrieved anonymous ftp publications ai mit edu 
build mathematical connection expectation maximization em algorithm gradient approaches maximum likelihood learning nite gaussian mixtures 
show em step parameter space obtained gradient projection matrix provide explicit expression matrix 
analyze convergence em terms special properties provide new results analyzing ect likelihood surface 
mathematical results comparative discussion advantages disadvantages em algorithms learning gaussian mixture models 
copyright massachusetts institute technology report describes research done center biological computational learning arti cial intelligence laboratory massachusetts institute technology 
support center provided part national science foundation contract asc 
support laboratory arti cial intelligence research provided part advanced research projects agency department research contract 
authors supported hk cuhk mcdonnell pew foundation human information processing research laboratories siemens ce naval research 
michael jordan nsf presidential young investigator 
expectation maximization em algorithm general technique maximum likelihood ml maximum posteriori map estimation 
emphasis neural network literature probabilistic models led increased interest em possible alternative gradient methods optimization 
em variations traditional theme gaussian mixture modeling ghahramani jordan nowlan xu jordan tresp ahmad xu jordan hinton novel chain structured treestructured architectures bengio frasconi jordan jacobs 
empirical results reported papers suggest em considerable promise optimization method architectures 
new theoretical results obtained link em topics learning theory amari jordan xu neal hinton xu jordan yuille stolorz 
despite developments grounds caution promise em algorithm 
reason caution comes consideration theoretical convergence rates rst order algorithm 
precisely key results available statistical literature convergence em 
established mild conditions em guaranteed converge local maximum log likelihood dempster laird rubin redner walker wu 
convergence monotonic value parameter vector iteration 
second considering em mapping xed point near surely 
em rst order algorithm 
rst order convergence em cited statistical literature major drawback 
redner walker widely cited article argued superlinear quasi newton method scoring second order newton methods generally preferred em 
reported empirical results demonstrating slow convergence em gaussian mixture model problem mixture components separated 
results include tests competing algorithms 
convergence optimal parameter values slow experiments convergence likelihood rapid 
redner walker acknowledge results show iterative algorithm said local convergence rate order su ciently large 
component populations mixture poorly separated em algorithm expected produce small number iterations parameter values mixture density determined sample data 
context current literature learning predictive aspect data modeling emphasized expense traditional statistician concern true values parameters rapid convergence likelihood major desideratum learning algorithm undercuts critique em slow algorithm 
current comparative analysis em optimization methods 
emphasize comparison em rst order methods gradient ascent conjugate gradient methods tended methods choice neural network literature 
compare em superlinear second order methods 
argue em number advantages including naturalness handling probabilistic constraints mixture problems guarantees convergence 
provide new results suggesting appropriate conditions em may fact approximate superlinear method explain promising empirical results obtained jordan jacobs temper critique em ered redner walker :10.1.1.136.9119
analysis current focuses unsupervised learning related results supervised learning domain see jordan xu press 
remainder organized follows 
rst brie review em algorithm gaussian mixtures 
second section establishes connection em gradient log likelihood 
comparative discussion advantages disadvantages various optimization algorithms gaussian mixture setting 
empirical results suggesting em condition number ective hessian 
fourth section presents theoretical analysis empirical nding 
nal section presents 
em algorithm gaussian mixtures study probabilistic model xj kx jj jp mj mj andd dimensionality parameter vector consists mixing proportions mean vectors mj covariance matrices independent identically distributed samples fx obtain log likelihood log ny nx log optimized iterative algorithm see dempster laird rubin pn pn pn pn posterior probabilities de ned follows jm jm connection em gradient ascent theorem establish relationship gradient log likelihood step parameter space taken em algorithm 
particular show em step obtained premultiplying gradient de nite matrix 
provide explicit expression matrix 
theorem iteration em algorithm eq 
ja mj mj mj vec vec vec mj pn pn denotes vector mixing proportions indexes mixture components denotes iteration number vec denotes vector obtained stacking column vectors focus maximum likelihood ml estimation straightforward apply results maximum posteriori map estimation multiplying likelihood prior 
matrix denotes kronecker product 
constraints pk de nite matrix matrices mj positive de nite probability su ciently large 
proof 
considering em update mixing proportions eqs 
nx premultiplying obtain ja nx nx pk pk update formula eq 
rewritten nx combining equations establishes update rule eq 

furthermore arbitrary vector ut diag ut jensen inequality wehave diag kx kx uj ut andp positive de nite constraints pk 
em update means mi 
follows eqs 
mj mj nx premultiplying mj yields mj mj mj pn nx eq 
pn positive de nite probability assuming large matrix full rank 
follows eq 
mj positive de nite probability 
prove third part theorem 
follows eqs 
nx mind rewrite em update formula nx pn nx pn utilizing identity vec abc vec obtain vec vec pn arbitrary matrix wehave vec 
vec tr tr vec vec equality holds 
equality impossible positive de nite su ciently large 
follows eq 
pn positive de nite probability 
notation vec vec diag pm pa combine updates theorem single equation conditions theorem positive de nite matrix probability 
recalling positive de nite matrix wehave corollary corollary iteration em algorithm eq search direction projection gradient em algorithm viewed variable metric gradient ascent algorithm projection matrix changes iteration function current parameter value results extend earlier results due baum sell 
baum sell studied recursive equations form having positive coe cients 
showed search direction recursive formula positive projection gradient respect see levinson rabiner sondhi 
shown baum sell recursive formula implies em update formula gaussian mixture 
rst statement theorem special case baum sell earlier 
baum sell theorem existence theorem provide explicit expression matrix pa transforms gradient direction em direction 
theorem provides explicit form pa generalize baum sell results handle updates mj provide explicit expressions positive de nite transformation matrices 
worthwhile compare em algorithm gradient optimization methods 
newton method obtained premultiplying gradient inverse hessian log likelihood newton method method choice applied algorithm di cult practice 
particular algorithm diverge hessian nearly singular computational costs computing inverse hessian step considerable 
alternative approximate inverse recursively updated matrix modi cation called quasi newton method 
conventional quasi newton methods unconstrained optimization methods modi ed order mixture setting probabilistic constraints parameters 
addition quasi newton methods generally require dimensional search performed iteration order guarantee convergence 
em algorithm viewed special form quasi newton method projection matrix eq 
plays role discuss remainder particular matrix number favorable properties em particularly attractive optimization mixture setting 
constrained optimization general convergence important property matrix em step parameter space automatically satis es probabilistic constraints mixture model eq 

domain contains regions embody probabilistic constraints positive de 
em algorithm update mixing proportions rewritten follows nx jm jm obvious iteration stays 
similarly update rewritten pn nx jm pk jm stays su ciently large 
em automatically satis es probabilistic constraints mixture model optimization techniques generally require modi cation satisfy constraints 
approach iterative step keep parameters constrained domain 
anumber techniques developed including feasible direction methods active sets gradient projection reduced gradient linearly constrained quasi newton 
constrained methods incur extra computational costs check maintain constraints theoretical convergence rates constrained algorithms need corresponding unconstrained algorithms 
second approach transform constrained optimization problem unconstrained problem unconstrained method 
accomplished penalty barrier functions lagrangian terms re parameterization 
extra algorithmic machinery renders simple comparisons unconstrained convergence rates problematic 
easy meet constraints covariance matrices mixture techniques 
second appealing property ofp iteration em guaranteed increase likelihood 
monotonic convergence likelihood achieved step size parameters line searches 
gradient optimization techniques including gradient descent quasi newton newton method provide simple theoretical guarantee assuming constrained problem transformed unconstrained 
gradient ascent step size chosen ensure ki 
requires dimensional line search optimization iteration requires extra computation convergence 
alternative isto small value generally ki close results slow convergence 
newton method iterative process usually required near solution hessian may nite iteration may converge 
levenberg marquardt methods handle inde nite hessian matrix problem dimensional optimization form search required suitable scalar added diagonal elements hessian 
fisher scoring methods handle inde nite hessian matrix problem non quadratic nonlinear optimization fisher scoring requires stepsize obeys ki bh fisher information matrix 
problems similar gradient ascent arise 
quasi newton methods conjugate gradient methods onedimensional line search required iteration 
summary gradient methods incur extra computational costs iteration rendering simple comparisons local convergence rates unreliable 
large scale problems algorithms change parameters immediately data point line algorithms signi cantly faster practice batch algorithms 
popularity gradient descent algorithms neural networks part ease obtaining line variants gradient descent 
worth noting line variants em algorithm derived neal hinton titterington factor weighs favor em compared conjugate gradient newton methods 
convergence rate comparisons section provide comparative theoretical discussion convergence rates constrained gradient ascent em 
gradient ascent result obtained taylor expanding log likelihood maximum likelihood estimate su ciently large ki kk ki hessian step size jg denote largest smallest eigenvalues respectively 
smaller values correspond faster convergence rates 
guarantee convergence require obtained 
minimum possible value rmin number larger values condition number correspond slower convergence 
wehave rmin corresponds superlinear rate convergence 
newton method viewed method obtaining desirable condition number inverse hessian balances hessian resulting condition number 
ectively newton regarded gradient ascent new function ective hessian identity matrix practice usually quite large 
larger di cult compute accurately 
di cult balance hessian desired 
addition mentioned previous section hessian varies point point parameter space iteration need recompute inverse hessian 
quasi newton methods approximate positive matrix easy compute 
discussion far treated unconstrained optimization 
order compare gradient ascent em algorithm constrained mixture estimation problem consider gradient projection method projection matrix projects gra 
gradient projection iteration remain long initial parameter vector 
iteration initial keep su ciently small iteration 
suppose em set independent unit basis vectors span space 
basis respectively ck representation projective gradient algorithm eq 
simple gradient ascent eq 
ket kk result convergence rate bounded rc ke negative de nite obtain rc hc hc equation hc hessian restricted 
see derivation convergence speed depends hc hc hc 
hc hc hc hc principle equal zero selected appropriately 
case superlinear rate obtained 
generally hc smaller values hc corresponding faster convergence 
analysis em algorithm 
em keeps parameter vector automatically 
new basis connection em gradient ascent cf 
eq 
ke ph kk rc ke ph ph ph equation manipulated yield rc phe phe see convergence speed em depends phe phe phe 
phe phe phe phe phe 
case superlinear rate obtained 
discuss possibility obtaining superlinear convergence em detail 
results show convergence gradient ascent em depend shape log likelihood measured condition number 
near con guration quite regular update direction points directly solution yielding fast convergence 
isvery large surface elongated shape search update direction zigzag path making convergence slow 
key idea newton quasi newton methods reshape surface 
nearer ball shape newton method achieves shape ideal case better convergence 
quasi newton methods aim achieve ective hessian condition number close possible 
interestingly results suggest projection matrix em algorithm serves ectively reshape likelihood yielding ective condition number tends 
rst empirical results support suggestion theoretical analysis 
condition number sign solid original hessian dash dot 
constrained hessian dashed em equivalent hessian learning steps condition number original hessian constrained hessian em equivalent hessian learning steps experimental results estimation parameters component gaussian mixture 
condition numbers function iteration number 
zoomed version discarding rst iterations 
terminology original constrained em equivalent hessians refers matrices phe respectively 
sampled points simple nite mixture model pi expf mi parameter values follows 
ran em algorithm gradient ascent data 
step simulation calculated condition number hessian condition number determining rate convergence gradient algorithm condition number determining rate convergence em 
calculated largest eigenvalues matrices results shown fig 

seen fig 
condition numbers change rapidly vicinity th iteration corresponding hessian matrices inde nite 
afterward hessians quickly de nite condition numbers converge 
shown fig 
condition numbers converge values 
matrix greatly reduced condition number factors 
signi cantly improves shape speeds convergence 
ran second experiment means component gaussians andm 
results similar shown fig 

distance distributions reduced half interestingly em algorithm converges soon afterward showing problem em spends little time region parameter space local analysis valid 
condition number sign solid original hessian dash dot 
constrained hessian dashed em equivalent hessian learning steps condition number solid original hessian constrained hessian em equivalent hessian learning steps experimental results estimation parameters component gaussian mixture cf 
fig 

separation gaussians half separation fig 

maximum eigenvalue original hessian constrained hessian em equivalent hessian learning steps maximum eigenvalue original hessian constrained hessian em equivalent hessian learning steps largest eigenvalues matrices phe plotted function number iterations 
plot experiment fig 
experiment reported fig 

shape irregular 
condition number increases increases increases 
see signi cant improvement inthe case em factors 
fig 
shows matrix reduced largest eigenvalues hessian 
demonstrates clearly stable convergence obtained em line search need external selection learning stepsize 
remainder provide theoretical analyses attempt shed empirical results 
illustrate issues involved consider degenerate mixture problem mixture single component 
case 
furthermore assume covariance matrix xed mean vector estimated 
hessian respect mean em projection matrix gradient ascent larger ci 
em hand achieves condition number exactly phe ph phe 
em new ton method simple quadratic problem 
general non quadratic optimization problems newton retains quadratic assumption yielding fast convergence possible divergence 
em conservative algorithm retains convergence guarantee maintains quasi newton behavior 
analyze behavior detail 
consider special case estimating means gaussian mixture gaussians separated 
theorem consider em algorithm eq 
parameters assumed known 
assume gaussian distributions separated su ciently large posterior probabilities nearly zero 
condition number associated smaller condition number associated gradient ascent 
furthermore approaches goes nity 
proof 
hessian hij mi nx hk hk nx ij mj mi ij ij 
projection matrix diag kk jj pn negligible su ciently large second term eq 
neglected yielding hii pn diag 
implies ph ph 
theorem restrictive assumptions gives indication projection matrix em algorithm appears condition hessian yielding improved convergence 
fact conjecture theorem extended apply widely particular case full em update mixing proportions covariances estimated limits cases means separated 
obtain initial indication possible conditions usefully imposed separation mixture components studied case second term eq 
neglected hii retained hij components consider example case univariate mixture having mixture components 
xed mixing proportions xed covariances hessian matrix eq 
projection matrix eq 
hij hii nx nx mj mi 
negative de nite show theorem remain true gaussians necessarily separated 
proof achieved lemma lemma consider positive de nite matrix diagonal matrix diag 
proof 
eigenvalues roots condition number written de ned follows furthermore eigenvalues roots gives 
de ning 
quotient 
monotonically increasing function 

think possible generalize lemma univariate component case weakening conditions general setting 
provided comparative analysis algorithms learning gaussian mixtures 
focused em algorithm forged link em gradient methods projection matrix analyzed convergence em terms properties matrix ect likelihood surface 
em number properties particularly attractive algorithm mixture models 
enjoys automatic satisfaction probabilistic constraints monotonic convergence need set learning rate low computational overhead 
em reputation slow algorithm feel mixture setting slowness em 
em converge slowly problems mixture components separated hessian poorly conditioned problems gradient algorithms including newton method perform poorly 
concern convergence likelihood em generally performs ill conditioned problems 
algorithm provides certain amount cases despite poor conditioning 
important emphasize case poorly separated mixture components viewed problem model selection mixture components included model handled regularization techniques 
fact em rst order algorithm certainly implies em panacea imply em advantages gradient ascent superlinear methods 
important appreciate convergence rate results generally obtained unconstrained optimization necessarily indicative performance constrained optimization problems 
demonstrated conditions condition number ective hessian em algorithm tends showing em approximate superlinear method 
cases poorly conditioned hessian superlinear convergence necessarily virtue 
cases optimization schemes including em essentially revert gradient ascent 
feel em continue play important role development learning systems emphasize predictive aspect data modeling 
em played critical role development markov models hmm important example predictive data modeling 
em generally converges rapidly setting 
similarly case hierarchical mixtures experts empirical results convergence likelihood quite promising jordan jacobs waterhouse robinson 
play important conceptual role organizing principle design learning algorithms 
role case focus attention missing variables problem 
clari es structure algorithm invites comparisons statistical physics missing variables provide powerful analytic tool 
amari 
press information geometry em em algorithms neural networks neural networks 
baum sell 
growth transformation functions manifolds pac 
math 
bengio frasconi input output hmm architecture 
advances neural information processing systems eds tesauro touretzky alspector san mateo ca morgan kaufmann 

convergence em algorithm royal statistical society 
dempster laird rubin 
maximum likelihood incomplete data em algorithm royal statistical society 
ghahramani jordan 
function approximation density estimation em approach advances neural information processing systems eds cowan tesauro alspector san mateo ca morgan kaufmann 
jordan jacobs 
hierarchical mixtures experts em algorithm 
neural computation 
jordan xu 
press convergence results em approach mixtures experts architectures neural networks 
levinson rabiner sondhi 
application theory applications hmm parameter estimation process employed solely yield models high likelihood parameters generally endowed particular meaning 
probabilistic functions markov process automatic speech recognition bell system technical journal 
neal hinton 
new view em algorithm justi es incremental variants university department computer science preprint 
nowlan 

soft competitive adaptation neural network learning algorithms tting statistical mixtures 
tech 
rep cmu cs cmu pittsburgh pa redner walker 
mixture densities maximum likelihood em algorithm siam review 
titterington 
recursive parameter estimation incomplete data royal statistical society 
tresp ahmad 
training neural networks de cient data advances neural information processing systems eds cowan tesauro alspector san mateo ca morgan kaufmann 
waterhouse robinson classication hierarchical mixtures experts ieee workshop neural networks signal processing 
wu 

convergence properties em algorithm annals statistics 
xu jordan 
unsupervised learning em algorithm nite mixture gaussians proc 
portland vol 
ii 
xu jordan 
em learning generalized nite mixture model combining multiple classi ers proc 
portland vol 
iv 
xu jordan 
theoretical experimental studies em algorithm unsupervised learning nite gaussian mixtures mit computational technical report dept brain cognitive science mit cambridge ma 
xu jordan hinton 
modied gating network mixtures experts architecture proc 
san diego vol 

yuille stolorz 
statistical physics mixtures distributions em algorithm neural computation 

