probabilistic abstraction hierarchies eran segal computer science dept stanford university eran cs stanford edu daphne koller computer science dept stanford university koller cs stanford edu dirk ormoneit computer science dept stanford university ormoneit cs stanford edu domains naturally organized abstraction hierarchy taxonomy instances nearby classes taxonomy similar 
provide general probabilistic framework clustering data set classes organized taxonomy class associated probabilistic model data generated 
clustering algorithm simultaneously optimizes things assignment data instances clusters models associated clusters structure abstraction hierarchy 
unique feature approach utilizes global optimization algorithms steps reducing sensitivity noise propensity local maxima characteristic algorithms hierarchical agglomerative clustering take local steps 
provide theoretical analysis algorithm showing converges local maximum joint likelihood model data 
experimental results synthetic data real data domains gene expression text 
domains naturally associated hierarchical taxonomy form tree instances close tree assumed similar instances away 
biological systems example creating taxonomy instances steps understanding system 
particular analyzing gene expression data focused creating gene hierarchies 
similarly text domains creating hierarchy documents common task :10.1.1.14.5443:10.1.1.33.8915
applications hierarchy unknown discovering hierarchy key part analysis 
standard algorithms applied problem typically agglomerative bottom approach divide conquer top approach 
methods shown useful practice suffer limitations proceed series local improvements making particularly prone local maxima 
second bottom approaches typically applied raw data models constructed post processing step 
domain knowledge type distribution data instances sampled rarely formation hierarchy 
probabilistic abstraction hierarchies pah probabilistically principled general framework learning abstraction hierarchies data overcomes difficulties 
bayesian approach different models correspond different abstraction hierarchies 
prior designed enforce intuitions taxonomies nearby classes similar data distributions 
specifically model pah tree node tree associated class specific probabilistic model cpm 
data generated leaves tree model basically defines mixture distribution components cpms leaves tree 
cpms internal nodes define prior models prefer models cpm child node close cpm parent relative distance function cpms 
framework allows wide range notions distance models essentially require distance function convex parameters cpms 
example cpm gaussian distribution simple squared euclidean distance parameters cpms 
novel algorithm learning model parameters tree structure framework 
algorithm structural em sem approach utilizes global optimization steps learning best possible hierarchy cpm parameters see similar global optimization steps sem :10.1.1.112.7737
step procedure guaranteed increase joint probability model data sem procedure guaranteed converge local optimum 
approach advantages 
provides principled probabilistic semantics hierarchical models 
model allows exploit domain structural knowledge easily 
utilizes global optimization steps tend avoid local maxima help model sensitive noise 
abstraction hierarchy tends pull parameters model closer nearby ones naturally leads form parameter smoothing shrinkage :10.1.1.14.5443
experiments pah synthetic data real data sets gene expression text 
results show pah approach produces hierarchies robust noise data learned hierarchies generalize better test data produced hierarchical agglomerative clustering 
probabilistic abstraction hierarchy domain random observation set possible assignments set features 
goal take set instances cluster set classes 
standard flat clustering approaches example autoclass means algorithm special cases generative mixture model 
models data instance belongs classes associated different class specific probabilistic model cpm 
data instance sampled independently selecting classes multinomial distribution randomly selecting data instance cpm chosen class 
standard clustering models relation individual cpms arbitrarily different 
propose model different classes related abstraction hierarchy classes nearby hierarchy similar probabilistic models 
precisely define definition probabilistic abstraction hierarchy pah tree nodes fv vm undirected edges exactly leaves node associated cpm defines distribution denote mm multinomial distribution leaves denote parameters distribution 
framework principle place restrictions form cpms probabilistic model defines probability distribution example may bayesian network case specification include parameters network structure different setting may hidden markov model 
practice choice cpms ramifications hierarchical model algorithm 
discussed assume data generated leaves tree 
augment additional hidden class variable data item takes values denoting leaf chosen generate item 
pah element value define multinomial distribution leaves conditional density data item cpm leaf induced pah leaves dimensional continuous state space visualization gaussian distribution rd dimension 
different weight preserving transformations tree leaves distribution data generated simply summed 
mentioned role internal nodes tree enforce intuitive interpretation model abstraction hierarchy enforcing similarity cpms nearby leaves 
achieve goal defining prior distribution abstraction hierarchies penalizes distance neighboring cpms distance function 
note require distance mathematical sense require symmetric chose undirected trees non negative iff obvious choice define id kl id kl id kl distributions define distance measure advantage applicable pair cpms space parameterization different 
definition define prior exp represents extent differences distances penalized larger represents larger penalty 
set data instances domain goal find pah maximizes equivalently log log 
maximizing expression trading fit mixture model leaves data desire generate hierarchy nearby models similar 
fig 
illustrates typical pah gaussian cpm distributions cpm close leaves tree specialized fairly peaked distributions 
conversely cpms closer root tree acting bridge neighbors expected peaked distributions peak parts distribution common entire subtree 
learning models goal section learn pah data set fd learning task fairly complex aspects unknown structure tree cpms mm nodes parameters assignment instances leaves likelihood function multiple local maxima general method exists finding global maximum 
section provide efficient algorithm finding locally optimal models considered identical 
care taken ensure proper probability distribution case choice 
note desired modify prior incorporate prior parameters 
simplify algorithm assume structure cpms mm fixed 
reduces choice pure numerical optimization problem 
general framework algorithm extends cases solve model selection problem computational issues somewhat different 
discuss case complete data data instance leaf generated 
case show learn structure tree setting parameters problem constructing tree set points fixed closely related steiner tree problem virtually variants np hard 
propose heuristic approach decouples joint optimization problem subproblems optimizing cpm parameters tree structure learning tree structure set cpms 
somewhat surprisingly show careful choice additive prior allows subproblems tackled effectively global optimization techniques 
task learning cpms 
assume structure tree assignment data instance leaves denoted 
remains find min min minimize log log 
substituting definitions get jdj log log term involving multinomial parameters separates rest optimization relative reduces straightforward maximum likelihood estimation 
optimize cpm parameters key property turns convexity function holds wide variety choices cpms particular holds models experiments 
convexity property allows find global minimum simple iterative procedure 
iteration optimize parameters fixing parameters remaining cpms 
procedure repeated round robin fashion convergence 
joint convexity iterative procedure guaranteed converge global minimum examination shows optimization cpm involves data cases assigned leaf parameters cpms neighbors tree simplifying computation substantially 
turn attention second subproblem learning structure tree learned cpms 
consider empty tree containing unconnected leaf nodes find optimal parameter settings leaf cpm described 
note cpms unrelated parameters computed independently cpms 
initial set cpms leaf nodes algorithm tries learn tree structure relative cpms 
goal find lowest weight tree subject restriction tree structure keep set leaves due decomposability log penalty tree measured sum edge weights 
problem variant steiner tree problem 
heuristic substitute follow lines minimum spanning tree mst algorithm constructing low weight trees 
iteration algorithm starts tree set nodes vm takes leaves tree constructs mst 
course resulting tree longer leaves 
problem corrected transformation pushes leaf tree duplicating model transformation preserves weight score tree 
algorithm simply throws away entire structure previous tree 
construct new msts built nodes vm previous tree 
nodes internal nodes perform transformation described 
cases transformation unique depends order steps executed see fig 

algorithm generates entire pool candidate trees vm generated different random resolutions ambiguities weight preserving transformation 
tree cpm learning algorithm find optimal setting parameters 
trees evaluated relative score log highest scoring tree kept 
tree just constructed new set cpms repeat process 
detect termination algorithm keeps tree previous iteration terminates score trees newly constructed pool lower score best tree previous iteration 
address fact data incomplete assignments data instances classes determined 
address problem incomplete data standard expectation maximization em algorithm structural em algorithm extends em problem model selection 
starting initial model algorithm iterates steps step computes distribution unobserved variables observed data current model 
case distribution unobserved variables computed evaluating jdj 
step learns new models increase expected log likelihood data relative distribution computed step 
case step precisely algorithm complete data described soft assignment data instances nodes tree 
full algorithm shown fig 

simple analysis lines show log probability log increases step 
obtain theorem theorem algorithm fig 
converges local maximum log 

initialize fm mkg models leaves 
randomly initialize 

repeat convergence step choose mst subset fv edge weights 
ii 
transform mst vk leaves 
step compute posterior probabilities indicator variable 
step update cpms 
fd arg max log abstraction hierarchy learning algorithm experimental results focus experimental results genomic expression data provide results text dataset 
gene expression data level mrna transcript gene cell measured simultaneously dna microarray technology 
genomic expression data provides researchers insight understanding cellular behavior 
commonly method analyzing data clustering process identifies clusters genes share similar expression patterns involved similar cellular processes 
apply pah data cpms form case kl distance simply id kl simply sum squared distances means corresponding gaussian components normalized variance 
define id kl 
popular clustering method genomic expression data date hierarchical agglomerative clustering hac builds hierarchy genes iteratively merging closest genes relative distance metric 
distance metric hac 
note hac metric distance data cases algorithm distance models 
perform direct comparison pah hac need obtain probabilistic model hac 
create cpms genes hac assigned internal node 
pah hac assign gene training set test set hierarchy choosing best highest likelihood cpm nodes tree including internal nodes recording probability best cpm assigns gene 
structure recovery 
algorithm learning abstraction hierarchies recover true hierarchy possible 
test generated synthetic data set measured ability method recover distances pairs instances genes generating model distance length path genes hierarchy 
generated data set sampling leaves pah data realistic sampled pah learned real gene expression data set 
allow comparison hac generated data instance leaf 
generated data imaginary genes experiments total measurements 
robustness generated different data sets ran pah hac data set 
correlation error pairwise distances original learned tree measures 
correlation pah compared worse hac 
average error pah hac 
results show pah recovers abstraction hierarchy better hac 
generalization 
tested ability different methods generalize unobserved test data measuring extent method captures underlying structure data 
ran tests yeast data set 
selected genes significant changes expression full set experiments 
ran pah hac evaluated performance fold cross validation 
pah different settings coefficient penalty term explores performance range fitting data greatly favoring hierarchies nearby models similar large 
cases learned model training data evaluated log likelihood test instances described 
results summarized fig 
clearly show pah generalizes better previously unobserved data hac pah works best tradeoff fitting data generating hierarchy nearby models similar 
robustness 
goal constructing hierarchy extract meaningful biological data 
data invariably partial noisy 
analysis produces different results slightly different training data biological meaningful 
want genes assigned nearby nodes tree close hierarchies learned perturbed data sets 
tested robustness noise learning model original data set perturbed data sets permuted varying percentage expression 
compared distances path length tree nodes assigned pair genes trees learned original data trees learned perturbed data sets 
results shown fig 
demonstrating pah preserves pairwise distances extremely data perturbed performs reasonably permutation hac completely deteriorates data permuted 
generalization test data robustness noise training set test set model avg 
difference avg 
difference pah hac pah hac pah hac robustness pah hac different subsets training instances 
word hierarchy learned cora data 
second important test robustness particular choice training data particular training set reflects subset experiments performed 
experiment yeast compendium data measures expression profiles triggered specific gene mutations 
selected genes arrays focusing genes changed significantly 
values ranging generated different training sets sampling replacement percent genes rest form test set 
placed training test genes hierarchy 
data set pair genes appear training set test set appear appears training set test set 
compared pair genes distances training sets appear distances test sets appear 
results summarized fig 

results training data show pah consistently constructs similar hierarchies different subsets data 
contrast hierarchies constructed hac consistent 
results test data striking 
pah consistent classification test instances ones construct hierarchy 
fact significant difference performance training data test data 
contrast hac places test instances different configurations different trees reducing confidence biological validity learned structure 
intuitiveness 
get qualitative insight hierarchies produced ran pah documents probabilistic methods category cora dataset cora whizbang com learned hierarchies stemmed words 
constructed vector word entry document value tfidf weighted frequency word document 
fig 
shows parts learned hierarchy consisting nodes list high confidence words node 
pah organized related words region tree 
region words arranged way consistent intuitive notion abstraction 
discussion probabilistic abstraction hierarchies general framework learning abstraction hierarchies data relates different classes hierarchy tree nodes correspond class specific probability models cpms 
utilize bayesian approach prior favors hierarchies nearby classes similar data distributions penalizing distance neighboring cpms 
unique feature pah global optimization steps constructing hierarchy finding optimal setting entire set parameters 
feature differentiates approaches build hierarchies local improvements objective function approaches optimize fixed hierarchy :10.1.1.33.8915
global optimization steps help avoiding local maxima reducing sensitivity noise 
approach leads naturally form parameter smoothing provides better generalization test data robustness noise clustering approaches 
principle probabilistic model cpm long defines probability distribution state space 
applied approach substantially complex problem clustering proteins amino acid sequence profile hmms 

nir friedman useful comments 
supported nsf aci nsf itr program sloan foundation 
eran segal supported stanford graduate fellowship 
cheeseman stutz 
bayesian classification autoclass theory results 
aaai press 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
eisen spellman brown botstein 
cluster analysis display genome wide expression patterns 
pnas 
friedman 
bayesian structural em algorithm 
proc 
uai 
friedman pe er 
structural em algorithm inference 
proc 
recomb 
genomic expression program response yeast cells environmental changes 
mol 
bio 
cell 
hofmann :10.1.1.33.8915
cluster abstraction model unsupervised learning topic hierarchies text data 
proc 
ijcai 
hofmann 
cluster abstraction model unsupervised learning topic hierarchies text data 
proc 
international joint conference artificial intelligence 
hughes functional discovery compendium expression profiles 
cell 
hwang richards winter 
steiner tree problem 
annals discrete mathematics vol 
north holland 
krogh brown mian haussler 
hidden markov models computational biology applications protein modeling 
mol 
biology 
mccallum rosenfeld mitchell ng :10.1.1.14.5443
improving text classification shrinkage hierarchy classes 
proc 
icml 
meila jordan 
learning mixtures trees 
machine learning 
segal koller 
probabilistic hierarchical clustering biological data 
recomb 
