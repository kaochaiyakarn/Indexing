artificial intelligence www elsevier com locate summarization sentence extraction probabilistic approach sentence compression kevin knight daniel marcu information sciences institute department computer science university southern california admiralty way suite marina del rey ca usa received may humans produce summaries documents simply extract sentences concatenate 
create new sentences grammatical capture salient pieces information original document 
large collections text pairs available online possible envision algorithms trained mimic process 
focus sentence compression simpler version larger challenge 
aim achieve goals simultaneously compressions grammatical retain important pieces information 
goals conflict 
devise noisy channel decision tree approach problem evaluate results manual compressions simple baseline 
elsevier science rights reserved 
keywords summarization compression noisy channel model 
research automatic summarization focused extraction identifying important clauses sentences paragraphs texts see representative collection papers 
determining important textual segments half summarization system needs cases simple extended version statistics summarization step sentence compression received outstanding awards aaai austin tx usa 
corresponding author 
mail addresses knight isi edu knight marcu isi edu marcu 
urls www isi edu knight knight www isi edu marcu marcu 
see front matter elsevier science rights reserved 
pii knight marcu artificial intelligence catenation textual segments yield coherent outputs 
number researchers started address problem generating coherent summaries mckeown barzilay jing mckeown barzilay marcu gerber context multidocument summarization mani context revising single document extracts 
approaches proposed witbrock mittal banko berger mittal jing hauptmann ones apply probabilistic model trained directly summary document pairs 
summary restricted headlines models scale generating multiple sentence abstracts formed grammatical sentences 
goal generate coherent abstracts 
contrast intend eventually text tuples widely available order automatically learn rewrite texts coherent abstracts 
spirit statistical mt community focused sentence sentence translations decided focus simpler problem sentence compression 
chose problem reasons problem complex require development sophisticated compression models determining important sentence determining convey important information grammatically words just scaled version text summarization problem 
problem simple worry discourse related issues coherence anaphors second adequate solution problem immediate impact applications 
example due time space constraints generation tv captions requires important parts sentences shown screen 
sentence compression module impact task automatic generation 
sentence compression module provide audio scanning services blind faster access web pda devices 
general systems aimed producing coherent abstracts implement manually written sets sentence compression rules sentence compression module impact quality systems 
particularly important text genres long sentences 
previous rule addressing sentence compression includes jing mckeown mahesh carroll canning 
new data driven approaches sentence compression problem 
take input sequence words wn sentence 
algorithm may drop subset words 
words remain order unchanged form compression 
compressions choose reasonable 
approach develops probabilistic noisy channel model sentence compression 
second approach develops decision deterministic model 
knight marcu artificial intelligence section evaluate manual compressions simple baseline 
approaches evaluate algorithms discuss extended problem text compression 

noisy channel model sentence compression section describes probabilistic approach compression problem 
particular adopt noisy channel framework successful large number nlp applications including speech recognition machine translation partof speech tagging transliteration information retrieval 
framework look long string imagine originally short string added additional optional text 
compression matter identifying original short string 
critical original string real hypothetical 
example statistical machine translation look french string say originally english added noise 
french may may translated english originally removing noise hypothesize english source translate string 
case compression noise consists optional text material pads core signal 
larger case text summarization may useful imagine scenario news editor composes short document hands reporter tells reporter may access editor original version may may exist guess probabilities come 
noisy channel application solve problems source model 
assign string probability gives chance generated original short string hypothetical process 
example may want low ungrammatical 
channel model 
assign pair strings probability gives chance short string expanded result long string example extra word may want low 
word optional additional material 
decoder 
observe long string search short string maximizes 
equivalent searching maximizes 
advantageous break problem way decouples somewhat independent goals creating short text looks grammatical preserves important information 
easier build channel model focuses exclusively having worry 
specify certain substring may represent unimportant information need worry deleting result ungrammatical structure 
leave source model worries exclusively formedness 
fact extensive prior source language modeling speech recognition machine translation knight marcu artificial intelligence natural language generation 
goes actual compression decoding jargon re generic software packages solve problems application domains 

statistical models experiments report build simple source channel models 
departure discussion previous statistical channel models assign probabilities ptree pexpand tree trees strings 
range trees 
decoding new string parse large tree collins parser hypothesize rank various small trees 
source trees ones normal looking parse structure normal looking word pairs 
ptree combination standard probabilistic contextfree grammar pcfg score computed grammar rules yielded tree standard word bigram score computed leaves tree 
example tree np john vp vb saw np mary assigned score factors ptree pcfg top top pcfg np vp pcfg np john np pcfg vp vb np vp pcfg vp saw vb pcfg np mary np john eos saw john mary saw eos mary 
note probability assignments source model sum suits purpose stands 
interpolating bigram probabilities pcfg probabilities way straighten model 
stochastic channel model performs minimal operations small tree create larger tree internal node probabilistically choose expansion template labels node children 
example processing node tree may wish add prepositional phrase third child 
probability pexp np vp pp np vp 
may choose leave probability pexp np vp np vp 
choose expansion template new child node introduced grow new subtree rooted node example pp np pittsburgh 
particular subtree grown probability pcfg factorization bigrams 
simple narrow view text expansion 
channel model allows insertion new subtrees allow sort tree re organization 
phrase np jj roman nn history expanded np np dt nn history pp np rome 
view side imagine faced knight marcu artificial intelligence large tree want list small source trees 
allowed delete sets constituents 
example section show tell potential compression statistical models described 
suppose observe tree fig 
spans string abcde 
consider compression shown 
compute factors ptree pexpand tree 
breaking source pcfg describe ptree pcfg top top pcfg ha pcfg cd pcfg pcfg pcfg 
source word bigram factors eos eos 
channel expansion template factors part pexpand tree pexp ha ha pexp cbd cd new tree growth channel pcfg factors expansion pcfg qr pcfg pcfg pcfg 
different compression scored different set factors 
example consider compression leaves completely untouched 
case source costs ptree fig 

examples parse trees 
knight marcu artificial intelligence pcfg top top pcfg eos pcfg ha pcfg pcfg cbd pcfg pcfg qr pcfg pcfg pcfg channel costs pexpand tree eos 
pexp ha ha pexp cbd cbd pexp qr qr pexp words leave node unchanged 
simply compare tree ptree pexpand tree ptree versus tree ptree pexpand tree ptree select 
note denominator ptree new pcfg factors cancel 
quantities differ proposed compressions boxed 
preferred pcfg cd pexp cbd cd pcfg cbd pexp cbd cbd pexp qr qr pexp 

training corpus order train system ziff davis corpus collection newspaper articles announcing computer products 
articles corpus paired human written abstracts 
automatically extracted corpus set sentence pairs 
pair consisted sentence tn occurred article knight marcu artificial intelligence documentation typical quality excellent 
documentation excellent 
design goals achieved delivered performance matches speed underlying device 
design goals achieved 
reach mail product message management system designed initially lans eventually operating system independent 
eventually operating system independent 
modules may physically electrically incompatible provide industry standard connections 
cable specific provide industry standard connections 
basic level operations products vary widely 
operations products vary widely 
ingres star prices start 
ingres star prices start 
fig 

examples parallel corpus 
possibly compressed version sm occurred human written 
fig 
shows sentence pairs extracted corpus selected demonstrate various types compression dropping words 
possibly compressed sentence uses words long sentence words sentences occurred order 
fig 
shows sentence pairs extracted corpus common words long compressed version sentences displayed italics 
decided corpus examples shown fig 
consistent desiderata specific summarization human written sentences grammatical ii sentences represent compressed form salient points original newspaper sentences 
decided keep corpus uncompressed sentences want learn compress sentence 
ziff davis corpus domain specific results appear generalize model learns mainly syntactic level 

learning model parameters collect expansion template probabilities parallel corpus 
parse sides parallel corpus identify corresponding syntactic nodes 
example parse tree sentence may np 
vp 
pp knight marcu artificial intelligence parse tree compressed version may np 
vp 
nodes deemed correspond joint event np vp np vp pp normalize pexp np vp pp np vp competes ways expanding np vp node 
sample parameter values actual training pexp np dt nn np dt nn pexp np dt jj nn np dt nn pexp np dt nnp nn np dt nn pexp np dt jjs nn np dt nn pexp np dt nnp cd nn np dt nn nodes corresponding partners non correspondences due incorrect parses due legitimate reformulations scope simple channel model 
standard methods estimate source pcfg word bigram probabilities penn treebank unannotated wall street journal respectively 

decoding vast numbers potential compressions large tree pack efficiently shared forest structure 
node children generate new nodes non empty subset children pack nodes referred 
example consider large tree fig 

compressions represented rules encode forest shown fig 

ha bc cbd qr cb cd assign source pcfg expansion template probability node forest 
example node assign expansion probability pexp pexp qr 
knight marcu artificial intelligence fig 

compact representation compressions tree fig 

smooth expansion probabilities smooth source pcfg probabilities compressed nodes 
consider compressing node ways locally grammatical penn treebank rule observed appear forest 
point want extract set high scoring trees forest account expansion template probabilities word bigram probabilities 
fortunately generic extractor hand 
extractor designed hybrid symbolic statistical natural language generation system called nitrogen 
application rule component converts semantic representation vast number potential english renderings 
renderings packed forest promising sentences extracted statistical scoring 
scoring word ngrams current research aims extending scores account subcategorization long distance syntactic relationships verb direct object 
sophisticated extractor help determine prepositional phrases may safely deleted 
purposes extractor selects trees best combination expansion template scores 
returns list trees possible compression length 
example sentence basic level operations products vary obtain best compressions negative shown parentheses smaller basic level operations products vary widely level operations products vary widely basic level operations products vary level operations products vary basic level operations products vary operations products vary widely knight marcu artificial intelligence operations products vary widely operations products vary operations products vary operations products vary operations vary operations vary 
length selection useful multiple answers choose user may seek compression seeks compression 
compression going subsequently translated language may want multiply translation probabilities deciding particular length 
purposes evaluation want system able select single compression 
rely log probabilities shown choose shortest compression 
goal summarization produce compact text compact necessarily better loss information may great 
note word compression scores better word compression models entirely happy removing article 
create fair competition divide log probability length compression rewarding longer strings 
commonly done speech recognition 
plot normalized score compression length usually observe bumpy shaped curve illustrated fig 

typical difficult case word sentence may optimally compressed word version 
course user requires shorter compression may select region curve look local minimum 

decision model sentence compression section describe decision history model sentence compression 
decision models successful parsing interpretation applications magerman zelle mooney explore summarization 
noisy channel approach assume input parse tree goal rewrite smaller tree corresponds compressed version original sentence subsumed suppose observe corpus trees fig 

model ask may go rewriting 
possible solution decompose rewriting operation sequence shift reduce drop actions specific extended shift reduce parsing paradigm 
model propose rewriting process starts empty stack input list contains sequence words subsumed large tree word input list labeled name syntactic constituents start see fig 

step rewriting module applies operation aimed reconstructing smaller tree 
context sentence compression module need types operations knight marcu artificial intelligence fig 

adjusted log probabilities top scoring compressions various lengths lower better 
fig 

example incremental tree compression 
knight marcu artificial intelligence shift operations transfer word input list stack 
reduce operations pop syntactic trees located top stack combine new tree push new tree top stack 
reduce operations derive structure syntactic tree short sentence 
drop operations delete input list subsequences words correspond syntactic constituents 
drop operations deletes input list words spanned constituent operations change label trees top stack 
actions assign pos tags words compressed sentence may different pos tags original sentence 
decision model flexible channel model enables derivation trees skeleton differ quite drastically tree input 
example channel model unable obtain tree operations listed enable rewrite tree tree long order traversal leaves produces sequence words occur order words tree example tree obtained tree sequence actions effects shown fig 
shift shift reduce drop shift reduce 
save space show shift operations line reader understand correspond distinct actions 
see operation rewrites pos tag operations modify skeleton tree input 
increase readability input list shown format resembles closely possible graphical representation trees fig 


learning parameters decision model associate configuration shift reduce drop rewriting model learning case 
cases generated automatically program derives sequences actions map large trees corpus smaller trees 
rewriting procedure simulates bottom reconstruction smaller trees 
pairs long short sentences yielded learning cases 
case labeled action name set possible actions distinct actions pos tag 
distinct drop actions type syntactic constituent deleted compression 
distinct reduce actions type reduce operation applied reconstruction compressed sentence 
shift operation 
tree arbitrary configuration stack input list purpose decision classifier learn action choose set possible actions 
marcu discuss decision model tree rewriting permits leaves re ordered 
knight marcu artificial intelligence learning example associated set features classes operational features reflect number trees stack input list types operations 
encode information denote syntactic category root nodes partial trees built certain time 
examples features original tree specific features denote syntactic constituents start unit input list 
examples features cc pp decision compression module uses program order learn decision trees specify large syntactic trees compressed shorter trees 
fold cross validation evaluation action classifier yielded accuracy time system faced action choice shift reduce 
majority baseline classifier chooses action shift accuracy 

examples learned compression rules training data decision classifier learned automatically rules shown fig 

rule enables deletion wh prepositional phrases context follow constituents program decided delete 
rule enables deletion whnp constituents 
deletion carried stack contains np constituent follows rule applied conjunction complex occur sentences 
rule enables deletion adjectival phrases 
rule previous operation reduce previous operation shift previous operation input list starts syntactic constituent type drop input list words subsumed 
rule tree stack previous operation reduce syntactic label tree stack np input list starts syntactic constituent type whnp drop input list words subsumed whnp 
rule previous operation drop input list starts syntactic constituent type adjp input list start syntactic constituent type np drop input list words subsumed adjp 
fig 

examples rules learned automatically program 
knight marcu artificial intelligence 
employing decision model compress sentences apply shift reduce drop model deterministic fashion 
parse sentence compressed initialize input list words sentence syntactic constituents word shown fig 

incrementally inquire learned classifier action perform simulate execution action 
procedure ends input list empty stack contains tree 
inorder traversal leaves tree produces compressed version sentence input 
model deterministic produces output 
advantage compression fast takes milliseconds sentence counting parsing 
disadvantage produce range compressions system may subsequently choose 
straightforward extend model probabilistic framework applying example techniques 

evaluation evaluate compression algorithms randomly selected sentence pairs parallel corpus refer test corpus 
sentence pairs training 
fig 
shows sentences test corpus compressions produced humans compression algorithms baseline algorithm produces compressions highest word bigram scores 
examples original basic level operations products vary widely 
baseline basic level operations products vary widely 
noisy channel operations products vary widely 
decision operations products vary widely 
humans operations products vary widely 
original reliable worked accurately testing produces large dxf files 
baseline worked large dxf 
noisy channel reliable worked accurately testing produces large dxf files 
decision reliable worked accurately testing large dxf files 
humans produces large dxf files 
original debugging features including user defined break points variable watching message watching windows added 
baseline debugging user defined variable watching message watching 
noisy channel debugging features including user defined points variable watching message watching windows added 
decision debugging features 
humans debugging features added 
fig 

selected compression examples 
knight marcu artificial intelligence chosen reflect average bad performance cases 
sentence compressed manner humans algorithms baseline algorithm chooses compress sentence 
second example output decision algorithm grammatical semantics negatively affected 
noisy channel algorithm deletes word break affects correctness output 
example noisy channel model conservative decides drop constituents 
contrast decision algorithm compresses input substantially fails produce grammatical output 
original sentence test corpus judges compressions human generated compression outputs noisy channel decision algorithms output baseline algorithm 
judges told outputs generated automatically 
order outputs scrambled randomly test cases 
avoid confounding judges participated experiments 
experiment asked determine scale systems respect selecting important words original sentence 
second experiment asked determine scale grammatical outputs 
investigated sensitive algorithms respect training data carrying experiments sentences different genre 
took sentence articles available scientific archive 
created second test corpus refer corpus generating compressed grammatical versions sentences 
training done ziff davis corpus 
sentences corpus extremely long baseline algorithm produce compressed versions 
results table show compression rates mean standard deviation results judges algorithm corpus 
results show decision algorithm aggressive average compresses sentences half original size 
compressed sentences produced algorithms grammatical contain important words sentences produced baseline 
test experiments showed differences statistically significant individual judges average scores judges 
tests showed significant statistical differences algorithms 
table shows performance table experimental results corpus avg 
orig 
sent 
length baseline noisy channel decision humans test words compression grammaticality importance words compression grammaticality importance knight marcu artificial intelligence compression algorithms closer human performance baseline performance humans perform statistically better algorithms 
applied sentences different genre performance noisy channel compression algorithm degrades smoothly performance decision algorithm drops sharply 
due sentences corpus decision algorithm compressed words 
suspect problem fixed decision compression module extended style magerman computing probabilities sequences decisions correspond compressed sentence 
likewise substantial gains noisy channel modeling see clearly data statistical dependencies processes captured simple initial models 
grammatical output come account subcategory head modifier statistics addition simple word bigrams expanded channel model allow tree manipulation possibilities 
extending algorithms compressing multiple sentences currently underway 

described sentence compression summarization task requires reasoning fluency relative importance different pieces text 
corpus methods attacking problem noisy channel framework decision model 
previous corpus summarization focused keyword extraction shows feasible construct new sentences analyzing existing manually produced compressions 
addition having useful applications view sentence compression stepping stone building high quality document level summarization systems 
believe corpus approaches kind described provide best way scale full problem text compression vast amounts data widely available form document pairs 
steps path devise models human summarizer fit data trained 
banko mittal witbrock headline generation statistical translation proceedings th annual meeting association computational linguistics acl hong kong pp 

barzilay elhadad mckeown sentence ordering multidocument summarization proceedings international conference human language technology research hlt san diego ca pp 

barzilay mckeown elhadad information fusion context multi document summarization proceedings th annual meeting association computational linguistics acl university maryland pp 

berger lafferty information retrieval statistical translation proceedings nd conference research development information retrieval sigir berkeley ca pp 

berger mittal query relevant summarization faqs proceedings th annual meeting association computational linguistics acl hong kong pp 

knight marcu artificial intelligence brown della pietra della pietra mercer mathematics statistical machine translation parameter estimation comput 
linguistics 
garcia molina paepcke seeing parts text summarization web browsing handheld devices proceedings th international www conference hong kong china 
canning tait archibald cohesive generation syntactically simplified newspaper text workshop robust methods analysis natural language data lausanne pp 

carroll minnen canning devlin tait practical simplification english newspaper text assist aphasic readers proceedings aaai workshop integrating ai assistive technology madison wi 
doran srinivas motivations methods text simplification proceedings international conference computational linguistics coling copenhagen 
church stochastic parts program noun phrase parser unrestricted text proceedings second conference applied natural language processing austin tx pp 

collins generative lexicalized models statistical parsing proceedings th annual meeting association computational linguistics acl madrid spain pp 

grefenstette producing intelligent telegraphic text reduction provide audio scanning service blind working notes aaai spring symposium intelligent text summarization stanford university ca pp 

jelinek statistical methods speech recognition mit press cambridge ma 
jing mckeown decomposition human written summary sentences proceedings nd conference research development information retrieval sigir berkeley ca 
jing hauptmann title generation machine translated documents proceedings ijcai seattle wa pp 

knight machine transliteration comput 
linguistics 
langkilde forest statistical sentence generation proceedings st annual meeting north american chapter association computational linguistics seattle wa 
ellis closed captioning america looking compliance proceedings tao workshop tv closed captions hearing impaired people tokyo japan pp 

magerman statistical decision tree models parsing proceedings rd annual meeting association computational linguistics cambridge ma pp 

mahesh hypertext summary extraction fast document browsing proceedings aaai spring symposium natural language processing world wide web stanford ca pp 

mani gates bloedorn improving summaries revising proceedings th annual meeting association computational linguistics university maryland college park md pp 

mani maybury eds advances automatic text summarization mit press cambridge ma 
marcu carlson watanabe automatic translation discourse structures proceedings annual meeting north american chapter association computational linguistics naacl seattle wa pp 

marcu gerber inquiry nature multidocument extracts evaluation proceedings naacl workshop text summarization pittsburgh pa 
mckeown klavans hatzivassiloglou barzilay eskin multidocument summarization reformulation progress prospects proceedings aaai orlando fl pp 

quinlan programs machine learning morgan kaufmann san mateo ca 
robert pfeiffer ellison semi automatic captioning tv programs australian perspective proceedings tao workshop tv closed captions hearing impaired people tokyo japan pp 

witbrock mittal ultra summarization statistical approach generating highly condensed summaries proceedings nd international conference research development information retrieval sigir poster session berkeley ca pp 

zelle mooney learning parse database queries inductive logic programming proceedings aaai portland pp 

