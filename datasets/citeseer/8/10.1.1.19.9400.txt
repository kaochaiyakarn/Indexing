laplacian eigenmaps spectral techniques embedding clustering mikhail belkin niyogi depts 
mathematics computer science university chicago hyde park chicago il 
math uchicago edu niyogi cs uchicago edu drawing correspondence graph laplacian laplace beltrami operator manifold connections heat equation propose geometrically motivated algorithm constructing representation data sampled low dimensional manifold embedded higher dimensional space 
algorithm provides computationally efficient approach nonlinear dimensionality reduction locality preserving properties natural connection clustering 
applications considered 
areas artificial intelligence information retrieval data mining confronted intrinsically low dimensional data lying high dimensional space 
example gray scale theta images fixed object taken moving camera yield data points intrinsic dimensionality space images object number degrees freedom camera fact space natural structure manifold embedded large body dimensionality reduction general existing approaches explicitly take account structure manifold data may possibly reside 
interest tenenbaum roweis saul problem developing low dimensional representations data particular context 
new algorithm accompanying framework analysis geometrically motivated dimensionality reduction 
core algorithm simple local computations sparse eigenvalue problem 
solution reflects intrinsic geometric structure manifold 
justification comes role laplacian operator providing optimal embedding 
laplacian graph obtained data points may viewed approximation laplace beltrami operator defined manifold 
embedding maps data come approximations natural map defined entire manifold 
framework analysis connection explicit 
connection known specialists spectral graph theory example see best knowledge know application data representation 
connection laplacian heat kernel enables choose weights graph principled manner 
locality preserving character laplacian algorithm relatively insensitive outliers noise 
byproduct algorithm implicitly emphasizes natural clusters data 
connections spectral clustering algorithms developed learning computer vision see shi malik clear 
discussion roweis saul tenenbaum note biological perceptual apparatus confronted high dimensional stimuli recover low dimensional structure 
argue approach recovering low dimensional structure inherently local natural clustering emerge serve basis development categories biological perception 
algorithm points construct weighted graph nodes point set edges connecting neighboring points 

step 
constructing graph put edge nodes close 
variations ffl neighborhoods 
parameter ffl nodes connected edge jjx gamma jj ffl 
advantages geometrically motivated relationship naturally symmetric 
disadvantages leads graphs connected components difficult choose ffl 
nearest neighbors 
parameter nodes connected edge nearest neighbors nearest neighbors advantages simpler choose tends lead connected graphs 
disadvantages geometrically intuitive 

step 
choosing weights variations weighting edges heat kernel 
parameter 
nodes connected put ij gamma jjx gammax jj justification choice weights provided 
simple minded 
parameters 
ij vertices connected edge 
simplification avoids necessity choosing 
step 
eigenmaps assume graph constructed connected proceed step connected component 
left panel shows horizontal vertical bar 
middle panel dimensional representation set images laplacian eigenmaps 
right panel shows result principal components analysis principal directions represent data 
dots correspond vertical bars signs correspond horizontal bars 
compute eigenvalues eigenvectors generalized eigenvector problem ly dy diagonal weight matrix entries column row symmetric sums ii ji gamma laplacian matrix 
laplacian symmetric positive semidefinite matrix thought operator functions defined vertices gamma solutions equation ordered eigenvalues having smallest eigenvalue fact 
image embedding lower dimensional space ym 
justification recall data set construct weighted graph edges connecting nearby points 
consider problem mapping weighted connected graph line connected points stay close possible 
wish choose minimize gamma ij appropriate constraints 
map graph real line 
note gamma ij ly gamma see notice ij symmetric ii ij gamma ij written gamma ij ii jj gamma ij ly said may got told didn going felt saw want began see get know go take say put find look give help fragments labeled arrows left right 
contains infinitives verbs second contains prepositions third modal auxiliary verbs 
see syntactic structure preserved 
minimization problem reduces finding argmin dy ly 
frequent words brown corpus represented spectral domain 
constraint dy removes arbitrary scaling factor embedding 
matrix provides natural measure vertices graph 
eq 
see positive semidefinite matrix vector minimizes objective function minimum eigenvalue solution generalized eigenvalue problem ly dy 
constant function value vertex 
easy see eigenvector eigenvalue 
graph connected eigenvector 
eliminate trivial solution collapses vertices real number put additional constraint orthogonality obtain opt argmin dy ly solution opt eigenvector smallest non zero eigenvalue 
generally embedding graph theta matrix ym ith row denoted provides embedding coordinates ith vertex 
need minimize jjy gamma jj ij tr ly reduces opt argmin dy tr ly dimensional embedding problem constraint prevents collapse point 
dimensional embedding problem constraint prevents collapse subspace dimension laplace beltrami operator laplacian graph analogous laplace beltrami operator manifolds 
speech datapoints plotted dimensional laplacian spectral representation 
consider smooth dimensional manifold embedded riemannian structure metric tensor manifold induced standard riemannian structure suppose map gradient rf local coordinates written rf vector field manifold small ffi local coordinate chart jf ffi gamma ffi xij jj see jj small points near mapped points near 
look map best preserves locality average trying find argmin jjf jj jj minimizing jj corresponds directly minimizing lf gamma ij graph 
minimizing squared gradient reduces finding eigenfunctions laplace beltrami operator recall def div div divergence 
follows stokes theorem formally adjoint operators function vector field hx rfi div jj see positive semidefinite minimizes jj eigenfunction heat kernels choice weight matrix laplace beltrami operator differentiable functions manifold intimately related heat flow 
initial heat distribution heat distribution time 
heat equation partial differential equation lu 
solution heat kernel green function pde lf lu locally heat kernel approximately equal gaussian gamma gamma jjx jjx gamma yjj local coordinates sufficiently small 
notice tends heat kernel increasingly localized tends dirac ffi function lim 
small definition derivative lf gamma gamma gamma gamma jjx dy data points expression approximated lf gamma gamma gamma jjx gammax jj ffl gamma jjx gammax jj coefficient global affect eigenvectors discrete laplacian 
inherent dimensionality may unknown put ff noticing laplacian constant function zero immediately ff jjx gammax jj ffl gamma jjx gammax jj notice worry ff graph laplacian choose correct multiplier 
see choose edge weights adjacency matrix ij gamma jjx gammax jj jjx gamma jj ffl examples example toy vision example consider binary images vertical horizontal bars located arbitrary points theta visual field 
choose images containing vertical horizontal bar containing vertical bars horizontal bars random 
fig 
shows result applying laplacian eigenmaps compared pca 
example words brown corpus fig 
shows results experiment conducted frequent words brown corpus collection texts containing words available electronic format 
word represented vector dimensional space information frequency left right neighbors computed bigram statistics corpus 
example speech fig 
consider low dimensional representations arising applying laplacian algorithm sentence speech sh sh sh sh sh sh sh sh sh sh sh sh sh aa aa ao ao ao ao ao ao ao ao ao ao ao ao ao ao ao ao ao ao dcl kcl blowup selected regions left right 
notice phonetic homogeneity chosen regions 
note points marked symbol may arise occurrences phoneme different points utterance 
symbol sh stands fricative word aa ao stand vowels words dark respectively kcl dcl stand closures preceding consonants respectively 
stands silence 
sampled khz 
short time fourier spectra computed ms intervals yielding vectors fourier coefficients ms chunk speech signal 
vector labeled identity phonetic segment belonged 
fig 
shows speech data points plotted dimensional laplacian representation 
spokes correspond predominantly fricatives closures respectively 
central portion corresponds periodic sounds vowels nasals 
fig 
shows different regions representation space 
fan chung spectral graph theory regional conference series mathematics number fan chung yan 
yau higher eigenvalues isoperimetric inequalities riemannian manifolds graphs communications analysis geometry appear rosenberg laplacian manifold cambridge university press sam roweis lawrence saul nonlinear dimensionality reduction locally linear embedding science vol dec jianbo shi jitendra malik normalized cuts image segmentation ieee transactions pami vol august tenenbaum de silva langford global geometric framework nonlinear dimensionality reduction science vol dec 
