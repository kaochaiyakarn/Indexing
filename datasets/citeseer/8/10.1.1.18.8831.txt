distributed database management systems data grid heinz stockinger cern european organization nuclear research geneva switzerland institute computer science business informatics university vienna austria heinz stockinger cern ch tel currently grid research distributed database research deals data replication tackle problem different points view 
aim outline approaches try find commonalities worlds order efficient data grid manages data stored objectoriented databases 
target object oriented database management system objectivity db currently database choice existing high energy physics hep experiments generation experiments cern 
characteristics data grids described especially high energy physics community needs data grids defined 
globus toolkit grid middle ware base discussions grid research 
grid computing general comes highperformance computing super computing cluster computing processors stations connected high speed interconnect order compute mutual program 
originally cluster meant span local area network extended wide area 
grid supposed connect computing resources wide area network 
grid research field divided large sub domains computational grid data grid 
computational grid natural extension cluster computer large computing tasks computed distributed computing resources data grid deals efficient management placement replication large amounts data 
data place computational tasks run grid provided data 
need data grids stems fact scientific applications data analysis high energy physics hep climate modelling earth observation data intensive large community researchers globe wants fast access data 
remainder concentrate specific needs high energy physics regarded representative example data intensive research communities 
particular focus data intensive large collider lhc experiments cern european organization nuclear research geneva switzerland 
cern datagrid project initiated order set data grid 
working groups explicitly deals data management data grid datagrid project 
tasks solved include data access migration replication query estimation optimisation secure environment 
deal replication aspects need solved datagrid project 
globus toolkit middle ware grid infrastructure 
scientific data intensive applications large collections files storing data 
regards hep community data generated large detectors stored persistently mass storage systems disks tapes order available physics analysis 
hep experiments databases store terabytes petabytes persistent data 
usage databases unique feature data grid 
compare climate modelling community research domain large collections files available stored called flat files databases 
requires additional data management tasks keeping catalogue available files physics experiments hep community database management system dbms takes care 
currently new experiments hep object oriented databases management systems data management 
true experiment stanford university currently tb data available objectivity db cern experiments cms atlas 
cern experiments final decision dbms object oriented relational vendor current software development processes uses object oriented approach objectivity storing data persistently 
consequently base object oriented database management systems particular objectivity db 
grid research distributed database research tackles problem data replication different point view 
aim outline approaches find commonalities order efficient data grid manage petabytes data stored object oriented databases 
provide basis effort 
data grids new research community see clear need identifying characteristics requirements data grids met efficient way 
special attention data consistency communication issues 
optimising data replication access data wan addressed sufficiently database research 
dbms normally method accessing data 
instance data server serves pages client 
data grid single access method may optimal 
restrictions pointed possible solutions 
elaborate different data consistency models global transactions contribute 
global transactions built top transactions provided database management system local site 
opposed database research separation data communication required 
particular global control messages exchanged message passing library actual data files transported high speed file transfer protocols 
single communication protocol usually commercial exchanging small amounts data database transactions 
communication mechanism optimised relatively small transactions may optimised transferring large files wan high performance control information exchanged distributed sites 
propose solutions efficient asynchronous replication policies different levels data consistency data grid 
organised follows 
section give related database grid community 
issues raised analysed sections 
section deals replica catalogues directory services grid research points techniques mapped database approach 
section discusses objectivity replication option general aspects 
assume usage grid applications fully transparent user dedicate section possible implications 
section discusses data consistency replication methods known database research practise 
possible update synchronisation solutions section followed concluding remarks 
related data replication identify selected related database community grid community 
derive commonalities discuss briefly contribution efficient data grid 
common aspects dealt 
distributed dbms research distributed database research basically addresses issues 
replica synchronisation relatively small transactions transaction consists read write operations 
contrast hep typical data production job write file transaction relatively large 
synchronous asynchronous replication details section evaluation techniques amount communication messages needed 
cost functions network server loads rarely integrated replica selection process dbms 
low amount data compared scale hep 
data access wan optimisation replicas rarely addressed database research 
dbms normally internal method accessing data 
instance data server serves pages client 
detail client sends request data dbms dbms particular data server uses implemented method serving requested data 
read write access method independent amount data accessed 
data grid single access method optimal depending number objects accessed file 
accessing large amounts data remote file usage file transfer mechanism transfer entire file access file locally efficient terms response time remotely accessing streaming single objects file 
details section 
cost functions efficient send application program remote data 
grid research globus project provides tools grid computing job scheduling resource discovery security globus working data grid effort enables fast efficient file transfer replica catalogue managing files replica management functionality files 
replica update synchronisation addressed 
grid community general tendency deal replication file level single file lowest granularity replication 
advantage structure file need known replication manager responsible replicating files site sites wan 
valid simplification hep replication cases main focus 
related hep community data grid projects ppdg griphyn 
possibility deal replication object level requires sophisticated techniques file level replication 
object handling addressed 
soon data items replicated file changed changes propagated replicas 
requires knowledge data structure file 
possible solutions problem section 
commonalities usage replication middle ware research results combined communities introducing replication middleware layer manages replication files possibly objects account site data grid manage data locally database management system 
middle ware responsible site site replication synchronisation local dbms takes care transactions local data 
grids tools monitoring applications network parameters 
filling gap 
hybrid solution database grid research identified 
main questions answered level replication middle ware able replace pure distributed dbms approach inter site replication issues dealt 
clearly replication middle ware restrictions update synchronisation transparent access data 
performance penalty replica synchronisation 
aim replication middle ware provide relaxations concept transparent data access data consistency 
remainder assume replication middle ware data grid 
replica catalogues directory services access data important data analysis hep scientific community 
access replicated data requires specific data meta data structures addressed different ways 
dbms internal access method grid environment data structures provided explicitly service user 
section emphasis put combine techniques data grid database environment 
management replicas requires certain meta data structures store information distribution access data 
object location table data structure managing objects files directory service holding replica catalogue 
ways deal replica catalogues different protocols implementations available 
want limit approach globus 
globus data grid effort replica catalogue implemented ldap directory service dedicate section analysing directory service file replication files managed local 
principle dbms provides definition catalogue files available 
objectivity db explicit file catalogue 
ldap directory service basically flat files may connections associations objects files 
questions arise ldap data stored dbms 
order answer question identify key problems need addressed heterogeneous data grid environment sources data managing replicas files dealing heterogeneous data stores 
managing replicas objectivity file catalogue exists 
granularity replication assumed file level 
multiple copies file exist introduced global replica catalogue extended version native file catalogue dbms 
furthermore name space provided takes account multiple replicas 
features enhancements dbms considers single sources information 
note assume underlying dbms support file replication 
ldap protocol database backend required replica catalogue information stored 
approach simply build catalogue access methods native schema dbms store replica information particular file dbms 
heterogeneous data stores diversity hep community requires approach supports heterogeneous data stores different kinds data may stored different formats data stores different vendors 
different database paradigms relational objectoriented supported 
case standard protocol directory information ldap choice replica location information accessed uniform way knowledge underlying data store 
combining dbms directory service means expose replica catalogue larger user community necessary database specific front access directory information 
ldap protocol needs database backend file information stored concurrency mechanisms established 
database backend vendor specific ldap specialised database database choice oracle objectivity db 
usage directory service allows open environment allows integration data sources different kinds 
note problem accessing heterogeneous data sets addressed may require mediators data integration 
introducing ldap starting point extensibility 
point important point replica management synchronisation single sites keeping replica catalogues date 
ldap directory service implies site stores replica catalogue information runs ldap server locally 
ldap commands synchronisation 
homogeneous database approach single dbms store entire data data grid may necessary ldap server synchronisation communication mechanism exchanging synchronisation information sites data grid sufficient 
data site directory service ldap server server synchronisation point distributed sites 
file introduced site information public global name space spanning sites data grid 
replica synchronisation protocol depends data consistency requirements imposed application 
conclude usage ldap combination dbms useful heterogeneous environment synchronisation sites holding replica catalogue information 
objectivity db objectivity main system referring dedicate section explaining details product problems confronted objectivity wide area data replication 
problems mentioned specific object oriented data stores objectivity specific 
usage allows simplifications global namespace global schema information 
objectivity db distributed objectoriented dbms data replication option called dro 
option optimised synchronous replication local area network 
section briefly describe dro drawbacks state complications comes persistent objects replicated 
objectivity data replication option dro provides synchronous replication model file level entire objectivity databases mapped physical files replicated 
note section term replica refer physical instance copy objectivity file 
basically ways dro data written synchronised replicated populate replicate 
multiple replicas created synchronised data written replicas time replicate populate 
replica synchronised database transactions single replica synchronised replicas 
objectivity dro performs dynamic quorum calculation application accesses replica 
quorum required read write replicas changed provides flexibility concerning data consistency 
usage quorum method established database research goes back early 
possibility overcome immediate synchronisation 
replica set line 
quorum exists data written line replica 
clean way dro perform real asynchronous batch replication specify synchronisation point time 
asynchronous batch replication method allows replicas sync certain amount time 
reconciliation updates done certain synchronisation points hour 
lack explicit asynchronous replication method reasons dro considered option wan replication 
commercial databases objectivity provide optimisation accessing replicas 
replica catalogue contains number copies location single copy file 
object replicated file accessed objectivity tries get copy file appears catalogue 
examine bandwidth site takes account data server load 
operations database creation require replicas available 
talk objectivity section subsection ignore fact dro exists see objectivity distributed single copies file 
single copies file exist distributed sites data grid remote access objectivity data possible 
partial replication associations grid environments replication regarded done file object level file regarded smallest granularity replication 
objectivity single objects stored containers 
containers stored objectivity database corresponds file 
object associations called links pointers objects may reside database container 
assume objects different files connected association 
objectivity database file replicated association gets lost files replicated 
note possibility remotely accessing objects 
replication decision carefully possible associations objects considered 
may result replicating set files order keep associations files 
furthermore imposes severe restrictions partial replication particular objects file replicated 
certain set objects selected associations objects set replication set association safe 
particular restriction object oriented data stores hold objectivity 
import difference complication compared relational dbms 
object stored file relational database data items stored tables defined tables 
objectivity file catalogue integration files native objectivity file catalogue done tool called adds logical file name physical location catalogue 
furthermore guarantees links existing objectivity databases correct 
schema file checked 
feature file created site integrated file catalogue site 
native catalogue toone mapping logical physical file replicas visible local site account dro 
furthermore possible single links site 
instance site file called file shall shared replicated sites 
file name location integrated local file catalogue remote access file established 
note requires objectivity advanced multi threaded server ams running usage shared file system afs connects sites 
ams responsible transferring streaming objects machine establishes remote data access functionality 
want address general solution shared file system available 
hep community generally agreed proven dro optimised wan 
discussions neglect dro objectivity db conclude current implementation dro feasible solution hep community 
implications grid applications claim usage files replicated data grid implications grid applications new hep user community 
want address explicitly possibilities replicated file instance accessed give implications read ahead assumptions 
accessing replicated files standard way access database objectivity issue open command physical file 
concept unique object oriented database concept resembles unix way accessing files 
single object accessed file belongs transparent user 
object handle available object id object contains information file belongs 
native objectivity open works objectivity catalogue global replica catalogue user access name space data grid 
open adapted lookup data access directory service 
plug ins required transfer requested data user application 
open issue needs research concerning caching files locally adding file local catalogue creating replica may useful data accessed frequently transferring file versus sub sets file objectivity provides functionality transparently accessing data stored tapes 
done extension ams backend checks file locally stored disk fetches file tape disk 
functionality extended go global replica catalogue fetch files remote sites 
provide transparent way accessing replicas files 
investigate option 
read ahead reading data ahead pre fetching commonly optimisation technique 
imposes problem want illustrate example 
assume file size gb program wants access objects file object size kb 
obvious want transfer requested objects file having unnecessary information transferred network 
clear need mapping instance maps requested objects files order determine files requested objects reside 
type class definition data definition language objects roughly determined data needs transferred network depending dynamically allocated information stored object 
problem solved open file operator 
runtime application determine data needed pre fetching requested objects tackle problem 
summarise query optimisation problem access data shall optimised providing necessary objects required 
related 
pre fetching addressed file level 
assume user aware files distributed different sites grid time access data depends location requested file 
assume object file accessed file available locally client site 
assumed simplicity neglect fact accessing parts file remotely 
worst case requested files available remotely need transferred local site 
size file large amount requested files high part application takes long time actual computation data done 
application give hint system tell long need serve required files computation started 
data pre fetched application data 
grid environment necessary information prefetching files provided diverse monitoring tools information services 
sociological aspect grid applications sense user application requests data advance 
reserve network bandwidth start application certain point 
data consistency replication methods main issues data replication consistency replicas 
replica just simple copy original logical connection original copy address data consistency problem 
easiest way tackle consistency problem field read data 
updates replicas done data consistent 
state consistency reach highest degree 
updates possible replicas degree data consistency normally decreased provided read write access data order reasonable response time accessing data replicated wan 
clearly consistency depends frequency updates amount data items covered update 
state degree data consistency depends update frequency amount data items covered update expected response time replicated system data grid 
identify detail different consistency options possible reasonable hep datagrid project 
section introduce term global transaction distinguished local transaction 
local transaction done dbms local site global transaction inter site transaction spans multiple sites multiple database management systems 
furthermore local consistency maintained dbms global consistency spans multiple sites data grid 
synchronous replication highest degree consistency established having fully synchronous replicas 
means local database transaction needs get replicas majority replicas 
practice database research community gained global transactions span multiple sites 
objectivity dro supports replication model 
time single local write transaction takes place phase commit protocol normally phase locking protocol guarantee serialisability global consistency 
comes cost relatively worse performance global writes compared local writes replication 
consequently decide carefully application requires high degree consistency 
derive statement current database literature type replication protocol data consistency model adapted application 
data datagrid project different types require consistency level 
try point briefly different replication policies required 
furthermore claim replication system data grid offer single policy ones satisfy needs having different data types degrees consistency 
middle ware replication system difficult provide high degree consistency global synchronous transactions difficult establish 
dbms global synchronous transaction extension conventional local transaction dbms specific locking mechanism extended replicas 
distributed dbms objectivity built global transactions additional communication mechanism sockets message passing library required 
performance integrated distributed dbms superior middle ware replication systems external communication mechanisms 
question arises distributed dbms objectivity handle replication 
points covered section major point 
objectivity provide flexible consistency levels different kinds data 
aim hybrid solution local site stores data dbms handles consistency locally managing database transactions locally 
grid middle ware required provide communication ordination local sites 
degree independence single site needs flexibly managed 
form global transaction system necessary 
illustrate example 
data allowed sync low data consistency types data need synchronised immediately high consistency 
global transactions flexible necessarily provide highest degree consistency 
furthermore site may want independent data available 
asynchronous replication relative slow performance write operations synchronously replicated environment database research community searching efficient protocols asynchronous replication cost lower consistency 
currently standard replication available commonly agreed solutions primary copy approach known master slave approach 
basic idea data item file primary copy exists replicas secondary copies 
updates done primary copy owner file 
write request sent secondary copy request passed primary copy updates propagates changes secondary copies 
policy implemented object data stores objectstore 
oracle provides feature 
primary copy approach provides high degree data consistency improved write performance features compared synchronous replication lock file agreed replicas primary copy 
epidemic approach user operations performed single replica separate activity compares version information time stamps different replicas propagates updates older replicas lazy manner opposed eager synchronous approach 
update operations executed locally sites communicate exchange date information 
degree consistency low solution exclude dirty reads possible database anomalies 
system applied non time critical data propagation updates result conflicts solved manually 
subscription relatively independent sites similar epidemic approach policy site wants data care consistency 
site updates particular file certain sites notified updates 
follows subscription model site subscribes explicitly data producing site 
site subscribed responsible get latest information sites 
allows site local changes agreement sites 
site aware local data date 
valid solution problem provide export buffer newest information site stored import buffer local site stores information needs imported remote site 
instance local site finished writing different files puts file names export buffer 
remote site notified transfers information export buffer local import buffer requests files necessary 
approach allows flexibility concerning data consistency independence local site 
site decide data import information filter 
furthermore data production site may export locally available information filter export buffer accordingly 
valid implementation approach grid data management pilot gdmp 
communication transactions outlined clear need global transactions 
transaction necessarily need create locks site notification system required automate trigger replication data transfer process 
general clear separation exchanging control messages organise locks update notifications actual data transfer 
important difference current database management systems 
replication protocols compared amount messages sent order evaluate performance 
type message dependent dbms 
message type sent actual update communication protocol 
data grids data read divide required communication parts 
concept realised gdmp 
control messages messages synchronise distributed sites 
instance site notifies site new data available update information propagated sum replication protocols control messages 
data transfer includes actual transfer data meta data files site 
separation similar ftp protocol clear separation tasks 
main point separation appropriate protocol specific communication need 
simple message passing appropriate exchanging control messages fast file transfer protocol required transfer large amounts large files 
grid environment protocols provided 
specific globus library control messages grid ftp implementation wu ftp server nc ftp client serves data transport mechanism 
single communication protocol objectivity may optimal transferring large files wide area network 
append transactions hep requirements see need increase traditional dbms transaction system new transaction called append transaction 
conventional dbms exist different kinds transactions read write transactions 
write transaction write new data append update existing data 
terms data management operations require different tasks 
update transaction prevent concurrent users reading old data values append transaction sure data item added satisfies uniqueness condition 
uniqueness condition condition guarantees data item appears identified uniquely 
hep condition satisfied feature sites write data independently change previously written data items 
different sites write different data definition derive inherent parallelism data production phase uniqueness condition hold 
true hep specific reconstruction software simulated data created monte carlo processes 
objectivity provides object ids unique database ids single files 
guaranteed newly created objects oid true database ids 
append transaction lock file creation new objects allow multiple readers time append transaction creates new objects 
allows having different consistency response time levels 
file replication objectivity replication middle ware driven real world replication requirements high energy physics want give possible approach replication read objectivity database files 
principle replication objectivity database files impose big problem concerning data consistency 
site objectivity federation takes care managing files objectivity file catalogue 
ways deal file replication read files 
guaranteed unique objectivity naming scheme applied sites data grid 
outline possible approaches 
site create database file 
order provide unique database names global transaction called actual file creation 
transaction checks local objectivity file catalogue file exists 
site creates database locally initiates database creation remote sites 
remote replicas specific file locked site write file time 
local site completed writing process lock remote replicas released 
system require replica catalogue site addition local federation catalogue 
replica catalogue flag needed initiating lock file 
actual file locking implemented call objectivity database 
furthermore transaction file objectivity file catalogue done replication catalogue 
consequently database user allowed conventional objectivity open create new file contact replica catalogue order write new database file 
database transactions go high level api contacts replica catalogue 
easier approach currently cms experiment allocation objectivity database ids different remote sites 
guarantees unique database id allocation unique name database 
consequently order fully automatic system unique name space provided 
guaranteed creation database file name communicated sites agreed 
solution provide naming convention adding host domain name local site database name 
guarantees unique file names 
populate replicate approach require locking remote replicas allows faster local write 
common approach asynchronous replication 
possible update synchronisation data grid previous sections mainly addressed replication read files 
data high energy physics ready replicated data change need update synchronisation 
section provide possible update synchronisation strategies implemented replication middle ware data grid 
described difficult middle ware system replica update synchronisation object file level middle ware system access dbms internals pages object tables 
common solution database research communicate changes file remote sites 
objectivity database file interpreted correctly native objectivity process file appears binary file conventional process see structure file 
call binary difference approach 
second possibility update data stored objectoriented approach requires knowledge schema data format files updated 
approaches outlined section 
binary difference approach possibility tool called produces difference binary files 
difference sent remote site 
site update file date merging diff file original data file 
library interface application program designed compute changes files 
changes deltas similar output diff program may store transmit changes files 
diff output expressed human readable format apply deltas copy original file 
object oriented approach approach create objects aware replicas 
principle object created site creation method object take care delegate distribution object 
class definition designed way information amount site replicas 
instance object created site replicated sites typical creation method look follows object create site advantage approach necessary information available create object site 
update objects done update function aware replicas sites need updated 
compared stored procedure approach known relational database world 
principle model similar ideas 
local site may update immediately store updates log file 
consistency requirement remote sites log information sent remote sites apply update function original local site 
update synchronisation problem passed aware replicas replication policy 
turn provide different consistency levels updating remote sites immediately hour day large amounts data may scalability problem managing logging information 
object identified single oid parameters update method oid stored 
object update parameter oid log file stores triple argument parameter object second oid third new value parameter 
modus communicating changes 
local site gains exclusive global lock file updates required objects 
parallel log file written 
file transferred remote sites efficient file transfer protocol remote sites notified control messages 
replication policy cost intensive terms exchanging communication messages sending data applied relatively small amount data 
hep environment exist meta data sources replica catalogues indices require high consistency data 
data approach useful 
data management efforts research communities distributed databases grid deals problem data replication grid community specifically deals large amounts data wide area networks 
hep community data stored database management systems appropriate try understand research issues communities distributed databases grid analyse differences commonalities combine common ideas form efficient data grid 
research issues possible solutions 
provide basis effort combining research communities 
want colleagues groups fruitful discussions datagrid package data management including cern caltech lbl infn cms computing group cern princeton caltech globus project argonne isi experiment slac colleagues part data grid discussions grid forum boston 
european datagrid project www cern ch grid wolfgang hoschek javier martinez samar heinz stockinger kurt stockinger data management international data grid project st eee acm international workshop grid computing grid bangalore india dec 
globus project www globus org objectivity www objectivity com heinz stockinger kurt stockinger erich ian 
cost model distributed replicated data stores th euromicro workshop parallel distributed processing pdp ieee computer society press italy february 
particle physics data grid ppdg www ppdg net griphyn www griphyn org koen 
peter van der ian 
mass storage systems object granularity 
proceedings ieee mass storage systems technologies maryland usa march koen heinz stockinger building large location table find replicas physics objects proc 
computing high energy physics chep padova 

gifford 
weighted voting replicated data 
acm sigops symp 
operating systems principles pacific grove december 
kurt stockinger dirk wolfgang hoschek erich 
improving performance high energy physics analysis bitmap indices 
th international conference database expert systems applications london greenwich uk springer verlag sept 
bernardo shoshani sim 
access coordination tertiary storage high energy physics applications 
ieee symposium mass storage systems college park md usa march 
yuri breitbart henry korth 
replication consistency lazy helps proc 
acm sigact sigmod symposium principles database systems tucson az 
www com objectstore www com products objectstore html agrawal amr el abbadi epidemic algorithms replicated databases extended 
pods 
samar heinz stockinger 
grid data management pilot gdmp tool wide area replication iasted international conference applied informatics ai innsbruck austria 
postel reynolds rfc file transfer protocol ftp october 
globus project universal data transfer grid white 
joshua macdonald www berkeley edu html contact sheet heinz stockinger cern cms experiment computing group bat 
ch geneva switzerland heinz stockinger cern ch tel fax 
