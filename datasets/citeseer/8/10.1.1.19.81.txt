talking machines statistically speaking statistical methods long dominant approach speech recognition probabilistic modelling asr mature technology 
statistical methods areas spoken dialogue mature 
reviews spoken dialogue systems statistical modelling perspective 
complete system partially observable markov decision process 
various sub components exposed introducing appropriate intermediate variables 
samples existing reviewed framework including dialogue control optimisation semantic interpretation goal detection natural language generation synthesis 

statistical model spoken dialogue system shown fig 

system operates cyclically 
begins default system initiated dialogue act converted acoustic signal inviting user speak 
user current beliefs user generates signal corrupted noise input speech understanding component signal acoustic 
noisy signal speech decoded give set di acts dialogue acts interpreted domain model transform system state 
precise definition dialogue act critical 
suffices define frame information representing intention internal structure consisting slot value pairs 
referred semantic concepts 
example automated pizza ordering system say pizza world may help user respond please 
case single dialogue act purchase request qty result state system updated record request 
errors may occur components steve young cambridge university engineering department street cambridge england cb pz various dence levels associated encoded system state 
users beliefs noise user speech output speech input speech understanding response generation user generated dialog acts system generated dialog acts observer observations state estimator domain model system state system beliefs dialog manager fig 

structure spoken dialogue system decoded updated combined form observation beliefs system updated 
beliefs dialogue manager generates set system generated dialogue acts encoding request confirm give size size 
cycle repeats 
statistical model sds characterised minimal dependence explicit rules heavy dependence learning data 
dialogue represented partially observable markov decision process pomdp optimal dialogue management policy learned data 
speech understanding component designed traditional patternmatching paradigm estimating posterior distribution decoding maximum posteriori map rule 
response generation implemented sampling distribution 
simplistic certain aspects framework capable modelling hand crafted limited domain applications deployed today areas information inquiry commerce 
potential advantages statistical approach greatly reduced modelling deployment costs robust operation 
drawback major inhibitor progress difficulty obtaining suitable training data 
issue dependent level annotation required training key research issues find robust ways learning struc user su dialog policy fig 

mdp representation dialogue system ture unannotated partially annotated data 
sections explore issues detail 

dialogue management control indicated preceding spoken dialogue system regarded markov decision process reinforcement learning find optimal dialogue management policies 
early area pioneered pieraccini levin 
modelled sds fully observable mdp user speech understanding component combined form single system represented transition system state action applied system time see determined fig 

set function system dialogue policy function 
sds combination action acts state time determines expected immediate goal find policy maximises total 
immediate re wards selected meet dialogue design criteria reward reward commonly small negative reward dialogue turn 
larger positive reward completing successfully corresponding negative reward completing unsuccessfully 
system motivated succeed complete dialogue quickly possible 
model fits classic framework reinforcement learning see 
solution methods depend finding value function state function gives expected value reward dialogue proceeds terminal state policy 
useful closely related function gives expected value reward taken state 
function known action determined policy guaranteed better equal 
provides basis policy optimisation 
system transition function known dynamic programming update rule form transition function known methods sampling actual dialogues 
particular temporal difference learning technique function updated dialogue turn comparing actual step reward reward predicted function considered reasonable combinations necessary allow dialogue deviate optimal policy occasionally 
commonly done adopting stochastic soft policy 
pair determines learning rate 
order ensure set actions possible state 
policy designed follow locally optimal policy occasionally probability explore non optimal action 
soft policy optimal deterministic policy 
comparing fig 
dialog system shown fig 
seen extensions needed order handle realistic dialogue mdp framework 
particular complete knowledge system state unrealistic firstly state full system typically intractably large approximated 
secondly user beliefs directly observed inferred 
practice dialogue system base management strategy incomplete data 
fig 
shows framework partially observable markov decision process pomdp allows done 
beliefs system represented finite computationally tractable set states encode pertinent information relevant managing dialogue 
vector observation derived combination user acts dialogue system state encapsulates evidence needed update maintain system accurately possible 
beliefs unfortunately policy optimisation pomdp complex mdp 
pomdp belief state note system state encode relevant history plus system variables 
simplistic implementation example pizza dialogue require states 
distribution underlying state set continuous variable 
value functions belief states linear combinations value functions states belief value function hyperplane belief space choosing optimal policy involves finding step hyperplane maximum value belief point 
beliefs discrete enumerated turns belief states partitioned regions share optimal policy 
leads number algorithms policy optimisation 
unfortunately current exact algorithms effectively computationally intractable small state sets 
whilst mdp framework appears provide sound basis modeling spoken dialogues statistically practice substantial compromises order apply 
state space large methods intractable 
state space pruned simple mdp case system non markovian take account uncertain interpretation user beliefs 
pomdp framework user uncertainty accounted state space smaller 
cases difficulty collecting sufficient training data acute especially changing policy state space require complete new set data 
despite difficulties progress 
walker modelled real systems simple mdps tractable reducing state space focus characteristics specific interest prompt type confirmation type 
training data collected live sds running random policy designed explore state action space 
able obtain significant improvements reward measure able improve asr performance discovering finely tuned strategies interpreting confidence measures 
limitation approach taken walker forced fix state space advance collecting data 
young avoided problem adopting stage approach 
firstly trained user simulation model data obtained prototype dialogue system 
secondly performed policy optimisation synthetic data generated user model 
walker cut statespace traces compensate somewhat non markovian behaviour resulting system 
roy studied simple robot dialogues pomdp framework 
asr output directly observations avoided intractability finding exact solutions approximation known augmented mdps 
approximation essentially ranks belief vectors state entropy belief state 
pomdp framework led better behaviour recognition errors occurred 
zhang pomdps provide robustness errors 
studied system states actions observations compared performance grid pomdp policy optimisation compared various augmented mdps 
pomdp version consistently yielded better solutions suggesting finding efficient pomdp frameworks sds worthwhile way forward 

speech understanding referring back fig 
goal speech understanding component convert input speech waveform acts set dialogue current beliefs dialogue state seek solve convenient identify word sequence carried substituting equation making reasonable dependence assumptions gives suboptimal sequential solution equation achieved solving solving equation emphasises fact common practice factoring speech understanding problem stage process suboptimal word lattice preferred 
place single best string acoustic words concepts user dialog signal acts yi word semantic dialog act recognition decoding detection system beliefs fig 

speech understanding stages dependence state dialogue clearly shown 
indicates potential benefits making recogniser language model dependent dialogue context dialogue state crucial handling ambiguity identifying underspecified dialogue acts 
order evaluate equation convenient introduce intermediate representation sequence word gives decoding sequentially represents set semantic concepts encoded equation represents process semantic decoding equation represents process combining system beliefs decoded semantics find dialogue acts 
combination equations represent complete decoding chain speech understanding illustrated fig 

benefits statistical approach word recognition component established require discussion see reviews 
statistical approach semantic decoding dialogue act detection developed 

semantic decoding semantic decoding conventional limited domain sds typically depend semantic template grammar form robust parser extract required semantic concepts 
example utterance please yield parse shown fig 
semantic rules pizza qty pizza represent phrasal structure 
rules domain specific require iterations user testing achieve adequate coverage 
pizza qty please fig 

example semantic parse tree please statistical approach semantic decoding attempts solve equation directly explicit requirement produce parse tree 
example extraction semantic concepts qty required suggests approach word utterance simply tagged single discrete concept label 
example tagged qty 
irrelevant words tagged dummy marker subsequently discarded 
model semantic decoding equation rewritten equation referred semantic lexical model 
sequence modelled gram conditioned current system beliefs words modelled gram conditioned semantic concepts 
result conventional st order markov model states labelled concepts semantic concepts transitions concept bigram probabilities 
levin pieraccini demonstrated atis evaluations flat model semantic decoding give surprisingly results 
left right structure allow hierarchical grouping concepts weakens predictive power model model referred note practice sequential processing model shown fig 
may require set semantic concepts extended simple domain action entity arguments 
example fig 
retained semantic concept order assist subsequent decoding relevant dialogue act 
practice start sentence markers enable prediction word word sentence 
detail omitted clarity 
adjacent symbols weakly coupled 
furthermore expressive power limited inability represent nested structures 
representational power flat concept model extended converting simple finite state transition network underlying hmm recursive transition network concept state finite state network 
called hidden understanding model hum extends class supported languages regular context free 
apply hum modified version earley parsing algorithm generates concepts sequence parse path corresponding depth scan entire parse tree 
case probabilities semantic lexical models computed efficient interleaved manner denotes concept corresponding current model parse path semantic lexical models context dependent bigrams 
key problem building model semantic decoding providing suitable training data general large quantities fully annotated tree bank data available applications 
ideal semantic model properties powerful training cost capture necessary semantic structures data easily generated dialogue designer producing training data cost hand crafting semantic grammar sense models represent opposite ends spectrum 
flat model pieraccini levin adapted relatively simple training data annotations unaligned list semantic tags 
representational power flat model generally adequate 
hand hierarchical hum miller requires fully annotated tree bank data 
attempt simpler annotations em discover hidden structure far degrees freedom unrestricted context free model 
way forward compromise 
reasonable training data annotated corresponding dialogue acts equivalent semantic schema type data relatively straightforward dialogue designer produce legitimately regarded part application de qty item fig 

right branching parse tree please please sign process goal training algorithm learn associate words phrases particular schema elements 
wang demonstrated viability approach combination grammar templates derived automatically semantic schema constraint parsing 
basic idea schema generate set high level grammar rules 
key informational items schema augmented optional pre post 
robust parser scans training data associates phrases key informational items associating words modifiers side effect 
modifier word associations fill grammar required lower level rules 
semantic decoding viewed information extraction task 
chelba mahajan describe structured language model general treebank data trained specific sds 
essentially approach similar wang level grammar learnt 
high level derived semantic schema constraints parse training data learn lower slot level grammar rules 
alternative automatically inferred grammar templates provide needed constraints restrict underlying formalism 
example limiting stochastic grammar strictly right branching leads model em parameter estimation tractable 
parse tree example sentence shown fig 

note right branching constraint forces somewhat unnatural relationships essential phrase structure maintained 
general hierarchical model probabilistic model suited left right decoding 
partial path covering contains exactly number probabilities paths compared directly normalisation pruning 
compared general scfg model effective statespace model size reduced 
note methods lead essentially approach taken atis sql queries provided training utterances 
ci cj 
fig 

dialogue act detection bayesian network models assign probabilities interpretations word sequence extension process lattices alternative word sequences suggested section relatively straightforward 

dialogue act detection methods described previous section focussed solving equation section briefly discusses solution equation 
assuming set finite allowed dialogue acts set finite semantic concepts problem essentially determining mapping variety solutions possible 
limited subset subset single dialogue act binary decision trees leaf nodes correspond dialogue acts tree node labelled implying question input concept 
tree subset easily trained automatically examples occurring concept sets corresponding dialogue acts 
alternative provides soft classification allows detection multiple dialogue acts uses bayesian networks bns 
approach developed meng colleagues bn defined possible dialogue act shown fig 
conditional probabilities priors learnt training data 
set input concepts evidence computed act selected decoded output 
shown dotted line fig concepts conditionally independent performance improved including concept dependencies 
automatic method proposed learning significant dependencies minimum description length 
evidence entered network posterior concept probabilities computed 
concept high posterior supplied evidence probably needed information asked 
similarly concept low probability evidence spurious confirmed user 
bns way assist mixed initiative dialogue management 

context bellegarda silverman mentioned 
construct matrix records number times concept occurred utterance conveying dialogue act 
compute run time map input concept vectors re svd duced subspace simple distance metric identify dialogue acts 
fact include words concepts input vectors dispense semantic decoding altogether giving effective robust technique simple domains 

speech generation generation essentially inverse understanding process described section 
speech acts embedded concepts output dialogue manager acoustic signal required conveys intended meaning user natural possible way seek decoding sequentially leads equation suggests domain specific models understanding similar set trained sample outputs sample inputs reused generation mode produce word sequence 
understanding designed extract key content words ignore syntax 
generation model lead outputs unacceptable surface structure 
solution smooth equation separate language model trained large amount formed data 
model required capture surface structure simple gram model suffice 
equation controls weight placed grammatical wellformedness 
practice stage process illustrated generator filter fig 

statistical generation model fig 
implement equation 
generate lattice possible word sequences 
select single sequence lattice 
language generation filter paradigm proposed knight system called nitrogen 
hand crafted semantic template grammar gen erator bigram language model filter 
atomic concepts template grammar expanded lexicon morphological expansion performed rule 
model tolerant generation rules kept simple 
lack explicit tree representation syntax narrow domains wider coverage benefits general syntactic knowledge 
tree model introduced upenn xtag grammar 
claimed allow richer output forms including ability model long range effects separation parts collocation embedded adjuncts 
systems pay attention sentence planning 
sentence planner called spot described operates filter paradigm 
case generator takes input primitive dialogue acts stochastically combines set built combination operators merge conjoin relative clause filter trained corpus sentences graded human judges rank generator output number features features determined automatically human ranked data 
evaluation plans selected spot statistically indistinguishable best plan selected human judges 
turning generation acoustic signal speech synthesis systems implement equation essentially way 
large database phone models attempt find expansion meets various acoustic constraints 
systems store waveform segment corresponding phone model directly database making synthesis trivial concatenation process 
space precludes detailed dis synthesis methods 
theme tokuda noteworthy developed system uses hmms directly generators neatly implementing equation directly 

attempted provide general statistical framework spoken dialogue systems variety going described framework 
clearly hard problems solve general holistic approach ready widespread deployment 
long term pay ability construct systems trainable data cheap build robust operation adapt behaviour online 

levin pieraccini stochastic model interaction learning dialogue strategies proc eurospeech rhodes greece pp 

levin pieraccini eckert markov decision processes learning dialogue strategies proc int conf acoustics speech signal processing seattle usa 
rs sutton ag barto reinforcement learning adaptive computation machine learning 
mit press cambridge mass 
sj young probabilistic methods spoken dialogue systems philosophical transactions royal society series vol 
pp 

cc white partially observed markov decision processes survey annals operations research vol 

lp kaelbling ml littman ar cassandra planning acting partially observable stochastic domains artificial intelligence vol 
pp 

ma walker jc narayanan learning optimal dialogue strategies case study spoken dialogue agent email proceedings acl coling 
singh dj litman kearns walker optimizing dialogue management reinforcement learning experiments njfun system journal artificial intelligence research vol 
appear 
sj young corpus dialogue simulation automatic strategy learning evaluation proc naacl workshop adaptation dialogue systems pittsburgh usa 
sj young automatic learning dialogue strategy dialogue simulation reinforcement learning hlt san diego usa 
singh traces find best memoryless policy partially observable markov decision processes th conf machine learning icml 
roy pineau thrun spoken dialogue management probabilistic reasoning proceedings acl 
zhang cai mao chang spoken dialogue management planning acting uncertainty eurospeech aalborg denmark 
riccardi gorin stochastic language models speech recognition understanding int conf spoken language processing sydney australia 
xu rudnicky language modeling dialog system proceedings icslp beijing china 
sj young statistical modelling continuous speech recognition proc int conference uncertainty artificial intelligence seattle 
padmanabhan large vocabulary speech recognition algorithms ieee computer vol 
pp 

sj young ce proctor design implementation dialogue control voice operated database inquiry systems computer speech language vol 
pp 

ward understanding spontaneous speech proc int conf acoustics speech signal processing toronto canada pp 

zue seneff jr glass polifroni pao tj hazen jupiter telephone conversational interface weather information ieee trans acoustics speech signal processing vol 
pp 

pieraccini levin lee stochastic representation conceptual structure atis task proc speech natural language workshop pacific grove ca pp 
darpa 
pieraccini levin stochastic representation semantic structure speech understanding speech communication vol 
pp 

levin pieraccini generation proc spoken language systems technology workshop austin texas pp 

miller bobrow schwartz ingria statistical language processing hidden understanding models proc human language technology workshop nj pp 
arpa 
stolcke efficient probabilistic context free parsing algorithm computes prefix probabilities computational linguistics vol 
pp 

lari sj young estimation stochastic contextfree grammars inside outside algorithm computer speech language vol 
pp 

wang grammar learning spoken language understanding proceedings workshop di italy 
wang evaluation spoken grammar learning atis domain proc int conf acoustics speech signal processing orlando florida 
chelba mahajan information extraction structured language model emnlp 
chelba jelinek structured language modeling computer speech language vol 
pp 

sj young statistical approach design spoken dialogue systems tech 
rep cued infeng tr cambridge university engineering department 
heckerman horwitz inferring informational goals free text queries bayesian approach proc th conf uncertainty ai pp 

hm meng lam wai believe understand eurospeech budapest hungary pp 

hm meng lam kf low learning belief networks language understanding ieee workshop automatic speech recognition understanding keystone colorado 
pulman conversational games belief revision bayesian networks clin vii th computational linguistics netherlands meeting 
hm meng wai pieraccini belief networks mixed initiative dialog modelling int conference spoken language processing beijing china 
bellegarda kea silverman unconstrained command control data driven semantic inference proc icslp beijing china pp 

langkilde knight generation exploits statistical knowledge proc th int conf computational linguistics coling acl montreal canada 
langkilde knight practical value grams generation proc th int natural language workshop inlg niagara lake canada 
bangalore oc rambow exploiting probabilistic hierarchical model generation proc th int conf computational linguistics coling saarbrucken germany 
walker oc rambow trainable approach sentence planning spoken dialogue computer speech language vol 
appear 
aw black taylor automatically clustering similar units unit selection speech synthesis proc eurospeech rhodes greece pp 

tokuda kobayashi kitamura speech parameter generation algorithms speech synthesis proc int conf acoustics speech signal processing istanbul turkey pp 

