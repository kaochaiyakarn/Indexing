bayesian statistics pp 
bernardo berger dawid heckerman smith west eds 
oxford university press variational bayesian em algorithm incomplete data application scoring graphical model structures matthew beal zoubin ghahramani gatsby computational neuroscience unit ucl uk beal gatsby ucl ac uk zoubin gatsby ucl ac uk summary ecient procedure estimating marginal likelihood probabilistic models latent variables incomplete data 
method constructs optimises lower bound marginal likelihood variational calculus resulting iterative algorithm generalises em algorithm maintaining posterior distributions latent variables parameters 
de ne family conjugate exponential models includes nite mixtures exponential family models factor analysis hidden markov models linear state space models models interest bound marginal likelihood computed simply modi cation standard em algorithm 
particular focus applying bounds problem scoring discrete directed graphical model structures bayesian networks 
extensive simulations comparing variational bounds usual approach bayesian information criterion bic sampling gold standard method known annealed importance sampling ais show variational bounds substantially outperform bic nding correct model structure relatively little computational cost approaching performance costly ais procedure 
ais allows provide rst serious case study tightness variational bounds 
analyse perfomance ais variety criteria discuss variational approaches estimating marginal likelihoods bethe kikuchi approximations outline directions extended 
keywords marginal likelihood latent variables variational methods graphical models annealed importance sampling structure scoring 

statistical modeling problems involve large number random variables convenient express conditional independence relations variables graphically 
graphical models intuitive tool visualising dependencies variables 
exploiting conditional independence relationships provide backbone possible derive ef cient message propagating algorithms conditioning variables model new evidence pearl lauritzen spiegelhalter heckerman cowell :10.1.1.112.8434
standard statistical models especially bayesian models hierarchical priors expressed naturally probabilistic graphical models 
representation helpful developing sampling methods beal ghahramani gibbs sampling exact inference methods junction tree algorithm models 
important dicult problem bayesian inference computing marginal likelihood model 
problem appears guises computing bayes factor ratio marginal likelihoods kass raftery computing normalising constant posterior distribution known statistical physics partition function machine learning evidence 
marginal likelihood important quantity allows select model structures 
dicult quantity compute involves integrating parameters latent variables usually high dimensional complicated integral simple approximations fail 
describe variational methods approximate marginal likelihood posterior distributions complex models 
variational methods extensively bayesian machine learning years provide lower bound marginal likelihood computed eciently 
subsections review bayesian approaches learning model structure 
section turn describing variational methods applied bayesian learning deriving variational bayesian em algorithm comparing em algorithm maximum posteriori map estimation 
section focus models family derive basic results 
section introduces speci problem learning conditional independence structure directed acyclic graphical models latent variables 
compare variational methods bic annealed importance sampling ais 
conclude discussion variational methods section areas general area learning model structure 

bayesian learning model structure term model structure denote variety things 
graphical models graph implies set conditional independence statements variables graph 
example directed acyclic graphs dags variables separated third see pearl de nition conditionally independent model structure learning problem inferring conditional independence relationships hold set complete incomplete observations variables 
special case problem input variable selection regression 
selecting input explanatory variables needed predict output response variable regression equivalently cast deciding input variable parent output variable corresponding directed graph 
statistical models interest contain discrete nominal latent variables 
model structure learning problem interest choosing cardinality discrete latent variable 
examples problem include deciding mixture components nite mixture model hidden states hidden markov model 
statistical models contain real valued vectors latent variables making necessary inference dimensionality latent vector 
examples include choosing dimensionality probabilistic principal components analysis pca factor analysis fa model linear gaussian state space model 
obvious problem maximum likelihood methods likelihood function generally higher complex model structures leading tting 
bayesian approaches overcome tting treating parameters model vb scoring graphical models set models unknown random variables averaging likelihood obtain di erent settings marginal likelihood data set assuming model prior distribution parameters 
integrating parameters penalises models degrees freedom models priori model larger range data sets 
property bayesian integration called ockham razor favors simpler explanations models data complex ones je reys berger mackay :10.1.1.51.7418:10.1.1.51.7418:10.1.1.51.7418
tting problem avoided simply parameter pure bayesian approach data 
prior distribution model structures prior distribution parameters model structure observing data set induces posterior distribution models bayes rule uncertainty parameters quanti ed posterior 
density new data point obtained averaging uncertainty model structure parameters predictive distribution 
theory average possible structures practice constraints storage computation ease interpretability may lead select probable model structure maximising 
focus methods computing probabilities structures 

practical bayesian approaches models interest computationally analytically intractable perform integrals required exactly 
involve high dimensional integrals models parameter symmetries mixture models integrand exponentially separated modes 
focusing brie outline methods approximate integral turning variational methods 
bayesian statistics community markov chain monte carlo mcmc method choice approximating dicult high dimensional expectations integrals 
mcmc methods result asymptotically samples posterior distribution parameters models symmetries hard get mixing modes crucial obtaining valid estimates marginal likelihood 
methods proposed estimating marginal likelihoods candidate method chib see neal bridge sampling path sampling gelman meng closely related annealed importance sampling neal :10.1.1.46.1344:10.1.1.44.183
large scale problems sampling methods machine learning community referred evidence model important keep mind realistic model data need complex 
advisable complex model possible inference ideally setting priors allow limit nitely parameters taken arti cially limit number parameters model neal rasmussen ghahramani 
beal ghahramani method choice slow posterior distribution parameters stored set samples inecient memory standpoint 
chose annealed importance sampling ais gold standard respect compare faster non sampling approximations 
approach bayesian integration laplace approximation local gaussian approximation maximum posteriori map parameter estimate kass raftery mackay :10.1.1.51.7418:10.1.1.51.7418:10.1.1.51.7418
fact large data limit regularity conditions posterior approaches gaussian map estimate 
models symmetries regularity conditions hold posterior approach mixture exponentially gaussians 
gaussianity assumption inaccurate small data sets principle advantages bayesian integration ml largest 
local gaussian approximations poorly suited bounded constrained positive parameters mixing proportions mixture model advisable gaussianity reasonable 
gaussian approximation requires computing approximating determinant hessian matrix map estimate computationally costly parameters 
drastic costly approximation marginal likelihood bayesian information criterion bic schwartz derived laplace approximation dropping terms scale number data points model determined parameters ml parameter estimate bic approximation log marginal likelihood ln ln ln 
marginal likelihood variational principle review variational methods approximate integrals required bayesian learning 
examples methods years machine learning community known bayesian statistics community 
general framework large family models investigate novel application framework scoring structures discrete graphical models provide rst serious assessments tight variational bounds compare sampling methods 
focus models latent hidden variables considering general incomplete data setting 
lower bounding marginal likelihood denote observed variables denote latent variables denote parameters 
log marginal likelihood data set lower bounded introducing distribution latent variables parameters support appealing jensen inequality due concavity logarithm function ln ln dx ln dx ln dx maximising lower bound respect free distribution results substituted turns inequality equality 
simplify problem evaluating true posterior distribution vb scoring graphical models requires knowing normalising constant marginal likelihood 
simpler factorised approximation ln ln dx fm quantity functional free distributions 

variational bayesian em variational bayesian algorithm iteratively maximises equation respect free distributions 
elementary calculus variations take functional derivatives lower bound respect holding xed 
results update equations superscript denotes iteration number 
exp ln exp ln dx clearly coupled iterate equations convergence 
readers familiar em algorithm dempster may note similarity iterative algorithm em 
call procedure variational bayesian em algorithm reasons clearer sections see attias ghahramani beal 
re writing easy see maximising equivalent minimising kl divergence joint posterior ln fm ln dx kl qkp note whilst factorisation posterior distribution latent variables parameters may drastic think replacing stochastic dependencies deterministic dependencies relevant moments sets variables 
variational methods lower bounding probabilities explored researchers past decade 
hinton van camp proposed early approach bayesian learning hidden layer neural networks restriction gaussian 
neal hinton generalisation em jensen inequality allow partial steps 
jordan 
jaakkola review variational methods general context 
variational bayesian methods applied various models latent variables waterhouse mackay bishop attias ghahramani beal 
structural em algorithm scoring discrete graphical models friedman closely related variational method described distribution replaced map estimate :10.1.1.24.1555:10.1.1.24.1555:10.1.1.24.1555
beal ghahramani 
conjugate exponential models consider particular class graphical models latent variables call conjugate exponential ce models 
explicitly apply variational method parametric families resulting simple generalisation em conjugate exponential models satisfy conditions condition 
complete data likelihood exponential family exp vector natural parameters functions de ne exponential family 
condition 
parameter prior conjugate complete data likelihood exp hyperparameters 
theorem 
conjugate exponential models 
iid data set fy model satis es conditions iteration variational bayesian em algorithm maxima conjugate form exp qx qx denote expectation variational posterior latent variable associated ith datum 
form known parameter posterior exp expectation natural parameter 
proof 
substitute parametric forms de nition ce family variational extrema revealing forms respectively 
ce models forms closed iterations variational bayesian em ensuring theorem continues hold convergence local maximum lower bound marginal likelihood 

comparison variational bayesian em em map estimation instructive compare em algorithm map estimation 
alternative derivation em due neal hinton em map estimation variational bayesian em goal maximise jy goal lower bound step compute vb step compute xjy xjy step vb step arg max ln dx exp ln dx variational bayesian em algorithm reduces ordinary em algorithm section follows exposition ghahramani beal includes general results directed undirected graphs 
vb scoring graphical models restrict parameter density point estimate dirac delta function 
vb step time complexity step ways identical re written terms expected natural parameters 
particular relevant propagation algorithms junction tree kalman smoothing belief propagation 
vb step computes distribution parameters conjugate family point estimate 
algorithms monotonically increase objective function 

variational scoring model structures apply variational method non trivial problem learning conditional independence structure directed acyclic graphical models latent variables 
compare vb bounds marginal likelihood standard method scoring graphs bayesian information criterion bic schwarz sampling gold standard annealed importance sampling ais neal :10.1.1.46.1344
consider small graph consisting discrete variables binary valued latent hidden variables observed variables cardinality 
restrict bipartite structures latent variables parents observed variables 
interested successful di erent scoring methods learning data true graph structure latent variables parents observed variables 
variables discrete placing independent dirichlet priors parameters model conjugate exponential 
data generated graph shown call true structure 
instantiated setting graph parameters prior generated incrementally larger data sets model 
chose particular structure contains links induce non trivial correlations observed variables whilst nodes exhaustively evaluate marginal likelihood possible alternative structures 

annealed importance sampling ais estimate computing 
ratio computed running metropolis sampler lower computing importance weights higher estimate unbiased step sampled equilibrium approximate sample changing slowly 

experiments scoring possible structures map bic vb ais 
distinct structures basic architecture described 
large range data set sizes ran em structure compute map estimate parameters computed bic score 
ran variational bayesian em algorithm initial conditions lower bound marginal likelihood 
estimated marginal likelihood ais annealing prior posterior steps nonlinear annealing schedule tuned experiments averaging results multiple true structures parameter settings prohibitive sampling runs took cpu hours 
exhaustive enumeration academic interest practice embed di erent structure scoring methods greedy model search outer loop friedman nd probable structures :10.1.1.24.1555:10.1.1.24.1555:10.1.1.24.1555
beal ghahramani latent variables observed variables number free parameters map ais vb bic 
left true structure generate data possible distinct structures accounting permutations latent variables 
right estimated marginal likelihood scores structures map ais vb bic function number free parameters structure 
points number parameters staggered slightly clarity 
true structure parameters vertical line highest scoring structure method shown bold symbol 
reduce variance estimate metropolis proposal tuned give reasonable acceptance rates 
shown map ais vb bic scores structure ordered number parameters data points 
data set size chosen smallest vb ais gave highest score true structure 
see general upward trend map prefers complex structures general downward trend bic penalises complexity 
ais lies vb lower bound structures expect 
data points vb appears close ais nds correct structure 
rank true model 
shows rank scoring method true structure data set sizes 
methods eventually nd correct structure ais rank noisy may due annealing rapidly examine ect annealing time ais 
bic nds data points vb true structure ranked better vb bic 
especially small amounts data ais outperforms vb bic nds true structure data points 
performance ais sampling time 
ran ais point data set varying duration annealing schedules ranging samples 
right shows annealing runs starting random initial parameters sampled prior starting true parameters generated data marked shown bic vb scores 
see runs converge suciently long annealing schedules 
takes roughly samples approach bic score pass vb lower bound 
computation time 
scoring structures data points ghz pentium iii processor took seconds bic seconds vb seconds hours ais samples main experiments 
referring back left infer example vb times faster ais locked true structure reliably ais 
vb scoring graphical models number data points ais vb bic duration annealing samples 
left rank true structure scoring method varying data set sizes higher plot better 
right ais estimates marginal likelihood di erent initial conditions sampler di erent duration annealing schedules indicates setting initial parameters true values 
shown bic score dashed vb lower bound solid 

evolving directions structure scoring variational lower bound method handling intractable marginal likelihoods 
promising direction explore bethe kikuchi family variational methods yedidia may accurate provide assurance bound 
re express negative log marginal likelihood free energy statistical physics approximate entropy posterior distribution latent variables parameters neglecting high order terms 
procedures minimising bethe free energy functional approximate posterior distribution obtain estimates marginal likelihood 
interestingly belief propagation algorithm run multiply connected graphs loopy graphs xed points minima bethe free energy 
belief propagation loopy graphs guaranteed converge works practice standard approach decoding state art error correcting codes 
furthermore convergent algorithms minimising bethe free energy derived yuille welling teh 
currently exploring bethe kikuchi approximations marginal likelihoods view comparing vb bic ais methods scoring graphs evaluated chickering heckerman 
comparisons take account tradeo accuracy computational complexity 
variational lower bound marginal likelihood criterion model selection computed orders magnitude quickly sampling criteria 
similar experiences usefulness vb model selection variety models note vb framework readily applied conjugate exponential models incomplete data including example graphical gaussian models 
acknowledgments carl edward rasmussen extensive input helpful suggestions especially regarding ais 
part done zg visiting center automated learning discovery carnegie mellon university 
beal ghahramani attias 

variational bayesian framework graphical models 
advances neural information processing systems mit press 
bishop 

variational pca 
proc 
ninth int 
conf 
arti cial neural networks 
icann 
chib 

marginal likelihood gibbs output 
jasa 
chickering heckerman 

ecient approximations marginal likelihood bayesian networks hidden variables 
proc 
th conf 
uncertainty arti cial intelligence 
uai pages 
morgan kaufmann publishers 
cowell dawid lauritzen spiegelhalter 

probabilistic networks expert systems 
springer verlag new york 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society series methodological 
friedman 

bayesian structural em algorithm 
proc 
th conf 
uncertainty arti cial intelligence uai san francisco ca 
morgan kaufmann 
gelman meng 

simulating normalizing constants importance sampling bridge sampling path sampling 
statistical science 
ghahramani beal 

variational inference bayesian mixtures factor analysers 
advances neural information processing systems mit press 
ghahramani beal 

propagation algorithms variational bayesian learning 
advances neural information processing systems mit press 
heckerman 

tutorial learning bayesian networks 
technical report msr tr ftp ftp research microsoft com pub tr tr ps microsoft research 
hinton van camp 

keeping neural networks simple minimizing description length weights 
sixth acm conference computational learning theory santa cruz 
jaakkola 

variational methods inference estimation graphical models 
technical report ph thesis department brain cognitive sciences mit press 
je reys berger 

ockham razor bayesian analysis 
amer scientist 
jordan ghahramani jaakkola saul 

variational methods graphical models 
jordan editor learning graphical models 
kluwer 
kass raftery 

bayes factors 
amer 
statist 
assoc 
lauritzen spiegelhalter 

local computations probabilities graphical structures application expert systems 
roy 
statist 
soc 

mackay 

probable networks plausible predictions review practical bayesian methods supervised neural networks 
network computation neural systems 
mackay 

ensemble learning hidden markov models 
technical report cavendish laboratory university cambridge 
neal 

bayesian learning neural networks 
springer verlag 
neal 

annealed importance sampling 
tech 
report dept statistics toronto 
neal 

erroneous results marginal likelihood gibbs output 
neal hinton 

view em algorithm justi es incremental sparse variants 
jordan editor learning graphical models pages 
kluwer 
pearl 

probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann san mateo ca 
rasmussen ghahramani 

occam razor 
advances neural information processing systems mit press 
schwarz 

estimating dimension model 
annals statistics 
waterhouse mackay robinson 

bayesian methods mixtures experts 
advances neural information processing systems mit press 
welling teh 

belief optimisation binary networks stable alternative loopy belief propagation 
proc 
th conf 
uncertainty arti cal intelligence uai seattle 
yedidia freeman weiss 

generalized belief propagation 
advances neural information processing systems mit press 
yuille 

cccp algorithms minimize bethe kikuchi free energies convergent alternatives belief propagation 
technical report smith eye research institute 
