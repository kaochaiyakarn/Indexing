learning labeled unlabeled data graph avrim blum cmu edu chawla cmu edu computer science department carnegie mellon university forbes avenue pittsburgh pa usa application domains suffer having labeled training data learning 
large amounts unlabeled examples gathered cheaply 
result great deal years unlabeled data aid classification 
consider algorithm finding minimum cuts graphs uses pairwise relationships examples order learn labeled unlabeled data 
algorithm uses similarity measure data construct graph outputs classification corresponding partitioning graph way minimizes roughly number similar pairs examples different labels 
give theoretical justifications approach provide experiments synthetic real datasets 
method seen robust noise labeled examples 

learning algorithms face lack sufficient labeled data 
task classify text documents web pages camera images need learning algorithm labeled training examples 
luckily cases large numbers unlabeled examples may readily available 
instance document classification easy access large database documents classified hand 
result deal years unlabeled data usefully employed order produce better predictions venkatesh cover nigam mitchell bennett demiriz zhang schuurmans 
method graph proposed vision literature problem cleaning pixel images greig roy cox boykov snow 
initial noisy image created stereo camera goal improve image minimizing appropriate energy function 
energy function combines term pair neighboring pixels different depths encouraging algorithm smooth image term number pixels changed original image encouraging algorithm change pixels 
insight greig 
boykov 
energy function minimized ap graph mincut 
show method applied machine learning problem combining labeled unlabeled data 
dataset labeled unlabeled examples construct graph examples minimum cut graph yields optimal binary labeling unlabeled data certain optimization functions 
approach inspired kleinberg tardos connect vision general classification setting call metric labeling problem 
fact converting learning problems technically simpler ver sion setting binary multi way classification solved exactly just approximated 
focus construct appropriate graph developing new algorithms solving graph problem kleinberg tardos 
approaches combining labeled unlabeled data high level idea method assign values unlabeled examples order optimize associated objective function 
mincut approach kinds functions optimized limited depend pairwise relationships examples 
approach especially appealing functions handle graph give polynomial time algorithm find true global optimum 
trade generality approach em hill climbing gradient descent applied confidence finding exact optimum 
natural question inter similar technique wu leahy image partitioning 
shi malik give sophisticated approach normalized cuts 
objective functions approach represent 
sense theoretically help experimentally 
provide results directions 
describe mincut approach detail prove number theoretical guarantees 
show experimentally method unlabeled data substantial advantage clear best way tune approach 
see experimentally mincut approach tends robust random noise 
surprising reduction noise images boykov snow 
concreteness example kind opti mization mincut algorithm perform 
set positive negative labeled examples set unlabeled examples find labeling points minimizes leave cross validation error nearest neighbor algorithm applied entire dataset notice optimization problem natural iterative relabeling style algorithm greedy local optimization 
setting graph mincut problem find global optimum polynomial time 
sense graph mincut approach relates nearest neighbor style algo rithms transductive svm bennett demiriz relates standard svm algorithm goal assign labels unlabeled data way underlying learning algorithm 
explore section 
nice feature having algorithm efficiently finds global optimum compare local optimization algorithm objective function see results differ 
particu lar interested global optimum really better typical local optimum terms value objective function translate significant difference terms prediction accuracy 
objective function model reality 
perform experiments form 
point assuming unlabeled training data comes underlying distribution test data really difference unlabeled data test data 
problem unlabeled data viewed question large set unlabeled test data properties entire test set better predictions standard approach particular criteria may best world optimize particular nearest neighbor graph isolated pockets give better criteria section 
fixing learned hypothesis test data seen 
particular experiments put unlabeled training data test data pot run algorithms read labels assigned test points predictions 
organized follows 
section describe graph mincut algorithm 
sections theoretical results motivate applicability algorithm 
section contains experimental results synthetic data datasets uci repository 
section 

graph mincut learning algorithm describe graph mincut learning algorithm 
introduce notation 
set labeled examples set unlabeled examples 
assume setting binary classification labels positive negative denote set positive examples denote set negative examples algorithm follows 
construct weighted graph associated edge weight 
call vertices classification vertices vertices example vertices 
classification vertices connected edges infinite weight labeled examples having label 
specifically cx andw cx 
edges example vertices assigned weights relationship examples similarity distance 
specific choice edge weights discussed 
rest function assigning weights edges example nodes referred edge weighting function determine minimum cut graph find minimum total weight set edges removal disconnects 
max flow algorithm source sink edge weights treated capacities see cormen 
removing edges cut partitions graph sets vertices call 
concreteness multiple minimum cuts set algorithm choose smallest defined easy obtain flow 

assign positive label unlabeled examples set negative label unlabeled examples set 
motivation algorithm edges examples similar high weight similar examples placed vertex subset obtained mincut 
conforms basic sumption learning algorithms nearest neighbor similar examples classified similarly 
motivation fact suggests weight edges 
notion distance examples expect nearby examples gen label just distance feature space natural weighting function put high weight edges nearby examples low weight edges edges farther away examples 
initially handed distance function expect straightforward feature values helpful may wish feed labeled data auxiliary learning algorithm learns distance function 
example labeled data weight attributes information gain 
freedom scale weight edge allows interpolate putting unlabeled data similar footing labeled data ignoring unlabeled data completely 
see choice edge weighting function greatly influence quality output algorithm 

motivation minimizing loocv error mincut approach reasonable try 
section motivate approach considering goal assigning labels unlabeled data order maximize happiness learning algorithm prove related technically different kinds results 
certain learning algorithms define edge weights mincut algorithm pro duces labeling unlabeled data possible labelings results having leave cross validation error applied entire dataset 
certain learning algorithms define edge weights mincut algorithm labeling results having zero leave cross validation error examples held 
types learning algorithms able handle nearest neighbor style 
simple result type basic nearest neighbor algorithm 
theorem suppose define edge weights example nodes way pair nodes define nearest neighbor 

binary labeling examples cost associated cut equal number leave cross validation mistakes nearest neighbor theorem implies minimizing value cut corresponds minimizing loocv error 
proof fix binary labeling examples loocv error nearest neighbor simply number label different label near est neighbor 
sum ordered pairs different labels 
exactly value cut produced putting positive examples putting negative examples 
natural extend result nearest neighbor algorithm unfortunately majority vote operation knn causes problem 
replace majority vote operation averaging 
specifically define averaging knn algorithm examines nearest neighbors test example pre dicts fraction number positive examples set viewing positive examples having label negative examples having la bel 
generally set labeled examples test example define locally weighted averaging algorithm predicts label weighted average labels examples nearest examples averaging knn instance weight examples function distance theorem locally weighted averaging algorithm define edge weights minimum cut yields labeling unlabeled data possible labelings minimizes ll norm loocv error proof 
ordered pair examples define weight example asked classify example 
define edge weight 
fix binary labeling unlabeled examples denote set positive examples denote set negative examples 
cost cut equal wy xv yv xv yv yv xcv denotes classification algo rithm loocv error entire dataset norm 
labeling minimizes value cut minimizes loocv error 
algorithms represent exactly achieve zero loocv error just unlabeled examples particular define symmetric weighted nearest neighbor weighted nearest neighbor algorithm prediction example weighted majority vote examples dataset weights symmetric weight predicting weight predicting 
example weights distance examples 
knn symmetric possible nearest neighbor nearest neighbor theorem weight function symmetric weighted nearest neighbor algorithm 
function weighting edges graph mincut classification returned graph mincut results algorithm having zero leave cross validation error proof follows directly lemma 
lemma boolean classification returned graph mincut dataset view positive negative sgn sgn sgn 
proof 
graph mincut divide set examples positive set negative set 
suppose 
solved minimum cut ue ue moving strictly improve value cut 
implies sgn ucs desired 
case claim strict inequality defined smallest set minimum cut 
sgn ues desired 
proof thm immediate lemma 
discussion results state graph mincut produce labelings unlabeled data sense 
think learning algorithm particular loocv error measuring nice dataset mincut assigns labels data nice nearest neighbor style algorithms 
fact points worry 
labeled examples unlabeled examples self consistency cause assign unlabeled examples class 
instance just labeled positive example labeled negative example connect example edges weight nearest neighbors labeling unlabeled points negative gives cut value 
may minimum cut dataset really sep distinct blobs 
worse classifier unlabeled data completely ignored 
shi malik address problem context image segmentation normalized version mincut algorithm attempts equalize sizes partitions np hard find best split graph approach encourages balance 
potential problem graph sparse number disconnected components 
example graph nearest neighbor unlabeled examples near form component 
mincut algorithm free label component likes policy label negative 
important weighting function allow happen 

motivation generative model vision literature mincut approach various extensions motivated generarive model known markov random field 
model assumes points examples picked advance labels determined probabilistically ac cording certain distribution pietra 
distribution probability global labeling product unary pairwise terms higher probability nearby points labeling lower probability different labeling 
take log get mincut objective function 
model sense physical systems ising model spins behave iron pixel images satisfying considering learning examples 
particular doesn address examples chosen similar examples ought similar labels 
consider generarive model satisfying 
model assume underlying distribution examples union regions unique label 
positive regions negative regions 
regions separated minimum distance 
example picked randomly choosing point inside union re giving region label 
idea set edge weights appropriately weights regions weak see unlabeled examples edges regions grow stronger 
furthermore region dumbbell regions highly connected single labeled example inside region allow correctly classify points inside 
bit precise 
say regions live dimensional space normalization assume total volume 
define interior region set points distance boundary 
define set points distance interior 
points ball radius contained touch 
say region round fraction volume interior connected non empty 
analysis borrows nice proof ideas tenenbaum 

vr denote volume dimensional ball radius need unlabeled examples re resulting graph interior essentially single connected component 
analyze region fill possible balls radius center ball inside ball notice fill entire interior point inside uncovered just greedily add new ball centered balls important features get unlabeled example get point labeled example edges graph 
second shrink radii balls factor disjoint 
second fact means regions total number balls 
number unlabeled examples need vs log 
discussion generarive model theorem nearest neighbor style algorithms perform pretty 
case regions long thin labeled data sparse 
case examples easily closer labeled examples regions 
experimental results shown figures corroborate fact comparing mincut nearest neighbor synthetic dataset type 
section contains discussion fig ures 
theorem suppose data generated uni random union round re distance regions classification point depends solely region belongs 
weighting function graph 
logk la examples log vs unlabeled examples correctly classify fraction unlabeled examples high probability 
note similar result achieved weighting function drops suciently rapidly distance 
proof sketch ignore regions probability mass property need remaining region labeled example example 
log labeled examples sucient occur high probability 
add noise labels point simplicity leave 
generalize assuming regions axe dimensional manifolds lying higher dimensional space tenenbaum roweis saul affecting results 

experimental analysis tested graph mincut algorithm standard datasets real synthetic synthetic dataset intended fit generarive model section 
standard datasets compared mincut algorithm standard learning algorithms id nearest neighbor datasets obtained uc irvine machine learn ing repository uci 
mincut algorithm degrees freedom terms edge weights defined 
order experiments clean possible consider weighting functions specifically motivated analysis previous sections learning algorithms compare 
follows mincut algorithm connect unlabeled example edge weight nearest neighbors 
avoid having iso lated components see section force labeled example 
specifically example connected nearest labeled example nearest examples 
mincut algorithm metric table 
classification accuracies graph mincut algorithms various datasets uci repository 
mushroom dataset mi synthetic monks problems 
datasets small amount noise training set 
best result dataset bold 
cases best mincut opt involves picking fact second best bold 
dataset mincut id nn 
es mincut mincut opi mincut mincut tae tae voting musk pima bupa mi compute distances points 
points closer connected edge 
parameter depends dataset 
mincut choose maximum graph cut value 
mincut value size largest connected component graph half number datapoints 
mincut opt choose value corresponds classification error hindsight 
performance algorithm gives benchmark measure performance mincut variants 
datasets attributes cat hamming notion distance 
datasets euclidean distance 
datasets large number attributes take cue id define distance metric weights attributes information gain 
information gain computed labeled data 
specifically weighting function gain constants 
specific choice constants affect algorithm performance simply leads convergence different value 
experimental results corresponding mincut variants described shown table 
seen mincut opt outperforms algorithms datasets main exceptions monks datasets 
best value highly problem dependent 
attempt algorithm automatically learn value experimented techniques reasonable comparison 
examine mincut value resulting graph select example mincut algorithm form 
mincut fails inher noisy datasets compare example datasets tae tae datasets 
technique performs better observe size largest connected component resulting graph 
idea technique datasets somewhat noisy allow long distance dependencies graph effect noise 
idea mincut gorithm 
technique works datasets particular presence noise 
extension interesting explore new methods finding value 

classification error graph mincut room dataset function alpha observed graph mincut similar nearest neighbor style algorithms 
nearest neighbor bases classification labeled examples graph mincut takes account unlabeled examples treats similarly 
performance mincut differ differential treatment labeled unlabeled examples 
precisely scale edge weights unlabeled examples factor value result regular mincut algorithm value give weight unlabeled examples making algorithm resemble underlying supervised learning algorithm 
sense signifies confidence unlabeled examples compared labeled examples 
demonstrates results experiment mushroom dataset mincut algorithm 
minimum classification error achieved 

classification errors graph mincut nn synthetic dataset function number labeled exam les 
number unlabeled examples 

classification errors graph mincut nn synthetic dataset function number unlabeled exam les 
number labeled examples 
nearest mincut number unlabeled examples synthetic datasets order study various properties graph mincut approach closely tested algorithm synthetic dataset regions model described section 
dataset contains regions dimensional space separated minimum distance specifically region dimensional plane yz space located coordinates respectively 
regions alternate sign positive negative 
data generated uniformly random union regions 
dataset norm determining distance assign edge weights exponentially decreasing function distance 
dependence classification error graph mincut nearest neighbor labeled unlabeled examples shown figures 
plots performance mincut nn fixed number unlabeled examples amount labeled data varies 
seen mincut performs substantially better nn little labeled data gap shrinks number labeled examples increases algorithms performing equally labeled examples 
fixes number labeled examples increases amount unlabeled data 
seen mincut able unlabeled data substantially improve formance 
results indicate real advan tage graph mincut achieved huge amount unlabeled data paucity labeled data 

classification error graph mincut iterative re labeling synthetic dataset function number unlabeled examples 
re size test set instructive compare performance graph mincut iterative re labeling style algo experiment involved independent iterations randomly selecting data performing classification algorithm 
reported values axe means iterations 
rithm tries minimize objective function mincut finds local global optimum 
particular natural iterative approach random labeling perform hill climbing flipping labels long reduces cut value reach locally optimal labeling 
compare algorithms synthetic dataset find mincut gives far better performance 
fact iterative algorithm performs extremely poorly 

describe new method utilizing unlabeled data classification graph cuts 
essence approach assign values unlabeled examples way optimizes consistency nearest neighbor sense similar examples classified similarly 
approach interesting theoretical point view optimization performed polynomial time 
motivate approach self consistency measures minimizing loocv error certain algorithms fairly natural gen model 
find experimentally graph mincut algorithm performs reasonably compared learning algorithms unlabeled data especially labeled examples 
underlying algorithm degrees freedom particular design edge weighting function may best way compute weighting function information available time learning 
example simplest case setting weights single real valued parameter experiments show significant gap rules choosing advance best hindsight 
ecient algorithm adds new technique algorithm repertoire 
find experimentally mincut approach robust noise 
interesting vari ation method described noise reduction real world datasets 
noise robustness suggests mincut approach conjunction learning algorithm learning algorithm give initial labels unlabeled data mincut algorithm clean labeling enforcing kind global consistency 
intend explore 
compare performance algorithm transductive svm 
nikhil bansal gupta john langford number helpful discussions suggestions 
research supported part nsf ccr ccr 
bennett demiriz 

semi supervised support vector machines 
advances neural information processing systems pp 

mit press 
blum mitchell 

combining labeled unlabeled data training 
proceedings conference computational learning theory 
boykov veksler zabih 

markov random fields efficient approximations 
ieee computer vision pattern recognition conference 
castelli cover 

relative value labeled unlabeled samples pattern recognition unknown mixing parameter 
ieee transactions information theory 
cormen leiserson rivest 

algorithms 
mit press 
greig 

exact maximum posteriori estimation binary images 
journal royal statistical society series 


text categorization labeled unlabeled data model approach 
nips workshop unlabeled data supervised learning 
kleinberg tardos 

approximation algorithms classification problems pairwise relationships metric labeling markov random fields 
oth annual symposium foundations computer science 
nigam mccallum thrun mitchell 

learning classify text labeled unlabeled documents 
proceedings fifteenth national conference artificial intelligence 
aaai press 
pietra pietra lafferty 

inducing features random fields 
ieee transactions pattern analysis machine intelligence 
venkatesh 

learning mixture labeled unlabeled examples parametric side information 
proceedings th annual conference computa tional learning theory pp 

acm press new york ny 
roweis saul 

nonlinear dimensionality reduction locally linear embedding 
science 
roy cox 

maximum flow formulation camera stereo correspondence problem 
international confer ence computer vision iccv pp 

schuurmans 

new metric approach model selection 
aaai iaai pp 

shi malik 

normalized cuts image tion 
proc 
ieee conf 
computer vision pattern recog nition pp 

snow viola zabih 

exact voxel occupancy graph cuts 
ieee conference computer vision pattern recognition 
tenenbaum de silva langford 

global ge framework nonlinear dimensionality reduction 
science 
uci 
repository machine learning databases 
www ics uci edu mlearn mlrepository 
html 
wu leahy 

optimal graph theoretic approach data clustering theory application image segmentation 
ieee trans 
pattern analysis machine intelligence 
zhang oles 

probability analysis value unlabeled data classification problems 
seventeenth international conference machine learning 
