multiresolution markov models signal image processing alan willsky dept electrical engineering computer science laboratory information decision systems massachusetts institute technology keywords autoregressive processes bayesian networks data assimilation data fusion estimation fractals geophysical signal processing graphical models hidden markov models image processing image enhancement image segmentation inverse problems kalman filtering machine vision mapping markov random fields maximum entropy methods multiresolution methods quadtrees signal processing sparse matrices state space methods stochastic realization trees wavelet transforms 
mailing address prof alan willsky room mit cambridge ma telephone fax email willsky mit edu research described supported part afosr onr muri aro daad 
reviews significant component rich field statistical multiresolution modeling processing 
methods application literature widely scattered set disciplines principal objectives single coherent picture framework 
second goal describe topic fits larger field methods concepts particular making ties topics wavelets multigrid methods 
third provide alternate viewpoints body methods concepts describe intersect number fields 
principle focus presentation class markov processes defined trees 
attractiveness models stems efficient algorithms admit expressive power broad applicability 
show variety methods models relate framework including models self similar processes 
illustrate methods practice 
discuss construction models trees show questions arise context contact wavelets state space modeling time series system parameter identification hidden markov models 
discuss limitations tree models algorithms artifacts introduce 
describe concern ways overcome 
leads discussion models general graphs ties known emerging methods inference graphical models 
multiresolution concepts methods statistical analysis phenomena data remain topics tremendous interest wide variety disciplines see example special issues devoted subject book 
reasons intensity activity variety methods developed myriad intent put entire subject simple coherent picture 
objective provide significant component vast field provided fertile ground theory application 
personally perspective framework yields provides useful platform organizing ones understanding broader field analysis processing 
distinguishing characteristics framework describe start algorithms processing analyzing phenomena multiple resolutions wavelet transforms produce decompositions signals multiple resolutions begins modeling phenomena multiple resolutions 
development methodologies modeling time series random fields intent construct statistical models rich capture large important classes phenomena broad interest possess structure exploited gain insight phenomena design powerful classes algorithms provide statistical tools analyzing precision models appropriate resulting algorithms perform 
multiresolution 
principle objective modeling framework capturing important ways data analysis signal processing problem characteristics 
phenomenon modeled exhibit distinctive behavior range scales resolutions 
example physical processes geophysical fields atmospheric phenomena possess behavior vast ranges spatial spatio temporal scales 
studies large classes natural imagery show characteristic variability multiple scales mathematical models self similar fractal processes fractional brownian motion fbm motivating examinations properties wavelet transforms signals images :10.1.1.54.299:10.1.1.22.6131
secondly phenomenon displays behavior may case available data multiple resolutions 
simple result transforming data wavelet transforms problems collected data directly measure quantities interest multiple resolutions 
example large scale data assimilation problems geosciences quite frequently involve fusion distinct sources data representing different measurement probing geophysical medium different resolutions 
example fusion satellite measurements variables measurements surface ships data tomographic collections 
similar examples variety problems involving remotely sensed probed data including fusion sar imagery geophysical inversion data fusion 
addition advances biomedical sensing require development new methods fusing data sets different characteristics pet mri images 
thirdly phenomenon data multiresolution may case objectives user users may multiple resolutions 
certainly case large scale geophysical mapping different scientific studies focus behavior different ranges scales variability concern scientist simply noise 
addition contexts pure scientific inquiry objective data assimilation stated high levels mapping oil reservoir assess production rates total yield characterization threat subsurface populated areas 
finds military applications maps environmental situational variables maps terrain elevation vegetation disposition friendly unfriendly forces required multiple users typically large scale maps comparatively coarse scales strategic planners localized finer scale maps tactical forces 
phenomenon data objectives naturally described multiple resolutions may compelling reasons consider developing algorithms multiple resolutions 
particular algorithms offer promise computational efficiency 
seen variety methods solution large systems equations representing discretizations partial differential equations 
multigrid methods represent class examples coarser computationally simpler versions problem guide accelerate solution finer versions finer versions turn correct coarsening aliasing errors coarser versions 
multipole algorithms approximate effects distant parts random field coarser aggregate values providing substantial computational gains problems 
similarly methods provide potentially significant speed ups variety computationally intensive problems 
starting point key characteristic methods models introduce dimensional quantity scale resolution exploited define recursions dynamics time temporal phenomena 
point departure exploitation recursions scale investigation statistical models defined multiresolution trees 
examples trees depicted 
dyadic tree prototypical structure representations signals processes signals functions single independent variable 
level tree corresponds distinct resolution representation finer representations lower levels tree 
similarly quadtree example tree structures representation signals images phenomena 
figures represent structures widely describe require regular tree structure example equally trees number branches descending node different fact vary node node resolution resolution 
models describe node associated random variable random vector 
roughly speaking variable represents set information relevant phenomenon available data resolution location corresponding node 
variables related signals images phenomena data interest varies considerably application application 
example situations fundamental physical variables signals observed variables wish estimate wish reason reside finest scale 
coarser scale variables case simply represent decompositions finest scale variables coarser scale components wavelet decompositions laplacian pyramid representations images 
problems coarser scale variables may measured directly occurs problems wish fuse data sets collected differing resolutions 
generally coarser scale variables may may directly observed may may deterministic functions finest scale variables inclusion representation may serve purposes exposing statistical structure phenomenon study capturing global quantities estimation desired 
example analogy stochastic realization theory concept state dynamic systems variables may simply play role capturing intrinsic memory signals observed primary interest 
models describe close ties hidden markov models hmm hidden variables may represent higher level descriptors wish estimate speech analysis image segmentation higher level vision problems :10.1.1.150.82
nature variables defined tree critical property satisfy collectively define markov process tree concept discuss detail subsequent sections 
see processes possessing markov property contact standard markov processes time markov random fields mrf large class bayes nets belief networks graphical models :10.1.1.114.4996:10.1.1.115.4360
exploitation markovian property leads efficient algorithms describe 
getting oriented fair question ask written 
reply partially author 
reason self promotion author guilty frequently resorting notation examples familiar ambitious set personal goals 
particular field analysis sufficiently involved interconnected forming complex singly connected graphical structures contact disciplines writing provided opportunity author sort particular point models trees 
result intended reach overlapping distinct audiences scientists engineers interested applying methods problems complex data analysis researchers signal image processing interested understanding current state active area research relationship researchers fields may find connections specialties intellectual interest 
meet ambitious objective presentation detours way order touch topics ranging graphical models stochastic realization theory solution methods large systems linear equations 
occasions step back provide additional navigation tools reader particular explaining methods describe relate frameworks notably wavelets multigrid renormalization methods methods inverse problems 
addition provide pointers areas current research pointers forward backward relationships concepts describe accommodated severe constraints linearly ordered text 
result path followed optimized audiences mind hope finds detours pointers navigation aids interesting minimally distracting 
section providing initial look sampling applications provide context motivation vehicles illustrating methods describe sections 
section introduce class models focus provide initial simple examples processes described models take look ties graphical models mrfs factoring sparse matrices recursive modeling time series 
case discussion focuses detail exclusively linear linear gaussian models 
reasons doing include importance models applications simple explicit form computations allow certain points clearly relationships setting provides fields linear state space modeling time series linear algebra 
course describe linear case extends quite nicely general nonlinear models attempted clear concepts algorithms extend directly general models issues arise cases 
section describe structure illustrate application efficient inference algorithms models admit 
take detour examine bit detail models admit powerful algorithms making contact graphical models solution large sparse linear systems equations 
section take step back examine question models methods sections relate wavelet methods multigrid algorithms process describe relationships research inverse problems image reconstruction 
intent doing provide reviews tutorials important substantial lines research clear methods intersect focus diverge 
principal section problem modeled framework section efficient solution constructed 
course begs question modeled effectively framework models constructed 
examination question section uncovers connections number topics including state space realization theory hmm graphical models wavelets maximum entropy modeling algorithms constructing sample paths processes theory stochastic processes fractal generation 
useful modeling framework nontrivial extensive domain applicability return occasions applications introduced section order provide insight classes processes effectively modeled models illustrate power methods practice 
addition case truly useful modeling framework utility isn universal sections provide insights limitations 
section take brief look characteristics surprisingly critical power models limitations making ties richer class graphical models restricted trees provide brief glimpse emerging extensions framework expand domain applicability 
section concludes perspective framework prospective thoughts 
sampling applications methods describe employed wide variety applications including lowlevel computer vision image processing problems image denoising deblurring edge detection optical flow estimation surface reconstruction texture classification image segmentation name higher level recognition vision problems photon limited imaging network traffic modeling atmospheric geophysical remote sensing data assimilation data fusion speech multisensor fusion hydrology applications process control synthetic aperture radar image analysis fusion geographic systems medical image analysis models neural responses human vision mathematical physics :10.1.1.42.5473:10.1.1.32.3823
section introduce applications serve provide context motivation illustrations development follows indicate breadth problems methods applied 
ocean height estimation application described detail problem mapping variations sea level satellite measurements satellites 
shows example region pacific ocean tracks poseidon satellite provides measurements estimate sea level variations 
challenges data assimilation problems 
dimensionality mapping problems enormous involving estimates grids points 
second illustrates data collected irregular sampling pattern 
third substantial non sea level variations fidelity measurements derived data 
example statistical structure sea level variations regions strong currents gulf stream current pacific quite different ocean regions 
quantity estimated variation sea level relative surface earth gravitational field raw satellite data adjusted account spatial variations 
known certainty fact significant errors near large features hawaiian islands extended subsurface sea mounts resulting estimated sea height relative additional corrections remove effects variations quite frequently temporally averaged ocean circulation pattern see 
adjusted measurements errors spatially varying uncertainties 
fourth need compute estimates sea level variations statistical quality estimates error variances statistics needed fuse estimates information ocean circulation models identify statistically significant anomalies 
oceans display variations extremely large range scales typical spectral models sea level variations fractal spectra 
dimensionality sea level estimation problem desire compute error variances estimates daunting computational task precluding brute force solution methods 
non stationarity phenomenon varying quality data sampling pattern measurements efficient methods fft applicable 
see section advantage fractal character sea level variations surprisingly simple model yields effective solution 
surface reconstruction second closely related problem widely studied field computer vision reconstructing surfaces regular irregularly sampled measurements surface height normal surface shape shading problem 
known approach reconstruction problems involves variational formulation 
particular denote planar region surface defined denote gradient surface 
similarly denote measurements surface denote measurements gradient 
measurements noisy may available irregular locations spatially varying quality take estimate surface gradient quantities minimize functional dr dr dr dr non negative coefficients terms allow control closely wish reconstruction follow measurements third fourth terms represent smoothness penalties reconstructed surface 
particular terms referred thin membrane penalty penalizes nonzero surface gradients term referred thin plate penalty penalizes curvature bending surface 
adjusting wecan adjust relative strengths penalties 
complication integrability constraint fact gradient surface 
particular clear indicate power law exponent generally vary frequency spatially statistics ocean height variation locally stationary space 
generally measurement dot product normal known vector situation requires minor variation variational formulation 
example don measurements type subregions simply set zero subregions 
minimizing constraint variations problem example hard constraint relaxed replaced quadratic penalty difference sides equation classic variational problem 
alternatively discussed see section optimization problems interpreted estimation problems fractal priors 
computing optimal estimates problems involves solving partial differential equations computationally intensive overwhelming task 
computation statistics errors estimates daunting task 
discuss illustrate section developed greater detail alternative replace smoothness penalties correspond prior model surface reconstructed different prior model qualitative fractal characteristics leads efficient algorithms computation estimates error statistics 
image denoising problem removing additive noise images subject vast number studies 
linear methods wiener filters spatially stationary models gaussian mrf models long history see 
methods generally aim minimize mean squared estimation error second order statistics images restored serious limitations applications image field restored edges areas substantial high frequency high contrast behavior 
particular generally low pass nature linear methods implies reduce noise expense blurring distorting important features 
example depicts noisy image scene shown noise great deal edge high frequency behavior 
seen performing linear wiener filtering offers comparatively poor tradeoff amount noise rejection versus amount blurring features 
numerous approaches developed combat problems essence attempting remove noise regions images away features preserving features minimal distortion 
included literature methods explicit modeling edges boundary features see example approaches non gaussian models order better capture heavy tail nature imagery example generalized gaussian models studied depth array procedures wavelet transforms :10.1.1.161.8697:10.1.1.34.5975:10.1.1.29.5390
set methods general idea exploit localization properties wavelets allow easier transparent adaptive processing order minimize distortion important image features removing noise 
see methods explicitly involve modeling framework developed close ties 
texture discrimination problem importance computer vision image processing applications texture discrimination 
known class statistical texture models mrf 
example shows synthetic mrf textures modeling sand 
problem discriminating textures noisy measurements standard hypothesis testing problem solution hinges computation likelihood ratio textures observed imagery 
calculating likelihoods prohibitively complex operation data correspond irregularly spaced samples region data available irregular shape data spatially varying statistics fft methods aren applicable 
discuss section likelihood calculations class models trees far simpler remain tractable high dimensional image processing problems 
describe section possible develop models capture statistical variability textures 
alternate models identical mrf models generate examples sufficiently close represent equally valid mathematical models real textures task discrimination admit efficient solutions 
image segmentation image processing low level computer vision problem arises applications segmentation 
segmentation images multispectral image document page shown challenging computationally intensive task involves accounting image variability class potentially combinatorially explosive set candidate segmentations considered 
example mrf models described include discrete hidden label variables estimation corresponds specification segmentation 
search optimal estimates models computationally demanding requiring methods simulated annealing solution leading suboptimal methods iterated conditional mode icm 
problems led variety authors consider algorithms models 
describe methods fall directly framework focus relate 
multisensor fusion groundwater hydrology mentioned section motivations multiresolution methods comes applications available measurements multiple resolutions variables estimated may represent aggregate coarser scale variables 
application examined field groundwater hydrology 
objective application estimate characterize estimation errors travel time traveling groundwater 
travel time highly uncertain considerable uncertainty large dynamic range spatial variability hydraulic conductivity controls spatially varying transport behavior groundwater system see 
specifically denote log conductivity field function spatial location 
basic governing equation qrc potential field known hydraulic head qrc called recharge rate whichis particular assumed known 
local groundwater velocity isa function conductivity head particular proportional product conductivity gradient potential field data available generally come sparse irregularly spaced set wells log conductivity hydraulic head measured 
measurements wish estimate travel time specified points point representing central source location point boundary containment region 
travel time turn determined velocity field 
complexity problem evident 
discussed measurements represent point measurements random field locations measurements hydraulic head related complicated nonlocal manner 
section see framework describe capture statistical structure nonlocal head measurements 
see methodology provides alternative methods fusion measurements estimation travel time 
large dynamic range conductivity common log conductivity fundamental variable 
see discussion boundary conditions accompany 
simply model travel time nonlocal quantity included explicitly model estimated directly estimation methodology described section 
alternative approach involves widely concept conditional simulation samples generated entire log conductivity field samples drawn distribution field conditioned available measurements 
see drawing samples models extremely efficient comparable complexity generating sample outputs time series model driven white noise efficient corresponding methods random field models 
image reconstruction inverse problems preceding section described application data combined included local measurements quantities interest nonlocal measurements resulting indirect probing medium field imaged 
data type rule exception applications including tomographic reconstruction deblurring deconvolution problems 
observed data correspond projections sets line integrals field interest 
field estimated reconstructed blurred measurement process 
image reconstruction inverse problems challenges variety reasons 
reason purely computational performing reconstruction nontrivial task 
ill posedness problems 
example operations involve integration smoothing tomography convolution significantly attenuate high frequency features result operators type may invertible inverses may undesirable properties particular amplification high frequency noise 
result regularization methods interpretable specifying prior statistical model field estimated section employed 
section see example reconstruction algorithm model type focus 
multiresolution models trees basic model structure general class models interest markov processes defined trees organized levels resolutions 
section review concept general graphs suffices point markov property trees particularly simple condition value process node tree leaf node nodes finest scale sets values process disconnected components formed removing node mutually independent 
way specify complete probabilistic description process generalization specification temporal markov process terms initial distribution transition probabilities 
specifically denote root node single node top tree coarsest resolution 
node specify marginal distribution 
node tree denote parent node connected coarser scale see specify step coarse fine transition probability 
initial distribution full set transition probabilities nodes completely specifies joint probability distribution entire tree 
class models illustrate concepts class linear gaussian models gaussian random vector values process finer scale nodes specified coarse fine linear stochastic dynamics 
denotes probability density function continuous variable discrete probability mass function takes discrete set values combination hybrid continuous discrete quantity 
matrix specified node possibly varying node node gaussian white noise process set mutually independent gaussian random vectors defined node 
model simple generalization usual linear state space model temporal processes systems 
analogous manner define classes processes generalization finite state markov chains trees 
numerous occasions find comparison temporal useful interpret results identify places extension trees introduces issues encountered time series 
examples help gain initial intuition models breadth variability initial examples 
example 
example model form introduced context image denoising see 
specifically suppose interested modeling random field defined square region simplicity assume number pixels edge square power allowing simple quadtree structure 
case index node thought tuple denotes scale node pair specifies spatial coordinates coarsened spatial region corresponding note root node node need spatial coordinates number resolutions consecutively increasing scale corresponding finer resolutions 
scalar gaussian random variable define entire process tree recursion scalar gaussian white noise process tree intuitively thought coarse scale representation random field modeled scale spatial location corresponding node simple model allows introduce concepts issues arise models 
concerns choice variance 
simple choice constant variance entire tree 
note case examine sequence values corresponding path tree root node leaf node see sequence simple constant variance gaussian random walk 
hand choose variances constant scale decrease geometrically scale scale variances decrease scale factor resulting process rudimentary type self similarity fractal character variance variation scale power law dependence scale 
say self similarity fractal processes see somewhat different example 
second observation corresponds simplest coarse fine interpolation process interpolation consists simply copying value coarser node term right hand side adding independent detail 
interpretation multiscale dynamics coarse fine interpolation combined addition new detail resolution clearly rings concepts common areas multiresolution analysis notably wavelets 
say relationships wavelets 
point clear tie wavelets ideas interpolation requires considerably thought 
particular standard multiresolution decomposition signal image values variables coarser nodes simply functionals weighted averages smoothed differences values finer scales 
certainly case wavelet analysis 
note process defined certainly case average descendent values white noise values added children independent 
consequence primary true standard temporal models signals simplicity assume variables model gaussian results concepts models second order properties means covariances hold broadly wide sense concepts 
example assume uncorrelated node node wide sense white noise tree estimation algorithm section represents best linear estimator 
interest random field finest scale values coarser scales model represent true hidden variables deterministic functions finest scale process 
contexts reasons building models hidden variables lead efficient algorithms 
addition discuss class called internal models coarser scale variables hidden 
example 
class models introduced gauss markov processes markov random fields 
simplest example uses paul vy construction brownian motion midpoint deflection 
specifically suppose wish construct sample path brownian motion process time interval say length interval 
generate samples dimensional gaussian random vector illustrated draw straight line generated values process endpoints 
represents best estimate values process points values endpoints 
consequently error estimate specific point independent values points 
midpoint interval generate independent zero mean random variable variance equal error variance estimate endpoint values 
straight line midpoint adding new random variable samples desired joint distribution sample path brownian motion 
process continues advantage critical fact brownian motion markov process conditioned value midpoint values brownian motion half intervals mutually independent subsequent deflection midpoints half intervals carried independently 
result procedure generating denser denser samples brownian motion depicted 
suggests procedure described corresponds exactly linear gaussian model form dimensional state node consists endpoint midpoint values subinterval identified node coarse fine dynamics precisely midpoint deflection scheme just described 
node corresponds half interval associated parent node 
result components dimensional state child node simply copied parent node endpoints parent interval midpoint value new midpoint value generated child interval average endpoints adding independent zero mean gaussian random variable variance equal error estimate new midpoint endpoint values 
straightforward calculation write dynamics example see doing explicitly important observations 
procedure just described works equally gauss markov processes including higher order 
difference best estimate midpoint value endpoint values general complex linear function endpoint values depending correlation structure field 
note fact increments brownian motion variances scale linearly length interval increment taken model depicted self similar scaling behavior variances midpoint decrease geometrically move finer scales 
addition example step brownian motion construction involve coarse fine interpolation plus addition independent detail midpoints nature interpolation detail different example fact state process node doesn represent spatial average process different type coarse scale representation simple point piecewise linear approximation brownian motion sample path illustrated 
note contrast example model brownian motion internal state node completely deterministic function children 
example 
class nonlinear models plays just important role theory practice linear model class markov chains trees 
model variables see related constructions called brownian bridge 
tree takes finite set values nature cardinality set may vary node node scale scale 
described previously model completely specified terms distribution root node parent child transition distributions node 
models long history extending back studies statistical physics dynamic programming artificial intelligence investigations graphical models signal image processing 
illustrate examples models different purposes 
class image segmentation problems introduced section discrete variable node represents coarse level label image region corresponding resolution location node 
standard example problems multiscale variant potts model child node takes value parent probability equally take value different parent label index set node 
allow probability child label parent vary scale described may wish increase probability finer scales 
second example see discrete models context wavelet image denoising problems markov chain represents hidden variables node control distribution wavelet coefficient node 
see model capture cascade behavior seen real imagery large wavelet coefficients occur localized patterns scale corresponding locations abrupt changes edges high frequency signal image features 
including hidden variables leads denoising algorithms automatically adapt presence edges alleviating blurring occurs space invariant linear filtering performed 
ties graphical models time series matrix factorization indicated models trees special class graphical models :10.1.1.114.4996:10.1.1.115.4360
eye generalizations describe lay foundation relating framework briefly summarize basic graph theoretic concepts associated larger class models 
consists set vertices set edges pairs vertices 
general distinguish directed graphs edge directed node node edges represent different objects undirected graph represent different objects inclusion equivalent inclusion 
purposes sufficient focus moment comments shortly 
consider undirected graph random process defined index set particular importance class mrf graph specifically node denote set neighbors set nodes connected edge 
node simplicity assume connected exist paths edges connect pair nodes conditioned values process neighbors independent remaining values process nodes 
alternative characterization requires bit graph theoretic terminology 
path graph sequence nodes edge corresponding successive pair sequence 
subset cuts graph remaining nodes partitioned disconnected subsets subsets path includes element introduce notation xs set values subset generally denote xv simply 
markov subset cuts disconnected subsets xu xw xa xu xa xw xa conditioned values 
ona set values 
independent set values 

note separates disconnected subsets 
sets values process subsets mutually independent values specification markov models general graphs requires care 
particular contrast temporal markov processes see tree models markov models graphs general specified terms marginal density single node transition probabilities pairs small groups nodes fact general graph loops cycles nontrivial paths node 
loops imply constraints typically complex numerous marginal transition probabilities represent simple parametrization markov distributions 
clifford theorem provides parametrization terms called clique potentials 
particular clique fully connected subset pair distinct nodes 
denote set cliques hammersley clifford theorem states 
markov respect probability density written form exp xc xc function values 
clique known clique potential 
normalizing constant referred partition function 
points worth noting 

gaussian process know exponent quadratic form vector minus mean matrix appearing quadratic form inverse covariance matrix ofx 
examination hammersley clifford theorem process yields observation 
markov respect matrix blocks indexed nodes denotes block 
general specification markov process providing natural unconstrained parametrization leads significant computational challenges 
example recovering marginal probability distributions process individual node specification complexity note exponential form implies case necessary sufficient condition 
conditions stated case detail unnecessary exposition 
refer reader start section aspects graphical models 
grow size graph gaussian case 
similarly estimating parameters models performing estimation process measurements extremely complex 
situation far simpler acyclic loop free tree illustrated 
way see case consider relationship directed graphical models undirected ones 
directed graphical model quantities specified include conditional distribution node values parents parent directed edge 
straightforward convert directed graphical model undirected see construction directed graphical model equivalent undirected generally complex fact requires defining new node edge sets nodes consist entire cliques nodes original undirected graph see graphical models section insight 
tree construction directed graphical model undirected straightforward fact change nodes graph graphical structure edges directed undirected 
specifically consider undirected graphical model tree choose node designate root node 
consider hanging tree node redraw graph root node top level neighbors level example labeled node root node neighbors denoted nodes andu 
redrawn tree appears hang 
straightforward see distribution graphical model specified exactly section terms marginal distribution root node set parent child transition distributions 
particular note acyclic graph single node leaf node cuts graph disconnected components 
result mrf graph processes subtrees rooted mutually independent conditioned value 
probability distribution 
canbe factored terms individual marginal distribution conditional distributions subtrees rooted 
continuing process subtree conditional distributions specified terms individual transition densities conditioned transition densities leaf nodes conditioned parent 
preceding discussion framed graph theoretic language ideas quite familiar signals systems community describe terms time series matrix factorizations 
specifically consider discrete time gauss markov process scalar valued simplicity defined interval form vector ordering values 
sequentially 
case graph interest simply set integers interval edges consecutive integers 
know inverse covariance tridiagonal 
tridiagonal inverse covariance corresponds undirected representation statistical structure process 
know process simple sequential directed representation graphical structure connecting time point successor 
specifically take point root node acyclic graph process corresponding directed representation markov process familiar order autoregressive ar model 
set independent gaussian random variables independent initial condition value root node 
representation precisely form directed model 
matrix interpretation representation equally simple 
specifically define vector just seen diagonal covariance denote discrete state processes complexity combinatorially explosive linear gaussian case complexity linear algebraic computations grows polynomially 
case required computations prohibitive markov processes arbitrary graphs 
say subsequent sections refer reader start section 
discussions inferred graphical model start section 
discussions discussion called reciprocal processes trees 
collect set equations trivial equation obtain vector equation form fx matrix lower reflecting fact equation involves single value predecessor trivial equation added involves initial value 
note simple calculation reveals corresponds simple square root factorization tridiagonal inverse covariance matrix case form factorization 
points particular note 
upper lower triangular factors fill compared tridiagonal structure equivalent statement graph corresponding causal recursion order directed graphical structure original undirected graphical model corresponding tridiagonal inverse covariance 
computation factors simple seen calculation variance involve joint statistics andx 
contrast general gaussian graphical model calculating square root factorization computationally involved results additional fill square root implying particular directed version model complicated graphical structure 
gaussian markov model tree procedure outlined hanging tree root node proceeding recursively tree implies calculation parameters analogous node child exactly simple temporal markov process fill 
precisely special properties tree models lead efficient algorithms describe section 
final point interesting note procedure outlined convert undirected graphical model tree directed representation type specified section allowed choose node root node define recursions relative choice 
implications standard temporal markov processes known define recursive model forward backward time 
widely known widely fact define recursive model proceeds center interior point ends interval see 
estimation inference algorithms models trees preceeding discussion suggests known fields graph theory theoretical computer science artificial intelligence linear algebra computations tree structures performed efficiently 
subsequent sections see implications efficiency statistical signal image processing large scale data assimilation substantial turn leads asking different questions typically arise contexts involving tree structured computations 
computation prior statistics simulation models discussing optimal estimation inference problems models examine related problems computation prior statistics process 
generation sample element nonzero corresponding element zero 
path simulation process 
computations important right provide initial look computational challenges performing statistical calculations challenges met effectively model tree 
problems focus primarily linear gaussian model comment analogous issues arise discrete state models 
discussed section specification gaussian model markov general graph corresponds directly specifying inverse covariance process 
calculating actual elements covariance specification calculating marginal joint statistics values 
individual pairs nodes far computationally easy task general graph 
particular naive approach simply invert inverse covariance computation complexity possibly large nd number nodes graph dimension state node graph complexity prohibitive fact applications describe contexts graphical models 
example consider model 
simple calculation shows case roughly times number pixels finest scale 
image order remote sensing problems introduced section easily millions 
applications computations scale worse linearly constant complexity pixel spatial estimation problems prohibitive 
applications remote sensing able store look full covariance contains billions distinct elements 
compute efficient manner selected elements covariance diagonal blocks corresponding covariances variables individual nodes small number diagonal blocks capturing correlation space scale selected variables 
sure efficient methods devised exploit structure particular graphs trees obtain especially simple scalable algorithms computations 
particular consider linear model white noise process covariance independent state root node covariance denote px 
see covariance satisfies coarse fine recursion px px generalization usual lyapunov equation evolution state covariance temporal state space systems driven white noise 
note computation directly produces diagonal blocks covariance matrix total complexity calculation nd 
quadratic dependence dimension reflects matrix multiplies additions linear dependence reflects fact recursion passes node tree 
calculation individual diagonal block covariance performed efficient manner 
particular nodes tree denote closest common ancestor statistical structure model find covariance px px nodes ancestor wehavethat denotes state transition matrix node descendent satisfies recursion scale analogous usual state transition matrix state space models situations dimension states different nodes may vary 
case dimension vector states simply sum dimensions variables reduces nd states dimension complexity operations involving matrix polynomial dimension possibly cubic complexity special structure exploited tree models 
interesting note strict generalization formula temporal models 
temporal context index set completely ordered equals result state transition matrix appears side 
models general trees appear general 
note calculation particular value computationally simple complexity bounded log factor log comes fact path length 
described computation statistics linear gaussian case concepts hold general models 
example consider case 
isa finite state process values node 
discussed preceding section computation marginal distributions individual nodes joint distributions small sets nodes extremely complex loopy graph graph cycles 
particular case distribution process entire graph involves state set size dn grows exponentially explicit computation projections distribution corresponding particular marginals joints shown np hard general graphs 
process tree 
represents generalization markov chain computations marginals nodes computed fine tree recursion generalizing usual chapman kolmogorov equation recursive computation distributions markov chains 
similarly joints node descendents calculated efficiently averaging ancestor node obtain joints set nodes yielding counterpart 
cases complexity computation grows linearly exponentially number nodes joint distribution required 
linear case computing storing joints prohibitively complex 
typically interested calculating modest number low dimensional joint probabilities computations performed efficiently 
models admit efficient simulation methods 
example generation sample path linear model simple generalization straightforward simulation linear state space model driven white noise 
need generate sample gaussian random vector corresponding root node independent samples perform coarse fine computation corresponding 
similarly discrete state model draw sample distribution coarse fine manner draw samples distribution node conditioned previously drawn sample value parent 
contrast simulation mrf graphs loops complex 
example discrete state processes iterative procedures metropolis gibbs sampling algorithms employed 
procedures generally require revisits node graph compared single pass node models 
final comment important note complexity algorithms described describe rest section scale extremely problem size measured number nodes tree 
case algorithms scale polynomially measures size variables stored node tree dimension state linear model cardinality state set finite state model 
consequently utility methods depends critically quite small compared observation essence provides acid test see particular problem successfully addressed methods described section 
issues constructing models manageable state sizes characterizing processes may possible subject section 
note essentially bit additional storage modest level additional computation calculation particular pair nodes yields values covariances pairs nodes path assumes balanced tree figures diameter tree length longest direct path pair nodes log 
classes gaussian graphical models loopy graphs simulations performed efficiency approaching comparable complexity tree models 
example fft methods simulate spatially stationary random fields nlogn operations follows directly fact fourier transform stationary processes 
alternatively efficient sparse matrix methods solve elliptic partial differential equations exploited loopy sparsely connected graphs resulting procedures sample generation complexity see section related discussion 
pass estimation algorithms section consider problem estimating process noisy measurements values 
previous section discussion linear case model specified simplicity exposition assume zero mean 
problem considered estimating process set linear measurements zero mean white noise process tree independent covariance matrix specifies measured node tree 
note principle model allows measurements multiple resolutions estimation algorithm describe provides means seamlessly fusing multiresolution data 
addition see applications introduced section revisited sections measurements finest scale nodes finest scale algorithm describe substantial computational advantages 
xs denote optimal estimate data tree denotes set nodes tree pe denote covariance error estimate 
developed detail computation quantities entire tree accomplished pass algorithm analogous pass rauch tung rts smoother temporal state space models 
smoother consists sweep algorithm 
sweep forward time yields optimal causal estimate optimal estimate time data including time computation performed kalman filter 
time interval forward sweep yields optimal estimate final point data optimal smoothed noncausal estimate terminal point data time smoothed estimate serves initial condition second sweep backward data compute optimal smoothed estimate point time 
time backward sweep combines optimal causal estimate time computed sweep smoothed estimate just computed time order determine optimal smoothed estimate time covariance error estimate 
general trees situation requires modest amount additional care notation 
rts algorithm time series equally applied interval just easily start kalman filter runs reverse time starting time followed sweep forward time asymmetry fine coarse fine directions trees 
result generalization rts smoother tree fine coarse child parent sweep starting finest nodes followed coarse fine parent child sweep 
fine coarse sweep computational flow illustrated generalization temporal kalman filter 
objective sweep computation node optimal estimate data vs subtree rooted node node descendents covariance error estimate 
temporal kalman filter recursive computation estimates involves steps intermediate quantities 
particular suppose computed best estimate corresponding error covariance node data vs measurement node 
computations produce updated estimate associated error covariance incorporates measurement node identical form analogous equations usual kalman filter optimality defined squares sense optimal estimate simply conditional mean available data 
addition smoothing algorithms graphical models trees somewhat different computational structures general complexity 
see example particular general characterization algorithm yields optimal smoothed estimates finite number steps 
equally define smoothing algorithm takes node root sweeps leaf nodes root node followed root leaf sweep 
note procedure property data node fact find way computations smoothed estimate node tree 
measurement update 
measurement innovations zero mean covariance gain updated error covariance second component fine coarse recursion step counterpart temporal kalman filtering involves fusion estimates come immediate children node specifically denote optimal estimate node data vs subtree rooted node denote corresponding error covariance 
fusing estimates produces estimate error covariance node data nodes descendent fusion subtree estimates andp ks denote number descendents node ks ks px prior covariance node computed 
third step recursion involves computation estimates error covariances child node step identical nature step prediction step usual kalman filter predict state time data time 
difference detail prediction fine coarse model specified coarse fine manner 
result form step involves called backward model analogous temporal models fine coarse prediction 
andp px px px px recursion initialized contrast temporal kalman filter provide initial conditions finest scale leaf nodes tree 
done setting initial estimate leaf node prior mean assumed initial covariance prior covariance initialization finest scale 
finest scale leaf node px note temporal kalman filters gain covariance matrices precomputed fact form tree generalization riccati equation error covariance 
fine coarse sweep reaches root node estimate covariance computed node provide initial conditions second coarse fine sweep exactly temporal rts algorithm xs pe derived computations second sweep identical form temporal rts algorithm 
particular computation node tree involves fusing optimal smoothed estimate covariance just computed parent statistics computed node kalman filtering sweep 
difference case trees node children computation carried parallel illustrated children node xs xs pe pe note covariance computations precomputed 
computation prior statistics described section smoothing algorithm just described significant computational advantages 
particular note computations node upward downward sweep involve matrix vector matrix matrix multiplies matrix inversions matrices vectors involved dimension involving measurement associated matrices 
addition node tree visited twice sweep 
consequently total complexity algorithm worst nd scale linearly number nodes tree 
furthermore result computation produces estimates error covariances 
understand significance result bit deeply consider alternate vector form estimation equations 
specifically denote vector values tree px denote covariance 
denote corresponding vectors measurements measurement noises respectively cx covariance block diagonal matrices formed respectively values andr tree 
form equations optimal estimate xs xs denotes vector optimal smoothed estimates corresponding error covariance fact markov tree tree structured pattern nonzero elements 
block diagonal signifying measurements independent noise individual nodes matrix left hand side tree structure 
implications observation 
solved efficiently tree structured generalization gaussian elimination fine coarse kalman filtering sweep followed back substitution rts smoothing step yielding complexity discussed previously 
sure methods numerical linear algebra conjugate gradient multipole methods solve equation order complexity 
particularly important algorithm non iterative nature especially fact yields diagonal blocks error variance matrix pe part computation 
error statistics extremely important applications see example follow subsequent sections major benefit tree models 
second implication tree structure directly generalizes known results temporal models implies error process xs process tree parameters matrices analogous andq original process automatically available result rts smoothing computations 
computations yield individual covariances values error process node method described section particular equations analogous smoothing error model compute covariances errors different nodes 
importantly see result smoothing process produces model remaining errors form original model directly fusion subsequent measurements available 
error model provides basis extremely efficient conditional simulation processes feature illustrated example 
interesting note connections preceding development discussion matter problems arising field decentralized control 
problems different nodes network correspond different agents controllers observe influence behavior dynamical system 
data collected agents centralized principle determine optimal estimate system measurements determine optimal coordinated control policy agents 
constraints include computation cases dominated issues communication constraints geographic separation agents centralization information possible consider alternative strategies coordination particular may produce estimates suboptimal 
issues decentralized control arise processing problems considered worth noting case relatively simple solutions agents referred partially nested information patterns example significant complication arises due indirect communication occurs agent control actions influences subsequent measurements agents 
construct directly related singly connected structure models mrf acyclic graphs generally 
example 
example consider optimal estimation sea level variations satellite measurements briefly described section detail 
mentioned section sea level variations fractal spectrum 
model variances noise process decrease geometrically finer scales represents simple model captures statistical behavior type spectral fall 
finest scale representation corresponds pixels size resolution satellite data 
data simply modeled measurements finest scale nodes corresponding locations tracks irregular pattern finest scale nodes 
shows results applying estimation algorithm described section data tracks shown 
shows optimal estimates sea level variations shows corresponding error variances entire field 
clarifying remarks example 
simply observation dimensionality example nontrivial attempting estimate roughly pixels approximately measurements time calculate diagonal elements error covariance matrix 
estimation algorithm simple model relatively modest computational task 
second point described measurement noise model case highly non stationary due fact errors knowledge known larger regions significant gradients due example significant features variations sea floor sea mounts 
advantages having error variances computed estimation algorithm detect statistically significant anomalies differences measurements estimates large compared estimation algorithm expect computed error variances 
shows locations set detected anomalies superimposed map ocean showing substantial correlation significant features ocean floor 
suggests things anomalies maps provide localized corrections 
third point having error variances multiple scales allows identify optimal scale reconstruction different points field 
particular reasonable greater confidence higher resolution reconstructions nearer regions covered satellite measurements regions farther satellite track 
method quantifying identify finest scale pixel coarser resolution ancestor smallest error variance 
location estimation finer resolutions leads increase uncertainty 
example applied different application estimation optical flow image sequences 
possible algorithm sophisticated models attempt capture known ocean surface accurately 
example discussed knowledge spatial inhomogeneities current adapt model locally increasing variances particular spatial regions particular scales 
addition possible higher order models developed surface reconstruction problems see example section 
likelihood function computation methodology described section tune models finding maximum likelihood estimates parameters model rate geometric fall noise variances see 
important note results shown obtained quite simply discussion point imply 
particular discuss section pointed authors models trees especially simple models produce results significant artifacts major tree boundaries points finest scale close spatially far apart measured path tree :10.1.1.42.5473
approaches dealing including described section note analogous methods proposed wavelet algorithms see example 
section 
produce results simple method averaging estimation results different tree models shifted slightly respect average smoothes artifacts 
refer reader section discussion important issue 
preceding discussion couched context linear gaussian models sweep structure optimal estimation holds markov model tree propagating means covariances upward downward sweeps propagate probability distributions 
example consider finite state process discussed example suppose observations assume conditionally independent measurements variables individual nodes 
discrete case raises number issues encountered linear gaussian case specific objective processing 
particular gaussian case algorithm described previously section viewed solving problems simultaneously provides joint conditional distribution entire process implicitly specified tree model yields individual marginal conditional distribution individual node yields estimates leastsquares optimal estimates individual node maximum posteriori map estimates map estimate entire process 
discrete processes computing node map estimates computing map estimate entire process quite different depending application may preferable 
example image segmentation applications strong arguments computation individual node estimates map estimate decidedly preferable reflects directly objective minimizing number misclassified pixels 
criteria third briefly discuss example considerable interest graphical models trees algorithms studied developed authors 
particular variety algorithms developed computation conditional marginals individual nodes 
class called message passing algorithms briefly described section 
explicitly implicitly places involves structure exactly described previously linear gaussian case see particular detailed development 
preliminary step perform fine chapman kolmogorov computation compute prior marginal distribution node 
algorithm proceeds fine coarse step analogous kalman filter computation distribution node conditioned measurements subtree rooted node 
coarse fine sweep analogous rts sweep yields marginals node conditioned data entire tree 
choosing mode marginals yields called mpm mode posterior marginals estimate 
furthermore linear gaussian case distribution entire process conditioned data tree structure model parameters conditional model conditional distribution root node conditional parent child transition distribution immediately available result sweep estimation algorithm 
note discussed obtaining shift invariant algorithms devoid artifacts noted text requires principle considering full set possible shifts tree :10.1.1.42.5473
models thought mixtures trees probabilistic draw tree models 
note straight averaging estimates trees result shift invariant algorithm technically optimal bayesian estimate 
particular optimal bayesian estimate require weighting tree estimate conditional probability observed data particular tree drawn mixture 
certainly possible likelihood computation methods described section knowledge 
benefit additional complexity believe negligible 
computation map estimate entire process involves somewhat different computations structure spirit emphasized investigations 
computing map estimate involves generalization known viterbi algorithm traced back study called dynamic programming artificial intelligence graphical models 
description algorithm mirrors closely pass structure estimation algorithms described far clear algorithm generalizes standard dynamic programming procedures 
fine coarse sweep performed functions computed node 
specifies optimal estimate node optimal estimate parent 
second optimal cost go maximum value conditional distribution entire subtree rooted node data subtree state value parent node 
quantity passed back parent node computation analogous pair quantities node 
top tree reached optimal estimate node easily computed initiating coarse fine recursion estimate parent node function computed upward sweep yield optimal estimate child 
mpm algorithm computation likelihoods described section key existence efficient structure fact conditional distribution process tree recursively factored 
likelihood functions addition optimal estimation algorithms described preceding section efficient algorithms exist computation likelihood functions quantities needed solution problems hypothesis testing parameter estimation 
specifically exploiting recursive factorizations processes develop algorithm computing likelihood function involves single fine coarse sweep data see 
algorithm follows equalities displayed discrete state case 
vs vs ks vs vs note root node doesn exist node right hand side simply likelihood function transition probability simply prior marginal 
case continuous variables summation integral reduces simple matrix vector equations linear gaussian case 
fine coarse application quite efficient method choice sole objective computation likelihoods alternative pass method linear gaussian case total complexity linear slightly larger proportionality constant 
alternate method computes quantities value problems anomaly detection brings interesting important similarities differences known ideas state space estimation temporal processes 
particular keys computation likelihood functions temporal state models fact key concepts generally temporal models forms concept whitening measurements recursively producing predictions successive measurement temporal kalman filter subtracted actual measurement values yields sequence independent random vectors referred innovations covariance depends known way temporal state model 
whitened measurements informationally equivalent original ones likelihood function written terms joint distribution innovations product marginals innovations successive time point 
estimation algorithm described preceding section particular fine coarse kalman filtering sweep produce set measurement prediction errors 
tree structure process white entire tree 
particular structure fine coarse sweep depicted value process involves predicting data subtree node reason difficult see certainly independent path leaf node root node nodes direct descendent generally independent 
described complete whitening operation likelihood written product distributions individual innovation values define total ordering extending partial order tree essence placing orders cousins kth cousins times removed complete whitening operation 
choosing ordering systematic fashion illustrated accomplish remaining whitening efficiently fact somewhat different coarse fine sweep complementing kalman filtering fine coarse computation 
result algorithm total computational load scales linearly example 
application likelihood function computations texture discrimination application introduced section developed thoroughly 
mentioned section textures shown modeled effectively mrf models 
likelihood function computations mrf highly nontrivial suboptimal methods 
approach starts simple obvious statement mrf model represent truth idealization real textures 
result reasonable seek alternate models lead simpler likelihood computations resulting performance essentially able achieve original mrf models 
illustrates results texture discrimination system mrf models models constructed reduced order cutset models type described subsequently section 
depicts probability error texture discrimination models models mrf model sand texture model parametrized scalar parameter corresponds model texture corresponds sand texture intermediate values correspond mrf parameters weighted average parameters sand textures textures discriminated different increasingly similar increases value identical 
shows discrimination performance simple approximate models textures yields performance significantly better suboptimal method nearly performance achieved likelihoods exact mrf models 
shows algorithm performance varies order model increased see section detailed discussion modeling 
indicates low order models results performance essentially achieved exact mrf models textures 
ties graphical models provide additional perspective algorithms described preceding sections take brief glimpse inference algorithms models general graphs topic remains subject numerous investigations see discussed 
represents martingale increment process partially ordered set defined tree 
order model refers dimension state node 
return section 
consider estimation state assumed zero mean simplicity gaussian graphical model possibly loopy graph set linear measurements 
section collect state values single vector covariance px similarly collect measurements vector measurement equation optimal estimate xs solution error covariance matrix 
just analysis section see block diagonal structure implies structure diagonal blocks nonzero blocks corresponding edges graph result estimation error markov respect graph result number places literature generalizes results time series trees discussed section 
true nonlinear discrete state models conditioning mrf independent measurements individual nodes yields conditional distribution markov graph 
results section indicate graph loop free efficient methods computing estimates error covariances linear gaussian case marginal conditional distributions general nonlinear case 
reason efficiency explained terms existence elimination orders orders variables eliminated sweep added back second sweep fill 
random field graph theoretic perspective lack fill elimination orders simple interpretation subsample random field graphical model eliminating set variables restricted model remains markov respect restriction original graph 
additional edges need added see particular discussions issue general graphical models 
similarly saw section computation likelihoods loop free models performed efficiently 
interpreted terms existence elimination orders fill directly tied factorizations probability distribution graphs terms root node marginal parent child transition densities section 
particular existence factorizations implies partition function function values parameters model loop free graph matrices andq ina linear gaussian model fact greatly simplifies maximum likelihood estimation parameters models 
addition models focus loop free graphs processes examined literature primarily context image processing 
class received early attention investigation recursive estimation algorithms random fields see involves imposing complete order regular grid points typically raster scan order past point lattice consists points previous lines lattice plus points line come earlier scan order 
example class called markov mesh models gaussian discrete state processes impose partial order pixels 
problems imposing total partial orders clearly artificial 
high order models types needed capture accurately statistics random fields interest 
result considerable developing stochastic models images impose orderings pixel locations 
example mrf models called order mrf nearest neighbor graph associated regular lattice represent widely studied class type 
consequently investigation inference loopy graphs rule exception fields including artificial intelligence turbo coding great interest image processing 
indicated previously distributions markov models loopy graphs admit elimi graphical model literature distinction measurements variables defined nodes graph simply knowledge variables wish perform inference estimation likelihood calculation example information 
cosmetic substantive difference easily add nodes graph corresponding nodal measurement single edge node connecting corresponding original node wish perform inference observation variables new nodes 
particular order solve gaussian elimination back substitution new nonzero elements introduced matrix left hand side variables eliminated 
nation orders fill simple factorizations terms local marginal distributions 
furthermore partition function graphical model generally complex function parameters graphical model clique potentials 
linear gaussian case partition function proportional square root determinant process covariance known whittle see random fields computation likelihoods optimal parameter estimates models loop free graphs computationally tractable straightforward absence simple factorizations dependence partition function model parameters optimal parameter estimation hypothesis testing far challenging computationally 
analogous challenges arise solving estimation problems computing conditional marginal distributions general nonlinear discrete valued models linear gaussian case solving determining diagonal elements error covariance inverse 
particular general graphical models successive elimination variables order induces fill implying set variables remaining elimination step markov respect graph additional frequently additional edges see subsequent stages computation may increasingly complex 
result complications considerable interest developing exact approximate computationally feasible methods inference problems 
example simple graphs consisting single loop efficient non iterative algorithms exist closely related methods solving point boundary value problems performs sweep computations analogous rts computations ignoring boundary conditions fact endpoints sweep linked 
subsequent correction step boundary conditions account produces correct estimates covariances correct likelihood function 
described section possible principle construct exact non iterative algorithms inference loopy graphs essentially converting problems loop free graphs nodes correspond groups nodes original graph 
methods correspond eliminating groups variables linear algebra literature called nested dissection methods 
exact methods computationally feasible graphs result considerable interest developing approximate iterative algorithms 
linear estimation problem variety methods exist especially underlying graph loopy far fully connected matrix left hand side sparse 
example solving equation order mrf essentially corresponds solving discretized version elliptic partial differential equation extremely fast algorithms conjugate gradient multipole exist 
methods compute error covariances diagonal blocks pe diagonal blocks difficulty briefly discussed section 
implies methods allow compute likelihood functions 
graphical model literature contains variety iterative methods apply linear nonlinear discrete models directly yield approximations conditional distributions estimates covariances linear gaussian case cases computation likelihoods 
methods generating samples conditional distribution entire process techniques metropolis gibbs sampling algorithms estimate marginal distributions individual nodes approximate mpm estimates obtained part simulated annealing procedure computation map estimate 
variety alternative deterministic methods exist 
method iterated conditional modes icm value node iteratively modified maximize conditional distribution node current iteration values neighbors method reduces gauss seidel iteration solution 
include called mean field methods rich class variational methods 
method deserves mention developed called bethe tree approximation 
approximation computation statistics specific individual node approximated replacing original graph tree rooted node see methodology applied time series analogous results trees results discrete state graphical models single loops 
path away node original graph replaced path tree 
path original graph contains loop goes node second time corresponding path bethe tree passes distinct node corresponding second copy actual node original graph 
tree algorithms compute approximations desired statistics 
alternate methods setting boundary conditions tree described 
uses particular approach extending tree terminating finite point concept explored intersects important class iterative methods widely known graphical model literature subject considerable current interest object analysis point departure developing understanding emerging methods 
class called belief propagation bp algorithms originally developed context discrete models 
variety forms bp especially trees version introduced applies loopy graphs message passing iterative algorithm node iteratively passes messages incorporates messages neighbors 
single step iteration node information nearest neighbors step includes information nodes distance node question providing expanding sphere influence node iteration proceeds 
key algorithm method incorporating successive messages set neighbors generating messages passed back set neighbors 
intuitively bp message passing algorithm incorporates message assuming new information contains independent previously incorporated information information provided messages neighbors 
case long messages propagate loops graph 
tree algorithm yields optimal estimate number iterations equal diameter tree information propagates node node 
linear nonlinear model tree local message passing version bp represents alternative sweep algorithms described section total computational load slightly greater sweep algorithm roughly speaking nodes performs computations iteration number iterations corresponding diameter tree balanced tree order log 
furthermore elucidated tree bp provides parameters needed construct factorizations distribution entire graphical process providing basis efficient likelihood function computation construction equivalent directed models choice root node 
bp applied graph loops sets information provided different neighbors successive iterations independent example message sent node incorporated succession messages eventually way back originating node loop graph 
dependencies accounted bp inference computations roughly speaking information counted multiple times propagates loops graph 
result bp may converge converge typically converge correct statistical answers 
empirical success algorithm number applications including turbo decoding artificial intelligence known failures bp cases led intensification efforts analyze bp develop enhancements variants 
example shown bp applied approximate solution linear gaussian estimation problem simultaneous approximate computation diagonal error covariance blocks pe may may converge 
converges estimate xs fact converge exact solution 
computed approximations error covariances converge correct values typically underestimate size estimation errors 
discussion indicates complexity inference graphs loops sparked considerable body research active lines inquiry develop new algorithms analyze performance existing procedures 
section return briefly topic describe lines current inquiry exploit efficiency inference trees order develop new algorithms inference complex graphs 
orientation wavelets multigrid inverse problems pointed methods span broad array concepts approaches section examine components large field 
objective doing help reader navigate larger domain understand lines inquiry relate models algorithms focus 
wavelets wavelets analyze stochastic processes familiar concept comes mind idea analysis stochastic processes raised 
reason stems analyses demonstrate wavelet transforms provide substantial decorrelation important classes processes fbm constructions processes wavelet synthesis 
section take brief look relationships wavelets models trees subject divided components 
describes set important examples explicitly form models trees variables nodes corresponding individual detail scaling coefficients wavelet decomposition signal image 
second focuses interpretation wavelet synthesis coarse fine scale dynamic system viewpoint received significant attention literature explicit connection tree models topic defer section obvious 
elementary wavelet models trees hidden markov models wavelet cascades fact wavelet coefficients stochastic processes nearly decorrelated leads directly elementary method multiresolution modeling simply assume coefficients completely decorrelated 
approach example proposed developed modeling fractal gaussian processes 
models trivially identified simple models trees individual detail coefficients serve variables populate nodes different levels tree 
example signal dyadic tree node corresponds particular scale shift coefficient placed node detail coefficient corresponding scale shift 
identifying variable state linear model leads model capturing fact simple model coefficients independent gaussian 
case quadtree node corresponds scale shift variables resident node detail coefficients associated scale shift 
implies coefficients independent node node 
take covariance ofw diagonal done literature coefficients node independent 
class models described preceding paragraph degenerate coarse fine dynamics serves useful point departure examination wavelet model constructs 
involves issue great importance image processing applications particular wavelet coefficients nearly decorrelated definitely gaussian independent 
particular discussed authors distribution wavelet coefficients tends highly heavy tails resulting fact coefficients come relatively smooth regions image quite small corresponding locations edges large 
furthermore discussed literature large wavelet coefficients generally form cascades localized space propagate scale reflecting presence edges 
called embedded zerotree approaches image coding take explicit advantage properties wavelet coefficients 
standard wavelet analysis images coefficients correspond high frequency detail dimensions low frequency horizontal high vertical high frequency horizontal low vertical 
class models captures non gaussianity wavelet coefficients dependence involves simple modification model described previously 
particular non gaussian distribution 
possibility heavy tailed distribution class called generalized gaussian distributions form 
alternate class choices distributions distributions 
example simpler models introduced consists modeling wavelet coefficients independent distributions consisting finite weighted gaussian mixtures 
model truly heavy tailed gaussian fall capture substantial range non gaussian behavior 
example simple component mixture consisting low variance high variance gaussians 
note case think node tree having hidden variable set hidden variables corresponding random choices mixture component wavelet coefficient 
alternatively discussed see obtain rich variety truly heavy tailed distributions called scale mixtures multiplying unit mean gaussian random variable positive random variable generated nonlinear function second independently drawn gaussian random variable 
gaussians essence hidden variables heavy tailed product observed 
optimal estimation choices distribution mentioned paragraph corresponds performing nonlinear operations individual wavelet coefficients 
algorithms result models called wavelet shrinkage algorithms 
described independent mixture models result simple nonlinear operations individual wavelet coefficients optimal estimation 
discrete state hidden models continuous ones open door building models trees capture non gaussianity wavelet coefficients dependencies displayed coefficients real imagery 
number approaches capturing dependency developed see contributions 
method described wavelet shrinkage obtain coarse estimate denoised wavelet coefficients denoised coefficients subtracted observed data linear model sweep algorithm section estimate smaller scale fluctuations 
approaches attempt capture directly cascade behavior occurrence large wavelet coefficient scale implies nearby coefficients scales large 
example illustrates important method accomplishing example 
series papers methodology developed signal image processing applications called hidden markov trees 
basic idea models builds gaussian mixture model just described 
particular model case finite state model described example 
basic version model state consists single binary random variable signals set binary random variables correspond choices low high variance gaussians corresponding wavelet coefficients 
order capture cascade effects large wavelet coefficients choose parent child transition probabilities choice high parent node high chosen child node 
wavelet coefficients modeled conditionally independent value corresponding discrete state 
model discrete state version sweep estimation algorithm described section directly applied order perform image denoising 
depicts result applying methodology noisy image depicted set complex valued wavelets chosen minimize problem non shift invariance arises decimated wavelet decompositions 
comparing wiener filtering results figures see visually nonlinear processing inherent hidden markov tree model leads excellent noise rejection blurring evident linear wiener filter 
refer reader cascade heavy tailed behavior image wavelet coefficients captured linear gaussian model hidden state measurement model involves multiplying nonlinear function hidden state white noise sequence produce model actual wavelet coefficients 
coefficients uncorrelated independent fact display type cascade behavior seen real imagery 
furthermore mentioned previously model capture truly heavy tailed distributions 
estimation nonlinear model requires iterative 
linear model hidden variables iteration performed efficiently sweep algorithm described section 
idea multiplicative models capture cascades multiresolution decompositions rich literature areas ranging mathematical physics study random cascades multifractals multiscale models counting processes multifractal cascade models communication network traffic :10.1.1.42.5473:10.1.1.32.3823
example model state variables node scaling wavelet coefficients described example section 
lack shift invariance associated decimated wavelet representations led variety authors consider alternatives generally involving overcomplete undecimated wavelet representations overcomplete steerable pyramids allow avoid artifacts due lack rotational invariance 
overcomplete representations implies faithful statistical model capture fact constraints coefficients representation 
including constraints greatly complicates statistical model common practice ignore model produces estimates sets variables inconsistent correspond coefficients overcomplete representation signal 
practice variety methods projecting estimates consistent set equivalently directly reconstructing signal estimates developed refer reader cited previously paragraph 
note inconsistency closely related concept internal models introduced example discussed section section 
substantial body called adaptive representations entire families dictionaries bases taken generally form vastly overcomplete sets :10.1.1.46.2080
objective methods select basis collection leads best representation terms maximal decorrelation sparsity resulting expansion coefficients signal question 
argument approach produces tree structured signal decomposition correlation dependence coefficients tree minimized 
result optimized representation improve accuracy resulting model signal trivial linear model complex model hidden markov trees 
scale recursive models wavelet synthesis turn alternate approach wavelets scale dynamic models 
method described authors wavelet synthesis equation 
specifically xm denote vector scaling coefficients th scale orthogonal wavelet decomposition wm denote corresponding vector wavelet coefficients scale 
assuming compactly supported wavelet corresponding particular pair wavelet scaling filters wavelet synthesis equation written scale scale recursion xm hm gm matrices corresponding conjugate filter pair particular wavelet decomposition chosen term right hand side corresponds coarse fine interpolation scaling coefficients second insertion additional detail finer scale 
eq 
starting point important observations investigations 
involves direct multiscale modeling estimation wavelet coefficients modeled note representation signals xm twice dimension xm corresponding full set variables corresponding level pyramidal representation 
number coefficients increases factor move scale finer scale 
reason linear operators hm gm rectangular dimensions vary scale 
white noise 
case estimation model noisy measurements wavelet coefficients corresponds standard scale recursive kalman filter state scale corresponding entire vector scaling coefficients xm 
note model allows direct fusion nonlocal measurements long correspond observations individual wavelet scaling coefficients 
second better modeling estimation residual correlation account 
authors considered methods doing suggests simple method type similar approach described section 
particular suppose objective construct model finest scale process covariance closely approximates covariance fbm 
coefficients xm wm scaling wavelet coefficients finest scale process xm directly specified desired statistics fine scale process determine statistics coefficients coarser scales 
example computation determine variances wavelet coefficients wm 
ignore correlations computed variances assume wm white obtain approximate model type described 
easy obtain accurate approximation analogous increase dimensionality 
specifically suppose assuming wm white weaker assumption xm forms markov sequence scale 
case capture correlation wm xm writing wm best estimate wm xm uncorrelated xm fact white sequence scale assumption xm markov 
lm covariance directly computable terms target fine scale statistics 
substituting obtain dynamic model xm hm xm gm represents accurate approximation captures residual correlation wavelet scaling coefficients 
course consider higher order approximations modeling xm higher order markov process scale approximations require defining state recursive estimation consists successive resolutions scaling coefficients 
similarly attaching hidden markov tree principle synthesize approach described introduced example possibility date examined literature 
addition nonlinear variations type structure estimation wm xm takes form sense general restrictive 
particular models individual detail coefficients comprising wm assumed conditionally independent conditioned specified window neighboring scaling coefficients corresponding subvector xm 
dependence coefficient local window scaling coefficients resulting conditional independence entire vector coefficients represent principle restrictions compared general form restrictions generally required order obtain computationally tractable models see similar section 
general conditional distribution component wm modeled gaussian mean variance nonlinear functions window neighboring scaling coefficients see section brief discussion 
note models represent consistent models values vectors xm wm probability scaling wavelet coefficients finest scale process xm 
discussed possible specify models consistency exploit wavelet structure 
particular model studied variant xm wm wm white noise 
note additional degrees freedom models provide additional flexibility potential greater accuracy approximating statistics process 
examples exist demonstrating potential models see discussion section exploiting flexibility systematic fashion remains open problem 
true estimation models admits efficient solution exactly cases 
particular note dimensions state variables substantial corresponding scaling coefficients single scale 
result direct implementation kalman filtering equations prohibitively complex signals images modest size 
discussed data available independent noise corrupted measurements wavelet scaling coefficients transformed form kalman filter implemented decoupled diagonalized form wavelet transform 
case occurs dense measurements process scales corrupted independent additive noise variance vary scale scale constant scale 
case application wavelet transform data transform independent measurements individual wavelet coefficients 
hand available data sparse irregularly sampled simply varying noise variances scale wavelet transform yield simplification complexity kalman filter associated models prohibitive 
reasons apparent complexity models correspond models trees case haar wavelet 
fortunately careful component component look structure wavelet synthesis equation different definition state allow construct models trees 
construction wavelet models described section 
multigrid coarse fine algorithms section discuss classes algorithms multigrid coarse fine algorithmic structures 
exception approaches describe methods fundamentally different focus 
reason brief descriptions exhaustive review literature 
algorithms fall category focus correspond problems originally described single finest resolution data desired estimates available required single resolution 
variety reasons direct solution single resolution problem complex consider directly subject large numbers local minima far optimal solution 
general idea coarse fine methods problems construct approximate coarser versions problem typically multiple resolutions solution coarser hopefully simpler problems guide solutions finer eventually finest scales 
simplest examples approaches problems spatial phenomenon estimated modeled random field simply viewed unknown represented multiresolution fashion allow coarse fine algorithms maximum likelihood estimation 
example approach context inverse conductivity estimation problem unknown conductivity field modeled piecewise constant sequence resolutions coarse fine corresponding haar wavelet approximations solution problem resolution initial condition solution finer resolution 
similarly coarse fine approaches detection localization significant anomalies modeled unknowns refer reader significant extension ideas construction wavelet packets wavelets wavelet packet transform dense measurements scales transforms problem set decoupled tree estimation problems coupled common root node 
background field modeled random field developed allow efficient zooming features interest 
extensive literature mrf models full multigrid computational algorithms purely coarse fine algorithmic structures 
examples treatment represents author knowledge thorough examination application multigrid methods image processing computer vision problems 
full multigrid methods discussed depth devoted subject involve coarse fine fine coarse operations iterative algorithmic structure 
idea coarse fine step essentially methods described preceding paragraph interpolate coarser approximation estimate finer scale provide starting point optimization scale 
various types interpolation 
example general class wavelet interpolation schemes interpolation scale involves simply propagating scaling coefficient scale finer corresponding detail coefficient set zero wm set 
interpolation schemes coarser variables simply represent subsampled versions field sparser coarser scales 
case coarse fine interpolation correspond replication complicated bilinear interpolation 
discussed fine coarse operations multigrid reflect fact applications multigrid problem solved coarser scale represents approximation original problem 
example approximating solution partial differential equations arise continuous space mrf estimation problems derivatives scale replaced differences 
approximations suffer aliasing errors principle reduced estimates finer scale 
discussed time general interpretation multigrid algorithms applied random field estimation resolution algorithm computes optimal estimate assuming finer scale detail field fine interpolation process exact finer scale detail coefficients 
example assume image constant block pixels noisy measurements pixels simply averaged computing corresponding optimal estimate resolution 
subsequently available finer scale estimates general vary coarser blocks fine scale detail correct erroneous averaging coarser scale allowing new coarser scale estimates computed 
exactly fine coarse multigrid correction step 
number mrf estimation algorithms full multigrid structure coarse fine operations performed 
see problems solved coarser scales correspond exactly original problems constrained set allowed reconstructions finding optimal estimate fields piecewise constant resolution case 
certainly possible view coarse fine algorithms purely computationally motivated constructs clear statistical interpretations computations representations embedded algorithms 
result number authors looked detail question suppose mrf model finest resolution corresponding statistical structure coarsened version field corresponding coarse wavelet approximation subsampled version field 
discussed inferred discussion begins nearest neighbor mrf uses haar coarsening block averaging set increasingly coarse scales coarser fields nearest neighbor mrf 
treatments mrf models explicitly discussed 
regularization formulations involve variational penalties reconstructed field smoothness direct interpretations mrf models 
see section 
closely related fact mentioned preceding paragraph methods produce exact solutions original estimation problem resolution constraints finer scale detail random field 
discussed detail special set circumstances coarser scale fields resulting coarsening procedures subsampling called renormalization group methods simple exact descriptions may correspond graphical models fully connected graphs 
cases generally coarser scales approximations exact statistics coarsened representation tractable computationally 
example methods constructing mrf models represent optimized approximations coarsened fields described renormalization approaches statistical methods means averaging finer scale fluctuations order construct approximate coarser scale mrf models 
approximate models implies problems solved coarser scales approximations constrained versions finest scale problem 
course completely consistent philosophy multigrid coarse scale computations intrinsic interest serve purpose helping guide finer scale computations 
preceding discussion clear multigrid coarse fine image processing random field estimation algorithms described far vast majority methods literature coarse scale representations phenomenon interest introduced primarily computational purposes explicit implied statistical structure isolated scale scale single consistent statistical model scale 
result methods different focuses 
investigations fall general category multigrid coarse fine procedures involve random quantities multiple resolutions explicitly linked statistically scale graphical model 
example models trees coarse fine sar segmentation 
involves graphs trees image segmentation originally developed see describe illustrate section 
addition modeling frameworks described literature including basic graphical structure superficially resembles quadtree edges resolution scale connectivity exactly mrf nearest neighbor edges order models larger neighborhoods higher order dependencies resolution 
graph complexity considerably greater single scale order mrf methods simulated annealing need applied order obtain solutions 
fact authors develop multi temperature multiresolution annealing algorithms demonstrate potentially advantages approach result usual multigrid coarse fine philosophy coarser grids guide solutions finer ones 
refer reader multiresolution models involve structures trees algorithmic structures reminiscent multigrid coarse fine procedures 
final point methods mentioned preceding paragraph concerns modeling measurements 
critical properties graphical estimation problems algorithms discussed preceding sections assumption observation consists measurement state single node graph corrupted independent noise 
case actual graphical structure estimation problem reflect additional dependencies introduced measurements 
methods assumption conditional independence clearly satisfied measurements individual finest scale pixels 
original finest scale measurements transformed simply replicating measurements different resolutions transforming raw data wavelet transforms extracting features multiple resolutions 
furthermore algorithms papers implicitly assume transformed data conditionally independent condition cases measurement replication clearly true represents implicit assumption whitening resulting wavelet transformation feature extraction 
complexity assessed terms loop structure graph precisely terms complexity associated junction tree see section multiresolution algorithms inverse problems section take look multiresolution algorithms inverse problems problems measurements nonlocal functions random field reconstructed 
inverse problems span broad array applications mathematical formulations literature investigating properties developing methods solution equally vast see example 
problems methods field tangential connection statistical models processing algorithms primarily concerned 
consequently discussion brief focusing exclusively aspects area intersects main themes 
comment note multiresolution methods inverse problems significant overlaps topics preceding sections 
example multigrid coarse fine algorithms combat problems computational complexity local minima documented representing examples 
wavelets motivations include fact wavelets decorrelate stochastic processes cases lead significant sparsification nonlocal operator relating measurements underlying random field wish reconstruct 
example number authors exploited fact specific context tomographic reconstruction order develop deterministic inversion algorithms efficient allow fast high resolution reconstructions localized regions 
number authors decorrelation sparsification properties orthogonal wavelet decompositions order develop statistical reconstruction algorithms tomography deblurring inverse problems 
part methods involve simplest class statistical models described section resulting assuming wavelet coefficients uncorrelated random variables 
assume wavelet transform truly diagonalized measurement operator associated measurement model form corresponding uncorrelated measurements individual wavelet coefficients 
discussion section obvious variation methods wavelet shrinkage techniques corresponding modeling wavelet coefficients independent non gaussian random variables 
methods developed see example predominant wavelet shrinkage inverse problems introduced popularized donoho uses variation known wavelet decomposition 
shown area see designed important classes inverse problems including tomography 
decompositions correspond orthogonal wavelet decomposition random field reconstructed biorthogonal basis measurement domain exactly diagonalize measurement operator 
shrinkage transformed domain corresponds projecting measurements nonorthogonal measurement basis shrinking component component reconstructing orthogonal image domain basis 
shrinkage applications algorithms important asymptotic optimality properties related explicitly implicitly heavy tailed nature wavelet coefficients imagery 
refer reader details 
approaches described preceding paragraphs interpreted employing degenerate models modeled gaussian heavy tailed statistical approaches inverse problems involve complex models 
suggests development section knowledge explored combine models capture cascade behavior wavelet coefficients hidden markov trees described example decompositions described preceding paragraph 
second studied detail uses nonlinear models briefly introduced section section relate wavelet scaling coefficients note shrinking individual coefficients decompositions implicitly corresponds assuming transformed measurement coefficients independent see explicit discussion related issues 
perfectly valid probabilistic model leads impressive results applications worth noting measurements corrupted white noise coefficients measurement expansion exactly uncorrelated basis orthogonal half biorthogonal pair 
scale pyramidal graphical structure generally complex tree leading authors employ iterative estimation algorithm tomographic reconstruction problem focus 
random field model multiresolution model wavelet domain reconstruction tomographic reconstruction problem performed directly image domain ensure positivity reconstruction full nonlocal tomographic measurement operator measurements local respect nodes graph implies estimation structure quite different described section 
examples described far section tangentially related models trees interpreted involving degenerate models investigations explicit substantive contact primary focus 
example involving poisson data multiplicative scale scale dynamics illustrated example 
example 
example take brief look models introduced studied context poisson measurements modeling traffic communication networks 
idea modeling counting processes multiple resolutions substantial history including number examinations models self similar counting processes variety applications 
models introduced simple properties poisson counting processes facts numbers counts process non overlapping intervals independent sum independent poisson random variables poisson 
poisson process time interval properties suggest simple dyadic partitioning interval construct tree exactly node identified corresponding dyadic subinterval example nodes correspond halves interval corresponding parent 
variable placed node simply number counts subinterval 
variables obvious fine coarse relationship sum child values andx model termed internal value node deterministic function descendents 
leads directly coarse fine model reflecting fact andx complementary fractions random variable independent node node values 
discussed conditional distribution child conditioned parent binomial distribution parameter corresponding fractional rate counts half interval versus :10.1.1.32.3823
denote mean number counts interval corresponding node binomial distribution authors develop alternate modeling framework works directly pixels state variables wavelet scaling coefficients 
approach tomographic reconstruction involves coarse fine multigrid algorithm data resolution perform estimation assuming finer scale detail absent simple scale scale pyramidal model 
graphical structure algorithms closely related introduced illustrated example section 
rate function poisson counting process known simple tree model 
central interest poisson estimation imaging applications case rate function random estimated observations count process 
mean sum poisson random variables sum individual means count function additivity property count process 
parent child relationships rate takes values precisely fractional rate appearing binomial parameter 
complete formulation problem estimating observation need specify distributions independent random variables random variable corresponding total mean count entire interval 
discussed particularly judicious set choices model gamma distribution beta distribution 
particular choice conjugate choice poisson distribution total number counts binomial distributions 
choices conditional distribution rates fractional rates forms priors simply obtained updating parameters distributions observed count values 
estimation problem put exactly form considered section leading sweep algorithm 
internality special structure model upward fine coarse sweep consists simply summing counts intervals computing unnormalized haar scaling coefficients numbers counts dyadic intervals followed nonlinear coarse fine sweep specify parameters conditional distributions optimal estimates node 
refer reader details 
important comments point 
haar scaling coefficients case critical structure model depends crucially summing independent poisson random variables 
ideas wavelets 
secondly mentioned section multiplicative form models contact research wavelet cascades multifractals see 
furthermore carefully choosing parameters beta distributions mixture models obtain wide variety count models including ones self similar bursty behavior estimation algorithms shrinkage characteristics 
refer reader details 
extension models deserves comment 
particular approach described uses asymmetric model coarse level square resolution split horizontally half split vertically 
steps corresponds splitting region halves resulting models analogous 
pointed directly consider quadtree structure counts square split separate child counts quadrants 
case conditional distributions child parent multinomial binomial corresponding conjugate distributions fractional rates dirichlet beta 
case resulting sweep estimation algorithm exactly structure case 
estimation problem considered far example directly observe counts function time space direct measurements counts time interval pixel 
applications emission computed tomography estimation problem true inverse problem total count recorded measurement detector sum counts corresponding photons pixel field imaged impinge detector 
quite tree model andx independent conditioned sum equal 
issue mentioned section concerning relationship tree models examined solved section simple state augmentation particular example state tree model 
glance problem require different solution techniques ones described previously example directly observe total number photons emitted location image 
iterative expectation maximization em algorithm employed step involves algorithms type just described guaranteed converge optimal estimate 
key method clever choice called complete information em algorithm number photons impinging measurement detector individual image pixel 
example application procedure simulated emission computed tomographic data illustrated 
final comment note methods inverse problems described section certainly involve wavelet transforms observed data assume availability regular set nonlocal measurements 
example near diagonalization measurement operators wavelet transforms rely regularity transform observed data modeled measurements individual wavelet coefficients 
interest problems may relatively irregularly spaced nonlocal measurements arises data fusion problems described section 
situation wavelet methods apply special case nonlocal measurements correspond exactly approximately measurements individual wavelet coefficients methods required fuse invert data measurements 
section describe way accomplished models trees multiresolution model construction identification development section clear models offer possibility efficient scalable algorithms 
caveat course requirement phenomenon data interest modeled framework modeled refers capturing desired statistics required level fidelity parsimony resulting model 
particular complexity various tree computations scales linearly problem size measured number nodes tree estimation algorithms linear gaussian problems scale quadratically dimension state model node inference algorithms discrete state processes polynomial cardinality state set node 
requirement useful model state dimension cardinality comparatively small increase modestly problem size 
examples preceding sections clear substantial body inference problems modeled solved models trees 
section step back specific examples part predetermined notions variables wavelet coefficients populate nodes model take careful look topic modeling different vantage starting points 
modeling model estimation course vast topics variety guises different fields contact 
certainly explore connections real depth hope serves show unique blend ideas enter modeling provide points entry exploration 
starting point modeling components available data statistical information characteristics wish capture model model class available 
possibilities sets measurements variables wish model explicit complete probabilistic specification variables statistical variability wish capture model implicit specification probabilistic specification terms graphical model 
mrf model 
specification class models available possibilities models nature variables node tree specified required modeling identification coarse fine see section brief description em algorithm 
probabilistic dynamics models tree structure specified variables specified determine nature hidden variables tree dynamics models tree structure specified needs identified learned 
interestingly possibilities standard fields signal processing systems control graphical models completely foreign point highlight occasions follows 
graphical model tree model ideas known graphical models field nearly common fields signals systems involve idea known fields aggregation variables state augmentation usually considered complete redefinition index set graph 
purposes discussions follow useful describe alternate viewpoints graphical constructions explicitly cutsets construct junction tree 
cutset models consider graphical model 
graph suppose cutset graph see section partitioned disconnected subsets separated subsets induced graph subset edges elements repeat process finding cutsets partition graphs disconnected subsets subgraphs continue reach small subsets nodes ultimate finest scale singleton nodes 
eq allows construct tree model 
example xu xw conditionally independent xa place vector xa root node tree continue process finer finer cutsets partitions 
example depicted 
shown set cutset indicated xa plays role state root node tree 
singleton set cutset subgraph lead take state corresponding second level node 
independent conditioned full graph connections nodes 
augment value state second level cutset values parent level conditioned values independent 
result tree model shown solid lines 
points worth noting construction 
individual values graphical process model solid lines show nodes different levels tree 
straightforward extend model shown additional dashed lines model individual values process appear finest scale 
corresponds simply deterministic copying variables parent node root node value copied children 
secondly note incorporate dashed part model resulting model internal sense introduced section 
particular states left branch tree root leaf deterministic function finest scale descendents 
true right side easily accomplished redundant copying values finest scale 
obviously models corresponding solid lines solid dashed lines considerable redundancy may may value representation certainly increase dimensionality raw computational complexity algorithms models 
simple fact augmenting state node full state parent generally unnecessarily complex 
example examine graph see nodes independent conditioned information contained cutset augmented full cutset need add boundary values internal value accomplish 
similarly need augment cutset single boundary value order achieve desired conditional independence 
considerably parsimonious tree model shown 
refer reader discussion examples construction reduced dimension tree models 
note model brownian motion described example cutset model see style construction equally construct model temporal markov chain process 
case dimensions states depend level representation extent time interval interest essentially simple structure minimal cutsets linear graph markov chain 
complex graphs state dimensions vary particular grow size original graph 
example consider regular nearest neighbor graph shown 
case obvious cutset red line nodes center grid conditioned values graphical process cutset sets values remaining halves graph independent 
note case cardinality cutset equals linear dimension grid square grids square root total number nodes implying dimension state linear gaussian model cutset discrete state model cardinality state set corresponding cutset exponentially large 
consequently resulting representations closely related called nested dissection methods scientific computing numerical linear algebra lead inference algorithms complexities scale linearly problem size 
develop detail section possible construct reduced dimension approximate models keep lower dimensional projection vector values node tree neglecting residual dependency remains descendent nodes condition projection 
simple version approach texture discrimination problem described section example 
particular assuming quadrants independent entire set values field red blue boundaries assume independent coarser version values boundaries essentially coarser wavelet approximations sets boundary values 
shown models artifacts tree boundaries reduced somewhat judicious choice lower dimensional projections described section adequate texture discrimination problems results indicate 
junction tree models second viewpoint constructing tree model graphical model involves concept junction tree 
step construction junction tree representation addition edges order triangulate graph resulting referred chordal graph 
particular graph defined property cycle graph passing nodes contains chord edge graph connects nonconsecutive nodes cycle 
example depicted triangulation graph 
second step construction identify set maximal cliques triangulated graph form junction tree 
tree shown 
note node graph note choice somewhat larger cutset red blue lines leads partitioning graph disjoint components turn leads quadtree structure 
obviously find sets cutsets lead general trees unbalanced trees trees differing numbers children node 
maximal clique clique contained proper subset clique 
treatments graphical models junction tree explicitly includes intermediate nodes representing separator sets sets nodes common cliques separator connected 
example lead including node labeled root node node 
including nodes convenient facilitates checking junction tree property identifying explicit form resulting factorization distribution 
nodes unnecessary probabilistically coarse fine dynamics just easily defined model 
corresponds maximal clique graph 
furthermore corresponding vectors values original graphical process populate junction tree form markov model tree 
example straightforward see original graph conditioned vectors mutually independent 
clique tree property required junction tree node original graph appears cliques corresponding nodes junction tree appear cliques node path junction tree nodes 
example node original graph appears root node bottom right node appears intervening node 
reason simple statistical model wish construct tree equivalent statistical model variables original graph provide unambiguous values variables original model 
consequently variables probability take value vectors clique variables involving node 
ensured junction property satisfied dynamics simply copy value root node children include node value copied bottom right node 
notion consistency model closely related idea internal model introduced previously explore depth section note construction cutset model implicitly provides triangulation graph grouping entire cutsets variables single nodes thought cliques augmented graph 
typically different ways triangulate graph finding triangulation leads junction tree small maximal cliques challenging graph theoretic problem 
graphs triangulations maximal cliques quite large 
example triangulation regular graph nontrivial requiring simply adding diagonal connections blocks nodes cycles require triangulation required inclusion dashed edge 
hand graphs construction low dimensional cutset junction tree models straightforward 
example model considered quadtree structure edges connecting children parents 
case set children single parent form elementary cutset resulting model set variables aggregated single node resulting quadtree model 
methods constructing linear models section describe general approaches linear model construction contact examples discussions previous sections introduce important concepts 
perspective pursued section motivated image signal processing applications ultimate objectives performing estimation hypothesis testing quite different objective exact matching statistical model 
applications implied imposed prior model quantity estimated analyzed known approximately represents statistical regularization truth phenomenon interest 
contexts suggest idea approximating replacing prior model model hopefully modest dimension serves desired purposes just terms capturing expected behavior phenomenon better terms admitting efficient scalable algorithms 
line inquiry subject considerable attention section describe lines thought pursued linear models 
note cutset models generally lead smallest maximal clique sizes 
example model samples brownian motion depicted corresponds creating cliques consisting subinterval endpoints midpoints 
case original graph time point connected points immediately junction tree 
modest increase dimensionality cutset model 
cases cutting grid vertical horizontal lines resulting clique sizes order optimal triangulations 
multiresolution variants smoothness priors earliest exploiting models resulted simple observation concerning class image processing algorithms variational formulations termed smoothness priors example smoothness priors introduced section revisited example follow context problem surface reconstruction 
simpler example problem image denoising 
particular suppose observe noise corrupted version underlying image defined image domain approach denoising involves choosing estimate function minimizes functional dr dr term data fidelity term second penalty size gradient penalizing roughness reconstruction 
terms simple statistical interpretation consists measurements corrupted white gaussian noise intensity 
precise statistical interpretation second term requires care take perspective attempting capture intent regularization penalty simple observation replace simple quadtree model 
particular pointed second term thought gaussian fractal prior example functions yields identical values version second term equally implied prior 
alternatively argued penalty term corresponds gaussian process fractal spectrum 
known literature see discussed examples exploited example self similar scaling behavior exhibited fractal processes captured simple models scalar model variance taken decrease geometrically move finer scales 
definitely identical prior model implied smoothness penalty model similar characteristics smoothness prior allows efficient sweep estimation algorithm section compute estimates associated error variances 
contrasted computation optimal estimate corresponding minimizing requires solution elliptic partial differential equation computation optimal estimates essentially inversion elliptic differential operator calculate corresponding error variances 
concept applied somewhat complicated image processing computer vision problems optical flow surface reconstruction example follow provide illustration including showing error statistics computed sweep estimation algorithm localize significant anomalies spatial locations smoothness penalty relaxed edge abrupt change image field imaged 
simple model raise issue encountered previously example revisit detail possibility artifacts estimates produced tree models 
particular consider tree finest scale tree represents actual image field reconstructed wish impose smoothness fractal penalty 
simple model certainly accomplishes spatially variable job 
particular consider finest scale shaded nodes near center image opposite main branches tree 
case tree distance nodes far greater spatial distance result reconstructions model lead noticeable discontinuities major tree boundaries 
argued significance depends application 
significance simple model inadequate desired purpose 
see example case alternatives consider 
relaxing requirement tree allowing edge nodes tree boundaries 
course leads requirement solve estimation problems loopy graphs discussed complex 
return possibility section 
second possibility increase state dimension order capture correlation major boundary accurately 
particular note conditioned root node tree shaded nodes independent 
smoothness dictates pixel values strongly dependent suggests state root node capture significant portion dependence 
basic idea approaches described section 
third possibility example investigations authors see construct models trees spatially offset respect major boundaries coincide combine results estimation trees 
multiple shifted trees approach overcoming problem placing nodes side major tree boundary nodes sides boundaries 
approach aimed objective employing single tree involves called overlapping trees 
standard tree models shown nodes level tree correspond non overlapping portions field represented 
example standard quadtree nodes corresponding square regions field subdivided non overlapping subregions finer scale 
overlapping tree framework region subdivided overlapping subregions scale subregions linear dimensions somewhat larger dimensions parent 
consequences construction 
overlapping tree resolutions standard linear dimensions decrease slowly level level 
consequence total number nodes overlapped tree model larger non overlapped tree model fact increases geometrically number additional scales added implying number scales added degree overlap achieved needs carefully controlled order maintain computational feasibility 
second finest scale pixel image domain corresponds finest scale nodes overlapped tree 
precisely intention construct set fine scale nodes single real pixel guaranteed include elements reside different sides major tree boundaries 
course creates question tree real data desired estimates reside real image space 
described straightforward way lift image domain estimation problem redundant overlapped domain 
construct involves linear operators takes image domain pixel value replicates redundant nodes corresponding pixel 
second operator left inverse collapses values set redundant tree nodes single weighted average mapped corresponding image pixel 
estimator overlapped tree consists application operator lift image domain measurements tree domain followed application tree estimation procedure described section application second operator project tree estimates back image domain 
example 
example successful application overlapping technique surface reconstruction problem introduced section 
full development combines important features 
construction models accommodate smoothness terms thin membrane thin plate penalties simultaneously 
second overcoming need deal explicitly integrability condition mentioned section captured consistency relation 
method deal construct model dimensional state signals possible maintain continuity signal modeled model approach example endpoints intervals included states node tree 
problem complicated motivating development section 
note appears approach real measurement replicating tree nodes 
shown avoided modeling resulting tree measurements having measurement noise covariance multiple covariance real measurement multiple cardinality set tree nodes correspond real image pixel measured 
approximates smoothness penalties consistency condition 
state variables node correspond surface height surface gradient scale spatial location 
addition replace hard constraint softer consisting additional measurement node corresponding requiring difference gradient value node zero mean white noise process tree small variance 
third component model construction overlapping effect modifying geometric decay covariance process noise moves finer resolutions 
shows illustration method surface reconstruction problem 
illustrates effectiveness method reconstruction relatively smooth surface ocean surface height discussed example sophisticated model show known problem smoothness reconstruction methods discontinuities actual field reconstructed smoothness penalty leads blurring smoothing surface discontinuity 
ready availability error variances methods employ statistics identify regions difference raw surface measurements reconstructed surface statistically anomalous 
illustrates result case indicated sign anomalies provide clear indication nature discontinuous jumps surface height example 
fact error statistics allow localize discontinuities suggests possible develop adaptive algorithm performs optimal estimation blurring high contrast edges discontinuities 
method developed considering complex model leads estimates surface height auxiliary field roughly corresponding controls spatially varying smoothness field 
alternate approach developed addresses related problems expectation maximization formalism 
internal models approximate stochastic realization section describe general formal construction linear models 
approach concepts adapted state space theory adaptation trees uncovers important differences temporal case 
contrast usual temporal state space framework matter framework implicitly graph theoretic studies consider problems random process field statistical behavior wish realize corresponds variables fraction nodes tree 
particular focus initially primarily problems process realized covariance power spectrum assumed simplicity assume zero mean resides finest scale tree leaf nodes 
standard time series analysis index set completely ordered interval integers set leaf nodes singleton consisting single point interval 
dyadic trees quadtrees finest scale accommodate entire processes 
issue arises constructing models specifying map process interest finest scale tree specification determines finest scale values common parents common grandparents problem estimating identifying graph structure arise standard time series analysis important problem right trees general graphs 
return problem section section assume structure fixed 
typical example refer occasion shown 
zero mean gaussian process discrete multiscale approximation gradient see formulation represents relaxed version widely studied mumford shah functional image denoising segmentation 
simplicity illustrate concepts dyadic trees examples ideas quadtrees fields 
second order statistics wish realize exactly approximately finest scale tree shown indicated finest scale nodes mapped consecutive integers interval question 
note ordering node coarser scales corresponds interval process node labeled corresponds interval exactly type correspondence seen brownian motion example 
graph structure fixed problem modeling bears number similarities significant differences state space modeling time series 
example refer reader development state space theory deterministic dynamics trees counterparts autoregressive modeling efficient algorithms analogous levinson algorithm time series developed class isotropic processes trees processes covariance variables different nodes depends distance nodes 
section focus problems approximate stochastic realization standard state space realization theory deal basic issues need define state process unspecified coarser scale nodes tree ii define coarse fine dynamics state variables resulting model markov respect tree 
special property trees problem bears resemblance standard temporal problem gaussian processes role state point time decorrelate sets variables values process past values 
referring tree blue dashed lines role state example decorrelate sets variables subtree rooted node subtree rooted large set variables entire tree subtree rooted node note definition state case manner couples definitions distinct nodes decorrelate fact general analysis realization complex temporal counterpart complete treatment remains developed 
considerable amount said borrow concept temporal realization theory counterpart encountered informally occasions internal model 
temporal systems internal model state point time deterministic function process realized 
particular state internal model typically taken function past process modeled capturing memory process required decorrelate past 
internal state introduce additional randomness process realized 
important result standard state space realization theory possible find internal state space realizations minimal smallest state dimension possible considering non internal realizations doesn buy terms model dimensionality 
analogy problem define concept internal model state model node deterministic linear function values process finest scale subtree descending 
example state internal model node linear function values forn 
shown significant implications restricting attention internal models 
course models introduce additional randomness coarser scales coarse scale nodes technically observed really hidden 
second potential price paid excluding non internal models possible find non internal models smaller state dimension 
third importantly internality considerably reduces complexity constructing state variables 
particular shown case states defined resolution resolution sufficient design state node simply looking nodes scale finer 
example state linear function states children andx sufficient choose linear function descendent values remaining nodes finer scale red dotted region 
note state linear functional children maintain consistency paths coarser finer scales 
equally possible define state internal model function past 
see example particular fact consists linear functionals children implies fine dynamics deterministically copy information parent children 
consistency different detail identical purpose consistency requirements junction tree described section 
note performed fine coarse process defining state node immediately determine coarse fine dynamics matrices covariance 
particular best estimate simply error estimate 
computing andq requires joint statistics andx 
states linear functions finest scale process example process second order statistics specified joint statistics andx computed terms statistics finest scale 
return issue constructing states individual nodes 
points problem challenging form 
required state dimensionality achieve complete decorrelation may prohibitively high 
example note state root trees principle completely decorrelate disjoint regions associated root node children task may require high state dimension saw section realization mrf regular nearest neighbor lattices 
large state dimensionality course problem standard state space realization adopt ideas context 
particular may wish construct reduced order models yield realizations finest scale approximate desired statistics process attempting model 
suppose fact follow procedure defined define reduced order states fine coarse manner state linear function children 
case define coarse fine dynamic matrices procedure previously described assured noise uncorrelated simply error estimate 
reduced order states particular may completely decorrelate children generally true values white entire tree 
case standard state space models idea neglect residual correlations model dynamics andq constructed simply assume white 
resulting model yields process statistics finest scale completely match target process 
specifically variance individual samples captured exactly model cross covariances different points time may realized exactly 
remains specify precisely state order reduction carried 
standard state space models developed field particularly time invariant systems state definitions identical points time impact decision define state global measures fidelity computed 
contrast special situations example general modeling self similar gaussian processes states different resolutions models represent different quantities may fact varying dimension 
major domains application methods involves highly nonstationary phenomena 
result standard global measures model accuracy kullback leibler divergence led computationally tractable algorithms model construction methods developed borrowed standard time series contexts focus local criteria defining states individual nodes 
particular fundamental objective defining state decorrelate set variables inside dashed region 
consequently natural consider choosing approximations terms perform decorrelation 
context standard state space systems idea led statistical notion noises 
guaranteed white coarse fine paths necessarily uncorrelated nodes path finest scale root node 
result construct reduced order model neglecting residual correlation guaranteed desired statistics preserved approximate model statistics coarse fine path particular covariances individual finest scale nodes complete statistical description finest scale process 
development computationally feasible methods global measures evaluate accuracy approximate model design models directly optimize measures remains open research topic 
canonical correlations 
roughly speaking canonical correlation analysis takes random vectors identifies ranked ordered pairs linear functionals individual variables unit variance functionals uncorrelated functionals pairs functionals correlation 
represents highly correlated functionals vectors correlated context time series represents past process functionals form rank ordered set natural state variables providing quantitative basis choosing state variables keep reduced order model discussed potential drawbacks metric corresponding canonical correlations rank ordering correlation coefficients linear functionals normalized unit variance 
result components past contribute little total variance may rank higher components contain process variance 
observation suggests alternate criterion referred predictive efficiency opposed canonical correlations treats variables involved asymmetrically random vectors objective produce set scalar linear functionals ct uncorrelated respect rank ordered terms amount variance reduction functionals provides estimation 
ifz represents past time series provides alternative criterion ranking possible state variables time series model 
computation functionals canonical correlations predictive efficiency criteria involves singular value decomposition svd matrix derived covariance matrix sets variables past 
time series typically dimension isn computational advantage approach 
tree models generally case addition important differences time series case 
example consider node 
note case wish design decorrelate random vectors large vector call values 
atthe remaining nodes scale 
complication variables making problem defining mean best variables include complex 
second dimensions vectors quite different asymmetry greatly favors concept predictive efficiency playing role part 
described leads procedure sequentially add variables form including linear functionals value estimating adding functionals additional value account functionals constructed possibly alternately adding functionals andx enhance fidelity state design 
resulting approach constructing model complexity 
complexity may prohibitive applications need performed build model 
hand cases complexity large point wasteful 
example time series problems primary correlation past relative time captured comparatively small interval time 
suggests example dimensional vector order define safely replace dimensional vector values closest nodes ands 
called boundary approximation yields algorithm complexity 
number examples illustrating method 
example applied markov process brownian motion algorithm identify correct linear functionals keep node consist boundary points time interval corresponding node example 
similarly nearest neighbor mrf tree nodes corresponds square region image domain full state required decorrelate region rest image consists values boundary region exactly type construction saw cutset models section 
framework described allows typically done ways fix state dimension say value choose functionals corresponding largest set threshold residual correlation keep functionals needed sum remaining falls threshold 
note quadtree sets variables need decorrelated state node variables children vector tree variables scale children 
consider reducing dimensionality full cutset state keeping functionals process boundary rank highest predictive efficiency estimating rest domain 
illustrates example case approximate modeling fbm process markov possess fractal self similar scaling properties generalize brownian motion 
shown optimal choices linear functionals keep state node approximate self similarity scale 
show approximation errors realized process covariance really important result differences estimation accuracy exact fbm statistics approximate ones captured model statistically especially fbm represents mathematical idealization real processes 
worth noting scale recursive procedure state construction described important extension problems specific functionals finest scale process include state variables coarser scales tree 
simple example consider fine scale process suppose require particular linear functional finest scale linear combination available component state node 
case common ancestor points node 
discussed placing desired linear functional directly node generally advisable conditioning linear functional increase correlation variables children node particular correlation random vectors reduced condition linear functional may fact increase conditioned linear combination involving vectors uncorrelated random variables longer uncorrelated conditioned sum 
result place nonlocal functionals node separate functionals sum required linear functional 
maintain internality need ensure individual linear functionals fact expressed linear functionals andx respectively general specify components states 
general purely algebraic process involves successive examination descendents nodes nonlocal functionals placed 
node internality requires specific functionals available requirement reducing increasing correlation children node typically leads functionals broken separate functionals 
completed process fine coarse construction full state node come design state node may components state pre specified 
case change procedure outlined choice additional linear functionals measure predictive efficiency account information provided prespecified functionals 
example illustrates ability incorporate particular functionals field question states model allows fuse measurements estimate specific coarse scale functionals estimation algorithm described section 
example 
example construction exploitation models specific nonlocal variables included states coarser scale nodes groundwater hydrology problem examined introduced section 
particular problem random field modeled tree represents log hydraulic conductivity field region interest 
measurements represent point finest scale measurements process scattered set fine scale nodes corresponding locations measurements 
discussed section hydraulic head measured location strongly nonlocal nonlinear function log conductivity 
approach taken linearize nonlinear relationship known background log conductivity value region interest resulting linearized model alternate approach reducing approximate cutset models involves reducing dimension state corresponding boundary mrf reducing complexity model state 
briefly discuss idea section 
fractional brownian motion processes characterized called hurst parameter controls rate spectral fall 
particular fbm nonstationary power spectral density defined band frequencies falls hydraulic head location weighted integral log conductivity 
method developed just described model constructed nonlocal functionals included state individual nodes tree 
discussed section real objective application considered estimation travel time particles migrate spatial point 
quantity complex nonlinear nonlocal functional log conductivity alternate methods described estimation 
analogous method incorporate head measurements linearize relationship travel time log conductivity producing model travel time precisely perturbation travel time nominal value computed assumed background conductivity field weighted linear functional log conductivity 
linear functional augmented model manner linearized head measurements resulting estimation algorithm automatically estimates travel time perturbation 
discussed illustrated approach works long perturbations background conductivity large 
case alternate method emphasizes feature formalism 
particular suppose augment model linearized model travel time simply available log conductivity head measurements estimate log conductivity field 
discussed section result estimation process just best estimate log conductivity field model errors estimate 
model perform conditional simulations pointed section known concept geophysics 
particular generate samples error model add best estimate resulting log conductivity field solve hydrology equation turn yields velocity field compute travel time particular log conductivity field 
repeating conditional simulation process times estimate probability distribution travel times 
key tree structure models construct conditional realizations log conductivity field efficiently 
linear models wavelet representations section return topic models wavelets 
mentioned section wavelet synthesis readily viewed dynamic recursion scale needed build models trees wavelets 
consider simple case haar transform suppose wish represent stochastic process finest scale dyadic tree 
case haar transform suggest states coarser scale node corresponding normalized unnormalized haar scaling coefficient referring consider note case dynamics haar wavelet synthesis imply refer reader example models conditional simulation geophysics 
andw independent fact deterministically related 
solution simply assume sa andw sa independent corresponding model case variables model longer correspond scaling coefficients finest scale process deterministic average children 
alternatively wish maintain internality interpretation states components wavelet representation finest scale process detail coefficients included components state node finest scale need individual signal value 
example dimensional state node consists scaling coefficient wavelet coefficient 
note definition state coarse fine dynamics model partially deterministic 
example show components scaling coefficients states children node deterministic functions simple sum simple difference components state node components states nodes new wavelet coefficients step coarse fine synthesis 
haar transform exactly whiten process coarse fine dynamics simply insert white noise value detail coefficient 
structure model allows better haar transform perfectly whiten process 
particular procedure constructing andq matrices section define dynamics take advantage residual correlation wavelet coefficients best job predicting finer scale wavelet coefficient parent scaling wavelet coefficients 
idea example obtain better approximations fbm methods completely neglect residual correlation 
haar case particularly simple obvious connection tree models non overlapping support shifted scaled haar scaling functions wavelets comprise scaled orthogonal basis 
example scaling coefficients involve values fine scale process points subtree corresponding nodes 
consider complex orthogonal biorthogonal wavelets ones additional vanishing moments higher degrees smoothness situation appears complicated 
particular case synthesis signal value requires contributions wavelet scaling functions support includes point 
achieving form coarse fine wavelet synthesis dynamics structure requires required scaling wavelet coefficients part state 
discussed state augmentation done half story condition define state node resulting model internal 
implications lack internality case severe 
particular need augment state node individual scaling detail coefficients appear multiple nodes 
suppose example particular coefficient appears state nodes junction tree constraint precisely counterpart linear models require coefficient appear precisely deterministic linear function state nodes path augmentation done simply sure needed wavelet synthesis particular node included state node satisfy condition 
result violation junction tree constraint multiple replicas supposed single coefficient need typically probability equal 
key see overcome problem recover internality examination fine wavelet analysis dynamics wavelet scaling coefficients successively coarser resolutions constructed linear combinations scaling coefficients previous finer resolution 
overlapping supports wavelets scaling coefficients scale coarser scaling coefficients say located node linear combination number finer scale note exploiting residual correlation order best job possible predicting finer scale wavelet detail coefficients similar spirit discussion section particular model 
key difference model allows entire vector scaling coefficients preceding scale estimate detail coefficient scale 
tree approach described haar transform detail coefficient predicted state single wavelet detail coefficient parent node 
scaling coefficients 
model construct internal required coefficients resident children node accomplishing requires additional state augmentation blush lead believe necessary revisit synthesis side problem order guarantee need node consistent coarse fine dynamics revisit analysis side shown necessary combination preliminary state definition synthesis dynamics followed second augmentation ensure internality leads defined internal linear model state node consists probability vector scaling detail coefficients finest scale process 
surprisingly dimension state resulting model grows linearly support wavelet wavelets biorthogonal case typically degree wavelet transform process fbm 
pure wavelet analysis suggests fairly high order wavelets residual correlation safely neglected 
coarse fine dynamics need neglect residual correlation fact exploit enhance fidelity resulting model 
result high fidelity approximations processes fbm created representations wavelets smaller support typically 
benefit described preceding paragraph interesting wavelets intellectually satisfying compelling case want representation 
reason model consider estimation process sparse irregular multiresolution measurements problems data distributed direct application wavelet analysis possible 
illustrates internal tree model daubechies tap orthogonal wavelet estimate fbm process measurements differing quality ends interval process estimated 
note resulting estimate nearly identical exact fbm statistics deviations tiny fraction standard deviation errors optimal estimates 
covariance extensions maximum entropy models section turn important topic statistical signal processing strong ties models 
problem covariance extension 
specifically applications unreasonable expect provided complete covariance random process field data available complete specification estimated 
example dealing large scale remote sensing problems random fields interest dimensionality millions making availability storage full covariance matrix prohibitive 
problems case comparatively small part covariance matrix specified seek model consistent partial specification 
idea modeling partial specifications known signal processing field 
particular consider construction stochastic model stationary time series matches partially specified correlation function 
specifically need match values correlation function 
partial specification valid correspond values completely specified correlation function possible find models match partial specification maximum entropy extension corresponds autoregressive ar signal model coefficients efficiently calculated specified portion correlation function example celebrated levinson recursions see example 
typically emphasized resulting ar model efficient recursive computation covariance values specified originally 
interestingly maximum entropy extension important implications modeling 
particular resulting ar model kth order markov process number covariance values originally specified 
result construction analogous brownian motion example principle construct model process analogous shown number boundary points kept subinterval 
construction requires knowledge appears considerable number covariance values originally specified 
example referring order markov process specifying coarse fine dynamics requires knowledge covariance points mid points near 
principle computed ar model mentioned preceding paragraph recursive method calculates elements covariance sequence needed construct model 
raises question needed elements computed directly efficiently turn leads important ties graph theory 
particular suppose partially specified covariance matrix certain elements including diagonal specified 
furthermore valid completely filled principal submatrix submatrix consisting choices rows columns elements specified positive definite 
extension corresponds filling unspecified values maintaining validity completion extension element specified yielding full positive definite covariance matrix 
important questions partially specified covariance matrix extensions completions maximum entropy extension 
answers questions important graph theoretic interpretations 
particular matrix consider undirected graph nodes labeled include edge distinct nodes element specified 
results hold particular graph type extensions completions exists valid partially specified covariance graph structure graph chordal 
completion exists partially specified covariance maximum entropy extension markov respect graph determined typical example holds consecutive set diagonal bands specified simply generalization usual ar modeling framework allow time series nonstationary diagonals band need constant values 
case resulting maximum entropy model said kth order markov identical process markov respect graph determined suppose graph ofp chordal 
question calculating particular elements precisely possible recursive orders elements calculated graph theoretic interpretation shown 
specifically pe extension pe agrees defined ge graph corresponding pe ge 
ge chordal calculate additional elements pe having calculate elements pe 
accomplished recursively constructing chordal sequence ge step sequence corresponds adding single edge preceding graph 
sequence provides recursive ordering computation required elements pe 
shown step recursion defines range values new element extension providing essence complete characterization possible extensions reflection coefficients standard time series models 
particular shown required computations steps sequence involves submatrix corresponding new maximal clique formed addition new edge 
submatrix single new element computed 
choosing value submatrix positive definite valid choosing particular value maximizes determinant submatrix corresponds maximum entropy extension 
cases maximal clique size grow recursion progresses apparently implying required graph chordal existence extensions completions depend specific numerical values specified elements computations grow 
refer reader additional set graph theoretic conditions chordal sequence guarantee essence computations required stage recursion performed previous stages 
result provides nontrivial extension levinson recursions 
examine specific extension required form tree model partial covariance consisting diagonal bands side main diagonal 
case described previously additional elements maximum entropy extension computed unusually distributed fact fractal pattern see 
extremely sparse subset elements having elements 
surprisingly graph corresponding extension chordal 
corresponding chordal sequence constructed compute needed elements find resulting sequence new maximal cliques remains bounded size total computational load construct resulting model 
refer reader details 
estimation model parameters learning models section take brief look problem estimating learning models data 
separate classes problems describe detailed sections 
estimation model parameters class problems involves estimation parameters models fixed known structure discussed section computation likelihood functions performed efficiently models implying example computations basis maximum likelihood parameter estimation 
examples linear models estimation hurst parameter fbm estimation noise correlation structure parameters models remote sensing 
discrete hybrid models segmentation model hidden markov tree models illustrated example developed detail effective approach parameter estimation involves em algorithm 
employment em requires specification called complete data includes actual measured data additional hidden variables available computation parameter estimates easier 
example described hidden markov tree models clear choice complete data consists observed wavelet coefficients hidden discrete states node discussion denote vectors wavelet coefficients discrete state variables respectively 
denote vector parameters estimated consisting probabilities defining discrete state hidden markov model means variances wavelet coefficient conditioned possible values corresponding discrete state 
expectation step consists computing conditional expectation log likelihood function log conditioned previous iteration estimate 
corresponds averaging possible values conditioning information 
maximization step involves maximizing function computed step order compute iteration estimate 
note vector consisting choices discrete states node tree number possible values exponential number nodes tree 
discussed section computation expectations general graphical models prohibitively complex trees computations performed extremely efficiently 
result implementation em algorithms parameter estimation models trees computationally attractive 
refer reader discussion examples 
cases quantities estimated referred hyperparameters typically small number unknowns actual parameters model elements matrices andq linear models typically nonlinear functions unknowns 
learning models alternative parametric estimation methods discussed preceding section nonparametric machine learning philosophy building models 
briefly describe lines investigation wavelet transforms populate variables model models learned training data 
described involves construction nonlinear coarse fine statistical models wavelet transforms models estimate wavelet coefficient allowed nonlinear function window nearby scaling coefficients 
specifically wavelet coefficients scale modeled independent conditioned local window scaling coefficients conditional distribution coefficient assumed gaussian 
mean variance conditional distribution modeled piecewise affine constant function window scaling coefficients 
piece linear parametric model 
determining linear pieces specifying region linear function applied require nonparametric estimation techniques 
refer reader details application models problems tomographic reconstruction 
note wavelet coefficient depends scaling coefficients question arises resulting model forms tree precisely state augmentation methods described section applied transform model tree model 
cases certainly case 
result complex graph yield junction tree cutset tree model acceptably small state dimension 
interesting approach modeling image processing developed 
basic idea approach quite simple sample image form pyramid performing decomposition image specific decomposition overcomplete steerable pyramid 
node quadtree vector coefficients denoted sensitive variations different directions location scale corresponding node image sample small set images wish learn non gaussian nonlinear coarse fine statistical dynamics 
particular done nonparametric density estimation methods estimate distribution vector values root node conditional distributions node direct ancestors 
node root node estimate density way done assume interesting variation stationarity node scale conditional distribution nodes scale 
implies single image learn distribution significant number samples estimate densities finer scales nodes resulting learned densities coarser scales certain 
note distributions correspond markov model tree case conditioning conditioning distant ancestors informational value 
just time series analysis simple turn higher order markov description order representation state augmentation defining state node consist vector values 
ancestors node 
hybrid nonlinear wavelet methods motivation modeling methodology capture non gaussian nature wavelet statistics cascade behavior characteristic wavelet decompositions natural images 
illustrates example suggesting promise approach modeling natural imagery 
learning tree structure discussions entire section focused aspects modeling problem identifying learning structure tree focused identifying variables place particular nodes prespecified tree problem determining parameters model variables specified 
strong argument reasonable signal image processing applications nodes variables nodes rough intuition associated related representation phenomena different scales spatial locations 
worth noting topic identifying structure tree received attention fields signal image processing 
best known area chow liu 
idea index set set random variables number independent realizations set variables 
assume joint distribution variables tree distribution form graphical model respect tree node set know trees index set correct distribution variables 
objective available measurements determine maximum likelihood estimate tree distribution respect tree 
specific choice tree structure ml estimates distribution tree simply empirical factored distribution observed data central problem reduces identifying best choice tree structure 
shown problem solved efficiently 
special nature trees reinforced observation solution problem allow graphs trees difficult fact np hard 
important advance area reported focuses chordal graphs bounded treewidth maximal clique sizes bounded 
limited set graphs optimal identification prohibitively complex results show possible develop computationally feasible algorithms ranks respect maximizing likelihood provably bounded relative optimal 
significance results modeling developed additional motivation considering graphs sense close trees section 
moving trees preceding sections clear models trees attractive properties lead powerful efficient signal image processing algorithms extensive domains application 
fact models markov trees graphs loops leads power algorithms apparent limitations applicability 
particular discussed section case process mrf graphical process loopy graph modeled exactly approximately model tree cases resulting dimensionality cardinality state tree model large 
complexity algorithms described grows polynomially state dimension cardinality alternatives reduce dimension cardinality state develop alternative algorithms approximations reduce complexity allow keep higher dimensional higher cardinality states consider models graphs loops 
preceding sections described variety approaches alternatives 
cases smoothness models described section reduction state size accomplished replacing model serves essentially purpose 
approximate stochastic realization methods section method reduce state dimensionality keeping limited dimensional projection full state node chosen minimize residual correlation variables full state completely decorrelated 
alternatively discussed section method overlapping trees overcome severe burden especially coarser scales placed state tree model providing complete conditional mutual independence sets variables separate subtrees descendent node 
indicated prices overlapping tree total number tree nodes increases factor dyadic tree factor quadtree additional scale 
amount overlap accommodated maintaining computational efficiency resulting inference algorithms limits 
result considerable motivation consider alternatives mentioned previously 
section take brief look approach keeps state size large reduces computational burden resulting inference algorithms context contact important field space time processes algorithms counterparts dynamic bayes nets dbns graphical model literature 
section take brief look introduced methods developed deal estimation graphs loops 
methods originally motivated inference problems pyramidal structures applied arbitrary graphical models independent interest graphical model community 
reduced complexity cutset models time recursive approximate modeling section return cutset models discussed section ideas describe applied general graphs nonlinear models illustrative purposes frame discussion terms linear gaussian nearest neighbor mrf regular lattice 
discussed section exact model formed state root node full set variables red row blue column grid leading dyadic tree structure example alternatively bisect regions horizontally vertically take root node state set variables red row blue column leading quadtree model 
case dimension state root node number nodes grid 
note dimensions nodes successively finer scales half dimension parents complexity inference algorithms bad state dimension case large state dimension small number nodes leads problems procedures estimation algorithm described section 
particular dynamic model estimation algorithm written form requires explicit representation computation storage various estimates error covariances state node tree see general covariance matrices full 
key method described section alternative form estimation equations known information filter quantities stored computed directly information matrices inverses covariances information states covariances estimates pairs appearing algorithm known time series information filter algorithm form analogous information quantities recursively computed 
glance approach bought general guarantee computations involved alternate form demanding addition additional computations perform recover pointed section inverse covariance matrix interpretation specifying model random vector process key alternate form approximation cutset models considered section 
particular consider set variables corresponding values order gaussian mrf center red row 
think set values signal 
inverse entire covariance full process sparse graphical structure model inverse covariance matrix center row generally full implying graphical model associated signal fully connected 
hand discussed illustrated cases particular order mrf information matrices nearly banded dominant nonzero values relatively narrow diagonal band main diagonal matrix 
consequently approximate matrices setting zero values deemed small obtain particular discussed relatively easy local computations compute known partial approximation inverse covariance signal banded 
corresponds pruning edges fully connected graphical model produce approximate markov model order equal width nonzero diagonal band 
suggests structure modeling algorithm corresponding estimation algorithm recursively compute banded approximations information matrices information states cutset states tree model lattice 
approximate information matrices small numbers nonzero elements computations involved step simpler linear worst quadratic state dimension 
result approximate algorithm total complexity worst 
recovery node straightforward fact corresponds precisely kalman filter rauch tung smoother banded structure approximation refer reader examples details including examples applied regular graphs 
addition principles applied discrete state nonlinear graphical models criterion pruning edges obviously take form examination elements inverse covariance matrix see discussion dbns 
method just described close relationships known methods numerical solution partial differential equations algorithms ideas space time processes dbns 
particular described see suppose middle red row top row leftmost column march downward row row left right column column case propagate approximate version inverse covariance approximate markov model values field successive row column 
note approach effectively treats spatial dimensions time variable row row column column recursion turn provides direct connection space time processes processes grids independent variables time 
idea propagating approximate graphical models time topic significant current interest refer readers details 
note particular authors confront problem considerable concern dbns approximate modeling methods described previous sections issue approximation errors propagate accumulate time 
particular authors obtain results show long temporal dynamics process interest sufficient mixing kullback leibler divergence exact approximate models decreases temporal propagation implies comparable approximation errors time step accumulation errors time measured divergence remains bounded 
developing comparable results possibly stronger ones linear case tree models remains open topic resolution provide way relate local approximations node tree impact global model accuracy 
refer reader related research space time estimation spatial phenomenon objective propagate mrf model higher order mrf grid approaches direct temporal propagation tree model space time process 
important issue begins address temporal mixing spatial scales features scale point time interact temporal dynamics produce features different scales time evolves 
characteristic implies statistical relationships parent child node point time depend relationships previous time nodes may different scales 
resulting structure consists temporal sequence models trees directed edges nodes tree time nodes tree time order capture temporal dynamics mixing 
correlation coefficient corresponding particular edge conditional correlation nodes connected edge conditioned rest variables entire graph 
removing edges corresponding small partial correlations approach taken 
tree algorithms graphs loops section describe approaches relax requirement model live loop free graph reducing decorrelation burden dimension coarse scale nodes allowing additional paths finer scale nodes 
example models trees consider models pyramidal structures edges pairs nodes various scales essence providing short circuit nodes far apart measured distance tree 
examples graphs loops ones node connected parent nodes see mentioned previously 
graph confront problem inference discussed section challenging problem subject considerable current interest 
graphical model literature contain important results behavior belief propagation algorithms including results linear gaussian models new classes iterative algorithms :10.1.1.20.127
intention describe review vast active area research 
refer interested reader just graphical models cited previously limit brief descriptions investigations directly motivated models tree algorithms example 
example 
example return problem image segmentation introduced section particular approach introduced subsequently employed variety contexts see example 
simplicity describe idea context developed employs discrete state potts model introduced example describe coarse fine dynamics hidden markov tree representing segmentation labels sequence resolutions 
actual observed data consists data finest scale measurements assumed conditionally independent measurement finest scale node conditioned value hidden discrete label node form conditional distribution taken gaussian gaussian mixture examples generalized gaussian 
model described previous paragraph essentially identical structure described see example significant additional components complete approach developed distinguish lead inference algorithms different structure 
map mpm criterion optimal estimation discrete label set authors suggest alternative measure aimed part overcoming problematic map estimation segmentation exploiting structure models 
particular argument errors coarser scales geometrically expensive correspond misclassifications geometrically larger regions authors derive criterion puts exponentially larger weights errors occur coarser scales see precise formulation 
result criterion precise optimization complicated 
authors demonstrate approximation criterion optimized scale results simple intuitively appealing structure fine coarse sweep sweep algorithms described previously performed compute node conditional log likelihood data subtree node conditioned discrete state value node 
denote discrete state node ys denote data finest scale nodes descendent coarse fine sweep computes log ys node value take 
root node compute optimal estimate arg max approximate estimation algorithm proceeds coarse fine manner stage essentially assume estimate parent node correct 
approximate coarse fine recursion arg max log pointed pointed tree model lead artifacts leads alternative tree potts model example directed graph model coarse fine scales discrete state node depends values nodes preceding scale parent node neighbors scale simplicity denote set nodes 
model straightforward variation state influenced parents see details 
result adopting non tree model computation likelihood fact entire structure computation optimal estimates complex loopy structure graph corresponding parent model 
principle maximization performed node node scale nodes scale need considered simultaneously combinatorially explosive requirement finer scales 
inference problem complex graph loops approximation iterative scheme needed 
approach taken define non iterative pass algorithm motivated perspective processing decidedly different approaches general graphical model literature 
particular assumes tree potts model example parent non tree model available discrete state process 
tree model fine coarse sweep computing likelihoods 
optimal root node estimate computed estimates nodes computed node node coarse fine sweep recursion place arg max log comparing see difference conditioning set parents simply transition distribution 
discussed scale algorithm resembles equal optimal estimate parent potts model coarser scales tree potts model finer scales 
shows result applying algorithm multispectral image data pixel consists vector multispectral measurements image segmented regions pixels modeled multivariate gaussian mixtures 
shows corresponding results segmentation document page regions text picture background 
algorithm second example developed uses structure tree fine coarse likelihood computation non tree coarse fine estimation sophisticated involved data model structures 
refer reader details 
briefly describe research motivated loopy graphs efficient algorithms described section inference trees 
specifically consider estimation graphical model connected loopy graph noisy measurements variables nodes graph 
basic idea algorithms carry algorithm involves haar wavelets transform raw measurements detail coefficients affine class dependent model scale scale dynamics coefficients directed graph model multiscale class labels transition probabilities general complex 
aspects model learned training small set sample images 
estimation process inference tree models basic engine 
particular suppose identify trees st graph trees connects nodes set edges subset edge set ofg 
example depict spanning trees graph 
general structure algorithms class involve iterative application tree inference statistical structure implied trees individually 
specifically zero mean gaussian graphical process covariance matrix px suppose linear measurements covariance measurement noise block diagonal 
discussed section optimal estimate xs solution corresponding error covariance pe 
discussed previously nonzero element diagonal block tree allow apply fast estimation algorithm section calculate xs diagonal blocks pe 
tree spanning trees st write nonzero diagonal blocks ki correspond edges spanning tree si nonzero elements blocks corresponding edges eliminated order form si possibly diagonal blocks corresponding nodes involved edges eliminated 
result ki rank proportional number edges removed rewrite ct xs ki xs suggests iterative algorithm form 
denote sequence designates spanning trees nth iteration chosen cycle periodically trees chosen randomly xn denote approximation optimal estimate nth iteration solution equation ct xn ki xn matrix left hand side tree structure corresponding th tree equation solved efficiently estimation algorithm section 
examples demonstrating embedded tree algorithms lead efficient methods computing optimal estimates :10.1.1.20.127
belief propagation algorithm converges optimal estimate 
bp yield correct error covariances shown computations performed algorithm compute sequence approximations converge correct error covariances :10.1.1.20.127
furthermore experimental evidence reported indicates algorithms converge broader class processes general local message passing version bp 
roughly speaking due fact algorithm takes advantage global structure process captured spanning trees 
refer reader examples theoretical analysis including conditions guaranteeing convergence tree algorithm 
refer reader investigation set algorithms discrete state processes embedded trees closer belief propagation 
roughly speaking idea take advantage interpretation discussed section exact computations conditional probabilities tree corresponds probability distribution graphical model essentially terms distribution root node parent child transition distributions 
tree reparameterization algorithm developed corresponds iterative entire distribution loopy graph step factorization involves edges corresponding spanning trees algorithm 
refer edge eliminated may nonzero 
reader theoretical analysis algorithm examples show promise inference loopy graphs 
methods introduced results obtained far explicit global local graph structure suggests may result investigation years 
described framework modeling processing signals images 
seen framework markov models structured multiresolution trees admits efficient processing algorithms rich capture broad classes statistical phenomena 
result methods application variety different contexts 
formalism methodology deep interest intellectually contact variety topics including wavelets graphical models hidden markov models multigrid coarse fine algorithms inverse problems data fusion state space system theory stochastic realization theory maximum entropy modeling covariance extensions 
believe theory methodology described value researchers practitioners different fields 
addition believe area remains fertile ground basic research 
example methods developed successfully applied problems known anecdotally problems applied done deepen understanding problems methods appropriate limits applicability 
particular signal image processing problems texture discrimination groundwater hydrology examples discussed underlying phenomenon may extremely complex available data inference objectives simpler lower dimensional 
suggests examples support idea inference problems may acceptable relatively simple crude approximations yield near optimal performance specific inference problems interest 
formalizing idea developing general methods constructing models suited particular processing tasks remain accomplished 
theoretical topics remain explored 
development data driven algorithms model construction analogous ones described explicit knowledge covariance structure process modeled 
motivated fact standard temporal modeling methods ar arma models versions directly covariance specifications versions recursively data 
expect data driven algorithms considerable computational advantages developed refer reader initial efforts direction addition variety reasons considerable interest develop methods modeling graphs cycles 
reasons developed set algorithms loopy graphs described section show power tree algorithms graphical models loopy graphs 
build models 
variations stochastic realization covariance extension methods described section loopy graphs 
example covariance extension results described section assume known covariance elements form chordal graph variables modeled 
applications especially remote sensing case 
particular applications knowledge correlations fine scale variables temperature variations ocean close proximity spatially correlations coarser spatial averages variables longer distances forming non chordal graph known covariance values 
problem local fine scale distant coarse scale statistical characterizations importance note tree reparameterization algorithm developed fundamentally different embedded tree algorithm specialization linear gaussian models yield embedded tree algorithm 
shown belief propagation viewed special variant tree simple embedded spanning node trees graph step iteration 
reminiscent structure exploited multipole algorithms solution partial differential equations 
refer reader attempt adapt multipole ideas estimation results modeling covariance extension graphs cycles 
directions research virtually corner 
investigation called non internal realizations topic offers possibility additional flexibility constrain internal models see initial results lines 
investigation methods space time problems time treated multiresolution fashion called multirate kalman filtering estimation theory methods discussed section time treated sequential variable space treated graphical manner 
results literature represent start important area extends modeling investigation dynamic bayesian networks 
discussion results summarized preceding sections illustrate multiresolution statistical modeling inference remains fertile active important area investigation 
author hope help stimulate methods exist inquiry extensions enhance understanding methods range problems successfully applied 
am indebted individuals contents 
foremost friends valued colleagues albert benveniste mich le basseville hosting sabbatical suggesting time see sense multiresolution methods particular wavelets statistical perspective 
suggestion continuing rewarding collaboration albert mich le certainly exist 
acknowledge students colleagues worked interacted research area ken chou bernard levy bernhard claus stuart golden mark karl bob eric fabre bill irving paul carl wunsch dimitris hamid krim st phane mallat tom allen eric miller bhatia les novak charlie ron taylor andrew kim mike chin patrick perez fabrice heitz patrick bouthemy mike daniel dennis mclaughlin dave rossi bob mike schneider terrence ho austin lev ari andy tsai jun zhang rachel learned john fisher eero simoncelli dewey tucker martin wainwright erik sudderth tommi jaakkola jason johnson 
individuals allowing figures jointly authored papers 
am indebted charlie bouman rich baraniuk early discussions rich baraniuk eric kolaczyk feedback earlier draft charlie rich eric justin romberg choi nicholas kingsbury michael shapiro hui cheng rob nowak paul viola jeremy willingness figures research 
addition owe debt gratitude mike jordan scholarship intellectual enthusiasm helped broaden deepen perspective rich research area 
kelly efforts helping put anonymous reviewers careful reading valuable suggestions impressive stamina 
researchers authors worked working area owe profound research field interesting rich apology authors cited undoubtedly imperfect authors inadvertently omitted due ignorance 
abend hartley kanal 
classification binary patterns 
ieee trans 
information theory 
abramovich silverman 
wavelet thresholding bayesian approach 
journal royal statistical society 
abramovich silverman 
wavelet decomposition approaches statistical inverse problems 
biometrika 
adams willsky levy 
linear estimation boundary value stochastic processes part role construction complementary models 
ieee trans 
automatic control september 
adams willsky levy 
linear estimation boundary value stochastic processes part ii smoothing problems 
ieee trans 
automatic control september 
adelson burt 
image data compression laplacian pyramid 
proc 
patt 
recog 
info 
proc 
conf pages 
aji mceliece 
generalized distributive law 
ieee trans 
information theory march 
akaike 
stochastic theory minimal realizations 
ieee trans 
automatic control december 
akaike 
markovian representation stochastic processes canonical variables 
siam control january 
allen willsky 
multiscale approaches moving target detection image sequences 
optical engineering july 
ambrosio 
approximation free discontinuity problems 

anderson moore 
optimal filtering 
prentice hall upper saddle river nj 
andrews hunt 
digital image restoration 
prentice hall englewood cliffs nj 

image segmentation pyramids 
comput 
vision graphics image process 

random cascades wavelet dyadic trees 
math 
phys aug 
arun kung 
balanced approximations stochastic systems 
siam matrix anal 
appl january 

stochastic realization approach smoothing problem 
ieee trans 
automatic control december 
bakshi zhong jiang 
analysis flow gas liquid bubble columns multiresolution methods 
chem 
eng 
res 
des aug 
katsaggelos 
spatially adaptive wavelet multiscale image restoration 
ieee trans 
image processing april 
bar joseph el yaniv lischinski werman 
texture mixing texture movie synthesis statistical learning 
ieee trans 
visualization comp 
graphics april june 
barrett johnson 
formulae matrix completion associated chordal graphs 
linear alg 
appl 
barrett johnson 
critical graphs positive definite completion problem 
siam matrix anal 
appl 
basseville benveniste chou golden willsky 
modeling estimation multiresolution stochastic processes 
ieee trans 
information theory march 
basseville benveniste willsky 
multiscale autoregressive processes part schur levinson parametrizations 
ieee trans 
signal processing august 
basseville benveniste willsky 
multiscale autoregressive processes part ii lattice structures whitening modeling 
ieee trans 
signal processing august 
baxter 
exactly solved models statistical mechanics 
academic press new york 
bello willsky levy 
construction applications discrete time smoothing error models 
int control july 
bello willsky levy casta 
smoothing error dynamics solution smoothing mapping problems 
ieee trans 
information theory july 
benveniste willsky 
multiscale system theory 
ieee trans 
circuits systems fundamental theory applications january 
beran 
statistics long memory processes 
chapman hall new york 
berliner cressie 
long lead prediction pacific bayesian dynamic modeling 
journal climate appear 

dynamic programming 
academic press new york 

linear inverse ill posed problems 
advances electronics electron physics 
poggio torre 
ill posed problems early vision 
proc 
ieee 
besag 
spatial interaction statistical analysis lattice systems 
journal royal statistical society 
besag 
statistical analysis dirty pictures 
journal royal statistical society 
beylkin coifman rokhlin 
fast wavelet transforms numerical algorithms comm 
pure appl 
math march 
bhatia karl willsky 
wavelet method multiscale tomographic reconstruction 
ieee trans 
medical imaging 
bhatia karl willsky 
tomographic reconstruction estimation multiscale natural pixel bases 
ieee trans 
image processing march 
bouman liu 
multiple resolution segmentation textured images 
ieee trans 
pattern analysis machine intelligence february 
bouman sauer 
generalized gaussian image model edge preserving map estimation 
ieee trans 
image processing july 
bouman shapiro 
multiscale random field model bayesian image segmentation 
ieee trans 
image processing march 
boyen koller 
tractable inference complex stochastic processes 
proc 
th ann 
conf 
uncertainty ai pages madison wi july 
brandt 
multi level adaptive solutions boundary value problems 
math 
comp 
briggs 
multigrid tutorial 
society industrial applied mathematics philadelphia 
simoncelli 
image compression joint statistical characterization wavelet domain 
ieee trans 
image processing december 
burt adelson 
laplacian pyramid compact image code 
ieee trans 
communications com april 
burt hong rosenfeld 
segmentation estimation image region properties cooperative hierarchical computation 
ieee trans 
systems man cybernetics smc december 
chang yu vetterli 
spatially adaptive wavelet thresholding context modeling image denoising 
proc 
icip pages 
chellappa chatterjee 
classification textures gaussian markov random fields 
ieee trans 
acoustic speech signal processing assp august 

chen 
lin li 
chen 
optimal signal reconstruction noisy filter bank systems multirate kalman synthesis filtering approach 
ieee trans 
signal processing november 
chen donoho 
atomic decomposition basis pursuit 
spie international conference wavelets san diego july 
cheng bouman 
multiscale bayesian segmentation trainable context model 
ieee trans 
image processing april 
chin karl willsky 
sequential filtering multi frame visual reconstruction 
signal processing aug 
chin karl willsky 
probabilistic sequential computation optical flow temporal coherence 
ieee trans 
image processing november 
chin mariano 
space time interpolation oceanic fronts 
ieee trans 
geoscience remote sensing 
chipman kolaczyk mcculloch 
adaptive bayesian wavelet shrinkage 
journal american statistical association 
choi baraniuk 
multiscale image segmentation wavelet domain hidden markov models 
ieee trans 
image processing sept 
choi romberg baraniuk kingsbury 
multiscale classification complex wavelets hidden markov tree models 
ieee int conf 
image processing october 
vancouver canada 
chou willsky benveniste 
multiscale recursive estimation data fusion regularization 
ieee trans 
automatic control march 
chou willsky 
multiscale systems kalman filters riccati equations 
ieee trans 
automatic control march 
chou golden willsky 
multiresolution stochastic models data fusion wavelet transforms 
signal processing december 
chou willsky 
multiresolution probabilistic approach inverse conductivity problems 
signal processing 
chow liu 
approximating discrete probability distributions dependence trees 
ieee trans 
image processing may 
claus 
multiscale statistical signal processing identification multiscale ar process sample ordinary signal 
ieee trans 
signal processing december 
claus 
multiscale signal processing isotropic random fields homogeneous trees 
ieee trans 
circuits systems analog digital signal proc august 
wilson 
squares image estimation multiresolution pyramid 
proc 
icassp 
clyde vidakovic 
multiple shrinkage subset selection wavelets 
biometrika 
cohen 
analyse solution des 
acad sci paris 
cohen cooper 
simple parallel hierarchical relaxation algorithms segmenting noncausal markovian random fields 
ieee trans 
patt 
anal 
mach 
intell mar 
cohen fan patel 
classification rotated scaled textured images gaussian markov random field models 
ieee trans 
pattern analysis machine intelligence february 
coifman donoho 
translation invariant denoising 
lecture notes statistics wavelets statistics pages 
springer verlag new york 
coifman wickerhauser 
entropy algorithms best basis selection 
ieee trans 
information theory march 
comer delp 
segmentation textured images multiresolution gaussian autoregressive model 
ieee trans 
image processing march 

schur parameters factorization dilation problems 
birkhauser berlin 
cooper 
computational complexity probabilistic inference bayesian belief networks 
artificial intel 
costantini 
fusion different resolution sar images 
proc 
ieee 
cressie davidson 
image analysis partially ordered markov models 
computational statistics data analysis 

multirate multiresolution recursive kalman filter 
signal processing sept 
crouse nowak baraniuk 
wavelet statistical signal processing hidden markov models 
ieee trans 
signal processing april 
daniel willsky 
efficient implementations dimensional noncausal iir filters 
ieee trans 
circuits systems ii july 
daniel willsky 
multiresolution methodology signal level fusion data assimilation applications remote sensing 
proc 
ieee january 
daniel willsky 
modeling estimation statistically self similar processes multiresolution framework 
ieee trans 
information theory april 
daniel willsky mclaughlin 
multiscale approach estimating travel time distributions 
advances water resources 
willsky 
multiscale autoregressive models wavelets 
ieee trans 
information theory april 
daubechies 
lectures wavelets 
siam press philadelphia 
daubechies mallat willsky 
special issue wavelet transforms multiresolution signal analysis 
ieee trans 
information theory mar 
davidson cressie hua 
texture synthesis pattern recognition partially ordered markov models 
pattern recognition 
dawid 
applications general propagation algorithm probabilistic expert systems 
stat 
comput 

multiresolution sampling procedures analysis synthesis texture images 
sig graph pages 
viola 
nonparametric multiscale statistical model natural images 
neural information processing systems december 
viola 
texture recognition nonparametric multiscale statistical model 
proc 
ieee computer soc 
conf 
computer vision pattern recognition 

simulating mapping spatial complexity multiscale techniques 
int 

inf 
syst sept oct 
roux 
wavelet method multifractal image analysis 
ii 
applications synthetic multifractal rough surfaces 
european physical journal june 

multiresolution tomographic reconstruction wavelets 
ieee trans 
image processing june 

optimal filter banks signal reconstruction noisy subband components 
ieee trans 
signal processing feb 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
derin elliott geman 
bayes smoothing algorithms segmentation binary images modeled markov random fields 
ieee trans 
pattern analysis machine intelligence 
pr 
estimating gaussian markov random field parameters nonstationary framework application remote sensing imaging 
ieee trans 
image processing april 

wavelet representations stochastic processes multiresolution stochastic models 
ieee trans 
signal processing july 
bagchi 
reciprocal processes tree modeling estimation issues 
ieee trans 
automatic control february 

correlation structure wavelet coefficients fractional brownian motion 
ieee trans 
information theory 
dobson 
convergence reconstruction method inverse conductivity problem 
siam appl 
math 
donoho johnstone 
adapting unknown smoothness wavelet shrinkage 
amer 
stat 
assoc 
donoho 
nonlinear solution linear inverse problems wavelet decomposition 
appl 
comput 
harmonic anal 
duff reid 
direct methods sparse matrices 
oxford univ press oxford england 
miller 
statistical signal restoration wavelet domain prior models 
sig 
processing november 
edwards 
graphical modeling 
springer new york 

investigations multigrid algorithms estimation optical flow fields 
comput 
vision graphics image process 
fabre 
new fast smoothers multiscale systems 
ieee trans 
signal processing august 
karl willsky 
efficient multiresolution counterparts variational methods surface reconstruction 
computer vision image understanding may 
karl willsky wunsch 
multiresolution optimal interpolation statistical analysis poseidon satellite 
ieee trans 
geoscience remote sensing march 
ho willsky 
mapping mediterranean altimeter data multiresolution optimal interpolation algorithms 
atmospheric oceanic technology april 
willsky 
fractal estimation models multiscale 
ieee trans 
signal processing may 

multipole motivated reduced state estimation 
icip 
flandrin 
spectrum fractional brownian motion 
ieee trans 
information theory january 
flandrin 
wavelet analysis synthesis fractional brownian motion 
ieee trans 
information theory march 
forney 
viterbi algorithm 
proceedings ieee 
krim irving karl willsky 
multiscale segmentation anomaly enhancement sar imagery 
ieee trans 
image processing january 
karl willsky 
multiscale hypothesis testing approach anomaly detection localization tomographic data 
ieee trans 
image proc june 
lev ari willsky 
generalized levinson algorithm covariance extension applications multiscale autoregressive modeling 
ieee trans 
information theory accepted publication 
willsky 
scale recursive method constructing stochastic models 
multidimensional signal processing 
freeman weiss 
optimality solutions max product belief propagation algorithm arbitrary graphs 
ieee trans 
inf 
theory 
bouman hutchins sauer 
bayesian multiresolution algorithm pet reconstruction 
proc 
ieee int conf 
image proc volume pages vancouver bc canada september 
bouman sauer 
multiscale bayesian methods discrete tomography 
gabor herman editors discrete tomography foundations algorithms applications pages 
birkhauser boston cambridge ma 
bouman sauer 
multiscale models bayesian inverse problems 
spie conference wavelet applications signal image processing vii volume pages denver colorado july 
bouman sauer 
adaptive wavelet graph model bayesian tomographic reconstruction 
ieee trans 
image processing review 
frey 
graphical models machine learning digital communication 
mit press cambridge ma 

self similarity brownian motion large deviation principle random fields binary tree 
prob 
theory related fields january 
wunsch 
estimates altimeter data waves northwestern atlantic ocean 
physical oceanography 
geiger 
scaling images image features renormalization group 
proc 
ieee cvpr june 
geman geman 
stochastic relaxation gibbs distributions bayesian restoration images 
ieee trans 
pattern analysis machine intelligence pami november 
george 
nested dissection regular finite element mesh 
siam numer 
anal april 
ghahramani jordan 
factorial hidden markov models 
machine learning 

renormalization group approach image processing problems 
ieee trans 
pattern analysis machine intelligence february 
greiner 
wavelet cascades 
physica december 
golden 
identifying multiscale statistical models wavelet transform 
master thesis mit june 
golub van loan 
matrix computations 
johns hopkins university press baltimore 
mclaughlin 
scale recursive estimation precipitation site 
adv 
water resources review 
heitz perez pr 
hierarchical markov random field models applied image analysis review 
spie conf 
neural morphological stochastic methods image signal processing volume pages san diego ca july 
greene levy 
new smoother implementations discrete time gaussian reciprocal processes 
int 
control 
johnson sa wolkowicz 
positive definite completions partial hermitian matrices 
linear alg 
appl 
guyon 
random fields network modeling statistics applications 
springer verlag new york 
heitz perez bouthemy 
multiscale minimization global energy functions visual recovery problems 
comput 
vision graphics image process january 
heitz perez bouthemy 
parallel visual motion analysis multiscale markov random fields 
proc 
workshop motion october 

estimation fractal signals wavelets filter banks 
ieee trans 
signal processing june 
ho willsky 
computationally efficient multiscale estimation diffusion processes 
automatica march 
ho chu 
team decision theory information structures optimal control problems part ieee trans 
automatic control feb 
ho chu 
team decision theory information structures optimal control problems part ii 
ieee trans 
automatic control feb 
hong 
filtering wavelet transform 
ieee trans 
aero 
elec 
sys october 
hong 
distributed filtering 
ieee trans 
automatic control april 
horn 
robot vision 
mit press cambridge ma 
horn 
height gradient shading 
int 
comput 
vision 

wavelets probability statistics bridges 
benedetto frazier editors wavelets mathematics applications 
crc press boca raton fl feb 
huang cressie 
deterministic stochastic wavelet decomposition recovery signal noisy data 
technometrics aug 
huang cressie 
multiscale graphical modeling space applications command control 
spatial statistics methodological aspects applications springer lecture notes statistics volume 
moore ed springer verlag new york 
huang mumford 
statistics natural images models 
proc 
cvpr 
huang cressie 
fast resolution consistent spatial prediction global processes data 
computational graphical statistics appear 
irving willsky 
overlapping tree approach multiscale stochastic modeling estimation 
ieee trans 
image processing november 

irving novak willsky 
multiresolution approach discrimination sar imagery 
ieee trans 
aerospace electronic systems october 
irving willsky 
multiscale stochastic realization canonical correlations 
ieee trans 
automatic control september 
ivanov 
filtering noise data applications kara black seas 
marine syst feb 
jain angel 
image restoration modeling reduction dimensionality 
ieee trans 
computers 

subsampling markov random fields 
visual com 
image sept 
johnson kumar 
modeling analyzing fractal point processes 
proc 
icassp 
johnson 
estimation gaussian mrf recursive cavity modeling 
sm thesis mit dept eecs may 
johnson sudderth tucker wainwright willsky 
multiresolution graphical models images spatial data 
siam conf 
imaging science march 
jordan editor 
learning graphical models 
mit press 
jordan 
probabilistic graphical models 
preparation 
jordan ghahramani jaakkola saul 
variational methods graphical models 
learning inference graphical models pages 

conditional simulation ore bodies 
econ 


fundamentals lessons 
amer 
geophysics union washington dc 

mining statistics 
academic press new york 
kailath hassibi 
linear estimation 
prentice hall upper saddle river nj 
kannan ostendorf karl casta fish 
ml parameter estimation multiscale stochastic processes em algorithm 
ieee trans 
signal processing june 
kaplan 
kuo 
fractal estimation noisy data discrete fractional gaussian noise haar basis 
ieee trans 
signal processing december 
karger 
learning markov networks maximum bounded tree width graphs 
proc 
th acm siam symp 
discrete algorithms january 
kashyap chellappa 
texture classification features derived random field models 
pattern recognition letters october 
kato 
hierarchical markov random field model annealing parallel image classification 
graphical models image processing january 
kato 
unsupervised parallel image classification markovian models 
pattern recognition april 
schneider karl willsky 
statistical method efficient segmentation imagery 
int patt 
recog 
art 
intell 
kay 
fundamentals statistical signal processing estimation theory 
prentice hall upper saddle river nj 
bouman 
automated assembly inspection multiscale algorithm trained synthetic images 
ieee robotics automation magazine june 
murray allen 
data fusion sea surface temperature data 

kim krim 
hierarchical stochastic modeling sar imagery segmentation compression 
ieee trans 
sig 
proc feb 
kingsbury 
image processing complex wavelets 
phil 
trans 
royal society london sept 
kolaczyk 
wavelet shrinkage approach tomographic image reconstruction 
journal american statistical association 
kolaczyk 
bayesian multi scale models poisson processes 
journal american statistical association 
kolaczyk huang 
multiscale statistical models hierarchical spatial aggregation 
anal april 
konrad dubois 
multigrid bayesian estimation image motion fields stochastic relaxation 
proc 
nd int conf 
computer vision pages december 
krim 

multiresolution analysis class nonstationary processes 
ieee trans 
information theory 
krim 
minimax description length signal denoising optimized representation 
ieee trans 
information theory 
krim tucker mallat donoho 
denoising best signal representation 
ieee trans 
information theory november 
krim willinger tse 
special issue multiscale statistical signal analysis applications 
ieee trans 
information theory apr 
krishnamachari chellappa 
multiresolution gauss markov random field models texture segmentation 
ieee trans 
image processing february 
krishnan hoo 
multiscale model predictive control strategy 
ind eng 
chem 
res may 
kschischang frey 
iterative decoding compound codes probability propagation graphical models 
ieee sel 
areas comm feb 
kumar 
multiple scale state space model characterizing subgrid scale variability near surface soil moisture 
ieee trans 
geoscience remote sensing 

perez heitz 
discrete markov image modeling inference quadtree 
ieee trans 
image processing march 
lakshmanan derin 
gaussian markov random fields multiple resolutions 
chellappa editor markov random fields theory applications pages 
academic press new york 
lam wornell 
multiscale representation estimation fractal point processes 
ieee trans 
signal processing november 
lam wornell 
multiscale analysis control networks fractal traffic 
appl 
comp 
harm 
analy 
lang guo wells 
noise reduction undecimated discrete wavelet transform 
ieee signal processing letters 
lauritzen 
graphical models 
clarendon press oxford 
learned willsky 
low complexity optimal joint detection saturated multiple access communications 
ieee trans 
signal processing january 

lee lucier 
wavelet methods inverting radon transform noisy data 
ieee trans 
image processing january 


optimal decomposition clique separators 
discrete mathematics 
levy 
noncausal estimation discrete gauss markov random fields 
proc 
int symp 
mathematical theory networks systems 
levy 
multiscale models estimation discrete gauss markov random fields 
nd siam conf 
linear alg 
systems control sig 
proc 
levy 
modeling estimation discrete time gaussian reciprocal processes 
ieee trans 
automatic control september 
vy 
le 
mem 
sci 
math 

li wilson 
image segmentation multiresolution bayesian framework 
proc 
ieee int conf 
image proc volume pages chicago il october 
li gray olshen 
multiresolution image classification hierarchical modeling dimensional hidden markov models 
ieee trans 
information theory aug 

stochastic realization problem 
siam control optimization may 
liu 
multiresolution method distributed parameter estimation 
siam sci 
comput 
ljung 
system identification theory user 
prentice hall upper saddle river nj 
ljung theory practice recursive identification 
mit press cambridge ma 
ramchandran orchard 
image coding mixture modeling wavelet coefficients fast estimation quantization framework 
data compression conference proceedings pages snowbird utah 
lovejoy 
generalized scale invariance atmosphere fractal models rain 
water resources res august 
lovejoy 
multifractals rain 
new uncertainty concepts hydrology modeling 
cambridge univ press cambridge 
teich 
fractal renewal generate noise 
phys rev 

stochastic models allow baum welch training 
ieee trans 
signal processing november 
karl willsky 
efficient multiscale regularization applications computation optical flow 
ieee trans 
image processing january 
karl willsky 
multiscale representations markov random fields 
ieee trans 
signal processing december 
willsky 
likelihood calculation class multiscale stochastic models application texture discrimination 
ieee trans 
image processing february 
willsky 
multiscale smoothing error models 
ieee trans 
automatic control january 

data assimilation overview motivation purposes 
naval research reviews 
mallat 
wavelet tour signal processing 
academic press san diego 
mallat zhang 
matching pursuits time frequency libraries 
ieee trans 
signal processing december 

approximating discrete probability distributions decomposable models 
ieee trans 
systems man cybernetics 
mandelbrot 
self similar error clusters communication systems concept conditional stationarity 
ieee trans 
commun 
van ness 
fractional brownian motions fractional noises applications 
siam review 
manjunath chellappa 
unsupervised texture segmentation markov random field models 
ieee trans 
pattern analysis machine intelligence may 
mitter poggio 
probabilistic solution ill posed problems computational vision 
journal american statistical association march 

wavelet transform stochastic processes stationary increments application fractional brownian motion 
ieee trans 
information theory january 
mceliece mackay 
cheng 
turbo decoding instance pearl belief propagation algorithm 
ieee sel 
areas comm 
meila 
learning mixtures trees 
ph thesis mit 
meila 
accelerated chow liu algorithm fitting tree distributions high dimensional sparse data 
icm 
meila jaakkola 
tractable bayesian learning tree belief networks 
uai 
meila jordan 
estimating dependency structure hidden variable 
nips 

error estimates ocean general circulation model altimeter acoustic tomography data 
mon 
weather rev mar 
wunsch willsky 
adaptation fast optimal interpolation algorithm mapping data 
geophysical research may 
ak ramchandran moulin 
low complexity image denoising statistical modeling wavelet coefficients 
ieee sig 
proc 
letters 
miller 
nonlinear inverse scattering methods thermal wave slice tomography wavelet domain approach 
opt 
soc 
amer 

miller willsky 
multiscale statistical anomaly detection analysis algorithms linearized inverse scattering problems 

systems sig 
proc 
miller willsky 
multiscale approach sensor fusion solution linear inverse problems 
appl 
comput 
harmonic anal 
miller willsky 
multiscale statistically inversion scheme linearized inverse scattering problem 
ieee trans 
geoscience remote sensing march 
miller willsky 
wavelet methods nonlinear inverse scattering problem extended born approximation 
radio science 
milne cohen 
multiscale assessment binary continuous variables validation mapping modeling applications 
remote sens environ october 
moulin liu 
analysis multiresolution image denoising schemes generalized gaussian complexity priors 
ieee trans 
info 
theory april 
ller vidakovic editors 
bayesian inference wavelet models 
springer new york 
mumford shah 
optimal approximation piecewise smooth functions associated variational problems 
comm 
pure appl 
math 
munk worcester wunsch 
ocean acoustic tomography 
camb 
univ press 
murphy weiss 
factored frontier algorithm approximate inference dbn uncertainty artificial intelligence 
proceedings th conference uai morgan kaufmann 
murray allen 
combined avhrr skin sea temperature analysis 
egs 
leighton white 
preconditioned adaptive iterative methods kind integral equations potential theory 
siam sci 
comput 
petrou 
generalisation group methods multiresolution image analysis 
proc 
ieee computer soc 
conf 
computer vision pattern recognition pages 
adams willsky levy 
estimation boundary value descriptor systems 
circuits systems sig 
proc april 
taylor levy willsky 
graph structure recursive estimation noisy linear relations 
math 
syst est contr 
willsky levy 
kalman filtering riccati equations descriptor systems 
ieee trans 
automatic control sept 
nowak 
multiscale hidden markov models bayesian image analysis 
bayesian inference wavelet models 
vidakovic ller eds springer verlag new york 
nowak 
shift invariant wavelet statistical models processes 
ieee dsp workshop 
nowak kolaczyk 
statistical multiscale framework poisson inverse problems 
ieee trans 
information theory august 
olson 
wavelet localization radon transform 
ieee trans 
image processing 
ostendorf digalakis kimball 
hmm segment models unified view stochastic modeling speech recognition 
ieee trans 
speech audio proc october 
papoulis 
probability random variables stochastic processes 
mcgraw hill new york 
pearl 
probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann publishers san mateo california 
pentland 
fractal description natural scenes 
ieee trans 
pattern analysis machine intelligence november 
perez heitz 
restriction markov random field graph multiresolution statistical image modeling 
ieee trans 
information theory january 

krim 
time invariant orthonormal wavelet representations 
ieee trans 
signal processing 
goutte 
multiscale reconstruction tomographic images 
proc 
sp int symp 
time frequency time scale anal 
rabiner 
tutorial hidden markov models selected applications speech recognition 
proc 
ieee february 
ramanathan 
wavelet transform fractional brownian motion 
ieee trans 
information theory july 
rao ballard 
dynamic model visual recognition predicts neural response properties visual cortex 
neural comput may 
rao pearlman 
analysis linear prediction coding spectral estimation subbands 
ieee trans 
information theory july 
rashid liu walnut 
wavelet multiresolution local tomography 
ieee trans 
image processing october 
rauch tung 
maximum likelihood estimates linear dynamic systems 
aiaa journal august 
richardson 
geometry turbo decoding dynamics 
ieee trans 
information theory 
riedi crouse ribeiro baraniuk 
multifractal wavelet model application network traffic 
ieee trans 
information theory april 
rokhlin 
rapid solution integral equations classical potential theory 
comput 
phys 
romberg choi baraniuk 
bayesian tree structured image modeling 
ieee trans 
image processing 
romberg choi baraniuk kingsbury 
hidden markov tree models complex wavelet transforms 
submitted ieee trans 
image proc 
ronen rohlicek ostendorf 
parameter estimation dependence tree models em algorithm 
ieee signal processing letters august 
yu 

random fields stochastic partial differential equations 
kluwer academic boston 
van roy 
analysis turbo decoding gaussian priors 
nips pages 
mit press 

image reconstruction projections wavelet constraints 
ieee trans 
signal processing december 

iterative inversion radon transform image adaptive wavelet constraints 
proc 
ieee int conf 
image proc october 
taqqu 
stable non gaussian random processes stochastic models infinite variance 
chapman hall new york 
bouman sauer 
non homogeneous mrf model multiresolution bayesian estimation 
proc 
ieee int conf 
image proc volume pages lausanne switzerland september 
bouman sauer 
multiresolution nonhomogeneous mrf model bayesian tomography 
ieee trans 
image processing 
submitted 
saul jordan 
exploiting tractable substructures intractable networks 
nips 
schneider karl willsky 
multiscale statistical methods segmentation images 
ieee trans 
image processing march 

synth se de aide de la transformation par ondelettes 
paris ser 
math 
shafer shenoy 
probability propagation 
ann 
math 
artificial intell 
shafer shenoy 
valuation systems bayesian decision analysis 
operations research 
shapiro 
embedded image coding wavelet coefficients 
ieee trans 
signal processing december 
simoncelli 
embedded wavelet image compression joint probability model 
icip volume pages santa barbara california october 
simoncelli freeman adelson heeger 
shiftable multiscale transforms 
ieee trans 
information theory march 
simoncelli 
statistical models images compression restoration synthesis 
proc 
st asilomar conf pages november 
simoncelli 
bayesian denoising visual images wavelet domain 
muller vidakovic editors bayesian inference wavelet models volume lecture notes statistics pages 
springer verlag new york june 
simoncelli adelson 
noise removal bayesian wavelet coring 
proc 
icip 
smyth heckerman jordan 
probabilistic independence networks hidden markov probability models 
neural 
comput 

maximum likelihood bounded tree width markov networks 
th conf 
uncertainty art 
intell aug 

starck 
multiscale image processing data analysis 
cambridge univ press cambridge 

regularized image restoration multiresolution spaces 
optical engineering june 
gil dyer 
multi scale aspects model predictive control 
process control apr jun 
stoica moses 
spectral analysis 
prentice hall upper saddle river nj 
portilla simoncelli 
image denoising local gaussian scale mixture model wavelet domain 
proc 
spie conf san diego july 
thelen gorman reilly 
multiresolution detection coherent radar targets 
ieee trans 
image processing january 
sudderth 
embedded trees estimation gaussian processes graphs cycles 
sm thesis mit dept eecs feb 
sussman 
analysis pareto model error statistics telephone circuits 
ieee trans 
communications 
szeliski 
bayesian modeling uncertainty low level vision 
kluwer norwell ma 
taqqu 
bibliographic guide self similar processes long range dependencies 
eberlein taqqu editors dependence probability statistics 
birkh user boston ma 
tarjan 
decomposition clique separators 
discrete mathematics 
tarjan yannakakis 
simple linear time algorithms test graphs test acyclicity hypergraphs selectively reduce acyclic hypergraphs 

taylor 
parallel estimation dimensional systems 
ph thesis mit dept eecs feb 
colchester editors 
medical image computing computer assisted intervention miccai 
springer new york 
teich johnson kumar 
rate fluctuations fractional power law noise recorded cells lower auditory pathway cat 
hearing res 
terzopoulos 
image analysis multigrid relaxation methods 
ieee trans 
pattern analysis machine intelligence pami march 
tewfik kim 
correlation structure discrete wavelet coefficients fractional brownian motion 
ieee trans 
information theory march 
tewfik levy willsky 
parallel smoothing 
systems control letters march 
nowak 
multiscale modeling estimation poisson processes application photon limited imaging 
ieee trans 
information theory april 
bouman 
multiscale stochastic image model automated inspection 
ieee trans 
image processing december 
tsai zhang willsky 
em algorithms image processing multiscale models mean field theory applications laser radar range profiling segmentation 
optical engineering july 
tucker 
multiresolution modeling data partial specifications 
ph thesis mit dept eecs completed aug 
harris 
scale issues verification precipitation forecasts 
geophys 
res atmos june 
bakshi 
multiscale bayesian error variables approach linear dynamic data rectification 
comput 
chem 
eng july 
verghese kailath 
note backwards markovian models 
ieee trans 
information theory 
vetterli kovacevic 
wavelets subband coding 
prentice hall englewood cliffs nj 
vidakovic 
nonlinear wavelet shrinkage bayes rule bayes factors 
amer 
statist 
assoc 

integrated altimeter situ data understanding water exchanges seas 
geophys 
res oceans aug 
wainwright jaakkola willsky 
tree reparameterization framework approximate estimation graphs cycles 
ieee trans 
information theory review 
wainwright simoncelli willsky 
random cascades wavelet trees modeling natural images 
applied computational harmonic analysis 
wainwright sudderth willsky :10.1.1.20.127
tree modeling estimation gaussian processes graphs cycles 
neural information processing systems december 
wang zhang 
pan 
solution inverse problems image processing wavelet expansion 
ieee trans 
image processing may 
irving johnson willsky 
image compression image fusion algorithms 
technical report feb 
weiss 
correctness local probability propagation graphical models loops 
neural comp 
weiss freeman 
correctness belief propagation gaussian graphical models arbitrary topology 
nips pages 
mit press 
whittaker 
graphical models applied multivariate statistics 
wiley new york 
whittle 
stationary processes plane 
biometrika 
whittle 
stochastic processes dimensions 
bull 
int 
stat 
inst 
berliner cressie 
hierarchical bayesian space time models 
environmental ecological statistics june 
willsky 
relationships digital signal processing control estimation theory 
proc 
ieee sept 
woods 
dimensional discrete markovian fields 
ieee trans 
information theory 
woods 
kalman filtering dimensions 
ieee trans 
information theory 
wornell 
wavelet representations family fractal processes 
proceedings ieee october 
wornell oppenheim 
estimation fractal signals noisy measurements wavelets 
ieee trans 
signal processing march 
wornell oppenheim 
wavelet representations class self similar signals applications fractal modulation 
ieee trans 
information theory march 
wornell 
signal processing fractals 
wavelet approach 
prentice hall englewood cliffs new jersey 
wornell 
karhunen lo expansion processes wavelets 
ieee trans 
information theory july 
wu 
tree approximations markov random fields 
ieee trans 
pattern analysis machine intelligence april 
wunsch 
low frequency variability sea 
warren wunsch editors evolution physical oceanography scientific studies honor henry 
mit press 
wunsch 
satellite determine general circulation oceans application improvement 
revs 
geophys 
space phys 
wunsch 
global frequency spectrum oceanic variability estimated poseidon measurements 
geophys 
res 

yang wilson 
adaptive image restoration multiresolution hopfield neural network 
fifth international conference image processing applications iee conference publication pages edinburgh uk july 
ye bouman webb 
nonlinear multigrid algorithms bayesian optical diffusion tomography 
ieee trans 
image processing june 
yedidia freeman weiss 
generalized belief propagation 
nips pages 
mit press 

random field representation synthesis wavelet bases 
app 
mech 
trans 
asme december 
zhang 
mean field theory em procedures markov random fields 
ieee trans 
signal processing 
zhang walter 
kl expansion wide sense stationary processes wavelet basis 
ieee trans 
signal processing july 
zhao 
wavelet filtering filtered backprojection computed tomography 
appl 
comp 
harm 
anal 
zhu wang deng yao 
wavelet multiresolution regularization squares reconstruction approach optical tomography 
ieee trans 
medical imaging april 
symbol usage meaning agraph node vertex set tree graph vs set nodes subtree rooted node node edge set graph subsets nodes graph clique graph set cliques graph nodes trees graphs children node tree parent node tree closest common ancestor nodes tree index scale representation scale node tree matrices define models trees random variables vectors node tree graph collection vector variables xa collection vectors variables entire tree graph px prior covariance model xs smoothed estimate model pe covariance error estimate xs estimate data vs covariance error estimate estimate data vs measurements node covariance error estimate estimate data vs covariance error estimate px prior covariance vector optimal estimate pe covariance error estimate spatial variable dimensions planar region height surface region gradient surface table notation list figures examples trees organized resolutions 
dyadic tree typically representation signals including notation nodes tree subsequent sections quadtree frequently representations imagery random fields 
pictorial representation emphasizes node tree represents pixel spatial region spatial resolution spatial location corresponding node 
shading fine scale pixels associated discussion section 
set poseidon measurement tracks north pacific ocean 
taken 
noise free image noisy version image restored version image optimal wiener filtering image blocks restored version wiener filtering image blocks 
taken 
illustration textures markov random field models 
sand 
taken 
remotely sensed multispectral spot image document page 
illustrating midpoint deflection construction samples brownian motion tree model structure corresponding midpoint deflection construction 
typical example tree tree part redrawn appears node labeled 
illustrating recursive structure statistical processing algorithms trees 
fine coarse upward sweep optimal estimation algorithm see equations 
coarse fine downward sweep producing optimal smoothed estimates see equations 
hybrid recursive structure whitening data upward fine coarse portions computations comprise kalman filtering upward sweep shown produce partially whitened measurement residuals downward portion computations complete whitening particular total ordering nodes tree compatible partial order implied tree 
adapted 
estimates ocean height relative set poseidon measurements tracks estimation error variances associated estimates 
maps computed estimation algorithm 
overlay ocean contours locations statistically anomalous measurement residuals 
taken 
comparison probabilities correct classification function parameter 
dashed line represents optimal performance exact gaussian markov random field gmrf likelihood ratio test lrt solid line corresponds performance model mm lrt referred th order model corresponding keeping scalar state node quadtree model dash dot line performance suboptimal minimum distance md classifier 
results image chip snr db 
illustrating performance approaches optimal achievable increase order approximate model results image chip snr db 
taken 
denoised version noisy image shown hidden markov tree model complex wavelet decomposition 
taken 
noise free image reconstructed emission computed tomographic ect measurements different reconstructions ect measurements different models corresponding different parameter values corresponding strengths prior models ranging weakest strongest see details 
illustrating construction cutset tree models 
graph graphical model 
defined 
illustrating particular choice root cutset disjoint subsets separated 
cutset tree model resulting choice root cutset dashed lines indicate redundant values added finest scale entire process resides finest scale 
lower dimensional model unnecessarily repeated variables removed various nodes 
regular nearest neighbor lattice horizontal cutset red containing number nodes equal linear dimension lattice 
cutset 
triangulation graph 
junction tree triangulation 
set signals yield value version smoothness penalty 
taken 
ocean surface reconstruction poseidon data discontinuous surface reconstructed reconstruction surface noisy measurements estimation algorithm described employing thin plate membrane priors lead smooth reconstructions surface discontinuities distribution locations signs statistically significant measurement residuals providing clear statistical evidence detection localization estimation surface discontinuity 
taken 
illustrating role state linear gaussian models 
general conditioned value state sets variables indicated dashed lines uncorrelated 
internal models state need decorrelate set values single set nodes encircled dotted red line remaining decorrelation required automatically satisfied internality illustrating result applying scale recursive method constructing internal approximate stochastic realizations case fractional brownian motion hurst parameter 
plot exact covariance matrix window fbm plot covariance achieved model state dimension difference covariances plotted image set noisy measurements fbm process ends interval interest estimates algorithm dimensional state model solid line optimal estimates exact fbm statistics dashed line completely obscured solid line plus minus standard deviation error bars dotted line error standard deviations estimator solid line exact fbm statistics dashed line completely obscured solid line 
taken 
illustration effectiveness algorithm estimating travel time perturbations 
dashed line represents distribution predicted direct estimation travel time perturbation modeled linearized functional log conductivity directly incorporated model state coarse scale node dashed gaussians means variances corresponding resulting estimate error variance computed estimation algorithm 
histogram depicts result conditional simulation estimates fine scale conductivity model errors estimates draw sample conductivity fields drive equations yielding sample values travel time 
corresponds case log conductivity perturbation background value comparatively small order magnitude larger 
taken 
illustrating internal tree model fbm hurst parameter daubechies tap orthogonal wavelet 
consider problem estimating sample path fbm noisy measurements shown process subintervals extreme ends interval interest 
plot depicts estimation results model solid line exact fbm statistics dash dot line plus minus standard deviation bars dashed line 
taken 
example method developed nonparametric estimation models steerable wavelet pyramids sample image 
small region included black border image right real image model learned 
entire image shown entirely synthetic learned model small region real image located 

example loopy graphical structure including direct connections major boundaries underlying tree 
spanning trees segmentation results images technique described example multispectral pixel classified region types sophisticated algorithm building framework described pixel classified classes text picture background 
examples trees organized resolutions 
dyadic tree typically representation signals including notation nodes tree subsequent sections quadtree frequently representations imagery random fields 
pictorial representation emphasizes node tree represents pixel spatial region spatial resolution spatial location corresponding node 
shading fine scale pixels associated discussion section latitude north satellite track locations longitude east set poseidon measurement tracks north pacific ocean 
taken 
noise free image noisy version image restored version image optimal wiener filtering image blocks restored version wiener filtering image blocks 
taken 
illustration textures markov random field models 
sand 
taken 
remotely sensed multispectral spot image document page 
illustrating midpoint deflection construction samples brownian motion tree model structure corresponding midpoint deflection construction 
typical example tree tree part redrawn appears node labeled taken root node 
illustrating recursive structure statistical processing algorithms trees 
fine coarse upward sweep optimal estimation algorithm see equations 
coarse fine downward sweep producing optimal smoothed estimates see equations 
hybrid recursive structure whitening data upward fine coarse portions computations comprise kalman filtering upward sweep shown produce partially whitened measurement residuals downward portion computations complete whitening particular total ordering nodes tree compatible partial order implied tree 
adapted 
latitude north estimates cm intervals longitude east latitude north estimation error longitude east latitude north contours large residual overlay longitude east estimates ocean height relative set poseidon measurements tracks estimation error variances associated estimates 
maps computed estimation algorithm 
overlay ocean contours locations statistically anomalous measurement residuals 
taken 
comparison probabilities correct classification function parameter 
dashed line represents optimal performance exact gaussian markov random field gmrf likelihood ratio test lrt solid line corresponds performance model mm lrt referred th order model corresponding keeping scalar state node quadtree model dash dot line performance suboptimal minimum distance md classifier 
results image chip snr db 
illustrating performance approaches optimal achievable increase order approximate model results image chip snr db 
taken 
denoised version noisy image shown hidden markov tree model complex wavelet decomposition 
taken noise free image reconstructed emission computed tomographic ect measurements different reconstructions ect measurements different models corresponding different parameter values corresponding strengths prior models ranging weakest strongest see details 
illustrating construction cutset tree models 
graph graphical model 
defined 
illustrating particular choice root cutset disjoint subsets separated 
cutset tree model resulting choice root cutset dashed lines indicate redundant values added finest scale entire process resides finest scale 
lower dimensional model unnecessarily repeated variables removed various nodes 
regular nearest neighbor lattice horizontal cutset red containing number nodes equal linear dimension lattice 
cutset vertical blue leads quadtree model structure 
triangulation graph 
junction tree triangulation 
set signals yield value version smoothness penalty 
taken 
latitude north longitude east estimate observation estimate observation ocean surface reconstruction poseidon data discontinuous surface reconstructed reconstruction surface noisy measurements estimation algorithm described employing thin plate membrane priors lead smooth reconstructions surface discontinuities distribution locations signs statistically significant measurement residuals providing clear statistical evidence detection localization estimation surface discontinuity 
taken 

illustrating role state linear gaussian models 
general conditioned value state sets variables indicated dashed lines uncorrelated 
internal models state need decorrelate set values single set nodes encircled dotted red line remaining decorrelation required automatically satisfied internality 

illustrating result applying scale recursive method constructing internal approximate stochastic realizations case fractional brownian motion hurst parameter 
plot exact covariance matrix window fbm plot covariance achieved model state dimension difference covariances plotted image set noisy measurements fbm process ends interval interest estimates algorithm dimensional state model solid line optimal estimates exact fbm statistics dashed line completely obscured solid line plus minus standard deviation error bars dotted line error standard deviations estimator solid line exact fbm statistics dashed line completely obscured solid line 
taken 
occurences occurences illustration effectiveness algorithm estimating travel time perturbations 
dashed line represents distribution predicted direct estimation travel time perturbation modeled linearized functional log conductivity directly incorporated model state coarse scale node dashed gaussians means variances corresponding resulting estimate error variance computed estimation algorithm 
histogram depicts result conditional simulation estimates conductivity model errors estimates draw sample conductivity fields drive equations yielding sample values travel time 
corresponds case log conductivity perturbation background value comparatively small order magnitude larger 
taken 
illustrating internal tree model fbm hurst parameter daubechies tap orthogonal wavelet 
consider problem estimating sample path fbm noisy measurements shown process subintervals extreme ends interval interest 
plot depicts estimation results model solid line exact fbm statistics dash dot line plus minus standard deviation bars dashed line 
taken 
example method developed nonparametric estimation models steerable wavelet pyramids sample image 
small region included black border image right real image model learned 
entire image shown entirely synthetic learned model small region real image located 

example loopy graphical structure including direct connections major boundaries underlying tree 
spanning trees graph shown segmentation results images technique described example multispectral pixel classified region types sophisticated algorithm building framework described pixel classified classes text picture background 

