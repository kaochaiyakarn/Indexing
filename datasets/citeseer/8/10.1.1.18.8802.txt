performance optimizations bounds sparse matrix vector multiply richard james demmel katherine yelick rajesh benjamin lee computer science division university california berkeley berkeley california usa demmel yelick cs berkeley edu consider performance tuning code data structure reorganization sparse matrix vector multiply spm important computational kernels scientific applications 
addresses fundamental questions limits exist performance tuning closely tuned code approaches limits 
specifically develop upper lower bounds performance mflop spm tuned previously proposed register blocking optimization 
bounds non zero pattern matrix cost basic memory operations cache hits misses 
evaluate tuned implementations respect bounds hardware counter data different platforms test set sparse matrices 
find get upper bound particularly class matrices finite element modeling fem problems non fem matrices performance improvements possible 
lastly new heuristic selects optimal near optimal register block sizes key tuning parameters accurately previous heuristic 
new heuristic show improvements spm performance mflop untuned implementation 
collectively results suggest performance improvements demonstrated spm come sources consideration higher level matrix structures exploiting symmetry matrix reordering multiple register block sizes optimizing kernels opportunity data reuse sparse matrix multiple vector multiply multiplication vector 
research supported part llnl memorandum agreement 
department energy contract 
eng doe 
de fg er national science foundation 
asc nsf cooperative agreement 
aci nsf infrastructure 
eia gift intel 
information necessarily reflect position policy government official endorsement inferred 
ieee consider problem building high performance implementations sparse matrix vector multiply spm important ubiquitous computational kernel 
call source vector destination vector 
making spm fast complicated modern hardware architectures overhead manipulating sparse data structures 
unusual see spm run peak floating point performance single processor 
hardware oft cited performance gap processor memory drives need exploit locality memory hierarchy 
designing data structures algorithms daunting time consuming task best implementation vary processor processor compiler compiler matrix matrix 
need different data structure sparse matrix major distinction problem tuning dense matrix kernels dense blas information matrix structure available run time 
approach prior tuning dense blas generate set candidate algorithms best search set combination running algorithms performance modeling :10.1.1.108.3487
set candidate algorithms known matrix structure run time careful spend time generating set candidate algorithms searching 
contrast algorithm generation search done line dense blas 
prior sparsity system version im developed algorithm generator search strategy spm quite effective practice 
sparsity generators employed variety performance optimization techniques including register blocking cache blocking multiplication multiple vectors 
focus register blocking section ask fundamental questions limits exist performance tuning close tuned code gets limits section 
develop upper lower bounds execution rate mflop spm nonzero pattern matrix cost basic memory operations cache hits misses 
bounds differ assumption conflict misses occur upper bound value modeled cache hit conflict misses lower bound assumes data reloaded 
detailed hardware counter data collected different computing platforms table test set sparse matrices table show upper bound fact quite accurate approximation reality conflict misses rare 
show results new register block selection heuristic significantly outperforms previous heuristics machines favor non square block sizes 
new heuristic part sparsity version 
new heuristic improve spm unblocked implementation takes modest amount time execute 
tuning works best matrices natural block structure matrices finite element modeling fem 
fem matrices itanium pentium iii ultra platforms performance tuning heuristic usually performance upper bound indicating compiler level tuning effort warranted class matrices 
especially surprising itanium worst performance machines relative hardware peak 
non fem matrices performance improvements possible 
power architecture tuning significantly improve performance farther upper bound platforms 
analyses suggest additional effort low level tuning instruction scheduling beneficial 
taken results show performance tuning spm beneficial additional improvements come considering higher level matrix structures exploiting symmetry reordering matrix cache blocking optimizing kernels opportunity data reuse multiplying multiple vectors multiplying vector 
related dense matrices cache memory behavior studied 
variety sophisticated static models developed goal providing compiler sufficiently precise models selecting memory hierarchy transformations parameters tile sizes 
difficult apply analyses directly sparse matrix kernels due presence indirect irregular memory access patterns 
despite difficulty analysis sparse case number notable attempts 
jalby heras fraguela developed sophisticated probabilistic cache models assume uniform distribution non zero entries 
models primarily distinguished ability account self misses 
study see current machines cache sizes continue grow conflict misses important accurate modeling section 
gropp bounds similar ones develop analyze tune computational fluid dynamics code detailed performance study fracture mechanics code itanium 
interested tuning matrices come variety domains 
furthermore explicitly model execution time just modeling misses order evaluate extent tuned implementations achieve optimal performance 
mention examples sparse compiler literature bik pugh bernoulli compiler 
analyzes matrices high level structure techniques complementary consider consider sparse code specification generation issues specialize specific matrix structures 
tools technology developed research projects serve code generation infrastructure automatic tuning system sparsity 
distinguish hybrid line line model selecting transformations register blocking sizes described section 
experimental setup platforms conducted experimental evaluations machines microprocessors shown table 
table summarizes platform hardware compiler configurations performance results key dense kernels 
latency estimates obtained published sources confirmed experimentally memory system microbenchmark due saavedra barrera 
matrices evaluate spm implementations matrix benchmark suite im 
table summarizes size source matrix 
matrices available collections nist university florida 
matrices table arranged roughly groups 
matrix dense matrix stored sparse format matrices arise finite element method fem applications come assorted applications including chemical process engineering oil reservoir modeling circuits finance linear programming examples 
timing papi library access hardware counters platforms cycle counters timers 
counter values reported median consecutive trials 
largest cache machines notably cache power large contain matrices 
avoid inflated findings platform report performance results subset cache matrices 
figures numbering scheme shown table 
spm reported performance mflop uses ideal flops 
transformation matrix requires filling explicit zeros register blocking described section arithmetic extra zeros counted flops determining performance 
standard deviation trials typically median 
sun intel ibm intel property ultra pentium iii power itanium clock rate mhz mhz mhz mhz peak main mb mb gb gb memory bandwidth peak flop mflop mflop rate dgemm mflop mflop mflop mflop mflop mflop stream triad mb mb mb gb bandwidth data kb kb kb kb cache size line size latency cy cy cy cy int cache size mb kb mb kb line size latency cy cy cy cy int cy double cache size mb line size latency cy int cy double tlb entries tlb tlb page size kb kb kb kb minimum cy cy cy cy memory latency maximum cy cy cy cy memory latency sizeof double sizeof int compiler sun intel ibm intel flags tpp native xk unroll xo pwr pwr table evaluation platforms 
list basic configuration data machines compilers experiments 
performance figures blas sun ultra platform best sun performance library atlas pentium iii platform figures reported best intel mkl atlas power platform ibm itanium platform intel mkl 
name application area dimension nonzeros dense dense matrix fluid structure interaction accuracy problem bcsstk stiff matrix automobile frame venkat flow simulation fem crystal free vibration fem crystal free vibration shuttle rocket booster pressure tube ct ct engine block bai airfoil eigenvalue calculation buckling problem ex steady flow rdist chemical process separation vavasis pde problem economic modeling rim fem fluid mechanics problem circuit simulation power flow lhr light hydrocarbon recovery goodwin fluid mechanics problem bayer chemical process simulation bayer chemical process simulation simulation coating flows financial portfolio optimization harmonic balance method structural engineering problem structure problem wang semiconductor device simulation fluid flow modeling lns fluid flow modeling sherman oil reservoir modeling sherman oil reservoir modeling oil reservoir simulation oil reservoir modeling viscous flow calculation wang semiconductor device simulation astrophysics circuit physics modeling gupta linear programming matrix linear programming problem linear programming problem linear programming problem linear programming problem table matrix benchmark suite 
matrices categorized roughly follows dense matrix stored sparse format arise finite element applications come assorted applications linear programming examples 
improving register reuse section provides brief overview sparsity register blocking optimization technique improving register reuse conventional implementation 
register blocking describe designed exploit naturally occuring dense blocks reorganizing matrix data structure sequence small fit register dense blocks 
close section description new heuristic selecting register block size overcomes short coming previously published sparsity heuristic 
section sparsity system including new heuristic sparsity version 
concreteness assume baseline stores matrix compressed sparse row csr format 
register blocking optimization register blocked implementation consider matrix divided logically submatrices submatrix size assume simplicity divides divides sparse matrices blocks contain non zero stored 
computation spm proceeds block block 
block reuse corresponding elements source vector elements destination vector keeping registers assuming sufficient number available 
sparsity implementation register blocking uses blocked variant compressed sparse row storage format 
blocks block row stored consecutively elements block stored consecutively row major order 
example shown 
reduces csr 
note potentially stores fewer column indices csr implementation block non zero 
effect reduce memory traffic reducing storage overhead 
furthermore sparsity implementations fully unroll submatrix computation reducing loop overheads exposing scheduling opportunities compiler 
example implementation appears appendix shows imposition uniform block size may require filling explicit zero values resulting extra computation 
define fill ratio number stored values original non zeros plus explicit zeros divided number non zeros original matrix 
conversion register blocked format profitable depends highly fill turn non zero pattern matrix 
analogy tiling dense case difficult aspect applying register blocking knowing see barrett list common formats 
row major sparsity convention column major layouts possible 
performance code comparable csr implementation nist sparse blas 
row start col idx value block compressed sparse row storage format 
format uses arrays 
elements dense block stored contiguously value array 
column index entry block stored col idx array row start array points block row starting positions col idx array 
sparsity blocks stored row major order 
taken im 
matrices apply select block size 
assume single block size suitable matrix 
difficulty striking examine register blocking performance various values show hardware platforms performance mflop block sizes regular sparse problem dense matrix stored sparse format 
performance strong function architecture compiler block size 
estimate potential performance gains performance tuning maximum speedups range itanium power pentium iii 
furthermore irregularity spaces suggests performance general difficult model 
profiles shown clearly contain lot information exploit heuristic selecting block size 
selecting register block size best block size depends machine matrix 
general best block size may square 
instance matrix naturally expressed blocks blocks may considerably faster suggested itanium register profile shown 
assume general case know matrix runtime 
depending application cost exhaustively searching note performance profiles shown matrix size power large mb cache platforms 
block sizes considered true matrix size differs case 
mode usage unreasonable fact implied mode operation new sparse blas standard 
row block size row block size register blocking performance mflop dense ultra solaris column block size register blocking performance mflop dense power aix column block size row block size register blocking performance mflop dense pentium linux icc column block size row block size register blocking performance mflop dense itanium linux ecc column block size register profiles 
performance mflop register blocked implementations dense matrix stored format block sizes 
results platforms shown clockwise upper left sun ultra intel pentium iii intel itanium ibm power 
power 
platform square implementation shaded performance mflop top implementations labeled speedup relative implementation range speedup power pentium iii 
performance irregular structure appears difficult model best implementations differ performance little furthermore appear semi clustered optimal block size prohibitive itanium observed cost reorganizing matrix value times cost running spm 
sparsity uses register profiles shown estimate performance implementations assuming fill 
matrix known sparsity randomly samples fraction matrix rows compute estimate fill see 
sparsity chooses block size maximizing ratio dense performance fill ratio estimate 
register profile depend particular sparse matrix collected line machine 
describe new sparsity version heuristic selecting contrast briefly previous heuristic 
estimate fill select block rows uniformly random count number zeros filled simultaneously 
currently limit estimate sizes matrix benchmark suite observed optimal sizes greater 
perform scan independently obviously improved simultaneously scanning factors scanning simultaneously search 
implemented scan matrix 
cost procedure usually cost reorganization able estimate fill ratio fem matrices average matrices benchmark suite 
previously published sparsity heuristic estimated fill ratio sizes block sizes 
selected independently maximizing separately ratios dense performance fill ratio estimate dense performance fill ratio estimate matrices previous heuristic tended selected square block sizes led performance best block sizes itanium platform 
bounds register blocking performance develop upper lower bounds performance understand evaluate sparsity register blocking optimization new heuristic 
level accuracy probably adequate detailed study currently subject investigation 
preliminaries format outlined section 
count number loads stores required spm format follows 
matrix non zeros 
number non zero blocks required store matrix format blocks matrix requires storage double precision values integers column indices integers row pointers 
fill ratio frc 
matrix entry loaded 
assume spm iterates block rows entries destination vector kept registers duration block row multiply 
need load element destination vector store element 
assume source vector elements kept registers multiplication block requiring total loads source vector 
terms number non zeros fill ratio total number loads floating point integer data loads rc matrix dest vec source vec rc total number stores observe little fill dense matrix stored sparse format increasing block size reduce overhead storing column indices rc note source vector load term depends introducing slight asymmetry number loads function block size 
bounds modeling cache misses estimate analytic upper bound performance specifying lower bound cache misses 
start cache 
cache line size words 
compulsory read cache line incurred matrix element value index destination vector element 
source vector count complicated predict 
source vector size cache size best case incur compulsory cache line source vector elements 
lower bound lower misses lower rc 
size floating point value equals integers 
double precision bit floating point data bit integers 
factor accounts line size 
analogous expression applies cache levels simply substituting right line size 
worst case access source vector element due capacity conflict self cross interference misses upper bound misses upper rc 
model execution time follows 
want upper bound performance lower bound time assume overlap latencies due computation memory access 
hi number hits cache level mi number misses 
execution time hi mem access time cycles seconds cache level lowest level cache mem memory access time 
hits loads 
assuming perfect nesting caches level access level hi mi mi 
performance mflop get estimate upper bound performance mi lower equation convert mflop similarly get lower bound performance letting mi upper 
interaction tlb complicates estimate mem 
incorporate tlb performance upper bound letting mem minimum memory access latency shown table 
latency assumes memory access tlb hit 
lower bound assume mem maximum memory access latency shown table corresponds memory access tlb appropriate apply slight refinements model incorporate features evaluation platforms 
instance power itanium commit loads cycle hit cache 
reduce latency obtain performance upper bound 
take account fact itanium cache hit times depend data tied integer double precision registers 
note model execution time equation charges full latency memory access 
address assumption respect main memory bandwidth section 
equation loose upper bound essentially ignores spatial locality accesses source vector 
principle refine bound matrix nonzero pattern identify spatial locality simplicity 
validating cache bounds collected data register block sizes matrices platforms measuring execution time cache misses papi 
validate cache bounds figures 
matrix compared cache misses measured papi cache lower upper bounds equation equation respectively 
figures show data best block size chosen exhaustive search 
see bounds match true misses 
particular vector lengths matrix suite small lower bounds assume capacity conflict misses count true misses accurately chip caches cache ultra pentium iii power platforms cache itanium platform 
evaluating register blocking performance evaluate register blocking optimization respect upper lower bounds performance derived 
figures summarize evaluation hardware platforms table matrix benchmark suite table respect upper lower performance bounds 
compare implementations unblocked implementation represented asterisks 
sparsity heuristic implementation chosen sparsity heuristic described section represented circles 
sparsity exhaustive implementation chosen exhaustive search represented squares 
refer block size opt opt analytic upper lower bounds analytic upper lower bounds performance mflop opt opt implementation computed section shown dash dot solid lines respectively 
papi upper bound upper bound performance opt opt implementation obtained substituting measured cache data papi equation minimum memory latency mem represented triangles 
particularly interested making observations topics performance sparsity vs implementation figures show sparsity implementations chosen heuristic exhaustively achieve speedups implementation 
register blocking particularly effective matrices arising fem applications 
power architecture observe relatively small speedups fem matrices 

misses millions 
misses millions upper bound papi lower bound misses ultra solaris matrix upper bound papi lower bound misses ultra solaris matrix cache model validation sun ultra 
upper lower bounds cache misses compared papi measurements implementation best performance 
bounds match data 
true misses match lower bound larger cache suggesting vector sizes small conflict misses play relatively small role 

misses millions 
misses millions upper bound papi lower bound misses pentium linux icc matrix upper bound papi lower bound misses pentium linux icc matrix cache model validation intel pentium iii 
upper lower bounds cache misses intel pentium iii compared papi measurements 
lower bounds match largest cache 

misses millions 
misses millions upper bound papi lower bound misses power aix matrix upper bound papi lower bound misses power aix matrix cache model validation ibm power 
upper lower bounds cache misses ibm power compared papi measurements 
lower bounds match largest cache 

misses millions 
misses millions upper bound papi lower bound misses itanium linux ecc matrix upper bound papi lower bound misses itanium linux ecc matrix cache model validation intel itanium 
upper lower bounds cache misses intel itanium compared papi measurements 
lower bounds match largest cache 
performance mflop performance summary ultra solaris sparsity heuristic sparsity exhaustive analytic upper bound upper bound papi analytic lower bound matrix performance summary ultra 
performance mflop implementation chosen sparsity compared best implementation chosen exhaustive search upper lower bounds 
show data ultra 
fem matrices sparsity performance estimated upper bound 
performance mflop performance summary pentium linux icc sparsity heuristic sparsity exhaustive analytic upper bound upper bound papi analytic lower bound matrix performance summary pentium iii 
pentium iii 
sparsity implementations achieve upperbound fem matrices 
performance mflop performance summary power aix sparsity heuristic sparsity exhaustive analytic upper bound upper bound papi analytic lower bound matrix performance summary power 
ibm power 
sparsity implementations achieve upper bound estimate 
performance mflop performance summary itanium linux ecc sparsity heuristic sparsity exhaustive analytic upper bound upper bound papi analytic lower bound matrix performance summary itanium 
intel itanium 
sparsity implementations achieve upper bound fem matrices 
quality sparsity block size selection heuristic sparsity heuristic selecting block size generally chooses optimal block size block size performance optimal 
detailed comparison block sizes performance appears tables appendix 
proximity sparsity performance upper lower bounds valid compare sparsity exhaustive shown squares upper lower bounds block size opt opt gap sparsity upper bound indicates room left improvement cases 
elaborate high level specific matrices platforms 
ultra pentium iii itanium platforms sparsity implementations achieve upper bound matrices fem set matrices 
matrices natural dense structure register blocking able exploit proximity bound suggests additional low level tuning register block implementations lead significant additional gains 
performance non fem matrices varied ultra pentium iii itanium platforms tends closer lower performance bound upper typically ranging upper bound performance 
recall lower bound assumes accesses source vector cache lines due capacity conflict misses 
suggests non fem matrices possibly due particular sparsity patterns exhibit conflicts reduced spatial locality 
form matrix reordering multiple block sizes effective way address performance issue :10.1.1.31.7599
power performance implementations falls estimated upper bound smaller fraction machines 
factor differentiating power architecture study high performance memory system resulting comparable performance implementations 
results encouraging limited 
upper bounds expected upper bounds respect particular register blocking data structure 
possible data structures instance remove uniform block size assumption change dependence frc better 
note ultra pentium iii appears sparsity implementation running faster dense matrix stored sparse format matrix 
vendor supplied routine case optimally tuned 
report useful scale advocate conversion dense sparse formats 
model justification latency vs bandwidth primary assumption execution time model equation charge full latency mem memory cache access 
combination high peak memory bus bandwidth ability modern processors tolerate multiple outstanding misses principle allow higher data stream rates 
compute upper bound total memory transfer requirements algorithm memory bus speeds obtain higher upper bound 
section explore bandwidth numbers obtained examining simpler benchmarks known stream benchmarks show 
model charges full memory latency fill element cache line better match actual performance peak memory bus bandwidth 
example consider time load uncached double precision word memory processor ultra perspective peak memory bandwidth latency model 
assuming word moves peak memory bus bandwidth mb time load double precision word main memory mb ns approximately cycles 
contrast apply full latency model find streaming double precision array costs cycles word effective bandwidth full latency model cy mhz mb just half theoretical peak 
natural ask full latency model justified streaming applications practice gap effective bandwidth full latency model peak memory bus bandwidth 
standard benchmark assessing sustainable memory bandwidth stream benchmark 
stream consists vector kernels operating long cache vector operands 
ran stream benchmark evaluation platforms wrote additional kernels intended mimic access patterns characteristic sparse matrix vector multiply 
kernels summarized table standard stream benchmark comprises kernels 
kernel stream calculates memory bandwidth dividing volume data moved time execute kernel 
kernels deserve additional explanation sum dot kernels manually unrolled loops hand summing separate scalars order reduce dependence successive additions scalar 
explored unrolling depths report bandwidth results best case section 
cost full latency model ultra determined follows 
words line size word requires access main memory incurring mem cy second word hits cache incurring cycles final pairs words cost cy 
total time execute double precision loads cy cy word 
note stream give credit additional cache line load required store operations write back caches 
details see stream home page 
volume data vector transferred kernel operation doubles copy scale add triad sum dot load sparse matrix ind spm external cache ind spm chip cache ind table memory bandwidth microkernels 
standard stream benchmark comprises kernels consider additional kernels mimic selected aspects spm letters denote double precision arrays ind denote array integer indices 
denote double precision scalars denote integer scalars 
scalars assumed cpu registers execution kernel 
vector index ranges elements vector 
vectors elements chosen platform double precision vectors times larger largest cache 
spm chip length chosen size largest chip cache spm external size largest chip cache 
load sparse matrix ind 
kernel measures peak rate matrix non zero values integer indices brought memory 
note model access csr storage integer index non zero ignore row pointers 
spm chip cache performs spm operation ind 
kernel simulates indirect access true spm contains stores memory source vector sized size largest chip cache ita platforms reduce effect conflicts 
load sparse matrix kernel kernel models access csr storage 
spm external cache similar spm chip size source vector size largest external cache itanium 
summarize results executing kernels evaluation platforms 
specifically compare bandwidths peak bandwidth axis measures bandwidth relative peak memory bus bandwidth peak bandwidth table peak bandwidth shown implicitly 
measured bandwidth kernel measured bandwidth computed volume data shown table column divided measured execution time 
model bandwidth refer bandwidth predicted model equation model bandwidth 
compute model bandwidth applying latency model kernel computing equation dividing volume data shown table equation 
shows set bars platform bar indicates measured bandwidth normalized peak bandwidth kernels 
addition kernels bars shown platform measured bandwidths spm routine dense matrix csr format best register blocked routine dense matrix format 
bars measured execution time matrix computed measured bandwidth dividing volume data measured time 
volume data volume vector data plus volume matrix data 
volume vector data count load source destination vector elements plus store destination vector total doubles 
matrix data volume double precision words rc doubles dense matrix sparse format 
model bandwidth shown kernels asterisks vertically aligned corresponding bar 
synthetic kernels formulas hi equation 
blocked unblocked spm papi data hi 
model bandwidth best spm corresponds papi upper bounds matrix figures 
observations results 
note report bandwidth volume data generally different kernel care taken attempting compare infer execution time kernels 
measured bandwidth peak bandwidth typically platforms 
ultra bandwidth peak 
model bandwidth upper bound measured bandwidth kernels ultra power measured bandwidth pentium iii itanium platforms 
model bandwidth measured bandwidth indicates may possible achieve additional performance top predicted model figures 
proximity model bandwidth measured bandwidth suggests gains small 
gap model measured bandwidth fraction peak main memory bandwidth sustainable memory bandwidth stream ultra pentium iii platform power itanium ind ind chip ind ext spm dense spm dense best full latency model memory bandwidth benchmark results 
show measured main memory bandwidth mb fraction theoretical peak shown table kernels shown table evaluation platforms 
set bars bars shows bandwidth kernels 
bars show measured bandwidth spm routine dense matrix stored sparse format exhaustive best register blocked spm routine dense matrix 
show asterisks model bandwidth computed bars non standard stream kernels 
largest dot product kernel itanium approximation spm chip spm external true spm kernels 
platforms dot product kernel achieves highest bandwidth synthetic kernels 
furthermore bandwidth loading double precision streams dot product higher bandwidth loading single stream sum 
comparable ultra pentium iii itanium treated latency model 
power sum kernel runs lower bandwidth dot product know 
expect bandwidth true unblocked spm fall synthetic variants spm chip spm external 
case 
fact unblocked bandwidth tends close spm chip bandwidth platforms pentium iii pentium iii unblocked bandwidth close spm external 
phenomenon explained size chip cache pentium iii smallest chip caches 
consider unblocked blocked spm bandwidths shown 
platforms power improvement model bandwidth similar observed improvement measured bandwidth showing tuning realized positive effect expected model 
section shows power exists significant gap performance register blocked implementation upper bound 
suggests additional low level tuning prove beneficial platform 
particular gap measured bandwidth model bandwidth dot product kernel smaller gap spm results confirm latency model offers reasonable upper bound spm certain settings dot product may possible better limit suggested model additional lowlevel tuning 
observed bandwidth results clear improvements realized spm yield performance benefits additional 
exception power additional effort scheduling lead substantial performance gains 
directions results show sparse matrices natural block structures computer architectures close best possible performance spm leads ask performance improvements lie 
changing sum kernel include scalar multiply add change measured bandwidth 
matrices natural block structures remain difficult 
techniques reordering rows columns create blocks multiple block sizes cache blocking storing large rectangular submatrices separate sparse matrices show promise pursuing 
second register blocking techniques effective power architecture 
need determine better 
third need exploit structure sparse matrices improve data reuse 
example matrix symmetric matrix entry reused twice 
preliminary results indicate go significantly faster factor 
fourth identify tune higher level sparse kernels permit matrix reuse 
example applying register blocking sparse matrix multiple vector multiplication spm 
kernel exploited iterative solvers multiple right hand sides block 
itanium ultra observed speedups times spm single right hand side 
example computing ax information retrieval computing singular value decomposition entry twice 
gautam intel comments memory bandwidth issues 
barrett berry chan demmel donato dongarra der vorst 
templates solution linear systems building blocks iterative methods nd edition 
siam philadelphia pa 
bik 
automatic nonzero structure analysis 
siam journal computing 
bilmes chin demmel 
optimizing matrix multiply portable high performance ansi coding methodology 
proceedings international conference supercomputing vienna austria july 
acm 
see www icsi berkeley edu bilmes 
corliss demmel dongarra duff hammarling henry hu kahan kaufman krogh li whaley von 
document basic linear algebra subprograms blas standard blas technical forum 
www netlib org blast 
barrett dongarra 
matrix market web resource test matrix collections 
editor quality numerical software assessment enhancement pages london 
chapman hall 
math nist gov 
browne dongarra garner london 
scalable infrastructure application performance tuning hardware counters 
proceedings supercomputing november 
carr kennedy 
compiler numerical algorithms 
proceedings supercomputing pages 
chatterjee parker lebeck 
exact analysis cache behavior nested loops 
proceedings acm sigplan conference programming language design implementation pages snowbird ut usa june 
davis 
uf sparse matrix collection 
www cise ufl edu research sparse matrices 
fraguela zapata 
memory hierarchy performance prediction sparse blocked algorithms 
parallel processing letters march 
ghosh martonosi malik 
cache equations compiler framework analyzing tuning memory behavior 
acm transactions programming languages systems 
gropp keyes smith 
realistic bounds implicit cfd codes 
proceedings parallel computational fluid dynamics pages 
alt mazurkiewicz 
fracture mechanics intel itanium architecture case study 
workshop epic architectures compiler technology acm micro austin tx december 
henry 
flexible high performance matrix multiply self modifying runtime code 
technical report tr university texas austin december 
heras perez rivera 
modeling improving locality irregular problems sparse matrix vector product cache memories case study 
europe pages 

im 
optimizing performance sparse matrix vector multiplication 
phd thesis university california berkeley may 

im yelick 
optimizing sparse matrix computations register reuse sparsity 
proceedings international conference computational science volume lncs pages 
springer may 
intel 
intel itanium processor manual software optimization november 
james white sadayappan 
improving performance sparse matrix vector multiplication 
proceedings international conference high performance computing 

memory bandwidth machine balance current high performance computers 
newsletter ieee technical committee computer architecture december 
tab computer org news dec dec htm 

stream measuring sustainable memory bandwidth high performance computers 
www cs virginia edu stream 
mckinley carr 
tseng 
improving data locality loop transformations 
acm transactions programming languages systems july 
navarro garc pey juan 
algorithms sparse matrix computations high performance workstations 
proceedings th acm international conference supercomputing pages pa usa may 
heath 
improving performance sparse matrix vector multiplication 
proceedings supercomputing 
pugh 
generation efficient code sparse matrix computations 
proceedings th workshop languages compilers parallel computing lncs august 

nist sparse blas user guide 
technical report nist 
gams nist gov 
saavedra barrera 
cpu performance evaluation execution time prediction narrow spectrum benchmarking 
phd thesis university california berkeley february 

relational approach automatic generation sequential sparse matrix codes 
phd thesis cornell university august 
jalby 
characterizing behavior sparse algorithms caches 
proceedings supercomputing 
toledo 
improving memory system performance sparse matrix vector multiplication 
proceedings th siam conference parallel processing scientific computing march 
whaley dongarra 
automatically tuned linear algebra software 
proc 

wolf lam 
data locality optimizing algorithm 
proceedings acm sigplan conference programming language design implementation june 
register blocking example implementation register blocked code 
bm number block rows number rows matrix bm 
dense sub blocks stored row major order 
void smvm int bm const int row start const int col idx const double value const double double int jj loop block rows bm register double register double jj row start jj row start jj col idx value value col idx value col idx value col idx value col idx register block sizes tables show platform matrix performance mflop register block sizes sparsity exhaustive best value yielding best performance performance 
sparsity heuristic value chosen new heuristic performance 
sparsity heuristic value chosen heuristic sparsity performance 
addition performance data annotated pairwise comparisons 
particular asterisk best performance indicates performance block size chosen heuristic best performance 
circle indicates performance block size chosen heuristic best performance 
shows performance implementation selected heuristic chosen heuristic 
sparsity sparsity sparsity exhaustive best heuristic heuristic block perf 
fill block perf 
fill block perf 
fill matrix size mflop ratio size mflop ratio size mflop ratio table register block sizes sun ultra 
aid pairwise comparisons data annotated follows 
asterisk indicates performance mflop best performance mflop circle indicates performance best indicates performance performance 
ultra heuristic selected block size best 
sparsity sparsity sparsity exhaustive best heuristic heuristic block perf 
fill block perf 
fill block perf 
fill matrix size mflop ratio size mflop ratio size mflop ratio table register block sizes intel pentium iii 
data annotated table 
pentium iii heuristic selected block size best 
sparsity sparsity sparsity exhaustive best heuristic heuristic block perf 
fill block perf 
fill block perf 
fill matrix size mflop ratio size mflop ratio size mflop ratio table register block sizes ibm power 
data annotated table 
power heuristic selected block size best 
sparsity sparsity sparsity exhaustive best heuristic heuristic block perf 
fill block perf 
fill block perf 
fill matrix size mflop ratio size mflop ratio size mflop ratio table register block sizes intel itanium 
data annotated table 
matrix heuristics chose implementation achieved best 
note fill ratio best block size nearly 

