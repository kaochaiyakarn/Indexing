adaptive bayesian logic programs kristian kersting luc de raedt institute computer science machine learning lab albert university georges geb freiburg brg germany informatik uni freiburg de 
order probabilistic logics combine rst order logic probabilistic knowledge representation 
context introduce continuous bayesian logic programs extend introduced bayesian logic programs deal continuous random variables 
bayesian logic programs tightly integrate de nite logic programs bayesian networks 
resulting framework nicely qualitative logical component quantitative probabilistic 
show quantitative component learned gradient maximum likelihood method 
years increasing interest integrating probability theory rst order logic leading di erent types rst order probabilistic logics 
streams concentrates rst order extensions bayesian networks aims integrating powerful popular knowledge representation frameworks bayesian networks rst order logic :10.1.1.25.2452
investigating state art stream cf 
important shortcomings mentioned techniques 
allow model continuous random variables logical languages allow functor symbols 
features highly desirable true rst order probabilistic logics 
real world domain including biology medicine nance involves continuous variables domains involving potentially nite number random variables occur quite naturally practice temporal processes requires functors model domain 
rst contribution continuous bayesian logic programs allow model nite domains functors continuous random variables 
semantics bayesian logic programs context discrete time stochastic processes 
argued discrete bayesian logic programs serve kind common kernel rst order extensions bayesian networks probabilistic understand continuous variable variable having compact interval domain :10.1.1.25.2452
discrete random variable countable domain 
discrete bayesian logic programs bayesian logic programs allowing discrete random variables see :10.1.1.25.2452
logic programs relational bayesian networks probabilistic relational models continuous bayesian logic programs novel 
generalize dynamic bayesian networks kalman lters hidden markov models second contribution addresses famous question numbers parameters quantitative aspects come 
far issue attracted attention context rst order extensions bayesian networks exception 
context rst time calculate gradient maximum likelihood estimation parameters bayesian logic programs 
gives rich class optimization techniques conjugate gradient possibility speed techniques em algorithm see 
proceed follows 
motivating continuous bayesian logic programs simpli ed example quantitative genetics section introduce continuous bayesian logic programs section 
section formulate likelihood parameters bayesian logic program data section gradient method nd parameters maximize likelihood 
discussing related section reporting experimental experiences section conclude 
assume familiarity logic programming see bayesian networks 
quantitative genetics natural domain bayesian logic programs help genetics 
family relationship forms basis dependencies random variables biological laws provide probability distribution 
genotype may best modeled discrete random variables phenotypes height person naturally represented continuous variables 
situations phenotypes uenced environmental continuous quantities amount nuclear radiation 
sub eld genetics deals continuous phenotype called quantitative genetics cf 

example consider simpli ed model inheritance heights persons 
height speci person interpreted continuous random variable having dom depends genotype discrete random variable 
genotype depends genotype mother father 
furthermore assume uenced heights mother father 
shows graph modelling described dependencies 
graph seen dependency structure bayesian network 
interested representing joint probability density function 
denote probability density probability distribution 
chain rule probability states xn set fx xn random variables 
known biological dependencies express conditional independency statements conditional independent psfrag replacements fig 

dependencies genetic domain 
height person depends heights mother father joint state 
chain rule biological laws follows 




de ne nite set densities possible joint value parents pa fh direct predecessors variable dependency graph take 
dom function cpd pa denotes conditional probability density pa 
call function probability density function pdf 
note upper types indicate variables logical sense 
representation far inherent propositional regularities intensionally represented 
describe family framework continuous bayesian logic programs aims intensionally representing regularities 
continuous bayesian logic programs bayesian logic program consist components rstly logical set bayesian clauses cf 
encodes assertions conditional independence equation secondly quantitative set conditional probability functions combining rules cf 
corresponding logical structure 
particular bayesian de nite clause expression form bayesian atoms bayesian atoms implicitly universally quanti ed 
de ne head body fa di erences bayesian logical clause atoms predicates arising bayesian associated domain dom 
furthermore logical notions carry bayesian logic programs 
uta john 
peter john 
uta 
peter 
john uta john uta peter john peter 
uta uta 
peter peter 
john john uta john uta peter john peter 
fig 

bayesian logic program essentially encodes bayesian network 
uta peter parents john 
bayesian logic program grounded version bayesian logic program 
speak bayesian predicates terms constants functors substitutions ground bayesian clauses instance consider bayesian clause dom ftrue falseg dom says height person depends height mother intuitively bayesian predicate generically represents set random variables 
precisely bayesian ground atom corresponds random variable dom dom 
long ambiguity occurs distinguish bayesian predicate atom corresponding logical predicate atom 
order represent probabilistic model associate bayesian clause probability density function cpd encoding head body 
generically represents conditional probability densities ground instances clause general may clauses clause corresponding substitutions ground clauses head head 
specify cpd cpd needs head body body 
standard solution obtain densities required called combining rules see functions map nite sets fp mg conditional probability functions combined conditional probability function fb fa assume bayesian predicate corresponding combining rule cr noisy case discrete random variables linear regression model case gaussian variables 
summarize bayesian logic program consists nite set bayesian clauses 
bayesian clause associated conditional probability function cpd bayesian predicate exactly associated combining rule cr 
declarative semantics bayesian logic programs annotated dependency graph 
dependency graph dg directed graph nodes correspond ground atoms herbrand model lh cf 

encodes directly uenced relation random variables lh edge node node exists clause substitution head body atoms appearing lh 
herbrand model lh consists proper random variables 
de ned logical de nite program cf 

point immediate consequence operator cf 
applied empty set 
node dg combined pdf associated result combining rule cr corresponding bayesian predicate applied set cpd head body lh 
dependency graph encodes similar bayesian networks independency assumption node independent non descendants joint state parents pa dependency graph 
program renders john independent uta joint state john uta peter uta john peter john 
assumption proposition holds proposition 
bayesian logic program 
ful lls lh dg acyclic usual graph theoretical sense node dg uenced nite set random variables speci es unique probability density lh 
proof sketch detailed proof see 
herbrand lh exists unique countable 
dg uniquely exists due condition combined pdf node dg computable 
furthermore condition total order dg exists see stochastic process lh 
inductions condition shows family nite dimensional distribution process projective cf 
see nite subset lh uniquely de ned dy 
preconditions kolmogorov theorem page hold follows speci es probability density function lh 
proves proposition total order induction refer speci total order dg 
program ful lling conditions called de ned consider programs rest 
think bayesian networks simple example de ned programs 
graphically represented dependencies encoded nite propositional bayesian logic program shown 
program encoding intensional regularities genetic domain 
interesting properties follow proof sketch 
assume clauses bayesian logic program range restricted variables appearing part clause appear condition part 
common restriction computational logic facts entailed program ground cf 

uta peter 
john peter 
uta 
peter 

uta uta 
peter peter 

fig 

bayesian logic program encoding example genetic domain 
interpreted bayesian logic program stochastic process 
places wider context cowell call highly structured stochastic systems cf 
bayesian logic programs represent discretetime stochastic processes exible manner 
known probabilistic frameworks dynamic bayesian networks rst order hidden markov models kalman lters special cases 
unique semantics pure discrete programs clear hybrid programs programs discrete continuous variables unique semantics :10.1.1.25.2452
proof indicates important support network concept 
support networks graphical representation nite dimensional distribution cf 
needed formulation likelihood function see answering probabilistic queries bayesian logic programs 
support network variable lh de ned induced subnetwork fxg fy lh uencing xg 
support network nite set fx lh union networks single consider de ned bayesian logic programs lh uenced nite subset lh 
provable support network nite set fx lh random variables nite bayesian network computable nite time 
support network models nite dimensional distribution speci ed interesting probabilistic density value subsets speci ed proofs ective inference procedure implementation prolog refer 
maximum likelihood estimation far assumed expert designs bayesian logic program 
case 
possessing necessary expertise knowledge 
access data 
focus classical maximum likelihood estimation mle method learn parameters associated probability density functions bayesian logic program 
bayesian logic program consisting bayesian clauses fd dm set data cases 
data case partially observed joint state variables lh 
examples data cases fm peter uta true peter john true uta peter fg peter uta peter john fh uta john stands unobserved state 
parameters ecting associated pdfs cpd constitute set version parameters set denoted likelihood probability observed data function unknown parameters pb search space spanned product space possible values seek nd max 
usually speci es density function countably nite set random variables compute considering dependency graph 
argued preceding section sucient consider support network random variables occurring compute 
remembering logarithm monotone seek nd max log pn words expressed original problem terms parameter mle problem bayesian networks 
need careful 
nodes hidden values observed furthermore noted depends data data cases determine sucient subnetwork dg calculate likelihood 
hand may ect generalization learned program hand similar situation unrolling dynamic bayesian networks recurrent neural networks 
learning setting mle parameters intensional rules 
assume observe john john holds 
case problematic estimate ml parameters john 
estimate ml parameters support network data cases intensional rules data long ambiguities occur distinguish parameters particular instance 
nodes observed simple counting ml parameter estimation bayesian networks see :10.1.1.112.8434
psfrag replacements am am am fig 

scheme decomposable combining rules 
rectangle corresponds ground instance bayesian clause cp 
de nition combining rule 
node deterministic node 
cases induce bayesian network variables data cases 
surprising sees learning setting probabilistic extension ilp setting learning interpretation 
discussion analogy learning intensional bayesian clauses parameters associated densities refer 
gradient maximize likelihood 
classical method nding maximum evaluation function gradient ascent known hill climbing 
computes gradient vector partial derivatives respect parameters pdfs point takes small step direction gradient point step size parameter 
algorithm converge local maximum small compute partial derivatives pn respect particular parameter sake simplicity assume decomposable combining rules rules expressed set separate deterministic nodes support network shown 
combining rule commonly employed bayesian networks noisy linear regression decomposable cp 

decomposable combining rules imply node exist clause substitution body lh head clause induce node nodes identical local structure associated pdfs parameters algorithm requires additional step 
sure cpd maps dom head dom body cpd du 
done renormalizing constrained surface step direction case general combining rules partial derivatives inner function computed 
may dicult possible close form 
identical subst 
cpd cpd 
example consider clause de ning nodes uta peter john 
situation dynamic bayesian networks parameters encode stochastic model state evolution appear times network 
adapt solution chain rule di erentiation dynamic bayesian networks 
simplicity current instantiation parameters write 
applying chain rule yields log pn subst 
support log pn refers grounding substitutions support true fhead body assuming data cases independently sampled distribution separate contribution di erent data cases partial derivative single ground instance resulting log pn log pn pn head body cpd cpd dom head dom body 
term pn exactly calculated kinds distributions 
stochastic simulation techniques markov chain monte carlo methods see appendix help 
solution restrict types random variable 
continuous bayesian networks gaussian distributions density functions conditional gaussian distributions 
done bayesian logic programs solution integrand equation closed form adapted inference engine bayesian logic programs get exact solution 
general integrals equation intractable 
stochastic simulation algorithms may solve problem 
nally state equations partial derivatives pure discrete programs 
doing steps equations noting densities distributions parameterized entries yields log pn cpd jk subst 
support pn head body uk cpd jk dom head dom body refer corresponding entries cpd cpd 
dicult adapt equations due space restrictions leave derivation equation 
basically derivation equation adapted notation 
table 
simpli ed skeleton algorithm adaptive bayesian logic programs 
function basic ablp returns modi ed bayesian logic program inputs bayesian logic program associated pdfs parameterized nite set data cases repeat 

set pdfs set evidence clause ground instance fhead body single parameter 

log pn 

return hybrid bayesian logic programs 
simpli ed skeleton gradient algorithm shown table 
related extent bayesian logic programs related bugs language aims carrying bayesian inference gibbs sampling 
uses concepts imperative programming languages loops model regularities probabilistic models 
relation bayesian logic programs bugs akin general relation logical imperative languages 
holds particular relational domains family relationships 
notion objects relations objects family trees hard represent 
furthermore single bugs program speci es probability density nite set random variables bayesian logic program represent distribution nite set random variables 
parameter estimation rst order probabilistic logics rely bayesian networks 
cussens investigates em methods estimate parameters stochastic logic programs :10.1.1.112.8434
sato shown ecient method em learning prism programs 
learning bayesian networks investigated uncertainty ai community see :10.1.1.112.8434
binder approach adapted results gradient method 
far ml parameter estimation rst order extensions bayesian networks 
koller pfe er adapt em algorithm probabilistic logic programs framework contrast bayesian logic programs sees ground atoms states random variables 
framework theoretically allow continuous random variables exists practical query answering procedure case best knowledge ngo haddawy give procedure variables having nite domains 
furthermore koller pfe er approach utilizes support networks requires intersection support networks data cases empty 
opinion cases restrictive case dynamic bayesian networks 
data cases ful ll property inference faster 
friedman concentrate learning probabilistic relational models framework combines entity relationship models bayesian networks 
adapt em algorithm estimate parameters learn structure techniques known bayesian network learning 
consider general problem setting hand entity relationship model lacks concept functors 
limited nite sets entities relations 
detailed discussion relations bayesian logic programs rst order extensions bayesian networks probabilistic logic programs relational bayesian networks probabilistic relational models refer 
related rst order extensions bayesian networks mainly di ers points underlying logical frameworks lack important knowledge representational features bayesian logic programs 
adapt em algorithm particularly easy implement 
problematic issues regarding speed convergence convergence local sub optimal maximum likelihood function 
di erent accelerations gradient discussed 
em algorithm dicult apply case general probability density functions relies computing sucient statistics cf 
:10.1.1.112.8434
experimental prospects experimental results summarized follows support network approximation entire likelihood equality constraints parameters speed learning gradient methods promising 
prove basic principle approach testing hill climbing algorithm simple model genetic domain 
generated data cases version program information expressed omitted 
describes family tree person 
associated probability functions cpd true cpd true cpd table denotes normal density mean variance 
learning task estimate function table mean cpd true true 

true false false true false false table 
probability density function experiments 
case true true heights parents taken account weighted sum 
constant addend mean rst normal densities denoted 
cpd starting 
iterations estimated parameters step size 
implementation obviously su ers known dependency chosen initial parameters xed step size 
investigate advanced gradient methods conjugate gradient 
conducted experiments learning weights sum function table 
algorithm converges weights summing local minima data generating model likelihood 
contributions 
introduced continuous bayesian logic programs 
argued basic query answering procedure discrete programs applicable ability represent intensional regularities variables continuous random variables reduces size modelled domains 
second addressed question numbers come showing compute gradient likelihood ideas known dynamic bayesian networks 
intensional representation bayesian logic programs compact representation speed learning provide generalization 
perform detailed comparison learning approach em algorithm 
accelerations em algorithm gradient interesting 
ultimate goal learning structure 
currently see looking combinations techniques known inductive logic programming re nement operators techniques scoring functions bayesian networks 
just ml parameter estimation basic technique structural learning bayesian networks basic technique structural learning bayesian logic programs 
acknowledgments authors stefan kramer manfred jaeger helpful discussions ideas 
anonymous reviewers helpful comments 

heinz bauer 

walter de gruyter berlin new york 
edition 

binder koller russell kanazawa 
adaptive probabilistic networks hidden variables 
machine learning pages 

cowell dawid lauritzen spiegelhalter 
probabilistic networks expert systems 
springer verlag new york 

cussens 
parameter estimation stochastic logic programs 
machine learning 
appear 

dean kanazawa 
probabilistic temporal reasoning 
proceedings seventh national conference arti cial intelligence aaai 

falconer 
quantitative genetics 
longman new york 
edition 

friedman getoor koller pfe er 
learning probabilistic relational models 
proceedings sixteenth international joint conferences arti cial intelligence ijcai 

gilks thomas spiegelhalter 
language program complex bayesian modelling 
statistician 

heckerman 
tutorial learning bayesian networks 
technical report msr tr microsoft research 

heckerman breese 
causal independence probability assessment inference bayesian networks 
technical report msr tr microsoft research 

jaeger 
relational bayesian networks 
proceedings thirteenth annual conference uncertainty arti cial intelligence uai 

kersting de raedt 
bayesian logic programs 
reports tenth international conference inductive logic programming ilp 
sunsite informatik rwth aachen de publications ceur ws vol 

kersting de raedt 
bayesian logic programs 
technical report university freiburg institute computer science april 

kersting de raedt 
combining inductive logic programming bayesian networks 
volume 

koller 
probabilistic relational models 
proceedings ninth international workshop inductive logic programming ilp 

koller pfe er 
learning probabilities noisy rst order rules 
proceedings fifteenth joint conference arti cial intelligence ijcai 

lloyd 
foundations logic programming 
springer berlin 
edition 

krishnan 
em algorithm extensions 
john sons 

muggleton 
stochastic logic programs 
de raedt editor advances inductive logic programming 
ios press 

ngo haddawy 
answering queries form context sensitive probabilistic knowledge bases 
theoretical computer science 

pearl 
reasoning intelligent systems networks plausible inference 
morgan kaufmann 
edition 

poole 
probabilistic horn abduction bayesian networks 
arti cial intelligence 

sato kameya 
viterbi algorithm em learning statistical abduction 
proceedings uai workshop fusion domain knowledge data decision support 

williams zipser 
gradient learning algorithms recurrent networks complexity 
back propagation theory architectures applications 
hillsdale nj erlbaum 
