covariance kernels bayesian generative models matthias seeger institute adaptive neural computation university edinburgh forrest hill edinburgh eh ql seeger dai ed ac uk propose framework mutual information kernels learning covariance kernels support vector machines gaussian process classi ers unlabeled task data bayesian techniques :10.1.1.19.8785
describe implementation framework uses variational bayesian mixtures factor analyzers order attack classi cation problems high dimensional spaces labeled data sparse unlabeled data abundant 
kernel machines support vector machines gaussian processes powerful frequently tools solving statistical learning problems 
kernel function encodes task prior knowledge bayesian manner 
propose framework mutual information mi kernels learning covariance kernels unlabeled task data bayesian techniques 
section introduces terms concepts 
discuss general ideas discriminative semi supervised learning kernel design context 
section de ne general framework give examples 
note fisher kernel special case mi kernel :10.1.1.19.8785:10.1.1.19.8785:10.1.1.44.7709:10.1.1.44.7709
mi kernels mixture models discussed detail 
section describe implementation mi kernel variational bayesian mixtures factor analyzers models show results preliminary experiments 
semi supervised classi cation problem labeled dataset xm unlabeled set xm training drawn unknown distribution labels observed 

typically jd small jd aim models bayesian way extracting posterior information information build covariance kernel plugged supervised kernel machine trained labeled data perform classi cation task 
simplicity discuss binary labels 
important distinguish clearly learning scenarios 
tting bayesian density estimation 
having chosen model family fp xj prior distribution parameters posterior distribution jd encodes information contains latent unobserved parameters learning scenario supervised classi cation kernel machine 
architectures model smooth latent function random process classi cation noise model 
covariance kernel speci es prior distribution process priori assumed gaussian process zero mean covariance function see details 
notation vectors matrices respectively 
prime denotes transposition 
diag matrix diagonal 
xj denotes gaussian density mean covariance matrix 
standard discriminative bayesian classi cation scenario unlabeled data 
straightforward modify scenario introducing concept conditional priors see 
discriminant model family fp tjx conditional prior wj allows encode prior knowledge assumptions information uences assumptions priori probabilities discriminants example occam priors expressing intuitive fact problems notion simplicity discriminant function depends strongly known input distribution 
problem general easy come useful conditional prior 
prior speci ed principle powerful techniques approximate bayesian inference developed supervised discriminative settings 
semi supervised techniques seen employing conditional priors include training feature selection clustering fisher kernel :10.1.1.19.8785:10.1.1.19.8785:10.1.1.44.7709:10.1.1.44.7709
probabilistic kernel technique fully speci ed covariance function depending 
problem nd covariance kernels gp priors favour discriminants sense compatible learned input distribution 
kernel techniques seen nonparametric smoothers prior assumption input points similar close distance labels latent outputs highly correlated 
generic way learning kernels unlabeled data learn distance input points information 
frequently assumption classi cation labels may depend cluster hypothesis assume discriminants decision boundaries lie clusters priori label clusters inconsistently 
general way encoding hypothesis learn distance consistent clusters points cluster closer distance points di erent clusters 
try embed learned distance approximately euclidean space learn mapping 
pairs natural kernel function exp 
follow simpler approach considering similarity measure practice computation hardly feasible powerful approximation techniques 
natural choice binary classi cation represent log odds log jx jx 
immediately gives rise covariance kernel having compute approximate euclidean embedding 
main aim construct kernels learned unlabeled data 
contrast task learning kernel labeled data somewhat simpler approached generic way start parametric model family fy interpretation models log odds log jx jx 
fitting models labeled data obtain posterior wjd 
natural covariance kernel problem simply dw say wjd approximation thereof 
obtain prior covariance kernel model larger kernel incorporates posterior information 
kernel proposed seen approximation approach 
mutual information kernels section introducing framework mutual information kernels 
mediator distribution parameters de ne joint distribution mediated sample mutual information distribution log called mutual information mi score 
concrete sense measures similarity respect generative process represented mediator distribution amount information share mediator variable 
note seen inner product space functions 
features weighted distribution represented likelihood possible models 
covariance kernels satisfy property positive de niteness mi score :10.1.1.19.8785
applying standard transformation called exponential embedding ee arrive ee familiar note transforms standard inner product known radial basis function rbf kernel kx comparing inner product interested correlating features uniformly focus regions high volume positive de nite matrix positive de nite set fx distinct points :10.1.1.19.8785
show kernel ee kernel see :10.1.1.21.2820:10.1.1.21.2820
weighted inner product dx squared exponential kernel 
easy show valid covariance kernel refer mutual information mi kernel 
example xj xj spherical gaussian mean 
mi kernel rbf kernel 
rbf kernel special case mi kernel 
mediator distribution 
model trust scaling 
mediator distribution motivated earlier section ideally encode information generation process just bayesian posterior jd 
hand need able control uence information sources unlabeled data kernel relying sources results lack robustness see details 
propose model trust scaling mts setting varies usually vague prior sharp posterior jd rendering information model uence kernel concrete ect mts kernel depends model family :10.1.1.19.8785
example continued xj xj prior mean 
jd :10.1.1.19.8785
mi kernel rbf kernel 
exible model xj xj conjugate je reys prior mi kernel computed :10.1.1.19.8785:10.1.1.28.850
bayesian analysis done conjugate prior model pairs corresponding mi kernel computed easily cases mts simple analytic form see :10.1.1.19.8785:10.1.1.28.850
general approximation techniques developed bayesian analysis applied 
example applying laplace approximation computations model prior results fisher kernel see :10.1.1.19.8785:10.1.1.19.8785:10.1.1.19.8785:10.1.1.28.850:10.1.1.44.7709:10.1.1.44.7709
favour approximation technique see section 
mutual information kernels mixture models apply mi kernel framework mixture models xj xj run problem 
mentioned section kernel partly encode cluster hypothesis small come di erent clusters opposite true small inner product kernel rest argument see section :10.1.1.19.8785:10.1.1.21.2820:10.1.1.21.2820
essentially observed authors workshop talks published knowledge :10.1.1.19.8785:10.1.1.19.8785:10.1.1.44.7709:10.1.1.44.7709
fascinating idea fisher kernel main motivation inspiration 
mean priori believe di erent labels label better latent 
depend strongly 

overcome problem generalize symmetric nonnegative entries positive elements diagonal 
mi kernel de ned new true cases interested see original mi kernel arises special case 
choosing diag arrive mi kernel typically behaves expected cluster separation see gure exhibit long range correlations joined components 
restrict diagonal mixture kernel 
note kernel seen normalized mixture mi kernels component models 
lambda lambda lambda kernel contours cluster dataset shows contour plots diagonal mixture kernel vb see section learned cases dataset sampled gaussians equal covariance see subsection :10.1.1.19.8785
plot xed marked cross height contour lines 
left middle plot lower cluster centre respectively right plot lies cluster centres :10.1.1.19.8785
ect mts seen comparing left middle note di erent sharpness slopes cluster di erent sizes shapes high correlation regions 
seen right points clusters highest correlation inter cluster points feature may useful successful discrimination 
experiments mixtures factor analyzers section describe implementation mi kernel variational bayesian mixtures factor analyzers vb density models 
able combine local dimensionality reduction noisy linear transformations low dimensional latent spaces global data mixtures 
vb variational approximation bayesian analysis models able deliver posterior approximations require mi kernel 
employ diagonal mixture kernel see subsection 
implementing mts analytically compute vb approximation true posterior simply apply scaling distribution 
factorizes required subsection 
integrals produced rst order approximation see mi kernel 
plots step variational approximation see somewhat richer structure 
analytically tractable 
rst idea approximate applying vb technique called step variational approximations 
unfortunately mi kernel approximation terms shown positive de nite anymore moment elegant feel accurate approximation details rst order taylor expansions :10.1.1.19.8785:10.1.1.28.850
remainder section compare vb kernel rbf kernel datasets laplace gp classi er see 
case sample training pool kernel dataset test set mutually exclusive 
vb diagonal mixture kernel learned training set size run consists sampling training set holdout set size training pool tuning kernel parameters validation testing test set 
kernels 
training set size runs 
results plotting means test con dence intervals test errors runs :10.1.1.19.8785
gaussian clusters dataset sampled gaussians non spherical covariance see gure class bayes error 
points training pool test set points :10.1.1.19.8785
learning curves gure show simple toy problem tted vb model represents cluster structure perfectly vb mi kernel outperforms rbf kernel samples sizes 
training set size test error rbf training set size test error vb mi learning curves cluster dataset :10.1.1.19.8785
left rbf kernel right mi kernel handwritten digits mnist report results preliminary experiments subset mnist handwritten digits database jd training pool contains test set cases 
employ vb model components tted simple baseline bl algorithm see section component densities vb model anonymous reviewer pointing aw 
images downsampled size 
estimates xjs obtained integrating parameters variational posterior approximation 
integral analytic step variational approximation 
allows assess purity component clusters labels algorithm kernel 
furthermore show results step variational approximation mi kernel :10.1.1.19.8785
learning curves shown gure 
training set size test error rbf training set size training set size test error vb mi training set size test error vb mi learning curves mnist :10.1.1.19.8785
upper left rbf kernel upper middle baseline method upper right vb mi kernel rst order approx lower left vb mi kernel step var 
approx 
results disappointing 
fact rst order approximation mi kernel performs worse step variational approximation may fail positive de nite indicates poorer approximation 
renders results close baseline method smoothing rbf kernel better growing number labeled examples indicates conditional prior represented vb mi kernel behaves nonsmooth overrides label information regions 
suspect problem related high dimensionality input space case probability densities tend large dynamic range mixture component responsibility estimates tend behave nonsmooth 
necessary extend basic mi kernel framework new scaling mechanisms order produce smoother encoding prior assumptions 
baseline algorithm assumption component index input point label independent 
conditional probabilities tjs learned xjs obtained vb model tted unlabeled data 
success failure method closely related degree purity component clusters labels 
somewhat inconsistent kernel function positive de nite context gp classi cation requires covariance function :10.1.1.19.8785
note rbf kernel matrices evaluated signi cantly faster vb mi kernel 
related 
discussion probably closely related fisher kernel see subsection 
arguments concerning mixture models see subsection apply 
haussler contains wealth material kernel design discrete objects watkins mentions expressions valid kernels discrete countable parameter spaces :10.1.1.21.2820:10.1.1.21.2820
came essentially describes special case diagonal mixture kernel see subsection gaussian components diagonal covariances author calls stochastic equivalence predicate 
interested distance learning apply method kernel machines give bayesian interpretation 
general framework kernel learning unlabeled data described approximate implementation vb models 
straightforward application technique high dimensional real world data prove successful explore new ideas extending basic mi kernel framework order able deal high dimensional input spaces 
acknowledgments chris williams inspiring discussions furthermore ralf herbrich amos hugo zaragoza neil lawrence 
matt beal helped lot vb 
author gratefully acknowledges support research studentship microsoft research avrim blum tom mitchell 
combining labeled unlabeled data cotraining 
proceedings colt 
ghahramani beal 
variational inference bayesian mixtures factor analysers 
advances nips 
mit press 
david haussler :10.1.1.21.2820:10.1.1.21.2820
convolution kernels discrete structures 
technical report university california santa cruz july 
tommi jaakkola david haussler :10.1.1.19.8785:10.1.1.19.8785:10.1.1.44.7709:10.1.1.44.7709
exploiting generative models discriminative classi ers 
advances neural information processing systems 
matthias seeger :10.1.1.19.8785:10.1.1.28.850
covariance kernels bayesian generative models 
technical report 
available www dai ed ac uk seeger papers html 
matthias seeger 
learning labeled unlabeled data 
technical report 
available www dai ed ac uk seeger papers html 
martin szummer tommi jaakkola 
partially labeled classi cation markov random walks 
advances nips 
mit press 
tsuda gunnar sonnenburg uller 
new discriminative kernel probabilistic models 
advances nips 
mit press 
chris watkins 
dynamic alignment kernels 
technical report csd tr royal holloway university london 
christopher williams david barber 
bayesian classi cation gaussian processes 
ieee trans 
pami :10.1.1.19.8785
peter yianilos 
metric learning normal mixtures 
technical report nec research princeton :10.1.1.19.8785
parameter related mts case 
