playing believing role beliefs multi agent learning yu han chang artificial intelligence laboratory massachusetts institute technology cambridge massachusetts ai mit edu leslie pack kaelbling artificial intelligence laboratory massachusetts institute technology cambridge massachusetts ai mit edu propose new classification multi agent learning algorithms league players characterized possible strategies possible beliefs 
classification review optimality existing algorithms including case play 
propose incremental improvement existing algorithms achieve average payoffs nash equilibrium payoffs fair opponents 
topic learning multi agent environments received increasing attention past years 
game theorists begun examine learning models study repeated games reinforcement learning researchers begun extend learning models multiple agent case 
traditional models methods fields adapted tackle problem multi agent learning central issue optimality worth revisiting 
expect successful learner 
matrix games nash equilibrium 
game theory perspective repeated game generalization traditional shot game matrix game 
matrix game defined reward matrix player set actions available player purely competitive games called zero sum games satisfy player simultaneously chooses play particular action mixed policy pd probability distribution possible actions receives reward joint action taken 
common examples single shot matrix games shown 
traditional assumption player prior knowledge player 
standard game theory literature reasonable assume opponent fully rational chooses actions best interest 
return play best response opponent choice action 
best response function player br defined set optimal policies player players playing joint policy br set possible policies agent players playing best responses players strategies br matching pennies rock scissors hawk dove prisoner common examples single shot matrix games 
game said nash equilibrium 
players playing nash equilibrium single player incentive unilaterally deviate equilibrium policy 
game solved nash equilibria quadratic programming player choose optimal strategy fashion prior knowledge game structure 
problem arises multiple nash equilibria 
players manage coordinate equilibrium joint policy may worse 
hawk dove game shown example problem 
nash equilibria occur players coordinate may playing joint action receive reward 
stochastic games reinforcement learning 
despite problems general agreement nash equilibrium appropriate solution concept shot games 
contrast repeated games range different perspectives 
repeated games generalize shot games assuming players repeat matrix game time periods 
researchers reinforcement learning view repeated games special case stochastic markov games 
researchers game theory hand view repeated games extension theory shot matrix games 
resulting frameworks similar key difference treatment game history 
reinforcement learning researchers focus attention choosing single stationary policy maximize learner expected rewards time periods time max may finite infinite pd 
infinite time horizon case include discount factor 
littman analyzes framework zero sum games proving convergence nash equilibrium minimax algorithm playing minimax agent 
claus boutilier examine cooperative games hu wellman focus general sum games 
algorithms share common goal finding playing nash equilibrium 
littman hall greenwald extend approach consider variants nash equilibrium convergence guaranteed 
bowling veloso propose relax mutual optimality requirement nash equilibrium considering rational agents learn play stationary best response opponent strategy opponent playing equilibrium strategy :10.1.1.11.7195
motivation allows agents act rationally opponent acting rationally physical computational limitations 
fictitious play similar algorithm game theory 
game theoretic perspective repeated games 
alluded previous section game theorists take general view optimality repeated games 
key difference treatment history actions taken game 
recall table summary multi agent learning algorithms new classification 
minimax nash bully godfather learning wolf phc fictitious play assumes public knowledge opponent policy period stochastic game model took pd 
redefine pd set possible histories length histories observations joint actions 
player strategy time expressed 
essence endowing agent memory 
agent ought able form beliefs opponent strategy beliefs ought converge opponent actual strategy sufficient learning time 
pd player belief opponent strategy 
learning path defined sequence histories beliefs personal strategies 
define nash equilibrium repeated game terms personal strategy beliefs opponent 
prediction opponent strategy accurate choose appropriate best response strategy 
holds players game guaranteed nash equilibrium 
proposition 
learning path 
converges nash equilibrium iff conditions hold optimization br 
play best response prediction opponent strategy 
prediction lim 
time belief opponent strategy converges opponent actual strategy 
shows requirement simultaneous prediction optimization impossible achieve certain assumptions possible strategies possible beliefs 
design agent learn predict opponent strategy optimize beliefs time 
despite fact assume extra knowledge opponent design algorithm approximates best response stationary policy time opponent 
game theory literature concept called universal consistency 
levine freund schapire independently show algorithm exhibits universal consistency game theory machine learning perspectives 
give strong result requires strong assumption know opponent policy time period 
typically case 
new classification new algorithm propose general classification categorizes algorithms cross product possible strategies possible beliefs opponent strategy hb 
agent possible strategies classified amount history memory memory agent formulate complex policies policies maps histories action distributions 
agents memoryless play stationary policies 
agents recall actions previous time period classified execute reactive policies 
extreme agents unbounded memory formulate complex strategies game played time 
agent belief classification mirrors strategy classification obvious way 
agents believe opponent memoryless classified players players believe opponent bases strategy previous periods play forth 
explicitly stated existing algorithms assumptions hold beliefs types possible opponents world 
think different league players players league roughly equal terms capabilities 
clearly leagues contain capable players 
define fair opponent opponent equal lesser league 
idea new learning algorithms ideally designed beat fair opponent 
key role beliefs 
league assume players fully rational sense fully available histories construct policy 
important observation definition full rationality depends beliefs opponent 
believe opponent memoryless player player fully rational strategy simply model opponent stationary strategy play stationary best response 
belief capacity history capacity inter related 
rich set possible beliefs opponent available history 
similarly obviously rich set historical observations hope model complex opponents 
discussion current algorithms 
existing algorithms fall league 
discussed previous section problem players full access history fully rational strategy stationary due limited belief set 
general example player policy hill climber phc 
maintains policy updates policy history attempt maximize rewards 
originally phc created stochastic games policy depends current state repeated games state 
agent policy hill climbing phc proceeds follows 
learning rates 
initialize 
repeat state select action mixed policy exploration 
observing reward state update max 
update constrain legal probability distribution argmax basic idea phc values help define gradient execute hill climbing 
bowling veloso wolf phc modifies phc adjusting depending agent winning losing :10.1.1.11.7195
true league phc players play stationary opponents 
opposite spectrum littman stone propose algorithms leader strategies sense choose fixed strategy hope opponent follow learning best response fixed strategy 
bully algorithm chooses fixed memoryless stationary policy godfather memory time period 
opponents included normal learning players similar learners explicitly learn period memory believe opponent memory learn 
interesting result godfather able achieve non stationary equilibria repeated prisoner game rewards players higher stationary nash equilibrium rewards 
demonstrates power having belief models 
algorithms access period history attempt construct statistical models opponent 
godfather works built best response learners attempting learn best response experience 
hu wellman nash littman minimax classified players attempt learn nash equilibrium experience play fixed equilibrium learned 
furthermore assume opponent plays fixed stationary nash equilibrium hope half equilibrium strategy 
algorithms summarized table 
new class players 
discussed existing algorithms form beliefs opponent approaches able capture essence game playing world threats generally opponent 
wish open door possibilities designing learners model opponent information achieve better rewards 
ideally design algorithm able win come equilibrium fair opponent 
impossible start proposing algorithm league plays restricted class opponents 
current algorithms best response players choose opponent class phc example best response player demonstrate algorithm beats phc opponents fact existing fair opponents 
new algorithm phc 
algorithm different previous explicitly modeling opponent learning algorithm simply current policy 
particular model players rational construct models believe opponent learning adapting time history 
idea fool opponent thinking stupid playing decoy policy number time periods switch different policy takes advantage best response decoy policy 
learning perspective idea adapt faster opponent fact switch away decoy policy adjustment new policy immediate 
contrast opponent adjusts policy small increments furthermore unable model changing behavior 
repeat bash cycle ad infinitum achieving infinite total rewards 
opponent catches believes play stationary policies 
example player phc 
bowling veloso showed restricted version wolf phc reaches stationary nash equilibrium player action games general wolf phc experimental trials 
long run wolf phc player achieves stationary nash equilibrium payoff phc player 
wish better exploiting knowledge phc opponent learning strategy 
construct phc algorithm agent proceeds phc steps continues follows observing action time update history calculate estimate opponent policy window estimation opponent action time equal 
estimate similarly 
update estimating learning rate phc opponent update 
winning update argmax losing update argmax note derive opponent learning rate opponent policy estimates observable history actions 
assume game matrix public information solve equilibrium strategy run wolf phc finite number time periods obtain estimate equilibrium strategy 
main idea algorithm take full advantage time periods winning 
analysis 
phc algorithm phc exhibits behavior phc games single pure nash equilibrium 
agents generally converge single pure equilibrium point 
interesting case arises competitive games equilibria require mixed strategies discussed singh bowling veloso :10.1.1.11.7195
matching pennies shown game 
phc able model opponent learning algorithm choose better actions 
full knowledge case know opponent policy learning rate time period prove phc learning algorithm guarantee unbounded reward long run playing games matching pennies 
proposition 
zero sum game matching pennies nash equilibrium requires mixed strategies phc able achieve unbounded rewards phc opponent play follows cycle defined segments shown 
play proceeds cw jumps follows line segments jumps back 
point graph probability player plays heads know expected reward 
player probability choosing heads action distribution agent system player probability choosing heads action distribution agent system agent winning agent losing theoretical left empirical right 
cyclic play evident empirical results play phc player phc player 
time period agent total reward time total rewards agent increase gain reward cycle 
wish show dt cw dt dt consider part separately 
losing section 
dt dt similarly show receive reward cw dt shown receive payoff greater nash equilibrium payoff zero cycle 
easy see play follow cycle approximation depending size section demonstrate estimate sufficiently past observations eliminating full knowledge requirements ensure cyclic nature play 
experimental results 
phc algorithm described play phc variants different iterated matrix games including matching pennies prisoner rock scissors 
give results matching pennies game analyzed playing wolf phc 
window time periods estimate opponent current policy opponent learning rate shown play exhibits cyclic nature predicted 
solid vertical lines indicate periods phc player winning dashed roughly diagonal lines indicate periods losing 
analysis previous section derived upper bound total rewards time time step 
estimate various parameters experimental run achieve level reward 
gain average total reward time period 
plots total reward phc agent time 
periods winning losing clear graph 
experiments tested effectiveness phc fair opponents including 
existing fair opponents shown table achieved average equilibrium payoff long run 
surprisingly posted score played multiplicative weight learner 

new classification multi agent learning algorithms suggested algorithm dominate existing algorithms fair opponent leagues playing certain games 
ideally create algorithm league provably dominates larger classes fair opponents game 
discussion contained dealt case iterated matrix games 
extend framework general stochastic games multiple states multiple players 
interesting find practical applications multi agent learning algorithms 

supported part graduate research fellowship national science foundation 
michael littman 
markov games framework multi agent reinforcement learning 
proceedings th international conference machine learning icml 
caroline claus craig boutilier 
dynamics reinforcement learning cooperative systems 
proceedings th natl 
conf 
artificial intelligence 
hu michael wellman 
multiagent reinforcement learning theoretical framework algorithm 
proceedings th int 
conf 
machine learning icml 
michael littman 
friend foe learning general sum games 
proceedings th int 
conf 
machine learning icml 
keith hall amy greenwald 
correlated learning 
dimacs workshop computational issues game theory mechanism design 
michael bowling manuela veloso :10.1.1.11.7195
multiagent learning variable learning rate 
submission 
shin ishii kenji doya 
multi agent reinforcement learning approach agent internal model 
proceedings international conference multi agent systems icmas 
drew david levine 
consistency cautious fictitious play 
journal economic dynamics control 

non computable strategies discounted repeated games 
economic theory 
yoav freund robert schapire 
adaptive game playing multiplicative weights 
games economic behavior 
michael littman peter stone 
leading best response repeated games 
th int 
joint conf 
artificial intelligence ijcai workshop economic agents models mechanisms 
singh kearns mansour 
nash convergence gradient dynamics general sum games 
proceedings th conference uncertainty artificial intelligence 
