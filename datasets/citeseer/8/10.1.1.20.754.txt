incorporating invariances nonlinear support vector machines olivier chapelle olivier chapelle lip fr lip paris france technologies bernhard sch olkopf bernhard tuebingen mpg de max planck institute germany technologies choice svm kernel corresponds choice representation data feature space improve performance incorporate prior knowledge known transformation invariances 
propose technique extends earlier aims incorporating invariances nonlinear kernels 
show digit recognition task proposed approach superior virtual support vector method previously method choice 
classi cation tasks priori knowledge known invariances related task 
instance image classi cation know label image change small translation rotation 
generally assume know local transformation depending parameter instance vertical translation pixels point considered equivalent transformed point 
ideally output learned function constant inputs transformed desired invariance 
shown nd non trivial kernel globally invariant 
reason consider local invariances purpose associate training point tangent vector dx dx lim practice dx computed nite di erence di erentiation 
note generally consider invariance transformation 
common way introducing invariances learning system add perturbed examples training set 
points called virtual examples 
svm framework applied svs leads virtual support vector vsv method 
alternative modify directly cost function order take account tangent vectors 
successfully applied neural networks linear support vector machines 
aim extend methods case nonlinear svms achieved mainly kernel pca trick 
organized follows 
introducing basics support vector machines section recall method proposed train invariant linear svms section 
section show extend nonlinear case nally experimental results provided section 
support vector learning introduce standard notations svms complete description see 
set training examples belonging classes labeled 
kernel methods map vectors feature space kernel function de nes inner product feature space 
decision function svm maximal margin hyperplane space sign coecients obtained maximizing functional constraints formulation svm optimization problem called hard margin formulation training errors allowed 
rest consider called soft margin svm algorithm training errors allowed :10.1.1.15.9362
invariances linear svms linear svms wants nd hyperplane normal vector orthogonal possible tangent vectors 
easily understood equality dx 
dx purpose suggested minimize functional 
dx subject constraints 
parameter trades normal svm training full enforcement orthogonality hyperplane invariance directions 
introduce dx dx square root regularized covariance matrix tangent vectors 
shown training linear invariant svm minimizing equivalent standard svm training linear transformation input space method led signi cant improvements linear svms small improvements linear preprocessing step nonlinear svms 
hybrid system unclear theoretical foundations 
section show deal nonlinear case principled way 
extension nonlinear case nonlinear case data rst mapped high dimensional feature space linear decision boundary computed 
extend directly previous analysis nonlinear case need compute matrix feature space new kernel function 
due high dimension feature space impossible directly 
propose di erent ways overcoming diculty 
decomposition tangent gram matrix order able compute new kernel propose diagonalize matrix eq similar approach kernel pca trick 
article showed possible diagonalize feature space covariance matrix computing eigendecomposition gram matrix points 
presently having set training points set tangent vectors fd tangent covariance matrix right term sum introduce gram matrix tangent vectors ij 
dx dx dx dx dx dx matrix computed nite di erences equation analytical derivative expression equation 
note linear kernel reads ij dx dx standard dot product tangent vectors 
writing eigendecomposition kernel pca tools show algebra details new kernel matrix reads ip dx ip dx kernel pca map drawback previous approach appears wants deal multiple invariances tangent vector training point 
requires diagonalize matrix cf eq size equal number di erent tangent vectors 
reason propose alternative method 
idea directly called kernel pca map rst introduced extended 
map fact high dimensional feature space training set fx xn size mapped feature space spans subspace dimension precisely vn orthonormal basis principal axis xn kernel pca map de ned coordinatewise 
principal direction linear expansion training points coecients expansion obtained kernel pca 
writing orthonormal matrix diagonal turns kernel pca map reads xn note de nition lie 


re ects fact retain principal components kernel pca just basis transform leaving dot product training points invariant 
consequence training nonlinear svm fx xn equivalent training linear svm xn nonlinear mapping directly linear space exactly technique described invariant linear svms section 
invariance directions necessarily belong projecting information lost 
hope approximation give similar decision function exact obtained section 
proposed algorithm consists training invariant linear svm described section training set xn invariance directions fd xn dx expressed equation dx jp comparisons vsv method wonder di erence enforcing invariance just adding virtual examples training set 
approaches related equivalence shown 
just add virtual examples idea virtual support vector vsv method 
reason training point far margin adding virtual example change decision boundary points support vector 
adding virtual examples svm framework enforces invariance decision boundary aside main reason virtual sv method adds virtual examples generated points support vectors earlier iteration 
argue points far decision boundary provide information anyway 
hand merit keeping output label invariant transformation real valued output 
justi ed seeing distance point margin indication class conditional probability 
appears reasonable invariance transformation ect probability 
experiments experiments compared standard svm methods account invariances standard svm virtual examples cf 
vsv method vsv invariant svm described section invariant hyperplane kernel pca coordinates described section 
hybrid method described see section perform better vsv method included experiments reason 
note experiments tangent vector normalized average length pp jj order scale independent 
toy problem toy problem considered training data generated uniformly true decision boundary circle centered origin sign priori knowledge want encode toy problem local invariance rotations 
output decision function training point image obtained small rotation similar possible 
training point associate tangent vector dx orthogonal training set points generated experiments repeated times 
gaussian kernel exp jjx yjj chosen 
results summarized gure 
adding virtual examples vsv method useful test error decrease best choice 
yields better performance 
toy problem invariances enforced better performances see right side gure reaching test error 
comparing log log right side gure notices decrease test error speed 
dual phenomenon observed left side gure value gamma test error tends increase larger 
analysis suggests needs adapted function 
done automatically gradient descent technique described 
log sigma svm vsv log gamma log sigma log sigma log sigma left test error di erent learning algorithms plotted width rbf kernel xed 
right test error di erent values 
test errors averaged splits error bars correspond standard deviation means 
handwritten digit recognition real world experiment tried incorporate invariances handwritten digit recognition task 
usps dataset extensively past purpose especially svm community 
consists training test examples 
best performance obtained polynomial kernel degree results described section performed kernel 
local transformations considered translations horizontal vertical 
tangent vectors computed nite di erence original digit pixel translated 
split training set subsets training examples random permutation training test set 
concentrated binary classi cation problem separating digits 
gain performance valid multiclass case 
compares vsv di erent values gures seen di erence original method approximation larger toy example 
di erence toy example probably due input dimensionality 
dimensions rbf kernel examples toy problem span feature space dimensions longer case 
noteworthy experiments proposed method better standard vsv 
explained section reason invariance enforced training points support vectors 
note call vsv standard svm double size training set containing original data points translates 
horizontal invariance yields larger improvements vertical 
reason digits usps database centered vertically 
log gamma vsv log gamma vsv vertical translation top horizontal translation right comparison vsv usps dataset 
left plot corresponds standard svm right part plot means lot emphasis put enforcement constraints 
test errors averaged splits error bars correspond standard deviation means 
extended method constructing invariant hyperplanes nonlinear case 
shown results superior virtual sv method 
broken record nist database gold standard handwritten digit benchmarks appears promising try new system task 
propose large scale version method needs derived 
rst idea tried compute kernel pca map subset training points 
encouraging results obtained class usps database training set methods currently study 
burges 
geometry invariance kernel methods 
sch olkopf burges smola editors advances kernel methods support vector learning 
mit press 
chapelle sch olkopf 
incorporating invariances nonlinear support vector machines 
www connex lip fr chapelle 
chapelle vapnik bousquet mukherjee 
choosing multiple parameters support vector machines 
machine learning 
cortes vapnik :10.1.1.15.9362
support vector networks 
machine learning 
decoste sch olkopf 
training invariant support vector machines 
machine learning 
press 
todd leen 
data distributions regularization invariant learning 
nips volume 
mit press 
niyogi poggio girosi 
incorporating prior information machine learning creating virtual examples 
ieee proceedings intelligent signal processing november 
john platt 
probabilities support vector machines 
smola bartlett sch olkopf schuurmans editors advances large margin classi ers 
mit press cambridge ma 
sch olkopf burges vapnik 
extracting support data task 
fayyad uthurusamy editors international conference knowledge discovery data mining 
aaai press 
sch olkopf burges vapnik 
incorporating invariances support vector learning machines 
arti cial neural networks icann volume pages berlin 
springer lecture notes computer science 
sch olkopf simard smola vapnik 
prior knowledge support vector kernels 
mit press editor nips volume 
sch olkopf smola 
uller 
nonlinear component analysis kernel eigenvalue problem 
neural computation 
simard lecun denker 
transformation invariance pattern recognition tangent distance tangent propagation 
orr muller editors neural networks tricks trade 
springer 
tsuda 
support vector classi er asymmetric kernel function 
verleysen editor proceedings esann pages 
vapnik 
statistical learning theory 
john wiley sons 
