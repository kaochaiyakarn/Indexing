minimax probability machine gert lanckriet department eecs university california berkeley berkeley ca gert eecs berkeley edu laurent el ghaoui department eecs university california berkeley berkeley ca eecs berkeley edu bhattacharyya department eecs university california berkeley berkeley ca eecs berkeley edu michael jordan computer science statistics university california berkeley berkeley ca jordan cs berkeley edu constructing classi er probability correct classi cation data points maximized 
current desideratum translated direct way optimization problem solved methods convex optimization 
show exploit mercer kernels setting obtain nonlinear decision boundaries 
worst case bound probability misclassi cation data obtained explicitly 
consider problem choosing linear discriminant minimizing probabilities data vectors fall wrong side boundary 
way attempt achieve generative approach distributional assumptions class conditional densities estimates controls relevant probabilities 
need distributional assumptions casts doubt generality validity approach discriminative solutions classi cation problems common attempt dispense class conditional densities entirely 
avoiding class conditional densities useful attempt control misclassi cation probabilities worst case setting possible choices class conditional densities 
minimax approach viewed providing alternative justi cation discriminative approaches 
show minimax programme carried setting binary classi cation 
approach involves exploiting powerful theorem due extended bertsimas robotics eecs berkeley edu gert sethuraman sup bg inf random vector constants supremum taken distributions having mean covariance matrix theorem provides ability bound probability misclassifying point making gaussian speci distributional assumptions 
show exploit ability design linear classi ers 
appealing features formulation obtains explicit upper bound probability misclassi cation data 
second appealing feature approach linear discriminant analysis possible generalize basic methodology utilizing mercer kernels forming nonlinear decision boundaries 
show section 
organized follows section minimax formulation linear classi ers section deal method 
empirical results section 
maximum probabilistic decision hyperplane section minimax formulation linear decision boundaries 
denote random vectors binary classi cation problem mean vectors covariance matrices respectively means random variable speci ed mean covariance matrix distribution unconstrained 
note want determine hyperplane separates classes points maximal probability respect distributions having means covariance matrices 
boils max inf bg inf bg max sup bg sup bg consider second constraint 
recall result bertsimas sethuraman sup bg inf write inf solve rst notice assume classi ed correctly decision hyperplane nd particular optimal value 

form lagrangian maximized respect minimized respect optimum dc yields inf second constraint 
account boils handle rst constraint similar way just write apply result second constraint 
optimization problem max xa ya monotone increasing function write max xa ya constraints get ya allows eliminate max want maximize obvious inequalities equalities optimum 
optimal value xa ya optimal values respectively 
rearranging constraint get xa ya positively homogeneous satis es sa 
furthermore implies 
restrict 
optimization problem max xa ya allows eliminate min equivalently min ak ak convex optimization problem precisely second order cone program socp 
furthermore notice write fu yk orthogonal matrix columns span subspace vectors orthogonal write unconstrained socp min fu fu solve problem various ways example interior point methods socp yield worst case complexity 
course rst second moments estimated data example plug estimates respectively brings total complexity ln number data points 
complexity quadratic programs solve support vector machines 
implementations took iterative squares approach form equivalent min fu fu iteration rst minimize respect setting fu fu minimize respect solving squares problem gives update steps objective cop increase iteration converge global minimum fu fu optimal value obtain fu 
classi cation new data point new done evaluating sign new classi ed class new classi ed class interesting see happens distributional assumptions particular assume 
leads optimization problem max xa ya cumulative distribution function standard normal gaussian distribution 
form cf 
result cherno 
solve optimization problem disappears optimization problem monotone increasing nd decision hyperplane di erence lies value associated higher case hyperplane higher predicted probability classifying data correctly 
section describe minimax approach described previous section 
seek map problem higher dimensional feature space mapping 
linear discriminant feature space corresponds nonlinear discriminant original space 
carry programme need try reformulate minimax problem terms kernel function satisfying mercer condition 
data mapped 

fx nx fy ny training data points classes corresponding respectively 
decision hyperplane need solve optimization problem min optimal value optimal values respectively 
wish solve cop form want avoid explicitly 
component orthogonal subspace spanned component won ect objective constraint 
implies write nx ny substituting expression estimates nx nx ny ny nx nx ny ny means covariance matrices objective constraint optimization problem see objective constraints written terms kernel function 
obtain min kx ky kx ky 
nx 
ny kx nx ny kx nx nx ky nx ny ny ny nx de ned kx nx ky ny kx ky column vector ones dimension ky contain respectively rst rows rows gram matrix de ned ij 
write min kx ky kx ky second order cone program socp form socp solved similar way 
notice case optimizing variable nx ny dimension optimization problem increases solution powerful corresponds complex decision boundary similarly optimal value optimal values respectively 
known get nx ny 
classi cation new data point new done evaluating sign new sign nx ny new terms kernel function new classi ed class classi ed class experiments section report results experiments carried test algorithmic approach 
validity worst case bound probability misclassi cation data checked assess usefulness kernel trick setting 
compare linear kernels gaussian kernels 
experimental results standard benchmark problems summarized table 
wisconsin breast cancer dataset contained missing examples 
breast cancer pima diabetes ionosphere sonar data obtained uci repository 
data twonorm problem data generated speci ed 
dataset randomly partitioned training test sets 
kernel parameter gaussian kernel kx yk tuned cross validation random partitions 
reported results averages random partitions linear kernel gaussian kernel chosen 
results comparable existing literature obtained support vector machines 
notice smaller table test set accuracy tsa compared best performance performance svm linear kernel svm gaussian kernel dataset linear kernel gaussian kernel tsa tsa twonorm breast cancer ionosphere pima diabetes sonar test set accuracy cases 
furthermore smaller linear decision boundary nonlinear decision boundary obtained gaussian kernel 
clearly shows method leads powerful decision boundaries 
problem linear discrimination long distinguished history 
results misclassi cation rates obtained making distributional assumptions anderson bahadur 
results hand moment problems semide nite optimization obtain distribution free results linear discriminants 
shown exploit mercer kernels generalize algorithm nonlinear classi cation 
computational complexity method comparable quadratic program solve support vector machine svm 
simple iterative squares approach believe gain exploiting analogies svm developing specialized cient optimization procedures algorithm particular tools break data subsets 
extension large scale applications current focus research problem developing variant algorithm multiway classi cation function regression 
statistical consequences plug estimates mean vectors covariance matrices needs investigated 
acknowledge support onr muri nsf iis ecs belgian american educational foundation 
anderson bahadur 
classi cation multivariate normal distributions di erent covariance matrices 
annals mathematical statistics 
bertsimas sethuraman 
moment problems semide nite optimization 
handbook semide nite optimization kluwer academic publishers 
breiman 
arcing classi ers 
technical report statistics department university california december 
cherno 
selection ective attributes deciding hypothesis linear discriminant functions 
frontiers pattern recognition watanabe ed 
new york academic press 
boyd vandenberghe 
convex optimization 
course notes ee stanford university 
available www stanford edu class ee 

sharpness chebyshev type inequalities 
ann 
inst 
stat 
math 

mika weston sch olkopf uller 
fisher discriminant analysis kernels 
neural networks signal processing ix new york ieee press 
nesterov nemirovsky 
interior point polynomial methods convex programming theory applications 
philadelphia pa siam 
