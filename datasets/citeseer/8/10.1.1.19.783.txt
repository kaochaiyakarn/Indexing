framework robust subspace learning fernando de la torre department communications signal theory la school engineering universitat ramon barcelona spain phone fax email edu michael black department computer science brown university box providence ri usa phone fax email black cs brown edu july computer vision signal processing statistical problems posed problems learning low dimensional linear multi linear models 
models widely representation shape appearance motion computer vision applications 
methods learning linear models seen special case subspace fitting 
drawback previous learning methods squares estimation techniques fail account outliers common realistic training sets 
review previous approaches making linear learning methods robust outliers new method uses intra sample outlier process account pixel outliers 
develop theory robust subspace learning rsl linear models continuous optimization framework robust estimation 
framework applies variety linear learning problems computer vision including eigen analysis structure motion 
synthetic natural examples develop illustrate theory applications robust subspace learning computer vision 
keywords principal component analysis singular value decomposition learning robust statistics subspace methods structure motion 
submitted ijcv special issue vision brown 
automated learning low dimensional linear multi linear models training data come standard paradigm computer vision 
variety linear learning models tech niques principal component analysis pca factor analysis fa autoregressive analysis ar singular value decomposition svd widely representation high dimensional data appearance shape mo tion temporal dynamics approaches differ noise assumptions prior information underlying statistical models directly indirectly related linear bilinear regression :10.1.1.12.7580
learning linear models posed problem alternated squares als estimation referred criss cross sion 
develop robust formulation estimation processes exploited improve robustness linear learning methods statistical outliers 
particular pca popular technique parameterizing shape appearance motion :10.1.1.12.7580
learned pca representations proven useful solving problems face object recognition tracking detection background modeling :10.1.1.132.3753
typically training data pca pre processed way faces aligned generated vision algorithm optical flow computed training data 
automated learning methods applied realistic problems amount training data increases impractical manually verify data 
general training data may contain undesirable artifacts due occlusion hand front face illumination specular reflections image noise scanning archival data errors underlying data generation method incorrect optical flow vectors 
view artifacts statistical outliers develop theory robust subspace learning rsl pca construct low dimensional linear subspace representations noisy data 
pca provides simple domain motivate develop illustrate approach 
show general framework extended variety linear multi linear learning problems 
commonly known traditional pca constructs rank subspace approximation zero mean training data optimal squares sense 
com illustrative training set different types outliers 
top images original training set images 
middle training set sample outliers 
bottom training set intra sample outliers 
known squares techniques robust sense outlying measurements arbitrarily skew solution desired solution 
vision community pre vious attempts pca robust treated entire data samples images outliers 
approach appropriate entire data samples contaminated illustrated middle 
argued common case computer vision applications involves intra sample outliers affect pixels data sample bottom 
presents simple example illustrate effect intra sample outliers 
row shows mean principal components image training set top 
second row shows bases recovered pca training set bottom contains intra sample outliers 
notice outliers affected basis images 
accounting intra sample outliers robust principal component analysis method described constructs linear basis shown bottom influence outliers reduced recovered bases visually similar produced traditional pca data outliers 
shows effect outliers reconstruction images learned linear subspaces 
row shows noiseless images training set faces 
middle row shows reconstruction obtained projecting face pca basis 
effect intra sample outliers learned basis images 
top standard pca applied noise free data 
middle standard pca applied training set corrupted intra sample outliers 
bottom robust pca applied corrupted training data 

reconstruction results subspaces constructed noisy training data 
top original noiseless test images 
middle squares reconstruction images standard pca basis bottom reconstructed images basis images learned corrupted training data 
projection operation corresponds squares estimate linear reconstruction coefficients influenced outlying data training set 
appearance squares method robust technique bottom mean squared reconstruction error defined reduced 
section review previous statistics neural networks vision communities addressed robustness subspace methods 
particular describe method xu yuille detail quantitatively compare method standard pca 
show linear multi linear general methods modified outlier process account outliers pixel level 
robust estimation method derived details algorithm complexity convergence properties described 
estimation methods robust subspace learning rsl inherent scale parameter determines considered outlier 
method estimating parameter data resulting fully automatic learning method 
synthetic experiments illustrate different robust approaches treat outliers quantitatively evaluate method 
results natural images show method robustly learn subspace illumination variation background modeling 
previous full review linear learning methods applications computer vision scope 
concreteness focus principal component analysis show robust methods generalize linear learning methods 
illustrative purposes loss generality examples learning models images 
advantage consider ing pca task widely applicable vision community improving robustness 
formulation techniques robust estimation developed statis tics community 
goal recover solution learned model best fits majority data detect outlying data 
loosely term outlier refers data conform assumed statistical model 
robust estimation method tolerate percentage outlying data having solution arbitrarily skewed 
computer vision applications outliers typically noise traditional sense violations highly simplified models world example presence specular reflections assumes lambertian reflectance violation bright ness constancy assumption motion boundaries 
review robust statistical methods computer vision see 
note issues robustness addressed 
prin components black jepson addressed issue robustly recovering coefficients linear combination basis vectors reconstructs input image step com known inference machine learning community 
address general problem robustly learning principal components place 
address general problem involves learning basis vectors linear coefficients robustly 
preliminary results 
energy functions pca pca statistical technique useful dimensionality reduction 
matrix column data sample image number training images number pixels image 
previous formulations assume data zero mean 
squares case achieved subtracting mean entire data set column 
case standard pca consider data zero mean 
robust formulations robust mean explicitly estimated principal components described 
principal components columns directions maximum variation data 
principal components maximize constraint covariance matrix 
columns form orthonormal basis spans principal subspace 
effective rank approximate column space principal components 
data approximated linear combination principal components linear coefficients obtained projecting training data principal subspace 
method calculating principal components widely statistics neural network community formulates pca squares estimation basis images minimize bold capital letters denote matrix bold lower case letters column vector 
represents identity matrix tuple ones 
represents th column matrix column vector representing th row matrix 
denotes scalar row column matrix scalar th element column vector 
th scalar element vector non bold letters represent scalar variables 
operator transforms vector diagonal matrix matrix column vector diagonal components 
operator calculates inverse element matrix 
denotes hadamard point wise product matrices equal dimension 
trace operator square matrix denotes frobenius norm matrix 
subspace dim denotes dimension subspace 
denotes norm reconstruc tion error vector reconstruction error 
alternatively linear coefficients explicit variables minimize approach estimating bases coefficients uses criss cross regression seen particular case expectation maximization em algorithm probabilistic pca ppca :10.1.1.33.4726
ppca assumes data generated noisy random process defines proper likelihood model 
noise infinitesimal equal directions ppca equivalent standard pca 
case em algorithm reduced coupled equations step cc step algorithm alternates solving linear coefficients expectation step solving basis maximization step 
equations closed form solution terms eigen equation bases eigenvectors covariance matrix high dimensional data em approach efficient space time 
complex noise model factor analysis assumes diagonal noise coefficients gaussian distributed unit variance 
principal component analysis techniques extended cope problem missing data occurs frequently vision applications 
shum solve pca problem known missing data minimizing energy function similar weighted squares technique ignores missing data 
method model sequence range images occlusion noise similar method gabriel zamir described 
tenenbaum freeman yuille similar trick model missing data 
rao proposed kalman filter approach learning bases coefficients incremental fashion 
observation process assumes gaussian noise corresponds error 
rao robust learning method estimating minimize black jepson suggest robust rule estimating coefficients bases learned 
previous robust approaches methods estimating principal components robust outliers common training data arbitrarily bias solution 
happens energy functions covariance matrix derived squares norm framework 
robustness pca methods computer vision received little attention problem studied statistics neural networks literature algorithms proposed 
approach replaces standard estimation covariance matrix robust esti covariance matrix 
formulation weights mean outer products form covariance matrix 
calculating eigenvalues eigenvectors robust covariance matrix gives eigenvalues robust sample outliers 
mean robust covariance matrix calculated scalar weights function mahalanobis distance iteratively estimated 
numerous possible weight functions proposed huber weighting coefficients 
approaches weight entire data samples individual pixels appropriate vision applications 
related approach robustly estimate element covariance matrix 
guaranteed result positive definite matrix 
methods robust estimation full covariance matrix computationally impractical high dimensional data images note just computing covariance matrix requires operations practical applications difficult gather sufficient training data guarantee covariance matrix full rank 
alternatively xu yuille proposed algorithm generalizes energy func tion introducing additional binary variables zero data sample image considered outlier 
minimize 
binary random variable 
sample taken consideration equivalent discarding outlier 
second term penalty term prior discourages trivial solution zero 
energy smaller threshold algorithm prefers set considering sample inlier greater equal 
minimization involves combination discrete continuous optimization problems xu yuille derive mean field approximation problem marginalizing binary variables solved minimizing function related robust statistical estimators 
varied annealing parameter attempt avoid local minima 
techniques limited application computer vision problems reject entire images outliers 
vision applications outliers typically correspond small groups pixels seek method robust type outlier reject pix els data samples 
gabriel zamir give partial solution 
propose weighted singular value decomposition svd technique construct principal sub space 
approach minimize recall column vector containing elements th row 
effectively puts weight pixel training data 
related gives partial solution problem factorizing matrices known weighting data introducing gener singular value decomposition 
approach applies known weights separable weight row column 
basic idea whiten data weights perform svd un whiten bases similar idea see 
benefit approach takes advantage efficient implementations svd algorithm 
disadvantages weights known individual pixel outliers allowed 
general robust case weights unknown may different weight pixel training image solution leverages svd solve minimization problem criss cross regressions involve iteratively computing dyadic rank fits weighted squares 
approach alternates solving fixed similar em approach probabilistic interpretation :10.1.1.33.4726
spirit gabriel note quadratic formulation robust outliers propose making rank fitting process robust 
propose number methods criss cross regressions robust apply approach low dimensional data optimization methods scale high dimensional data images 
related similar idea construct robust matrix factorization weighted norm 
context neural networks cichocki proposed sequential method computing principal components equation 
proposed robust pca algorithm similar spirit treat outliers missing data add term encourage spatial coherence 
section develop approaches give complete solution estimates parameters interest connect method robust estimation techniques unify previous results 
robust principal component analysis section extend previous robust pca methods adding intra sample outlier process motivated necessity dealing type outliers typically occur image data 
previous approach xu yuille equation suffers main problems single bad pixel value image lie far subspace entire sample treated outlier influence estimate 
second xu yuille squares projection data computing distance subspace coefficients reconstruct data 
reconstruction coefficients arbitrarily biased outlier 
binary outlier process completely rejects includes sample 
introduce general analogue outlier process computational advantages provides connection robust estimation 
address issues reformulate analog outlier process depends images pixel locations penalty function 
error specifies scale parameter pixel locations 
observe explicitly solve mean estimation process 
squares formulation mean computed closed form subtracted column data matrix 
robust case outliers defined respect error reconstructed images include mean 
mean longer computed performing deflation procedure estimated robustly analogously bases 
recall pca assumes isotropic noise model 
formulation allow noise vary row pixel data 
exploiting relationship outlier processes robust statistics minimizing equivalent minimizing robust energy function technique applied efficiently computing eigenvectors iteratively estimating eigenvector removing influence data making remaining eigenvectors orthogonal 
original training images 
second log original image 
particular class robust functions 
define robust magnitude vector sum robust error values component geman mcclure error function scale parameter controls convexity robust function deterministic annealing optimization process 
robust function corresponds penalty term 
choices geman mcclure function widely shown 
additionally functions twice differentiable useful optimization methods gradient descent 
details method described 
note robust methods ransac median squares theoretically robust estimation clear apply methods efficiently high dimensional problems robust estimation basis images 
quantitative comparison order better understand pca method xu yuille influenced intra sample outliers consider contrived example face images shown 
second image contaminated outlying pixel times energy sum image pixels 
order visualize large range pixel magnitudes displayed log image 
learned bases standard pca xu yuille method proposed method shown 
force method explain data basis images 
note approach xu yuille solve mean fair comparison solved subtracted mean methods 
case mean approximately recovered bases 
pca bases capture outlier second training image principal component energy 
bases learned basis images 
top traditional pca 
middle xu yuille method 
bottom 
approximately capture principal subspace spanning images 
xu yuille method hand discards second image far subspace uses bases represent remaining images 
method proposed constructs subspace takes account images ignoring single outlying pixel 
recover bases approximate images 
project original images outliers learned basis sets 
pca wastes basis images outlying data basis images approximate training images 
xu yuille method ignores useful infor mation image result single outlier unable reconstruct image 
uses basis images represent images represent perfectly 
method provides approximation images basis images 
methods error pca xu yuille method give errors respectively 
reconstruction noiseless images 

original images 

noiseless reconstruction pca bases 

xu yuille method 


computational issues section describe robustly compute mean subspace spanned principal components 
imposing orthogonality bases imposed needed schmidt procedure :10.1.1.33.4726
section organized follows section introduce iteratively re weighted squares approach solve equation allow relate method previous provide insight problem 
section examines special cases problem admit closed form solutions 
section derives continuous optimization formulation results efficient algorithm particularly useful high dimensional image data 
section shows compute scale parameter robust function automatically 
section discusses practical issues initialization selection number bases criteria convergence 
weighted squares problem seen previous section robust problems general posed tion energy function cost function 
optimization methods exist instructive useful formulate minimization equation weighted squares problem solve iteratively reweighted squares irls 
particular pro vide insight relationships previous methods 
originally proposed turkey widely statistics computer vision irls approximate iterative algorithm solving robust estimation problems 
define residual error matrix notation matrix defined contains positive weights pixel image 
calculated iteration function previous residuals related influence pixels solution 
element equal geman mcclure function 
previous estimators robust estimation function solved irls 
iteration irls equation transformed weighted squares problem rewritten diagonal matrices containing positive weighting cients data sample recall column 
diagonal matrices containing weighting factors pixel training set 
note symmetry recall represents column data matrix column vector contains row 
observe non unique solutions linear invertible transformation matrix give solution reconstruction subspace 
ambiguity solved imposing constraint orthogonality bases graham schmidt orthogonalization 
computer vision applications subspace methods recognition important measurement distance subspace particular axes subspace irrelevant 
order find solution differentiate differentiate find necessary sufficient conditions minimum 
conditions derive coupled system equations giving updates parameters approximate algorithm minimizing equation employ step method minimizes alternated squares als criss cross regressions 
summarizing irls procedure works follows initial basis set coefficients initial error calculated parameters described 
weighting matrix computed successively alternate minimizing respect closed form equations 
converged recomputed error calculate weighting matrix proceed manner convergence algorithm 
process anneal 
point worth noting possible ways update parameters efficiently closed form solution see instance 
special cases general case arbitrary weighting matrices clear exists closed form solution terms weighted covariance matrix 
instance consider simple scenario mean zero weights known goal compute just weighted eigenvector coefficient observe simple scenario energy function expressed quotient due fact normalization factor depends solve equation terms eigen equation 
worth mentioning interesting special cases solutions terms eigenvectors weighted covariance matrix useful practical applications 
problems solved generalized singular value decomposition 
number parenthesis indicates iteration number 
sample image different weight 
imposing constraint easy show 
useful image may weight example data collected sequentially want weight data higher old data 
solution problem eigenvectors weighted covariance matrix note particular case just weight sample analogous previous robust pca methods robust covariance matrices method xu yuille 
formulation provides simple efficient algorithm class problems provided weights known 
observe case equation regular squares projection entire sample weight just rows bases weighted second special case occurs fixed weight matrix applies images training set 
occurs example systematic missing data weight pixel locations binary value indicating training set contains data pixel 
may useful particular spatial weight matrix application face modeling example give weight eyes mouth see obtain accurate reconstruction areas 

solution known generalized eigenvalue problem case solution minimizing rayleigh quotient 
interesting observe dual property previous case squares estimation problem computation bases weights involved just coefficients calculated weighted information 
updating parameters solving general case weights different pixel image employ step method minimizes explained section 
computationally expensive part algorithm involves computing 
computational cost iteration case efficient algorithm exploit fact matrices linear system equations rows simultaneously solve systems equations 

typically example estimating bases involves computing solution systems equations large computationally expensive 
directly solving systems systems solve gradient descent local quadratic approximation determine estimation step sizes see information 
robust learning rules updating successively follows recall element wise inverse matrix 
partial derivatives respect parameters reconstruction error estimate step size derivative respect similarly matrix contains derivatives robust function element 
matrix element matrix contains maximum second derivative component upper bound second derivative recall column matrix robust error alternated weighted squares normalized gradient time robust error versus computation time seconds 
experiment uses images pixels involves computing bases 
graph plots reconstruction error versus computation time gradient descent alternated squares approaches 
computed maximum diagonal hessian matrix column 
applied matrices 
update update error 
iterations update possible 
computational cost iteration updating normalized gradient descent linear parameters testing approaches implemented matlab updating schemes compare convergence properties 
plots energy versus computation time normalized gradient descent compares als 
observed gradient descent algorithm approaches local minimum faster typically requires computationally expensive iterations converge 
benefit normalized gradient updating rule allows incremental line learning 
performing line learning particular interest data available time re estimate parameters account new data 
incremental update algorithm useful due memory limitations data loaded memory 
case iteratively load different subsets original data adapt model parameters convergence properties method addressed 
local measure scale value scale parameter controls shape robust function determines residual errors treated outliers 
absolute value robust error larger local values estimated regions training set face images 
function begins reducing influence pixel image solution 
estimate scale parameters pixel automatically median absolute deviation mad pixel approaches possible 
mad viewed robust statistical estimate standard deviation compute max med med med indicates median taken region pixel mad image 
constant factor sets outlier times estimated deviation 
calculating mad need initial error obtained follows compute standard pca data calculate number bases preserve energy 
achieved ratio energy reconstructed vectors original ones larger 
observe standard pca ratio calculated terms eigenvalues covariance matrix 
number bases compute squares reconstruction error obtain robust estimate 
methods proposed statistical literature recalculate iteration method stable 
shows local values training set 
observe larger values estimated eyes mouth boundary face 
indicates higher variance training set regions larger deviations estimated subspace required training pixel considered outlier 
initialization issues minimization iterative scheme initial guess parameters 
initial estimate robust mean minimizing alternatively simply median mean sufficient 
initial guess chose standard principal components 
parameters just solution linear system equations minimize parameters calculate starts process 
general energy function non convex minimization method get trapped local minima 
deterministic annealing scheme helps avoid local minima 
method begins large multiple pixels inliers 
successively lowered value reducing influence outliers 
guaranteed converge global minimum experimental results shown reasonable convergence points 
concerned local minima algorithm run multiple times different initial conditions 
solution lowest minimum error chosen 
practice reasonable initial estimates algorithm converges similar results visually terms robust error 
method iterative nature necessary impose termination criterion 
methods chosen difference successive errors threshold norm consecutive updates parameters certain parameter 
method converge subspace stopping criterion defined terms principal angles consecutive subspaces largest principal angle related distance subspaces 
principal angles computed efficiently qr factorization svd algorithm 
principal angle smaller certain reached maximum number iterations iterative procedure 
standard pca number bases usually selected preserve percentage energy example 
criterion straightforward apply particularly case real problems high dimensional data 
robust error depends number bases directly compare energy functions different scale parameters 
energy outliers confused energy signal 
experimented different methods automatically selecting number basis images including minimum descriptor length mdl criterion akaike information criterion aic model selection methods scale high dimensional data require manual selection number normalization factors 
exploited heuristic methods practice 
apply standard pca data calculate number bases preserve energy 
number bases apply minimizing convergence 
process matrix contains weighting pixel training data 
detect outliers matrix set values obtaining value represents point robust function begins weighting contribution data thought outlier rejection point 
incrementally add additional bases minimize method maintaining constant weights proceed adding bases percentage energy accounted bigger linear subspace learned images robustly reconstructed variety applications 
order robustly compute coefficients new image subtract robust mean compute coefficients equation 
note need update bases case 
note values robust reconstruction learned training process 
pseudocode describes optimization process compute robust mean standard pca solution svd 
calcu late residuals initialize select initial number bases 
calculate scale parameter equation mad 
multiply constant pixels inliers principal angle chosen compute estimation step size equations 
compute partial derivatives parameters equation 
update parameters equations 
lower annealing schedule bigger 
additionally compute thresholding weight matrix 
keep adding bases solving equation 
note robust svd singular value decomposition returns factorization real matrix matrices matrices orthogonal span column row space respectively 
svd gives best rank approximation real matrix minimizes invariant norm norm frobenius norm 
observe point rename assume zero mean data pca model subspace spanned matrix matrix svd number bases 
pca svd formulated bilinear regression problems 
performing robust singular value decomposition proceed manner 
number bases parameters calculate robust subspace spanned bases minimizing 
robustly reconstruct data effectively filters outlying data 
reconstructed data free outliers perform standard svd compute note singular values non zero robust reconstructed subspace dimension 
take approach method developed straightforward apply 
alternative approach consists explicitly calculating sinusoidal patterns linear combination 
svd experiment 
original data 
column matrix sample image 
training data addition outliers 
squares svd reconstruction training data 
robust svd automatically removes outliers results reconstruction similar original data 
imposing orthogonality schmidt tion 
show benefits synthetically generate sinusoidal pattern pixels 
sinusoidal pattern composed sum outer products unidimensional sinusoidal signals matrices left hand side dimensional sinusoidal signals drawn 
right hand side weighted sum 
original dimensional sinusoidal signals create pattern plotted top plotted order 
training data artificially contaminated outliers see outliers generated uniformly sampling positions matrix replacing values zero mean gaussian noise having variance signal 
row shows true factorization matrix outliers original sinusoids generate outer products 
term corresponds column matrix corresponds row coefficient matrix robust svd 
factorization non contaminated data 
bases coefficients learned traditional svd 
bases coefficients learned robust svd 
similarly correspond second mode 
shows squares solution standard version svd 
similarly shows full reconstructed matrix svd result 
observed due effects outliers solution noisy 
contrast show robust solution factorization problem 
observe results achieved closer original data obtained svd 
error original noiseless matrix reconstructed matrix standard svd 
original matrix produced sinusoidal signals eigenvalues matrix respectively 
eigenvalues recovered standard svd contrast spreads energy eigenvalues having lower values 
experiments illustrate range applications robust subspace learning rsl consider prob lems current interest computer vision 
involves learning background appearance model person detection tracking :10.1.1.132.3753
generally rsl applied eigen image learning problem 
consider problem computing structure motion tracked feature points 
show problems benefit robust formulation reject intra sample outliers 
learning subspace illumination behavior illustrated collection images gathered static camera days 
column shows example training images addition changes illumination static background images contain people various locations 
people pass view camera quickly remain relatively multiple frames 
applied standard pca training data build background model captures illumination variation detect track people 
second column shows result reconstructing illustrated training images pca basis basis vectors 
presence people scene affects recovered illumination background results images people poorly reconstructed 
third column shows reconstruction obtained basis vectors 
able capture illumination changes ignoring people 
fourth column outliers plotted white 
observe outliers primarily correspond people specular reflections graylevel changes due motion trees background 
column plots final weights image 
bright areas correspond high weights inliers black areas correspond outliers 
weight images illustrate influence individual pixels recovered bases 
rsl method better job accounting illumination variation scene provides basis person detection 
algorithm takes approximately hours mhz pentium iii matlab rough approximation basis takes half hour 
approximately order magnitude slower svd disadvantages energy minimization formulation 
robust method beneficial situations robustness important dimensionality relatively low learning performed line 
plots value robust energy function versus number iterations core algorithm 
robust energy function depends order verify iteration decreases monotonically plot energy function achieved final annealed value 
shows convergence algorithm terms principal angle consecutive subspaces 
energy function minimized decreases monotonically principal angle necessarily 
shows mean upper left standard pca bases learned 
observe principal components appear capture major variations illumination effects people outliers appear bases bright dark regions 
comparison shows mean bases recovered 
note patches corresponding people appear 
structure motion sfm recovering shape motion feature correspondences multiple views known studied problem computer vision 
provide brief overview factorization approach structure motion sfm details reader referred :10.1.1.31.1497
presentation follows closely irani anandan :10.1.1.31.1497
address problem factoring shape motion measurable uncertainty locations features 
set feature points rigid object tracked frames coordinates points stacked measurement matrix time instant compute mean feature points center object subtract mean feature locations 
defines model shape relative object coordinate frame 
rows matrices contain relative object coordinates single time instant 
shown affine camera orthographic weak perspective noise rank 
conditions matrix factored product structure matrix motion matrix 
matrix recovers rotation object respect arbitrary coordinate frame frame matrix encodes relative positions feature reconstructed object 
errors due occlusion missing data noise matrix longer rank similar problem posed presence extra feature points corresponding independently moving objects scene 
problem structure motion multiple moving objects difficult see solution probabilistic mixture models 
robust formulation similar spirit robustly estimating multiple parameter ized motions optical flow community 
outliers squares approximation minimizing unitary invariant norm 
case implicitly assuming isotropic noise model error optimal case error feature time instant distributed isotropic gaussian 
assumptions compute svd factorization measurement matrix setting largest singular values zero gives matrix 
best rank approximation matrices provide squares estimate motion shape affine transformation 
deal outliers apply robust svd techniques developed 
sfm problem additional constraints taken account robust factorization 
coordinate feature point outlier treat entire point outlier implies coupling elements thing happens case optical flow color images 
order incorporate additional constraint algorithm simply modify robust function depend vector valued input scalar robust energy function algorithm basically explained interest space develop weighted squares approach 
initial parameters error computed computed define joint error matrix element contains error residual 
define weight matrix algorithm alternate solving recomputing error calculating weight matrix 
note similarity motivation approach morris kanade irani anandan :10.1.1.31.1497
irani anandan perform covariance weighted svd min mahalanobis distance feature space 
assume covariance factored 
morris kanade allow general covariance matrix provide robust formulation 
section report results experimental evaluation robust factorization algo rithm compare results traditional svd 
irani anandan similar synthetic data analyze performance algorithm 
shows frames original synthetic data cube 
actual feature points located intersections grid lines drawn visualization purposes 
cube undergoes rotational motion axis 
shows orthographic projection samples contained outliers crosses 
outliers synthetically generated uniform distribution coordinates different frame 
synthetic outliers simu late problem mismatches points caused failures feature tracker 
results multiple independent motions similar 
shows standard svd reconstruction shape animated recovered motion 
observed due outlying data estimation shape cube noisy 
plots solution obtained robust svd method proposed produces accurate results 
error shape traditional squares robust svd respectively 
additionally error motion traditional squares robust case 
issue practical interest sfm computation involves missing data feature points appear views 
sfm problem missing points typically known weights set zero points 
weights fixed robust estimation performed 
discussion related methods section explore possible applications extensions rsl computer vision problems 
de la torre black proposed robust parameterized component analysis technique robustly learn subspace invariant geometric transformations useful misalignment training images 
de la torre black proposed dy namic coupled component analysis robustly learn temporal spatial dependencies high dimensional training sets 
exist subspace problems benefit robust formulation 
interest space simply point possible applications ideas developed 
robust formulation problems proceed similarly done research needed 
multi linear models exist problems vision signal processing best modeled interaction multiple factors 
example tenenbaum freeman factoring style content bilinear model 
numerous extensions idea vision modeling facial appearances linear combination illumination expression identity 
multi linear models include tensorial approaches structure motion indepen dent component analysis ica 
tensor factorization seen generalization pca dimensions 
unique extension pca multi linear models see example 
views tensor factorization methods terms minimization energy function robust subspace learning methods developed applied multi linear models relatively straightforward way 
weighted subspace analysis weighted subspace analysis wsa provides formalism learning linear models data weighted known weights 
note equations perform wsa 
approach constructing appearance models articulated human figures image views 
wsa provides formalism constructing subspaces missing data weighting data different ranges constructing active appearance models shape graylevel pixels different variance 
idea computing structure motion measure certainty tracked feature points available :10.1.1.31.1497
weights separable provides efficient method pixel weighted uncertainty account 
general case method proposed straightforward apply 
additionally rsl provides framework line pca svd computation new data available 
minor component analysis mca obvious extension robust estimation subspace spanned smallest eigenvalues 
mca useful technique solving total squares problems 
formulation robust optimization method clear research needed 
regularized component analysis situations useful find subspaces spatial coherence bases 
instance subspace captures illumination variations composed sum smooth patterns bases 
case try recover smooth eigenspace bases vary smoothly minimizing symmetric positive definite sparse matrix 
application regularization involves adding spatial coherence outliers expect correspond coherent spatial structures 
eigenvalue problem finding eigenvalues positive definite matrix important problems applied mathematics 
possible application symmetric eigenvalue problem relating eigenvalue problem robust energy minimization problem 
energy minimization approach may extension feasible come cost terms computation 
finding subspace spanned largest eigenvectors achieved minimizing easy show saddle point energy function related finding solution eigenvalue problem 
introducing intra sample outlier useful matrix contains outliers 
note covariance matrix naturally expands sum outer products directly method proposed 
generally problems computer vision linear nant analysis lda segmentation generalized eigenvalue problem 
formulating applications intra sample outlier process relatively straightforward solution resulting robust generalized eigenvalue problem remains un clear 
discussion examples illustrate benefits method worth consid ering algorithm may give unwanted results 
consider example face database contains small fraction subjects wearing glasses 
case pixels corresponding glasses treated outliers 
learned basis set contain pixels impossible reconstruct images people wearing glasses 
desirable behavior depend application 
situation people glasses considered different classes objects appropriate robustly learn multiple linear subspaces corresponding different classes 
detecting outliers robust techniques may prove useful identifying training sets contain significant subsets modeled majority data separated represented independently 
classic advantages robust techniques data analysis 
issue take account fact training set contain intra sample outliers sample outliers 
order solve problem introduce sample outlier intra sample outlier equation 
learning rules cated derive 
approach hierarchical sample outliers detected removed training set 
apply method proposed removing intra sample outliers 
order implement efficient sample iterative reweighted squares idea initial weights com pute iteratively weighted covariance matrix data high dimensional perform 
estimation eigenvectors calculate error comput ing weights convergence 
practical approach discard sample outliers samples intra sample outliers certain threshold 
method robust subspace learning automatic learning linear models data may contaminated outliers 
particular applied formalism problems principal component analysis singular value decomposition 
approach extends previous vision community modeling outliers typically occur pixel level 
furthermore extends statistics community connecting explicit outlier formulation robust estimation developing fully automatic algo rithm appropriate high dimensional data images 
method tested natural synthetic images shows improved tolerance outliers compared techniques 
illustrated methods examples eigen image modeling structure motion 
important problems computer vision robust approaches may help provide solutions situations realistic amounts un modeled noise 
general linear models vision widespread increasing 
hope robust techniques proposed prove useful linear models represent realistic data sets 
matlab implementation method results downloaded www edu acknowledgments supported darpa project onr contract national science foundation itr program award 
author par tially supported government 
supported gift xerox foundation 
allan jepson discussions robust learning pca 
providing face image database 
images columbia database www cs columbia edu cave research examples 
moura 
factorization rank problem 
conference computer vision pattern recognition pages 
baldi hornik 
neural networks principal component analysis learning examples local minima 
neural networks 
tukey 
fitting power series meaning polynomials illustrated band data 
technometrics 
black anandan 
robust estimation multiple motions parametric piecewise smooth flow fields 
computer vision image understanding march 
black jepson 
eigentracking robust matching tracking objects view representation 
international journal computer vision 
black rangarajan 
unification line processes outlier rejection robust statistics applications early vision 
international journal computer vision 
black sapiro heeger 
robust anisotropic diffusion 
ieee transactions image processing 
black jepson fleet 
learning parameterized models image motion 
conference computer vision pattern recognition pages 
blake isard 
active contours 
springer verlag 
blake zisserman 
visual reconstruction 
mit press series massachusetts 
campbell 
robust procedures multivariate analysis robust covariance estimation 
applied statistics january 
cardoso 
independent component analysis survey algebraic methods 
international symposium circuits systems vol pages 
carroll chang 
analysis individual differences multidimensional scaling way generalization eckart young decomposition 
psychometrika january 
cichocki unbehauen 
robust learning algorithm blind separation signals 
electronics letters 
cootes edwards taylor 
active appearance models 
european conference computer vision pages 

robust factorization data matrix 
proceedings computational statistics pages 
de la torre black 
dynamic coupled component analysis 
computer vision pattern recognition pages 
de la torre black 
robust principal component analysis computer vision 
international conference computer vision pages 
de la torre black 
robust parameterized component analysis theory appli cations facial modeling 
european conference computer vision pages 

principal component neural networks applications 
john wiley sons 
young 
approximation matrix lower rank 
psy 
everitt 
latent variable models 
london chapman hall 
fukunaga 
statistical pattern recognition second edition 
academic press boston ma 
gabriel 
resistant lower rank approximation matrices 
data analysis informatics iii pages 
gabriel zamir 
lower rank approximation matrices squares choice weights 
technometrics vol 
pp 
geiger pereira 
outlier process 
ieee workshop neural networks signal proc pages 
geman mcclure 
statistical methods tomographic image reconstruction 
international statistical institute lii 
golub van loan 
matrix computations 
nd ed 
johns hopkins university press 
golub van der vorst 
eigenvalue computation th century 
journal computational applied mathematics 

theory applications correspondence analysis 
academic press london 

correspondence analysis multivariate categorical data weighted squares 
biometrika 
hampel rousseeuw 
robust statistics approach influence functions 
wiley new york 
hartley zisserman 
multiple view geometry computer vision 
cambridge university press 
holland 
robust regression iteratively reweighted squares 
communications statistics 
huber 
robust statistics 
new york wiley 
irani anandan 
factorization uncertainty 
european conference com puter vision pages 
magnus 
matrix differential calculus applications statistics econometrics 
john wiley 
jolliffe 
principal component analysis 
new york springer verlag 
karhunen 
generalizations principal component analysis optimiza tion problems neural networks 
neural networks 
de leeuw 
principal component analysis mode data means alternating squares algorithms 
psychometrika 
lai 
robust image alignment partial occlusion spatially varying illumination change 
computer vision image understanding 
li 
robust regression 
mosteller tukey editors exploring data tables trends shapes 
john wiley sons 
maclean jepson 
recovery egomotion segmentation inde object motion em algorithm 
british machine vision conference pages leeds uk 
mardia kent bibby 
multivariate analysis 
academic press london 
meer mintz kim rosenfeld 
robust regression methods computer vision international journal computer vision 
meer stewart tyler eds 
special issue robust statistics 
computer vision image understanding 

symmetric gauge functions invariant norms 
quart 

oxford 
moghaddam pentland 
probabilistic visual learning object representation 
pat tern analysis machine intelligence july 
morris kanade 
unified factorization algorithm points line segments planes uncertainty models 
international conference computer vision pages 
murase nayar 
visual learning recognition objects appearance 
international journal computer vision 
oja 
simplified neuron model principal component analyzer 
journal mathematical biology 
oliver rosario pentland 
bayesian computer vision system modeling human interactions 
christensen editor int 
conf 
computer vision systems volume lncs series pages gran spain january 
springer verlag 
parlett 
symmetric eigenvalue problem 
prentice hall englewood cliffs nj 
kanade 
factorization method shape motion recovery 
international conference computer vision pages 
rao 
optimal estimation approach visual perception learning 
vision research april 
rousseeuw leroy 
robust regression outlier detection 
john wiley sons 
roweis 
em algorithms pca spca 
neural information processing systems pages 

robust principal component analysis 
journal multivariate analysis 
sanger 
optimal unsupervised learning single layer linear feedforward neural network 
neural networks november 
shi malik 
normalized cuts image segmentation 
ieee transactions pattern analysis machine intelligence august 
shum ikeuchi reddy 
principal component analysis missing data application polyhedral object modeling 
pattern analysis machine intelligence 
sidenbladh de la torre black 
framework modeling appearance articulated figures 
face gesture recognition pages 
bischof leonardis 
robust pca algorithm building representations panoramic images 
european conference computer vision pages 
tenenbaum freeman 
separating style context bilinear models 
neural computation 
tipping bishop 
probabilistic principal component analysis 
journal royal statistical society 
tomasi kanade 
shape motion image streams orthography factorization method 
int 
computer vision 
turk pentland 
eigenfaces recognition 
journal cognitive neuroscience 
van vandewalle 
total squares problem computational aspects analysis 
society industrial applied mathematics philadelphia 
xu 
mean square error self organizing nets 
neural net works 
xu yuille 
robust principal component analysis self organizing rules statistical physics approach 
ieee transactions neural networks 
yang wang 
robust algorithms principal component analysis 
pattern recognition letters 
yuille snow epstein belhumeur 
determining generative models objects varying illumination shape albedo multiple images svd integrability 
international journal computer vision 
zhang 
parameter estimation techniques tutorial application conic fitting 
image vision computing 
original data 
pca reconstruction 
reconstruction 
outliers 
weights 
original data 
pca reconstruction 
reconstruction 
outliers 
weights 
robust error iterations principal angle iterations convergence properties 
robust energy function vs iterations principal angle vs iterations 
standard pca 
learned model standard pca 
mean image appears upper left followed bases left right top bottom 
notice effects people outliers 
robust pca 
learned linear model 
compared effect outliers reduced bases primarily capture changes illumination 
data rotating cube 
views cube rotates axis 
orthographic projection feature points addition outliers 
svd robust svd svd robust svd svd robust svd reconstruction shape motion 
reconstructed shape cube displayed view determined recovered motion 
standard squares factorization 
robust factorization 

