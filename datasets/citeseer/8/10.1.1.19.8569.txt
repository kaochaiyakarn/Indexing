learning discriminative feature transforms low dimensions low dimensions kari torkkola motorola labs south river parkway md ml tempe az usa kari torkkola motorola com members home net torkkola marriage renyi entropy parzen density estimation shown viable tool learning discriminative feature transforms 
suffers computational complexity proportional square number samples training data 
sets practical limit large databases 
suggest immediate divorce methods renyi entropy semi parametric density estimation method gaussian mixture models gmm 
allows computation take place low dimensional target space reduces computational complexity proportional square number components mixtures 
furthermore convenient extension hidden markov models commonly speech recognition possible 
feature selection feature transforms important aspects pattern recognition system 
optimal feature selection coupled particular classifier done training evaluating classifier combinations available features 
obviously wrapper strategy allow learning feature transforms possible transforms enumerated 
feature selection feature transforms learned evaluating criterion reflects importance feature number features jointly 
called filter configuration feature selection 
optimal criterion purpose naturally reflect bayes error rate 
approximations example bhattacharyya bound divergence criterion 
usually accompanied parametric estimation gaussian densities hand 
classical linear discriminant analysis lda assumes classes gaussian shared single covariance matrix 
discriminant analysis hda extends allowing classes covariances 
maximizing particular criterion joint mutual information mi features class labels shown minimize lower bound classification error 
mi popular definition shannon computationally expensive 
evaluation joint mi number variables plausible histograms variables 
remedy principe showed renyi entropy shannon combined parzen density estimation leads expressions mutual information computational complexity number samples training set 
method formulated express mutual information continuous variables discrete class labels order learn dimension reducing feature transforms linear non linear pattern recognition 
note regarding finding extrema definitions entropy equivalent see pages page 
formulation mi evaluates effect sample sample transformed space parzen density estimation kernel 
effect called information force 
large huge databases hard due complexity 
remedy problem alleviate difficulties parzen density estimation high dimensional spaces formulation combining mutual information criterion renyi entropy semi parametric density estimation method gaussian mixture models gmm 
essence parzen density estimation replaced gmms 
order evaluate mi evaluating mutual interactions mixture components gmms suffices having evaluate interactions pairs samples 
approach maps output space gmm back input space output space adaptive feature transform taken 
allows computation take place target low dimensional space 
computational complexity reduced proportional square number components mixtures 
structured follows 
maximum mutual information mmi formulation discriminative feature transforms renyi entropy parzen density estimation 
discuss different strategies reduce computational complexity formulation gmms 
empirical results known databases conclude discussing connection hidden markov models 
mmi discriminative feature transforms set training data fx samples continuous valued random variable class labels samples discrete valued random variable objective find transformation parameters maximizes mutual information mi transformed data class labels procedure depicted fig 

need express function data set fy differentiable form 
done perform gradient ascent follows derive expression mi non parametric density estimation method apply renyi quadratic entropy shannon entropy described computational advantages 
estimating density sum spherical gaussians centered sample expression renyi quadratic entropy hr log dy log dy log fact convolution gaussians gaussian 
renyi quadratic entropy computed sum local interactions defined kernel pairs samples 
order convenient property measure mutual information making quadratic functions densities desirable 
discrete variable continuous variable measure derived follows dy dy dy number samples class kth sample regardless class pj sample emphasizing belongs class index class 
expressing densities parzen estimates kernel width results fy nc jp jp pk pl nc nc jp pj mutual information fy interpreted information potential induced samples data different classes 
straightforward derive partial accordingly interpreted information force samples exert sample components sum give rise components information force samples class attract samples regardless class attract samples different classes repel 
force coupled factor inside sum tends change transform way samples transformed space move direction information force increase mi criterion fy 
see details 
learning feature transforms maximizing mutual information class labels transformed features 
term consists double sum gaussians evaluated pairwise distance samples 
component consists sum interactions class second interactions regardless class third sum interactions class samples 
bulk computation consists evaluating gaussians forming sums 
information force gradient gaussians addition pairwise differences samples 
large complexity problem 
rest explores possibilities reducing computation method applicable large databases 
reduce computation 
essence trying learn transform minimizes class density overlap output space trying drive class singularity 
kernel density estimate results sum kernels samples divergence measure densities necessarily requires operations 
alternatives reduce complexity reduce form simpler density estimates 
straightforward ways achieve clustering random sampling 
case clustering needs performed high dimensional input space may difficult computationally expensive 
transform learned find representation discriminates cluster centers random samples belonging different classes 
details densities may lost random sampling bring problem computable level 
alternative accomplished gmm example 
gmm learned low dimensional output space class comparing samples comparing samples components gmms suffices 
parameters transform learned iteratively fy change iteration gmms need estimated 
guarantee change transform fy small simple re estimation previous suffice 
depends optimization method 
step reducing computation compare gmms different classes output space comparing actual samples 
addition inconvenience re estimation lack notion mapping 
transformed input space output space change transform order increase mi criterion 
possible evaluate effect sample mixture component effect component mi yk yk due double summing pursue mapping strategy outlined section 
gmm mapping strategies io mapping 
gmm available high dimensional input space models directly mapped output space transform 
call case io mapping 
writing density class gmm mixture components pj mixture weights get xjc kp pj pj pj consider linear transforms 
transformed density low dimensional output space simply kp pj pj pj mutual information output space class labels densities transformed gmms expressed function possible evaluate insert 
great advantage strategy input space gmms created em algorithm example actual training data needs touched optimization 
viable approach gmms available high dimensional input space see section expensive impossible estimate em algorithm 
case 
mapping 
alternative construct gmm model training data low dimensional output space 
getting requires transform gmm constructed having transformed data example random informed guess transform 
density estimated samples output space class kp pj pj pj output space gmm constructed samples construct gmm input space exact assignments samples mixture components output space gmms 
running em algorithm input space unnecessary know samples belong mixture components 
similar strategy learn gmms high dimensional spaces 
notation eq denote density input space 
result gmms spaces transform mapping 
transform learned io mapping equalities pj pj pj pj case called mapping 
biggest advantage avoiding operate high dimensional input space time procedure 
learning transform mapped gmms derivation adaptation equations linear transform apply mapping 
step express mi function gmm constructed output space 
gmm function transform matrix mapping input space gmm output space gmm 
second step compute gradient half equation 
information potential function gmms gmm output space class expressed 
need equalities denotes class prior nc 
denote terms vall dy nc kp pi pi pi dy nc kp kp pi pj pi pj pi pj compact notation change indexing substitutions kl kl kl kl total number mixture components pq cp cq 
write vall convenient form 
nc pp vall nc nc nc pq nc nc pq gradient information potential gaussian mixture component function corresponding input space component transform matrix straightforward albeit tedious write gradient terms composed different sums need gradient kl kl kl kl input space gmm parameters kl kl equalities kl kl kl kl expresses convolution mixture components output space 
components high dimensional input space gradient expresses convolution output space changes maps mixture components output space changed 
mutual information measure defined terms convolutions maximizing tends find crudely stated minimizes convolutions classes maximizes classes 
desired gradient gaussian respect transform matrix follows kl kl kl kl kl kl kl total gradient obtained simply replacing gradient 
evaluating bulk computation evaluating pq componentwise convolutions 
computational complexity 
addition requires pairwise sums differences mixture parameters input space need computed 
empirical results step evaluating approach compare performance computationally expensive mmi feature transforms parzen density estimation 
repeated pattern recognition experiments exactly lvq classifier 
experiments done publicly available databases different terms amount data dimension data number training instances 
details data sets please see 
mapping diagonal gaussians class learn dimension reducing linear transform 
gradient ascent optimization results tables 
column denotes original dimensionality data set 
performance average databases reduced dimensions ranged original dimension minus pca mmi parzen combination mmi gmm combination tests altogether 
lda calculated databases small lda produce features 
results satisfactory best hope performance equal mmi parzen combination 
significant reduction computation caused minor drop performance classifier 
discussion method learn discriminative feature transforms maximum mutual information criterion 
formulating mi renyi entropy gaussian example video clips viewed members home net torkkola mmi 
table accuracy phoneme test data set lvq classifier 
output pca lda mmi parzen mmi gmm table accuracy landsat test data set lvq classifier 
output dimension pca lda mmi parzen mmi gmm table accuracy letter test data set lvq classifier 
output dimension pca lda mmi parzen mmi gmm table accuracy pipeline data set lvq classifier 
output dimension pca lda mmi parzen mmi gmm table accuracy pima data set lvq classifier 
output dimension pca lda mmi parzen mmi gmm mixture models semi parametric density estimation method allows computation take place low dimensional transform space 
compared previous formulation parzen density estimation large databases possibility 
convenient extension hidden markov models hmm commonly speech recognition possible 
hmm speech recognition system state discrimination enhanced learning linear transform highdimensional collection features convenient dimension 
existing hmms converted high dimensional features called single pass retraining compute probabilities current features re estimation high dimensional set features 
state discriminative transform lower dimension learned method 
round single pass retraining converts existing hmms new discriminative features 
advantage method speech recognition state separation transformed output space measured terms separability data represented gaussian mixtures terms data actual samples 
advantageous regarding recognition accuracies hmms exact structure 
battiti 
mutual information selecting features supervised neural net learning 
neural networks july 
sanjoy dasgupta 
experiments random projection 
proceedings th conference uncertainty artificial intelligence pages stanford ca june july 
fano 
transmission information statistical theory communications 
wiley new york 
fisher iii principe 
methodology information theoretic feature extraction 
proc 
ieee world congress computational intelligence pages anchorage alaska may 
fukunaga 
statistical pattern recognition nd edition 
academic press new york 
xuan chai wu 
bhattacharyya distance feature selection 
proceedings th international conference pattern recognition volume pages 
ieee aug 
kapur 
measures information applications 
wiley new delhi india 
kapur 
entropy optimization principles applications 
academic press san diego london 
kumar andreas 
discriminant analysis reduced rank hmms improved speech recognition 
speech communication 
principe fisher iii xu 
information theoretic learning 
simon haykin editor unsupervised adaptive filtering 
wiley new york ny 
principe xu fisher iii 
pose estimation sar information theoretic criterion 
proc 
spie 
george mukund padmanabhan 
minimum bayes error feature selection continuous speech recognition 
todd leen thomas dietterich volker tresp editors advances neural information processing systems pages 
mit press 
sinkkonen samuel kaski 
clustering conditional distributions auxiliary space 
neural computation 
kari torkkola 
nonlinear feature transforms maximum mutual information 
proceedings ijcnn pages washington dc usa july 
kari torkkola william campbell 
mutual information learning feature transformations 
proceedings th international conference machine learning pages stanford ca usa june july 
vlassis 
supervised dimension reduction intrinsically low dimensional data 
neural computation january 
yang moody 
feature selection joint mutual information 
proceedings international icsc symposium advances intelligent data analysis rochester new york june 
