tree modeling estimation gaussian processes graphs cycles martin wainwright erik sudderth alan willsky laboratory information decision systems department electrical engineering computer science massachusetts institute technology cambridge ma mit edu embedded trees algorithm iterative technique estimation gaussian processes defined arbitrary graphs 
exactly solving series modified problems embedded spanning trees computes conditional means efficiency comparable better techniques 
methods embedded trees algorithm computes exact error covariances 
error covariance computation efficient graphs removing small number edges reveals embedded tree 
context demonstrate sparse loopy graphs provide significant increase modeling power relative trees minor increase estimation complexity 
graphical models invaluable tool defining manipulating probability distributions 
modeling stochastic processes graphical models basic problems arise specifying class graphs model approximate process ii determining efficient techniques statistical inference 
fact exists fundamental tradeoff expressive power graph tractability statistical inference 
extreme tree structured graphs lead highly efficient algorithms estimation modeling power limited 
addition edges graph tends increase modeling power introduces loops necessitate sophisticated costly techniques estimation 
areas coding theory artificial intelligence speech processing graphical models typically involve discrete valued random variables 
domains image processing control oceanography appropriate consider random variables continuous distribution 
context gaussian processes graphs great practical significance 
gaussian case provides valuable setting developing understanding estimation algorithms 
focus estimation modeling gaussian processes defined graphs cycles 
develop estimation algorithm exploiting trees embedded loopy graph 
set noisy measurements embedded trees algorithm computes conditional means efficiency comparable better techniques 
methods algorithm computes exact error covariances node 
applications error statistics important conditional means 
demonstrate example relative tree models graphs small number loops lead substantial improvements modeling fidelity significant increase estimation complexity 
linear estimation fundamentals problem formulation consider gaussian stochastic process markov respect undirected graph node corresponds subvector refer state variable th node length state dimension 
hammersley clifford theorem gamma inherits sparse structure partitioned blocks state dimensions th block nonzero edge nodes cx set noisy observations 
loss generality assume subvectors observations conditionally independent state estimation purposes interested jy marginal distribution state node conditioned noisy observations 
standard formulas exist computation xjy bx gamma theta gamma gamma gamma conditional error covariances block diagonal elements full error covariance block sizes equal state dimensions 
exploiting graph structure tree structured conditional means error covariances computed direct efficient algorithm 
maximal state dimension node total number nodes 
algorithm generalization classic kalman smoothing algorithms time series involves passing means covariances node chosen root 
graphs cycles calculating full error covariance brute force matrix inversion principle provide conditional means error variances 
computational complexity matrix inversion dn proposal practically feasible applications image processing may order motivates development iterative techniques linear estimation graphs cycles 
groups analyzed pearl belief propagation application gaussian processes defined loopy graphs 
gaussians trees belief propagation produces results equivalent kalman smoother chou 
graphs cycles groups showed belief propagation converges computes correct conditional means error covariances incorrect 
complexity iteration belief propagation loopy graphs iteration corresponds updating message 
gamma gamma tree gamma gamma tree gamma 
embedded trees produced different cutting matrices nearest neighbor grid observation nodes shown 
important note conditional means efficiently calculated techniques numerical linear algebra 
particular seen equation computing conditional mean equivalent computing product matrix inverse vector 
sparsity gamma iterative techniques conjugate gradient compute mean associated cost dn iteration 
belief propagation techniques compute means error covariances 
embedded trees algorithm calculation means section iterative algorithm computing conditional means error covariances gaussian process defined graph 
central algorithm operation cutting edges loopy graph reveal embedded tree 
standard tree algorithms exactly solve modified problem results subsequent iteration 
gaussian process graph operation removing edges corresponds modifying inverse covariance matrix 
specifically apply matrix splitting gamma gamma gamma tree gamma gamma symmetric cutting matrix chosen ensure gamma tree corresponds valid tree structured inverse covariance matrix 
matrix splitting allows consider defining sequence iterates recursion gamma tree gamma gamma gamma indexes embedded tree th iteration 
example shows spanning trees embedded nearest neighbor grid 
matrix gamma tree gamma positive definite possible solve iterate terms data previous iterate 
starting point generate sequence iterates recursion gamma gamma gamma gamma tree gamma 
comparing equation equation seen computing th iterate corresponds linear gaussian problem solved efficiently directly standard tree algorithms 
convergence means stating convergence results recall matrix spectral radius defined ae max jj ranges eigenvalues proposition 
conditional mean original problem loopy graph consider sequence iterates generated equation 
starting point unique fixed point recursion error gamma obeys dynamics gamma tree typical implementation algorithm cycles embedded trees fixed order say case convergence algorithm analyzed terms product matrix gamma tree proposition 
convergence algorithm governed spectral radius particular ae algorithm converge ae bx gamma gamma 
geometrically rate fl ae note cutting matrices chosen order guarantee gamma tree tree structured gamma tree gamma positive definite 
theorem adapted results gives conditions guaranteeing validity convergence algorithm cutting single tree 
theorem 
define gamma gamma suppose cutting matrix symmetric positive semidefinite 
guaranteed ae gamma 
particular bounds max max max ae gamma max max min noted conditions theorem sufficient means necessary guarantee convergence algorithm 
particular find indefinite cutting matrices lead faster convergence 
furthermore theorem address superior performance typically achieved cycling embedded trees 
gaining deeper theoretical understanding phenomena interesting open question 
calculation error covariances exist variety iterative algorithms computing conditional mean linear gaussian problem methods correctly compute error covariances node 
show algorithm efficiently compute covariances iterative fashion 
applications oceanography error statistics important estimates 
assume simplicity notation expand equation yield iteration gamma gamma matrix satisfies recursion gamma gamma gamma gamma initial condition 
straightforward show recursion conditional means equation converges matrix sequence ff gamma converges full error covariance cutting matrices typically low rank say number cut edges 
basis shown iteration log error convergence means conj 
grad 
embedded tree belief prop 
iteration convergence error variances embedded tree 
convergence rates computing conditional means normalized error 
convergence rate algorithm computing error variances 
decomposed sum rank matrices 
directly updating low rank decomposition gamma requires operations 
efficient restructuring update requires en operations 
diagonal blocks low rank representation may easily extracted added diagonal blocks gamma computed standard tree smoothers 
may obtain error variances en operations iteration 
computation error variances particularly efficient graphs number edges cut small compared total number nodes results applied algorithm variety graphs ranging graphs single loops densely connected mrfs grids 
compares rates convergence algorithms conjugate gradient cg embedded trees belief propagation bp theta nearest neighbor grid 
algorithm employed embedded trees analogous shown 
find cg usually fastest exhibit convergence 
accordance proposition algorithm converges geometrically 
bp converge faster depending choice clique potentials 
experimented optimizing performance adaptively choosing edges cut 
shows contrast cg bp algorithm compute conditional error variances convergence rate geometric 
modeling graphs cycles issues model design variety graphical structures may approximate stochastic process 
example simplest model time series markov chain shown 
high order model may required adequately capture long range correlations 
associated increase state dimension leads inefficient estimation 
shows alternative model structure 
additional coarse scale nodes added graph directly linked measurements 
nodes auxiliary variables created explain fine scale stochastic process interest 
properly designed resulting tree structure fine scale nodes observations auxiliary nodes 
markov chain 
multiscale tree model 
tree augmented extra edge 
desired covariance 
error jp gamma tree desired covariance realized tree covariance 
error jp gamma loop desired covariance covariance realized loopy graph 
capture long range correlations increase state dimension higher order markov model 
previous group developed efficient algorithms estimation stochastic realization multiscale tree models 
gains provided multiscale models especially impressive quadtrees approximate dimensional markov random fields 
statistical inference mrfs notoriously difficult estimation quadtrees remains extremely efficient 
significant weakness tree models boundary artifacts 
leaf nodes adjacent original process may widely separated tree structure see 
result dependencies nodes may inadequately modeled causing discontinuities 
increasing state dimension hidden nodes reduce reduce estimation efficiency total 
potential solution add edges pairs fine scale nodes tree artifacts arise shown 
edges able account short range dependency neglected tree model 
furthermore optimal inference near tree models algorithm extremely efficient 
application multiscale modeling consider dimensional process length exact covariance shown 
approximate process different graphical models multiscale tree near tree containing additional edge nodes tree boundary see 
models state dimension node constrained finest scale contains nodes model process points 
shows absolute error jp gamma tree tree model realization performed scale recursive algorithm 
tree model matches desired process statistics relatively center tree structure causes boundary artifact 
shows absolute error jp gamma loop graph obtained adding single edge largest fine scale tree boundary 
addition reduces peak error substantial gain modeling fidelity 
algorithm implemented cutting different embedded trees converges extremely rapidly rate fl 
discussion contributions estimation modeling gaussian processes graphs 
developed embedded trees algorithm estimation gaussian processes arbitrary graphs 
contrast techniques algorithm computes means error covariances 
densely connected graphs algorithm comparable better techniques computing means 
error covariance computation especially efficient graphs cutting small number edges reveals embedded tree 
context shown modeling sparsely connected loopy graphs lead substantial gains modeling fidelity minor increase estimation complexity 
results arise number fundamental questions trade modeling fidelity estimation complexity 
order address questions currently working develop tighter bounds convergence rate algorithm considering techniques optimally selecting edges removed 
modeling side expanding previous trees order develop theory stochastic realization processes graphs cycles 
lastly current focused gaussian processes similar concepts developed discrete valued processes 
acknowledgments partially funded onr afosr supported nserc fellowship fellowship 
pearl 
probabilistic reasoning intelligent systems 
morgan kaufman 
chou willsky 
multiscale systems kalman filters riccati equations 
ieee trans 
ac march 
gallager 
low density parity check codes 
mit press cambridge ma 
karl willsky 
efficient multiscale regularization application optical flow 
ieee trans 
im 
proc jan 
karl willsky wunsch 
multiresolution optimal interpolation satellite 
ieee trans 
geo 
rem march 
van roy 
analysis turbo decoding gaussian densities 
nips pages 
mit press 
weiss freeman 
correctness belief propagation gaussian graphical models arbitrary topology 
nips pages 
mit press 
besag 
spatial interaction statistical analysis lattice systems 
roy 
stat 
soc 
series 
demmel 
applied numerical linear algebra 
siam philadelphia 
axelsson 
bounds eigenvalues preconditioned matrices 
siam matrix anal 
appl july 
sudderth wainwright willsky 
embedded trees modeling estimation gaussian processes graphs cycles 
preparation dec 
willsky 
computationally efficient stochastic realization internal multiscale autoregressive models 
mult 
sys 
sig 
proc 
appear 
