convolution kernels natural language michael collins labs research park avenue new jersey nj research att com nigel duffy department computer science university california santa cruz cse ucsc edu describe application kernel methods natural language processing nlp problems 
nlp tasks objects modeled strings trees graphs discrete structures require mechanism convert feature vectors 
describe kernels various natural language structures allowing rich high dimensional representations structures 
show kernel trees applied parsing voted perceptron algorithm give experimental results atis corpus parse trees 
kernel methods widely extend applicability known algorithms perceptron support vector machines principal component analysis :10.1.1.128.7613
key property algorithms operation require evaluation dot products pairs examples 
may replace dot product mercer kernel implicitly mapping feature vectors new space applying original algorithm new feature space 
kernels provide efficient way carry calculations large infinite 
describes application kernel methods natural language processing nlp problems 
nlp tasks input domain neatly formulated subset objects modeled strings trees discrete structures require mechanism convert feature vectors 
describe kernels various nlp structures show allow computationally feasible representations high dimensional feature spaces example parse tree representation tracks subtrees 
show tree kernel applied parsing perceptron algorithm giving experimental results atis corpus parses 
kernels describe instances convolution kernels introduced haussler watkins involve recursive calculation parts discrete structure :10.1.1.21.2820
concentrate nlp tasks kernels useful computational biology shares similar problems structures 
natural language tasks shows typical structures nlp tasks 
structure involves observed string sentence hidden structure underlying state sequence tree 
assume training set structures task learn lou gerstner chairman ibm np lou gerstner vp np chairman pp np ibm lou gerstner chairman ibm lou sp gerstner cp chairman ibm sc lou gerstner chairman ibm nlp tasks function learned string hidden structure 
hidden structure parse tree 
hidden structure underlying sequence states representing named entity boundaries sp start person cp continue person sc start entity 
hidden states represent part speech tags noun verb preposition 
mapping input string hidden structure 
refer tasks involve trees parsing problems tasks involve hidden state sequences tagging problems 
problems ambiguity key issue analysis plausible may possible analyses 
common way deal ambiguity stochastic grammar example probabilistic context free grammar pcfg parsing hidden markov model hmm tagging 
probabilities attached rules grammar context free rules case pcfgs state transition probabilities state emission probabilities hmms 
rule probabilities typically estimated maximum likelihood estimation gives simple relative frequency estimates 
competing analyses sentence ranked probabilities 
see methods 
proposes alternative generative models pcfgs hmms 
identifying parameters rules grammar show kernels form representations sensitive larger sub structures trees state sequences 
parameter estimation methods describe discriminative optimizing criterion directly related error rate 
parsing problem running example kernels nlp structures ways example pca discrete structures classification regression problems 
structured objects parse trees prevalent nlp convolution kernels applications 
tree kernel previous section introduced pcfgs parsing method 
approach essentially counts relative number occurences rule training data uses counts represent learned knowledge 
pcfgs fairly strong independence assumptions disregarding substantial amounts structural information 
particular appear reasonable assume rules applied level parse tree unrelated applied level 
alternative attempt capture considerably structural information considering tree fragments occur parse tree 
allows capture higher order dependencies grammar rules 
see example 
pcfg new representation tracks counts single rules sensitive larger sub trees 
conceptually enumerating tree fragments occur training data note done implicitly 
tree represented dimensional vector th component counts number occurences th tree fragment 
define function number occurences th tree fragment tree represented hn 
np jeff vp ate np apple np apple np apple np np apple example tree 
sub trees np covering apple 
tree contains sub trees 
define sub tree subgraph includes node restriction entire partial rule productions included 
example fragment np excluded contains part production np note huge tree number subtrees exponential size 
design algorithms computational complexity depend representations kind studied extensively bod 
involves training decoding algorithms depend computationally number subtrees involved 
parameter estimation techniques described correspond maximum likelihood estimation discriminative criterion see discussion 
methods propose show score parse calculated polynomial time spite exponentially large number subtrees efficient parameter estimation techniques exist optimize discriminative criteria studied theoretically 
goodman gives ingenious conversion model equivalent pcfg number rules linear size training data solving computational issues 
exact implementation bod parsing method infeasible goodman gives approximation implemented efficiently 
method suffers lack justification parameter estimation techniques 
key efficient high dimensional representation definition appropriate kernel 
examining inner product trees representation 

compute define set nodes trees respectively 
define indicator function sub tree seen rooted node 
follows 
step efficient computation inner product property proved simple algebra 
define 
note computed polynomial time due recursive definition productions different 
productions pre terminals 
training parameter explicitly estimated sub tree 
searching best parse calculating score parse principle requires summing exponential number derivations underlying tree practice approximated monte carlo techniques 
pre terminals nodes directly words surface string example productions pre terminals nc ch ch nc number children tree productions nc nc 
th child node ch 
see recursive definition correct note simply counts number common subtrees rooted cases trivially correct 
recursive definition follows common subtree formed production choice child simply non terminal child common sub trees child 
child child possible choices th child 
note similar recursion described goodman goodman application conversion bod model equivalent pcfg 
clear identity 
recursive definition 
calculated jn time matrix values filled summed 
pessimistic estimate runtime 
useful characterization runs time linear number members productions 
data typically linear number nodes identical productions values running time close linear size trees 
recursive kernel structure kernel objects defined terms kernels parts quite general idea 
haussler goes detail describing construction operations valid context operations maintain essential mercer conditions 
previous lodhi examining application convolution kernels strings provide evidence convolution kernels may provide extremely useful tool applying modern machine learning techniques highly structured objects :10.1.1.130.2853
key idea may take structured object split parts 
construct kernels parts combine kernel object 
clearly idea extended recursively needs construct kernels atomic parts structured object 
recursive combination kernels parts object retains information regarding structure object 
issues remain kernel describe trees convolution kernels general 
value depend greatly size trees may normalize kernel satisfies essential mercer conditions 
second value kernel applied copies tree extremely large experiments order value kernel different trees typically smaller experiments typical pairwise comparison order 
analogy gaussian kernel say kernel peaked 
constructs model linear combination trees svm perceptron output dominated similar tree model behave nearest neighbor rule 
possible solutions problem 
haussler may kernel clear result valid kernel 
appear help experiments 
problems motivate simple modifications tree kernel 
tree fragments larger size say depth versus depth symbols 
consequently training data sense contribution larger tree fragments kernel 
method doing simply restrict depth tree fragments consider 
second method scale relative importance tree fragments size 
achieved introducing parameter modifying base case recursive case definitions respectively nc ch ch corresponds modified kernel 
size size number rules th fragment 
kernel contribution tree fragments exponentially size 
straightforward design similar kernels tagging problems see common structure nlp dependency structures 
see details 
tagging kernel implicit feature representation tracks features consisting subsequence state labels underlying word 
example paired sequence sp gerstner cp chairman ibm scg include features fsp cpg fsp gerstner cp ng fsp cp ng 
linear models parsing tagging section formalizes kernels parsing tagging problems 
method derived transformation ranking problems margin classification problem 
related markov random field methods parsing suggested boosting methods parsing 
consider set training data set example input output pairs 
parsing training examples fs sentence correct tree sentence 
assume way enumerating set candidates particular sentence 
ij denote th candidate th sentence training data fx denote set candidates loss generality take correct parse 
candidate ij represented feature vector ij space parameters model vector define ranking score example 
ij 
score interpreted indication plausibility candidate 
output model training test example argmax 

considering approaches training parameter vector note ranking function correctly ranked correct parse competing candidates satisfy conditions 
ij 
simple modify perceptron support vector machine algorithms treat problem 
example svm optimization problem hard margin version find minimizes jj subject constraints 
ij 
explicitly calculating perceptron algorithm support vector machines formulated search achieved modified dynamic programming table stores number common subtrees nodes depth 
recursive definition modified appropriately 
context free grammar taken straight training examples way enumerating candidates 
choice hand crafted grammar lfg grammar take probable parses existing probabilistic parser 
define 
ij 
initialization set dual parameters ij ij ij perceptron algorithm ranking problems 
depth score improvement table score shows parse score varies maximum depth sub tree considered perceptron 
improvement relative reduction error comparison pcfg scored 
numbers reported mean standard deviation development sets 
dual parameters ij determine optimal weights ij shorthand 
follows score parse calculated dual parameters inner products feature vectors having explicitly deal feature parameter vectors space 

ij 
example see perceptron algorithm applied problem 
experimental results demonstrate utility convolution kernels natural language applied tree kernel problem parsing penn treebank atis corpus :10.1.1.14.9706
split treebank randomly training set size development set size test set size 
done different ways obtain statistically significant results 
pcfg trained training set beam search give set parses pcfg probabilities sentences 
applied variant voted perceptron algorithm robust version original perceptron algorithm performance similar svms 
voted perceptron kernelized way svms considerably computationally efficient 
generated ranking problem having pcfg generate top candidate parse trees sentence 
voted perceptron applied tree kernel described previously re ranking problem 
trained trees selected randomly top sentence choose best candidate top test set 
tested sensitivity parameter settings maximum depth sub tree examined second scaling factor weight deeper trees 
value parameters trained training set tested development set 
report results averaged development sets tables 
report parse score combines precision recall 
define number correctly placed constituents th test tree number constituents scale score imp 
table score shows parse score varies scaling factor deeper sub trees varied 
imp 
relative reduction error comparison pcfg scored 
numbers reported mean standard deviation development sets 
proposed number true parse tree 
constituent defined non terminal label span 
score precision recall th parse respectively 
score average precision recall weighted size trees give relative improvements pcfg scores 
pcfg score perceptron score relative improvement relative reduction error 
development set cross validation choose best parameter settings split 
best parameter settings development sets split train training development sets tested test set 
gave relative goodness score best choice maximum depth score best choice scaling factor 
pcfg scored test data 
results obtained running perceptron training data 
noted previously freund schapire voted perceptron obtains better results run multiple times training data 
running data twice maximum depth yielded relative goodness score larger number iterations improve results significantly 
summary observe simple experiments voted perceptron appropriate convolution kernel obtain promising results 
methods perform considerably better pcfg nlp parsing see overview investigate kernels give performance gains methods 
compressed representation algorithms perceptron convolution kernels may computationally attractive traditional radial basis polynomial kernels 
linear combination parse trees constructed perceptron algorithm viewed weighted forest 
may search subtrees weighted forest occur 
linear combination trees bt contain common subtree may construct smaller weighted acyclic graph common subtree occurs weight process may repeated arbitrary linear combination trees collapsed weighted acyclic graph subtree occurs 
perceptron may evaluated new tree straightforward generalization tree kernel weighted acyclic graphs form produced procedure 
nature data parse trees high branching factor words chosen dictionary relatively small comparison size training data drawn skewed distribution ancestors leaves part speech tags relatively small number subtrees lower levels parse trees occur frequently majority data 
appears approach described save considerable amount computation 
intend explore 
described convolution kernels apply standard kernel algorithms problems natural language 
tree structures ubiquitous natural language problems illustrated approach constructing convolution kernel tree structures 
problem parsing english sentences provides appealing example domain experiments demonstrate effectiveness kernel approaches problems 
convolution kernels combined techniques kernel pca spectral clustering may provide computationally attractive approach problems natural language processing 
unfortunately unable expand potential applications short note issues spelled longer technical report 
aizerman braverman 

theoretical foundations potential function method pattern recognition learning 
automation remote control 
bod 

grammar experience theory language 
csli publications cambridge university press 
charniak 

statistical techniques natural language parsing 
ai magazine vol 

collins 

discriminative reranking natural language parsing 
proceedings seventeenth international conference machine learning 
san francisco morgan kaufmann 
collins duffy 

parsing single neuron convolution kernels natural language problems 
technical report ucsc crl university california santa cruz 
cortes vapnik 

support vector networks 
machine learning 
freund schapire 

large margin classification perceptron algorithm 
machine learning 
freund iyer schapire singer 

efficient boosting algorithm combining preferences 
machine learning proceedings fifteenth international conference 
san francisco morgan kaufmann 
goodman 

efficient algorithms parsing dop model 
proceedings conference empirical methods natural language processing emnlp pages 
haussler 

convolution kernels discrete structures 
technical report university santa cruz 
johnson dop estimation method biased inconsistent 
appear computational linguistics 
lodhi shawe taylor watkins :10.1.1.130.2853

text classification string kernels 
appear advances neural information processing systems mit press 
johnson geman canon chi riezler 

estimators stochastic unification grammars 
proceedings th annual meeting association computational linguistics 
san francisco morgan kaufmann 
marcus santorini marcinkiewicz :10.1.1.14.9706

building large annotated corpus english penn treebank 
computational linguistics 
scholkopf smola muller :10.1.1.128.7613

kernel principal component analysis 
scholkopf burges smola editors advances kernel methods sv learning pages 
mit press cambridge ma 
watkins 

dynamic alignment kernels 
smola bartlett schuurmans editors advances large margin classifiers pages mit press 
