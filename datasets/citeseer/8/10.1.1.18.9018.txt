submitted publication ieee signal processing letters july conditions non negative independent component analysis mark plumbley consider noiseless linear independent component analysis problem case hidden sources non negative 
assume random variables grounded non vanishing pdf positive neighbourhood zero 
orthonormal rotation wx pre whitened observations certain reasonable conditions show permutation apart scaling factor non negative probability 
suggest may enable construction practical learning algorithms particularly sparse non negative sources 
index terms independent component analysis non negative matrix factorization sparse coding ica 
authors suggested decomposition observation non negative factors non negative matrix factorization able produce useful meaningful representations real world problems 
letter show observations generated nonsingular linear combination non negative independent sources combination pre whitening removal covariance nding rotation give non negative outputs necessary sucient nding set underlying independent components provided sources non vanishing pdf zero 
ii 
problem statement suppose dimensional random vector real valued component random variables sources independent bounded non zero variance 
call source non negative pr grounded non vanishing pdf positive neighbourhood zero pr 
suppose noiseless observations bounded matrix full rank shall assume loss generality sources unit variance 
case simply scale sources gs giving ag diag sources bounded non zero variance 
means set sources identi ed subject strictly positive scaling factor 
nd linear transformation vz sources fact possible identify original scaling sources speci order source components sucient nd vz ps permutation matrix 
audio music lab department electronic engineering king college london strand london wc ls uk 
email mark plumbley kcl ac uk 
conditions non negative independent component analysis deal original observations task simpli ed perform step generating qz matrix chosen covariance mean note contrast pre whitening methods subtract mean data pre whitening lose information non negativity sources 
clear covariance ac aa exactly non zero bounded eigenvalues bounded rank calculate suitable compact eigenvector eigenvalue decomposition matrix orthogonal dimensional column vectors diag zn bounded non zero 
set orthonormal rotation matrix rr example may choose qc required 
note matrix product qa square orthonormal qa qa write wx orthonormal rotation matrix ww ij square orthonormal uu qa square orthonormal 
task nd orthonormal rotation permutation permutation matrix construct wq required 
iii 
main theorem shall write useful results 
positive real constant 
suppose real valued independent random variables 
pr pr pr pr pr pr suppose fp jp non empty set strictly positive independent random variables 
jij jij number elements independent pr pr case empty sum zero empty product inequality holds 
suppose fq jq non empty set non negative independent random variables 
pr pr lemma ij orthonormal matrix uu elements non negative permutation matrix matrix sequence fj ng distinct integers ij ij 
conditions non negative independent component analysis proof suppose ij ij ik positivity elements means ij implies ik similarly ij kj ij implies kj element non zero row column 
ij sum squares element column unity non zero elements positive 
permutation matrix 
converse clearly true elements permutation matrix take values non negative 
position state main theorem 
theorem dimensional random vector real valued non negative independent sources unit variance orthonormal rotation uu permutation matrix non negative probability 
proof case permutation matrix simply permutation nonnegative source vector non negative 
prove direction suppose non negative probability true pr tentatively suppose strictly negative element write sets mutually exclusive zero valued omitted 
note 
pr pr choose positive constant pr non zero variance 
applying results equation noting independent appear rhs case get pr pr pr pr jj pr pr pr jj ju random variables grounded probabilities rhs strictly positive resulting product implying pr 
simple verify holds case 
know pr tentative supposition element negative false 
elements non negative lemma permutation matrix 
practical algorithms perform ica related tasks expressed terms minimization cost function error function 
corollary follows directly 
conditions non negative independent component analysis corollary conditions theorem suppose cost function non negative probability 
permutation matrix permutation sources course practical algorithms cost function determined example suppose recti ed output max construct re estimate noting ww easy verify cost functions pr jx xj suitable cost functions corollary 
iv 
discussion fig 
shows illustration separation process 
gure intuitively clear possible rotation outputs negative 
appreciate depends groundedness sources pdf extend zero slack allowing output rotate going negative 
number interesting points insights emerge 
firstly system requirement inputs underlying generator matrix non negative contrast non negative matrix factorization 
indicates non negative ica approach may applicable problems non negative source causes negative ect observation presence shadow image reducing pixel intensity 
addition apart recti cation nonlinearity approach requires second order statistics 
need assumptions shape source pdfs kurtosis requirement source pdfs non negative grounded zero 
interesting explore requirements relaxed favour bounded minimum requirement 
note proof relies sources call grounded non vanishing pdf zero 
current interest analysis observations generated sparse sources :10.1.1.57.7504
sparse sources large concentration probability zero representing high probability 
expect learning algorithm developed principles outlined letter particularly sources type distribution 
previous experimentally investigated learning algorithms inhibitory connections remove covariance current proof slightly di erent pre whitening ensures covariance removed theorem convergence proof algorithms 
number authors studied learning algorithms involving orthonormal matrices types cost functions contrast functions 
discusses framework learning algorithms concept stiefel manifold set ir matrices ww inhomogeneous function rw orthonormal matrix corollary suggest candidates inhomogeneous function possible construct learning algorithm nd separated basis 
note noise free approach 
signi cant noise example original observations disrupt pre whitening step apparent correlations reduced seriously mean may rotation entirely non negative perfect reconstruction possible 
conditions non negative independent component analysis interesting investigate ect noise learning algorithms developed 
conjecture example learning algorithm robust expected rely heavily larger scale inputs 
considered task identifying independent sources noiseless nonsingular linear mixture case sources known non negative 
certain reasonable conditions including sources non vanishing pdf positive neighbourhood zero showed pre whitened sources multiplied square orthonormal rotation matrix give outputs wx underlying sources identi ed components non negative probability equivalently reconstruction error recti ed output reduced zero 
suggest non negative ica approach may lead practical learning algorithms may particularly suitable separation sparse sources 
vi 
author abdallah lai wan chan useful discussions suggestions 
lee seung learning parts objects non negative matrix factorization nature vol 
pp 
october 
comon independent component analysis new concept signal processing vol 
pp 

field goal sensory coding neural computation vol 
pp 

charles fyfe modelling multiple cause structure recti cation constraints network computation neural systems vol 
pp 

hoyer hyv non negative sparse coding network learns contour coding integration natural images submitted manuscript :10.1.1.57.7504
available online www cis hut fi pub html 
plumbley adaptive lateral inhibition non negative ica international conference independent component analysis blind signal separation ica 
theory learning weight ow stiefel manifold neural computation vol 
pp 

conditions non negative independent component analysis fig 

visualization separation process simple arti cial example showing observations pre whitened observations nal non negative outputs shading shows region removed rotation wx 
