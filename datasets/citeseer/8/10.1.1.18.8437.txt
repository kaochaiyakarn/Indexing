improving cluster availability workstation validation demonstrate framework improving availability cluster internet services 
approach models internet services collection interconnected components possessing defined interfaces failure semantics 
decomposition allows designers engineer high availability understanding interconnections isolated fault behavior component opposed ad hoc methods 
focus entire commodity workstation component possesses natural fault isolated interfaces 
define failure event reboot workstation unavailable reboot reboots symptomatic larger class failures configuration operator errors 
observations distinct clusters show time reboots best modeled weibull distribution shape parameters implying workstation reliable longer operating 
leveraging observed property design allocation strategy rebooted workstations active service validating stability allowing return service 
show simulation policy leads rule thumb constant utilization approximately workstation failures masked clients extra capacity added cluster provided reboots strongly correlated 
technique sensitive burstiness reboots opposed absolute lengths workstation 

years seen proliferation large scale internet services 
spite current economic conditions services continue everyday life 
construction highly available internet services inexact science best 
complete partial outages due failures common 
failures services important front page news 
years highly available systems produced variety contexts aerospace manufacturing 
common domains general step framework de heath richard martin thu nguyen department computer science rutgers university rd piscataway nj cs rutgers edu compose system components isolated defined interfaces characterize fault behavior component isolated unit architect system tolerate expected failure scenarios components failure behaviors 
design raid systems example similar approach successfully construction highly available storage systems 
apply framework construction highly available internet services 
particular identify commodity workstation critical component focus characterizing failure behavior evaluate method hiding node failures clients internet service 
motivated focus commodity workstations internet services run workstation clusters workstation unit replacement failures occur 
sub component disk card workstation fails entire workstation shut taken cluster 
operating system node crashes resources node lost service node rebooting 
view workstation single black box component services 
unfortunately definition constitutes failure workstation ambiguous 
example failed disk request retried operating system failure 
os memory leak socket connections painfully slow 
circumvent ambiguity defining failure shutdown crash workstation node regardless cause 
central metric concentrate time reboot time pairs adjacent reboots machine appropriate metric spite broad scope aggregates failure repair rates myriad sub components operating system hardware operator single metric discussed node unit replacement cluster services 
having defined component interest constitutes failures metric step methodology characterize course cluster systems include important components network attached disks hubs switches cables 
characterizing components scope immediate clearly important application framework design internet services 
actuality better metric time failure ttf time machine starts reaches definition failure clean shutdown crash 
unfortunately system logs contain sufficient information compute 
forced approximate ttf 
component failure behavior respect metric 
key advantage definitions reduce characterization straightforward quantification statistical distribution 
analyze logs clusters operating distinctively different environments gather data 
clusters include remote access workstations supporting general academic research workload general access workstations supporting undergraduate laboratory projects workstations supporting medium scale internet service 
obtained logs vary length slightly month years giving total observed reboots 
fit observed distribution cluster number theoretical distributions compute provides best fit 
analysis shows clusters best modeled weibull distribution shape parameter 
important implications 
distribution memory suggesting models exponentially distributed times may adequately describe node failure behaviors 
second shape parameter implies workstations reliable time 
model workstation component increasingly reliable longer operating 
propose possible resource allocation policy leverages observed failure characteristics cluster nodes hide node failures clients 
particular cluster support internet service divide available machines pools stable pool proving ground pool 
incoming client request directed machine stable pool front load director 
machine stable pool fails moved proving ground 
size stable pool decreases reboots subsequent migration nodes proving ground load increase 
load crosses threshold choose workstation node longest proving ground move back stable pool 
allocation policy elucidates fundamental trade excess capacity improved availability 
excess capacity increases rebooted machines kept proving ground longer periods time 
observation increasing reliability nodes uptime longer keep machine proving ground fail brought service stable pool 
failures occur machines proving ground failures masked clients machines process real client requests 
course machines proving ground placed test load possibly duplicate part real load utilization significant factor node failures 
evaluate allocation policy simulating clusters nodes fitted distributions generate reboot events 
validate results simulated smaller clusters actual trace data observed clusters generate reboot events 
results derived sets experiments consistent lending confidence modeling workstation weibull distribution 
results lead rule thumb excess capacity available mask approximately node failures clients provided reboots highly cor related 
excess capacity increases node failures hidden clients 
shall seen correlation reboots reduce effectiveness validation technique 
correlated failures general difficult achieve high availability example faulttolerance technique site replication tolerate failure entire cluster due external events power cooling outage service designers operators strive avoid correlated failures 
efforts maximize benefits validation technique 
analysis potential sources inaccuracies includes time repair ttr may distort characterization nodes differentiate node functioning correctly opposed node functionality impaired non crashed fault state 
example slow socket connections degraded file system performance intermittent disk failures 
fact failure distributions observed clusters best described weibull distribution shape gives confidence source significant distortion ttr affect data cluster way clusters operating different environments 
real way address concern obtain better data 
section propose ideas record information 
believe second issue serious 
clusters studied maintenance 
machines severe non crashed fault states long rebooted 
despite imprecisions introduced quality data able obtain key contributions 
argue entire workstation node primary component failure behavior understand pieces controllers network interfaces operating systems 
show gross measure workstation nodes improve service availability 
fact believe gross measure useful set measures difficult incorporate design 
second show data clusters operating different environments suggest distribution uptime exponential memory 
show appropriate policies designed implemented take advantage understanding isolated component failure characteristics provided components fault isolated interfaces 
remainder organized follows 
section describes clusters reboot measurement methodology 
section match observed distributions theoretical distributions show weibull best fit 
section describes resource allocation algorithm evaluates simulation 
section discuss weibull shape impacts strategies improve availability 
section discusses related section concludes implications 

measuring time re boots part step methodology measure actual workstation clusters 
analyzed logs clusters operating different environments system time days system time days system time days reboots function time 
shows raw reboots machine room cluster undergrad cluster internet service cluster 
time represented axis machine number axis 
point plotted time reboot occurs 
leftmost point line data point available machine 
time lines different machines cluster synchronized vertical lines imply correlated failures machines 
cluster shortest log longest log average log reboots observed machine room days days days undergraduate days days days internet service days days days table reboot log coverage 
table shows minimum maximum average coverage periods reboot logs clusters 
table shows average total number reboots observed 
machine room cluster cluster comprised machines physically housed machine room accessible remote login faculty graduate students 
machines sun ultra workstations sun sparc workstations 
machines managed different administrators necessarily coordinate maintenance schedules 
machines ran mixture solaris 
undergraduate cluster cluster comprised sun ultra workstations running solaris 
machines physically housed laboratory exclusively junior senior undergraduate computer science students projects upper division courses 
single system administrator manages entire cluster 
internet service cluster cluster comprised machines running solaris 
machines ultra workstations quad processor sun enterprise remaining netra thin cluster boxes 
physically housed machine room internet service considerable physical security 
team system administrators responsible managing cluster 
intentionally collected data clusters operating environments different characteristics accessibility machine room open number administrators type institution university corporate target user group faculty undergraduates general public 
show spite differences clusters exhibit similar reboot behavior 
confident results widely applicable 
unix machine boots records time log 
compute set sequence boot times log 
discussed discrepancy computed actual node include repair times 
unable remove inaccuracy machines record crash halt shutdown time went 
logs vary amount time covered ranging month years time periods 
plots raw reboot data table shows length time logs number reboots recorded cluster 

shape failure gathering raw data step see measured data fits theoretical distribution 
plots cumulative fraction equal time period vs time 
curves effect cumulative distribution functions cdf measured data assuming reboots completely independent 
task fit curves known cdf 
shows cdfs fitted weibull distributions superimposed observed data shall seen weibull best describes data 
fit observed data cluster cdfs theoretical distributions including exponential weibull pareto rayleigh 
theoretical distribution maximum likelihood estimation mle compute needed parameters match distribution empirical data 
computed quantile quantile plots pair empirical data theoretical distribution 
theoretical distribution matches empirical data quantile quantile plot give straight line intercept slope 
variation away straight line indicates difference observed data theoretical distribution 
decided theoretical distribu cumulative distribution failures time days cumulative distribution failures time days cumulative distribution failures time days distributions 
cumulative percentage equal time vs time machine room cluster undergrad cluster internet service cluster 
cluster cdf mle fitted theoretical weibull distribution shown superimposed observed data 
data points plotted symbols fitted curve shown dashed line 
expected measured expected measured expected measured fitting weibull distribution observed 
quantile quantile plots undergrad cluster mle parameters weibull exponential pareto distributions 
unit axes day 
graphs diagonal dashed line represents line quantile quantile data approximate 
dotted line graphs shows th percentile data 
shape scale minutes machine room cluster undergrad cluster internet service cluster table fitted shape scale parameters 
table shows resulting shape scale parameters weibull distributions fitted observed data 
tion best describes empirical data provides reasonable model visual inspection quantile quantile plots 
choosing weibull exponential mle computation serves validation visual inspection 
weibull distribution shape parameter exponential distribution 
process led conclude weibull distribution best describes reboot behaviors clusters 
gives quantile quantile plots empirical distribution vs weibull exponential pareto theoretical distributions undergrad cluster 
quantile plots clusters similar 
clearly pareto fit raleigh equally inaccurate shown interest saving space 
comparing weibull exponential observe weibull fits data closely th quantile theoretical distribution starts give longer observed 
table gives mle parameters matching weibull clusters providing evidence weibull better fit data exponential shape parameters 
shows fit weibull distribution superimposing theoretical cdfs empirical ones 
implications distribution 
important question design highly available systems particular component operating time fail time 
answer conditional probability examining hazard rate known distribution 
hazard rate distribution represents conditional probability density item fail vs time 
weibull distribution follows leading hazard rate hazard rate implications 
weibull hazard rate constant chance rebooting independent long node 
reboot increasingly time passes 
hand true clusters reboots longer node 
increasing reliability may counter intuitive 
plausible scenarios explain observed hazard rates machine taken hardware upgrade administrator may introduce configuration errors requiring rounds reboots machine stable new hardware components burn time probability failure may significantly higher operating installation new software patches may introduce bugs leading frequent crashes administrator finds stable configuration operator confusion configuration changes mount points may lead rounds reboots correct setting 
assumption independence 
distribution fitting methodology assumes reboots independent events 
visual inspection lead observer think dependency exist example vertical lines suggest reboots correlated multiple machines 
internet service cluster example correlated reboots occurred due failure air conditioning system 
preclude theoretical distribution useful 
imply carefully validate policies formulated properties theoretical distribution practice 
section propose evaluate policy validate practicality trace driven simulation addition relying synthetic reboot sequences generated theoretical distribution 

improving cluster availability having characterized failure characteristics commodity workstations step methodology leverage knowledge increase availability cluster services 
fact nodes observed clusters best described weibull distributions shape parameters corresponding monotonically decreasing hazard rate leads heuristic reboot machine placed active service proven stable operating time requiring reboot 
course machine operating validation mode loaded duplication real load best validation load workstation supporting put back service 
machine proven stable placed back active service confidence need rebooted soon 
heuristic quite intuitive system administrators 
previously impossible answer question long machine remain validation mode placed back active service 
section resource allocation policy provides automatic dynamic answer question 
follow evaluation policy effectiveness simulation 
validation policy assuming cluster nodes identical node service client request propose resource allocation policy mask node failures clients divide cluster nodes logical pools stable pool proving ground pool 
original division arbitrary stable pool start machine 
direct client requests machines stable pool 
load stable pool exceeds predefined threshold choose node longest time proving ground pool move stable pool 
repeat load drops threshold 
threshold specified variety metrics depending actual configuration cluster 
possible metrics include average number active client requests stable node queue length front load director throughput stable pool average latency servicing client requests 
node stable pool fails remove stable pool repair reboot 
node back operational mode move proving ground pool 
node proving ground pool fails repair reboot 
node back operational mode leave proving ground pool resetting uptime 
recall definition failure includes orderly shutdown subsequent reboot machine operational crashes 
means machines patches upgrade covered validation policy 
may ask 
wouldn system administrator hold machine operation validated 

currently validation completely ad hoc 
load driven migration machines proving ground stable pool gives system administrators defined validation process 
practice expect system administrators perform validation doing modifying configuration machine 
done put machine proving ground pool final validation current ad hoc validation methods insufficient 
example numerous times system administrator told authors machine operational mis configured 
strongly believe validation realistic load proving ground pool necessary regardless cause machine active service 
evaluation metric evaluating validation policy consider question metric 
originally yield proposed percentage successful requests 
yield turned coarse metric 
particular yield sensitive time interval measured 
example service loses requests evenly spread hours different loses requests minute remaining hours minutes 
despite difference equal yields hour interval 
second problem yield assumptions cluster design data partitioning order compute 
internet service designer take account focus masking node failures clients 
decided percentage reboots observable clients appropriate metric measure remainder section 
percentage reboots masked clients easily computed number reboots occur proving ground pool divided total number reboots 
scale load generate avoidance curve 
armed avoidance curve data actual numbers reboots designer compute metrics harvest yield availability data partitioning scheme parametric assumptions table 
simulation model simple model cluster internet services 
model comprised frontend load director cluster back nodes 
front distributes client requests back nodes queue lengths back nodes means load balancing 
front fails 
back node service request time simplification real practice think request representing batch client requests serviced concurrently 
metric centers reboots occur simplification affect validity simulation 
assume data resources structured way back node service client request 
event driven simulator uses file reboot events describe upset load sequence fault events 
input files real traces synthetic reboot events simulation environment 
real traces come processed logs synthetic event sequences generated small external program 
arrival client requests exponentially distributed average rate set achieve particular average load service 
request service times exponentially distributed 
node repair time constant 
lag time load director register node failure constant 
table summarizes parameters 
keep simulation overly complex known average load drive validation allocation policy 
particular algorithm parameter distribution value load poisson varies total capacity service poisson minute repair constant minutes lag constant seconds failures weibull varies shape scale table simulation parameters 
table shows constants distributions event driven internet service simulation 
failure occurs failed machine stable pool move machine proving ground pool repair reboot machine move machine greatest uptime proving ground pool stable pool maximum size proving ground pool average load standard deviation load 
average load plus standard deviations ensure size stable pool sufficient service bursts requests 
evaluation performed sets experiments study effectiveness technique 
investigated impact changing shape changing scale changing cluster size compared effectiveness technique real trace file synthetic upset load 
set experiments simulated operation clusters nodes minutes approximately days 
generated random upset load weibull distribution constant scale minutes varied shape parameter 
table gives shape parameters studied corresponding numbers reboots 
second set generated random reboot traces weibull distribution variable scale parameter constant shape average shape parameters empirical data see table 
simulated clusters nodes minutes 
table gives scale parameters studied corresponding numbers reboots 
third set studied effect cluster size keeping constant shape constant scale upset load distribution 
simulated clusters different sizes different constant loads minutes 
final set experiments real reboot traces observed clusters assess differences performance policy arising differences real reboot data synthetic upset load generated known weibull distributions 
trace driven simulation realistic theoretic distribution disadvantage percent reboots masked masked reboot percentage vs utilization constant scale time utilization percent reboots masked masked reboot percentage vs utilization constant shape time utilization percentage failures masked successfully 
shows percentage failures masked successfully vs load shape parameter varied constant scale minutes shows percentage reboots masked successfully vs load scale varied constant shape 
shows technique sensitive shape insensitive scale implying absolute uptime critical predictability 
scale shape shape reboots scale reboots table number reboots 
table shows number reboots changes constant minute interval keep scale constant minutes change shape keep shape constant vary scale 
scale experiments observed cluster size time period 
studying effects scale shape difficult limited data 
addition real traces contain external correlation effects designer may able mask 
results derived trace data validation results sets experiments 
synthetic upset load 
shows results sets experiments 
shows results third set experiment 
results observations 
validation allocation policies successfully mask significant numbers reboots 
example shape representative observed clusters policy masks close reboots load 
load increases policy masks reboots 
designers take away general rule thumb 
service extra capacity reserve mask approximately reboots clients provided reboots percent reboots masked masked reboot percentage vs cluster size constant shape scale time cluster size effects cluster size 
shows percentage reboots masked successfully vs cluster size different utilizations synthetic upset load 
correlated 
change depending exact characteristics particular cluster roughly accurate scale parameters observed 
availability sensitive failures predicted opposed absolute length 
shows policy sensitive shape parameter scale parameter 
result shows improved system availability come concentrating predicting failures simply improving component lifetimes 
recall smaller shape parameter higher burstiness reboots 
failure process exhibits memory behavior characterize shape better algorithm predict likelihood reboot effective proving ground 
sufficient excess capacity validation policies 
clear regime operation avoidance curve changes slowly offered load 
shape pa cluster size simulated total scale time days reboots shape days machine room undergraduate internet service table trace driven simulation parameters 
table shows cluster sizes time periods total reboots events able simulate trace driven methodology 
dropped machines cluster logs shorter rest shortened simulated time 
percent reboots masked percent reboots masked vs utilization real data machine internet utilization failures masked trace driven simulation experiments 
shows percentage reboots masked successfully vs load failures driven collected traces 
note case ignore reboot machine cluster 
allows closely approximate steady state system limited trace data 
rameters interest stable operating regime ends utilization 
inflection point avoidance curve drops sharply 
expected stable pool excess capacity ensure reasonable service face bursty arrival client requests 
little excess capacity machines quickly migrate stable pool reboot limiting effects policy 
technique effective cluster contains machines 
effect seen sharp drop left modest utilizations 
cluster size matters workstations moved pools discrete units entire workstations 
larger cluster size larger proving ground pool 
larger proving ground pool choices validated workstations move back stable pool necessary 
trace driven upset load 
advantage simulator design actual reboot traces drive simulation allowing verify policy effective practice 
order capture real correlation effects degrade performance proving ground fix window realtime generate upset load normalizing time log hypothetical starting point 
logs varied length portion cluster trace 
choosing time window balance maximizing cluster size maximizing time period observe steady state behaviors 
table gives parameters simulations 
gives close view portion days raw data selected clusters 
table shows shape parameters undergraduate internet service clusters substantially higher vs chosen time period 
shows avoidance vs utilization curves clusters 
observe avoidance curves follow expected trend 
curve stable operating regime followed sharp drop load exceeds algorithm ability mask failures 
performance technique worse predicted earlier simulations cases 
effects account discrepancy synthetic trace driven upset loads 
small size clusters degrade performance 
synthetic upset loads shows sharp drop avoidance curve workstations 
second important effect degradation due correlation 
workstations fail may excess capacity proving ground compared reboots correlated 
example omitted component factor model air conditioning unit may cause reboots 
omitted components may policy ineffective load single reboot signifies loss large portion nodes closely spaced time 
machines rushed back stable pool shown stable 
machine cluster smallest size careful visual inspection reveals cluster correlation 
coupled fact cluster smallest shape parameter consistent machine cluster giving best performance smallest rate performance degradation load increases 
case rule changes approximately excess capacity mask upwards reboots clients 
avoidance curve internet cluster starts expected slightly worse predicted theoretical distribution 
utilization performance degrades rapidly predicted 
suspect due high correlation reboots cluster casual inspection shows vertical bands suggesting high degree correlation 
authors know case correlation reboots caused air conditioning failures machine room 
pair air conditioning units failed system administrator powered cluster 
correlation difficult faulttolerance technique focus workstations sub components including approach mask failures 
better method case expand set modeled components include air 
avoidance curve undergrad cluster shows behavior internet cluster inflection point comes system system time days time days enlargement timeline 
shows enlargement days undergraduate cluster shows similar enlargement machine room cluster 
notice events aligned undergraduate cluster indicated higher degree correlation 
earlier performance degradation sharper 
believe due high correlation reboots 
shows number vertical bands suggesting reboots highly correlated 
factors causing correlations cluster may maintained single administrator environment conveniently shutdown entire cluster events upgrading operating system 
relatively smaller cluster size machines compared internet cluster reduces ability validation algorithm mask faults 
despite differences results quite encouraging show policy mask significant percentage failures despite effects smaller cluster sizes importantly potential correlation reboots long correlation overwhelming 

discussion broad approaches design space mask component failures validation rejuvenation replication 
section argue choice approach dependent nature component behavior 
cases excess capacity required mask failures distinct underlying assumptions component behavior knowledge predict failures 
validation approaches components prevented acquiring system load crossed critical uptime threshold pass burn period 
time failure predictor increasing uptime leads decreasing likelihood failure 
validation policy example technique applied workstation clusters 
techniques effective component hazard rate decreasing time true respect workstations 
hand rejuvenation techniques appropriate hazard rate increases time 
components behave manner system designed avoid component burn burn works examined technique viewing operating system component see section 
replication approaches appropriate failure distribution exponential memory 
conditions time predict component behavior 
parameter predict component failures replicating way bring increase availability 
designs raid take approach storage total replication errorcorrecting codes 
course techniques mutually exclusive 
example sense replication hide failures masked validation policy 
choice approach critically hinges definition component 
complex systems components deeply nested 
choosing level draw line component boundaries currently art science 
general component defined interface failure semantics 
context internet service design choosing entire workstation node component reboots failure semantic reasonable choice 
research needed guide designer choosing component boundaries implicitly resulting behavioral distribution ease cost validation rejuvenation duplication approaches component 
example internet service context replacing entire rack mounted workstation easier cost effective operator time replacing internal component motherboard 

related vast amount related measuring software hardware reliability schemes improving system availability contexts 
section focus contexts applicable clusters running scalable internet services 
closest related study characterized behavior microsoft windows nt machines :10.1.1.22.1901
study examined reboots events machines recorded month period 
quite encouraging shape parameter fitted weibull distributions failures close weibull distributions solaris clusters 
addition able windows nt event logging delve deeper causes reboots able logs 
majority reboot events caused reasons hardware software failures 
observed correlated reboots 
extend result workstation clusters internet service setting 
weibull shape parameters information propose validation strategies masking failures 
study group produced detailed state machine model workstation node requires fairly detailed knowledge windows nt 
examined failure components online image service 
focused failure rates node components examining entire node 
interestingly commonly blamed culprits failures operating system scsi drives fact quite reliable 
examined causes software failures 
typically look characterize application bugs operating system errors 
works differ fundamentally choice component 
applications operating systems large components internet service perspective nested larger component workstation node 
rejuvenation approaches proposed different contexts 
original looked telecommunication switches long running scientific programs 
study examined rejuvenation context clusters rejuvenation policies effective 
models assumed increasing hazard function 
empirical group expected mean time operating system run memory days far longer observed average day reboot interval 
factors leading rejuvenation effective operating systems require greater mean 
proposed extending idea rejuvenation layers software 
works improve software robustness mask software failures lead node reboot characterization shows improve masking node failures entire workstation node increases availability time 
discrepancy software failure node reboots leads search causes 
studies tandem machines suggested addition hardware software humans operators play key role generating faults 
context works limited specialized machines tightly controlled environments 
argues administrator operator errors soon dominant factor component failures 
windows nt context validates arguments :10.1.1.22.1901
expanding theme emerged human factor studies determine actual humans behave context raid repair 
choice workstation reboot metric capture human factor effects large degree leaves open question human factor dominant cause observed reboot distribution 

proposed application general step framework construction highly available internet services 
framework understanding individual failure behavior components isolated defined interfaces 
demonstrated framework defining workstation component critical internet services studying failure behavior designing resource allocation algorithm improve system availability 
viewed complete system hardware software indication system extended period time rebooted system 
fact results show opposite sun workstations running various versions solaris distinct environments machine long period time left 
demonstrated simple policy takes advantage understanding behavior mask significant percentage node failures clients 
simulations show validation technique results rule adding additional excess capacity workstation node reboots cluster masked clients provided cluster sufficient large machines reboots highly correlated 
trace driving simulation results show high correlation reboots significantly degrade ability algorithm mask faults 
results cluster correlation reboots show able achieve rule thumb practice 
reboots cluster correlated maintained multiple administrators suffer external faults air conditioning failures 
points importance ensuring non correlated reboots administrator operational cluster take entire cluster significant portions cluster line upgrades patching internet service providers take care adequate facilities external failures lead cluster failures 
rejuvenation techniques proving ground algorithm relies predicting component failures time predictor 
data shows assumed direction prediction rejuvenation techniques components decay time may flawed respect machines viewed entirety 
results probable countless number event types lead workstation failures far outweigh effect individual component decay time :10.1.1.22.1901
results cover timescales order years 
may case extended observations time scales see increasing failure rates 
useful operating range computer hardware years software long regimes may irrelevant construction highly available services 
results point system starts may far clean state 
configuration errors different components may cause need subsequent reboots leading idea machine infant mortality period fully goes away 
lend credibility system tends stay 
evidence aside initial data claim true warrant investigation 
research area greatly aided installing workstation nodes detailed recording information flight recorder box 
observation noted detailed knowledge windows nt event types outages classified :10.1.1.22.1901
envision recording just error messages proactively recording machine load configuration state user operator actions 
example nvram os heartbeat record reboots accurately measure downtime 
file fingerprinting techniques record node configuration application changes 
newer server machines temperature intrusion detection sensors logged 
placing proximity detectors record human movement near machine may prove quite valuable predicting machine fail 
low cost storage today possible record events long periods 
analysis performed historical bases predictions stability individual machines real time 


reducing cost system administration disk storage system built commodity components 
technical report csd university california berkeley 
brewer 
lessons giant scale services 
ieee internet computing july august 
brown patterson 
availability benchmarks case study software raid systems 
usenix annual technical conference june 
brown patterson 
err human 
workshop evaluating architecting system dependability easy july 
candea fox 
recursive turning reboot scalpel 
th workshop hot topics operating systems hotos viii may 
chandra chen 
fail faulty programs 
symposium fault tolerant computing ftcs june 
chou yang chelf hallem engler 
empirical study operating systems errors october 
claudia keefe 
doomed ebay 
salon com tech feature doomed ebay index html oct 
fox brewer 
harvest yield scalable tolerant systems 
proceedings hot topics operating systems hotos vii rio rico az mar 
fox gribble chawathe brewer 
scalable cluster network services 
proceedings th acm symposium operating systems principles oct 
gray 
computers done 
proceedings fifth symposium reliability distributed software database systems jan 
gray 
census tandem system availability 
ieee transactions reliability oct 
hu 
britannica com user volume 
news cnet com news html oct 
huang fulton 
software rejuvenation analysis module applications 
th symposium fault tolerant computer systems pages june 
jain 
art computer systems performance analysis 
john wiley sons 
iyer 
failure data analysis lan windows nt computers 
th symposium reliable distributed systems srds pages 
murphy gent 
measuring system software reliability automated data collection process 
quality reliability engineering international 
murphy 
windows dependability 
ieee international conference dependable systems networks dsn june 
ross 
course probability 
prentice hall 
internet com 
ebay outage twice week 
internet com news article html aug 
patterson 
analysis error behaviour large storage system 
technical report ucb csd university california berkeley computer science division 
harper hunter trivedi 
analysis software rejuvenation cluster systems 
joint international conference measurement modeling computer systems acm sigmetrics june 
trivedi 
measurement model estimation software aging operational software systems 
international symposium software reliability engineering november 
white 
sigma design example 
seventh annual applied power electronics conference exposition feb 
xu iyer :10.1.1.22.1901
networked windows nt system field failure data analysis 
pacific rim international symposium dependable computing dec 
