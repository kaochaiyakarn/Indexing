learning scope application information extraction classi cation david blei computer science division berkeley blei cs berkeley edu andrew robotics institute carnegie mellon university ieee org andrew mccallum whizbang labs research carnegie mellon university mccallum whizbang com probabilistic approaches text classi cation information extraction typically builds statistical model words assumption data exhibit regularities training data 
text data sets additional scope limited features predictive power applicable certain subset data 
example information extraction web pages word formatting may indicative extraction category di erent ways di erent web pages 
central diculty features capturing exploiting new regularities encountered previously unseen data 
propose hierarchical probabilistic model uses local features formatting global features word content 
local regularities represented unobserved random parameter local data set regularities captured inference process 
process akin automatically classi er local regularities newly encountered web page 
exact inference intractable approximations point estimates variational methods 
empirical results large collections web data show method signi cantly improving performance traditional models global features 
fascinating aspect classi cation extraction web richness information formatting layout directory structures linkage 
particular web site structural regularities powerful features common classi cation extraction tasks 
example wished extract titles books amazon com rely fact main page book book title appears location font 
diculty information web site di erent structural regularities successfully apply models built site extraction classi cation 
response people created tools facilitate hand tuning site speci extractors cohen jensen :10.1.1.119.6900
chief obstacle modeling types regularities hand intervention statistical models assume data independent identically distributed iid 
example case certain disjoint subsets data share identi able useful regularities occur entirety 
examples subsets may local regularity include patients particular hospital voice sounds particular speaker vibration data particular airplane 
leveraging local regularities signi cantly improve performance learner local features simple highly indicative 
central diculty trained algorithm applied new subsets data local regularities encountered training 
knowledge directly applicable new data traditionally global regularities features independent identically distributed data 
presents scoped learning probabilistic framework takes advantage local regularities previously unseen data 
introduce new graphical model framework parameters globally learned training data parameters particular local subsets psfrag replacements cn wn fn graphical model representation scoped learning model text domain 
square plates represent repetitions documents words 
global word content wn generated independently parameters local document 
local formatting feature fn generated dependent document speci parameters 
parameters turn generated document 
data represented unobserved random variables 
inference model naturally leads process learn local regularities new subsets data information inferred global features 
describe generative discriminative approaches training model 
generative model learning scope section describe generative model multi dimensional data components data exhibit kinds scope 
components called global features governed parameters shared datapoints 
components called local features governed parameters shared particular subset datapoints 
subset local features exhibits kind regularity called locale 
assume data model naturally organized similar locales 
furthermore regularities respect local features unseen data may di erent local regularities data parameter estimation 
simplicity explanation describe model information extraction web pages data naturally exhibits kind structure regularities model exploit 
note model easily applied kind data exhibits scope way described 
cast classi cation problem goal label certain individual words web page di erent extraction categories job title book review 
datapoints speci cally word content corresponding formatting easily divided subsets form di erent locales page boundaries 
training data multiple locales observed class label word formatting pair 
data unlabeled web pages new locales label word formatting pair extraction category 
global features web data individual words page formatting 
type feature predictor extraction category way documents 
example word engineer probably part job title appears page www whizbang com www google com 
local features domain formatting individual words 
features font size predictor category particular web page 
regularity capture necessarily hold pages data set 
example job titles www whizbang com may green job titles www google com may red 
traditional probabilistic classi cation approach problem treat word formatting pairs iid probabilistic model 
approach formatting features limited predictive power estimating parameters learn specialized meaning local features domains 
contrast model developed successfully exploit formatting features unseen pages 
generative process parameterization key problem constructing generative model local global features word content global features formatting local features iid joint distribution depends particular document individual pair belongs 
model documents iid introducing unobserved random parameter governs local features particular document 
particular word document contain corre sponding word content fw wn formatting ff fn class labels fc assume document generated process 
extraction categories generate formatting feature parameters 

words document generate nth class label 
generate nth word wn jc 
generate nth formatting feature jc 
models joint distribution local parameters class labels words formatting features wn jc jc exhibited graphical model 
number extraction categories size vocabulary di erent possible values formatting feature 
parameters model multinomials words wjc multinomial categories distributions random parameters note document extraction category random parameter multinomial values formatting feature jc cn random parameter point simplex parameterize dirichlet distribution recall dirichlet exponential family distribution simplex 
need dirichlet parameters full distribution document speci parameters 
important observe parameters explicitly formatting features generated particular document parameters random document speci parameters govern features 
key element model formatting features generated di erent parameters di erent documents 
note assume local features global features independent class label 
reasonable assumption 
formatting word depends role document extraction category word particular 
training model maximum likelihood estimation set documents class labels observed 
label global words local formatting features independent 
global parameters wjc estimated knowledge formatting words divided documents 
words train class conditional word distributions class prior traditional generative probabilistic classi er naive bayes 
global parameters dirichlet parameters 
learning quantify general trends local contexts important font size usually indicative class label 
experiments parameters give uniform distribution random local distributions 
parameters xed need estimate parameters formatting features training set 
formatting data de nition model generated di erent parameters generated training data 
inference power scoped learning model lies inference routines 
new web page classify word formatting pairs words formatted respect rest document 
model inference routines class labels naturally lend process leverage know globally order infer local parameters 
re ne original globally estimated labels local features 
key di erence inference routines scoped learning model standard classi er aim simultaneously infer information labels document treating word formatting pair separately 
labels dependent unobserved local parameter 
particular compute posterior distribution set information labels document 
principle computed marginalizing document speci parameter invoking bayes rule cjw wn jc jc cn wn jc jc exact computation integral computationally infeasible denominator integral simplex product sums 
approaches approximate inference 
approaches operate model induced single document question 
approach maximum likelihood estimation model 
bayesian approach uses variational methods approximate infeasible integral equation 
map estimates local parameter approximate point estimate posterior tractable 
see observe integral equation disappears replace instance 
yields labels word formatting pair independent 
label pair arg max cn wn jc jc natural point estimate local parameter posterior mode map estimate arg max jf 
simply maximum likelihood estimate local parameters maximize likelihood single document question hold global parameters xed 
set unobserved random variables expectation maximization em algorithm maximize expected log likelihood formatting features document cn fn log jc em algorithm coordinate ascent function 
step computes posterior distribution extraction categories fn jc wn jc step nds new estimate jc jc fcn cg wn notation sum indicates training instances alternating steps guarantees take positive steps equation 
resulting nd best value word formatting pair 
variational approximation alternative point estimate approximate true posterior distribution class labels variational methods 
approach better approximate actual integrals equation 
particular de ne factorized variational distribution new document unobserved random variables set variational dirichlet parameters extraction category set variational multinomial parameters categories word formatting pair 
free parameters optimize nd variational distribution close possible kl divergence true posterior arg max kl kp jw similar framework blei 
yields similar updates variational distribution 
ij dirichlet parameter jth value ith information category 
variational dirichlet parameters maximized ij ffn jg note similar posterior dirichlet distribution uniform prior expected counts variational distribution 
variational multinomial parameters updated wn jc exp log note equation similar step section replaced exponential expected value log posterior variational dirichlet 
easily compute expectation log cfn cf digamma function 
iterating updates converge variational parameters closest true posterior 
important notice variational approximation may mitigate problem em algorithm tting inside locale maintains computational complexity implementation simplicity 
view attempt approximate integral bayesian point view 
particular point estimate local parameters justi ed large data limits jw peaked particular value 
speci cally modeling small subsets word formatting pairs large data assumption generally invalid expect gain attempt proper integration 
perspective see update equations variational distribution follow variational bayes recipe attias 
discriminative model learning scope assumed section training generative model local formatting features 
cases generative model places strong unwarranted independence assumption local features may desirable learn local discriminative model captures 
graphically model reversing arc pointing random parameter category node 
exact inference intractable 
nd point estimate optimal conditional parameters maximizing conditional log likelihood log log cn jf wn jc note implicitly assuming generative global classi er 
recover completely discriminative solution bayes rule rewrite wn jc wn 
rewriting exposes dependence likelihood marginal probability global features wish avoid modeling 
enforcing constraint constant respect independent factorizes likelihood 
optimize local parameters respect complete data likelihood cond cn fn log jf em algorithm derivation follows map approximation 
step computes posterior distribution fn jf step corresponds maximizing weighted log likelihood cn fn log jf trivial implement discriminative probabilistic classi ers maximum entropy 
conditional likelihood objective function interesting information theoretic content 
note criteria equivalent maximizing empirical mutual information 
consider class variables local parameters channel try recover information possible global features local features 
example empirical results data test scoped learning model real world data sets obtained web 
rst data set html documents automatically divided sets words similar layout characteristics 
group words called text node hand labeled contained job title 
split data testing set training set keeping text nodes single document documents single site 
unlabeled document goal model correctly label text nodes 
described global features data word content local features formatting words 
formatting features chosen formatting tags html source 
second data set consists web pages web sites page hand labeled press release 
new site goal nd press release pages site 
global features words documents 
local features come url 
expects particular web site url structure consistent function document refers 
example press release pages particular may subdirectory news may les named similarly html 
particular local features grams characters appear url 
job title data set split data training testing sets 
note actual scoped learning model data sets slightly augmented depicted 
particular class label associated multiple local global features 
repetitions empirical mutual information mean log empirical distribution features 
nodes inside repetition class labels 
training inference procedures practically chose simpli ed setting 
example page example web page job title corpus aid understanding scoped learning model power gain inference local parameters 
shows webpage test set 
clearly job titles page consistent formatting 
simple naive bayes classi er jobs page due lack training data 
particular misses family services director massage server teachers 
job titles exception server classi er relatively unsure label slightly away classifying correctly 
furthermore global classi er sure jobs correctly labels teacher art coordinator customer service rep 
apply scoped learning model document learn correctly labeled jobs probability font orange high conditioned job title class label 
posterior inference class labels yields correct labeling server giving signi cant increase classi cation accuracy 
see section improvement typical data set dramatically improve classi cation performance 
quantitative results data sets binary classi cation problems quantify performance precision recall 
particular change threshold probability classify example positive class 
doing trade classifying negative examples positive nding true positive examples low precision high recall missing true positives reducing number false positives high precision low recall 
graph left illustrates performance naive bayes classi er generative variants inference scoped learning model job title data set 
rst variant map estimate local parameters learned em 
note consistently dominates performance global 
second variant variational inference algorithm 
algorithm dominates global classi er generally dominates map estimate 
particular note high levels recall important part graph practical perspective algorithms exploit local features show signi cant improvement global classi er 
measure performance traditional accuracy percentage correct labels threshold measure method reduces error 
graph right illustrates performance algorithms press release data set 
case global classi er discriminative classi ers maximum entropy classi ers gaussian prior parameters 
relatively easy data set obtain high precision low moderate recall interesting task maintain precision higher recall 
scoped learning models outperform global classi er levels recall reducing error third recall half recall 
note data set local features highly dependent discriminative approach outperforms map estimate 
related central focus emphasize utility features exhibit scope limited regularity methods learning regularity previously unseen data 
thread related blum mitchell pac style training independent views data sucient predict class label error uence making nal prediction 
training exploit scope limited features conditional independence assumption similiar 
probabilistic classi cation relational data taskar extends notion multiple views data set multiple kinds relationships connections components data 
authors probabilistic relational models prm represent entire data set interconnected graphical model 
inference unknown class variables implicitly exploits regularities learned di erent kinds features data 
sense models cast prm locale represented separate group nodes separate local parameter 
representation maximum likelihood sample webpage job title dataset 
global classi er depicted bold rectangles correctly labeled jobs 
inference scoped learning model depicted thin bold rectangles correctly labeled jobs 
estimate parameters exactly supports map approximation section 
treating local parameter random variable explicitly treat locale iid integrate 
seen section leads signi cant increase performance map estimate 
related learning supervised tasks combination labeled unlabeled data particularly transduction unlabeled set test set joachims 
connection truly labeled data particular locale need learn local parameters uncertain labels obtained existing classi er 
previous area model locales represent di erence local global features opportunity locales training time learn hyperparameters local features 
nal related thread slattery mitchell authors build system automatically adjusts trained classi er regularities discovered data classifying 
similar scoped learning model method explicit notion locale designed features web page links 
furthermore probabilistic model method ord bene ts integrating parameter uncertainty 
probabilistic model training inference procedures exploit local global features various prediction tasks 
large real world problems model performs significantly better traditional global classi er cases reducing error half 
comparing approximation methods nd map estimate tends integration variational methods alleviates issue 
furthermore discriminative approach outperform generative methods particularly local features independent 
described model terms web data emphasize occurence global features scope limited features naturally arises domains text 
furthermore developed model levels scope local global 
clear recall precision map global nb var dirichlet recall precision map global maxent maxent scoped learning dominates global classi er 
left precision recall curve summarizes experimental results job title information extraction task 
right precision recall curve summarizes experimental results press release classi cation task 
concept recursively extended arbitrary levels 
example features may common english language web sites particular corporate domain particular web page 
developed tested model simple mixture models underlying local global models 
quite natural extend incorporate sophisticated models local global features relationship class labels 
example treated task independent classi cation natural model nite state sequence problem 
extending model way straightforward introducing chain structure class labels 
model inference algorithms variant standard inference procedures hidden markov models 
promising model scoped learning 
assume local global features disjoint 
imagine models features appear global local settings inference determine model generated particular occurrence feature 
useful example situations word usage exhibits local regularities 
second assumed uniform distribution local parameters locale 
may situations want nd maximum likelihood estimate parameters determine useful data 
acknowledgments done authors working whizbang labs summer discussing matters playing pool 
huan chang help job title data william cohen advice code related generating local page features jonathan baxter tom minka andrew ng insightful discussions quass extraction 
attias 

variational bayesian framework graphical models 
advances neural information processing systems 
blei ng jordan 

latent dirichlet allocation 
advances neural information processing systems 
blum mitchell 

combining labeled unlabeled data training 
proceedings conference computational learning theory 
cohen jensen 

structured wrapper induction system extracting information semistructured documents 
proceedings ijcai workshop adaptive text extraction mining 
joachims 

transductive inference text classi cation support vector machines 
proc 
th international conf 
machine learning pages 
morgan kaufmann san francisco ca 
slattery mitchell 

discovering test set regularities relational domains 
proc 
th international conf 
machine learning pages 
morgan kaufmann san francisco ca 
taskar segal koller 

probabilistic classi cation clustering relational data 
ijcai pages 
