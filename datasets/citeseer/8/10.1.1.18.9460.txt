featureboost meta learning algorithm improves model robustness joseph sullivan cs cmu edu john langford jcl cs cmu edu rich caruana caruana cs cmu edu avrim blum avrim cs cmu edu school computer science carnegie mellon university forbes ave pittsburgh pa machine learning algorithms lazy extract training set minimum information needed predict labels 
unfortunately leads models robust features removed obscured test data 
example backprop net trained steer car typically learns recognize edges road learn recognize features stripes painted road useful road edges disappear tunnels obscured passing trucks 
net learns minimum necessary steer training set 
contrast human driving remarkably robust features obscured 
motivated propose framework robust learning biases induction learn different models inputs 
meta algorithm robust learning called featureboost demonstrate problems backprop nets nearest neighbor decision trees 

motivation consider backprop net learning steer car 
alvinn system pomerleau principal internal features learned alvinn nets detect left right edges road 
typically alvinn nets learn internal features detect road phenomena useful steering road centerlines signs trees traffic people creates problem left right edges road obstructed passing vehicles missing bridges tunnels 
human steering remarkably robust loss features 
human drivers fall back number alternate features different subsets road features come view 
backprop nets learn steer better learn recognize road features centerlines caruana 
force backprop nets learn variety road features learning steer 
related problem arises health care cooper 
basic inputs age gender blood pressure available patients enter hospital 
measurements rbc counts available patients 
expect models trained predict patient risk pre hospital features usually outperform models trained predict risk inputs 
models perform poorly patients admitted hospital missing hospital features 
models pre hospital inputs accurate patients admitted hospital marginalized models trained features 
force learning learn models better predictions input features hospital attributes missing test cases 
edges road hospital features available models learned usual way perform 
alvinn health care problems difficulty arises features missing obscured test cases 
boosting algorithms adaboost way learned models robust feature 
main features edges road obscured missing training cases boosting places emphasis cases predicted poorly 
emphasis forces learning algorithm features road centerlines cases 
unfortunately boosting learns centerlines strongly emphasizing cases missing road edges centerlines may visible images 
boosting learn features better training data containing features learn 
boosting take full advantage redundant information training set 
introduces general framework induction called robust learning motivated desire model situations features may corrupted missing ways adequately represented training set 
guided framework devise meta learning algorithm called featureboost trains models different subsets features 
final prediction featureboost combines predictions models depend different overlapping subsets features robust missing obscured features 
develop follows general framework robust learning 
examine specialization framework suggests way improve robustness 
develop meta learning algorithm featureboost inspired model 
test featureboost variety learning problems machine learning algorithms 

framework robust learning basic goal force ordinary base learning algorithm extract information training data order learn prediction rules robust possibility missing corrupted features test cases 
precise theoretical model perfect useful way thinking problem motivates algorithm section 
usual pac model assume training examples features fixed distribution input space 
assume target concept wish learn access base learning algorithm chooses hypotheses hypothesis class simplicity fix arbitrary error cutoff say hypothesis formalizing notion training data contains useful redundant information 
specifically say error bad 
set hypotheses robust subset features remains features corrupted 
purposes model need pin precisely corrupted means long error defined hypothesis error non decreasing additional features corrupted 
error tends increase features corrupted 
low error hypothesis features corrupted bad high error subset features corrupted say destroys access examples labeled goal algorithm produce robust subset exists 
producing single hypothesis want algorithm produce set hypotheses matter features corrupted hypotheses achieve goal assume base learning algo rithm property feed labeled examples subset features corrupted produce respect subset fea tures corrupted exists 
say things setup 
natural brute force method achieves goal making calls set features feed data features corrupted add hypothesis produced fails output hypothesis know robust robust exists 
brute force algorithm works impractical powerset features exponential number features ideally want algorithm running time polynomial unfortunately perverse may possible 
consider instance case contains hypotheses depends different subset features changes bad just corrupted 
case robust subset 
hand natural strategy proved polynomially calls certain special cases 
strategy follows initialize done find smallest set features destroy initially run examples features corrupted place hypothesis fails halt failure 
statements algorithm 
theorem suppose hypothesis associated feature set feature remains uncorrupted 
algorithm calls runs linear time iteration 
proof start time new added assumption smallest set features destroys iteration increases size number iterations course ideally single robust rule weighted vote hypotheses fact goal experimental section 
goal appears trickier model theoretically consider weaker goal producing robust set 
algorithmically way find depend kind hypotheses considering worst case brute force try size size 
theorem suppose hypothesis associated feature set features remains uncorrupted 
algorithm calls proof consider graph node features initially edges 
iteration algorithm put edge pair uncorrupted features edge new hypothesis produced 
notice set produced step assumption vertex cover set vertices covering edges iteration adds new edge graph 
algorithm done longer vertex cover size theorem follows twin facts node reach degree greater point set size minimum vertex cover size maximum matching graph 
algorithm develop section thought practical version strategy real data real goal producing single robust hypothesis 

special case robust learning consider specialization robust learning framework preceding section 
particular wish motivate idea majority vote boost robustness 
previous section assume exists unknown underlying distribution examples features drawn 
examples labeled unknown target function assume disjoint subsets features predict label cotraining assumption feature sets blum mitchell 
learning algorithm labeled examples asked produce hypothesis predictions 
goal minimize error wish understand error changes alteration test distribution 
specifically consider test distribution percentage disjoint feature sets corrupted uniform randomly 
corruption feature set example accomplished picking second example substituting values feature set approach leaves marginal distribution feature set values unchanged corruption 
extremes consider 
occam razor motivated learner decision tree attempts find small set features predict label 
extreme assume decision tree focuses particular feature sets feature sets corrupted decision tree affected time 
just labels decision tree outputs random value relies corrupted feature results error linearly increasing second extreme occurs model uses features 
difficult state effect accuracy making assumptions learning algorithm 
assume decision tree disjoint feature set resulting tree outputs passed majority voting function errors decision tree independent errors decision trees 
increases error increase rate dependent value number feature sets 
decision tree error viewed flipping random coin bias 
coins come wrong label overwhelm predictors vote 
probability error distributed cumulative distribution binomial tail 
algorithm second extreme dominates doing feature set corruption doing significantly better 
motivates create learning algorithm mixture experts expert focusing different set features 

featureboost algorithm variant boosting features boosted examples 
boosting base learning algorithm learn called multiple times 
time different distribution training examples 
step distribution altered increase probability harder parts space 
different hypotheses combined single hypothesis 
adaboost boosting algorithms alter distribution emphasizing particular training examples 
think examples matrix inputs row case 
adaboost emphasizes rows examples matrix featureboost emphasizes columns features matrix 
similar nearest neighbors appeared bay goals algorithms differ considerably 
goal featureboost see table search alternate hypotheses features 
distribution features kept updated iteration distribution altered stored distribution updated conducting sensitivity analysis features model learned current iteration 
distribution increase emphasis unused features iteration attempt produce different table 
pseudocode describing featureboost algorithm 
featureboost input learn examples features learn learning algorithm number iterations learn importance output sub hypotheses 
repeated sub hypotheses combined meta hypothesis robust 
intuition combination adaboost freund schapire 
update fac tor decreases turn increases weight associated final hypothesis 
accurate hypotheses influence final 
calculate importance individual features repeatedly picking random training example assigning random value feature distribution values feature training set 
error hypothesis example calculated 
iterations change average error hypothesis detected 
error changes yield importance vector features scaled en tries pseudocode importance calculation provided table 
experimented approaches function biasing learn distribution features step table 
options range hard removing features soft scaling adding noise individual features 
hard method progressive feature removal applicable learning algorithm 
calculate current full marginalization error calling learn 
defined error feature picked randomly 
gives upper bound error marginalize features training data weighted distribution features marginalized performance worse table 
pseudocode describing importance calculation algorithm importance input examples features hypothesis labeling examples number iterations random example random example output importance feature process seeks feature set marginalized performance worse approaching practice observe occasionally easy hard worse threshold require features removed features remain 
pseudocode algorithm helper function marginalize provided table table 
table 
pseudocode describing marginalize algorithm marginalize input examples features distribution features number features marginalize total number statistical iterations previous hypothesis random example draw feature replacement random example output marginalized error table 
pseudocode describing algorithm input examples features distribution features iteration number total number featureboost iterations total number statistical iterations previous hypothesis error previous hypothesis marginalize marginalize output 
empirical results features removed demonstrate artificial neural nets ann nearest neighbor knn decision trees dt 
dt ind buntine 
ann layer backprop nets hidden units conjugate gradient descent early stopping hold sets 
knn unweighted euclidean distance contrast featureboost meta learning algorithms mixture simple mixture experts ad freund schapire 
test meta learning algorithm domains uci vote domain real pneumonia problem synthetic problem created demonstrate 
vote domain consists voting record votes 
label party democrat republican belongs 
trial examples training testing 
pneumonia domain uses real patient data consisting hospital features cooper 
label risk death 
trial examples training testing 
synthetic domain threshold threshold function drawn uni interval examples drawn uni example features encoded ways gray code peaks code binary code unary code value divided gray code similar binary code feature changes time counting peaks code consists features values calculated gaussian centered example value caruana 
unary code thermometer code outputs example value fills reach features 
examples training testing 
experiments investigate robust learning algorithm random uniform feature corruption test data 
case test set random percent features corrupted 
features corrupted choosen independently case case 
results suggest featureboost dominates adaboost simple mixtures experts features test set corrupted 
simple mixtures experts improve robustness mixture experts typically uses features learn 
improvement featureboost simple mixture experts focus learning different sets features 
graphs suggest benefit featureboost pronounced dt pronounced knn 
expected dt biased features knn unweighted euclidean distance uses features 
synthetic domain threshold allows corrupt features different ways 
input encoded redundant ways gray code peaks code binary code unary code value divided multiple disjoint subsets features predict target 
shows performance featureboost threshold decision trees 
graphs compare error dt voting algorithms show robust method corrupted test data 
test sets corrupted different ways left hand graph example percentage features selected random corruption 
right hand graph example percentage feature sets grey code binary code corrupted 
feature set corruption simulates clusters features tending occluded corrupted parts image occluded event admitted hospital causes entire set features missing 
adaboost mixture voting methods hit hard grouped featureboost weakened demonstrates robustness 

discussion featureboost addresses problem benefit redundancy input features 
machine learning methods lazy learn abridged models 
redundancy learning algorithm capable learning accurate models number different subsets features 

learn simplest models accurate training data 
simple models depend fewer features fail exploit redundancy input features robust features missing oc pneumonia threshold voting adaboost mixture featureboost knn ann dt 
percent improvement learning domains pneumonia threshold voting learners knn ann dt ways generating experts 
results confidence intervals trial experts 
axis percent random corruption individual features 
axis percent difference error learn error methods 
corrupted 
featureboost meta learning algorithm underlying learning algorithm abridged learning multiple models different possibly overlapping sets features 
ultimate learning procedure train separate models power set input features combine models weighted estimated accuracy 
approach impractical course problems having attributes power set large 
despite success featureboost test problems difficulties 
optimal schedule biasing feature depends learning algorithm target function 
featureboost considered heuristic search space ideal subsets features 
goal search find diverse sets explanations output label 
synthetic threshold function features encoded multiple redundant disjoint subsets inputs want feature different models 
real world problems tion rarely clean may best overlapping subsets features 
hard version featureboost 
conceive soft versions 
learning algorithms examined allow ways features 
neural nets multiply value feature emphasis 
theoretically possible alteration change learned hypothesis practice hypothesis vary 
nearest neighbor weighted inner product 
decision trees multiply information gain feature features weight comparing features 
difference corrupted features missing features important 
values missing technique similar sleeping experts freund may effective 
setting sub hypotheses taken combined form robust hypoth change test error adaboost mixture featureboost features adaboost mixture featureboost feature sets 
improvement boosting dt featureboost mixture models adaboost threshold 
left individual features randomly corrupted 
right corruption happens entire sets features 
results confidence intervals trials experts 
axis percent difference error learn error methods higher better 
weighted majority hypotheses missing values vote 
softer form vote strength proportional confidence may real world settings 
just algorithms boosting examples believe algorithms boosting features 
fact augment adaboost robust 
corrupt training examples adaboost adaboost eventually places emphasis examples main features corrupted learns models depend features 
find corrupted training sets performs corruption training set identical corruption test set perform poorly test set corrupted differently training set 
combining adaboost featureboost may yield best methods 
note begun develop theoretical framework robust learning theoretical guarantees featureboost comparable adaboost 

related pomerleau noted alvinn nets learned models failed unexpected new features arose expected features obscured road edges appearance disappearance irrelevant features disrupt network driving network training demonstrate irrelevance 
may obscure replace features appear normal image remain features image theory driving network possible 
reason type transitory disturbance causes trouble network trained relatively short stretch road miles 
result training network exposed possible driving situations encounter driving autonomously pomerleau 
clearly motivation robust learning similar pomerleau devised method called structured noise bears resemblance process corrupting selected features noise featureboost 
structured noise method added subtracted occluded localized regions road images training set way biasing network robust transitory feature inclusion occlusion 
featureboost meta algorithm goal train network robust train multiple models different subsets features 
suspect featureboost benefit pomerleau method corrupting localized regions images applying featureboost image recognition problems 
robust learning related uav unspecified attribute value goldman rfa restricted attention ben david models learning studied computational learning theory literature 
uav model query model training test data may missing features partially specified example goal answer completions label output correct label 
rfa learning learning algorithm allowed examine small number features training example goal produce hypothesis low pac style error fully specified data 
framework similar rfa setup roles training test data reversed assume learning algorithm choose features missing 

framework robust learning encompass ways input features differ train test distributions 
featureboost assumes difference modelled random corruption elimination selected features 
effective means focussing learner attention away features probably accurate model features get corrupted real world 
example features images tend corrupted connected regions 
medicine procedures may return results dozens measurements time 
examples features come go clumps 
way corrupted features clump depends domain 
may useful devise specialized versions robust learning data corruption model better fits processes affect features domain 
alternate approach robust learning examining uses feature selection train models compact sets features 
removes selected features trains additional models remaining features 
approach may better model domains medicine features missing common features corrupted occluded 
featureboost model robust learning limitations 
limitation current version featureboost restricted classification problems 
unable test featureboost autonomous vehicle steering problem initially motivated investigation 
currently developing extension featureboost handle regression problems 
able provide theoretical guarantees featureboost similar available boosting algorithms adaboost 
part difficulty stems fact robust learning useful train test distribution differ ways may difficult characterize 
goals find restricted possibly realistic models robust learning able strong theoretical guarantees 

summary machine learning algorithms extract minimum information training set need accurately predict labels 
learn model performs training data learning 
laziness models learn robust test cases missing occluded features 
redundancy inputs exploited learn models robust loss corruption input features 
learning algorithm lazy try learn different models available inputs possible 
motivated introduced general learning framework called robust learning 
metalearning algorithm called featureboost machine learning algorithm applied robust 
demonstrated featureboost different learning methods backprop nets nearest neighbor decision trees 
results suggest featureboost methods learn models robust loss important features test data 
bay 

combining nearest neighbor classifiers multiple feature subsets 
proceedings fifteenth international conference machine learning pp 

san francisco morgan kaufmann 
ben david 

learning restricted focus attention 
journal computer system science 
blum mitchell 

combining labeled unlabeled data training 
proceedings th annual conference computational learning theory pp 

new york ny acm press 
buntine 

learning classification trees 
statistics computing 
caruana 

multitask learning 
doctoral dissertation school computer science carnegie mellon university 
cooper aliferis aronis buchanan caruana fine glymour gordon meek mitchell richardson spirtes 

evaluation machine learning methods predicting pneumonia mortality 
artificial intelligence medicine 
freund schapire 

decision theoretic generalization line learning application boosting 
proceedings second european conference computational learning theory pp 

springer verlag 
freund schapire singer warmuth 

combining predictors specialize 
proceedings ninth annual acm symposium theory computing pp 

goldman scott 

learning examples unspecified attribute values 
proceedings th annual conference computational learning theory pp 

new york ny acm press 
pomerleau 

neural network perception mobile robot guidance 
boston kluwer academic publishers 
