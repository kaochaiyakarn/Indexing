japanese dependency analysis cascaded chunking propose new statistical japanese dependency parser cascaded chunking model 
conventional japanese statistical dependency parsers mainly probabilistic model efficient scalable 
propose new method simple efficient parses sentence deterministically deciding current segment modifies segment immediate right hand side 
experiments kyoto university corpus show method outperforms previous systems improves parsing training efficiency 
dependency analysis recognized basic process japanese sentence analysis number studies proposed 
japanese dependency structure usually defined terms relationship phrasal units called segments segments 
previous statistical approaches japanese dependency analysis matsumoto kudo matsumoto probabilistic model consisting steps 
estimate modification probabilities words probable segment tends modify 
second optimal combination dependencies searched candidates dependencies 
probabilistic model efficient needs calculate probabilities possible dependencies creates number segments sentence training examples sentence 
addition probabilistic model assumes pairs dependency structure independent 
propose new japanese depen taku kudo yuji matsumoto graduate school information science nara institute science technology taku ku aist nara ac jp dency parser efficient simpler probabilistic model performs better training testing kyoto university corpus 
method parses sentence deterministically deciding current segment modifies segment immediate right hand side 
assume independence constraint dependencies probabilistic model section describes general formulation probabilistic model parsing applied japanese statistical dependency analysis 
define sentence sequence segments 
bm syntactic structure sequence dependency patterns dep dep 
dep dep means segment bi depends modifies segment bj 
framework assume dependency sequence satisfies constraints 

japanese head final language 
rightmost segment modifies exactly segment segments appearing right 

dependencies cross 
statistical dependency analysis defined searching problem dependency pattern maximizes conditional probability input sequence mentioned constraints 
assume dependency probabilities mutually independent rewritten dep fij fij 
fn dep fij represents probability bi modifies bj 
fij dimensional feature vector represents various kinds linguistic features related segments bi bj 
obtain argmax combination probabilities 
generally optimal solution identified bottom parsing algorithm cyk algorithm 
problem dependency structure analysis estimate dependency probabilities accurately 
number statistical machine learning approaches maximum likelihood estimation matsumoto decision trees maximum entropy models support vector machines kudo matsumoto applied estimate probabilities 
order apply machine learning algorithm dependency analysis prepare positive negative examples 
usually probabilistic model possible pairs segments dependency relation positive examples segments appear sentence dependency relation negative examples 
total training examples number segments sentence produced sentence 
cascaded chunking model probabilistic model estimate probabilities dependency relation 
machine learning algorithms svms estimate probabilities directly 
kudo matsumoto sigmoid function obtain pseudo probabilities svms 
theoretical endorsement heuristics 
probabilistic model scalability usually requires total training examples sentence 
hard combine probabilistic model machine learning algorithms svms require polynomial computational cost number training examples 
introduce new method japanese dependency analysis require probabilities dependencies parses sentence deterministically 
proposed method combined type machine learning algorithm classification ability 
original idea method stems cascaded method applied english parsing abney 
introduce basic framework cascaded chunking parsing method 
sequence base phrases input algorithm 

scanning input sentence chunk series base phrases single non terminal node 

chunked phrase leave head phrase delete phrases inside chunk 
finish algorithm single non terminal node remains return step repeat 
apply cascaded chunking parsing technique japanese dependency analysis 
japanese head final language chunking regarded creation dependency segments simplify process japanese dependency analysis follows 
put tag segments 
tag indicates dependency relation current segment undecided 

segment tag decide modifies segment immediate right hand side 
tag replaced tag 

delete segments tag immediately followed segment tag 

terminate algorithm single segment remains return step repeat 
shows example parsing process cascaded chunking model 
input model linguistic features related modifier output model tags 
training model simulates parsing algorithm consulting correct answer training annotated corpus 
training positive negative examples collected 
testing model consults trained system parses input cascaded chunking algorithm 
moved warm heart 
initialization input tag input tag deleted input tag deleted input tag deleted input tag deleted input warm heart moved tag finish example parsing process cascaded chunking model think proposed cascaded chunking model advantages compared traditional probabilistic models 
simple efficient cyk algorithm probabilistic model requires parsing time number segments sentence 
hand cascaded chunking model requires worst case segments modify rightmost segment 
actual parsing time usually lower segments modify segment immediate right hand side 
furthermore cascaded chunking model training examples extracted parsing algorithm 
training examples required cascaded chunking model smaller probabilistic model 
model reduces training cost significantly enables training larger amounts annotated corpus 
assumption independence dependency relations probabilistic model assumes depen dency relations independent 
cases parse sentence correctly assumption 
example coordinate structures parsed independence constraint 
cascaded chunking model parses estimates relations simultaneously 
means dependency relations narrower scope current focusing relation considered feature sets 
describe details section 
independence machine learning algorithm cascaded chunking model combined machine learning algorithm works binary classifier cascaded chunking model parses sentence deterministically deciding current segment modifies segment immediate right hand side 
probabilities dependencies necessary cascaded chunking model 
dynamic static features linguistic features supposed effective japanese dependency analysis head words parts speech tags functional words inflection forms words appear segments distance segments existence punctuation marks 
solely defined pair segments refer static features 
japanese dependency relations heavily constrained static features inflection forms particles constrain dependency relation 
sentence long possible dependency static features determine correct dependency 
cope problem kudo matsumoto introduced new type features called dynamic features created dynamically parsing process 
example relation determined modification relation may influence dependency relation 
segment determined modify segment information kept segments added new feature 
specifically take types dynamic features experiments 
modify 

modifier types dynamic features segments modify current candidate 
boxes marked segments modify current candidate modifier 
boxes marked segment modified current candidate 
boxes marked support vector machines kind machine learning algorithm applied cascaded chunking model support vector machines vapnik experiments state art performance generalization ability 
svm binary linear classifier trained samples belongs positive negative class follows 
xl yl xi yi xi feature vector th sample represented dimensional vector yi class positive negative class label th sample 
svms find optimal separating hyperplane maximal margin strategy 
margin seen distance critical examples separating hyperplane 
omit details maximal margin strategy realized optimization problem minimize subject yi xi 

furthermore svms potential carry non linear classifications 
leave details vapnik optimization problem rewritten dual form feature vectors appear dot products 
simply substituting dot product xi xj dual form kernel function xi xj svms handle non linear hypotheses 
kinds kernel functions available focus dth polynomial kernel xi xj xi xj th polynomial kernel functions allows build optimal separating hyperplane takes account combinations features experiments discussion experimental setting annotated corpora experiments 
standard data set data set consists kyoto university text corpus version kurohashi nagao 
sentences articles january st january th training examples sentences articles january th test data 
data set kudo matsumoto 
large data set order investigate scalability cascaded chunking model prepared larger data set 
sentences kyoto university text corpus version 
training test data generated fold cross validation 
feature sets experiments shown table 
static features basically taken list 
head word hw rightmost content word segment 
functional word fw set follows fw rightmost functional word functional word segment fw rightmost inflection form predicate segment fw hw 
static features include information existence brackets question marks punctuation marks features show relative relation segments distance existence brackets quotation marks punctuation marks 
segment dynamic feature type set functional representation fr feature fw fw follows fr lexical form fw pos fw particle adverb conjunction fr inflectional form fw fw inflectional form 
fr pos tag fw 
segment dynamic feature set pos tag pos subcategory hw experiments carried alpha sever mhz training linux ghz testing 
third degree polynomial kernel function exactly setting kudo matsumoto 
performance test data measured dependency accuracy sentence accuracy 
dependency accuracy percentage correct dependencies dependency relations 
sentence accuracy percentage sentences dependencies determined correctly 
experimental results results new cascaded chunking model previous probabilistic model svms kudo matsumoto summarized table 
employ experiments probabilistic model large dataset data size large current svms learning program terminate realistic time period 
number training examples cascaded chunking model quarter probabilistic model feature set dependency accuracy sentence accuracy improved cascaded chunking model 
time required training parsing significantly reduced applying cascaded chunking model sec sec 
probabilistic model vs cascaded chunking model seen table cascaded chunking model accurate efficient scalable probabilistic model 
difficult apply probabilistic model large data set takes hours weeks carry experiments standard data set svms require quadratic computational cost number training examples 
impression may natural higher accuracy achieved probabilistic model candidate dependency relations training examples 
experimental results show cascaded chunking model performs better 
list significant contributions cascaded chunking model behaves compared probabilistic model 
probabilistic model trained candidate pairs segments training corpus 
problem training exceptional dependency relations may training examples 
example suppose segment appears right hand side correct similar content word pair segment negative example 
negative better correct candidate different point sentence 
may true negative example meaning positive sentences 
addition segment modified modifier cross dependency constraints similar content word correct relation exception 
ignore exceptions segments modify segment immediate right hand side 
candidates dependency relation training examples committed number exceptions hard trained 
looking particular powerful heuristics dependency structure analysis segment tends modify nearer segment possible important train current segment modifies segment immediate right hand side 
cascaded chunking model designed heuristics remove exceptional relations potential improve performance 
effects dynamic features shows relationship size training data parsing accuracy 
shows accuracy dynamic features 
generally results dynamic feature set better results 
dynamic features constantly outperform static features size training data large 
cases improvements considerable 
table summarizes performance dynamic features 
results static features modifier segments head word surface form pos inflection type functional word surface form pos pos subcategory inflection type inflection form brackets punctuation marks position sentence seg distance case particles brackets ments quotation marks punctuation marks dynamic features type form inflection represented functional representation type pos pos subcategory head word table features experiments data set standard large model cascaded chunking probabilistic cascaded chunking probabilistic dependency acc 
sentence acc 
training sentences training examples training time hours parsing time sec sentence dependency accuracy table cascaded chunking model vs probabilistic model dynamic static number training data sentences training data vs accuracy cascaded chunking standard data set conclude dynamic features effective improving performance 
comparison related table summarizes results japanese dependency analysis 

report kyoto university corpus training testing achieve accuracy building deleted type diff 
model dynamic features dynamic feature dependency acc 
sentence acc 
ab ac bc abc table effects dynamic features standard data set statistical model maximum entropy framework 
extend original probabilistic model learns class modify modify learns classes modify 
model avoid influence exceptional dependency relations 
training test data achieve accuracy 
difference considerable 
model training corpus sentences acc 
model cascaded chunking svms kyoto univ 
kyoto univ 
kudo probabilistic model svms kyoto univ 
probabilistic model posterior context kyoto univ 
probabilistic model hpsg edr probabilistic model dt boosting edr probabilistic model ml edr 
hpsg japanese grammar restrict candidate dependencies 
model uses candidates restricted grammar features nearest second nearest farthest modifier 
model take longer context account disambiguate complex dependency relations 
features static dynamic features model 
directly compare model different corpus edr corpus times large corpus 
reported accuracy worse model 

report edr corpus training testing achieve accuracy decision tree boosting 
decision tree take combinations features svms easily overfits 
avoid overfitting decision tree usually weak learner boosting 
combining boosting technique decision tree performance may improved 

report performance decision tree falls added lexical entries lower frequencies features boosting 
think decision tree requires careful feature selection achieving higher accuracy 
new japanese dependency parser cascaded chunking model achieves accuracy kyoto university corpus 
model parses sentence deterministically deciding current segment modifies segment immediate right hand side 
model outperforms previous probabilistic model respect accuracy efficiency 
addition showed dynamic features significantly con table comparison related tribute improve performance 
steven abney 

parsing chunking 
principle parsing 
kluwer academic publishers 
yuji matsumoto 

japanese dependency structure analysis lexicalized statistics 
proceedings emnlp pages 
satoshi shirai 

decision trees construct practical parser 
machine learning 
hiroshi jun ichi tsujii 

hybrid japanese parser hand crafted grammar statistics 
proceedings coling pages 
taku kudo yuji matsumoto 

japanese dependency structure analysis support vector machines 
empirical methods natural language processing large corpora pages 
kurohashi makoto nagao 

kyoto university text corpus project 
proceedings anlp japan pages 
satoshi sekine hitoshi 

japanese dependency structure analysis maximum entropy models 
proceedings eacl pages 
murata satoshi sekine hitoshi 

dependency model posterior context 
procedings sixth international workshop parsing technologies 
vladimir vapnik 

statistical learning theory 
wiley interscience 
