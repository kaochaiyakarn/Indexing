trainable methods surface natural language generation systems surface natural lan guage generation trainable annotated corpora 
systems called nlg nlg require corpus marked domain specific semantic attributes system called nlg requires corpus marked semantic attributes syntactic dependency infor mation 
systems attempt produce ical natural language phrase domain specific semantic representation 
nlg serves baseline system uses phrase frequencies generate phrase step nlg nlg maximum entropy probability models indi generate word phrase 
sys tems nlg nlg learn determine word choice word order phrase 
experiments generate phrases describe flights air travel domain 
presents trainable systems sur face natural language generation nlg 
surface nlg purposes consists generating grammatical natural language phrase expresses meaning input semantic representation 
systems take corpus machine learning approach surface nlg learn generate phrases semantic input cally analyzing examples phrases cor responding semantic representations 
nation content semantic representation deep generation discussed 
systems assume input semantic repre sentation fixed deal express natural language 
discusses previous approaches sur face nlg introduces trainable systems surface nlg called nlg nlg nlg 
quantitative evaluation experiments air travel domain discussed 
adwait ratnaparkhi ibm tj watson research center box yorktown heights ny ibm 
com previous approaches templates easiest way implement surface nlg 
template describing flight noun phrase air travel domain flight departing city fr time dep arriving city time arr words starting variables representing departure city departure time arrival city arrival time respectively values extracted environment template 
approach writing individual templates convenient may scale complex domains hundreds thousands templates necessary may shortcomings maintainability text quality see reiter discussion 
sophisticated surface genera tion packages fuf surge elhadad robin bateman meteer lavoie ram bow produce natural language text semantic representation 
packages require linguistic sophistication order write semantic representation flexible minor changes input accomplish major changes generated text 
trainable approaches known au thor surface generation purely statistical machine translation mt systems berger corpus generation sys tem described langkilde knight :10.1.1.103.7637
mt systems berger learn gen erate text target language straight source language aid explicit se mantic representation :10.1.1.103.7637
contrast langkilde knight uses corpus derived statistical knowl edge rank plausible hypotheses grammar surface generation component 
trainable surface nlg trainable surface nlg goal learn mapping semantics words wise need specified grammar knowledge base 
systems attribute value pairs semantic representation suffice representation limited domain air travel 
example set attribute value pairs city fr new york city city seattle time dep date dep wednesday represent meaning noun phrase flight seattle departs new york city day 
goal specifically learn optimal attribute ordering lexical choice text generated attribute value pairs 
example nlg system auto matically decide attribute ordering flights new york evening better worse ordering flights evening new york 
furthermore automatically decide lexical choice flights departing new york better worse choice flights leaving new york 
motivation trainable surface generator solve problems way reflects observed usage language corpus manual effort needed construct grammar knowledge base 
trainable nlg systems sume existence large corpus phrases values interest replaced corresponding attributes words corpus generation templates 
shows sample training data words marked attributes 
nlg systems steps shown table 
systems nlg nlg nlg implement step produce sequence words intermixed attributes template tributes 
values ignored step replace corresponding attributes phrase produced step 
baseline surface generation model nlg simply chooses frequent template training data corresponds set attributes 
perfor mance intended serve baseline result sophisticated models discussed 
specifi cally returns phrase corresponds attribute set empty string phrase tata ta phrases occurred training data phrase training data frequency natural language phrase phrase set attributes nlg fail generate novel combi nation attributes 
nlg gram model surface generation system nlg assumes best choice express attribute value set word sequence highest probabil ity mentions input attributes exactly 
generating word uses local infor mation captured word grams certain non local information subset original attributes remain generated 
local non local information integrated features maximum entropy prob ability model highly pruned search procedure attempts find best scoring word sequence ac cording model 
probability model probability model nlg conditional dis tribution genera tion vocabulary 
special symbol 
generation vocabulary consists words seen training data 
form maximum entropy probability model identical berger ratnaparkhi wi attri wi wi wi ri yij wi wi attri wi ranges :10.1.1.103.7637
wi wi attri history wi de notes ith word phrase attri denotes attributes remain generated posi tion phrase 
fj fj called features capture information history useful estimating wi wi wi attri 
features nlg described section feature weights aj obtained improved iterative scaling algorithm berger set maximize likelihood training data :10.1.1.103.7637
probability sequence wl wn attribute set length pr wa wi attri feature selection feature patterns nlg shown table 
actual features created match ing patterns training data ac tual feature derived word bi gram template wi wi wi wi attr wi flight city fz attri flights air city fr city time date dep strip flights air city fr city leaving time date dep flights leaving city fr going city stime date dep flights leaving city fr city time dep air flight city fr city date dep city fr city air flight date dep strip flights city fr city input step output step input step output step table sample training data city fr city time dep date dep flight city departs city fr stime dep date dep flight city departs city fr time dep date dep city fr new york city city seattle time dep date dep wednesday flight seattle departs new york city wednesday low frequency features involving word grams tend unreliable nlg system uses features occur times training data 
search procedure search procedure attempts find word se quence wl wn length input attribute set 
wn symbol 
attributes mentioned 
attributes mentioned heuristically set maximum phrase length 
search similar left right breadth search fraction word sequences considered 
specifically search procedure implements recurrence wn top wg top wn set wn top scoring sequences length expression wn returns sequences wl wi wl wi wn wi 
expression top wn finds top sequences wg 
search sequence ends 
removed placed set table steps nlg process completed sequences 
completed hypotheses discovered wn computed search terminates 
incomplete sequence satisfy condition discarded com plete sequence satisfy condition discarded 
search terminates completed sequences possibly differing lengths 
currently normalization different lengths sequences length equiprobable pr len nlg chooses best answer express tribute set follows nlg pr len pr completed word sequences satisfy conditions nlg search described 
nlg dependency information nlg addresses shortcoming nlg previous words necessarily best informants predicting word 
stead nlg assumes conditioning cally related words history result accurate surface generation 
search procedure nlg generates syntactic dependency tree description attributes remaining word bi gram attribute word tri gram attribute feature wi wi wi attri 
wi 
attri wi 
wi 
attri wi 
wi 
attri table features patterns nlg 
occurrence instantiated actual value training data 
top bottom word sequence left right word predicted context syntactically related parent grandparent siblings 
nlg requires corpus notated tree structure sample depen dency tree shown 
probability model probability model nlg shown conditions parent closest siblings direction child relative parent attributes remain generated 
just nlg distribution improved iterative scaling algo rithm find feature weights aj 
expression chi denotes ith closest child headword par denotes parent headword dir left right denotes direc tion child relative parent denotes attributes remain generated tree headword predicting ith child 
example flights chl evening generating left children chl generating right children 
shown proba bility dependency tree expresses tribute set computing word tree probability generating left children right children 
formulation left children generated inde right children 
nlg nlg assumes uniform distribution length probabilities pr left children pr right children certain maxi mum length 
feature selection feature patterns nlg shown ta ble 
actual features created matching patterns training data 
features nlg access syntactic informa tion features nlg 
low fre quency features involving word grams tend unreliable nlg system uses fea tures occur times training data 
furthermore feature derived table looks particular word chi attribute allow occurred descendent dummy root node generate top head word phrase chi dependency tree training set 
example condition allows features look chi city toe dis allows features look ch city fre search procedure idea search procedure nlg similar search procedure nlg explore fraction possible trees con sorting advancing top trees point 
dependency trees built left right word sequences nlg built current head initially root node order 
predict left child call xt 
jump 
recursively predict children xt 
resume 
predict right child call xr 
done predicting children current head 
recursively predict children 
resume incomplete trees generated particular attribute twice completed trees generated necessary attribute discarded search 
search terminates complete trees trees max imum length discovered 
nlg chooses best answer express attribute set follows argmax pr tia iga completed dependency trees satisfy conditions nlg search described 
experiments training test sets evaluate nlg nlg nlg derived semi automatically pre existing annotated corpus user queries air travel domain 
annotation scheme total attributes represent flights 
flights chicago afternoon sample dependency tree phrase evening flights chicago afternoon 
signs indicate left right child respectively 
ik yj chi chi chi pa dtr att chi chi par dir attr jffi chi chi par di chi par dir lk ol chi chl par tv dir att pr tia prl lt wla ht nlg equations probability ith child head word chi description siblings yi ft wla prr ght wl pr left children yl ln chi lw chi chi par dir left attr pr right children ri chi lw chi chi par dir right parent sibling nlg equations probability dependency tree parent grandparent feature chi ch chi par dir 
chi 
chi 
chi 
dir 
chi 
chi 

dir 


par 
dir 
table features patterns nlg 
occurrence instantiated actual value training data 
system parameters nlg nlg io nlg io correct ok bad output error reduction table weighted evaluation trainable surface generation systems judge system parameters correct ok bad output error reduction nlg nlg nlg io nlg table weighted evaluation trainable surface generation systems judge system nlg nlg nlg parameters correct ok bad output error reduction nlg io io table unweighted evaluation trainable surface generation systems judge system parameters nlg nlg io nlg io correct ok bad output error reduction nlg table unweighted evaluation trainable surface generation systems judge training set consisted templates ing flights test set consisted tem plates describing flights 
systems training set tested attribute sets extracted phrases test set 
ex ample test set contains template flights city leaving stime dep surface gener ation systems told generate phrase attribute set city stime dep 
put nlg attribute set city city fr time dep shown table 
appear objective auto matic evaluation method generated text correlates actual person judge output 
judges author colleague manually evaluated output systems 
judge assigned phrase systems rankings correct perfectly acceptable ok tense agreement wrong word choice correct 
errors corrected post processing morphological analyzer 
bad words missing extraneous words output system failed produce put total attribute sets test examples judges needed evalu ate unique attribute sets attribute set city fr city occurs times test data 
subjective evaluation generation output measuring word overlap edit distance sys tem output set automatic scoring method 
believe method accurately measure correctness grammaticality text 
ideal arguably superior auto matic evaluation fails correlate human linguistic judgement 
results manual evaluation values search feature selection param eters systems shown tables 
values determined manually evaluating output common attribute sets training data 
weighted results tables account mul tiple occurrences attribute sets un weighted results tables count unique attribute set city fr city counted times weighted results un weighted results 
weighted results represent testing conditions realistically unweighted results judges im provement nlg nlg nlg nlg 
nlg cuts error rate nlg counting rank correct wrong 
nlg cuts error rate nlg requires far annotation training data 
nlg chance generating data fails completely novel attribute sets 
unweighted results judges improve ment nlg nlg surprisingly judge slight decrease judge increase accuracy nlg nlg 
un weighted results show baseline nlg common attribute sets correctly generates unweighted cases weighted cases 
discussion nlg nlg systems automatically generalize knowledge inherent training corpus templates generate templates novel attribute sets 
probability generated text time dep flights city fr city time dep flights city fr city stime dep flights city fr city flights city fr city stime dep stime dep flights city fr city table sample output nlg 
dependency tree structures shown 
typical values attributes time dep city fr new york city miami additional cost associated producing syntactic dependency annotation necessary nlg virtually additional cost associated nlg collecting data identifying attributes 
trainable surface nlg systems pa differ grammar systems determine attribute ordering lexical choice 
nlg nlg automatically determine attribute ordering simultaneously searching multiple 
grammar approaches pref erences need manually encoded 
nlg nlg solve lexical choice problem learning words features maximum entropy probability model correlate tribute local context elhadad uses rule approach decide word choice 
trainable approaches avoid expense crafting grammar determine attribute order ing choice accurate grammar approaches 
short phrases ac typically grammar ap proaches grammar writer cor rect add rule generate phrase interest error detected 
nlg nlg tune feature patterns search pa rameters training data guarantee tuning result gener ation accuracy 
approach differs corpus surface generation approaches langkilde knight berger :10.1.1.103.7637
langkilde knight maps semantics words concept ontology grammar lexicon ranks resulting word lattice corpus statistics nlg nlg automatically learn mapping semantics words corpus 
berger ai describes statistical machine translation approach generates text target language directly source text 
nlg nlg statistical learning ap proaches generate actual semantic rep resentation 
comparison suggests statis tical mt systems generate text interlingua way similar knowledge translation systems 
suspect statistical generation ap proach perform accurately domains sim ilar complexity air travel 
air travel main length phrase fragment describe attribute usually words 
domains require complex lengthy phrase fragments describe single attribute ing model features look word grams 
domains greater ambiguity word choice require thorough search larger value expense cpu time memory 
im semantic annotation scheme air travel property rich accurately represent meaning domain simple yield useful corpus statistics 
approach may scale domains freely occurring newspaper text semantic notation schemes property 
current approach limitation ignores values attributes strongly influence word order word choice 
limitation overcome features values nlg nlg discover hypothetical example flights leaving city fr preferred flights city fr city fr particular value miami 
presents systems known author statistical learning approach produce natural language text directly se mantic representation 
information solve attribute ordering lexical choice problems normally specified large hand written automatically collected data feature patterns combined maximum entropy framework 
nlg shows just local gram information perform baseline nlg shows syntactic information improve genera tion accuracy 
conjecture nlg nlg domains com plexity similar air travel available notated data 
author scott serving second judge scott axelrod kishore papineni todd ward helpful comments 
supported part darpa contract mda 
john bateman 

development envi ronment multilingual linguistic resource devel opment sentence generation 
technical re port german centre information technol ogy gmd institute integrated information publication systems ipsi darmstadt ger 
adam berger stephen della pietra vin cent della pietra 

maximum entropy approach natural language processing 
com putational linguistics 
michael elhadad jacques robin 

overview surge reusable comprehensive syn tactic realization component 
technical report ben gurion university beer israel 
michael elhadad kathleen mckeown jacques robin 

floating constraints lexical choice 
computational linguistics pages 
irene langkilde kevin knight 

genera tion exploits corpus statistical knowl edge 
proceedings th annual meeting association computational linguistics th international conference computa tional linguistics university montreal mon quebec canada 
benoit lavoie owen rambow 

fast portable realizer text generation systems 
proceedings fifth conference ap plied natural language processing pages washington march april 
meteer mcdonald anderson forster gay si bun 

design implementa tion 
technical report technical report coins university massachusetts amherst 
adwait ratnaparkhi 

maximum entropy models natural language ambiguity tion 
ph thesis university pennsylvania 
ehud reiter 

nlg vs templates 
proceed ings th european workshop natural language generation leiden netherlands 

