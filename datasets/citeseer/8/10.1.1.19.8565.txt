cameras new eyes structure motion jan neumann ferm ller aloimonos center automation research university maryland college park md usa fer umd edu 
examine influence camera design estimation motion structure scene video data 
camera captures subset light rays passing volume space 
relating differential structure time varying space light rays different known new camera designs establish hierarchy cameras 
hierarchy stability complexity computations necessary estimate structure motion 
low hierarchy standard planar pinhole camera structure motion problem non linear ill posed 
high camera call full field view camera problem linear stable 
develop design suggestions camera new design propose linear algorithm structure motion estimation combines differential motion estimation differential stereo 
think vision usually think interpreting images taken eyes images acquired planar eyes 
called camera type eyes pinhole principle commercially available cameras founded 
clearly eyes exist biological world reveals large variety designs 
estimated eyes evolved fewer times independently diverse parts animal kingdom 
developed designs range primitive eyes pinhole eye marine eye different types compound eyes insects camera eyes land vertebrates fish eyes highly evolved 
evolutionary considerations tell design system eye related visual tasks system solve 
way images acquired determines difficult perform task systems cope limited resources eyes designed optimize subsequent image processing relates particular tasks 
task chose recovery descriptions space time models image sequences large significant part vision problem 
space time models mean descriptions shape descriptions actions action defined change shape time 
specifically want determine ought collect images dynamic scene best recover scene shapes actions video sequences words camera collecting video subsequently facilitate structure motion problem best possible way problem wide implications static world moving cameras moving object static cameras fig 

hierarchy cameras 
classify different camera models field view fov number proximity different viewpoints captured axis 
turn determines structure motion estimation posed ill posed problem motion parameters related linearly non linearly image measurements 
camera models clockwise lower left small fov pinhole camera spherical pinhole camera spherical camera small fov camera 
variety applications vision recognition navigation virtual reality tele immersion graphics 
classify cameras study complete visual representation scene plenoptic function changes differentially time :10.1.1.2.9848
imaging device captures subset plenoptic function 
know considering different subsets plenoptic function problem structure motion easier harder 
theoretical model camera captures plenoptic function part space surface point pinhole camera 
call camera camera camera observe scene view different viewpoints theoretically point 
camera obtained arrange ordinary cameras close fig 

camera additional property arising proximity individual cameras form large number orthographic images addition perspective ones 
consider direction space consider individual camera captured ray parallel rays camera form image rays parallel 
furthermore term plenoptic camera proposed physical device capture true time varying plenoptic function prefer term emphasize difference continuous concept discrete implementation 
fig 

design camera fig 

capturing parallel rays fig 

capturing pencils rays 
different directions different orthographic image formed 
example fig 
shows select appropriate pixel camera form orthographic image looks side blue rays red rays 
fig 
shows captured rays illustrating individual camera collects conventional pinhole images 
short camera unique property captures simultaneously large number perspective affine images projections 
demonstrate property structure motion problem linear 
contrast standard single pinhole cameras capture ray point space ray different times structure motion estimated 
estimation viewing geometry non linear problem 
factor affects estimation surface light captured 
long known ambiguity estimation motion parameters small field view cameras noticed ambiguity disappears full field view camera 
planar camera construction limited field view problem nonlinear ill posed 
field view approaches pencil light rays cut sphere problem posed stable nonlinear 
basic understanding influence field view attracted investigators years 
study question detail refer literature information 
principles relating camera design performance structure motion field view linearity estimation 
principles summarized fig 

spherical camera ultimate camera combines stability full field view motion estimation linear formulation structure motion problem 
outline follows 
framework plenoptic video geometry section show camera structure motion estimation linear problem image information captured camera relates image information conventional pinhole cameras 
insights gained propose linear algorithm accurately compute structure motion plenoptic derivatives conclude suggestions applications new cameras implement construct cameras 
plenoptic video geometry differential structure space light rays location free space light intensity color light ray coming direction time measured plenoptic function intensity color images unit sphere directions :10.1.1.2.9848
transparent medium air change color light constant intensity color view direction long chosen free space 
plenoptic function free space reduces dimensions time varying space directed lines representations overview see 
choose plane parameterization introduced computer graphics 
lines passing space interest parameterized surrounding space contain camera object nested cubes recording intersection light rays entering camera leaving object planar faces cubes 
describe parameterization rays passing side cube extension sides straight forward 
loss generality choose planes perpendicular axis distance denote inner plane focal plane indexed coordinates outer plane image plane indexed defined local coordinate sytem respect see fig 

aligned axes world coordinates distance origin world coordinate system 
enables parameterize light rays pass planes time record intensity time varying lightfield 
fixed location focal plane corresponds image captured perspective camera 
fix view direction capture orthographic image scene 
sensor element imaging surface captures light ray indexed 
neighbourhood ray radiance varying continuously smoothly varying reflectance albedo tangent scene surface develop lightfield neighborhood ray taylor series dx dy du dv dt dx dy du dv dt lx 
lt partial derivatives lightfield 
disregarding higher order terms linear function relates local change view ray position direction differential brightness structure plenoptic function sensor element 
camera moves static world assume intensity light ray scene remains constant consecutive time instants 
allows spatio temporal brightness derivatives light rays captured imaging surface constrain lightfield flow dx dt dy dt du dt dv dt difference index correspond physical light ray consecutive time instants dt 
generalize known image brightness constancy constraint lightfield brightness constancy constraint dt lt dx dy du dv lx ly lu lv 
dt dt dt dt note formalism applied observe rigidly moving object set static cameras seen fig 

case attach world coordinate system moving object relate relative motion image sensors respect object spatio temporal derivatives light rays leave object 
plenoptic motion equations assuming imaging sensor undergoes rigid motion instantaneous translation rotation origin world coordinate system motion point world coordinate frame camera located position plane known equations relating motion parameters motion flow perspective orthographic images define ray flow ray indexed denotes vertical stacking vectors dx dt dy dt du dt dv dt uy ux vy vx uv uv combining eqs 
leads lightfield motion constraint 
lt lx ly lu lv linear constraint motion parameters relates differential image information imaging sensor capture 
combining constraints lightfield form highly determined linear system solve rigid motion parameters 
knowledge time temporal properties lightfield related structure motion problem 
previous dimensional static studied context image rendering computer graphics 
important realize lightfield derivatives lx 
lt obtained directly image information captured camera 
recall camera envisioned surface point corresponds pinhole camera see fig 

convert image information captured pinhole cameras lightfield camera simply intersect rays optical center pixel planes set corresponding lightfield value pixel intensity 
measurements general scattered appropriate interpolation schemes compute continuous fig 

lightfield parameterization shown relationship perspective orthographic derivatives accuracy plenoptic motion estimation 
plot shows ratio true estimated motion parameters vertical axis dependence distance sensor surface scene horizontal axis spheres unit radius 
scene shown fig 

lightfield function example see 
lightfield derivatives easily obtained applying standard image derivative operators 
lightfield motion constraint extended faces nested cube premultiplying appropriate rotation matrices rotate motion vectors local lightfield coordinates 
relationship structure motion formulation conventional single viewpoint cameras formulation cameras easily established 
assume surface slowly varying reflectance apply simple triangulation argument get identity du change view direction dx corresponding change view position illustrated fig 

illustration set du dv dx dy dx dy denotes depth scene measured location plane direction indicated 
eq 
deduce replace flow view direction variables inversely proportional depth scene projection translational flow flow view points independent scene 
enables formulate structure motion problem independent scene view linear problem 
see depth encoded scaled ratio positional directional derivatives lu lx lv ly identical relation depth differential image measurements differential stereo image analysis 
interpret plenoptic motion estimation integration differential motion estimation differential stereo 
experiments examine performance algorithm lightfield motion constraint experiments synthetic data 
distributed spheres textured smoothly varying pattern randomly scene filled horizon camera see fig 

computed faces nested cube surrounding camera raytracing fig 
computed derivatives stacked linear equations eq 
form linear system solved motion parameters 
derivatives scale find motion recovered accurately seen fig 

long relative scales derivatives similar scene distance matches filter widths derivative operators error motion parameters varies 
accurate egomotion estimate compute depth differential measurements formulas mi denotes ith row coefficient matrix eq 
lu flx lv fly lum lvm lt lum lvm lt differential depth measurements refined large baseline stereo information widely views path camera construct accurate dimensional descriptions image representations scene 
fig 

subset example scene corresponding unfolded lightfield size detailed view upper left corner 

ancient greek mythology argus guardian defeated army 
power eyes real proposed mathematical analysis new cameras 
principles relating camera design performance structure motion algorithms field view linearity estimation defined hierarchy camera designs 
design principles formulated introduced new family cameras called cameras 
cameras constructed placing large number individual cameras close 
cameras capture rays falling surface allow estimation plenoptic ray flow light ray rigid movement 
provides cameras capability solving ego motion scene models linear manner opening new avenues variety applications 
example open new avenues video development 
linear formulation study plenoptic video geometry relation local differential structure time varying lightfield rigid motion imaging sensor introduced 
biggest challenge making camera sure neighboring cameras distance allows estimation orthographic derivatives rays change ray intensity ray moved parallel 
special pixel readout array tightly spaced cameras obtain camera 
scenes close cameras necessary individual cameras tightly packed miniature cameras may sufficient 
currently developing different physical implementations eyes suggested evaluate proposed plenoptic structure motion algorithm benchmark set image sequences captured new cameras 

adelson bergen 
plenoptic function elements early vision 
landy movshon editors computational models visual processing pages 
mit press cambridge ma 

adelson wang 
single lens stereo plenoptic camera 
ieee trans 
pami 


inherent ambiguities recovering motion structure noisy flow field 
proc 
ieee conference computer vision pattern recognition pages 

bolles baker 
epipolar plane image analysis approach determining structure motion 
international journal computer vision 


geometric study light field representations 
technical report tr dept computer sciences university texas austin 


error sensitivity recovery object descriptions 
phd thesis department informatics university karlsruhe germany 
german 

ferm ller aloimonos 
observability motion 
international journal computer vision 

gortler grzeszczuk szeliski cohen 
lumigraph 
proc 
acm siggraph 

horn 
robot vision 
mcgraw hill new york 

jepson heeger 
subspace methods recovering rigid motion ii theory 
technical report tr university toronto 

koch pollefeys niemann 
calibration hand held camera sequences plenoptic modeling 
iccv pages 

levoy hanrahan 
light field rendering 
proc 
acm siggraph 
