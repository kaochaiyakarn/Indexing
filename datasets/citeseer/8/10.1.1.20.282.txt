forest statistical sentence generation presents new approach sta tistical sentence generation tive phrases represented packed sets trees forests ranked statistically choose best 
representation offers advantages compactness ability represent syntactic information 
fa efficient statistical ranking previous approach statistical generation 
efficient ranking algorithm described gether experimental results showing signif improvements simple enumeration lattice approach 
large textual corpora offer possibility statistical approach task sentence generation 
large scale nlp ai task task sentence generation requires immense amounts knowledge 
knowledge needed includes lexicons grammars ontologies collocation lists morphological tables 
ac applying accurate detailed knowl edge breadth poses difficult problems 
knight hatzivassiloglou suggested overcoming knowledge acquisition bottle neck generation tapping information inherent textual corpora 
performed ex periments showing automatically acquired corpus knowledge greatly reduced need deep hand crafted knowledge 
time approach generation im proved scalability robustness offering potential higher quality put 
approach adapted techniques speech recognition 
corpus sta tistical knowledge applied generation process encoding alternative irene langkilde information sciences institute university southern california marina del rey ca isi edu ings structure called lattice see fig ure 
lattice able represent large numbers alternative phrases requiring large amount space explicitly list individual alternatives re quire 
sentences lattice ranked statistical lan guage model sentence chosen output 
number phrases needed considered typically grew ex length phrase lattice usually large exhaustive search best algorithm heuristically narrow search 
lattice method promising drawbacks discussed shortly 
presents different method statistical generation forest struc ture packed set trees 
forest compact lattice offers hierar organization conducive repre senting syntactic information 
furthermore facilitates dramatically efficient statistical ranking constraints localized combinatorial explosion possibilities need considered reduced 
addition describing forest data structure presents forest ranking algo rithm reports experimental results efficiency time space 
favor ably compares results performance lattice approach 
representing alternative phrases enumerated lists lattices task sentence generation involves map ping representation mean ing syntax linear ordering words 
subtasks generation usually include choosing content words determining word order lattice representing different sen tences including may eat chicken chicken may eaten deciding insert function words forming morphological inflections satis fying agreement constraints tasks 
way leveraging corpus knowl edge explicitly enumerate alternate possibilities select corpus statistical model 
subphrases decisions common propose sentences lattice efficient way enumeration represent 
lattice graph arc la word 
complete path left node right node lattice represents possible sentence 
multiple arcs leaving particular node represent alter nate paths 
lattice allows structure shared sentences 
example lattice shown 
lattice encodes unique sentences 
practice lattice may represent sentences 
compact representation sentences statistical generation ble 
lattice illustrates types decisions need gen eration 
example choice tween root words chicken choice singular plural forms words decision article definite indefinite 
word choice decisions aux verb may express mode eating predicate obliged re quired 
choice active voice bottom half lattice pas sive voice top half 
inspection lattice reveals un avoidable duplication 
example word chicken occurs times sublattice noun phrase contain ing chicken repeated twice 
verb phrase headed auxiliaries may 
repetition com mon lattice representation text genera tion negative impact efficiency ranking algorithm set score calculations times 
drawback duplication representation consumes storage space necessary 
drawback lattice represen tation independence choices fully exploited 
stolcke 
noted word dependencies occur adjacent words 
means choices non adjacent parts sentence independent 
ex ample choice may independent choice precede chicken 
independence reduces combi nation possibilities considered allows decisions account rest context 
adjacent words indepen dent words tail ate sentence dog short tail ate bone 
lattice offer way representing parts sentence independent take advantage independence 
negatively impacts amount process ing needed quality results 
contrast forest representation discuss shortly allow independence explicitly annotated 
final difficulty lattices search space grows exponentially length sentence making exhaustive search sentence impractical long sentences 
heuristic searches fer poor approximation 
pruning done renders solution theoretically practice frequently ends pruning mathematically optimal solu tion 
forests weaknesses lattice representation overcome forest representation 
assign label unique arc group arcs occurs lattice lattice forest prob lems duplication lattice eliminated 
resulting structure represented set context free rewrite rules 
forest need necessarily comply particular theory syntactic structure wishes 
need derived specifically lattice generated directly semantic input 
forest representation quite nat ural incorporate syntactic information 
syn tactic information offers potentially signif advantages statistical language model ing 
discuss statis tical modeling syntax making men tion leaving 
stead focus nature forest repre sentation describe general algorithm ranking alternative trees language model 
forest representation corresponding lattice shown 
forest structure graph nodes represent sequences phrases nodes represent mutually exclusive particular relative po sition sentence 
example top level forest node encodes choice active passive voice versions sentence 
active voice version left child node labelled passive voice version right child node 
nodes forest corresponding distinct decisions mentioned earlier need deciding best sen tence output 
nodes uniquely numbered re node iden 
forest diagram left node drawn completely 
subsequent show node name written italics 
eases readability clarifies portions forest need scores computed ranking process 
nodes np vp prp repeated forest 
prp vp prp vp ez vp np np vp np np np internal representation top nodes forest illustrates forest repre sented internally showing context free rewrite rules top nodes forest 
nodes indicated label oc left hand side rule 
sample rules includes example multiple node node np occurs right hand side different rules 
generation forest differs parse forest parse forest represents different pos sible structures cover single phrase 
generation forest gener ally represents cal structures phrase represents different phrases generally express meaning 
previous packed generation trees previous developing representation packed generation forest structure 
describes extensions chart structure generation originally kay gen erate multiple paraphrases semantic put 
prominent aspect representation boolean vector expressions asso sub forest portions put covers control unification generation process 
primary gom representation guarantee part semantic input expressed possible output phrase 
contrast packed forest keeps association semantic put nodes forest separate forest representation 
system mappings maintained external cache mechanism described langkilde knight 
coverage semantic input implicit achieved process maps input forest 
forest ranking algorithm algorithm proposed ranking sen tences forest bottom dynamic pro gramming algorithm 
analogous chart parser performs inverse son 
comparing alternate syntactic structures indexed positions tit 
generation forest input sentence compares alternate phrases corresponding semantic input 
probabilistic chart parser key insight algorithm score phrases represented particu lar node forest decomposed context independent internal score context dependent external score 
inter nal score computed stored phrase external score computed combination sibling nodes 
general internal score phrase sociated node defined recur ij lj cj cl cj stands internal score ex score cj child node specific formulation pre cise definition context depends lan guage model 
example bigram model leaf nodes expressed ej cj depending language model phrase set externally relevant features 
features aspects phrase contribute context dependent scores sibling phrases 
case bi gram model features words phrase 
trigram model words 
elaborate lan guage models features include elements head word part speech tag tent category crucial advantage forest method node best scoring phrase unique combi nation externally relevant features needs maintained 
rest pruned sacrificing guarantee obtaining optimal solution 
pruning reduces exponentially total number phrases need considered 
effect ranking ia bigram model conditional probabil ities likelihood word phrase assumed depend immediately previous word 
likelihood phrase product conditional probabilities words phrase 
vp vp vb vbn may required may required required having may having having obliged may obliged obliged eaten may eaten eaten eaten pruning phrases forest node assuming bigram model algorithm exploits independence exists disjunctions forest 
illustrate shows exam ple phrases node pruned bigram model 
rule node vp forest shown gether phrases corresponding child nodes 
possible com bination phrases considered se quence nodes right hand side unique words may unique final word eaten 
words phrase externally relevant features bigram model best scoring phrases total need maintained node vp unique word word pair 
phrases ranked higher matter constituents vp combines 
pseudocode ranking algorithm shown 
node assumed record composed array child nodes node best ranked phrases node 
function con concatenates strings computes new score formula 
function prune guar best phrase unique set features values maintained 
core loop algorithm considers children node concatenating scoring phrases children ing results considering phrases third child concatenating intermediate results 
pseudocode seen complex ity algorithm dominated num ber phrases associated node number rules represent forest number children node 
specifically pruning de number features associated language model average number unique combinations feature values seen 
number features av erage number unique values seen node feature number best maintained unique set fea ture values cap number phrases algorithm complex ity vn assuming children nodes concatenated pairs 
note bigram model trigram model 
comparison complexity node node node forj ranked node node node node node node node temp node node pin prune temp temp node temp tive search algorithm lattice vn approximately length longest sentence lattice 
forest algorithm offers exponential reduction complexity guaranteeing opti mal solution 
capped best heuristic search algorithm hand complexity vn 
mentioned earlier typ ically fails find optimal solution longer sentences 
tables fig ure show experimental results comparing forest representation lattice terms time space rank sentences 
results generated test set inputs average sentence length ranged words 
ranked bigram model 
experiments run sparc ultra machine 
note time results lattice quite directly comparable forest include overhead costs loading portions hash table 
possible obtain timing measurements search algorithm 
estimate roughly time processing lattice search 
results interpreted comparison different kinds systems 
respect observed ta ble forest ranking program performs seconds faster time needed grow linearly num ber paths considered lattice program 
remains fairly constant 
consistent cal result forest algorithm depend sentence length number different alternatives consid ered position sentence 
table observed relatively moderate number sen tences ranked forest lattice fairly comparable space tion 
forest little extra overhead representing hierarchical structure 
space requirements forest grow linearly number paths lattice 
large numbers paths forest offers significant savings space 
spike graphs deserves particular comment 
current system producing forests semantic inputs generally produces nodes branches 
par ticular input triggered spike produced forest high level nodes larger number branches 
lattice increase number branches expo increases processing time stor age space requirements 
forest oo representation increase polynomial number branches produce spike 
io forest 

le lo time required ranking pro cess lattice versus forest representa tion 
axis number paths scale axis time seconds 
forest representation ranking algorithm implemented part gen generator system 
results shown previous section illustrate time space advantages forest representation calculating mathematically op sentence forest feasible particularly longer sentences 
obtaining mathematically optimal sentence valu able mathematical model provides fit 
forest representation possible add syntactic information mathematical model question ask model provide better fit natural english ngram models previously 
plan modify forests system produces le le le le le le le size data structure lattice versus forest representation 
axis number paths log scale axis size bytes 
conform penn treebank corpus marcus annotation style experiments models built tree bank data 
acknowledgments special go kevin knight daniel marcu anonymous reviewers comments 
research supported part nsf award 
kay 

chart generation 
proc 
acl 
knight hatzivassiloglou 

level paths generation 
proc 
cl langkilde knight 

generation exploits corpus statistical knowl edge 
proc 
coling acl 
marcus santorini marcinkiewicz 

building large annotated corpus english penn treebank 
computational linguistics 


generation paraphrases ambiguous logical forms 
coling 
stolcke 

linguistic knowledge empirical methods speech recognition 
ai magazine 
