discriminative vs generative classi ers comparison logistic regression naive bayes andrew ng computer science division university california berkeley berkeley ca michael jordan div 
dept stat 
university california berkeley berkeley ca compare discriminative generative learning typi ed logistic regression naive bayes 
show contrary belief discriminative classi ers preferred distinct regimes performance training set size increased algorithm better 
stems observation borne repeated experiments discriminative learning lower asymptotic error generative classi er may approach higher asymptotic error faster 
generative classi ers learn model joint probability inputs label predictions bayes rules calculate yjx picking label discriminative classi ers model posterior yjx directly learn direct map inputs class labels 
compelling reasons discriminative generative classi ers succinctly articulated vapnik solve classi cation problem directly solve general problem intermediate step modeling xjy 
leaving aside computational issues matters handling missing data prevailing consensus discriminative classi ers preferred generative ones 
piece prevailing folk wisdom number examples needed model roughly linear number free parameters model 
theoretical basis observation models vc dimension roughly linear low order polynomial number parameters see known sample complexity discriminative setting linear vc dimension 
study empirically theoretically extent beliefs true 
parametric family probabilistic models optimize joint likelihood inputs labels optimize conditional likelihood yjx minimize training error obtained thresholding yjx predictions 
classi er gen rst criterion model dis second third criterion parametric family models call gen dis generative discriminative pair 
example xjy gaussian multinomial corresponding generative discriminative pair normal discriminant analysis logistic regression 
similarly case discrete inputs known naive bayes classi er logistic regression form generative discriminative pair 
compare generative discriminative learning natural focus pairs 
consider naive bayes model discrete continuous inputs discriminative analog logistic regression linear classi cation show generative model higher asymptotic error number training examples large discriminative model generative model may approach asymptotic error faster discriminative model possibly number training examples logarithmic linear number parameters 
suggests empirical results strongly support number training examples increased distinct regimes performance rst generative model approached asymptotic error doing better second discriminative model approaches lower asymptotic error better 
preliminaries consider binary classi cation task case discrete data 
dimensional input space assumed binary inputs simplicity generalization ering diculties 
output labels ft fg joint distribution training set fx iid examples drawn 
generative naive bayes classi er uses calculate estimates jy probabilities jy follows jy bg bg similarly 
counts number occurrences event training set setting corresponds empirical estimates probabilities traditionally set positive value corresponds laplace smoothing probabilities 
classify test example naive bayes classi er gen 
predicts gen quantity positive gen log jy jy log jy jy log case continuous inputs remains assume jy parameterized univariate gaussian distribution parameters note depend 
parameters maximum likelihood example empirical mean th coordinate examples training set label note method equivalent normal discriminant analysis assuming diagonal covariance matrices 
sequel jy var jy true means variances regardless data gaussian 
discrete continuous cases known discriminative analog naive bayes logistic regression 
model parameters posits jx exp 
test example discriminative logistic regression classi er dis 
predicts dis linear discriminant function dis positive 
discriminative model parameters maximize conditional training set log jx minimize training error fh dis 
indicator function ftrueg 
insofar error metric classi cation error view alternative truly spirit discriminative learning frequently computationally ecient approximation 
largely ignore di erence versions discriminative learning abuse terminology loosely term logistic regression refer formal analyses focus method 
family linear classi ers maps classi er 
de ne generalization error pr 
analysis algorithms classes far linearly separable logistic regression naive bayes possibly linear classi ers 
obtain non trivial results interesting compare performance algorithms asymptotic errors cf 
agnostic learning setting 
precisely gen population version naive bayes classi er gen naive bayes classi er parameters xjy xjy 
similarly dis population version logistic regression 
propositions completely straightforward 
proposition gen dis generative discriminative pair classi ers gen dis asymptotic population versions 
dis gen 
proposition dis logistic regression dimensions 
high probability dis dis log dis dis hold high probability xed constant suces pick 
proposition states error discriminative logistic regression smaller generative naive bayes 
easily shown observing dis converges inf class linear classi ers asymptotically worse linear classi er picked naive bayes 
proposition provides basis widely held belief discriminative classi ers better generative ones 
proposition standard result straightforward application vapnik uniform convergence bounds logistic regression fact vc dimension second part proposition states sample complexity discriminative learning number examples needed approach asymptotic error order note worst case sample complexity lower bounded order 
technical assumption true classi ers including logistic regression family possible classi ers case logistic regression nite vc dimension 
picture discriminative learning fairly understood error converges best linear classi er convergence occurs order examples 
generative learning speci cally case naive bayes classi er 
lemma 
lemma xed 
assume xed log probability 
case discrete inputs jy jy 
case continuous inputs proof sketch 
consider discrete case 

cherno bound probability exp fraction positive examples implies positive negative examples cherno bound speci chance jy jy exp 
probabilities chance error union bound substituting de nitions see guarantee suces stated 
lastly smoothing adds small perturbation probabilities argument say arguing perturbation order gives result 
result continuous case proved similarly cherno bounds argument assumption 
number samples logarithmic linear parameters generative classi er gen uniformly close asymptotic values gen tempting conclude gen error generative naive bayes classi er converges asymptotic value gen examples implying log examples required naive bayes model 
shortly establish simple conditions intuition correct 
note implies naive bayes converges higher asymptotic error gen compared logistic regression dis may approach signi cantly faster log training examples 
way showing gen approaches gen showing parameters convergence implies gen predictions gen recall gen predictions thresholding discriminant function gen de ned 
gen corresponding discriminant function gen example gen gen fall side zero gen gen prediction 
long gen fairly high probability far zero gen small perturbation gen usually side zero gen 
theorem de ne pr gen gen 
assume xed jy case discrete inputs continuous case 
high probability gen gen log proof sketch 
gen gen chance gen correctly classi es randomly chosen example gen misclassi es 
lemma ensures high probability parameters gen log gen turn implies terms sum gen equation log corresponding term gen jl gen gen log 
letting log see possible gen correct gen wrong example gen possible gen gen gen 
probability exactly gen gen 
key quantity theorem small small order bound non trivial 
note upper bounded pr gen chance gen random variable distribution induced falls near zero 
gain intuition scaling random variables consider proposition suppose fraction features holds true jp jy jy xed case continuous inputs 
gen jy gen jy 
long class label gives information fraction features formally long features relevant class label expected value jl gen 
proposition easily proved showing conditioned say event terms summation gen equation replaced non negative expectation non negativity kl divergence fraction expectation bounded away zero 
proposition guarantees jl gen large expectation want order bound slightly stronger random variable jl gen large far zero high probability 
ways deriving sucient conditions ensuring small 
way obtaining loose bound chebyshev inequality 
rest discussion simplicity implicitly condition event test example label chebyshev inequality implies pr gen gen var gen gen sum random variables ignoring term involving priors 
conditioned random variables independent naive bayes assumption conditionally independent holds variance random variables completely independent variance may larger may smaller depending signs correlations 
gen jy guaranteed proposition setting chebyshev inequality gives pr gen worst case independent case 
gives bound note frequently loose 
unrealistic case naive bayes assumption really holds obtain stronger cherno bound exp exponentially small continuous case gen density small interval uniformly bounded 
case corollary theorem 
corollary conditions theorem hold suppose function independent satis es xed 
gen gen hold high error pima continuous error adult continuous error boston predict median price continuous error optdigits continuous error optdigits continuous error ionosphere continuous error liver disorders continuous error sonar continuous error adult discrete error promoters discrete error lymphography discrete error breast cancer discrete error lenses predict hard vs soft discrete error sick discrete error voting records discrete results experiments datasets uci machine learning repository 
plots generalization error vs averaged random train test splits 
dashed line logistic regression solid line naive bayes 
probability suces pick 
note previous discussion implies preconditions corollary hold case naive bayes proposition assumption holds constant long large exp similarly bounded var gen case restrictive 
means requiring sucient condition asymptotic sample complexity log 
experiments results previous section imply discriminative logistic regression algorithm lower asymptotic error generative naive bayes classi er may converge quickly higher asymptotic error 
number training examples increased expect generative naive bayes initially better discriminative logistic regression eventually catch quite overtake performance naive bayes 
test predictions performed experiments datasets continuous inputs discrete inputs uci machine learning repository 
results experiments shown 
nd theoretical predictions borne surprisingly 
cases logistic regression performance catch naive bayes observed primarily particularly small datasets presumably grow large observe expected dominance logistic regression large limit 
discussion efron analyzed logistic regression normal discriminant analysis continuous inputs concluded asymptotically slightly times statistically ecient 
marked contrast results key di erence assuming xjy gaussian diagonal covariance matrix efron considered case xjy modeled gaussian full matrix 
setting estimated covariance matrix singular fewer linear training examples surprise normal discriminant analysis learn faster logistic regression 
second important di erence efron considered special case xjy truly gaussian 
asymptotic comparison useful general case possible dis gen logistic regression superior algorithm 
contrast saw previously non asymptotic case interesting regime behavior observed 
practical classi cation algorithms generally involve form regularization particular logistic regression improved practice techniques maximize consistency theoretical discussion experiments avoided discrete continuous hybrids considering discrete continuous valued inputs dataset necessary 
train test splits random subject example class training set continuous valued inputs rescaled necessary 
case linearly separable datasets logistic regression distinction possible separating planes 
setting mcmc sampler pick classi er randomly errors reported empirical averages separating hyperplanes 
implementation normal discriminant analysis standard trick adding diagonal covariance matrix ensure invertibility naive bayes 
shrinking parameters constraint imposing margin constraint separable case various forms averaging 
regularization techniques viewed changing model family largely orthogonal analysis examining particularly clear cases generative discriminative model pairings 
developing clearer understanding conditions pure generative discriminative approaches successful better able design hybrid classi ers enjoy best properties wider range conditions 
discussion focused naive bayes logistic regression straightforward extend analyses models including pairs generated xed structure bounded fan bayesian network model xjy naive bayes special case 
acknowledgments andrew mccallum helpful conversations 
ng supported microsoft research fellowship 
supported intel nsf iis onr muri 
anthony bartlett 
neural network learning theoretical foundations 
cambridge university press 
efron 
eciency logistic regression compared normal discriminant analysis 
journ 
amer 
statist 
assoc 
goldberg jerrum 
bounding vc dimension concept classes parameterized real numbers 
machine learning 
mclachlan 
discriminant analysis statistical pattern recognition 
wiley new york 
rubinstein hastie 
discriminative vs informative learning 
proceedings third international conference knowledge discovery data mining pages 
aaai press 
vapnik 
statistical learning theory 
john wiley sons 
