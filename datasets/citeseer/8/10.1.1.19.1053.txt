potential algorithms line prediction game theory nicol cesa bianchi cesa bianchi dti unimi department information technologies university milan italy lugosi lugosi upf es department economics pompeu fabra university ramon barcelona spain august 
show known algorithms sequential prediction problems including weighted majority quasi additive family grove littlestone schuurmans playing iterated games including freund schapire hedge mw strategies hart mas colell boosting including adaboost special cases general decision strategy notion potential 
analyzing strategy derive known performance bounds new bounds simple corollaries single general theorem 
ering new uni ed view large family algorithms establish connection potential analysis learning counterparts independently developed game theory 
exploiting connection show certain learning problems instances general game theoretic problems 
particular describe notion generalized regret show applications learning theory 
keywords universal prediction line learning blackwell strategy perceptron algorithm weighted average predictors internal regret boosting 
describing sequential decision problem general strategy solve 
see detail subsequent sections previously known algorithms speci decision problems turn special cases strategy 
problem parametrized decision space outcome space convex twice di erentiable potential function extended appeared proceedings th annual conference computational learning theory th european conference computational learning theory springer 
kluwer academic publishers 
printed netherlands 
ml tex cesa bianchi lugosi ir ir step current state represented point ir 
decision maker observes vector valued drift function ir selects element decision space return outcome received new state problem drifted point 
goal decision maker minimize potential known unknown decision maker 
main goals point seemingly unrelated problems framework sequential decision problem described analysis may summarized general simple theorems 
problems include line prediction problems experts model classi cation algorithms methods learning repeated game playing usually think vector decision maker su ers time corresponding cumulative regret vector 
decision maker goal keep sense cumulative regret vector close origin 
applications described decision maker free choose potential function 
ll problem described meaning describe example detailed section 
example 
consider line prediction problem experts framework cesa bianchi 

decision maker predictor goal forecast hidden sequence elements outcome space time predictor computes guess outcome guess advice predictors experts xed pool 
guesses predictor experts individually scored loss function ir 
predictor goal keep small possible cumulative regret respect expert 
quantity de ned expert sum easily modeled decision problem associating coordinate expert de ning components drift function role potential function prediction experts framework provide generalized way measure size ml tex potential algorithms distance origin regret distance information predictor control regret 
introduce class predictors potential information keep drift halfspace negative gradient resides 
guarantee existence predictors need constrain decision problem making assumptions naturally satis ed applications 
notation 
stands inner product vectors de ned 
un 
generalized blackwell condition 
time decision exists sup 

additive potential 
potential written un ir ir ir nonnegative function variable 
typically monotonically increasing convex ir 

strategies satisfying condition tend keep point close possible minimum potential forcing drift vector point away gradient current potential 
gradient descent approach sequential decision problems new 
prominent example decision strategy type blackwell prove celebrated theorem blackwell generalizing vector valued payo von neumann minimax theorem 
application blackwell strategy sequential decision problems generalization arbitrary potentials due series papers hart mas colell condition rst introduced somewhat restricted context 
condition independently introduced grove 
de ne analyze new family algorithms solving line binary classi cation problems 
family includes special cases perceptron rosenblatt zero threshold winnow algorithm littlestone 
decision problem bears similarities schapire drifting game schapire 
rest organized follows 
section general result derived performance sequential decision strategies satisfying condition special cases important ml tex cesa bianchi lugosi types potential functions exponential polynomial discussed detail 
section return problem prediction expert advice recover known results main result section 
purpose section show variants perceptron algorithm line linear classi cation including winnow norm perceptron special cases general problem simple matter re derive known mistake bounds general framework 
section boosting revisited similar purpose 
section dedicated problems learning repeated game playing 
discuss family hannan consistent methods algorithm freund schapire general framework 
discuss general notion regret derive performance bounds generalization method adaptive game playing due hart mas colell 

general bounds section describe general upper bound potential location reached drifting point decision maker uses strategy satisfying condition 
result inspired partially builds hart mas colell analysis strategies hart mas colell playing iterated games analysis algorithms binary classi cation grove littlestone schuurmans 
theorem 
twice di erentiable additive potential function ir 
ir ir increasing concave twice di erentiable auxiliary function sup ir function ir ir ml tex potential algorithms 
rst sight obvious interpret result 
see may derive useful bounds easily large variety special cases 
point simply point interesting applications nds bounded function satisfying assumption 
cases obtains constant ct superlinear growth norm strictly convex sucient conclude independently outcome sequence 
examples theorem derive inequalities spirit 
proof 
estimate terms taylor theorem 
note rf 
obtain 
vector inequality follows fact 
additive straightforward calculation shows concave step hypothesis theorem 
obtained 
proof nished iterating argument 
ml tex cesa bianchi lugosi follows write taken arbitrary elements respectively denote 
review simple applications theorem 
rst polynomial potential functions 
de ne norm vector kuk ju denote maxf ag 
corollary 
assume prediction algorithm satis es potential function 
kr max kr proof 
apply theorem straightforward calculation px hand older inequality 
jr jr ml tex potential algorithms conditions theorem satis ed choice kr theorem implies rst statement 
second follows rst simply max simple important choice potential function exponential potential treated corollary 
corollary 
assume prediction algorithm satis es potential function parameter 
ln ln max particular max ln max proof 
choosing ln conditions theorem satis ed max yields result 

polynomial potential considered hart context binary classi cation grove 
de ne norm perceptron 
exponential potential reminiscent smooth ctitious play approach game theory fudenberg levine ctitious play player chooses pure strategy best past distribution adversary plays smoothing choice amounts introducing randomization 
learning theory algorithms exponential potential intensively studied applied variety problems see cesa bianchi freund schapire littlestone warmuth vovk vovk :10.1.1.32.8918
ml tex cesa bianchi lugosi choice ln polynomial potential yields bound max ln jr ln ln ln ln ln choice suggested gentile context norm perceptron algorithms 
similar bound obtained assumption setting ln exponential potential 
note tuning requires knowledge horizon 
weighted average predictors section consider main applications strategy induced generalized blackwell condition experts framework mentioned section 
recall framework th component drift vector time takes form regret loss predictor loss th expert 
denote assume ir remarkable fact application convex subset vector space loss function convex rst component predictor satisfying condition obtained averaging experts predictions weighted normalized potential gradient 
note condition equivalent convexity implied clearly satis ed choosing ml tex potential algorithms example 
consider exponential potential function corollary 
case weighted average predictor described simpli es exp exp exp exp known weighted majority predictor littlestone warmuth corollary recovers constant factors previously known performance bounds see cesa bianchi 
similarly corollary may derive performance bounds predictor polynomial potential 
results summarized follows 
corollary 
assume decision space convex subset vector space loss function convex rst component bounded 
exponential weighted average predictor parameter ln satis es sequences min ln polynomial weighted average predictor parameter ln satis es sequences min te ln beauty weighted majority predictor corollary depends past performance experts ml tex cesa bianchi lugosi predictions polynomial general potentials depend past predictions 

cases theorem gives suboptimal bounds 
fact arguments theorem taylor theorem bound increase potential function 
situations value potential function nonincreasing 
property proven repeating argument kivinen warmuth 
proposition 
consider weighted majority predictor 
loss function function concave exponential potential function 
particular max ln 
proof 
suces show equivalently exp exp denoting exp may written follows concavity jensen inequality 
simple common examples loss functions satisfying concavity assumption proposition include square loss logarithmic loss ln ln 
information type prediction problems refer vovk haussler kivinen warmuth :10.1.1.52.856
observe proof proposition explicit generalized blackwell condition 
close section mentioning classi cation algorithms time varying potentials nonadditive potential functions ml tex potential algorithms de ned analyzed auer cesa bianchi 

quasi additive algorithm section show quasi additive algorithm grove littlestone schuurmans speci instances norm perceptron gentile grove classical perceptron block rosenblatt winnow algorithm littlestone special case general decision strategy 
derive performance bounds corollaries theorem 
recall quasi additive algorithm performs binary classi cation attribute vectors xn ir incrementally adjusting vector ir weights 
weight vector observing th attribute vector quasi additive algorithm predicts unknown label thresholded linear function sgn 

correct label di erent weight vector updated precise way update occurs distinguishes various instances quasi additive algorithm 
analyze quasi additive algorithm framework specialize decision problem section follows 
decision space outcome space set equal 
drift vector time function fy feg indicator function event instances quasi additive algorithm parametrized potential function gradient current potential weight vector 
weight update de ned functional inverse show section inverse exists potentials considered 
check condition satis ed 
condition satis ed 

fy 
condition satis ed case 
rest section denote fy total number mistakes speci quasi additive algorithm considered 
ml tex cesa bianchi lugosi 
norm perceptron de ned grove norm perceptron uses potential juj just slight modi cation polynomial potential 
derive generalization perceptron convergence theorem block 
version somewhat stronger proven gentile 
arbitrary sequence labeled attribute vectors maxf 
total deviation freund schapire gentile gentile warmuth ir respect margin 
term sum de ning tells linear threshold classi er weight vector missed classify certain margin corresponding example 
measures notion loss called hinge loss gentile warmuth di erent number misclassi cations associated weight vector corollary 
ir sequence labeled attribute vectors 
number mistakes norm perceptron pre arbitrary length sequence kx hinge loss respect margin unit norm dual norm 
proof 
adapting proof corollary potential juj bound kx nd kr hand ir vector kv 
kr 
older inequality 
fy 

fy 
piecing inequalities solving resulting inequality yields desired result 
ml tex potential algorithms 
zero threshold winnow zero threshold winnow algorithm exponential potential 
norm perceptron derive corollary robust version bound shown grove 

corollary 
corollary 
ir sequence labeled attribute vectors 
pre arbitrary length sequence kx probability vector number mistakes zero threshold winnow tuned ln ln ln ln ln ln 
proof 
corollary implies ln ln obtain lower bound ln consider vector convex coecients 
known log sum inequality see cover thomas implies vectors ir nonnegative numbers ln ln ln entropy vector convex coecients 
ln ln 
ml tex cesa bianchi lugosi step proceeded just proof corollary 
putting upper lower bounds ln obtain ln dropping positive term implies ln show proof case ln letting verifying may rearrange follows ln set ln hypothesis choice holds hypothesis 

potentials bregman divergences alternative analysis quasi additive algorithm proposed warmuth jagota kivinen warmuth leads mistake bounds essentially equivalent 
alternative analysis notion bregman divergences bregman 
section re derive general form mistake bounds starting notion potential 
concrete bounds particular choices potential functions derived just potential analysis 
fix di erentiable strictly convex additive potential ir pair vectors ir bregman divergence de ned 
ml tex potential algorithms 
error rst order taylor approximation convex potential property bregman divergences trivial fact 
fact 
ir 


bregman divergence 
de ned directly additive potential quasi additive algorithm de ned turns wrong quantity deriving mistake bounds 
related potential ir ir form ds note inverse exists assumed 
additive potential key property 
fact 
proof 
pick ir 
ready derive bound cumulative hinge loss total deviation quasi additive algorithm 
hinge loss upper bounds number mistakes serve mistake bound algorithm 
derivation taken warmuth jagota kivinen warmuth just change notation 
theorem 
ir sequence labeled attribute vectors 
quasi additive algorithm run potential cumulative hinge loss 
pre arbitrary length sequence 

cumulative hinge loss margin ir 
ml tex cesa bianchi lugosi proof 
ir 
hinge loss 



un note 

drift vector 

ir 
positive 



taylor theorem convexity hinge loss 


fact recalling 


fact summing positivity bregman divergences recalling get desired result 

potential analysis analysis bregman divergences shown naturally applied binary classi cation problem regression problems arbitrary convex loss 
divergence analysis appears limited predictors weighted averages potential analysis handle general predictors introduced section 
interesting open problem understand complex predictors analyzed bregman divergences alternatively show problems section solved weighted average predictors 

boosting boosting algorithms binary classi cation problems receive input labeled sample generic instance space return linear threshold classi ers form sgn ir functions ml tex potential algorithms belong xed hypothesis space boosting resampling schema classi er built incrementally step booster weighs sample calls oracle called weak learner returns booster chooses performance weighted sample adds linear threshold classi er 
boosting resampling easily tted framework letting round decision maker choice ir outcome 
drift function de ned condition takes form 
de ne weighted margin see corresponds 
freund schapire adaboost special case schema potential exponential chosen way satis ed 
recover known bound training accuracy classi er output adaboost special case main result 
corollary 
training set sequence functions exponential potential training error classi er sgn satis es ff exp exp normalized weighted margin 
proof 
result follow directly corollary 
need slightly modify proof theorem negative term 
dropped 
term takes form exp exp exp keep term proceed noting max max ml tex cesa bianchi lugosi continuing proof corollary obtain ln ln ln rearranging exponentiating get exp exponential potential ff exp get desired result 

potential algorithms game theory decision problem applied problem playing repeated games 
consider rst game player adversary 
round game player chooses action pure strategy mg independently adversary chooses action player loss value loss function mg mg suppose th round game player chooses action mixed strategy probability distribution actions suppose adversary chooses action regret player vector ir th component rst term just expected loss predictor compared loss playing action th component drift measures expected change player loss deterministically choose action adversary change action 
components round cumulative regret vector close zero mean player played best pure ml tex potential algorithms action 
notion precise hannan follows player hannan consistent round regret vector converges zero vector grows nity 
general decision strategy play repeated games type letting decision space set distributions player set mg actions drift vector regret vector 
de ne general potential mixed strategy appropriate twice di erentiable additive potential function 
immediate see value outcome 
condition satis ed 
hedge algorithm freund schapire strategy blackwell proof theorem special cases respectively exponential potential polynomial potential :10.1.1.32.8918
corollaries imply hannan consistency algorithms 
hart mas colell characterize class potentials condition yields hannan consistent player 

multiplicative algorithms playing repeated games consider setup discussed freund schapire adaptive game playing 
game de ned loss matrix entries 
round row player chooses row mixed strategy column player chooses column mixed strategy 
row player loss time goal achieve cumulative loss small cumulative loss min best xed mixed strategy 
ml tex cesa bianchi lugosi freund schapire introduce algorithm multiplicative updating weights de ned exp exp set appropriately chosen constant 
point algorithm just special case algorithm de ned 
see de ne action space adversary set probability distributions columns loss function back problem described section 
de ning drift vector immediate see multiplicative weight algorithm freund schapire just exponential potential 
view observation straightforward derive performance bounds multiplicative update algorithm 
corollary implies ln ln ln obtain lower bound ln log sum inequality see section conclude probability vector ln 
comparing upper lower bounds ln obtain result 
corollary 
multiplicative update algorithm de ned satis es min ln ml tex potential algorithms bound similar derived freund schapire 
choosing ln nonnegativity entropy obtain min ln insigni cant improvement corollary freund schapire 
course potential functions di erent exponential may 
varying potential function obtain family algorithms performance bounds straightforward obtain theorem 

generalized regret learning experts section consider general notion regret call generalized regret introduced lehrer 
see generalized regret includes special cases notions regret de ned fudenberg levine foster vohra 
de nition repeated game viewed line prediction problem randomized predictor 
generalized regret analyze line prediction problems 
consider prediction experts framework mg denote predictions experts time expert de ne activation function mg 
activation function determines corresponding expert active current step index possibly predictor guess time instant values activation function revealed predictor decides guess 
de ne generalized regret randomized predictor respect expert round instantaneous generalized regret respect expert nonzero expert active illustrate power general notion regret describe important special cases 
example 
external regret 
simplest special case problem ml tex cesa bianchi lugosi reduces described previous section drift function just 
game theory regret called external opposed internal regret described 
example 
specialists 
general formulation permits consider wider family prediction problems 
examples include variants learning experts framework shifting experts general specialists freund 
specialists framework activation function depends arbitrarily round index actual predictor guess setup may useful model prediction scenarios experts allowed occasionally abstain predicting experts may want abstain reasons instance con dent prediction 
see cohen singer practical application specialists framework 
example 
internal regret 
discuss detail special case problem minimizing called internal conditional regret hart mas colell 
foster vohra survey notion regret relationship external regret 
minimization internal regret plays key role construction adaptive game playing strategies achieve asymptotically correlated equilibrium see hart mas colell hart mas colell 
formal description follows experts labeled pairs expert predicts active predictor guess fk jg component generalized regret vector ir cumulative internal regret respect expert total amount predictor cumulative loss increased predicted time predicted may interpreted regret predictor feels having predicted time predicted internal regret formally similar external regret important di erences 
easy see internal regret stronger usual regret sense player succeeds keeping internal regret small pairs component round cumulative ml tex potential algorithms external regret vector stays close zero 
assume maxf possible pairs sequence 
mg action minimal cumulative loss min 
cumulative regret just ma small cumulative internal regret implies small cumulative regret form considered experts framework 
hand easy show example small cumulative regret imply small internal regret fact signi cantly dicult construct strategies achieve small internal regret 
key question possible de ne predictor satisfying condition generalized regret 
existence shown result 
predictor may apply theorem corollaries obtain performance bounds 
theorem 
consider decision problem described drift function potential 
randomized predictor satisfying condition de ned unique solution set linear equations ff kg observe special case predictor theorem reduces predictor 
proof theorem relegate appendix generalization proof contained hart mas colell 
return special case internal regret 
hart rst proved existence predictor maximal cumulative internal regret max 
algorithm just special case predictor theorem polynomial potential 
algorithm hart obtains bound form max tm bound may improved signi cantly large values considering predictor theorem potential functions 
example exponential potential straightforward combination theorem corollary implies ml tex cesa bianchi lugosi corollary 
randomized predictor theorem run exponential potential parameter ln sequences internal regret satis es max ln similar bound may obtained polynomial potential exponent ln interesting open question nd best possible bound max terms appendix proof 
write condition follows 
ff jg ff jg ff jg ff jg ff kg ff kg ml tex potential algorithms arbitrary nonnegative implied ff kg solving yields result 
check predictor exists 
matrix entries ff kg condition implied mp nonnegative eigenvector equation mp positive solution perron frobenius theorem seneta 
authors acknowledge partial support esprit working group ep neural computational learning ii neurocolt ii 
second author supported bmf 
auer cesa bianchi gentile 

adaptive self con dent line learning algorithms 
journal computer system sciences 
blackwell 

analog minimax theorem vector payo paci journal mathematics 
block 

perceptron model brain functioning 
review modern physics 
bregman 

relaxation method finding common point convex sets application solutions problems convex programming 
ussr computational mathematics physics 
cesa bianchi 

analysis gradient algorithms line regression 
journal computer system sciences 
ml tex cesa bianchi lugosi cesa bianchi freund helmbold haussler schapire warmuth 

expert advice 
journal acm 
cesa bianchi gentile 

second order perceptron algorithm 
proceedings th annual conference computational learning theory 
lecture notes arti cial intelligence springer 
cohen singer 

context sensitive learning methods text categorization 
acm transactions information systems 
cover thomas 

elements information theory 
john wiley sons 
foster vohra 

calibrated learning correlated equilibrium 
games economic behaviour 
foster vohra 

regret line decision problem 
games economic behaviour 
freund schapire 

decision theoretic generalization line learning application boosting 
journal computer system sciences 
freund schapire 

adaptive game playing multiplicative weights 
games economic behavior 
freund schapire 

large margin classi cation perceptron algorithm 
machine learning 
freund schapire singer warmuth 

combining predictors specialize 
proceedings th annual acm symposium theory computing 
pp 

acm press 
fudenberg levine 

universal consistency cautious fictitious play 
journal economic dynamics control 
fudenberg levine 

conditional universal consistency 
games economic behaviour 
gentile 

robustness norm algorithms 
extended authored littlestone appeared proceedings th annual conference computational learning theory pages 
acm press 
gentile warmuth 

linear hinge loss average margin 
advances neural information processing systems 
mit press 
grove littlestone schuurmans 

general convergence results linear discriminant updates 
proceedings th annual conference computational learning theory 
pp 

acm press 
grove littlestone schuurmans 

general convergence results linear discriminant updates 
machine learning 
hannan 

approximation bayes risk repeated play 
contributions theory games 
hart mas colell 

simple adaptive procedure leading correlated equilibrium 
econometrica 
hart mas colell 

general class adaptive strategies 
journal economic theory 
haussler kivinen warmuth 

sequential prediction individual sequences general loss functions 
ieee transactions information theory 
kivinen warmuth 

averaging expert predictions 
proceedings fourth european conference computational learning theory 
pp 

lecture notes arti cial intelligence vol 

springer 
ml tex potential algorithms kivinen warmuth 

relative loss bounds multidimensional regression problems 
machine learning 
lehrer 

wide range regret theorem 
games economic behavior 
appear 
littlestone 

mistake bounds logarithmic linear threshold learning algorithms 
ph thesis university california santa cruz 
littlestone warmuth 

weighted majority algorithm 
information computation 


convergence proofs perceptrons 
proceedings symposium mathematical theory automata vol 
xii 
pp 

rosenblatt 

principles neurodynamics 
spartan books 
schapire 

drifting games 
machine learning 
seneta 
non negative matrices markov chains 
springer 
vovk 

aggregating strategies 
proceedings rd annual workshop computational learning theory 
pp 

vovk 

game prediction expert advice 
journal computer system sciences 
vovk 

competitive line statistics 
international statistical review 
warmuth jagota 

continuous discrete time nonlinear gradient descent relative loss bounds convergence 
electronic proceedings th international symposium arti cial intelligence mathematics 
ml tex ml tex 
