design cache memories dataflow architecture krishna hurson department computer science engineering university texas arlington arlington tx cse uta edu department computer science engineering pennsylvania state university university park pa hurson cse psu edu advance dataflow processing combine dataflow paradigm control flow paradigm brought new challenging issues 
hybrid organization possible study adapt familiar control flow concepts cache memories framework dataflow architecture 
concept cache memory proven effectiveness von neumann architecture due spatial temporal localities govern organization conventional programming execution 
dataflow paradigm support locality execution sequence enforced availability operands 
dataflow programs reordered various criteria enhance locality instruction 
achieved careful partitioning dataflow program vertical layers data dependent instructions ii proper distribution allocation recurrence portions dataflow program 
enhancing locality data dataflow architecture challenging problem 
studies design instruction data operand structure cache memories explicit token store ets model dataflow system 
performance results obtained various benchmark programs analyzed 
design cache memories dataflow architecture 
established fact von neumann arena locality program exploited cache memories achieve significant performance improvements 
dataflow architectures permit traditional storage models natural consider localities execution sequence dataflow program 
late trend bring dataflow computational model closer control flow model 
designs computer systems hybrid execution models 
reader referred numerous survey articles analyzed dataflow architectures 
research model known store permits storage hierarchy context dataflow 
context switching dataflow architecture occur instruction basis datum carries context identifying information form continuation tag 
permits long unpredictable latencies due remote memory accesses processor switch new contexts having wait memory accesses 
interestingly instruction level parallelism leads excessive overheads due dynamic scheduling instructions 
compromise instruction level context switching capability sequential scheduling instruction streams provides different perspective dataflow architectures multithreading 
thread sequence instructions instruction thread executed non blocking threads remaining instructions execute interruption 
thread defines basic unit requires synchronization execution 
evolution pure paradigm dataflow multithreading requires locality improved processor utilization remote memory accesses 
experiences current dataflow projects show trend adopting multithreading viable method build hybrid architectures combine features dataflow von neumann execution models 
multithreaded architectures viewed evolution dataflow architectures direction explicit control instruction execution order von neumann machines direction better support synchronization tolerance long latency operations 
success multithreaded systems depends quickly context switching supported 
possible threads resident fast small memories instruction buffers caches limits number active threads amount latency tolerated 
generality dataflow scheduling difficult fetch execute sequence logically related sets threads processor pipeline removing opportunity registers thread boundaries 
responsibilities scheduling storage management compiler alleviates problem extent 
conventional architectures reduction memory latencies achieved providing explicit programmable registers implicit high speed caches 
amalgamating idea caches register caches dataflow framework result higher exploitation parallelism hardware utilization 
various cache designs explicit token store ets dataflow model performance resulting inclusion cache memories dataflow architecture 
section briefly introduce ets architecture 
section describe instruction operand cache memory designs uniprocessor ets architecture 
section describe multi processor ets system structure cache memories 
results experiments section 
section outlines approach operand memory reuse dataflow 

dataflow architecture data driven model computation operations enabled input operands available predecessor instructions 
completion operation results available successor 
model necessary operands instructions wait matches 
static dataflow model proposed dennis research group mit 
general organization static dataflow machine depicted 
activity store contains instruction templates represent nodes dataflow graph 
instruction template contains operation code slots operands destination addresses 
availability operands determined contents presence bits pbs instruction template 
update unit detects executability instructions 
address enabled instruction sent fetch unit instruction queue 
fetch unit fetches sends complete operation packet functional units clears presence bits 
functional unit performs operation forms result tokens sends update unit 
update unit stores result appropriate operand slot checks presence bits determine active instruction 
dynamic dataflow model proposed arvind mit watson university manchester 
tokens received matching unit matching unit tries bring tokens identical tags 
match exists corresponding token extracted matching unit matched token set passed fetch unit 
match token stored matching unit await partner 
fetch unit tags token pair uniquely identify instruction fetched program memory 
instruction token pair form enabled instruction packet sent processing unit pe 
processing unit executes enabled instructions produces result tokens sent matching unit token queue 
fetch unit update unit activity store instruction queue operation unit result tokens operation packets basic organization static dataflow model 
fetch unit program memory processing unit matching unit data tokens matched token sets enabled instructions token queue data tokens general organization dynamic dataflow model 
direct matching complexity token matching process dynamic dataflow architectures success dataflow machines 
important developments design current dataflow proposals novel simplified process matching tags direct matching 
basic idea direct matching scheme eliminate expensive complex process matching tokens associative memory 
direct matching scheme storage activation frame dynamically allocated tokens needed instructions code block 
code block viewed sequence instructions comprising loop body function 
actual disposition locations activation frame determined compile time actual allocation activation frames determined runtime 
direct matching scheme computation completely described pointer instruction ip pointer activation frame fp 
pair pointers called continuation corresponds tag part token 
typical instruction pointed ip specifies opcode offset activation frame match input operands instruction take place displacements define destination instructions receive result token 
destination accompanied input port left right indicator specifies appropriate input arc destination instruction 
illustrate operations direct matching detail consider token matching scheme explicit token store ets 
example ets code block invocation corresponding instruction frame memory shown 
add sub neg ip add neg sub fp fp presence bits opcode dests instruction memory frame memory code block activation ets representation dataflow program execution 
token arrives actor add ip part tag points instruction contains offset displacement destination instruction 
actual matching process achieved checking disposition slot frame memory pointed fp slot empty value token written slot presence bit set indicate slot full 
slot full value extracted leaving slot empty corresponding instruction executed 
result token generated operation communicated destination instruction updating ip displacement encoded instruction execution add operation produces result tokens 
discussion far direct matching schemes pure dataflow organizations implicit architecture 
words token matching mechanism provides full generality dataflow model execution supported hardware 
depicts original hardware implementation ets architecture extended instruction operand structure caches 
detailed description ets monsoon see 
instruction fetch unit instruction frame memory communication network token queue execution pipeline communication network instr 
cache operand addr decode operand fetch operand cache processing unit form tag formation structure cache structure organization pure dataflow processing element 
creation continuation ets performs sequence operations 
instruction fetch incoming token instruction pointer ip access instruction local instruction memory 

address decode effective address fp operand memory location computed obtained instruction fp tag token 

operand fetch presence bit operand memory location examined 
value operand memory location read written exchanged ignored depending value presence bit 
bit reset data value stored match occurs leading read 

alu match stages alu executes opcode value retrieved operand memory value contained token 
tags computed concatenating destination offsets instruction pointer 

token form result packaged tokens tags tokens written token queue 

cache ets general design cache subject constraints trade offs main memory 
issues placement replacement policy fetch update policy homogeneity addressing scheme block size cache bandwidth taken consideration 
optimizing design cache memory generally aspects maximizing probability finding memory target cache hit ratio minimizing time access information residing cache access time minimizing delay due penalty minimizing overhead updating main memory maintaining multi cache consistency locality dataflow environment principle locality backbone cache design 
dataflow program pure form amenable cache primarily due self scheduling instructions execution 
reordering instructions program certain criteria produce synthetic localities justifying presence cache 
recurrent instructions different activation frames causes existence temporal localities 
working set von neumann environment refers smallest set instructions operands satisfying current processor requests 
working set dataflow program defined minimum set instructions keep execution unit busy 
working set concept dynamic dataflow program principle locality simultaneity execution 
working set dataflow program determined analyzing dataflow graph 
initial studies reordered instructions basis time availability operands 
done grouping instructions execution levels levels 
instructions ready inputs available time unit said level 
instructions level example ready execution time unit zero 
similarly level ready execution time unit 
instruction locality achieved level ordering 
execution instruction may produce operands may destined instructions subsequent blocks need prefetch block operand locations operand memory 
refer blocks working set 
block size working set size optimized cache implementation achieve desired performance 
optimum working set depends program working sets instructions yield significant performance improvements 
locality operand cache related ordering instructions instruction cache 
instruction block referenced corresponding block brought instruction cache 
simultaneously working set operand locations corresponding instructions block prefetched operand cache 
result subsequent operand cache caused instructions satisfied operand cache 
note operand cache block consists set waiting operands empty locations storing results 
prefetching ensure stores matches caused execution instructions block take place operand cache 
instruction cache design shows detailed structure instruction cache 
structure similar conventional set associative cache additional information maintained 
low order bits instruction address ip map instruction blocks sets set blocks searched associatively 
block cache tag valid bit process count associated 
tag valid bits serve purposes conventional set associative caches 
process count refers number activation frames refer instruction 
information instruction cache replacement instruction block large number activation frames loop iterations poor candidate replacement 
valid bit process count tag instruction block cache lines sets instruction cache organization 
operand cache design operand cache memory store activation frames associated code blocks matching operands 
similar ii examined level set associativity operand cache memories 
level associativity operand cache organized set superblocks 
active context thread occupies superblock 
second level associativity accessing individual locations frame 
shows organization operand cache 
superblock consists information cold bit indicate superblock occupied 
information eliminate misses due cold start 
dataflow model operand arrive stored written need fetch empty location memory 
cold bit superblock allocate entire frame context set operand written frame 
eliminates compulsory misses writes 
tag serves identify context frame occupies superblock 
fp address obtained token tag 
working set identifiers 
memory locations activation frame token matching divided blocks working sets paralleling blocks working sets instructions code block 
superblock contains working set accessed associatively second level set associativity 
working set superblock contains cold start bit 
bit eliminate unnecessary fetches memory operands stored activation frame 
level set associativity operand cache design presents new issues studying cache designs 
cache replacement strategies 
explored replacement algorithms working sets superblock replacing superblocks 
working set cold bits tag ws addr ws ptr super blocks operand cache sets accessed associatively ws addr ws ptr operand cache organization 
replacement investigated words policy replaces working sets containing memory locations matching operands needed activation 
investigated implication compile time analysis reusing operand memory locations activation frame 
results section 
superblock replacement studied dead context replacement policy replaces superblock representing completed thread activation frame 
process control 
operand cache accommodate threads activation frames corresponding different loop iterations frames belonging 
order minimize possibility thrashing number active contexts threads carefully managed 
number active contexts depend cache size size activation frame 
noted tolerating remote memory latencies processor keep larger number contexts 
reusing locations frame reduce size activation frame increase process count 
concept controlling number active threads adopted cache memories conventional multithreaded systems 

multiprocessor ets structure cache memories section describe cache memories structures multiprocessor environment 
structure special kind memory designed handle arrays dynamic dataflow computers 
operations defined structures allocate store fetch 
allocate returns element empty array element structure flagged empty 
element structure assigned value store 
th element array set full 
attempt store values full element results error 
element array accessed fetch 
th element defined indicated full status value element returned 
request deferred value available 
shows structure example pending requests unavailable array elements 
value pending requests pending requests structure memory value full value defined empty value defined waiting pending read requests structure 
structure cache memories treat structure memory shared memory multiprocessor environment 
processors communicate types data sending receiving tokens fp part tag identifies receiving context processor containing context 
single assignment property dataflow appears eliminate problems caching structure elements processors challenging design problems 
investigated directory protocol snoopy protocol cache cache memories 
shows general structure multiprocessor system 

directory protocol 
conventional directory methods memory maintains directory structure block identify processor responsible defining writing block 
structure cache cache exists processor store structure elements needed processor including elements defined processor elements processor defined different processor 
cache block allocated structure element ready definition 
words cache blocks allocated data elements written 
possibilities considered read request structure element block received structure memory controller 
interconnection network structure memory modules directory cache module cache module cache module pe pe pe instruction cache module operand cache module structure cache module snoop table multiprocessor ets structure cache memories 
element absent structure memory 
structure memory controller directory interrogate processor responsible defining element 
cases possible 
cache processor contain requested element 
requested element defined store address element tag local table known table reflect pending requests structure element 
deferred requests maintained structure memory usual manner 
eventually data defined local entry table force write back structure memory causing structure memory controller satisfy deferred requests 
note structure memory controller need interrogate processor read request 
feel maintaining table efficient requiring directory periodically poll processor 
cache processor contains requested element 
cache block written structure memory causing deferred request satisfied 
ii structure memory contains requested data element 
possible blocks experience replacement cache misses local processor forced write cache block global structure memory room blocks 
event structure satisfy request directly 
note data written back structure memory read requests satisfied directly structure 
believe compiler analysis relied improve performance directory protocol 
read requests scheduled sufficiently writes structure elements local caches written back global structure memory read requests arrive case ii little overhead directory protocol 
extreme read requests arrive elements defined directory protocol incurs maximum overhead maintaining local tables writing back requested data immediately definition table excessively large 

snoopy protocol 
snoopy protocols processor snoop subset structure elements defined processor 
local table called snoopy table list elements processor 
read requests sent global structure memory processors snoop requests satisfy 
cases possible 
processor containing requested data cache successful snooping request request satisfied processor cache 
ii processor containing requested data cache successful snooping request 
keep snoop table small processor snoop elements contained cache 
request handled global structure memory controller 
assume structure memory maintains directory request satisfied interrogating processor containing data similar directory protocol 
alternatively request deferred block written back structure memory 
possible processor snoop elements defined cache elements defined 
requires larger snoop tables 
believe compile time analysis minimize possibility request defined data manage snoop table efficiently 

performance evaluation conventional cache experiments benchmark programs traces dataflow architectures readily available 
developed translator takes graphs sisal compiler generates ets instructions simulator graphs incorporate optimizations conventional architectures target translator performs optimizations 
caused limitations size sisal programs experiments 
process eliminating limitations imposed translator hope repeat experiments larger sisal programs 
dataflow instruction set wanted maintain dataflow purity source 
ets instructions instructions designed implement ets model see section specific implementation 
allowed actual sisal programs studies find large sisal programs 
graphs preprocessed enhance locality discussed earlier section 
fft program matrix multiplication program loop livermore loops random graph studies 
random graph study effectiveness techniques reordering instructions 
programs dataflow researchers evaluate performance sisal dataflow architectures 
table lists characteristics programs current experiment 
table program statistics 
name instructions referenced operand structure fft livermore loop matrix mult random experiments conventional cache parameters ratios 
initial experiments cache designs involved performance evaluation various cache parameters cache size working set block size 
observed effects parameters ratio similar obtained conventional caches 
indicates localities synthesized dataflow environment 
increasing operand instruction cache sizes reduces ratio seen figures 
nearly instruction cache misses due cold start misses misses reduced increasing block size shown 
noted large block size adversely effects operand cache performance 
dataflow necessary assure presence input operands instructions assure destination locations results instructions available operand cache memory 
large instruction block sizes lead large operand working sets turn lead conflict misses cache shared contexts frames 
instruction cache size kbytes instr block associativity cache block superblock associativity working set associativity process threshold fft loop matmult random effect instruction cache size ratio 
inst 
cache size instruction block size associativity working set assoc working set size process threshold operand cache size kbytes fft loop matmult random effect operand cache size ratio 
block size fft loop matmult random inst 
cache size operand cache size associativity working set assoc working set size process threshold ratio vs instruction cache block size 
fft loop matmult random block size instruction cache associativity cache block superblock associativity working set associativity process threshold effect instruction block size operand cache ratio 
working set size operand memory locations fft loop matmult random inst 
cache size instruction block size operand cache size associativity working set assoc process threshold operand working set size vs ratio 
shows effect instruction block size ratio operand cache memories 
contrasted instruction ratios 
shows effect changing operand working set size operand cache misses 
results indicate optimal block size working set size sizes remaining experiments 
investigated significance associativity instruction cache design 
set associativity increased ratio increases 
suggests direct mapped caches perform dataflow instructions 
operand cache contains levels associativity superblock associativity working set associativity varied 
increasing superblock associativity result significant reduction ratio 
optimal associativity depends block size cache design 
shows benefit addressing operands levels 
operand address space divided superblocks threads contexts frames frame operands addressed smaller addresses 
believe gives freedom compilers allocating threads loop iterations processors losing localities 
ets significance associativity thread associativity working sets behaves somewhat similar conventional operand cache associativity 
increasing working set associativity reduces ratio 
increase ratio associativity increased mainly due small size cache fewer sets 
cold start misses operand cache memories eliminated allocate fetch cache blocks write see section 
fft loop matmult random superblock associativity instruction cache associativity cache working set associativity process threshold superblock set associativity 
operand cache working set associativity fft loop matmult random inst 
cache size operand cache size associativity process threshold working set associativity 
effect unconventional cache parameters ratio 
effect process control 
motivation introducing process control avoid active threads contexts contending limited operand cache resources 
appropriate threshold value allows disciplined cache resources better performance 
readily observed 
best value threshold depends number superblocks held operand cache way set cache process threshold example find cut value number superblocks 
increasing number active contexts loop iterations processes threshold degrades performance 

effect replacement strategies 
described section explored performance gains achieved dead context replacement superblocks words replacement working sets 
dead context replacement policy shows significant improvements small caches fewer superblock misses compared random replacement policy smaller caches 
working set replacement superblock experimented words policy 
working set exists contains operand locations instructions replaced 
shows percentage operand cache misses satisfied 
improvement resulting words policy led investigate impact operand memory reuse dataflow systems 
section address details investigation 
structure cache performance 
order investigate significance structure cache extended experiments implementing processor ets system sharing structure memory 
described section processor contains instruction cache operand cache cache 
shows ratios cache size increased 
sizes shown processor cache 
ratio cache depend protocol viz directory vs snoopy throughput depends protocol 
shows results obtained varying associativity cache 
seen direct mapped caches better suited structures 
believe separate direct mapped caches arrays beneficial conventional architectures 
cache memories instruction operand structure improve performance feel cache significant contributor performance gain 
primarily improvements latencies accessing remote structure elements 
shows throughput gains reduction execution times obtained memories directory snoopy approaches 
improvements fft loop matmult random process threshold instruction cache operand cache associativity superblock associativity working set associativity operand cache size 
fft loop matmult random process threshold instruction cache operand cache associativity superblock associativity working set associativity operand cache size 
significant process threshold 
operand cache misses satisfied words replacement policy inst 
cache size instruction block size working set assoc working set size process threshold superblock 
fft loop matmult random operand cache size significance word replacement policy 
structure cache size kbytes ratio fft loop matmult random inst 
cache size operand cache size working set assoc assoc 
process threshold cache associativity directory protocol block size words 
cache size vs ratio 
structure cache associativity fft loop matmult random inst 
cache size operand cache size cache size working set assoc process threshold directory protocol cache associativity vs ratio 
shown percentage gain compared multiprocessor ets system memory 
assumed takes cycle access shared remote structure memory 
snoopy protocol consistently behaves better smaller latency required compared directory protocol 
directory approach latency round trip delay remote memory 
snoopy protocol latency smaller snooping successful see section 
instruction cache operand cache associativity superblock associativity working set associativity process threshold cache cache associativity block size feo fft loop matmult random directory snoopy throughput gains cache 
shows significance cache size performance gains benchmarks 
graph shows snoopy protocol performs better directory protocol small cache memories expected caches snoop small subset entries cache memories 

reuse operand memory indicated previous section experiments indicated operand memory locations match inputs instruction call matching location released reuse instruction completes execution 
order reuse memory location matching operands instruction analyze program dependencies 
observed matching location instruction reused matching location instruction instruction completes execution inputs instruction available 
consider dataflow graph segment shown 
assume node represents ets instruction node inputs outputs 
matching location associated instruction storing operands awaiting match 
dependencies instructions completely specified data dependencies represented directed edges 
node called left right ancestor node directed path exists node left right input node example node left ancestor node 
node common ancestor node node left right ancestor node example node common ancestor node 
likewise define left right common descendants node node common descendent nodes 
readily observed memory location matching operands node reused match operands common descendants 
instruction match operands instruction 
implemented algorithm find common descendants dataflow graph nodes matching memory locations reused 
benchmark programs repeated experiments operand cache memories evaluate performance gained reusing matching locations 
figures show results benchmark programs 
graphs compare cache ratios obtained reusing operand memory locations curves reuse 
seen small caches reuse memory locations reduces working sets needed code block order fully benefit reuse operand memory locations cache replacement policies modified 
cache block operand memory locations reused replaced 
experiments enforced replacement policy 
necessary structure cache size performance gain directory snoopy instruction cache operand cache associativity superblock associativity working set associativity process threshold cache cache associativity block size snoopy vs directory 
data dependencies operand memory reuse 
operand cache size instruction cache instr 
block instr block associativity operand block superblock associativity working set associativity process threshold significance matrix multiplication 
operand cache size instruction cache instr 
block instr block associativity operand block superblock associativity working set associativity process threshold significance fft 
operand cache size instruction cache instr 
block instr block associativity operand block superblock associativity working set associativity process threshold significance loop 
operand cache size instruction cache instr 
block instr block associativity operand block superblock associativity working set associativity process threshold significance random graph 
modify ets matching rules bit reset match takes place indicate location reused 
hope study significance program partitioning scheduling reuse 
consider example dataflow graph shown 
instruction delayed execution instructions reuse matching location instruction instruction matching operands instruction 
necessitates instructions delayed 
interesting investigate trade higher reuse concomitant improvement cache performance loss instruction level parallelism 

summary directions shown performance dataflow machines enhanced cache memories 
addition demonstrated operand memory locations frame reused matching operands multiple instructions 
believe amount operand memory reuse increased forcing sequential execution instructions thread bring model closer conventional control flow 
necessitates architectural modifications ets 
addition may reduce amount parallelism limiting performance processor 
performance improved interleaving instructions sequential threads 
clear understanding issues supporting multiple threads dataflow framework permit adapt hybrid architectures 
hybrid systems interesting opportunities area multiprocessing directly address problems faced superscalar processors long memory latencies context switching overhead multiple active instruction streams fast efficient support task synchronization 
objective claim experiments exhaustive conclusive start 
inter related parameters influence performance multiprocessor systems 
hope continue studies expanding benchmark suite extrapolate results large scale systems 
allow investigate compiler optimizations extract optimum performance set cache designs 

arvind culler 

dataflow architectures annual reviews computer science vol 
pp 

arvind nikhil 

dataflow subsume von neumann computing 
proc 
th 
intl 
symp 
computer architecture pp 

culler 

tam compiler controlled threaded machine journal parallel distributed computing pp 

feo 

report sisal language project journal parallel distributed computing pp 

hicks ang arvind 

performance studies monsoon dataflow processor journal parallel distributed computing pp 

hill smith 

evaluating associativity cpu caches ieee transactions computers pp 



dataflow von hybrid architecture proc 
th 
intl 
symp 
computer architecture pp 

hurson abraham 

design cache memories multi threaded dataflow architecture proceedings nd intl 
symp 
computer architecture pp 

lebeck wood 

cache profiling spec benchmarks case study ieee computer pp 

lee 

program partitioning multithreaded dataflow computers proc 
th hawaii international conference system sciences pp 
ii 
lee hurson 

dataflow architectures multithreading ieee computer pp 

culler 

monsoon explicit token store architecture th intl 
symp 
computer architecture pp 



implementation general purpose dataflow multiprocessor 
mit press 
porterfield 

software methods improvement cache performance supercomputer applications phd thesis dept computer science rice university 


cache memory hierarchy design performance directed approach 
morgan kaufmann san mateo ca 
shirazi hurson 

cache memory explicit token store dataflow architecture proceedings th ieee symposium parallel distributed processing pp 
smith 

cache memories 
acm computing surveys pp 



unified resource management execution control mechanism dataflow machines 
proc 
th 
intl 
symp 
computer architecture pp 



cache memories data flow architectures 
ieee transactions computers pp 


cache coherence problem shared memory multiprocessors software solutions ieee computer science press 
long 

feasibility study memory hierarchy data flow environment 
proc 
intl 
conference parallel conference pp 

tokoro jagannathan 

working set concept data flow machines proc 
th 
intl 
symp 
computer architecture pp 

tullsen eggers levy 

simultaneous multithreading maximizing chip parallelism proceeding nd international symposium computer architecture pp 
