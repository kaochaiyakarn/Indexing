published ieee trans 
pattern analysis machine intelligence 
legal stuff material ieee copyright 
personal material permitted 
permission reprint republish material advertising promotional purposes creating new collective works resale redistribution servers lists reuse copyrighted component works obtained ieee 
bayesian classification gaussian processes christopher williams department artificial intelligence university edinburgh edinburgh eh ql scotland uk dai ed ac uk david barber theoretical foundation snn university nijmegen ez nijmegen netherlands kun nl january consider problem assigning input vector classes predicting cjx class problem probability class estimated oe oe gammay 
gaussian process prior placed combined training data obtain predictions new points 
provide bayesian treatment integrating uncertainty parameters control gaussian process prior necessary integration carried laplace approximation 
method generalized multi class problems softmax function 
demonstrate effectiveness method number datasets 
index terms gaussian processes classification problems parameter uncertainty markov chain monte carlo hybrid monte carlo bayesian classification 
consider problem assigning input vector classes predicting cjx classic example method logistic regression 
class problem probability class estimated oe oe gammay 
method flexible discriminant surface simply hyperplane space 
problem overcome extent expanding input set basis functions foe example quadratic functions components high dimensional input space large number basis functions associated parameter risks overfitting training data 
motivates bayesian treatment problem priors parameters encourage smoothness model 
putting priors parameters basis functions indirectly induces priors functions produced model 
possible argue natural put priors directly functions 
advantage function space priors impose general smoothness constraint tied limited number basis functions 
regression case task predict real valued output possible carry non parametric regression gaussian processes gps see 
solution regression problem gp prior gaussian noise model place kernel function training data point coefficients determined solving linear system 
parameters describe gaussian process unknown bayesian inference carried described 
gaussian process method extended classification problems defining gp input sigmoid function 
idea number authors previous treatments typically take fully bayesian approach ignoring uncertainty posterior distribution data uncertainty parameters 
attempts fully bayesian treatment problem introduces particular form covariance function gaussian process prior believe useful modelling point view 
structure remainder follows section discusses gaussian processes regression problems essential background classification case 
section describe application gaussian processes class classification problems extend multiple class problems section 
experimental results section followed discussion section 
revised expanded version 
gaussian processes regression useful consider regression problem prediction real valued output new input value set training data ng 
relevance strategy transform classification problem regression problem dealing input values logistic transfer function 
stochastic process prior functions allows specify set inputs xn distribution corresponding outputs def xn 
denote prior functions similarly joint distribution including specify probability observing particular values actual values noise model jt dy jy dy jy dy predictive distribution marginalization product prior noise model 
note order predictions necessary deal directly priors function space dimensional joint densities 
easy carry calculations densities involved special form 
gaussian jt gaussian mean variance calculated matrix computations involving matrices size theta specifying multidimensional gaussian values placements points xn means prior functions gaussian process 
formally stochastic process collection random variables fy jx xg indexed set case input space dimension number inputs 
gp stochastic process fully specified mean function covariance function gamma gamma finite set variables joint multivariate gaussian distribution 
consider gps 
assume noise model gaussian mean zero variance oe predicted mean variance oe gamma oe gamma oe gamma ij xn see 
parameterizing covariance function reasonable choices covariance function 
formally required specify functions generate non negative definite covariance matrix set points 
modelling point view wish specify covariances points nearby inputs give rise similar predictions 
find covariance function works expf gamma gamma lth component log log log log vector parameters needed define covariance function 
note analogous hyperparameters neural network 
define parameters log variables equation positive scale parameters 
covariance function obtained network gaussian radial basis functions limit infinite number hidden units 
parameters equation allow different length scale input dimension 
irrelevant inputs corresponding small model ignore input 
closely related automatic relevance determination ard idea mackay neal 
variable specifies scale prior 
specifies variance zero mean offset gaussian distribution 
gaussian process framework allows quite wide variety priors functions 
example ornstein uhlenbeck process covariance function gammax rough sample paths differentiable 
hand squared exponential covariance function equation gives rise infinitely differentiable process 
general believe gp method quite general purpose route imposing prior beliefs desired amount smoothness 
reasonably high dimensional problems needs combined modelling assumptions ard 
modelling assumption may build covariance function sum covariance functions may depend input variables see section details 
dealing parameters covariance function straightforward predictions new test points 
practical situations know covariance function 
option choose parametric family covariance functions parameter vector estimate parameters example method maximum likelihood bayesian approach posterior distribution parameters obtained 
calculations facilitated fact log likelihood log dj calculated analytically gamma log kj gamma gamma gamma log oe kj denotes determinant possible express analytically partial derivatives log likelihood respect parameters gamma tr gamma gamma gamma see 
derivatives respect straightforward feed information optimization package order obtain local maximum likelihood 
general may concerned making point estimates number parameters large relative number data points parameters may poorly determined may local maxima likelihood surface 
reasons bayesian approach defining prior distribution parameters obtaining posterior distribution data seen attractive 
prediction new test point simply averages posterior distribution jd jd jd gps possible integration analytically general numerical methods may 
sufficiently low dimension techniques involving grids space 
high dimensional difficult locate regions high posterior density techniques importance sampling 
case markov chain monte carlo mcmc methods may 
constructing markov chain equilibrium distribution desired distribution jd integral equation approximated samples markov chain 
standard methods constructing mcmc methods gibbs sampler metropolis hastings algorithms see 
conditional parameter distributions amenable gibbs sampling covariance function form equation metropolis hastings algorithm utilize derivative information available means tends inefficient random walk behaviour 
neal bayesian treatment neural networks williams rasmussen rasmussen hybrid monte carlo hmc method duane obtain samples jd 
hmc algorithm described detail appendix obtained squashing sigmoid function oe 
gaussian processes class classification simplicity exposition method applied problems extension multiple classes covered section 
logistic transfer function produce output interpreted probability input belonging class job specifying prior functions transformed specifying prior input transfer function shall call activation denote see 
class problem logistic function oe oe gammay 
denote probability activation corresponding input respectively 
fundamentally gp approaches classification regression problems similar error model oe regression case replaced bern oe 
choice equation affect hard classification hovers takes extreme values 
previous related approach discussed section 
regression case problems address making predictions fixed parameters dealing parameters 
shall discuss issues turn 
making predictions fixed parameters predictions fixed parameters compute jt requires find jt jt new input done finding distribution jt activation appropriate jacobian transform distribution 
formally equations obtaining jt identical equations 
gp prior gaussian usual expression gamma gammat classification data take values means marginalization obtain jt longer analytically tractable 
faced problem routes follow analytic approximation integral equations ii monte carlo methods specifically mcmc methods approximate 
consider analytic approximation laplace approximation approximations discussed section 
laplace approximation integrand approximated gaussian distribution centered maximum function respect inverse covariance matrix log 
finding maximum carried newton raphson iterative method allows approximate distribution calculated 
details maximization procedure appendix integration parameters predictions integrate predicted probabilities posterior jt tj saw 
regression problem tj calculated exactly tj yj dy integral analytically tractable classification problem 
psi log log 
log jy gamma log obtain psi gamma log gamma gamma gamma log jkj gamma log laplace approximation maximum find log tj psi gamma log jk gamma log denote right hand side equation log tj stands approximate 
integration space done analytically employ markov chain monte carlo method 
neal williams rasmussen hybrid monte carlo hmc method duane described appendix log tj approximation log tj broad gaussian priors parameters 
previous related gaussian processes regression classification developed observation large class neural network models converge gps limit infinite number hidden units 
computational bayesian treatment gps easier neural networks 
regression case infinite number weights effectively integrated ends dealing hyper parameters 
results show gaussian processes regression comparable performance state art methods 
non parametric methods classification problems seen arise combination different strands 
starting linear regression mccullagh nelder developed generalized linear models 
class classification context gives rise logistic regression 
strand development non parametric smoothing regression problem 
viewed gaussian process prior functions traced back far kolmogorov wiener 
gaussian process prediction known field see known kriging 
alternatively considering roughness penalties functions obtain spline methods overviews see 
close connection gp roughness penalty views explored 
combining non parametric regression obtains shall call non parametric glm method classification 
early method include discussions texts 
differences non parametric glm method usually described bayesian treatment 
firstly fixed parameters non parametric glm method ignores uncertainty need integrate described section 
second difference relates treatment parameters 
discussed section parameters attempt obtain point estimate parameters carry integration posterior 
point estimates may obtained maximum likelihood estimation cross validation generalized cross validation gcv methods see 
problem cv type methods dimension large computationally intensive search region grid parameter space looking parameters maximize criterion 
sense hmc method described doing similar search gradient information carrying averaging posterior distribution parameters 
defence cv methods note wahba comments referring back methods may robust unrealistic prior 
difference kinds non parametric glm models usually considered method exact nature prior 
roughness penalties expressed terms penalty kth derivative gives rise power law power spectrum prior 
differences parameterization covariance function example unusual find parameters ard introduced equation non parametric glm models 
hand wahba considered smoothing spline analysis variance ss anova decomposition 
gaussian process terms builds prior sum priors functions decomposition ff ff ff ff fi fffi ff fi important point functions involving orders interaction univariate functions give rise additive model included sum full interaction term possible obtain derivatives cv score respect knowledge practice 

bayesian point view questions kinds priors appropriate interesting modelling issue 
related method 
section mentioned necessary approximate integral equations described laplace approximation 
preliminary version gibbs mackay developed alternative analytic approximation variational methods find approximating gaussian distributions bound marginal likelihood tj 
approximate distributions predict jt 
parameters gibbs mackay estimated maximizing lower bound tj 
possible fully mcmc treatment classification problem discussed neal 
method carries integrations posterior distributions simultaneously 
works generating samples jd stage process 
firstly fixed individual updated sequentially gibbs sampling 
sweep takes time matrix gamma computed time sense perform quite gibbs sampling scans update parameters probably markov chain mix faster 
secondly parameters updated hybrid monte carlo method 
predictions averages predictions sample 
gps multiple class classification extension preceding framework multiple classes essentially straightforward notationally complex 
employ class coding scheme multiclass analogue logistic function softmax function describe class probabilities 
probability instance labelled class denoted upper index denotes example number lower index class label 
similarly activations associated probabilities denoted formally softmax link function relates activations probabilities exp sigma exp automatically enforces constraint 
targets similarly represented specified coding 
log likelihood takes form ln softmax link function gives gamma ln exp class case shall assume gp prior operates activation space specify correlations activations important assumption prior knowledge restricted correlations activations particular class 
whilst difficulty extending framework include inter class correlations encountered situation felt able specify correlations 
class represented vector length zero entries correct component contains 
formally activation correlations take form hy ffi element covariance matrix cth class 
individual correlation matrix form equation case 
shall separate set parameters class 
independent processes perform classification redundant forcing activations process say zero introduce arbitrary asymmetry prior 
simplicity introduce augmented vector notation gamma delta class case denotes activation corresponding input class notation define similar manner define excluding values corresponding test point 
definition augmented vectors gp prior takes form exp ae gamma gamma oe equation covariance matrix block diagonal matrices individual matrix expresses correlations activations class class case laplace approximation need find mode jt 
procedure described appendix case predictions averaging softmax function gaussian approximation posterior distribution simply estimate integral draws gaussian random vector generator 
experimental results newton raphson algorithm initialized time entries iterated mean relative difference elements consecutive iterations gamma hmc algorithm step size parameters large possible keeping rejection rate low 
trajectory leapfrog steps gave low correlation successive states 
priors parameters set gaussian mean gamma standard deviation 
simulations step size produced low rejection rate 

parameters corresponding initialized gamma 
sampling procedure run iterations third run discarded burn intended give parameters time come close equilibrium distribution 
tests carried coda package examples section suggested effective removing transients note widely recognized see determining equilibrium distribution reached difficult problem 
number iterations typically mcmc methods remembered iteration involves leapfrog steps ii hmc aim reduce random walk behaviour seen methods metropolis available comprehensive archive network www ci tuwien ac 
algorithm 
autocorrelation analysis parameter indicated general low correlation obtained lag iterations 
matlab code run experiments available ftp cs aston ac uk neural 
classes tried method known class classification problems crabs pima indian diabetes datasets rescale inputs mean zero unit variance training set 
matlab implementations hmc simulations tasks take hours sgi challenge machine mhz results obtained time 
tried standard metropolis mcmc algorithm crabs problem similar results sampling method somewhat slower hmc 
results crabs pima tasks comparisons methods tables respectively 
tables include results obtained gaussian processes estimation parameters maximizing penalised likelihood iterations scaled conjugate gradient optimiser neal mcmc method 
details set neal method appendix crabs problem attempt classify sex crabs basis anatomical attributes optional additional colour attribute 
examples available crabs sex colour making total labelled examples 
split training set available markov stats ox ac uk pub prnn 
crabs sex colour making training examples examples test set 
performance gp method equal best methods reported hidden unit neural network direct input output connections logistic output unit trained maximum likelihood network table 
neal method gave similar level performance 
estimating parameters maximum penalised likelihood mpl gave similar performance minute computing time 
pima indians diabetes problem data available prof ripley training test split examples respectively 
baseline error obtained simply classifying record coming gives rise error 
neal gp methods comparable best alternative performance error 
encouraging results obtained laplace approximation neal method similar estimated parameters maximum penalised likelihood monte carlo integration 
performance case little worse error minutes computing time 
analysis posterior distribution parameters covariance function equation informative 
plots posterior marginal mean standard deviation error bars input dimensions 
recalling variables scaled zero mean unit variance appear variables shortest performance obtained gibbs mackay similar 
method errors crab task colour errors pima dataset 
method colour colour neural network neural network linear discriminant logistic regression mars degree pp regression ridge functions gaussian process laplace approximation hmc gaussian process laplace approximation mpl gaussian process neal method table number test errors crabs task 
comparisons taken ripley ripley respectively 
network hidden units predictive approach ripley uses laplace approximation weight network local minimum 
variability associated 
multiple classes due long time taken run code able test relatively small problems mean data points classes 
furthermore full bayesian integration possible parameter settings computational means satisfied maximum penalised likelihood approach 
method pima indian diabetes neural network linear discriminant logistic regression mars degree pp regression ridge functions gaussian mixture gaussian process laplace approximation hmc gaussian process laplace approximation mpl gaussian process neal method table number test errors pima indian diabetes task 
comparisons taken ripley ripley respectively 
neural network hidden unit trained maximum likelihood results worse nets hidden units ripley 
plot log parameters pima dataset 
circle indicates posterior marginal mean obtained hmc run burn standard deviation error bars 
square symbol shows log parameter values maximizing penalized likelihood 
variables 
number 
plasma glucose concentration 
blood pressure 
skin fold thickness 
body mass index 
diabetes pedigree function 
age 
comparison wahba generalized linear regression variables important 
potential gradient hmc routine simply inputs scaled conjugate gradient optimiser attempting find mode class posterior average posterior distribution 
tested multiple class method forensic glass dataset described 
dataset examples inputs output classes 
dataset small performance estimated fold cross validation 
computing penalised maximum likelihood estimate multiple gp method took approximately hours sgi challenge gave classification error rate 
see table comparable best methods 
performance neal method surprisingly poor may due fact allow separate parameters processes constrained equal neal code 
small significant differences specification prior see appendix details 
discussion extended williams rasmussen classification problems demonstrated performs datasets tried 
believe kinds gaussian process prior easily interpretable models neural networks priors parameterization function space 
example posterior distribution ard parameters illustrated pima indians diabetes problem indicates relative importance method forensic glass neural network hu linear discriminant mars degree pp regression ridge functions gaussian mixture decision tree gaussian process la mpl gaussian process neal method table percentage test error forensic glass problem 
see ripley details methods 
various inputs 
interpretability facilitate incorporation prior knowledge new problems 
quite strong similarities gp classifiers support vector machines svms 
svm uses covariance kernel differs gp approach different data fit term maximum margin optimal quadratic programming 
comparison algorithms interesting direction research 
problem methods gps require computations trace determinants linear solutions involving theta matrices number training examples run problems large datasets 
looked methods bayesian numerical techniques calculate trace determinant techniques relatively small size problems tested methods 
computational methods speed quadratic programming problem svms may useful gp classifier problem 
investigating different covariance functions improvements approximations employed 
prof ripley making available crabs pima indian diabetes forensic glass datasets 
partially supported epsrc gr novel developments learning theory neural networks carried aston university 
authors gratefully acknowledge hospitality provided isaac newton institute mathematical sciences cambridge uk written 
mark gibbs david mackay radford neal helpful discussions anonymous referees comments helped improve 
appendix maximizing jt class case describe find iteratively vector jt maximized 
material covered 
provide completeness terms equation defined 
denote complete set activations 
bayes theorem log jt log log gamma log psi log log 
depend just normalizing factor maximum jt maximizing psi respect log jy gamma log obtain psi gamma log gamma gamma gamma log jk gamma log covariance matrix gp evaluated xn psi defined similarly equation 
partitioned terms theta matrix theta vector scalar viz 
enters equation quadratic prior term data point associated maximizing psi respect achieved maximizing psi respect doing quadratic optimization determine find maximum psi iteration new gamma rr psi gamma psi 
differentiating equation respect find psi gamma gamma gamma rr psi gammak gamma gamma noise matrix diag gamma gamma 
results iterative equation new gamma gamma gamma gamma avoid unnecessary inversions usually convenient rewrite form new wk gamma wy gamma note psi positive definite optimization problem convex 
converged solution easily gamma gamma gamma gamma equation 
var gamma gamma zero appended th diagonal position 
mean variance easy find jt mean distribution jt 
order calculate gaussian integral logistic sigmoid function employ approximation expansion sigmoid function terms error function 
gaussian integral error function error function approximation fast compute 
specifically basis set scaled error functions interpolate logistic sigmoid chosen points gives accurate approximation gamma desired integral small computational cost 
justification laplace approximation case somewhat different argument usually put forward asymptotic normality maximum likelihood estimator model finite number parameters 
dimension problem grows number data points 
consider asymptotics see number data points bounded region increases local average training data point provide tightly localized estimate reasoning parallels formal arguments 
expect distribution gaussian increasing data 
detail basis functions erf 
interpolate oe 
appendix derivatives log tj wrt 
hmc mpl methods require derivative log tj respect components example derivative involve terms due explicit dependencies psi gamma log jk gamma log change cause change chosen psi obtain fi fi fi fi explicit gamma log jk gamma dependence jk gamma arises dependence differentiating gamma obtains kw gamma gamma required derivative calculated 
appendix maximizing jt multiple class case gp prior likelihood defined equations define posterior distribution activations jt 
appendix interested laplace approximation posterior need find mode respect dropping unnecessary constants multi class analogue equation psi gamma gamma gamma log jk gamma ln exp principle appendix define psi analogy equation optimize psi respect performing quadratic optimization psi respect order optimize psi respect hessian rr psi gammak gamma gamma mn theta mn block diagonal matrix blocks form class case equation slight change definition noise matrix convenient way define introducing matrix pi mn theta matrix form pi diag diag 
notation write noise matrix form diagonal matrix outer product pi pi class case note psi positive definite optimization problem convex 
update equation iterative optimization psi respect activations follows form equation 
advantage representation noise matrix equation invert matrices find determinants identities pi pi gamma gamma gamma gamma pi pi gamma pi gamma pi gamma det pi pi det det pi gamma pi gamma diag 
block diagonal inverted blockwise 
requiring determinants inverses mn theta mn matrix need carry expensive matrix computations theta matrices 
resulting update equations form equation noise matrix covariance matrices multiple class form 
essentially results needed generalize method multiple class problem 
mentioned time complexity problem scale due identities equations calculating function gradient expensive 
experimented methods mode finding laplace approximation 
advantage newton iteration method fast quadratic convergence 
integral part newton step calculation inverse matrix acting vector gamma order speed particular step conjugate gradient cg method solve iteratively corresponding linear system mz repeatedly need solve system changes updated saves time run cg method convergence time called 
experiments cg algorithm terminated jr gamma mz gamma calculation derivative log tj wrt multiple class case analogous class case described appendix appendix hybrid monte carlo hmc works creating fictitious dynamical system parameters regarded position variables augmenting momentum variables purpose dynamical system give parameters inertia random walk behaviour space avoided 
total energy system sum kinetic energy potential energy potential energy defined jd exp gammae gamma log tj gamma log 
sample joint distribution exp gammae gamma marginal distribution required posterior 
sample parameters posterior obtained simply ignoring momenta 
sampling joint distribution achieved steps finding new points phase space near identical energies simulating dynamical system discretised approximation hamiltonian dynamics ii changing energy gibbs sampling momentum variables 
hamilton order differential equations approximated leapfrog method requires derivatives respect 
gaussian prior log straightforward differentiate 
derivative log tj straightforward implicit dependencies taken account described appendix calculation energy quite expensive new need perform maximization required laplace approximation equation 
proposed state accepted rejected metropolis rule depending final energy necessarily equal initial energy discretization 
appendix simulation set neal code fbm software available www cs utoronto ca radford fbm software html 
example commands run pima example gp spec pima log model spec pima log binary pima log pima tr te gp gen pima log fix mc spec pima log repeat scan values hybrid gp mc pima log follow closely example neal documentation 
gp spec command specifies form gaussian process particular priors parameters see equation 
expression specifies gamma distribution prior specifies hierarchical gamma prior 
note jitter specified prior covariance function improves conditioning covariance matrix 
mc spec command gives details mcmc updating procedure 
specifies repetitions scans values followed hmc updates parameters step size adjustment factor 
gp mc specifies sequence carried times 
aimed rejection rate 
exceeded stepsize reduction factor reduced simulation run 
barber williams 
gaussian processes bayesian classification hybrid monte carlo 
mozer jordan petsche editors advances neural information processing systems 
mit press 
carlin 
markov chain monte carlo convergence diagnostics comparative review 
american stat 
assoc 
cressie 
statistics spatial data 
wiley new york 
duane kennedy 
hybrid monte carlo 
physics letters 
gelman carlin stern rubin 
bayesian data analysis 
chapman hall london 
gibbs mackay 
efficient implementation gaussian processes 
draft manuscript available wol ra phy cam ac uk mackay homepage html 
gibbs mackay 
variational gaussian process classifiers 
draft manuscript available wol ra phy cam ac uk mackay homepage html 
green silverman 
nonparametric regression generalized linear models 
chapman hall london 
kimeldorf wahba 
correspondence bayesian estimation stochastic processes smoothing splines 
annals mathematical statistics 
mackay 
bayesian methods backpropagation networks 
van hemmen domany schulten editors models neural networks ii 
springer 
mardia marshall 
maximum likelihood estimation models residual covariance spatial regression 
biometrika 
mccullagh nelder generalized linear models 
chapman hall 
mller 
scaled conjugate gradient algorithm fast supervised learning 
neural networks 
neal 
monte carlo implementation gaussian process models bayesian regression classification 
technical report department statistics university toronto 
available www cs toronto edu radford 
neal bayesian learning neural networks 
springer new york 
lecture notes statistics 
sullivan 
automatic smoothing regression functions generalized linear models 
journal american statistical association 
rasmussen 
evaluation gaussian processes methods non linear regression 
phd thesis dept computer science university toronto 
available www cs utoronto ca carl 
ripley 
pattern recognition neural networks 
cambridge university press cambridge uk 
ripley 
statistical aspects neural networks 
jensen kendall editors networks chaos statistical probabilistic aspects pages 
chapman hall 
ripley 
flexible non linear approaches classification 
friedman wechsler editors statistics neural networks pages 
springer 
silverman 
density ratios empirical likelihood cot death 
applied statistics 
skilling 
bayesian numerical analysis 
jr editors physics probability 
cambridge university press 
vapnik 
nature statistical learning theory 
springer verlag new york 
wahba 
comparison gcv gml choosing smoothing parameter generalized spline smoothing problem 
annals statistics 
wahba 
spline models observational data 
society industrial applied mathematics 
cbms nsf regional conference series applied mathematics 
wahba gu wang chappell 
soft classification risk estimation penalized log likelihood smoothing spline analysis variance 
wolpert editor mathematics generalization 
addison wesley 
proceedings volume xx santa fe institute studies sciences complexity 
williams 
computing infinite networks 
mozer jordan petsche editors advances neural information processing systems 
mit press 
williams rasmussen 
gaussian processes regression 
touretzky mozer hasselmo editors advances neural information processing systems pages 
mit press 

comparison kriging nonparametric regression methods 
multivariate analysis 

