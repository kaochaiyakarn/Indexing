reinforcement learning play optimal nash equilibrium team markov games wang electrical computer engineering department carnegie mellon university pittsburgh pa andrew cmu edu tuomas sandholm computer science department carnegie mellon university pittsburgh pa sandholm cs cmu edu multiagent learning key problem game theory ai 
involves interrelated learning problems identifying game learning play 
problems prevail team games agents interests con ict 
team games multiple nash equilibria optimal 
optimal adaptive learning oal rst algorithm converges optimal nash equilibrium team markov game 
provide convergence proof show algorithm parameters easy set convergence conditions met 
experiments show existing algorithms converge problems oal 
demonstrate importance fundamental ideas oal incomplete history sampling biased action selection 
multiagent learning key problem game theory ai 
decade computer scientists worked extending reinforcement learning rl multiagent settings 
game theoretic terms multiagent rl problem learning play nash equilibria markov games aka 
stochastic games game structure unknown 
studied various types markov games zero sum markov games markov games team markov games :10.1.1.138.2589
multiagent rl markov games involves interrelated learning problems identifying game learning play 
problems prevail team markov games agents common interests 
team markov games multiple nash equilibria optimal maximize sum agents discounted payo 
agents learn game structure coordinate play optimal nash equilibrium 
hu wellman rl algorithm markov games agents learn play nash equilibrium exactly nash equilibrium :10.1.1.138.2589
littman introduced rl algorithm agents learn structure markov game agents learn play coordinate 
claus boutilier designed rl algorithm team markov games 
agents learn play nash equilibrium suboptimal 
learning play optimal nash equilibrium team markov games posed important open problems multiagent reinforcement learning 
equilibrium selection infamous problem game theory 
optimal adaptive learning oal rst algorithm converges optimal nash equilibrium team markov game section 
works number agents 
prove convergence show oal parameters easy set convergence conditions met section 
experiments show existing algorithms converge problems oal section 
demonstrate importance fundamental ideas oal incomplete history sampling biased action selection 
setting mdps reinforcement learning rl markov decision problem agent environment 
fully observable markov decision problem mdp tuple hs nite state space space actions agent take payo function expected payo action state transition function probability state action taken state 
agent deterministic policy aka 
strategy map ping states actions 
denote action policy prescribes state objective nd policy maximizes payo time discount factor 
exists deterministic optimal policy 
function policy de ned set equations max 
state optimal policy chooses arg max 
reinforcement learning rl viewed sampling method estimating payo function transition function unknown 
approximated function calculated agent experience time model approach uses samples generate models iteratively computes max 
learning policy assigns probabilities actions state 
learning policy greedy limit nite exploration glie property converge model model free approach agent converge behavior optimal policy 
glie state action pair visited nitely limit action selection greedy respect function 
common glie policy boltzmann exploration 
multiagent reinforcement learning team markov games game unknown natural extension mdp multiagent environments markov game aka 
stochastic game 
focus team markov games markov games agent receives payo words con icts agents learning game structure learning coordinate highly nontrivial 
de nition team markov game aka stochastic game tuple set agents nite state space joint action space agents common payo function transition function 
objective agents nd deterministic joint policy aka 
joint strategy aka 
strategy pro le mapping states joint actions maximize expected sum common discounted payo function expected sum discounted payo agents play joint action state follow policy 
optimal function function optimal policy captures game structure 
agents generally know advance 
know payo structure transition probabilities 
joint policy nash equilibrium individual policy best response 
individual policy joint policy agents agent nash equilibrium strict inequality strict 
optimal nash equilibrium nash equilibrium gives agents maximal expected sum discounted payo team games optimal nash equilibrium optimal joint policy optimal joint policies 
joint action optimal state treat payo joint action state obtain team game matrix form 
call game state game state optimal joint action optimal nash equilibrium state game 
task optimal coordination team markov game boils having agents play optimal nash equilibrium state game 
coordination problem arises multiple optimal nash equilibria 
player table player coordination game game table optimal nash equilibria fa fa fa 
act optimally agents coordinate equilibrium play 
problem called equilibrium selection 
learning approach widely advocated tackle problem ctitious play agent assumes agents stationary unknown joint policy models policy frequency past observations 
agent uses best response strategy model 
myopic adjustment converges nash equilibrium games including team games 
generally converge optimal nash equilibrium 
optimal adaptive learning oal algorithm agents may learn play game coordinating actions di erent game 
example adding arti cial payo may help guide robots learning 
likewise construct virtual game vg state team markov game eliminate strict suboptimal nash equilibria state 
payo agents nash equilibrium discuss subgame perfect nash equilibrium 
re nement nash equilibrium rst introduced di erent games 
receive vg state joint action arg max 
example vg game table gives payo optimal nash equilibrium fa fa fa payo joint action 
virtual games simplify coordination task learning agents 
example game table vg weakly acyclic 
de nition weakly acyclic game player game matrix form 
best response graph takes joint action vertex connects vertices directed edge exists exactly agent best response say game weakly acyclic initial vertex exists directed path vertex outgoing edge 
tackle equilibrium selection problem weakly acyclic games young proposed special ctitious play algorithm called adaptive play ap works follows 
joint action played time player game matrix form 
fix integers agent randomly chooses actions 
starting agent looks back plays randomly replacement selects samples number times reduced joint action joint action agent individual action appears samples 
agent payo joint action played 
agent calculates expected payo individual action ep fa randomly chooses action set best responses br fa arg max ep young showed ap weakly acyclic game converges strict deterministic nash equilibrium 
ap vg game table leads equilibrium payo 
example equilibrium optimal nash equilibrium original game 
unfortunately treatment trivially extended vgs vgs weakly acyclic 
fact vg table strict nash table virtual game strict nash equilibria 
equilibria 
show experiments ap converge deterministic joint policy furthermore learn avoid payo summary applicability ap limited 
order address general settings modify notion weakly acyclic game adaptive play accommodate weak optimal nash equilibria 
de nition weakly acyclic game biased set biased set containing nash equilibria game joint policies 
game initial vertex exists directed path nash equilibrium inside strict nash equilibrium 
convert vg setting biased set include joint policies give payo joint policies 
solve game introduce new learning algorithm equilibrium selection 
enables agent deterministically select best response action nash equilibrium biased set attained exist best responses nash equilibrium strict 
di erent ap players randomize action selection multiple best response actions 
call approach biased adaptive play bap 
bap works follows 
biased set composed nash equilibria game matrix form 
sp set samples drawn time replacement joint actions 

exists joint action sp 
exists joint action sp agent chooses best response action sp dg 
contained play nash equilibrium inside hand conditions met agent chooses best response action way ap 
show bap glie exploration converges nash equilibrium strict nash equilibrium 
far tackled learning coordination team markov games game structure known 
real interests learning game unknown 
multiagent reinforcement learning asymptotically approximated virtual game 
question construct assure 
method achieving notion optimality oal algorithm nd actual optimum 
de nition positive constant 
joint action optimal state time max denote set optimal joint actions state time 
idea decreasing bound estimate state time joint actions belonging set treated optimal nash equilibria virtual game give agents payo 
converges zero rate slower converges 
proportional function decreases slowly monotonically zero smallest number times state action pair sampled far 
ready entire optimal adaptive learning oal algorithm 
craft carefully understanding convergence rate model rl algorithm learn game structure 
optimal adaptive learning algorithm agent 
initialization 
jsj 

learning coordination policy randomly select action update virtual game state 
set fa 
glie exploration exploitation probability randomly select replacement records observations joint actions played state ii 
calculate expected payo individual action virtual game current state follows ep fa 
construct best response set state time br fa ja arg max ep iii 
conditions bap met choose best response action respect biased set randomly select best response action br 
randomly select action explore 

policy learning game structure observe state transition payo joint action 
ii 

iii 

iv 

max 

mins 
cb cb 
ii 
update 
iii 
max number times joint action played state time 
positive constant value 
number times joint action appears agent samples time joint actions taken state proof convergence oal section prove oal converges optimal nash equilibrium 
common rl assumptions payo bounded number states actions nite 
proof organized follows 
section show oal learns optimal coordination game known 
speci cally show bap known game structure converges nash equilibrium glie exploration 
section show oal learn game structure 
speci cally virtual game converted learned surely 
tracks merge section shows oal learn game structure optimal coordination 
learning coordinate known game section rst model biased adaptive play bap algorithm stationary markov chain 
second half section model bap glie exploration nonstationary markov chain 
bap stationary markov chain consider bap randomly selected initial plays 
take initial history hm initial state markov chain 
de nition states inductive successor state state obtained deleting left element appending new right element 
exception states member biased set strict nash equilibrium grouped unique terminal state state directing treated directly connected state transition matrix markov chain 
successor fa players new element appended right get transition probability agent exists sample size best response action selection rule bap 
agent chooses sample probability independent time markov chain stationary 
due clustering multiple states terminal state state connected ht ht model system reaches terminal state agent best response repeat action 
straightforward actual terminal state states clustered form terminal state strict nash equilibrium 
hand weak nash equilibrium case bap biases agent choose action conditions bap satis ed 
terminal state absorbing state nite markov chain 
theorem weakly acyclic game biased set length shortest directed path best response graph joint action absorbing vertex vertex lg max 
lg biased adaptive play converges strict nash equilibrium nash equilibrium proof 
proof similar proof theorem treatment terminal state biased set bap biased set game exactly absorbing state markov chain model 
show markov chain unique stationary distribution occurs 
basic idea show exists time independent pair positive integer positive probability starting state probability agents attain periods implies probability reaching steps drops exponentially zero 
starting random state time exists positive time independent probability agents keep sampling plays consecutive times choose best responses best response sets 
results new state identical elements 
conditions bap satis ed 
agent sticks individual action contained strict nash equilibrium agents adhere unique best responses positive probability agents sample periods akin modeled bap stationary markov chain young modeled adaptive play ap stationary markov chain 
di erences 
ap bap action selection biased 
second young model possible absorbing states model absorbing state exists team game model exactly absorbing state 
cluster absorbing states 
allows prove main convergence theorem 
cases learning process converge probability 
reasoning agents memory large contain takes periods 
strict nash equilibrium member set rst periods positive time independent probability periods exactly agent samples takes best response fa second vertex shortest directed best response path starting leading strict nash equilibrium nash equilibrium agents sample take individual best response contained brings new consecutive plays denoted belongs set strict nash equilibrium logic paragraph applies terminal state attained positive probability 
needs happens periods 
reasoning positive probability agents follow shortest directed best response path starting length 
nth step path agents need sample produce agents memory smaller agents totally take nk steps get absorbing state 
induction agents positive time independent probability complete best response path periods memory sizes lg max upper bound 
denote positive probability starting state getting time independent state independent probability agents reach periods min 
theorem says stationary markov chain bap lg unique stationary distribution terminal state probability appear 
bap glie exploration nonstationary markov chain section discuss problem identifying game 
accomplish learners need exploration 
section show exploration hurt learning task learning coordinate 
show rst modeling bap glie exploration non stationary markov chain 
glie exploration time step joint action occurs positive probability 
means system transitions state successor states positive probability 
glie exploration agents action selection comes increasingly greedy time 
limit probability transition probabilities converge bap exploration 
model learning process sequence transition matrices fp lim transition matrix stationary markov chain describing bap exploration 
objective show bap glie exploration converge clustered terminal state 
need lemma 
lemma nite transition matrix stationary markov chain unique stationary distribution fp sequence nite transition matrices 
probability vector denote 

lim lim 
lemma prove theorem 
theorem bap glie bap glie exploration lg converges strict nash equilibrium nash equilibrium proof 
theorem shows bap lg converges unique terminal state surely 
stationary markov chain unique limit distribution terminal state probability appear 
guaranteed properties glie learning policy nonstationary markov chain bap glie transition matrix sequence converging transition matrix stationary chain 
lemma starting state bap glie lg converges surely unique stationary distribution stationary chain stay terminal state forever 
learning virtual game far shown game structure known bap converge terminal state 
prove optimal convergence oal algorithm need demonstrate lemma easily obtained theorem theorem proofs 
theorem says stationary chain ergodic unique stationary distribution converges nonstationary markov chain strongly ergodic unique stationary distribution conditions theorem met 
proof theorem shows converges constant matrix row equal unique probability left eigenvector chain ergodic 
suces show starting distribution nonstationary markov chain converge unique stationary distribution stationary chain 
virtual game oal temporary virtual game converge correct virtual game 
rst issues handled lemma lemma virtual game vg player team state game weakly acyclic game biased set contains optimal nash equilibria joint actions 
de nition virtual game strict nash equilibria optimal ones 
length shortest best response path lemma implies bap known virtual game glie exploration converge optimal nash equilibrium 
theorem bap converge nash equilibrium biased set strict nash equilibrium lemma virtual game nash equilibria optimal 
proof 
consider team state game optimal joint action set biased set fa joint action outside loss generality re arrange individual actions inside exists integer fa satisfying contains formation virtual game agent best responses fa successors best response graph agent best response fa vertex line reasoning construct directed best response path length path get weakly acyclic game set lemmas link proof chain 
show oal cause agents obtain correct virtual game surely 
lemma team markov game part oal assures max jq log log constant 
proof 
oal algorithm model update rule max estimates environmental model transition function payo function agent experience time de ne function max 
triangle inequality jq jq components right hand side study second component rst 
jr jt jj max jr max jt jsj number states upper bound jq oal algorithm sample means random variables means 
law large number says converge 
addition convergence rate follow law iterated logarithm 
converges rate asymptotically log log minimal number samplings state action pairs mdp time study component right hand side follows jq jt jj max max max jq denote maximum norm 
largest time state action pair kq updated decompose triangle inequality jq kq 


oal algorithm state action pairs updated minimal number samplings increases 
state action pair updated times including time denote moment minimal number samplings attained bn clearly bn bn 
jq kq bn max 
max 
consider component right hand side 
similar discussion convergence 
follows law iterated logarithm number samplings bn 
convergence rate asymptotically log bn 
converges slower components right hand side 
get results lemma 
lemma lemma easy prove 
lemma consider team markov game 
event oal algorithm state 
decreases monotonically zero lim lim log log lim rf 
proof 
event max jq state initial value bound 
know lim log log cb positive constant lemma time rf 
denote set optimal joint actions 
aj arg max arg max 

jq know converging 
time cb 
max 
probability larger starting joint actions cb 

max 
max 
action cb max 
intuitively means suboptimal joint actions optimal joint actions 
implies time rf lemma states criterion including joint action optimal joint actions oal strict quickly quicker iterated logarithm agents identify optimal joint actions probability 
case set correct virtual game 
easy oal satisfy condition 
example function satis es condition main convergence theorem learning game structure coordination simultaneously ready prove oal converges optimal nash equilibrium team markov game game structure unknown 
idea show oal agents learn game structure vgs optimal coordination policy vgs 
oal tackles learning problems simultaneously speci cally interleaves bap glie exploration learning game structure 
convergence proof fact 
proof proceeds showing vgs learned rst coordination second learning algorithm know switch occurs occur 
theorem optimal convergence team markov game agents satis es lemma oal algorithm converges optimal nash equilibrium 
proof 
team markov game decomposed sequence state games 
optimal equilibria state games form optimal policy game 
de nition glie exploration state nite state space visited nitely 
sucient prove oal algorithm converge optimal policy individual state games 
event state positive constant 
condition theorem satis ed lemma exists time rf 
occurs condition theorem satis ed theorem oal converge strict nash equilibrium nash equilibrium biased set 
furthermore lemma know biased set contains optimal nash equilibria strict nash equilibria outside biased set 
occurs oal converges optimal nash equilibrium 
positive constant event agents play optimal joint action state notation previous sentence exists time rf put exists time rf rf gp rf proof parameters oal algorithm choose arbitrarily small 
oal converges optimal nash equilibrium 
easy set parameters oal convergence conditions met 
parameters try identify game rst learned try learning technique learn coordination 
approach su ers knowing game identi ed 
know switch second learning task 
theorem requires lv 
condition main theorem satis ed lemma lv 
set 
positive value guarantee convergence di erent values may lead di erent convergence speed 
example value example works log experimental results experiments demonstrate performance oal practice 
expected value boltzmann exploration exploration policy 
agent chooses action probability ep te ep te ep expected payo de ned oal algorithm section temperature parameter decreases slowly number rounds game played 
glie exploration policy 
oal jal temperature parameter initialized decreased time formula oal oal vs jal rst experiment compared oal algorithm claus boutilier joint action learner jal algorithm 
game player game table players know game structure 
oal converged optimal nash equilibrium runs 
jal converged suboptimal nash equilibrium times optimal nash equilibrium rest 
compare algorithms face noise conducted experiment payo joint action uniformly distributed value game matrix 
oal converged optimal nash equilibrium runs 
jal converged suboptimal nash equilibrium times optimal nash equilibrium rest 
impact convergence speed experiment studied impact oal parameters convergence speed game noise model 
agents know game structure advance 
algorithm converges slowly 
agents action selection subject noise agents deliberately explore suboptimal joint actions small gives agents high probability sample error containing memory bad decisions 
experiment exists exploration action number rounds impact convergence speed 
results averaged runs 
agent memory agent chance choosing making decision solely 
xed convergence slower increases 
large agent keeps information increases agent chance making decision old memory 
small preferred 
seen gure fastest convergence achieved 
interestingly parameter combination violates condition main convergence theorem theorem 
suggests condition necessary vgs 
necessity incomplete sampling analysis experiment show drop condition convergence theorem general 
highlight importance incomplete memory sampling key idea oal 
rst show analytically exploration oal complete sampling converge general game structure known 
table game bap complete memory sampling converge 
game table bap uses complete sampling agents start bap suboptimal joint action fa coordinate optimal joint action 
fact uncovered series player team games property players actions joint actions fai suboptimal give payo optimal give payo 
game players memory initial history consisting di erent suboptimal joint actions trapped loop composed suboptimal joint actions 
call games mw show experimentally incomplete sampling necessary bap exploration 
shows experiment game bap players know game structure 
game players actions ai bi joint actions fa fa fa give payo joint actions give payo 
complete sampling memory conducted trials 
runs agents coordinate actions loop composed suboptimal joint actions 
illustrates frequency suboptimal joint actions averaged past rounds 
number rounds joint action joint action joint action oal complete sampling 
results averaged trials 
satis ed condition setting agents optimal joint action trials 
converged trials condition slightly violated 
research required investigate condition necessary general 
necessity biased action selection experiment demonstrates necessity biased action selection key idea oal 
experiment game shown table 
experiments shows biased ap glie converge simple game structure known 
number rounds frequency joint action rounds adaptive play glie biased adaptive play glie frequency suboptimal joint actions 
results averaged trials 
research multiagent rl involves interrelated learning problems identifying game learning play 
problems prevail team games agents interests con ict 
team games multiple nash equilibria optimal 
optimal adaptive learning oal rst algorithm converges optimal nash equilibrium team markov game 
oal agents shape individual state games team markov game sequence virtual games coordinate actions vgs biased adaptive play 
proved play converges optimal nash equilibrium team markov game 
simultaneously appropriately decreasing bound agents learn structure virtual games surely 
rl algorithm oal combines techniques assured converge optimal joint policy 
oal uses model approach learn game structure 
plan consider model free approaches learning 
research thrust extend algorithm general sum markov games 
boutilier 
planning learning coordination multi agent decision processes 
tark 
claus boutilier 
dynamics reinforcement learning cooperative multi agent systems 
aaai 
fudenberg levine 
theory learning games 
mit press 
madsen 
markov chain theory applications 
john wiley sons 
wei learning coordinate actions multi agent systems 
ijcai 
hu wellman :10.1.1.138.2589
multiagent reinforcement learning theoretical framework algorithm 
icml 
kaelbling littman moore 
reinforcement learning survey 
jair 
littman 
friend foe learning general sum game 
icml 
littman 
value function reinforcement learning markov games 
cognitive system research 

markov decision processes discrete stochastic dynamic programming 
john wiley 
tan 
multi agent reinforcement learning independent vs cooperative agents 
icml 
howard 
dynamic programming markov processes 
mit press 
selten 
eines mit 
zeitschrift ur die 
singh jaakkola littman 
convergence results single step policy reinforcement learning algorithms 
machine learning 
sen hale 
learning coordinate sharing information 
aaai 

optimality equilibrium stochastic games 
centrum voor wiskunde en informatica 
sandholm crites 
learning iterated prisoner dilemma 
biosystems 
young 
evolution conventions 
econometrica 
