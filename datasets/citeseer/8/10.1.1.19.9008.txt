infinite mixtures gaussian process experts carl edward rasmussen zoubin ghahramani gatsby computational neuroscience unit university college london queen square london wc ar england edward zoubin gatsby ucl ac uk www gatsby ucl ac uk extension mixture experts model individual experts gaussian process gp regression models 
input dependent adaptation dirichlet process implement gating network infinite number experts 
inference model may done efficiently markov chain relying gibbs sampling 
model allows effective covariance function vary inputs may handle large datasets potentially overcoming biggest hurdles gp models 
simulations show viability approach 
gaussian processes williams rasmussen proven powerful tool regression 
combine flexibility able model arbitrary smooth functions data simplicity bayesian specification requires inference small number readily interpretable hyperparameters length scales function varies different dimensions contributions signal noise variance data gps suffer important limitations 
inference requires inversion covariance matrix number training data points computationally impractical large datasets 
second covariance function commonly assumed stationary limiting modeling flexibility 
example noise variance different different parts input space function discontinuity stationary covariance function adequate 
goldberg discussed case input dependent noise variance 
attempts aimed approximate inference gp models williams seeger smola bartlett 
methods selecting projection covariance matrix smaller subspace subset data points reducing computational complexity 
attempts deriving complex covariance functions gibbs difficult decide priori covariance function sufficient complexity guarantees positive definiteness 
simultaneously address problem computational complexity deficiencies covariance functions divide conquer strategy inspired mixture experts architecture jacobs 
model input space probabilistically divided gating network regions specific separate experts predictions 
gp models experts gain double advantage computation expert cubic number data point region entire dataset gp expert may learn different characteristics function lengths scales noise variances 
course learning experts gating network intimately coupled 
unfortunately may practically statistically difficult infer appropriate number experts particular dataset 
current sidestep difficult problem infinite number experts employing gating network related dirichlet process specify spatially varying dirichlet process 
infinite number experts may cases faithful prior expectations complex real word datasets 
integrating posterior distribution parameters carried markov chain monte carlo approach 
tresp alternative approach mixtures gps 
approach experts gating network implemented gps gating network softmax gps 
new model avoids limitations previous approach covered depth discussion 
infinite gp mixtures traditional likelihood apply experts non parametric 
normal model data assumed iid model parameters yjx jc jjx inputs outputs boldface denotes vectors parameters expert parameters gating network discrete indicator variables assigning data points experts 
iid assumption contrary gp models solely model dependencies joint distribution hyperparameters 
joint distribution corresponding possible assignment data points experts likelihood sum exponentially assignments yjx cjx fy jg cjx configuration distribution factors product experts joint gaussian distribution data points assigned expert 
original formulation expectations assignment variables called responsibilities inadequate inference mixture gp experts 
consequently directly represent indicators gibbs sample capture dependencies 
gibbs sampling need posterior conditional distribution indicator remaining indicators data jjc jjc denotes indicators number defer discussion second term defining gating network section 
discussed term likelihood indicators factors independent terms expert 
gibbs sampling need probability output gp number jg fx jg gp model conditional density known gaussian williams rasmussen jy covariance matrix depends parameters 
gp expert compute conditional density simply evaluating gp data assigned 
equation looks computationally expensive keep track inverse covariance matrices reuse consecutive gibbs updates performing rank updates gibbs sampling changes indicator time 
free choose valid covariance function experts 
simulations employed gaussian covariance function exp id hyperparameters controlling signal variance controlling noise variance controlling length scale inverse relevance th dimension relation predicting kronecker delta function 

gating network gating network assigns probability different experts entirely input 
derive gating network dirichlet process defined limit dirichlet distribution number classes tends infinity 
standard dirichlet process input dependent modify serve gating mechanism 
start symmetric dirichlet distribution proportions dirichlet positive concentration parameter 
shown rasmussen conditional probability single indicator integrating variables letting tend infinity components jjc components combined occupation number expert excluding observation total number data points 
shows probabilities proportional occupation numbers 
gating network input dependent simply employ local estimate occupation number kernel classifier delta function selects data points assigned class kernel function parametrized 
example gaussian kernel function exp id local estimate won generally integer doesn adverse consequences parameterized length scales dimension 
length scales allow dimensions space relevant gating network classification 
gibbs sample indicator variables multiplying input dependent dirichlet process prior eq 
gp conditional density eq 

gibbs sampling infinite model requires indicator variables take values indicator variable taken creating new experts 
auxiliary variable approach neal algorithm 
approach hyperparameters new experts sampled prior likelihood evaluated 
requires finding likelihood gaussian process data 
fortunately covariance function eq 
likelihood gaussian zero mean variance data points assigned single gp likelihood calculation cubic number data points gibbs sweep indicators 
reduce computational complexity introducing constraint gp expert nmax data points assigned 
easily implemented modifying conditionals gibbs sampler 
hyperparameter controls prior probability assigning data point new expert influences total number experts model data 
rasmussen give vague inverse gamma prior sample posterior adaptive rejection sampling ars gilks wild 
allowing vary gives model freedom infer number gps particular dataset 
need inference parameters gating function 
set indicator variables standard methods kernel classification optimize kernel widths different directions 
methods typically optimize leave oneout pseudo likelihood product conditionals computing likelihood model defined purely conditional distributions eq 
generally difficult pointed discussion section may single likelihood model multiply pseudo likelihood vague prior sample resulting pseudo posterior 
algorithm individual gp experts stationary gaussian covariance function single length scale dimension signal variance noise variance dimension input hyperparameters expert eq 

signal noise variances inverse gamma priors hyper separately variances 
serves couple hyperparameters experts allows priors evaluating auxiliary classes adapt 
give vague independent log normal priors lenght scale 
algorithm learning infinite mixture gp experts consists steps 
initialize indicator variables single value values individual gps kept small computational reasons 

gibbs sampling sweep indicators 

hybrid monte carlo hmc duane hyperparameters gp covariance function expert turn 
leapfrog iterations stepsize small rejections rare 

optimize hyper variance parameters 

sample dirichlet process concentration parameter ars 
simply set conditional probability joining class deemed full zero 
time ms acceleration stationary gp time ms acceleration left hand plot shows motorcycle impact data points median model predictive distribution comparison mean stationary covariance gp model optimized hyperparameters 
right hand plot show samples posterior distribution noise free function evaluated intervals ms jittered points plot time dimension adding uniform ms noise density seen easily 
std error confidence interval noise free function predicted stationary gp plotted thin lines 

sample gating kernel widths metropolis method sample pseudo posterior gaussian proposal fit current 
repeat markov chain adequately sampled posterior 
simulations simple real world data set illustrate algorithm motorcycle dataset fig 
discussed silverman 
dataset obviously non stationary input dependent noise 
noticed raw data discretized bins size accordingly cut prior noise variance 
model able capture general shape function input dependent nature noise fig 

seen right hand plot fig 
uncertainty function low ms owing small inferred noise level region 
comparison predictions stationary gp superimposed fig 

medians predictive distributions agree large extent left hand plot see huge difference predictive distributions right hand 
gp capture tight distribution ms offered 
large ms model predicts fairly high probability signal close zero 
note predictive distribution function multimodal example time ms multimodal predictive distributions principle obtained ordinary gp integrating hyperparameters mixture gp model arise naturally 
predictive distribution function appears significant mass somewhat artifactual 
explicitly normalize center data large range output 
gaussian fit uses derivative hessian log posterior wrt log length scales 
asymmetric proposal acceptance probabilities modified accordingly 
scheme advantage containing tunable parameters dimension large may computationally efficient hmc avoid calculation hessian 
number occupied experts frequency left hand plot shows number times samples indicator variables data points equal 
data sorted left value time variable data equally spaced time axis matrix aligned plot fig 
right hand plot shows histogram samples number gp experts model data 
gaussian processes zero mean priori coupled concentration data zero may explain posterior mass zero 
natural treat gp means separate hyperparameters controlled hyper hyperparameter centered zero inference fix 
scatter data predictive distribution looks somewhat messy multimodality important note assigns high density regions probable 
motorcycle data appears roughly regions flat low noise region followed curved region flat high noise region 
intuition bourne model 
see ways 
fig 
left shows number times data points assigned expert 
clearly defined block captures initial flat region points lie near line middle block captures curved region gradual transition flat region 
histogram number gp experts shows posterior distribution number needed gps broad peak occupied experts progressively 
note uses just single gp model data accords intuition single stationary covariance function inadequate 
point model trying model selection finite gp mixtures assumes infinitely available contribute small mass diffuse density background 
assessed convergence rate markov chain plotting autocorrelation function parameters 
conclude mixing time iterations consequently run chain total iterations discarding initial burn keeping th 
total computation time hour ghz pentium 
right hand panel shows distribution gating function kernel width total mass non represented experts posterior experiment peaked see bottom right panel corresponding total mass sum auto correlation coefficients estimate mixing time time lag iterations auto correlation coefficient log number occupied experts log gating kernel width log dirichlet concentration log base gating function kernel width frequency log base dirichlet process concentration frequency left hand plot shows auto correlation various parameters model iterations 
right hand plots show distribution log kernel width log dirichlet concentration parameter samples posterior 
concentration parameter dirichlet process 
posterior kernel width lies comparing scale inputs quite short distances corresponding rapid transitions experts opposed lengthy intervals multiple active experts 
corresponds visual impression data 
discussion return tresp 
ways infinite mixture gp experts differs believe improves model tresp 
model gating network divides input space gp expert predicts basis data 
data assigned gp expert spill predictions gp lead bias near region boundaries especially experts long length scales 
second experts tresp model gps experts noise models separate gating functions requires computations entire dataset resulting mn computations 
model experts divide data points experts equally dividing data iteration takes computations gibbs updates requires rank computation experts hybrid monte carlo takes times 
modest significant saving 
inference gating length scale parameters full hessian reduced diagonal approximation hybrid monte carlo input dimension large 
third going dirichlet process infinite limit allow model infer number components required capture data 
model gp hyperparameters fixed inferred data 
defined gating network prior implicitly terms conditional distribution indicator variable indicator variables 
specifically distribution indicator variable input dependent dirichlet process counts local estimates data density class eq 

able prove conditional distributions consistent single joint distribution comparison vague prior kernel width log normal mass corresponding short sub sample distances upto distances comparable entire input range indicators 
exist single consistent joint distribution gibbs sampler may converge different distributions depending order sampling 
encouraged preliminary results obtained motorcycle data 
include empirical comparisons state art regression methods multidimensional benchmark datasets 
argued single iterations mcmc inference computationally tractable large data sets experiments show mixing sufficiently rapid allow practical application 
hope extra flexibility effective covariance function turn improve performance 
automatic choice number experts may model advantageous practical modeling tasks 
wish come back modeling philosophy underlies 
computational problem doing inference prediction gaussian processes arises unrealistic assumption single covariance function captures behavior data entire range 
leads cumbersome matrix inversion entire data set 
find making realistic assumption data modeled infinite mixture local gaussian processes computational problem decomposes smaller matrix inversions 
gibbs 

bayesian gaussian processes regression classification 
phd thesis 
university cambridge 
goldberg williams bishop 

regression noise nips 
duane kennedy 

hybrid monte carlo physics letters vol 
pp 

gilks wild 

adaptive rejection sampling gibbs sampling 
applied statistics 
jacobs jordan nowlan hinton 

adaptive mixture local experts 
neural computation vol pp 
neal 

markov chain sampling methods dirichlet process mixture models 
technical report department statistics university toronto 
www cs toronto edu radford html 
rasmussen 

infinite gaussian mixture model nips solla leen 
muller eds pp 
mit press 
silverman 

aspects spline smoothing approach non parametric regression curve fitting 
royal stat 
society 
ser 
vol 
pp 

smola bartlett 

sparse greedy gaussian process regression nips 
tresp 

mixtures gaussian process nips 
williams seeger 

nystrom method speed kernel machines nips 
williams rasmussen 
gaussian processes regression touretzky mozer hasselmo editors nips mit press 
