discriminative training methods hidden markov models theory experiments perceptron algorithms michael collins labs research florham park new jersey 
research att com describe new algorithms training tagging models alternative maximum entropy models conditional random elds crfs 
algorithms rely viterbi decoding training examples combined simple additive updates 
describe theory justifying algorithms modi cation proof convergence perceptron algorithm classi cation problems 
give experimental results part speech tagging base noun phrase chunking cases showing improvements results maximum entropy tagger 
maximum entropy models justi ably popular choice tagging problems natural language processing example see ratnaparkhi part speech tagging mccallum faq segmentation task :10.1.1.116.2034
models advantage quite exible features incorporated model 
theoretical experimental results la erty highlighted problems parameter estimation method models 
response problems describe alternative parameter estimation methods conditional markov random fields crfs 
la erty give experimental results suggesting crfs perform signi cantly better models 
describe parameter estimation algorithms natural alternatives crfs 
algorithms perceptron algorithm rosenblatt voted averaged versions perceptron described freund schapire 
algorithms shown freund schapire competitive modern learning algorithms support vector machines previously applied mainly classi cation tasks entirely clear algorithms carried nlp tasks tagging parsing 
describes variants perceptron algorithm tagging problems 
algorithms rely viterbi decoding training examples combined simple additive updates 
describe theory justifying algorithm modi cation proof convergence perceptron algorithm classi cation problems 
give experimental results part speech tagging base noun phrase chunking cases showing improvements results maximum entropy tagger relative reduction error pos tagging relative reduction error np chunking 
concentrate tagging problems theoretical framework algorithm described section applicable wide variety models viterbi style algorithms decoding examples probabilistic context free grammars models parsing 
see collins du collins du collins applications voted perceptron nlp problems 
parameter estimation hmm taggers section motivating example describe special case algorithm algorithm applied trigram tagger 
trigram hmm tagger trigram theorems section proofs section apply directly papers 
tags tag word pair associated parameters 
write parameter associated trigram hx zi parameter associated tag word pair common approach take parameters estimates conditional probabilities log log 
convenience shorthand sequence words shorthand taq sequence 
trigram tagger score tagged sequence paired word sequence parameters conditional probabilities score estimate log joint probability 
viterbi algorithm nd highest scoring tagged sequence score 
alternative maximum likelihood parameter estimates propose estimation algorithm 
say training set consists tagged sentences th sentence length write examples training algorithm follows choose parameter de ning number iterations training set 
initially set parameters zero 
viterbi algorithm nd best tagged sequence sentence current parameter settings call tagged sequence tag trigram hx zi seen times times set tag word pair ht wi seen times times set example say th tagged sentence training data man saw dog current parameter settings highest scoring tag sequence take special null tag symbols 
usually chosen tuning development set 
man saw dog parameter update add parameters saw subtract parameters saw intuitively effect increasing parameter values features missing proposed sequence parameter values incorrect features sequence note proposed tag sequence correct changes parameter values 
local global feature vectors describe generalize algorithm general representations tagged sequences 
section describe representations commonly maximum entropy models tagging 
maximum entropy taggers ratnaparkhi mccallum tagging problem decomposed sequence decisions tagging problem left right fashion :10.1.1.116.2034
point history context tagging decision task predict tag history 
formally history tuple ht ii previous tags array specifying words input sentence index word tagged 
denote set possible histories 
maximum entropy models represent tagging task feature vector representation history tag pairs 
feature vector representation function maps history tag pair dimensional feature vector 
component arbitrary function 
common see ratnaparkhi feature indicator function 
example feature current word dt similar features de ned word tag pair seen training data 
feature type track trigrams tags example ht ti hd vi 
similar features de ned trigrams tags seen training 
real advantage models comes freedom de ning features example ratnaparkhi mccallum describe feature sets dicult incorporate generative model :10.1.1.116.2034
addition feature vector representations history tag pairs nd convenient de ne feature vectors pairs sequence words entire tag sequence 
denote function pairs ddimensional feature vectors 
refer global representation contrast local representation 
particular global representations considered simple functions local representations ht ii 
global feature simply value local representation summed history tag pairs 
local features indicator functions global features typically counts 
example de ned number times seen tagged dt pair sequences 
maximum entropy taggers maximum entropy taggers feature vectors parameter vector de ne conditional probability distribution tags history log probability form log log log probability pair log ht ii 
parameter values input sentence highest probability tagged sequence formula eq 
eciently viterbi algorithm 
parameter vector estimated training set sentence tagged sequence pairs 
maximum likelihood parameter values estimated generalized iterative scaling ratnaparkhi gradient descent methods 
cases may preferable apply bayesian approach includes prior parameter values 
new estimation method describe alternative method estimating parameters model 
sequence words sequence part speech tags take score tagged sequence ht ii 
note identical eq 
local normalization terms log 
method assigning scores tagged sequences highest scoring sequence tags input sentence viterbi algorithm 
identical decoding algorithm maximum entropy taggers di erence local normalization terms need calculated 
propose training algorithm gure 
algorithm takes passes training sample 
parameters initially set zero 
sentence turn decoded current parameter settings 
highest scoring sequence current model correct parameters updated simple additive fashion 
note local features indicator functions global features counts 
case update add parameter number times th feature occurred correct tag sequence number times inputs training set tagged sentences parameter specifying number iterations training set 
local representation function maps history tag pairs dimensional feature vectors 
global representation de ned eq 

initialization set parameter vector 
algorithm viterbi algorithm nd output model th training sentence current parameter settings arg max set tag sequences length update parameters output parameter vector training algorithm tagging 
occurs highest scoring sequence current model 
example features indicator functions tracking trigrams word tag pairs training algorithm identical section 
averaging parameters simple re nement algorithm gure called averaged parameters method 
de ne value th parameter th training example processed pass training data 
averaged parameters de ned nt simple modify algorithm store additional set parameters 
experiments section show averaged parameters perform signi cantly better nal parameters theory section gives justi cation averaging method 
theory justifying algorithm section give general algorithm problems tagging parsing give theorems justifying algorithm 
show tagging algorithm gure special case algorithm 
convergence theorems perceptron applied classi cation problems appear freund schapire results section proofs section show classi cation results inputs training examples initialization set algorithm calculate arg max gen 
output parameters variant perceptron algorithm 
carried problems tagging 
task learn mapping inputs outputs example set sentences set possible tag sequences 
assume training examples function gen enumerates set candidates gen input representation mapping feature vector parameter vector components gen de ne mapping input output arg max gen 

inner product 
learning task set parameter values training examples evidence 
tagging problem section mapped setting follows training examples sentence pairs set possible tags de ne gen function gen maps input sentence set tag sequences length representation de ned local feature vectors history tag pair 
see eq 
shows algorithm setting weights veri ed training algorithm taggers gure special case algorithm de ne gen just described 
give rst theorem regarding convergence algorithm 
theorem describes conditions algorithm gure converges 
need de nition de nition gen gen fy words gen set incorrect candidates example say training sequence separable margin exists vector jjujj gen 

jjujj norm jjujj pp state theorem see section proof theorem training sequence separable margin perceptron algorithm gure number mistakes constant gen jj jj theorem implies parameter vector zero errors training set nite number iterations training algorithm converged parameter values zero training error 
crucial point number mistakes independent number candidates example size gen depending separation training data separation de ned 
important nlp problems gen exponential size inputs 
convergence generalization results depend notions separability size gen questions come mind 
guarantees algorithm training data separable 
second performance training sample guarantee algorithm generalizes newly drawn test examples 
freund schapire discuss theory extended deal questions 
sections describe results applied algorithms 
theory inseparable data section give bounds apply data separable 
need de nition de nition sequence pair de ne 
max gen 
maxf de ne du pp value du measure close separating training data margin du vector separates data margin separates examples margin examples incorrectly tagged margin du take relatively small value 
theorem applies see section proof theorem training sequence rst pass training set perceptron algorithm gure number mistakes min du constant gen jj jj min taken jjujj 
theorem implies training data close separable margin exists du relatively small algorithm small number mistakes 
theorem shows perceptron algorithm robust training data examples dif cult impossible tag correctly 
generalization results theorems give results bounding number errors training samples question really interested concerns guarantees method generalizes new test examples 
fortunately theoretical results suggesting perceptron algorithm relatively small number mistakes training sample generalize new examples 
section describes results originally appeared freund schapire derived directly results helmbold warmuth 
de ne modi cation perceptron algorithm voted perceptron 
consider rst pass perceptron algorithm build sequence parameter settings test example de ne output arg max gen 

voted perceptron takes frequently occurring output set fv voted perceptron method parameter settings get single vote output majority wins 
averaged algorithm section considered approximation voted method advantage single decoding averaged parameters performed parameter settings 
analyzing voted perceptron assumption unknown distribution set training test examples drawn independently identically distributed distribution 
corollary freund schapire states theorem freund schapire assume examples generated random 
xn yn sequence training examples xn yn test example 
probability choice examples voted perceptron algorithm predict yn input xn en min du en expected value taken examples du de ned min taken jjujj 
experiments data sets ran experiments data sets part ofspeech tagging penn wall street journal treebank marcus base nounphrase recognition data sets originally introduced ramshaw marcus 
case training development test set 
part speech tagging training set sections treebank development set sections nal test set sections 
np chunking training set current word previous word word back word word ahead bigram features current tag previous tag tag back tag tag ahead bigram tag features trigram tag features feature templates np chunking experiments 
current word wn entire sentence 
pos tag current word pn pos sequence sentence 
chunking tag assigned th word 
taken section development set section test set section 
pos tagging report percentage correct tags test set 
chunking report measure recovering bracketings corresponding base np chunks 
features pos tagging identical features ratnaparkhi di erence rare word distinction table ratnaparkhi spelling features included words training data word feature regardless word rare 
feature set takes account previous tag previous pairs tags history word tagged spelling features words tagged various features words surrounding word tagged 
chunking experiments input sentences included words parts speech words tagger brill 
table shows features experiments 
chunking problem represented task tags words chunk continuing chunk outside chunk respectively 
chunks symbol regardless previous word tagged np chunking results method measure perc avg cc perc cc perc avg cc perc cc cc cc pos tagging results method error rate perc avg cc perc cc perc avg cc perc cc cc cc results various methods part ofspeech tagging chunking tasks development data 
scores error percentages 
number training iterations best score achieved 
perc perceptron algorithm maximum entropy method 
avg perceptron averaged parameter vectors 
cc means features occurring times training included cc means features training included 
results applied maximum entropy models perceptron algorithm tagging problems 
tested variants algorithm development set gain understanding algorithms performance varied various parameter settings allow optimization free parameters comparison nal test set fair 
methods tried algorithms feature count cut set ran experiments features training data included features occurring times included ratnaparkhi uses count cut 
perceptron algorithm number iterations training set varied method tested averaged parameter vectors de ned section variety values 
maximum entropy model number iterations training generalized iterative scaling varied 
shows results development data tasks 
trends fairly clear averaging improves results signi cantly perceptron method including features imposing count cut 
contrast models performance su ers features included 
best perceptron con guration gives improvements maximum entropy models cases improvement measure chunking reduction error rate pos tagging 
looking results di erent numbers iterations development data averaging improves best result gives greater stability tagger variant greater variance scores 
nal test perceptron taggers applied test sets optimal parameter settings development data 
pos tagging perceptron algorithm gave error compared error maximum entropy model relative reduction error 
np chunking perceptron algorithm achieves measure contrast measure model relative reduction error 
proofs theorems section gives proofs theorems 
proofs adapted proofs classi cation case freund schapire 
proof theorem weights th mistake 
follows 
suppose th mistake th example 
take output proposed example argmax gen 
follows algorithm updates 
take inner products sides vector 




inequality follows property assumed eq 


follows induction 

jjujj jj jj follows jj jj derive upper bound jj jj jj jj jj jj jj jj 
jj jj inequality follows jj jj assumption 
highest scoring candidate parameters follows induction jj jj kr combining bounds jj jj jj jj kr gives result jj jj kr proof theorem transform representation new representation follows 
de ne 
de ne 

parameter greater 
say pair corresponding values de ned 
de ne modi ed parameter vector 
de nitions veri ed gen 

gen jj jj jj jjujj 

seen vector jj separates data margin theorem means rst pass perceptron algorithm representation max 

mistakes 
rst pass original algorithm representation identical rst pass algorithm representation parameter weights additional features ect single example training data ect classi cation test data examples 
original perceptron algorithm max 
mistakes rst pass training data 
minimize max 
respect 
giving 
max implying bound theorem 
described new algorithms tagging performance guarantees depend notion separability training data examples 
generic algorithm gure theorems describing convergence properties applied models nlp literature 
example weighted context free grammar conceptualized way de ning gen weights generative models pcfgs trained method 
nigel du rob schapire yoram singer useful discussions regarding algorithms fernando pereira pointers np chunking data set suggestions regarding features experiments 
brill 

transformation error driven learning natural language processing case study part speech tagging 
computational linguistics 
collins du 

convolution kernels natural language 
proceedings neural information processing systems nips 
collins du 

new ranking algorithms parsing tagging kernels discrete structures voted perceptron 
proceedings acl 
collins 

ranking algorithms named entity extraction boosting voted perceptron 
proceedings acl 
freund schapire 

large margin classi cation perceptron algorithm 
machine learning 
helmbold warmuth weak learning 
journal computer system sciences june 
la erty mccallum pereira 

conditional random elds probabilistic models segmenting labeling sequence data 
proceedings icml 
mccallum freitag pereira 
maximum entropy markov models information extraction segmentation 
proceedings icml 
marcus santorini marcinkiewicz 

building large annotated corpus english penn treebank 
computational linguistics 
ramshaw marcus 

text chunking transformation learning 
proceedings third acl workshop large corpora association computational linguistics 
ratnaparkhi 

maximum entropy part ofspeech tagger 
proceedings empirical methods natural language processing conference 
rosenblatt 
perceptron probabilistic model information storage organization brain 
psychological review 
reprinted neurocomputing mit press 
