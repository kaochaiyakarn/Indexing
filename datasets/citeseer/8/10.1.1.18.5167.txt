variational extensions em multinomial pca wray buntine helsinki inst 
information technology box fin hut finland wray buntine hiit fi www hiit fi wray buntine 
authors years proposed discrete analogues principle component analysis intended handle discrete positive data instance suited analyzing sets documents 
methods include non negative matrix factorization probabilistic latent semantic analysis latent dirichlet allocation 
review basic theory variational extension expectationmaximization algorithm presents discrete component nding algorithms light 
experiments conducted bigram word data document bag word expose subtleties new class algorithms 
ecml 
text image analysis standard clustering algorithms unsatisfactory documents images mix additively contrast mutually exclusive mixture semantics standard clustering 
principle components analysis pca unsatisfactory multivariate gaussian interpretation dicult justify low discrete counts words documents comes dicult interpret components negative quantities 
cousin latent semantic indexing lsi interpretation problems due gaussian nature :10.1.1.1.4458
authors proposed analogues pca intended handle discrete positive data 
methods include non negative matrix factorization nmf probabilistic latent semantic analysis plsi latent dirichlet allocation lda general purpose extension pca bregman distances generalization kullback leibler kl divergence :10.1.1.110.4050:10.1.1.1.4458
discussion motivation techniques analysis related reduced dimension models earlier statistical literature simpler algorithms :10.1.1.1.4458
related models dubbed dirichlet mixtures applied extensively molecular biology 
common problem earlier formulations discrete component analysis fail full probability model target data question model hidden variables observed data assumptions clearly exposed 
relationship lda remains wray buntine unclear 
problem multinomial analogue pca standard pca spectral analysis come rescue yield simple solution 
usual statistical machinery mixture distributions needs wheeled applied 
reviews em algorithm variational extension applies multinomial pca 
problem document components consider tipping representation standard pca 
resultant algorithm simple application numerical methods nd largest eigenvectors 
span represents projection data containing variance 
hidden variable sampled dimensional gaussian noise 
entry represents strength corresponding component positive negative 
folded matrix component means mean dimensional gaussian 
documents represented bag words represent number words application dictionary expected considerably larger gaussian gaussian satellite telescope images instance analyzed pure high resolution positive pixel elements added form lower resolution pixel convex components suitable model pixel types 
likewise poisson distribution appropriate small counts seen telescope image data 
note total sum set independent poisson variables known joint distribution multinomial 
multinomial basic discrete distribution 
discrete analogue gaussian formulation rst sample probability vector represents proportional weighting components mix set probability vectors representing component means dirichlet entropic multinomial total number words document 
varying distribution model di erent behaviors 
dirichlet vector dimensional parameters entropic prior extension brand exp :10.1.1.3.2158
analogue example collins generalization pca 
gaussian case folded covariance matrix leaving mixture problem directly amenable solution em algorithm simpli ed th largest eigenvectors problem :10.1.1.16.3330
multinomial case appears transformation 
sophisticated mixture modeling needed 
multinomial pca ecml background theory theory exponential family distributions kullback leibler approximations brie reviewed 
formulations ghahramani beal buntine roughly followed 
notation convention indices sums products range respectively 
usually denotes sample index dictionary word index component index word document index 
exponential family distributions general exponential family distribution takes form follows 
individual sample point vector measurements vector functions parameters dimension possibly additional constraints 
probability form exp usually abbreviate add distinguishing subscript 
notation yj fag denote expected value quantity distributed yj 
key de nitions needed ft log log mean vector dimensionality matrix covariance 
region full rank data vector redundant 
remarkable condition holds treated dual set parameters 
full rank hessian change basis 
expected fisher information distribution 
note directly derivable univariate gaussian mean standard deviation 
examples form multinomial distribution probability vector count dirichlet distribution probability vector table 
gamma function digamma function parameter vector multinomial constraint 
note dirichlet hessian full rank computing parameter vector dual done minka xed point method 
key result exponential family computing maximum posterior map values parameters sample data points 
likelihood matters called sucient statistics 
conjugate prior functional form likelihood 
way model ective prior sample sucient statistics wray buntine table 
exponential family characterizations xk log 
log rk prior sample size case unique map exponential family parameters yields estimate dual kullback leibler approximations consider distribution posterior rendered impractical due normalization marginalization problems 
approximate distribution form 
called mean eld approximation choose minimizing kl divergence distributions kl jjp log rhs replace normalizing constants ignored 
kullback leibler approximations exponential family 
suppose exponential family described section 
minimization task xed point update formula optimize flog log proof 
sketch substitute expected log term kl entropy flog log di erentiate ut note eqn 
occurs linearly expected value easy 
gaussian multinomial dual parameters ones familiar anyway 
kullback leibler approximations products 
view approximations gained looking factored distribution consider approximating distribution factors independent non overlapping components 
show applies markov bayesian network models multinomial pca ecml functional analysis non zero show minimizes kl independent components form exp flog exp flog note cyclic de ned terms visa verse terms log probability repeatedly replaced mean 
result yields rules apply 
computational methods hidden variables called hidden variables data point sample form observed data unknown hidden data special subscript denote part th data 
denote full set vectors fg fg observed data exponential family hidden data known hidden data exponential family suppose parameter sets distributions respectively yielding full set 
details examples 
full joint distribution fg fg computational methods maximizing joint fg hidden variables marginalized 
approximating map directly variational methods 
known compute maximum posterior value fg exactly 
consider function log fg kl fg jj fg fg fg log fg fg fg lines easily shown equivalent 
note represents lower bound log fg maximizing yields variational algorithm 
repeat 
minimize kl divergence 

maximize expected value 
note rst step methods just developed eqns 

second step usually done eqn 
directly expected value look log probability exponential family likelihoods having unique global maximum 
wray buntine computing map em 
exponential family rich include previous variational method em algorithm 
case kl map obtained 
instance standard clustering algorithms discrete variable discrete distribution variable 
em variational algorithm condition bound tight 
algorithms document clustering develop mixture models extending basic idea multinomial pca mixture models convenient exponential family distributions grind formula just produce algorithms 
priors document clustering model dimensional probability vector represents proportion components particular document 
represent quite wide range values mean representing general frequency components sample 
potential priors probability vector discussed literature including dirichlet prior equal parameters uniform je reys prior respectively prior sample size eqn 

hierarchical prior parameters general dirichlet estimated data :10.1.1.110.4050
entropic prior tends extinguish small components :10.1.1.3.2158
convenience assume general dirichlet specialize forms required 
general dirichlet prior proportions priors needed 
prior corresponding prior sample uniform probability size apply eqn 
compute map 
component means columns represented 
dimensional probability vectors distinct words prior important ensure low count components handled 
suitable universe words empirical prior empirical proportion words universe 
dirichlet represents small prior sample size 
likelihoods document clustering ordering relevant 
case iterate words appear document 
dictionary index th word document takes value dirichlet discrete discrete 
multinomial pca ecml hidden variables document 
turns identical likelihood case combinatoric term instance canceled log term eqn 

ordering irrelevant 
case iterate word counts document 
index runs dictionary index values full hidden data matrix single document count number times th dictionary word occurs document representing th component 
derived vectors column wise totals number words document representing th component row wise totals observed data typically stored sparse form 
denote 
th row vector 
th column vector dirichlet multinomial 
multinomial 
hidden variables document 
full likelihood single document simpli es zd wk wk note 
marginalized yielding original model proposed section 
important aspect model hidden variables remain linked likelihood mean eld approximation product distribution kl eqn 
zero em algorithm appear feasible 
multinomial pca dirichlet prior rst algorithm estimates map parameters general dirichlet prior parameters extends prior simpler handling dirichlet parameters proof optimality product approximation :10.1.1.110.4050:10.1.1.110.4050
theorem 
likelihood model section eqn 
priors dirichlet 
dirichlet 
updates converge local maximum lower bound log optimal product approximations 
subscript indicates values th document 
exp wray buntine log exponential rst equation estimate exp flog reduces component entropy 
note equations standard maps multinomial dirichlet respectively 
equation rewrites terms dual representation exponential family convention immediately inverted minka methods 
proof outlined highlights simplicity exponential family machinery section 
proof 
sketch rst step kl approximation likelihood hidden variables fg fg 
observed data row totals product approximation 

expected value log yields form independent multinomial 

yields form dirichlet optimal product distribution dirichlet 
multinomial 
parameter vectors problem eqn 
eqn 
works equally 
inspect required log probabilities eqn 
flog flog table 
eqn 
ignoring constants log log log log rewrite rules extracted left hand sides updated values 
second step algorithm re estimate model parameters optimizing expected log probability eqn 

done inspection table guide 
rearrange full log probability look posteriors dirichlet sampling multinomial sampling respectively apply eqn 
write map values 
gives rewrites theorem 
ut simpler version theorem optimize log jointly 
multinomial pca ecml theorem 
context theorem updates converge local maximum log mj 
log log proof 
sketch case hidden variables primary objective function covered eqns 

move probability terms yield modi ed formula variational optimization log probability 
case disappears xed far kl approximation concerned 
optimization occurs second step algorithm 
minimum kl divergence zero exactly modeled multinomials 
ut comparisons algorithm thm 
ignoring priors equivalent nmf algorithm lee seung nal normalization done plsi algorithm hofmann includes regularization step :10.1.1.1.4458
correspondences tricky instance hofmann reverse direction 
di erence algorithms thms 
estimation exp flog versus map estimate 
note prior thm 
changes di erent posterior compute exp flog replacement removal 
entropic prior large replacement dirichlet experiments produce quiet di erent components 
experimental results previous researchers results comparing family methods lsi tasks document recall reduced features perplexity evaluation dimensionality reduction achieved comparisons statistical models dimensionality reduction discrete data :10.1.1.110.4050:10.1.1.1.4458
experiments focus number di erent aspects methods highlighted analysis 
apply algorithms wray buntine bag words document data word bigram data turn di erent statistical properties 
note analysis bigram data viewed thesaurus discovery task include part speech information word linkage dictionary 
bigram data collected words signi cant portion english language documents google august crawl 
identifying sentence breaks dicult task html seemingly random lists words occur infrequently web pages space allow description methods 
bigram data non zero matrix top words 
top word occurrences th word charity occurrences 
frequent bigram occurrences th frequent included occurrences 
david lewis reuters collection newswires document data 
words occurring times entire collection discarded bag words representation numbers converted tokens leaving words features approximately documents 
code lines space requirements runtime size input data iteration takes 
computational requirements comparable algorithm extracting top eigenvectors usually pca 
convergence iterations depending accuracy required slower pca counterpart 
experiments reported run old linux laptop overnight 
code outputs html le navigating documents words components assignments document 
useful diagnostic measures reported follows expected words component ew conditional entropy word probability vectors components raised power 
expected components document ec entropy component probability vectors averaged documents raised power 
expected components ec entropy observed component probabilities raised power 
plots fig show change values increased 
reuters data ec stays constant plotted ec continues growing linearly 
google bigram data ec grows plotted primarily sample sizes large 
intense subsampling thinning data factor eventually ec stay constant 
newswires words data sets equally distributed components typically newswire belongs components word google bigram data belongs components depending sample size 
note ecacy priors parameters component dirichlet measured computing kl divergence expected component proportions observed component proportions 
remarkably reuters data components kl divergence prior sample size compared multinomial pca ecml component dimension reuters ew ec ew ec component dimension google bigrams ew ec ec fig 

diagnostics data runs maximum likelihood estimates blei estimates average factor times accurate prior diagnostic measures remain unchanged 
poor quality maximum likelihood estimates expected hierarchical parameters directly estimated data 
components bigram data vary di erent values small general word forms verbs adjectives 
increases break people verbs internet nouns increases components include things months measurements states democracy verbs emotions media formats body parts components aspects thing new ideas unfolding components increasing remarkable method complete contrast pca simply adds components existing top model multinomial analogue pca kind espoused collins multinomial mean convex combination multinomial components sct 

model assigns single component topic word document sct 
:10.1.1.110.4050
support name multinomial pca non negative matrix factorization better descriptive non statistical matrix analysis task probabilistic lsi clear comparison lsi fails indicate far broader applicability latent dirichlet focuses minor aspect replacing dirichlet entropic prior leaves rewrite rules unchanged :10.1.1.1.4458:10.1.1.1.4458
theorems extended algorithm blei simpli ed proof clari ed relationship earlier algorithms nmf plsi :10.1.1.110.4050
re wray buntine means clustering versus em clustering generative model di er optimization criteria 
goal analogous pca results methods depart radically pca provide rich source new opportunities analyzing discrete data 
finding greater numbers components simply add components develops re ned components completely di erent scale 
multinomial pca ideal hierarchical analysis 
current versions mpca components document due small size newswires typically components newswire 
bigram task components word kept increasing highlighting current mpca algorithms really suited dimensionality reduction form relaxed clustering allows probabilistic assignment just classes 

earlier theoretical basis research conducted nasa subcontract nas qss group june bigram modeling machine learning research consultant google peter norvig sept oct 
tipping bishop mixtures probabilistic principal component analysers 
neural computation hofmann probabilistic latent semantic indexing :10.1.1.1.4458
research development information retrieval 
lee seung learning parts objects non negative matrix factorization 
nature blei ng jordan latent dirichlet allocation :10.1.1.110.4050
nips 
appear 
collins dasgupta schapire generalization principal component analysis exponential family 
nips 
hall hofmann learning curved multinomial subfamilies natural language processing information retrieval 
icml 
karplus brown hughey krogh mian haussler dirichlet mixtures method improved detection weak signicant protein sequence homology 
computer applications biosciences brand structure learning conditional probability models entropic prior parameter extinction :10.1.1.3.2158
neural computation roweis em algorithms pca spca 
nips 
ghahramani beal propagation algorithms variational bayesian learning 
nips 
buntine computation exponential family graphical models 
unpublished nato summer school graphical models italy minka estimating dirichlet distribution 
course notes jordan ghahramani jaakkola saul variational methods graphical models 
machine learning 
