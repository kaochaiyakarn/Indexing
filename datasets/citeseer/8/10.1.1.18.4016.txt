massachusetts institute technology artificial intelligence laboratory technical report june alignment maximization mutual information paul viola publication retrieved anonymous ftp ai mit edu 
anew information theoretic approach nding 
technique require information surface properties object shape robust respect variations illumination 
derivation assumptions imaging process 
result algorithms quite general wide variety situations 
experiments demonstrate approach registering magnetic resonance images computed tomography ct images aligning complex object model real scenes including clutter occlusion sequence aligning view object model 
method formulation mutual information model image called emma 
applied technique intensity feature 
works domains edge gradient magnitude di culty robust traditional correlation 
additionally cient implementation stochastic approximation 
number additional real world applications solved reliably emma 
emma machine learning informative projections high dimensional data 
emma detect correct corruption magnetic resonance images mri 
copyright massachusetts institute report describes research done atthe arti cial intelligence laboratory massachusetts institute technology 
support laboratory arti cial intelligence research part advanced research projects agency department ce naval research contract 
paul viola supported usaf assert program parent 
alignment maximization mutual information paul viola submitted department electrical engineering science june partial ful llment ofthe requirements degree doctor philosophy 
years problems image registration recognition proven di cult pessimistic predicted 
progress hampered sheer complexity ofthe relationship object involves object shape surface properties position illumination 
changes illumination radically alter intensity 
human visual system shading recognition image interpretation 
comparing objects information explicitly insensitive illumination 
measure unique compares object models directly raw images 
pre processing required 
show mutual information model image large 
making technique reality wehave de ned concrete cient technique evaluating entropy called emma 
derivation mutual information alignment assumptions nature imaging process 
result algorithms quite general wide situations 
experiments approach aligning anumber complex object models real images 
addition demonstrate solve problems medical registration 
alignment accomplished adjusting mutual information image object maximized 
gradient descent alignment procedure stochastic approximation cient implementation 
application stochastic approximation ords speed factor gradient descent 
addition stochastic approximation accelerate avariety vision applications 
describe existing vision application accelerated stochastic approximation 
number additional real world applications solved reliably emma 
emma machine learning informative projections high dimensional data 
emma detect correct corruption magnetic resonance images mri 
thesis committee prof tomas lozano perez supervisor prof christopher atkeson supervisor prof eric grimson prof berthold horn acknowledgments iwould mit arti cial intelligence laboratory intellectual freedom 
professors tomas lozano perez christopher atkeson eric grimson rodney brooks worked build environment resource 
chris tomas supported thick thin 
taught truly valuable unconditional support 
students precious resource ted discussions phil agre geiger david chapman jose tao alter jonathan karen maja mataric ian horswill colin angle cynthia henry minsky rick barbara moore 
wells stood met mit 
unique approach vision care research advice friend proven invaluable 
owe john shewchuk brown university introducing eld statistical learning 
memories intellectual curiosity serve continually motivate thinking 
outside mit inthe computational neurobiology laboratory terrence sejnowski 
terry devoted scientist met 
lab learned science pursuit truth 
science building onthe build rst understand 
terry lab pleasure working lawrence rich zemel tony bell peter dayan 
ect part thesis 
importantly recognize technical contributions sara 
cryptic mathematics clear cryptic thinking 
thesis exist 
sara love life 
parents mary ancona viola alfredo viola making possible 
contents alignment alignment example overview thesis probability random variables entropy di erential entropy samples versus distributions model selection likelihood modeling densities gaussian density parametric densities parzen window density estimation empirical entropy manipulation stochastic gradient descent empirical entropy estimating entropy parzen densities stochastic maximization algorithm estimating covariance principal components analysis information matching alignment alignment correlation maximum likelihood technique correlation mutual information weighted neighbor likelihood vs emma non functional signals alignment derivation matching minimum description length summary alignment experiments alignment objects alignment skull model alignment head model alignment surfaces medical registration experiments dimensional ct alignment view recognition experiments photometric stereo limitations emma alignment applications emma bias compensation alignment drawings related geometrical alignment appendix gradient descent chapter thesis anew information theoretic approach standing problems computer vision image processing 
example approach nd correct alignment dimensional model 
alignment component ofthe object recognition problem useful medical military applications 
applications including image processing application anew form unsupervised learning 
form applications quite di erent underlying theory derivations similar 
preliminary investigation imply theory 
computer vision proven di cult pessimistic predicted 
problem interest years progress slow 
best computer vision systems stand contrast human visual system perception images robust computer vision systems best 
di culties progress hampered sheer complexity ofthe relationship image object involves object shape surface properties position illumination 
computer vision program faced task interpreting intensities 
information shape location objects embedded intensities actual intensities arise image di 
example changes illumination radically alter intensity 
paul viola chapter 
human visual system shading recognition image interpretation existing computer object recognition systems 
systems throw information ort obtain illumination invariance 
comparing objects information explicitly insensitive changes illumination 
measure unique compares object models directly raw images 
pre processing required 
image model comparison measure rigorously derived information theory 
theory algorithms involved new cient scheme evaluating mutual information called emma alignment procedure requires assumptions nature imaging process 
result algorithms quite general wide variety situations 
approach align number complex object models 
addition solve problems medical registration 
alignment adjusts pose object mutual information image object maximized 
pose adjustment accomplished ascending gradient mutual information 
procedure stochastic ation ords speed gradient ascent 
addition stochastic approximation vision applications 
describe existing vision application accelerated stochastic approximation 
emma proven useful number tasks alignment 
example entropy minimization correct corruption magnetic resonance images mri 
emma de ne anew form unsupervised learning 
unsupervised learning popularized neural network literature scheme simplifying representations complex data 
emma projections high dimensional input space maximally informative 
emma random subset empirical entropy manipulation analysis 

alignment ai tr alignment general problem alignment entails comparing predicted image object actual image 
object model pose coordinate transformation model imaging process predict image result 
imaging model deciding image contained particular model pose straightforward compute predicted image compare actual image directly 
perfect imaging model images identical close 
course nding correct alignment remaining challenge 
relationship object model matter accurate object image complex 
appearance small patch surface function surface properties patch orientation position lights andthe position observer 
formulate equation equivalently imaging equation separable distinct components 
rst component called transformation pose relates coordinate frame ofthe model coordinate image 
transformation tells point inthe model responsible particular point inthe image 
second component isthe imaging function 
imaging function determines value image 
general pixel value may function ofthe model exogenous factors 
example dimensional object depends lighting 
parameter collects exogenous uences asingle vector 
principle possible de image convey information model 
clearly information problem nding computing dealing mutual information directly 
algorithm aligns maximizing mutual information model image 
requires priori model relationship surface properties assumes paul viola chapter 
model tells scene correctly aligned 
alignment example ofthe alignment problems address involves nding pose threedimensional object image 
problem involves comparing di erent kinds representations dimensional model shape object video image object 
example contains video image example object left map object right question person head ron 
depth map image displays depth camera visible point onthe object model 
depth map shape object visible parts 
depth map di cult see image model aligned 
task easier simulate imaging process construct image 
contains computer graphics renderings 
synthetic images constructed assuming lambertian surface lighting comes right 
immediately obvious model left closely aligned true image model right 
unfortunately nd trivial di cult fora computer 
intensities true video image synthetic images di erent 
true image correct model image fact uncorrelated 
person glance images decide images head looking roughly direction 
human visual system capable ignoring super cial di erences arise changes illumination surface properties 
easy build automated alignment procedure kind comparison 
harder construct system nd correct model pose 
built system 
system selected model shown left 
mentioned synthetic images ron generated assumption model surface lambertian lighting right 
lambert law simplest model surface re ectivity 
accurate model re matte 
alignment ai tr di erent views ron 
image 
right isa depth ron 
depth map describes distance visible points ofthe model 
white denotes points closer black 
left computer graphics rendering model ron 
position model asthe position actual head 
right rendering ofthe head model incorrect pose 
non shiny surface 
lambert law states visible intensity surface patch related dot product surface normal lighting 
lambertian object imaging equation liu model isthe normal vector surface patch onthe object li vector pointing light andi proportional intensity light source horn contains excellent review imaging relationship vision 
paul viola chapter 
drawing explicit parallel imaging function liu fi lig 
asthe illumination changes functional relationship model image change 
know aligning model image quite di cult 
di culties compounded surface properties object understood 
example objects having lambertian surface 
di erent surface nishes di erent re functions 
general re function lighting direction surface normal viewing direction 
intensity observed patch li vector pointing observer patch andr isthe re function surface 
unknown material great necessary completely categorize re function 
general vision system avariety objects general illumination conditions overly constraining assumptions re illumination avoided 
examine relationship real image model 
allow build intuition alignment process 
data real re function obtained aligning amodel real image 
alignment associates points image points fromthe model 
alignment correct pixel image interpreted sample imaging 
imaging function displayed plotting intensity lighting direction viewing direction surface normal 
unfortunately intensity di erent parameters resulting plot prohibitively complex impossible visualize 
signi cant simpli cation necessary detect structure data 
wide variety assume light sources far object terms dimensions object 
true shadows patch ofthe 
furthermore 
alignment ai tr assume observer far object viewing direction constant image 
resulting relationship normal intensity dimensional normal vector unit length parameters xand components intensity parameter 
dimensional scatter plot normal versus intensity really slice high dimensional space de ned 
graph simpler original dimensional plots quite di cult 
slice data points asingle value component ofthe normal 
contains graph intensities asingle scan ron 
shows similar data correctly aligned model ron 
model normals scan line displayed graphs rst shows ofthe normal second shows component 
notice wehave chosen portion model component ofthe normal constant 
result relationship normal intensity visualized dimensions 
shows intensities image plotted component ofthe normal model 
notice relationship appears consistent 
points fromthe model similar surface normals similar intensities 
data inthis graph approximated 
call imaging function consistent 
interestingly need information illumination surface properties object determine consistent relationship model normal image intensity 
shows relationship normal intensity model image longer aligned 
di erence graph intensities come scan line centimeters correct alignment model longer aligned centimeters low 
normals thesame 
resulting graph longer consistent 
look simple smooth curve data 
summary model image aligned consistent relationship image intensity normal 
predicted assumption imaging function relates models images 
actual form function depends surface properties correct alignment lead consistent relationship 
conversely model image misaligned relationship paul viola chapter 
real intensity 
position left video image ron single scan line highlighted 
right graph intensities observed scan line 
intensity normal inconsistent 
contribution thesis derivation formal technique delivers principled estimate consistency 
mutual information image amodel high 
making technique reality wehave de ned new approach entropy information called emma 
de ned cient scheme adjusting set parameters mutual information entropy 
emma ectively evaluate adjust alignment dimensional models dimensional images 
technology alignment types signals 
full generality emma need align images di erent sensors called sensor fusion problem 
example medical imaging data sensor resonance imaging aligned data sensor computed tomography 
emma solve problems 
developed alignment entropy informa applications 
correct inhomogeneities mri scans 
addition approach dimensionality reduction entropy 
similar principal components analysis technique nd low dimensional projections higher dimensional data preserve information 
alignment ai tr normal comp 
normal comp 
position position left depth map ron single scan line highlighted 
top right graph component ofthe surface normal 
bottom right isthe component ofthe normal 
paul viola chapter 
intensity component normal aligned case plot intensity ofthe video image versus component ofthe surface normal model 
image model correctly aligned 
intensity component normal misaligned case left misaligned scan line video image ron 
onthe right scatter plot intensity ofthis part video image versus component ofthe surface normal model 
possible 
overview thesis second chapter contains overview probability theory necessary understand emma doing chapter discusses estimation entropy samples 
number techniques currently exist emma combines computational ciency exibility necessary model wide variety 
overview thesis ai tr distributions 
returns discussion alignment 
show emma capable aligning signals simpler techniques 
chapter basic equations underly alignment mutual information 
fth chapter contains wide variety alignment experiments designed approach scope possible application 
chapter applications alignment applied emma 
example scheme ciently manipulating entropy includes stochastic form gradient descent 
describe ow estimation problem stochastic gradient descent speeds convergence factor 
chapter include discussion results comparison related 
chapter probability entropy ofthe key insights inthis thesis ofthe techniques common computer vision correlation easily interpreted statistics random variables 
broad range tools statistics 
theory statistics converge converge importantly appropriate 
chapter introduce basic mathematics underly probability 
subsequent chapters assume reader fairly thorough knowledge prob ability statistics entropy 
chapter intended review required techniques theorems bridge reader unfamiliar topics 
nal sections chapter analysis density estimation 
parzen play entropy subsequent chapters 
simpli ed looser de nition variables typical 
get overly confused reading chapter book probability clear things papoulis 
general proofs easily looked theory cited 
unfortunately probability con icting standard notations 
de nitions consistent prevailing conventions 

random variables ai tr random variables cases algebraic model physical system allows accurately predict behavior 
instance circuit theory analyze particular circuit predict closed current ow 
physics circuits modeled equations unknown quantities recorded variables 
case switched circuit model resistance switch values zero closed nity open 
current ows resistor predicted algebraic manipulations 
conversely knowing value current allows predict closed 
equivalence circuit circuit model fundamental elds physics engineering 
systems behavior particular measurements easily predicted 
voltage wire may complex circuit thermal noise resistor 
circuit variables known predicted accurately 
luckily lost 
may know actual voltage wemay know near higher 
probability random processes random variables provide tools quantify intuitive concepts near 
random variable rv value unpredictable 
recall variable range 
example range 
inthis thesis subset 
random variable called probability distribution 
example construct rv models roll sided die 
die fair know integers appear roughly time 
describes die includes variable possible outcomes probability distribution probability take 
particular value rv called trial example die roll 
collection trials called sample 

event px isthe expect see large paul viola chapter 
probability entropy sample 
sample space probability distribution equals xi denote elements ofthe sample lower case 
cases xi xi orp xi nite discrete set values known discrete random variable 
range includes nite set continuous values known continuous random variable 
bit thought conundrum regarding continuous rv possible outcomes probability outcome zero 
fact continuing annoyance nition entropy 
distributions continuous rv probability densities px lim probability just easily de ned density probability density integrates px dx px dx px true px 
probability densities non negative arbitrarily large values 
densities manipulated way distributions 
subsequent discussion duplication de nitions theorems distributions densities 
typically probability books say done chance confusion 
know better 

random variables ai tr simple statistics random variable model process allows answer variety questions behavior process 
voltage resistor unpredictable long term average 
de ne intuitive notion long term average expected value mean rv 
expected ned ex ex xi xi xp dx notational convenience refer expectation ofx ase 
mean random variable deterministic function distribution 
average rv value large sample 
denote somewhat non standardly collection xa size refer 
abuse notation ea na xa average 
mean sample mean random variable 
law large numbers allows prove sample mean equals expectation ex lim na ea lim na na xa statistic 
statistics computed rv gross long term behavior 
statistics ofx de ned expectation functions ofx 

example average lottery number help guess lottery number 
addition knowing mean know close samples ofx mean 
paul viola chapter 
probability entropy tell average lottery variation lottery numbers huge 
measure expected variation called variance ned var ex ex ex ex square root variance standard deviation 
standard deviation measure far samples ofx 
expectation rv equal nite mean explored relation sample mean 
sample mean estimate forthe true 
quali ed sense answer 
expectation sample mean asthe expectation ea na xa na xa expectation ned integral linear moved inside summation 
sample mean called unbiased estimator true mean 
close average sample mean true mean 
assumption di erent independent distributed standard deviation sample mean ea na deviation mean approaches approaches nity 
conclude sample mean unbiased estimate forthe true mean quality ofthe estimate larger samples 
mean variance zeroth rst elements nite class moment statistics 
statistics increasing accuracy 
algebra random variables random variables useful descriptions processes occur realworld 
rv algebraic equations just variables 
value equation includes rv random process 
new rv ned 

random variables ai tr discrete rv probability distribution ofy easily de ned py px 
continuous rv quite simple py px df dx equation tells scale density large 
acts stretch 
stretching 
new theory random variables identities really hinted analyze systems noisy circuit described 
answer questions random noise voltage power supply variation current resistor side ofthe circuit 
general kind starts description distribution derives distribution functionally related rv system 
joint distributions rv andx said 
allows rv related directly predictable 
example noisy voltage source noisy current source 
measuring voltage tells current doesn tell 
arises current source 
rv completely independent 
example di erent rolls fair die considered independent 
dependency formalized examining joint distribution rv 
joint distribution tells occurrence events andy itis random behavior andy joint distribution compute marginal distributions paul viola chapter 
probability entropy variables independent considered dependent joint product marginal distributions 
closely related distribution conditional distribution yjx probability ofy 
ned yjx complete functional dependence determined conditional probability case jx isknown bayes law concluded equation xjy yjx bayes law inverts conditional probabilities 
quite useful situations conclude distribution ofx measurement ofy known isp yjx 
entropy entropy summarizes randomness 
de nition random variable mention random 
number random roll die 
entropy helps answer question 
see random variable 
additional material entropy inthe excellent textbook cover thomas cover thomas 

entropy ai tr entropy form old concept 
origins clearly date rst century 
credit de ning entropy promoting data analysis engineering falls shannon shannon 
straightforward de nition entropy expectation ex log xi log xi xi de ne log thesis 
classical de nition discrete random variables 
nition continuous entropy known di erential entropy 
entropy de ned terms logarithm base 
case entropy isgiven units 
entropy way measuring randomness compose shortest message describes trials rv 
trial fair coin takes bit information encode heads tails 
cient technique encoding single trial restriction apply describes sample trials 
coin question comes tails number straightforward schemes encoding sample require bit trial 
instance send length ofthe position zeros 
take log na length na zero 
length ips coin average length log na na log na coding scheme describing trials average take bits 
number bits dependent number events andthe distribution random variable 
discussion comparison coding schemes take quite lot space 
luckily kraft inequality proven needs communicate trial random 
furthermore shannon showed possible provided coin doesn heads 
paul viola chapter 
probability entropy construct code 
simple algorithm discovered hu man construct shortest possible codes random variable 
entropy onthe code length required transmit trial entropy called information 
conditional entropy joint entropy information concept mutual information plays critical role thesis 
problems need solve random functionally dependent 
section rv independent joint density product marginal densities see 
entropy allow quantify extent rv dependent 
quantifying dependence randomness 
total dependence implies determines knowledge removes randomness 
independence just opposite knowledge ofx help 
just joint distributions relate occurrences rv entropy relate predictability oftwo rv 
conditional entropy joint entropy de ned yjx ex ey log yjx ex ey log conditional entropy randomness ofy knowledge ofx 
note expectation di erent events ofx average just random yjx expects ifx takes particular value 
random variables considered independent yjx 
entropy ai tr dependent yjx 
conditional entropy measure dependency 
value yjx may imply dependence may imply 
mutual information mi random variables yjx reduction entropy ofy 
anumber simple logarithm equalities relations conditional joint entropy 
instance conditional entropy expressed terms marginal joint entropies yjx allows provide equivalent expression mutual information useful identity yjx xjy extremely useful inequality expectations known jensen inequality allows prove concave functionf concave second derivative 
fact logarithm function concave jensen inequality allows prove useful inequalities yjx paul viola chapter 
probability entropy di erential entropy number main theorems entropy apply discrete distributions anumber theorems change signi cantly 
continuous version entropy erential entropy ned ex log log dx part di erential entropy 
fact equalities inequalities previous section hold 
thesis entropy applicable form entropy 
di erence matters 
di erence entropy di erential entropy longer direct relationship 
possible construct examples di erential entropy 
implication take greater 
code length negative 
di erential entropy provide measure randomness 
case di erential entropy unpredictable 
examples sort constructed embedding discrete process continuous space 
example model roll die continuous rv 
density series delta functions centered points 
delta function called dirac delta function de ned box car function box car function de ned 
delta function box car function limit approaches zero width lim delta function integrates 
shown dx 
samples versus distributions ai tr furthermore de nition convolution fg dx see delta function identity operator dx density ofthe continuous model die rolling formulated integrate furthermore de ne probability lim probability ofthe events 
show entropy ofx negative nity pretty clearly random 
log dx log di erential entropy measure randomness code length provide measure properties 
random predictable 
similarly event requires bits average encode event samples versus distributions random variable mathematical structure model behavior physical process 
cases excellent physical reasons believe accurate model process 
cases properties random physical paul viola chapter 
probability entropy process may unknown 
cases wemay wish probability analyze system 
rst step nd accurate data 
order insure probabilistic inferences correct model accurate possible 
possible model coin fair coin 
sense perform large number experiments intended test hypothesis coin fair 
important intuitions nding accurate model random process 
foremost want amodel explain data 
sense coin fair ips heads come times 
model plausible 
lifetime experience realize coins pretty fair sense assume heads unusual su ciently unusual assume coin biased 
model selection likelihood provides tools testing validity models 
lot shares particular form called maximum likelihood model selection 
goal select probable model large sample measurements 
maximum likelihood selection proceeds steps guess de nition random variable model process evaluate goodness model computing probability data observed generated model evaluating models retain model data probable 
probability probability ofthe occurrence px px xa xa probability sample usually called likelihood 
justi cation maximum likelihood model selection bayes law 
likelihood sample really conditional probability ajx 
bayes law allows turn conditional model sample xja ajx 
samples versus distributions ai tr order compute model likelihood multiply sample likelihood correcting factor arbitrary sample models 
prior unconditioned probability ofthe probability ofthe model poses problems 
maximum likelihood model selection assumes models equally occured constant 
result models probable model data probable 
reliable information prior probability available bayes law directly 
technique known maximum posteriori model selection 
instance wide variety experiments may observed fair coins far common unfair coins 
implausible coin unfair impossible 
prior knowledge bias anew coin fair determine 
likelihood model prior probability determine model highest probability explaining data 
want amodel explains data plausible 
general evaluating joint random able 
practice maximum likelihood schemes assume di erent trials ofx independent 
probability occurrence product independent rvs xa px xa maximizing daunting process 
signi cant simpli cation obtained maximizing logarithm log xa xa log likelihood maximum simpler derivative 
interesting parallel log likelihood entropy 
recall entropy ofx 
nite sample average entropy empirical entropy gure paul viola chapter 
probability entropy strongly thesis conclude ha ea ha na xa log provides model selection terms entropy 
nding model data nd model lowest empirical entropy 
conversely anew interpretation entropy distribution low entropy ifthe probability sample drawn distribution high 
distribution high entropy ifthe sample low probability 
density witha narrow low entropy samples fall region density 
avery broad density high entropy samples spread fall density 
close relationship entropy known overlooked students 
parallels research maximum likelihood easily missed 
fact system manipulates entropy better di erent likelihood 
instance log likelihood model selection derived directly entropy framework cross entropy 
entropy asymmetric divergence measure di erence distributions ex log px log px log px px dx px dx log px dx ex log ea log ha cross entropy negative reaching zero andp identical 

modeling densities ai tr maximum likelihood model selection searches sample cross selection searches closest cross entropy sense true distribution 
approximation procedures fact identical 
rst term constant role model selection 
ha na times log likelihood sample drawn 
minimization cross likelihood 
modeling densities section number techniques estimating densities data 
understanding process done important prerequisite understand ing main thesis 
discussion observed continuous density gaussian density derive closed form expression gaussian sample 
section include discussion parametric density functions nally non parametric technique estimating densities known parzen window density estimation 
gaussian density random processes gaussian normal density 
literally appears 
common justi cation arises central limit theorem shows density ofthe large number independent random vari ables tend gaussian 
equally important justi cation mathematics gaussian density quite simple 
linear function gaussian gaussian systems theory 
certainly majority continuous random processes modeled gaussians 
gaussian density de ned pe sake brevity refer density simply gaussian 
paul viola chapter 
probability entropy parameters variance mean density 
demonstrate clever integration 
gaussian density de ned higher dimensions exp space mean ad vector 
variance replaced matrix ad matrix determinant 
recall variance de ned expected square di erence mean covariance somewhat complex ij xi xi xj xj th 
diagonal entries contain variances components 
diagonal entries measure expected variation 
equation de nes nite family density functions 
family determined 
model unknown density family wemust rst decide ifthe density gaussian 
maximumlikelihood model selection estimate sample 
form gaussian density nding maximumlikelihood parameters easy 
log likelihood sample gaussian density minimizes log xa xa xa log px xa log xa log xa xa xa quadratic function di erentiating zeroes xa 
modeling densities ai tr sample true gaussian fit views gaussian density sample points drawn density 
represented vertical black line 
second density ofthe true gaussian 
third density ofthe gaussian estimated sample mean variance 
satisfying result 
estimate mean mean sample 
similar argument prove maximum likelihood estimate forthe variance isthe sample variance xa displays point sample drawn gaussian density 
shown model 
sample mean sample variance perfect measures true mean variance model perfect 
accuracy estimated mean variance gets better sample size increases 
sample points signi cant variability inthe estimated model di erent samples 
shows di erent estimates di erent samples density 
paul viola chapter 
probability entropy maximum likelihood density estimates di erent samples points drawn 
parametric densities finding gaussian model sample cient operation 
mean variance trivially computable linear time 
cient estimation property shared exponential densities class densities include gaussian density 
types densities possible likelihood parameter estimates directly statistics density 
set parameters determined process 
nite number possible parameter values nding values parameters optimal straightforward 
generally problems sort solved re nement process known gradient descent 
gradient descent procedure described appendix 
gaussian density 
sample 
simple answer real densities gaussian 
fact far gaussian 
ofthe strongest limitations gaussian exponential densities unimodal asingle peak 
modeling densities single peak 
shows attempt peaked function asingle gaussian 
situations may simplicity ciency arises density outweigh added accuracy arises accurate model 
see 
modeling densities ai tr sample true distribution fit views density constructed combination gaussians 
gaussians variance respectively 
sample contains points 
maximumlikelihood gaussian mean variance 

decision complex model set possible model densities literally nite 
terms accuracy advantage 
density guaranteed integrate toone 
common model simple gaussian mixture gaussians nx cig represents collection parameters fig ig 
ci mixture model guaranteed integrate toone 
mixture density need uni modal may asn peaks 
contains graph mixture gaussian density equal components 
large number gaussians density accurately 
maximumlikelihood parameters 
possible search forthe correct parameter vector gradient ascent gaussian mixture models cient technique known expectation maximization dempster 
case nding best parameter vector involve search process 
mixture models fairly popular parameterized function paul viola chapter 
probability entropy 
neural networks literature trained back propagation neural networks approximate densities jacobs see haykin excellent review neural network research 
terribly special network purpose 
just form parametric density estimation 
feel process critical important limitations pointed 
estimates important rst step required assumptions form density 
space possible functions large nite number density functions equally 
continuous density defy intuition 
instance possible de ne sample nitely 
take example density function functions 
section function trial xa likelihood model density guaranteed bigger density likelihood 
wouldn imply tting optimal 
intuition argues cial density principled scheme dealing dilemma 
written problem machine learning literature called function approximation 
simply information nite sample uniquely determine possible functions sample best 
solution strong assumptions correct function example smooth 
assumptions provide strong prior space possible functions 
likelihood function prior probability uniquely determine 
maximum likelihood model selection guaranteed es 
reasons model may fail model 
rst reason set evaluated models may contain correct model 
called inductive inadequacy arises underlying assumptions density wrong 
second maximum likelihood fooled 

modeling densities ai tr especially sample example unbiased coin see section lead model correct 
question con dence 
larger sample gives con dence model 
third reason search parameter space may fail 
solution may exist example local minima 
parzen window density estimation nal class density functions discuss called non parametric density estimators 
models search parameters needed 
parametric methods parameters model non parametric methods sample directly de ne model 
non parametric scheme focus known parzen window density estimation 
general form density xa sample valid density function 
called smoothing function 
quality ofthe approximation dependent functional form 
di erent window functions lead di erent density estimates 
gaussian density common selection forr making parzen density estimate mixture gaussians 
gaussian centered sample 
contains graph density sample parzen estimate constructed sample 
contains graph di erent parzen estimates di erent samples 
di erent parzen estimates show signi cantly variation gaussian estimates shown 
practice parzen density estimate exible parametric density estimate 
parametric techniques strong assumptions functional form density parzen estimation requires density smooth 
shows parzen density estimate bimodal distribution 
contrast parametric estimate ofthe 
paul viola chapter 
probability entropy sample true gaussian parzen fit views gaussian density sample points drawn density 
represented vertical black line 
second density ofthe true gaussian 
third parzen density estimate constructed sample 
window functions gaussians variance 
intuitively parzen density estimator average sample 
looking backto notice ifr symmetrical origin view window function centered query point data points 
viewed light density estimate query point sum sample weighting window function 
common window functions unimodal symmetric origin quickly zero 
ect window function de nes region centered sample points contribute density estimate 
points fall outside ofthis window contribute 
density ratio number weighted sample points window divided total number sample points na 
getting reliable estimate ofthis ratio involves having reasonable number points window query point 
number points expect fall window ofthe size sample size window 
number points window decreases parzen increases 
analyze parzen estimate chapter 
balance computation required qualitatively di erent parametric schemes 
constructing parametric model involves lengthy search parameter space takes time larger samples 
constructing 
modeling densities ai tr parzen density estimates di erent samples points drawn density 
parzen model cheap 
need memorize sample 
evaluating parametric model usually cient 
parameters known number operations required usually small grow withthe sample 
expensive requiring time proportional size sample 
computational complexity technique function 
parzen model mixture model 
estimate model constrained place gaussians sample points 
asymptotic proof parzen convergence relies law large numbers 
parzen estimate written sample mean xa ea limit true expectation convolution lim na dx rp sop converges top ifp rp 
distinct conditions equality holds 
rst delta function paul viola chapter 
probability entropy sample true density parzen fit views density constructed combination gaussians 
gaussians variance respectively 
sample contains points 
parzen estimate constructed gaussians variance 
sample size approaches nity 
second occurs convolution 
theory achieved bounded frequency content perfect low pass lter 
practice approximate equality holds low frequency content primarily low pass lter example smooth function gaussian 
rp parzen estimate unbiased estimator ofp 
conditions parzen estimate converge correct density estimate 
proof assumes samples corrupted measurement noise known density 
ofx corrupted random variable observed 
known probability ofx xj knowledge integrate possible values xj xj 
modeling densities ai tr wemust integrate integral approximated parzen estimator xj pex pex ea xa xa na xa sample probability ofthe uncorrupted random ap parzen estimate constructed samples smoothing function 
corrupted random variable similar argument na xa pp xa probability noise corrupted random variable approximated parzen estimate usingthe smoothing function pp 
result ofthe density ofx 
gaussian noise common assumption discussions entropy 
function gaussian density twice standard deviation finding functions seen priori information density isavailable parzen estimation converge tothe correct density 
smooth perturbed noise possible correct smoothing function 
absence priori information parzen estimate onthe variance smoothing functions 
figures display dependence density estimate parzen estimates computed point sample changed 
notice actual density function results isvery dependent variance 
qualitative nature dependence varies range variances paul viola chapter 
probability entropy plots ofthe parzen density estimates derived point sample 
gaussian variance 
di erent estimates di erent value variance component smoothing functions 
variances range factor 
shown 
variance smoothing function small resulting density changes rapidly variance changed 
small changes variance change resulting density nearly rapidly 
selection correct variance smoothing functions need hit process 
inthe way likelihood parameters gaussian sample variance gaussians parzen estimate 
general possible compute gaussian separately 
process requires great deal time data 
preserve simplicity ofthe parzen estimate single variance smoothing functions 
recall empirical entropy minimized see section 
subsequent chapters focus empirical entropy empirical entropy estimate optimal variance 
graphs empirical entropy ofthe sample versus variance 
sample graph estimate densities figures 
broad implies parzen density estimate critically dependent 
variance need factor optimal variance 

modeling densities ai tr parametric surface plot parzen density versus variance data shown previous graph 
horizontal vertical axes location density respectively 
variance changes depth inthe graph 
variance ranges 
true entropy variance 
optimal parzen density estimate empirical entropy 
close agreement coincidence 
argued chapter ectively estimated parzen density estimate 
small technical note overlooked 
careful sample construct parzen entropy 
recall entropy density estimate sample collection delta functions centered point see 
know delta function density nity 
density similar form delta function density 
centers function point sample 
limit variance smoothing functions tends zero smoothing function approximates delta function 
minimum empirical entropy obtained variance zero 
di culty arises sample density asthe sample empirical entropy calculated 
samples di erent density way point appear samples 
smoothing functions tends zero density points parzen sample tends zero 
result empirical entropy tend positive nity asthe variance paul viola chapter 
probability entropy log likelihood log plot negative log likelihood versus near minimum log likelihood terribly sensitive tothe values factor roughly equivalent 
tends zero 
ectively precludes solution smoothing functions zero 
di process called cross validation 
crossvalidation splits asingle samples 
sample single second contains remaining fxg 
arena di erent ways split sample parts 
draw di erent samples di erent split samples 
case larger sample parzen estimate smaller sample estimate entropy 
estimating loglikelihood empirical entropy versus log logp log xa crossvalidation 
cross validated empirical entropy estimate ofthe sample empirical entropy 

modeling densities ai tr quality ofthe parzen estimate way evaluate quality ofthe parzen estimate standard deviation estimate 
useful statistic standard deviation normalized mean normalized standard deviation measures expected deviation mean function scale 
types problems large small deviations mean usually unimportant 
mean small small deviation big di erence 
normalized standard deviation measure log variable important log likelihood entropy 
constant linear terms taylor expansion logarithm assuming standard deviation ofx small log deviation function number sample points density 
standard deviation estimate standard deviation expectation taken space possible samples 
equations equal estimator forp 
standard deviation estimate computed exactly smoothing functions box 
parzen number sample points fall box car window divided total number sample points xa na xa non zero andk chosen paul viola chapter 
probability entropy thatp integrates 
standard deviation parzen estimate assuming sample independent na na vu nap nin normalized standard deviation parzen estimate na vu na na vu nap nap larger normalized standard deviation probability 
parzen density estimate converges proportional na de nition parzen window estimation generalized higher dimensions replacing dimensional smoothing functions counterparts see section gaussian 
de nition parzen estimation number dimensions behavior algorithm di erent 
number dimensions grows number data points required rapidly increases 
dimensions window gaussian smoothing function sphere deviation 
volume dimensional sphere constant dependent 
assuming sample sphere data points fall randomly chosen window 
generally selected 
result increased dimension number points falling chosen window drops exponentially standard deviation ofp increase rapidly 
parzen density estimate unreliable dimensionality increases 

modeling densities ai tr theory remedied increasing sample exponentially things rapidly get 
empirical evidence argues parzen estimation dimensions 
chapter empirical entropy manipulation stochastic gradient descent chapter technique empirical entropy distribution called emma 
theory entropy manipulation plays critical role thesis forms algorithmic core applications 
number existing techniques 
signi cant theoretical practical limitations unsuitable purposes 
techniques simple applications 
second chapter describes new procedure evaluating empirical entropy emma 
cient stochastic gradient scheme emma estimates 
scheme applications outside manipulation 
nal section chapter presents application emma 
emma derive theoretic version principal components analysis 

empirical entropy ai tr empirical entropy saw previous chapter true density random variable known 
ofthe density obtained sample variable 
likewise direct procedure evaluating entropy sample 
common approach isto rst model density sample estimate entropy density 
divides problem manageable parts solved separately 
far density model entropy calculations gaussian 
considerations nding gaussian data easy see section entropy ofthe gaussian directly calculated variance 
entropy distribution log dx log dx log log log log log entropy 
wider gaussians gaussians 
simple procedure nding empirical entropy distribution compute variance sample evaluate 
equivalence log variance entropy reformulate known signal image processing problems entropy problems 
logarithm monotonically increasing function technique maximizes minimizes variance signal viewed entropy technique 
examples include principal components analysis variance maximized square solutions matching problems paul chapter viola 
empirical entropy manipulation stochastic gradient descent variance minimized 
signi 
variance maximization equivalent maximization density ofthe signal involved gaussian 
assumption violated possible reduce entropy increased 
avery techniques manipulate entropy assume signals gaussian exponential distribution linsker becker hinton bell sejnowski 
discuss techniques section 
principal components analysis number signal processing problems formulated entropy maximization problems 
known example principal components analysis 
principal component dimensional dimensional vector 
nes new random variable yv xv 
variance new variable var yv ex xv ex xv principal vector yv 
practice known 
projection variance computed points de ne cost function var yv vara yv ea xv ea xv vara yv minimized principal component vector 
schemes nding principal component 
ofthe elegant accomplishments linear algebra proof rst eigenvector covariance matrix isthe principal component vector 
assumption gaussian prove principal component projection 
projection gaussian gaussian 

empirical entropy ai tr second entropy monotonically related variance 
corresponding axis highest variance gaussian highest possible entropy 
principal components analysis nds contains information 
information yv yv equation components 
rst implies give information lot entropy 
second misleading 
removes randomness nity 
precisely relative entropy relative 
di erences rel ative entropies signi cant 
yields information yv yv yv yv jx yv yv jx yv yv conclude principal component axis carries information axis 
function learning known problems formulated entropy framework 
analyze simple learning problem 
random ne functionally dependent rv xv perturbed measurement noise 
samples joint occurrences rvs 

typically formulated squares problem 
cost de ned squares di erences predicted ya xa andthe actual samples ofy ya xa estimated parameter vector squared di erence justi ed log likelihood perspective 
assume paul chapter viola 
empirical entropy manipulation stochastic gradient descent noise added toy gaussian independent ofy log likelihood log log xa log ya xa ya xa constant value independent minimizes squares data observed 
problems gradient descent search show minimizing mutual information 
showed section log likelihood related sample entropy log na mutual information ha yj na yj yj rst term function log likelihood 
non gaussian densities commonly held misconception information techniques equivalent simple known algorithms 
contrary impression examples may give truth 
entropy equivalent squares data 
approach alignment bias correction describe chapters assume distributions gaussian 
show ifthe data gaussian alignment technique reduce correlation 
number non gaussian problems 
estimating entropy parzen densities ai tr information 
bell shown signal separation de correlation thought entropy problems bell sejnowski 
bell technique derived gaussian non gaussian distributions 
bell shows gaussian assumption leads awell known ine ective algorithm 
signals presumed non gaussian resulting algorithms ective 
compression image processing problems clearly involve non gaussian distributions 
theory empirical entropy estimation density model 
procedure density sample compute entropy density 
practice process computationally intensive 
rst part maximum likelihood density estimation iterative search parameter space 
second evaluating entropy integral may impossible 
example known closed form solution entropy gaussians 
entropy integral approximated sample mean hb eb log isthe sample mean taken estimate sample density isthe sample entropy rst introduced section 
sample mean converges true mean proportional nb nb 
insight samples estimate entropy rst estimate density second entropy 
sample approach estimate entropy practical algorithm entropy manipulation 
applications parameter vector ect densities approximated 
parameter space adjusts parameter vector new sample drawn new density estimated derivative evaluated 
estimating density complex search process search correct parameter vector take long time 
estimating entropy parzen densities section describe technique ectively estimate entropy non gaussian distributions 
basic insight maximum paul chapter viola 
empirical entropy manipulation stochastic gradient descent likelihood estimate density sample parzen window density estimation see section 
parzen signi cant advantages maximum likelihood directly sample search parameters derivative ofthe entropy ofthe parzen estimate simple compute 
general derivation assume wehave samples random 
entropy 
direct technique nding parameters search parameter space gradient descent 
derivation assumes vector random variable 
joint entropy random variables evaluated constructing vector random variable 
form parzen estimate constructed ya estimator constructed 
approximate entropy asthe sample mean eb log log yb computed second isthe empirical entropy 
order entropy wemust calculate derivative respect expressed dv nb yb ya dv yb ya ya ag yb ya di erentiating gaussian ya ag yb ya yb ya dv dv yb ya ya ag yb ya 
estimating entropy parzen densities ai tr expression may written compactly follows dv de nition bya wy yb ya wy yb ya yb ya dv yb ya yb ya ya ag yb ya wy yb ya takes values zero 
approach signi cantly closer element ofa 
near zero element ofa signi cantly closer 
distance interpreted respect squared distance see duda hart wy yb ya degree match arguments soft sense 
equivalent softmax function neural networks bridle negative ofthe distance indicate correspondence elements ofa 
equation may expressed dv bya wy yb ya dv yb ya form apparent entropy adjusted reduction average squared distance points indicates nearby 
moving worth density isvery dv di 
general derivation complex parzen derivation dv logp yb paul chapter viola 
empirical entropy manipulation stochastic gradient descent yb yb dyb yb dyb dv dap yb da dv yb numerator components 
rst dyb entropy results 
second dyb yb isthe change dv dap yb da dv far problematic 
second component change density estimate results 
parzen framework components ofthe derivative collapse asingle term directly computed samples 
maximum likelihood yb function sample 
closed form function computes density estimate fromthe sample computing derivative di cult 
stochastic maximization algorithm variance maximization minimization applications described principal components analysis learning deterministic procedures 
starting initial guess gradient descent uses derivative repeatedly update parameter vector 
di erent runs start parameters nal parameters 
justi cation probability problems purely convenience 
random problems samples drawn 
bene probabilistic interpretation problems introduce randomness understand ect 
simple example want know average large sample data 
knowing sense entire sample 
needed rough estimate ofthe average signi cant computation saved averaging sample 
furthermore knowledge sample variance allow compute size subsample needed estimate mean agiven precision 
similar analysis applied principal components analysis function learning 
cost particular parameter vector computed summing entire sample equation 
sample large expectation 
stochastic maximization algorithm ai tr approximated random sample 
argument applies gradient 
gradient ned average large sample may sense smaller random sample 
random samples error estimate andthe gradient estimate truly random 
large samples accurate error gradient estimates averaging sample 
problems gradient needs save signi cant computation 
random estimate ofthe gradient compute useless 
conditions sense gradient estimate 
theory stochastic approximation tells stochastic estimates gradient true gradient conditions hold gradient estimate unbiased parameter update rate asymptotically converges zero error surface quadratic parameters robbins ljung haykin 
rst condition requires gradient true gradient 
second insures moving randomly parameter space 
practice third condition relaxed include smooth non linear error surfaces guarantee parameters particular minimum 
returning attention equations notice calculation emma entropy estimate involve summation 
summation points points inb 
result evaluation quadratic sample size 
experiment derivative image containing pixels evaluated 
true derivative empirical entropy obtained exhaustively sampling data random estimate ofthe entropy obtained computation 
especially critical entropy manipulation problems derivative thousands times 
quadratic savings arise smaller samples entropy manipulation impossible 
problems involving large samples stochastic gradient descent 
stochastic gradient descent seeks local estimate ofthe gradient true gradient 
steps repeatedly taken proportional approximation mutual information respect parameters paul chapter viola 
empirical entropy manipulation stochastic gradient descent repeat fna samples drawn fnb samples drawn dh dv vis parameter dv estimated andthe parameter called 
procedure repeated xed number times convergence detected 
problems initial value reduced search 
samples ectively nd entropy maxima 
stochastic approximation known computer vision community 
believe anumber cost minimization problems arise computer vision 
stochastic gradient descent tasks evaluation true gradient expensive gradient easy compute 
examples include derivative pixels image 
cases stochastic gradient search orders magnitude faster complex second order gradient 
experimental sec tion chapter thesis joint existing vision application sped factor fty approximation 
convergence stochastic emma conditions insure convergence stochastic gradient descent easy ob tain practice 
example really necessary asymptotically converge zero 
non zero learning rates parameter vector move randomly minimum maximum endlessly 
smaller learning rates excursions true answer 
ective way terminate search average parameter changing andthen reduce learning rate 
needs approach zero goal zero error practical system achieve anyway 
idea reduce learning rate parameters reasonable variance take average parameters 

stochastic maximization algorithm ai tr rst proofs stochastic approximation required error quadratic parameters 
modern proofs general 
convergence particular optimum parameter vector guaranteed basin attraction nitely 
basin attraction optimum ned respect true gradient descent 
basin set points true gradient descent converge optimum 
quadratic error surfaces asingle optimum andthe basin attraction entire parameter space 
non linear error spaces may optima parameter space partitioned basins attraction 
nite number optima prove stochastic gradient descent converge 
proof proceeds contradiction 
assume parameter vector converges 
wanders parameter space forever 
parameter space partitioned basins attraction basin 
nite number basins basin nitely 
converge optimum basin 
condition give trouble 
stochastic estimate ofthe unbiased 
true approximation empirical entropy unbiased 
able prove consistent bias large 
section described conditions parzen density estimate unbiased 
conditions met number equalities hold limp na efa efa xa ex denotes expectation possible random samples drawn random 
assuming di erent samples ofx independent allows move expectation inside summation 
true entropy ofthe expressed de ne similar statistic ex fxg paul chapter viola 
empirical entropy manipulation stochastic gradient descent fxg eb xb xa ex efa log isthe expected value ofh 
provides unbiased estimate 
jensen inequality allows move logarithm inside expectation ex ex efa log stochastic emma estimate estimator statistic larger true entropy 
intuitively overly large estimates arise elements ofb fall regions wherep small 
points smaller 
de nition emma remedy bias 
statistic similar entropy ex dx randomness ofx ne shortly 
strongly peaked distributions large negative values distributions approaches zero 
known inequality log dx log dx parzen window density construct stochastic measure eb 
stochastic maximization algorithm ai tr expectation simplifying fxg fxg eb ex efa ex ex expectation pair events 
far large 
ned alternative statistics inexpensive unbiased estimates available andh 
statistics bound true entropy average large average small 
success estimate eh eb xb log ifp pmin pmin log pmin dx continuous 
see function designed plot function 
intuition possible sample points wherep large standard deviation 
standard deviation parzen density estimate points probability density wherep see section 
variable pmin allows continuously vary estimate fromthe extremes andh paul chapter viola 
empirical entropy manipulation stochastic gradient descent log plot andx 
di erent values plotted notice smaller values cause approximation 
di erence functions pmin unnoticeable 
stochastic search techniques non linear stochastic gradient descent commonly neural network literature called lms rule 
introduced widrow ho widrow ho extensively 
stochastic estimate gradient error cheaper compute true estimate ofthe gradient real problems lms faster gradient techniques 
textbook haykin haykin discusses algorithms network community 
excellent discussion stochastic approximation appears textbook ljung ljung 
simulated annealing related method optimization problems local minima kirkpatrick 
minima trap gradient techniques far optimal solution 
simulated annealing performs random usually local search parameter space 
random modi cation parameters proposed new cost evaluated 
new cost lower previous cost parameter modi cation accepted 
di erence cost positive modi cation accepted probabilistically 
probability acceptance proportional 
stochastic maximization algorithm ai tr negative exponential di erence exp di erence new old cost temperature controls likelihood bad modi cation accepted 
simulated annealing insight physical systems iron invariably nd energy minima heated cooled slowly 
process physical annealing basically gradient search perturbed thermal noise 
thermal noise provides energy systems local minima 
analog physical temperature initially set large values gradually cooled learning 
cases proven right annealing schedule simulated annealing converge global optimum 
stochastic gradient descent ectively penetrate narrow local minima trap gradient techniques 
local arise false matches high frequency components inthe model image 
false matches features small local minima narrow pose space 
narrow local minima trap gradient techniques overcome gradient descent 
believe stochastic approximation combines cient computation ective escape local minima 
estimating covariance addition learning rate covariance matrices smoothing important parameters emma 
parameters may chosen optimal maximum likelihood sense 
equivalent minimizing cross entropy ofthe estimated distribution true distribution see section 
goal parameters minimize empirical entropy 
paul chapter viola 
empirical entropy manipulation stochastic gradient descent simplicity covariance matrices diagonal diag identical section equation analogous dk xx wy yb ya yb component ofthe 
equation forms stochastic maximization likelihood 
repeat fna points drawn fnb points drawn dk procedure similar section 
entropy manipulation possible interleave covariance updates parameter updates 
principal components analysis information demonstration parameter estimation rule akin principal components analysis truly maximizes information 
new emma component analysis eca manipulates entropy ofthe random xv constraint jvj 
value entropy estimated sample ofx yv log log yb ya 
principal components analysis information ai tr variance parzen smoothing function 
estimate derivative dv yv bya nb xx wy yb ya yb ya dv yb ya wy yb ya yb ya xb xa decompose derivative parts easily 
rst analyze summand yb ya xb xa 
ignoring weighting left derivative functionf yv dv yv yb ya xb xa ea yb ya xb xa isf yv 
derivative ofthe squared di erence samples see dv yb ya yb ya dv yb ya dv xb xa yb ya xb xa yv ea yb ya expectation squared di erence pairs trials 
recall pca searches largest variance ea ya ea ya vara yv 
interestingly expected squared di erence pair trials precisely twice variance eb ea yb ya eb ea eb ea paul chapter viola 
empirical entropy manipulation stochastic gradient descent eca pca scatter plot sample dimensional gaussian density 
sample contains points 
principal axis eca axis plotted vectors origin 
vectors nearly identical 
eb ya ea ya eb eb yb ea ya ea eb eb ea ea vara wy nd exactly pca projection vector 
derivative eca act points equally 
ya yb distance 
large signi cantly closer element ofa 
result eca maximizes variance local way 
points far apart forced apart 
way interpreting eca type robust variance maximization 
points outliers far points play small role minimization 
robust characteristic stand pca isvery sensitive 
densities gaussian maximum entropy projection rst principal component 
simulations eca ectively nds projection pca 
shows sample data andthe eca principal components 
density larger variance horizontal axis pca axes point horizontal axis 
eca code take roughly seconds sparc workstation 

principal components analysis information ai tr eca pca scatter point sample dimensional density 
mixture horizontally stretched gaussians 
pca eca principal axes plotted vectors origin 
comparable time 
general pca nd highest entropy projection non gaussian densities 
complex densities pca axis di erent entropy maximizing axis 
shows density eca axes di erent 
pca axis spreads points inthe sample far apart possible 
eca axis spreads nearby points inthe sample far apart possible 
resulting densities graphed 
tightly peaked broadly spread 
nal variance larger vs entropy higher andh 
linsker argued pca axis separates clusters distribution linsker 
justify claim uses gures 
graphs show pca axis projecting points separated clusters remain separate 
proposed pca axis useful cluster classi cation high dimensional data 
words highdimensional data projected low dimensional space perturbing cluster structure 
general true 
pca separates clusters variance clusters higher variance clusters 
paul chapter viola 
empirical entropy manipulation stochastic gradient descent eca pca parzen density estimates 
ironically minimum entropy projection separate clusters 
assume generated prototypical point perturbed random noise 
little noise sample points associated cluster prototype clustered tightly 
density peaked cluster prototypes low entropy 
additional noise acts cluster adding entropy tothe density 
entropy inthis density noise clusters 
entropy maximizing algorithm nd projection vector maximizes projection noise 
onthe hand entropy minimizing algorithm possible nd projection noise 
eca nd entropy maximizing eca max minimizing eca min axes 
shows distribution noise ofthe clusters perpendicular axis separates clusters 
result pca axis separate clusters 
eca axis shown axis emma algorithm learning rate 
eca min axis separates clusters better pca axis see 
provide intuition regarding behavior eca run eca pca related procedures bcm bingo 
bcm learning originally proposed explain development receptive elds patterns visual cortex bienenstock 
argued rule nds projections far gaussian intrator cooper 
limited set conditions 
ai tr eca min pca scatter point sample dimensional density 
mixture horizontally stretched gaussians 
pca eca axes plotted vectors origin 
bcm nds projection 
bingo proposed nd axes bimodal distribution schraudolph sejnowski 
displays point sample di erent projection axes algorithms discussed discussed 
density mixture clusters 
cluster high kurtosis horizontal direction 
oblique axis projects data highest entropy eca max nds axis 
vertical axis data low entropy eca min nds axis 
interestingly vertical axis high variance pca nds entropy minimizing axis 
bcm may nd projections attracted kurtosis horizontal axis 
horizontal axis minimizes maximizes entropy 
bingo successfully discovers vertical axis bimodal 
densities di erent projections shown 
chapter new technique estimating entropy distribution called emma 
provided density smooth proven technique converge correct entropy estimate 
paul chapter viola 
empirical entropy manipulation stochastic gradient descent eca pca parzen density estimates previous graph 
computationally cient stochastic technique manipulating entropy 
reasonable sample sizes technique guaranteed optimize true entropy 
optimizes avery similar statistic retains salient characteristics entropy 
described simple application emma 
emma enables dimensional projections higher dimensional data minimize maximize entropy 

ai tr eca min eca max bcm bingo pca scatter point sample dimensional density 
cluster high kurtosis horizontal axis 
see text description projection axes 
density eca min eca max bcm bingo pca position densities various projection axes 
chapter matching alignment chapter important inthis thesis 
previous chapters mathematics algorithms underly computation empirical entropy 
seen empirical entropy ne anew algorithm nding informative projection distribution 
chapter show matching alignment formulated entropy problem 
addition discuss intuition framework suggest simpli ed schemes re ect intuitions 
chapter alignment problems drive discussions 
likelihood method 
derivation clear assumptions correlation may fail 
attempt generalize correlation wider set inputs 
generalization theoretically straightforward intractable 
dropping focus correlation ne approach alignment ciently computable 
intuition de ne likelihood technique concrete 
draw parallel technique mutual information 
experimental data synthetic alignment problems proposed alignment techniques 
chapter conclude di erent motivation mutual 
alignment ai tr information alignment 
show alignment problem thought minimum description length problem rissanen leclerc 
formulation naturally focus ciency entropy minimization 
similar set alignment equations arise considerations 
alignment signals time space andv 
model 
description physical object computed great care 
example model accurate dimensional description skull 
second signal image model 
general coordinate systems model image di erent 
example models collection dimensional points normals corresponding image dimensional array intensities 
assumed ofu example picture 
relationship andv physics imaging 
process constructing separate components 
rst component called transformation pose relates coordinate frame ofthe model coordinate frame ofthe image transformation tells part model responsible particular pixel image 
isthe imaging function 
imaging function determines value image 
general pixel value may function ofthe model exogenous factors 
example image object depends object lighting 
parameter collects exogenous uences asingle vector 
complete imaging model equivalently random variable models noise imaging process 
number practical problems transformation model paul viola chapter 
matching alignment intensity position graph ofu andv versus known 
alignment isthe process correct transformation extracted 
alignment di cult problem imaging functionf physical world di cult 
exogenous necessarily known di cult nd 
example computing lighting non trivial problem 
space transformations may dimensions di cult search 
rigid objects dimensional transformation space 
non rigid objects principle number pose parameters 
simple example lend intuition de nitions 
andv dimensional signals 
transformation space space possible translations imaging functionf identity function 
choosing leadsto contains graph signals relationship 
show image model aligned correct alignment known 

alignment ai tr intensity position graph ofu andv versus synthetic experiments random noise added tov noise course unavoidable 
importantly addition noise demonstrates algorithms numerically stable 
complex imaging functions possible 
example non linear contains graph ofu andv 
correlation maximum likelihood technique search forthe correct alignment cast maximum likelihood variance min problem see section 
probability model transformation noise distribution exogenous parameters imaging function xa xa xa white low passed ltered roughly cycles unit 
peak peak amplitude ofthe noise peak ofthe signal 
paul viola chapter 
matching alignment equation assumed pixel conditionally independent 
conditional independence imply thatthe pixels independent just andf known pixels independent 
assuming noise gaussian compute log likelihood transformation log logp logp xa xa xa xa xa xa ev andk constants computed variance noise number sample points 

wehave expanded square di erence show log likelihood transformation components arises variance model second correlation image predicted image arises variance predicted image 
problems variance image predicted image xed best transformation maximizes correlation actual predicted image 
convenience ne cost transformation log lowest cost transformation causes model match image best 
analysis principal components learning invented random variables andu 
random ranges points coordinate system ned 
random range values model image 
reality random processes involved matching alignment 
model image pre determined xed 
alignment proceed deterministically eval directly points inthe model image 
chosen interpret 
alignment ai tr summation pixels arises correlation expectation set random variables 
result insights probability brought bear problems 
correlation mutual information alignment isvery similar problem function learning encountered section 
equation identical 
problems looking fora set parameters cause inputs outputs 
function learning attempts nd best parameter alignment attempts best function learning draw analogy sample entropy 
log likelihood oft proportional conditional entropy ofthe image model log na ha ju conditional entropy ofv assumption conditionally gaussian 
problems described section possible led information solutions 
problem claim maximizing log likelihood equivalent maximizing mutual information andu 
information ju includes unconditioned entropy 
types transformations may change ast varied 
cases minimizing conditional entropy isnot equivalent mutual information 
maximize unconditioned entropy 
example translation varied signals periodic unconditioned entropy change ast varied 
returning tothe rst synthetic example versus translation 
signals assumed periodic boundary conditions signals 
see strong true translation pixels 
paul viola chapter 
matching alignment intensity position difference squared position left plot image model identical noise 
right ofc versus translation 
signi cant correct aligning translation pixels 
intensity position difference squared position left plot image model related non linearly 
right ofc versus translation 
minima atthe aligning translation pixels 
fact minima exist incorrect translations 
correlation works matching imaging model exogenous parameters known 
may faced wheref unknown 
cases alignment problems solved assuming imaging function identity function 
assumption ective aligning non monotonically related signals shown 
graphs versus translation signals 
notice actual minima incorrect translations 
align signals related non linear transformations generalized signals transformed linearly 
minimize squared di erence signals minimize squared di erence signals normalized 
normalized signal 
alignment ai tr intensity position graph ofu andv versus zero deviation computed bu normalized version signal invariant changes original 
squared di erences normalized signals nc computed directly minus normalized correlation 
normalized cost de ned nc ea ea ea shorthand wehave abbreviated sums expectations variances 
normalized cost signals ones shown 
versus translation identical 
cases normalized cost applied signals transformed non linear monotonic functions 
note signals shown related non monotonic function accommodated way 
examples translation ect mean standard deviation signals 
result normalized cost produce convincing minimum cost 
paul viola chapter 
matching alignment wemay wish align models related non monotonic functions 
case alignment performed jointly space possible imaging functions exogenous parameters transformations 
probability motivate procedure 
unknown unknown variables 
probability ofthe image zz xa xa xa equation integrates possible possible sets exogenous variables 
aware approach come close evaluating integral 
may feasible 
possible approach isto imaging function exogenous variables image max xa xa xa assumed integral equation approximated component integrand 
approximation 
ne alignment procedure nested search estimate forthe transformation image estimates forf andq transformation 
terminate transformation stabilized 
words transformation associates points model points inthe image 
functionf parameter sought relationship andv 
accomplished training function collection pairs fv xa xa algorithms andq similar density approximation learning described chapter 
notice alignment unknown imaging model similar entropy maximization 
entropy maximization search density estimate parameters 
alignment search imaging model transformation 
return analogy shortly 
ofthe pitfalls density approximation described chapter apply function approximation 
learn functionf rst aset assumptions form assumptions discontinuous estimates 
alignment ai tr data perfectly prevent convergence 
way prevent discourage behavior formulate strong prior probability space functions 
search imaging function exogenous parameters combined 
andq de ned 
combining functions common technique shape shading photometric stereo research 
techniques compute shape object shading images 
independently model exogenous variable lighting direction imaging function re function combined function represented manipulated 
combined re map horn 
maps normals object directly intensities 
dimensional alignment procedure manipulates similar combined function 
equation approximated ciently 
reasonable assume real imaging functions similar inputs yield similar outputs 
words unknown imaging function continuous piecewise smooth 
cient scheme alignment skip step approximating imaging function attempt directly evaluate consistency transformation 
transformation considered consistent points similar values model project similar values image 
similar mean similar physical location similar value ju xa xb xa xb ad hoc technique estimating consistency pick similarity constant evaluate sum consistency xb xa xb xa xb 
consistency awed number ways 
instance obvious clues picking wecan replace nature test gradual discrimination consistency xa xb xb xa xb xa gaussian standard deviation inorder minimize measure points close consistent apart 
paul viola chapter 
matching alignment problem consistency measure aggressive consistency maximized constancy 
transformation projects points ofthe model constant region image 
example scale ofthe transformation parameters entirely consistent transformation projects points ofthe model asingle point ofthe image 
number problems consistency need addressed serve source intuition analyze di erent approaches 
alternatives alignment imaging function unknown theoretical technique may intractable cient technique number important di culties 
combines best feature approach 
complex search forthe imaging function fq replaced search consistent imaging function 
type function approximator maximizes consistency known nearest neighbor function approximator duda hart 
neighbor function constructed directly 
value value point sample fn xa xa fn estimate likelihood model 
nearest neighbor formulation naive implementation need search 
image transformation de directly 
nearest neighbor function approximator plays role likelihood computation isvery similar role parzen density estimation plays entropy estimation see sections 
parzen estimate nearest neighbor approximation continuously di erentiable 
similar di erentiable version called weighted neighbor approximator ar xa xa ar xa weighting zero falls asymptotically away technique known kernel regression 

alignment ai tr zero 
common choice deviation re wa xa xa soft nearest neighbor function rst de ned section wa estimate log likelihood log ag xa xa xa xa wa xa xb xb wa xa xb xa wa xa xb xb step relies wa xa xb xa xb wa xa xb 
log likelihood transformation neighbor function approximation similar intuitive consistency measure 
addition derivative bears striking resemblance derivative ofthe see log dt xa xb xa xb dd xa xb dt dt bg xa xb xx wa xa xb xa xb dt xa xb paul viola chapter 
matching alignment intensity position log likelihood position left plot image model identical noise 
right logarithm weighted neighbor likelihood versus translation 
intensity position log likelihood position left plot image model related non linearly 
right logarithm weighted neighbor likelihood versus translation 
weighted neighbor likelihood evaluate erent translations 
shows graph weighted neighbor likelihood versus translation initial pair signals andv contains similar graph second non linear experiment andv graphs show strong minimum correct alignment ofthe signals 
conclude weighted neighbor likelihood situations cost normalized cost 
parallel emma weighted neighbor likelihood structural 
emma estimates density ofthe sample directly uses compute derivative entropy respect parameter vector weighted neighbor likelihood estimates directly log likelihood respect tothe transformation 
importantly techniques entropy ofthe ofthe joint 
emma evaluate weighted neighbor likelihood evaluates conditional ju 
alignment ai tr assumption thatp ju see sections commentary equivalence log likelihood sample entropy 
relax constraint conditionally gaussian conditional entropy ju rst term entropy ofthe model 
function transformation 
seemingly unrelated concepts weighted neighbor likelihood conditional entropy closely related 
gain intuition equivalence looking atthe joint distribution 
particular transformation sample points xa xa plot 
shows signals aligned 
thin line inthe weighted neighbor function approximation data data 
noticeable clumping clustering data 
clumps arise regions constant intensity signals 
large regions constant intensity clusters 
identical signals aligned strongly correlated 
large values signal corresponds 
conversely small values correspond values 
correlation measures tendency data normalized correlation measures tendency data lie line positive slope 
shows joint samples signals shown 
signals linearly related correlated functionally related 
weighted neighbor likelihood measures quality ofthe weighted neighbor function approximation 
graphs lie near neighbor function approximation 
ofthese graphs joint distribution samples tightly packed 
points distributed space lie small part joint space 
hallmark distribution 
generate similar graphs signals aligned 
figures show signals fact shifted units 
shifted signals structure joint distribution destroyed 
weighted neighbor function paul viola chapter 
matching alignment samples joint space ofu andv black square plotted pixel signals 
axis value ofu 
value ofv 
clumping points caused regions constant intensity inthe images 
thin line plotted data isthe weighted neighbor function estimate 
approximation terrible data 
result weighted neighbor likelihood 
alternatively look directly distributions 
signals aligned distributions compact 
misaligned distributions spread haphazard 
words aligned signals low joint entropy misaligned signals high joint entropy 
suggests neighbor likelihood emma approximation joint entropy 
graphed emma estimates joint entropy versus translation signal alignment problems discussed 
shows graph joint entropy forthe signals identical 
shows graph joint entropy model non linearly transformed image 
case graphs show strong minima atthe correct aligning translation 

alignment ai tr samples joint space ofu andv samples joint space ofu andv previous graph signal longer aligned 
paul viola chapter 
matching alignment samples joint space ofu andv signals aligned 
intensity position joint entropy position image model noise 
right estimated joint entropy versus translation 
intensity position joint entropy position image model related non linearly 
onthe right estimated joint entropy versus translation 

weighted neighbor likelihood vs emma ai tr weighted neighbor likelihood vs emma weighted neighbor likelihood emma smoothly di erentiable functions align signals imaging function unknown 
qualitatively emma estimate joint entropy better 
joint entropy synthetic experiments 
weighted neighbor likelihood emma similar di erence 
recall weighted neighbor likelihood measures conditional image model 
assumption conditional distribution image gaussian 
weighted neighbor likelihood data mean gaussian 
log likelihood point proportional squared di erence mean 
general log likelihood calculations sensitive outliers 
outliers points noise measurement error perturbed land far 
recall loglikelihood sample log likelihoods point inthe sample 
result asingle outlier ruin sample high likelihood 
reasonable measure introduce bound onthe penalty single point 
single point moved certain distance local mean longer increase 
calculating likelihood way closely related concept robust statistic emma hand assume conditional distribution image gaussian 
approximates non parametrically 
emma handle situations multiple peaks conditional distribution 
likelihood penalty group points perturbed away local mean distance mean 
points move outside ective range smoothing function additional penalty 
emma robust nature prevents getting swamped outliers joint distribution 
gives greater distributions arise model image 
sections describe number situations emma better weighted neighbor likelihood 
paul viola chapter 
matching alignment non functional signals point analysis alignment assumed exists function relates model image 
classes problems imaging function 
rst arises common situation computer vision occlusion 
second model contain information required predict image 
cases single function regardless exogenous variables predict model 
shows graph original pair signals corrupted occlusion 
occlusion proves alignment techniques proposed 
example basic assumption normalized cost violated occluded signal linearly transformed version model 
addition quick glance joint space shows assumption weighted neighbor likelihood violated see signals aligned longer function andv 
show graph weighted neighbor likelihood versus translation 
global minimum coincides correct translation 
cases emma align partially occluded signals 
joint entropy su er strong assumption signals functionally related 
part signal may corrupted remaining low entropy relationship 
show joint entropy forthe occluded pair signals 
simplest example non functional signals arises image swapped 
function model image non monotonic relationship image model non functional 
non monotonically related signals shown example 
shows joint space swapped signals neighbor function approximation 
function joint space terrible approximation data 
quality ofthe function approximation points limitation weighted neighbor likelihood 
normalized cost symmetric comparison metric weighted neighbor likelihood 
rst distinction 

symmetric measures allow images models models images 
critical possible construct detailed model 

weighted neighbor likelihood vs emma ai tr intensity position graph ofu andv perturbed noise portion occluded 
information symmetric measures 
plot emma entropy forthe swapped signals identical 
complex example non functional signals arises model image functions third signal 
call 
imaging functions creates fu qu creates fv qv 
medical registration detail chapter clear example sensor problem 
medical registration seeks alignment signals types sensors example ct scan mri scan 
gives perfect information object completely predictable 
sensor problem simulated transforming original signal di erent non linear transforms 
original signal de ne fu sin resulting aligned distribution fall approximately 
actual distribution shown 
emma shows strong minimum atthe correct alignment image see 
paul viola chapter 
matching alignment samples joint space ofu andv occluded 
signals aligned weighted neighbor function terrible data 
data segregated parts linearly related part arise non occluded signal constant projects tothe occluded part 
mutual information versus joint entropy recall conditional entropy dependence signals see section 
conditional di erent reasons small dependent 
mutual information better measure dependence 
simple examples described alignment 
complex examples model change scale project limited part image taken account 
general alignment problems maximizing mutual information 

weighted neighbor likelihood vs emma ai tr log likelihood position graph weighted neighbor likelihood versus translation occluded 
joint entropy position graph emma joint entropy translation occluded pair signals 
paul viola chapter 
matching alignment joint space ofu andv model image reversed 
sin vs graph function de ned byu sin andv 

weighted neighbor likelihood vs emma ai tr samples joint space simulated sensory data 
joint entropy position graph emma translation non functional pair signals 
paul viola chapter 
matching alignment alignment derivation derive equations algorithms alignment mutual information 
mutual information model image signals 
requires search ofthe aligning transformation space 
stochastic gradient descent algorithm described section 
derivative information dt dh dh dh dt dt dt oft calculations 
usingthe emma entropy estimate dt nb xi de nitions wv vi vj wi wj wv vi vj vi vj dt vi vj wi wj wi wj uv dt wi wj vi vj xk ag vi vk uv wi wj xk ag uv wi wk xi xj xk viv xi xj xk wi ui vi wj uj vj uk vk matrices scheme joint density block diagonal uv diag uu vv 
matching minimum description length ai tr estimate forthe derivative ofthe mutual information follows di dt vi vj wv vi vj wi wj vv dt vi vj mutual information ect rst term brackets may interpreted acting increase squared distance pairs samples nearby intensity acts squared distance pairs samples nearby image intensity andthe model properties 
emphasize distances space values intensities brightness surface properties coordinate locations 
term dt vi vj generally involve gradients ofthe image intensities derivative transformed coordinates respect transformation 
simple case linear operator obtain outer product expression dt xi rv xi matching minimum description length entirely di erent motivation mutual information alignment metric 
alignment vision problems reformulated minimum de scription length mdl problems rissanen leclerc 
mdl provide new insight problem derive useful term alignment equations 
standard framework sender receiver communicating descriptions images 
sender receiver agreed language describing images sender goal accurately describe image fewest bits 
concept description length code length introduced section 
problem alignment assume sender receiver share set object models 
sender goal communicate ofthese paul viola chapter 
matching alignment models 
knowing simply ignore describing entire image 
require message long asthe entropy ofthe image 
image cient approach possible 
example sender send pose model render 
receiver reconstruct part original image model lies 
entire image sender need encode errors reconstruction image unexplained model 
alignment process sender attempts model pose minimizes codelength ofthe message 
encoding ofthe entire image parts message describing pose describing imaging function message describing errors reconstruction describing parts image unexplained model 
length ofeach part message proportional 
assume poses uniformly distributed sending pose incurs small uniform cost 
length part image 
parts ways 
assume imaging function sent xed small cost 
part proportional conditional entropy ofthe image model imaging function 
precisely estimated minimized weighted neighbor alignment 
second interpretation comes emma 
emma estimates ofthe model image 
conditional entropy ofthe image model computed ash 
entropy ofthe model xed minimizes conditional entropy 
cases entropy alignment rst part chapter minimizes sending parts 
mdl suggests wemust minimize entropy ofthe unmodeled part image 
previous information theoretic formulation concept pixels proportion image explained model 
fact previous formulation part get larger model shrunk 
example assume model covers contiguous region image pixels constant value 
center region small patch containing varied pixels 
recall image sampled points projected model 
model points project region constant project varied patch 
resulting distribution image pixels samples 
matching minimum description length ai tr value fairly low entropy 
ifthe model shrunk cover varied patch points fromthe model fall varied region 
new distribution pixel values higher entropy 
mutual information model entire image measured 
direct parallel mdl framework 
mdl directs sender transmit model pose produce encoding entire image 
model explains image needs directly 
derive approximation mutual information model entire image 
andv represent model image respectively 
information equation rewritten dependence ofh ont implicit inthe equation 
logically image split parts vm vm part image model lies vm unmodeled part image 
ne de vmv vm assumed parts ofthe image independent 
allows split entropies distinct parts vm vm vm vm vm vm vm vm vm vm step relies assumption independent andu background image object object 
equation directs entropy ofthe modeled part image vm minimizing image 
paul viola chapter 
matching alignment emma estimate entropy isthe entropy image 
di cult question answer 
entropy function distribution vector random variable dimensions 
entropy modeled pixel entropies guaranteed ofthe true image entropy 
ofthe problems mutual information tell object 
length allow threshold presence object 
image easier encode 
unfortunately decision highly dependent onthe estimate ofthe entropy length explained part image 
naive overestimate entropy simply sums entropies pixels tight determine decision threshold correctly 
important area open research reasonable estimate ofthe code length 
previous derivations points model projected image 
explicitly modeling entropy ofthe image image project back model 
ey logp logp dy logp dx ex logp equation involves change variables 
plays role jacobian transformation measuring ratio area model projects image 
ne transformations projected area related model area determinant ofthe transformation 
projective transformations term arises foreshortening 
mutual information eb xb log xb log xb log fv xb xb mutual information model image emma estimate ofthe 
summary ai tr mutual information model image signals weighted projected area model 
new formulation mutual information includes term encourages model explain ofthe image possible 
derivative dt eb xb dt log xb log xb log fv xb xb log xb log xb log fv xb xb encourages model 
summary dt xb chapter motivations selection mutual information measure alignment 
primary existing measure alignment correlation maximum likelihood technique important weaknesses 
concept maximum likelihood matching extended de ne general measure alignment 
measure called weighted neighbor likelihood interpreted conditional model 
general correlation weighted neighbor likelihood limited 
generalized called emma alignment 
technique explicitly estimates mutual information image model 
number synthetic experiments demonstrate mutual information exible measure alignment 
nal section chapter concept minimum description length motivation mutual information alignment measure 
alternative framework encourages model explain ofthe image possible 
chapter alignment experiments chapter experiments designed demonstrate alignment maximization mutual information practical technique 
previous chapter contained avery general de nition alignment 
procedure adjusting pose parameters derived details regarding representations implementations left 
chapter include needed details discussion experimental framework 
chapter developed familiarity withthe application emma alignment 
chapter conclude describing explicit limitations approach 
addition describing problems emma alignment poorly suited emphasized emma complete object recognition system 
clarity parameters assumptions underly emma alignment identi ed 
broken process setting wide variety alignment problems see table 
withthe description experiment include speci realization step 

alignment objects video ai tr 
choose model image representation de neu 
de ne scheme coordinates 

choose scheme sampling model de nex 

space possible aligning transformations concrete representation de net 
de random andv complete 

derive expression dy 

pick computing distances pairs samples ofu 
pick variance component densities 
choose value 

number samples estimate distribution number estimate entropy 

pick parameter update rate update rate decrease time 
table process setting alignment 
alignment video rst example described alignment dimensional object video image 
alignment experiments assume entire object surface properties 
treat surface property exogenous variable 
table 
models collection points lie surface object 
chose representation capable representing including smoothly curved irregular forms 
equally capable representing objects faces polyhedra 
models constructed distribution surface points close uniform possible 
associated surface point isthe local surface normal unit vector perpendicular surface 
models points 
video images represented simple dimensional arrays 

random sample model image de ned paul viola chapter 
alignment experiments model 
trial randomly selected model point 
value trial location model point 
sample points ofthe model uniformly 

transformation space space rigid dimensional translations rotations followed perspective projection 
transformation concatenation rotations translations acting onthe pre de ned center object 
self occlusion point onthe model visible 
bu er rendering ofthe model 
bu er rendering takes point inthe model projects image 
multiple points point considered visible 
pose changes points 
theory bu ering needs repeated time pose object changes 
unfortunately bu ering takes time proportional size model 
cost far larger derivative 
pose change iterations gradient descent proven su cient bu er iterations 

dy spatial gradient ofthe image 

metric comparing points sampled image squared di erence 
representation joint events fv tx 
represent dimensions vector components 
normal unit vector component 
joint events dimensional vectors components model image 
distance measure distance joint events 

diagonal covariance matrices smoothing functions variances required 
entropy 
maximumlikelihood estimates aligned objects settled single set smoothing parameters alignment experiments 
variance components ofthe normal variance image intensity 
image entropy variance image intensity 
having asingle set parameters experiment possible part pre normalized images variance 
value 
little sensitivity 
repeated value 
alignment objects video ai tr 
results signi cantly di erent 
values smaller cause derivative entropy noisy see section 
noise prevent convergence correct pose 

draw di erent samples cross validation approximation see section 
experiments sample size 

wemust choose parameter update rate units translation di erent update rates necessary 
internally represent rotations radians translations millimeters 
object millimeter radius center mass translate model point upto millimeter 
translate amodel point millimeters 
derivative information respect amodel point position combination rotation translation 
small step direction derivative move model point upto times rotation translation 
single update rate poor compromise rapid changes arise rotation arise translation 
rotation update rate factor model point move approximately far rotation translation 
scale issues arise complex gradient example conjugate gradient descent levenberg marquardt 
unfortunately techniques stochastic estimates gradient 
models radius order millimeters chosen rotation update rates times smaller translation rates 
alignment experiments proceed stages 
rst stage rotation update rate translation update rate 
number iterations update rates reduced respectively 
wehave chosen simple automatic descent procedure ort simplify subsequent analysis convergence 
realization basic framework table 
paul viola chapter 
alignment experiments 
de ne model andv contains points distributed surface object 
point associated normal image intensities 

sampling distribution surface points close uniform 

transformation space rigid rotations translations perspective projection forthe camera parameters 

de nition dy intensity gradient 

distance metric euclidean distance 

variance assuming diagonal covariance matrices necessary image entropy estimate 
variances ofthe normal component normal image intensity 
image entropy 

minimum probability pmin 

number samples sample cross validation 

update rate rotation rate steps steps 
translation rate 
table video alignment 

alignment objects video ai tr alignment skull model typical image skull object 
rst experiment align skull model anumber di erent images 
skull model produced automatically computed tomography ct scan plastic skull skull photographed number di erent natural lighting skull model contains points 
video images pixels 
example video image skull 
contains representation shape skull model 
image displaying distance camera visible points onthe skull model 
white black nearer 
image computed projecting point image plane 
pixel model point projects records distance model point camera 
may number model points project image pixel 
case depth ofthe model point camera 
model constructed collection points dense 
result pixels point projects 
pixels remain white appear model 
ron kikinis skull model 
registration model described grimson 
mellor providing skull images 
registration described mellor 
paul viola chapter 
alignment experiments depth map skull model 
see text description 
lambertian re model render graphical picture skull 
lambertian model relates model normal image intensity tx liu model isthe normal vector surface patch li vector pointing light andi proportional intensity light source 
version model pose 
human eye sort image readily interpretable depth map 
bring bear substantial visual competence shape object rendered image 
immediately clear pose object model close correct 
simple relationship intensities video image rendered image 
goals rst experiment questions 
emma align complex object model anumber di erent images taken uncontrolled lighting 
long emma alignment take 
isthe range poses correct alignment obtained 
regarding third point true information object camera parameters video camera 
correct pose determined inspection alignment results 
ask related question reliability 

alignment objects video ai tr rendered image skull model 
away correct pose emma alignment reliably re align 
answer rst question establish inthe space rigid transformations maximum information alignment pose 
image object model initially adjusted close correct 
done 
emma alignment correct pose 
scheme assessing quality model pose video image 
done random collection model points projecting coordinate frame ofthe image 
pixels model points project set white 
nature alignment readily apparent images 
model image misaligned model points project background andthe coverage object image incomplete 
model image correctly aligned close agreement occluding contours model points andthe object image 
shows initial incorrect pose way 
shows nal pose obtained running emma alignment 
figures show nal images 
notice images skull close model points 
paul viola chapter 
alignment experiments initial pose skull model alignment 
experiments pre segmented image 
initial poses project model regions image contain signi cant amount 
emma reliably ofthe model points project background 
answer emma requires roughly seconds sun sparc station ofthe alignments shown 
run times identical chosen update iterations alignment experiment 
cases accurate alignment obtained iterations completed 
appeared nal alignment improved number iterations increased 
principled results onthe convergence stochastic approximation 
convergence detection subtle issue 
example emma direct estimate mutual information model image 
alignment estimate ofthe gradient isavailable 
may possible construct ad hoc procedure able detect convergence 
alignment continued pose estimate converged 
analysis program memory access computation patterns conclude implementation digital signal processor times faster current implementation 
major issue cache performance 
emma 
alignment objects video ai tr final pose skull model alignment 
randomly accesses ofthe points inthe image model time re cache 
cache general purpose processor fairly limited 
digital signal processors include large quantity fast sram eliminating need cache 
random memory accesses digital signal processor approximately times faster conventional computer 
procedure dominated simple oating 
modern digital signal processors execute instructions times faster conventional computers 
advantages lead improvement speed 
anumber randomized experiments performed determine reliability accuracy repeatability alignment 
data reported table 
initial alignment performed establish base pose 
pose shown point 
set randomized experiments performed base pose rst perturbed emma re align image model 
perturbation computed follows random uniformly distributed set added translational axis andthen model rotated randomly selected axis uniformly selected angle 
experiments including random ini tial poses 
distribution nal initial poses compared comparing variance location centroid computed separately furthermore average angular rotation 
number poses failed converge near correct solution reported 
nal statistics paul viola chapter 
alignment experiments final pose skull model alignment 
initial final mm mm mm table skull results table 
nal column contains percentage poses successfully converged pose near correct pose 
evaluated poses converged near correct solution 
experiments demonstrate alignment procedure reliable initial pose close correct pose 
outside ofthis range gradient descent capable converging tothe correct solution 
capture range unreasonably small 
translations large half diameter skull accommodated rotations plane degrees 
empirically alignment rotation depth 
terribly surprising visible points play calculation derivative 
result chin hidden derivative gives information move chin skull 
wehave done anumber experiments emma alignment 
alignment objects video ai tr final pose skull model alignment 
deal occlusion 
shows initial nal alignment image includes arti cial occlusion covers entire chin area 
nal alignment isvery close correct despite occlusion 
shows initial nal pose complex occlusion 
image replaced rectangular window randomly chosen window ofthe image 
source rectangle near bottom image 
number experiments alignment occluded images require time convergence 
paul viola chapter 
alignment experiments final pose skull model alignment 
image including arti cial occlusion 
white pose model 
left initial pose nal pose 
image including cial occlusion 
white model 
left initial pose nal pose 

alignment objects video ai tr alignment repeated ofthe skull experiments dimensional model hu man head 
model obtained cyberware scan subject taken approximately years video images cyberware scan complete dimensional representation shape subject head cylindrical coordinates 
surface normals computed surface smoothing di erencing neighboring surface points 
experiments inthis section designed answer questions techniques parameters di erent types models images 
possible pose re nement procedure track sequence 
shows image head ofthe model 
face experiments di erent fromthe skull experiments 
firstly face model smoother skull model 
really aren creases points high curvature 
result edge system construct representation image model stable changes illumination 
secondly albedo actual object exactly constant 
face contains eyebrows lips regions albedo 
result test emma ability objects assumption constant albedo violated 
thirdly occluding contours object inthe model 
model truncated atthe chin forehead 
result experiments model demonstrate emma occluding contours image model agreement 
previous experiment projecting points model image su cient describe pose 
model missing simply projecting model points image su cient quality alignment 
experiments withthe head model display original image augmented model points alongside rendered image model 
figures show model alignment 
experiment model rotated degrees vertical translated millimeters ron kikinis providing cyberware scan allowing images 
paul viola chapter 
alignment experiments initial incorrect pose 
model rotated degrees vertical translated millimeters right 
left image head points projected model 
ofthe model pose 
nal aligned poses 
left image head witha collection points projected model 
right rendering ofthe model pose 
right 
figures show experiment emma alignment corrects millimeter translation depth 
tested emma alignment sequence digitized video tape 
sequence taken asthe images camera lens di erent 
frames acquired video tape frames second 
quality ofthe resulting images low 
images degraded storage video tape bythe 
somewhat surprising images worked nearly higher quality frames 
motion video sequence tracked sequentially aligning model frames 
starting pose frame obtained usingthe nal estimated pose previous frame 
starting pose rst frame hand selected emma alignment acquire initial alignment 
sequence displayed 

alignment objects video ai tr initial incorrect pose 
model moved millimeters camera 
nal aligned poses 
paul viola chapter 
alignment experiments frames video sequence ron head 

alignment objects video ai tr bumps di erent lighting edges alignment curved surfaces third explore nature information emma alignment uses detect correct pose 
previous experiments real data di cult 
component information image model critical alignment 
example case emma image 
occluding contours object critical importance 
experiment created simple pathological synthetic example 
paul viola chapter 
alignment experiments target image final model pose initial model pose object set gaussian shaped bumps patch surface 
object sharp edges awell de ned occluding contour 
shows di erent images object 
image uses re model di erent illumination 
top light source moves gradually left right 
light source moves top 
simple lambertian surface image variation signi cant 
show output ofa canny edge detector run onthe di erent images canny 
di erent edges extracted quite striking 
shows image left 
inserted nite plane 
shown rendered version typical nal model 
experiments rendered images model surface lighting scheme visualization play role alignment process 
black regions rendered image lie outside borders model 
notice model boundaries coincide discontinuities target image 
stable edges usable occluding boundaries conclude emma alignment proceed shading information 
bumps mm bump sigma mm wide 
bump height mm 
lying region mm mm 
true pose mm away camera perpendicular camera axis 
camera viewing degrees 
experiment proceeds exactly previous dimensional experiments 
skull model performed analysis maxima mutual information 
experiments summarized table 

alignment objects video ai tr initial final success mm mm mm table curved surface alignment data paul viola chapter 
alignment experiments medical registration experiments emma alignment dimensional objects relies image function model lighting 
necessary know exact nature function 
medical imaging faced di erent related task 
di erent observations object 
example may computed tomography ct scan magnetic resonance image mri 
scans obtained gives perfect information patient 
ct nding bone 
mri distinguishing 
measurements taken di erent times di erent machines 
clinician information bone soft tissue integrate scans asingle self consistent picture 
done spatial relationships structures di erent scans apparent 
example distance tumor 
chapter able align signals mutual information 
experiment di erent images head aligned see 
images comprise proton density weighted images double echo scan 
chosen scans reasons clear images share great deal information identical taken simultaneously correct alignment close identity transformation 
know ground truth evaluate accuracy emma alignment procedure 
atypical initial alignment appears 
notice image scaled sheared rotated translated version original 
nal alignment 
pixel block model image aligned target image 
notice boundary brain images close agreement 
represent transformation space element project dimensional points fromthe image model 
scheme represent combination scaling shearing rotation translation 
remaining algorithmic details summarized table 
order precision alignment procedure random 
medical registration experiments ai tr 
de ne model andv model image target image 

sample pixels model image uniformly 

transformation space ne transformations mapping pixels locations model pixel locations image 

de nition dy intensity gradient 

distance metric euclidean distance 

variance assuming diagonal covariance matrices different variance necessary forthe image entropy 
cases 

minimum probability pmin 

number samples sample cross validation 

update rate steps steps 
table summary mri alignment experiments 
ized alignments performed 
initial transformations randomly selected having translation pixels width ofthe head rotation degrees scaling 
correct obtained experiments 
alignment ne transformations average translation error pixels 
remaining ne parameters represent rotation scale shearing 
somewhat di cult 
correct transformation identity matrix evaluate nal matrices measuring di erence identity 
average coe cients ofthe nal transformation error 
experiments demonstrate emma alignment precise reliable 
mri images fairly similar 
alignment probably obtained normalized correlation metric 
normalized correlation assumes locally signal scaled set version 
technique assumption 
fact wide variety non linear transformations 
di cult alignment problems easily simulated 
show model image non monotonic non linear function applied 
recall initially image lies range 
square renormalize 
operation shown right ofthe gure 
applying non linear transformation images anti correlated variant correlation correctly align 
emma paul viola chapter 
alignment experiments images alignment performance ected 
dimensional ct alignment alignment procedure described extended volumetric data 
resulting system image arrays full dimensional aligning transformation estimated 
number dimensional ct mri alignments performed 
results preliminary experimental details ux 
description necessarily brief 
complete description wells iii viola :10.1.1.18.4016
scans obtained di erent times dimensional scans alignment di cult problem 
entire scan shown feeling data displaying central slices 
central slices perpendicular planes pass trough centroid data 
shows central slices ct scan 
shows initial images standard transformation provided part project evaluation retrospective image registration national institutes health project number ns principal investigator michael fitzpatrick vanderbilt university nashville tn 

medical registration experiments ai tr initial pose display result ct pair checkerboard 
involved di erent checkerboard representation somewhat confusing 
shows nal alignment asthe composition withthe intensity edges computed ct data 
notice close agreement skull scans 
paul viola chapter 
alignment experiments transformed model transformation central slices ct experiments 

medical registration experiments ai tr initial condition ct registration maximization mutual information displayed checkerboard composite ofthe central slices 
nal con guration ct registration mutual information 
central slices edges registered ct data overlaid 
paul viola chapter 
alignment experiments view recognition experiments previous vision knowledge physics imaging surface normal object predictive ofthe intensity observed image 
unfortunately experimental situations dimensional model available 
situations frequently case information object collection images taken variety conditions 
approach problems images model 
called view approach model views model object 
novel image object model image compared 
model image close novel image model novel image considered aligned recognized 
signi cantly reduce number model images required adding ne transformation comparison process 
novel image compared model image set ne transformations 
commonly comparison metric correlation 
saw section correlation assumption model image identical possibly related linear function 
general set images arise single object varying illumination broad 
shows images object pose 
images di erent fact anti correlated bright pixels correspond right image dark correspond bright pixels 
variant correlation images 
techniques entropy correlated signals 
techniques require consistent relationship model image 
di cult images object consistent relationship 
shows novel image aligned model images 
contains scatter plots ofthe pixel values novel image versus pixel values model images 
clearly simple consistent relationship displayed graphs 
emma match novel image model image 

view recognition experiments ai tr photometric stereo car model images novel image car model 
model image contain information constrain match image model 
known taken collection images determine object 
su cient constrain match image model 
multiple images object available technique called photometric stereo horn 
photometric stereo works images taken di erent illumination conditions 
assumed detailed information illumination surface properties available image 
result re map computed image 
re map determines relationship normals object intensities image 
re map intensity pixel acts constraint onthe normal vector visible pixel normals usually lie closed curve unit circle 
second image re map set allowable normals computed 
intersecting constraints images su cient surface normal normals shape obtained integration 
paul viola chapter 
alignment experiments novel image model image novel image model image relationship pixels image images 
shape object determined correct alignment computed dimensional version emma 
imaging function new stage process xi xi xi photometric stereo function takes images re maps returns shape andf original imaging function predicts image intensities object normals 
practice performing photometric stereo requires kind metric information illumination controlled circumstances 
natural images lighting di 
luckily 
exist high mutual information novel image pair model images 
essence view emma alignment 
don perform photometric stereo simply assume 
result pair images give information third image 
demonstrate approach wehave built amodel images 
shows target image nal pose obtained alignment 
shows initial pose model 
technically experiment isvery similar mri alignment experiment 
main di erence model constructed pair model images 
dimensional vector containing intensity ofthe images 
similar component representation normal 
limitations emma alignment ai tr car image final pose initial dimensional alignment experiments 
experiment 
parameters updated iterations 
set randomized experiments determined capture range alignment length ofthe car degrees rotation 
limitations emma alignment complete discussion emma alignment important caveats emphasized 
emma alignment isnot recognition procedure 
emma play role recognition major missing components 
rst missing 
emma hypothetical pose close true pose 
number experiments performed empirical estimate close determined see tables 
capture range paul viola chapter 
alignment experiments alignment large expect randomly chosen transformation converge true solution 
result procedure required rapidly propose possible poses 
called indexing scheme 
recognition process 
recognition requires model object really inthe image 
object recognition similar concept problem pattern classi cation duda hart fukunaga 
pattern classi cation process input pattern classi ed example class 
task di cult structure complex speci ed 
example determining investment classi cation task complex classi ers human investors di culty performing 
pattern classi cation formulated maximumlikelihood maximum posteriori problem 
novel pattern likelihood turn 
class class considered correct class 
object recognition similar process 
novel image likelihood object model evaluated 
order con dence classi cation particularly important model null hypothesis image contain model 
likelihood null hypothesis proportional unconditioned likelihood image 
seen log likelihood closely related entropy 
result emma de ne classi cation procedure mutual information model novel image evaluated model selected provides information image 
measure unconditioned entropy information null hypothesis gives image 
assumption pixels independent underlies entropy 
leads inaccuracy independence assumption proven su cient alignment 
primarily alignment relative procedure 
model adjusted best explained 
recognition absolute procedure forgiving 
entropy required emma object recognition 
chapter applications emma theory algorithms thesis quite general principle applied avariety problems 
chapter devoted problems solutions 
rst part chapter emma correct images corrupted slowly varying bias eld 
examples include mri corruption arises non uniformity eld lightness correction visual images 
second chapter devoted application stochastic gradient descent outside manipulation 
jones poggio system aligns line drawings faces novel line drawings jones poggio 
published second order gradient descent technique known levenberg marquardt 
similar better results obtained stochastic gradient descent 
resulting algorithm operates roughly times fast original 
bias compensation resonance image mri dimensional image records density tissues inside body 
head parts ofthe body number distinct tissue classes including bone water white matter grey matter fat 
principle distribution pixel values mri scan clustered paul viola chapter 
applications emma cluster tissue class 
reality mri signals corrupted bias eld additive multiplicative set varies slowly space 
bias eld results unavoidable variations magnetic eld see wells iii overview problem 
bias eld constructing automatic tissue classi ers di cult 
wells built bias correction assumption mri scan particular distribution pixel values 
distribution peak type tissue 
physical model mri image formation construct prior model distribution mixture gaussians gaussian tissue type 
model compute likelihood mri 
corrupted mri bias clusters 
wells maximum likelihood select correction eld inverse bias eld corrupted mri 
reiterate system nds estimated correction eld applied look type clustered multi class data 
applying correction eld classes automatic classi cation easy 
learning problems encountered previous chapters assumption nature correction eld necessary condition problem 
prior knowledge bias eld varies slowly space correction eld vary slowly 
wells assume bias eld smooth 
encourage smoothness introduce probabilistic prior smooth elds non smooth ones 
main disadvantage mri correction system requires fairly accurate model tissue distribution 
models di cult construct 
furthermore model includes estimates relative proportions tissue types di erent model required region body 
entropy proceed way 
wells technique proven quite ective safely assume uncorrupted mri image clustered distinct classes 
distribution low entropy 
corruption bias clusters 
bias eld acts noise adding entropy tothe pixel distribution 
insight isthe central idea approach 
attempt low frequency correction eld applied image pixel distribution entropy 
bias corrected image clustering original distribution 

bias compensation ai tr insuring correction eld tricky 
wells correction eld pixel mri 
insure smoothness periodically iteration smoothing correction eld estimates 
approach represent bias rst place 
done parametrically representing correction eld smooth parameterized function 
done frequency correction image say pixel inthe image 
approach guaranteed possible represent correction eld explicit physical model 
case mri physics show bias eld low order polynomial location williams 
take approach correcting mri scans representing correction eld third degree polynomial coordinates scan 
minimizes entropy ofthe similar entropy manipulation applications described 
sample points image point value current estimate forthe bias eld 
proceeding approximate entropy ofthe bias compensated image 
bias eld adjusted direction derivative equation 
case parameters minimizing entropy coe cients ofthe bias eld polynomial 
derivatives bias eld polynomial easy compute 
rst experiment experiment proposed wells binary checkerboard prototypical example 
half pixels belong tothe black class half white class 
pixel entropy ofa checkerboard low 
checkerboard corrupted large unknown bias eld see figures 
corruption large data disappeared 
apparent distribution pixels corrupted image see figures 
entropy minimization comes close exactly bias eld 
shows corrected image 
distribution shown 
image perfectly corrected di erent bias eld representation wells goals correctly classify grey matter brain see bezdek comprehensive overview mri segmentation 
show classi cation easier bias eld scan known 
paul viola chapter 
applications emma original checkerboard image bias corrupts 
left corrupted checkerboard 
center thresholded version corrupted image 
right corrected checkerboard 
performed number experiments emma correction eld 
wells system designed give classi cation corrected scan emma correction provide classi cation 
examine distribution corrected scan attempt determine ifthe corrected way classi cation white grey matter easier 
shows slice mri data taken brain 
shows histogram scan correction 
original scan white grey matter tissue classes confounded asingle peak ranging corrected scan shows better separation classes 
highlight pixels distribution mapping pixels rst peak black pixels second linearly scaling white matter appears darker grey matter mri scan 
shows original corrected scans manner 
notice original image immediately apparent 
lower left hand portion original scan dark 
corrected scan show inhomogeneity grey matter corrected scan distinct 

bias compensation ai tr corrupted corrected pixel density ofthe corrupted checkerboard compensated checkerboard 
second experiment slice mri scan head 
procedure bias eld correction repeated erent scans 
figures show results experiment performed coronal slice mri scan 
paul viola chapter 
applications emma corrupted corrected distribution pixel value mri scan correction 
experimental details experiments smoothing function variance 
learning rate 
sample size 
run concluded parameter updates 
value 
checkerboard image pixels 
checkerboard experiment uses bias eld isa pixel low resolution image 
image interpolated create continuous bias set pixel checkerboard 
parameters bias eld simultaneously approximated emma technique 
mri scans varied size square square 
bias eld mri experiments rd order polynomial 
alignment drawings jones poggio constructed system automatically analyzes hand drawn faces jones 
system faces determine drawn happy surprised 

alignment line drawings ai tr left mri scan shown scale 
apparent 
center correct scan shown scale 
right isthe estimated correction eld 
coronal slice mri scan head 
system works constructing non rigid transformation maps novel drawing hand drawn neutral face see gure 
non rigid transformation determines emotion face 
jones poggio representation non rigid transformations called ow 
ow 
search ow minimizes di erence base andthe novel wheref summation pixels novel image 
problem non rigidly transforming heavily studied computer vision 
previous ow represented directly image displace paul viola chapter 
applications emma corrupted corrected distribution pixel value mri scan correction 
solid line isthe original distribution 
dashed line isthe corrected distribution 
scan modi ed intensity scale 
points intensity grey matter appear white 
points intensity white matter appear black 
linear scale 
notice lower part image darker uncorrected image 
right isthe estimated bias 

problem conditioned smoothness prior ow images 
jones poggio decompose ow combination component ows fig ifi search ow unconstrained optimization 
component ow represents di erent type emotion 
example ow transform neutral face smiling face 
face look surprised 
ows mixed produce images 
alignment line drawings ai tr rst face neutral face 
narrow eyes surprise eyebrows smile 
properties 
novel image set ofi determined provide closest match withthe neutral face 
determine extent face smiling 
shows novel images best reconstruction obtained transforming neutral face 
jones poggio levenberg marquardt second order gradient descent procedure ow parameters see william press excellent discussion optimization techniques 
replaced technique simpler stochastic gradient descent procedure 
course experiments stochastic alignment improved running times seconds seconds 
quality ofthe minima stochastic gradient descent better levenberg marquardt 
stochastic gradient descent gets stuck local minima 
number cases levenberg marquardt converges far best solution 
problems stochastic gradient descent nds solution 
paul viola chapter 
applications emma series novel faces best reconstruction displayed 
chapter maximization mutual information appears powerful means performing local alignment objects 
typical vision feature method 
intensity robust traditional correlation shown insensitivity demonstrated experiment section 
addition method insensitive image data variety non linear transformations defeat conventional intensity correlation 
weaknesses intensity correlation may corrected extent performing correlations gradient 
edge matching techniques perform objects having discontinuous surface properties useful silhouettes 
approaches image counterparts ofthese discontinuities reasonably stable respect illumination 
gradient magnitude correlation edge methods serious di culties domains lacking discontinuities asthe example shown section edges precursor gradient magnitude stable image position respect lighting changes 
technique works shading works domains having surface property discontinuities sil information see section 
paul viola chapter 
related alignment properties joint signal hawkes hill align mri ct medical image modalities 
third order clustering ofthe 
mutual information direct measure salient property ofthe alignment demonstrate cient means estimating 
schemes represent models images collections edges de metric edges coincide see excellent survey articles besl jain chin dyer 
smooth optimizable version metric de ned unmatched edges forthe distance matched lowe wells iii huttenlocher 
metric model comparison pose re nement 
edge metrics variety di erent lighting conditions strong assumptions arise stable changes lighting models described collection edges 
clearly smoothly curved objects real problem techniques 
alluded wells performed number attempts edges extracted varying lighting 
general moderately curved objects number unstable unreliable edges problematic 
faces cars fruit objects proven di cult edges 
direct techniques build models 
generally approaches revolve image object model 
objects need edges represented way taken deal changes lighting pose 
turk large collection face images train system construct representations lighting pose turk 
representations projection largest eigenvectors distribution images collection 
system addresses problem recognition alignment result ofthe emphasis ofthe results di erent 
instance clear variation pose handled system 
see straightforward extension similar eigenspace tothe problem pose re nement 
related note shashua shown images di erent lighting lambertian surface linear combination images 
parallel geometrical alignment ai tr shashua 
bears clear relation eigenvectors set images object span dimensional space 
entropy increasing role eld neural networks 
know onthe alignment images information vision problems 
ofthese techniques uses non parametric scheme density entropy estimation 
cases distributions assumed binomial gaussian 
simpli es limits approaches 
linsker concept information maximization motivate development inthe primary visual cortex linsker 
able predict development receptive elds reminiscent ofthe ones inthe primate visual cortex 
uses gaussian model forthe signal noise 
becker hinton maximization mutual information framework learning di erent low level processing algorithms curvature estimation becker hinton 
assume signals mutual informa tion gaussian 
addition joint information images information wish extract train disparity detectors random dot stereograms 
measure information separate signals linearly mixed bell sejnowski 
technique assumes di erent mixed signals carry little mutual information 
distribution particular functional form assume distribution matched preselected transfer function 
example gaussian matched logistic function applying positioned scaled logistic function results distribution 
geometrical alignment emma bears adjusting geometrical alignment 
similarities may seen revisiting entropy derivative equation comparing derivative ofthe construct 
paul viola chapter 
de ned half averaged distance values inb nearest correspondences ina min zi zi zj locally away discontinuities derivative ofthe expression dt min zj adt zi zj comparing expression equation analogy 
ifthe adjusted reduce averaged squared di erences points counterparts nearest signal value reduction entropy obtained 
entropy clusters signal value tighter nearby signal di erences smaller 
approximation analogy due dissimilarity max softmax 
equation essentially measure chamfer matching techniques asthe method described borgefors borgefors 
huttenlocher huttenlocher related measure feature matching applications hausdor distance uses maximum appears equation 
similarity geometrical matching stronger softmax operation weight closest simply selecting closest wells wells iii wells iii 
methods aggregate geometrical distance measuring signal values typically intensities surface properties 
appendix appendix gradient descent number problems described thesis nd evaluation function 
examples include nding parameters density sothat likelihood sample maximized nding pose parameters align model best nding weights network sothat best 
case parameters setf value maximized minimized 
parameters continuous variables faced possible solutions 
gradient descent procedure ective greedy technique searching space 
closely related gradient descent algorithms 
simplest steepest descent hill climbing 
starting initial guess parameters steepest descent procedure partial construct improved estimate parameters 
parameter updated update rate learning rate chosen carefully 
paul viola appendix appendix su ciently small taylor expansion prove take arbitrarily long 
chosen converge maximum relatively rapidly 
gradient techniques speed secondorder techniques newton raphson derivatives re estimate conjugate gradient techniques attempt directions gradient 
case careful theoretical advantages algorithm outweighed costs computing 
researchers neural networks problems di cult realize actual improvement convergence speed 
problems steepest descent works complex techniques large number parameters computing second derivatives quite expensive 
bibliography anderson rosenfeld editors 
neurocomputing foundations research 
mit press cambridge 
rota 

theory probability 
mit course notes 
becker hinton 

learning coherent predictions domains discontinuities 
moody hanson lippmann editors advances neural information processing volume denver 
morgan kaufmann san mateo 
bell sejnowski 

information maximisation approach separation 
advances neural information processing volume denver 
morgan kaufmann san francisco 
besl jain 

dimensional object recognition 
computing surveys 
bezdek hall clarke 

review image segmentation techniques pattern recognition 
medical physics 
bienenstock cooper munro 

theory development selectivity orientation speci binocular interaction visual cortex 
journal neuroscience reprinted anderson rosenfeld 
borgefors 

hierarchical chamfer matching parametric edge matching algorithm 
ieee transactions pami 
bridle 

training stochastic model recognition algorithms networks lead information estimation parameters 
touretzky editor advances neural information processing pages 
morgan kaufman 
canny 

computational approach edge detection 
ieee transactions pami pami 
paul viola bibliography chin dyer 

model recognition robot vision 
computing surveys 
cover thomas 
elements information theory 
john wiley sons 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
duda hart 

pattern classi cation scene analysis 
john wiley sons 
fukunaga 

statistical pattern recognition 
harcourt brace jovanovich 
grimson lozano perez wells 

automatic registration method image guided surgery visualization 
proceedings computer society conference computer vision pattern recognition seattle wa 
ieee 
haykin 

neural networks comprehensive foundation 
macmillan college publishing 
hill hawkes 

voxel similarity measures automated image registration 
proceedings third conference visualization biomedical computing pages 
spie 
horn 

robot vision 
mcgraw hill new york 
huttenlocher kedem sharir sharir 

upper envelope voronoi surfaces 
proceedings seventh acm symposium computational geometry pages 
intrator cooper 

objective function formulation bcm theory visual cortical plasticity statistical connections stability conditions 
neural networks 
jacobs jordan nowlan hinton 

adaptive mixtures local experts 
neural computation 
jones poggio 

model matching drawings linear combinations prototypes 
proceedings international conference computer vision 
kirkpatrick gelatt vecchi 

optimization simulated annealing 
science 
bibliography ai tr leclerc 

constructing simple stable descriptions image partitioning 
proceedings image understanding workshop 
linsker 

basic network principles neural architecture 
proceedings national academy sciences usa 
linsker 

self organization perceptual network 
ieee computer pages 
ljung 

theory practice identi cation 
mit press 
lowe 

perceptual organization visual recognition 
kluwer academic publishers 
meyer williams 

polynomial modeling reduction rf body coil spatial inhomogeneity 
ieee trans 
med 
imaging 
mellor 

realtime camera calibration enhanced reality visualization 
computer vision virtual reality robotics medicine pages 
nice france 
papoulis 

probability random variables stochastic processes 
mcgraw hill third edition 
rissanen 

modeling data description 
automatica 
robbins 

stochastic approximation method 
annals mathematical statistics 
schraudolph sejnowski 

unsupervised discrimination clustered data optimization binary information gain 
hanson cowan giles editors advances neural information processing volume pages denver 
morgan kaufmann san mateo 
shannon 

mathematical theory communication 
bell systems technical journal 
shashua 

geometry photometry visual recognition 
arti cial intelligence laboratory ai tr 
turk pentland 

face recognition eigenfaces 
proceedings computer society conference computer vision pattern recognition pages maui hawaii 
ieee 
paul viola bibliography wells iii 

posterior marginal pose estimation 
proceedings image understanding workshop pages 
morgan kaufmann 
wells iii 

statistical object recognition 
mit department electrical engineering computer science cambridge mass mit ai laboratory tr 
wells iii grimson kikinis jolesz 

statistical gain correction segmentation mri data 
proceedings computer society conference computer vision pattern recognition seattle wash ieee submitted 
wells iii viola 

multi modal volume registration maximization mutual information 
preparation 
widrow ho 

adaptive switching circuits 
ire convention record volume pages 
ire new york 
william press brian flannery 

numerical recipes art scienti computing 
cambridge cambridge england second edition edition 

