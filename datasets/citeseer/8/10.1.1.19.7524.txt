modular neural network architecture additional generalization abilities large input vectors albrecht schmidt intelligent systems group department computing manchester metropolitan university intelligent systems group department computing manchester metropolitan university proposes layer modular neural system 
basic building blocks architecture multilayer perceptrons trained backpropagation algorithm 
due proposed modular architecture number weight connections fully connected multilayer perceptron 
modular network designed combine different approaches generalization known connectionist logical neural networks enhances generalization abilities network 
architecture introduced especially useful solving problems large number input attributes 
multilayer perceptron mlp trained backpropagation bp algorithm solve real world problems prediction recognition optimization 
input dimension small network trained quickly 
large input spaces performance bp algorithm decreases 
cases difficult find parameter set leads convergence acceptable minimum 
difficult find useful solution especially recognition large input spaces common 
lot research done overcome problems ideas include modularity basic concept 
locally connected adaptive modular neural network described 
model employs combination bp training winner take layer 
modular neural system self organizing map multilayer perceptron 
applied cosmic ray space experiment 
modular neural network proposed enhance generalization ability neural email hydra informatik uni ulm de email doc mmu ac uk networks high dimensional inputs 
network consists mlps 
modules trained bp algorithm 
number proposed architecture significantly smaller comparable monolithic network 
modular architecture introduced training algorithm operation network described experiments 
network architecture proposed network system consists layer input modules additional decision module 
sub networks mlps 
input variable connected input modules 
connections chosen random 
outputs input modules connected decision network 
structure depicted 
parameters assumed dimension input vector number classes design issues select number inputs module layer decision determines number input modules 
assumed case spare inputs may connected constant inputs size networks may altered 
network layer dlog ke outputs 
required number represent classes binary code 
decision network dlog ke inputs 
number outputs neuron class 
number weights fully connected monolithic mlp number hidden neurons 
training system training occurs stages 
modules trained backpropagation algorithm 
phase sub networks input layer trained 
training set sub network binary code class binary code class input mlp mlp binary code class class class class decision input input mlp mlp fig 
multiple neural network architecture 
selected original training set 
training pair single module consists components original vector connected particular network input vector desired output class represented binary coding 
input modules trained parallel easily mutually independent 
second stage decision network trained 
training set decision module built output input layer original class number 
calculate set original input pattern applied input layer resulting vector desired output class represented coding form training pair decision module 
original training set ith component jth input vector class number number training instances 
module mlp connected deltan deltan deltan training set network mlp deltan deltan deltan bin mapping performed input layer phi nm 
ke training set decision network phi bit mapping decision network psi ke 
layer networks decision network input matrix mlp decision mlp mlp mlp fig 
example architecture 
calculation output mapping network phi ffi psi 
response test input determined function psi phi 
dimensional output decision module determine class number input 
experiments output neuron highest response chosen calculated class 
differences winning neuron runner may taken measure accuracy 
generalization ability generalize main property neural networks 
neural networks handle inputs learned similar inputs seen training phase 
generalization seen way reasoning number examples general case 
kind reasoning valid logical context observed human behaviour 
proposed architecture combines methods generalization 
way generalizing built mlp 
networks ability generalize input space 
type generalization common connectionist systems 
method generalization due architecture proposed network 
way generalizing similarity input patterns 
method generalization logical neural networks ff 
explain behaviour concretely simplified example recognition system 
input retina architecture shown assumed 
inputs reads continuous value zero recorded gray level black white 
simplified simplified fig 
training set 
distorted distorted distorted pattern pattern pattern fig 
test set 
network trained recognize simplified letters 
training set shown 
desired output input networks letter letter 
training subsets networks mlp mlp mlp mlp mlp mlp completing training layer networks assumed calculated output equivalent desired output resulting training set decision network phi phi training decision network assumed response system training set psi phi psi psi phi psi show different effects generalization distorted characters shown test set character tests generalization input modules second shows generalization number correct sub patterns third character example combination 
figures input vectors gray level pattern outputs taken typical neural network 
psi phi psi fig 
original distorted pictures 
psi phi psi psi phi psi experiments proposed architecture tested different real world data sets 
number input attributes 
experiment appeared modular network converged large range network parameters 
particularly huge input spaces difficult find appropriate learning coefficient monolithic network convergence problem modular structure 
time needed train modular network shorter monolithic network 
cases took half time train network similar performance 
larger input spaces training times quicker parallel training 
small input spaces attributes memorization generalization performance modular network monolithic mlp similar real world data sets 
task memorize pictures different faces 
gray level pictures size pixels continuous input variables 
original pictures 
training generalization performance tested distorted pictures 
training picture upper left picture shown 
modular network higher recognition rate manually distorted pictures 
fig 
examples noisy test pictures 
modular network structure monolithic network structure recognition testset noise level fig 
performance noisy inputs 
comparison ability recognize noisy inputs 
noise pictures generated randomly 
pictures different noise levels shown 
modular network recognize pictures significant higher noise level single mlp results shown 
experiments seen modular network superior generalization abilities high dimensional input vectors 
limitations network useful problems small input dimensions 
network ability solve problems linear separable 
proposed architecture certain theoretical limitations statistically neutral problems learned 
monolithic mlps able learn problems generalization performance poor 
usage modular architecture consisting small mlps solve real world problems demonstrated 
shown different real world data sets training easier faster modular architecture 
different approaches generalization combined model 
demonstrated results generalization advantage high dimensional input vectors 
due independence modules input layer parallel training readily feasible 
igor aleksander morton 
neural computing 
second edition 
chapman hall 
bellotti de 
signal background classification cosmic ray space experiment modular neural system 
proc 
spie international society optical engineering 
vol iss pt page 

kohonen 
statistical pattern recognition neural networks benchmarking studies 
proc 
ieee international conference neural networks 
page 
san diego 

mui agarwal gupta wang 
adaptive modular neural network application unconstrained character recognition 
international journal pattern recognition artificial intelligence 
vol iss page 
october 
picture directory 
university stuttgart 
ftp ftp uni stuttgart de pub graphics pictures rumelhart hinton williams 
learning internal representations error propagation 
rumelhart mcclelland eds parallel distributed processing 
volume foundations 
cambridge ma mit press 
stone 
artificial neural networks discover useful regularities 
artificial neural networks 
page 
conference publication iee 
june 
