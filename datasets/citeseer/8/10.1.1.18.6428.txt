maximum posteriori estimation multivariate gaussian mixture observations markov chains jean luc gauvain chin hui lee speech research department bell laboratories murray hill nj framework maximum posteriori map estimation hidden markov models hmm 
key issues map estimation choice prior distribution family specification parameters prior densities evaluation map estimates addressed 
hmms gaussian mixture state observation densities example assumed prior densities hmm parameters adequately represented product dirichlet normal wishart densities 
classical maximum likelihood estimation algorithms forward backward algorithm segmental means algorithm expanded map estimation formulas developed 
prior density estimation issues discussed classes applications parameter smoothing model adaptation experimental results illustrating practical interest approach 
adaptive nature bayesian learning shown serve unified approach wide range speech recognition applications 
estimation probabilistic function markov chain called hidden markov model hmm usually obtained method maximum likelihood ml assumes size training data large provide robust estimates 
investigates maximum posteriori map estimation continuous density hidden markov models 
derivations straight extended subcases discrete density hmm tied mixture hmm 
map estimate seen bayes estimate vector parameter loss function specified 
map estimation framework provides way incorporating prior information training process particularly useful dealing problems posed sparse training data ml approach gives inaccurate estimates 
map estimation applied classes applications parameter smoothing model adaptation related problem parameter estimation sparse training data 
sample denotes set observation vectors independent identically distributed drawn probabilistic function markov chain 
done jean luc gauvain leave speech communication group limsi cnrs orsay france 
article submitted ieee trans 
speech audio published april 
difference map ml estimation lies assumption appropriate prior distribution parameters estimated 
assumed random vector values space theta parameter vector estimated sample probability density function deltaj prior map estimate map defined mode posterior denoted map argmax jx argmax xj assumed fixed unknown knowledge equivalent assuming non informative prior improper prior constant 
assumption equation reduces familiar ml formulation 
map formulation key issues remain addressed choice prior distribution family specification parameters prior densities evaluation maximum posteriori 
problems closely related appropriate choice prior distribution greatly simplify map estimation process 
similar ml estimation map estimation relatively easy family ff deltaj thetag possesses sufficient statistic fixed dimension parameter xj factored terms xj jt independent jt kernel density function depends sufficient statistic 
case natural solution choose prior density conjugate family fk deltaj oeg includes kernel density deltaj 
map estimation reduced evaluation mode posteriori density jt problem identical ml estimation problem finding mode kernel density 
distribution families interest exponential families sufficient statistic fixed dimension 
sufficient statistic fixed dimension map estimation ml estimation difficult problem posterior density expressible terms fixed number parameters maximized easily 
finite mixture densities hidden markov models lack sufficient statistic fixed dimension due underlying hidden process state mixture component state sequence markov chain hmm 
cases ml estimates usually obtained expectation maximization em algorithm 
hmm parameter estimation algorithm called baum welch algorithm 
em algorithm iterative procedure approximating ml estimates general case models involving incomplete data 
locally maximizes likelihood function observed incomplete data 
algorithm exploits fact complete data likelihood simpler maximize likelihood incomplete data case complete data model sufficient statistics fixed dimension 
noted dempster em algorithm applied map estimation 
remainder organized follows 
hmm estimation types random parameters commonly involves parameters follow multinomial densities involves parameters multivariate gaussian densities 
section choice prior article submitted ieee trans 
speech audio published april 
density family addressed shown prior densities hmm parameters adequately represented product dirichlet densities normal wishart densities 
sections derive formulations map estimation multivariate mixture gaussian densities mixture gaussian state observation densities 
section important issue prior density estimation discussed 
experimental results illustrating practical interest approach section bayesian learning shown unified approach variety applications including parameter smoothing model adaptation 
findings summarized section 
choices prior densities section choice prior density family addressed 
sample observations drawn mixture dimensional multivariate normal densities 
joint specified equation xj jm 
mk parameter vector denotes mixture gain th mixture component subject constraint 
xjm th normal density function denoted xjm jr exp gamma gamma gamma dimensional mean vector theta precision matrix stated sufficient statistic fixed dimension exists parameter vector equation joint conjugate prior density specified 
finite mixture density interpreted density associated statistical population mixture component populations mixing proportions 

words xj seen marginal joint parameter expressed product multinomial density sizes component populations multivariate gaussian densities component densities 
consider mixture gains mixture density joint distribution form multinomial distribution 
practical candidate model prior knowledge mixture gain parameter vector conjugate density dirichlet density 
gamma term denote joint marginal cause confusion 
jrj denotes determinant matrix denotes transpose matrix vector tr denote trace matrix precision matrix defined inverse covariance matrix 
article submitted ieee trans 
speech audio published april 
parameters dirichlet density 
vector parameter individual gaussian mixture component joint conjugate prior density normal wishart density form ff jr ff gammap exp gamma gamma gamma exp gamma tr ff prior density parameters ff gamma vector dimension theta positive definite matrix 
assuming independence parameters individual mixture components set mixture weights joint prior density product prior defined equations 
shown choice prior density family justified noting em algorithm applied map estimation problem prior density belongs conjugate family complete data density 
map estimates gaussian mixture em algorithm iterative procedure approximating ml estimates context cases mixture density hidden markov model estimation problems 
procedure consists maximizing iteration auxiliary function defined expectation complete data log likelihood log yj incomplete data current fit log yj jx mixture density complete data likelihood joint likelihood unobserved labels referring mixture components 
em procedure derives facts log xj gamma log yjx jx value satisfies xj xj 
follows iterative procedure estimate mode posterior density maximizing auxiliary function log iteration maximization conventional ml procedures 
mixture densities ff deltaj mixture weights 
auxiliary function takes form log article submitted ieee trans 
speech audio published april 
psi exp function maximized 
case gaussian mixture component 
define notations kt kt kt kt gamma gamma equality kt gamma gamma gamma gamma tr follows definition xj equation psi jr exp gamma gamma gamma gamma tr relations easily verified psi delta belongs distribution family delta parameters ff satisfying conditions ff ff gamma gamma family densities defined conjugate family complete data density 
mode psi delta denoted 
may obtained modes dirichlet normal wishart densities gamma gamma ff gamma gamma em reestimation formulas derived follows gamma kt gamma kt kt kt gamma kt gamma gamma gamma gamma ff gamma kt gaussian mean vectors seen new parameter estimates simply weighted sum prior parameters observed data 
development suggests em algorithm maximum likelihood estimation natural prior density article submitted ieee trans 
speech audio published april 
conjugate family complete data density conjugate family exists 
example general case mixture densities exponential families prior product dirichlet density mixture weights conjugate densities mixture components 
assumed mixture component non degenerate kt sequence random variables non degenerate lim sup kt probability 
follows converges kt probability 
applying reasoning seen em reestimation formulas map ml approaches asymptotically similar 
long initial estimates identical em algorithms map ml provide identical estimates probability 
map estimates hmm development previous section mixture multivariate gaussian densities extended case hmm gaussian mixture state observation densities 
notational convenience assumed observation states number mixture components 
consider state hmm parameter vector initial probability vector transition matrix parameter vector composed mixture parameters fw ik ik ik state sample complete data unobserved state sequence sequence unobserved mixture component labels 
joint deltaj defined gamma initial probability state ij transition probability state state ik ik ik parameter vector th normal associated state follows likelihood form xj gamma ik jm ik ik summation possible state sequences 
prior knowledge assumed alternatively parameters assumed fixed known prior density chosen form defined equation 
general case map estimation applied observation density parameters initial transition probabilities dirichlet density initial probability vector row transition probability matrix definition proposed baum observation associated markov chain states symbol produced state article submitted ieee trans 
speech audio published april 
choice follows directly derivation discussed previous section complete data likelihood satisfies sj js sj product multinomial densities parameter sets fa prior density hmm parameters satisfies relation gamma ij gamma ij fj set parameters prior density initial probabilities fj ij set parameters prior density transition probabilities fa ij defined way equation 
subsections examine ways approximating map local maximization xj sj 
solutions map versions baum welch algorithm segmental means algorithm algorithms developed ml estimation 
forward backward map estimate equation straightforward show auxiliary function em algorithm applied ml estimation log yj jx decomposed sum auxiliary functions qa independently maximized 
functions take forms fl log qa pr gamma jjx log ij pr kjx log ik ik log ij fl ik ik il il log ik ik article submitted ieee trans 
speech audio published april 
pr gamma jjx probability making transition state state time model generates fl pr ijx probability state time model generates probabilities computed em iteration forward backward algorithm 
mixture gaussian case estimating mode posterior density requires maximization auxiliary function log 
form chosen permits independent maximization parameter sets fa map auxiliary function written sum term represents map auxiliary function associated respective indexed parameter sets 
recognize form seen mixture gaussian case 
follows kt equation replaced defined fl ik ik ik il il probability state mixture component label time model generates reestimation formulas maximize 
straightforward derive formulas applying derivations mixture weights 
em iteration parameter set gamma fl gamma fl ij ij gamma ij gamma ik ik gamma ik gamma ik ik ik ik gamma ik ik gamma ik gamma ik ik ik gamma ik ik gamma ik ff ik gamma multiple independent observation sequences fx tv maximize deltaj defined equation 
em auxiliary function log log jx deltaj defined equation 
follows reestimation formulas hold summations replaced summations tv 
values fl obtained applying forward backward algorithm observation sequence 
reestimation formula initial probabilities gamma fl gamma fl article submitted ieee trans 
speech audio published april 
reestimation formulation similar equations derived 
just mixture parameter case shown map reestimation formulas approach ml ones exhibiting asymptotical similarity estimates 
reestimation equations give estimates hmm parameters correspond local maximum posterior density 
choice initial estimates critical ensure solution close global maximum minimize number em iterations needed attain local maximum 
informative prior natural choice initial estimates mode prior density represents available information parameters data observed 
corresponding values simply obtained applying reestimation formulas equal observed data 
case discrete hmms possible uniform initial estimates trivial initial solution continuous density hmm case 
practice statistician adds information training process uniform manual segmentation observation sequence states possible obtain raw estimates hmm parameters direct computation mode complete data likelihood 
segmental map estimate analogy segmental means algorithm similar optimization criterion adopted 
maximizing jx joint posterior density parameter state sequence sjx maximized 
estimation procedure argmax max sjx argmax max sj refered segmental map estimate segmental means algorithm straightforward prove starting estimate alternate maximization gives sequence estimates non decreasing values sjx jx jx argmax sj argmax state sequence decoded viterbi algorithm 
maximization replaced hill climbing procedure subject constraint 
em algorithm candidate perform maximization initial estimate 
em auxiliary function log log yj jx deltaj defined equation 
straightforward show reestimation equations hold ffi gamma gamma ffi gamma fl ffi gamma ffi denotes kronecker delta function 
article submitted ieee trans 
speech audio published april 
prior density estimation previous sections assumed prior density member preassigned family prior distributions defined 
strictly bayesian approach vector parameter family fg deltaj oeg assumed known common subjective knowledge stochastic process 
alternate solution adopt empirical bayes approach prior parameters estimated directly data 
estimation marginal distribution data estimated prior parameters 
fact part available prior knowledge directly incorporated model assuming parameters fixed known tying parameters 
prior distribution information reduce uncertainty training process increase robustness estimates 
contrast prior distribution deterministic prior information definition changed large amount training data available 
adopting empirical bayes approach assumed sequence observations composed multiple independent sequences associated different unknown values hmm parameters 
multiple sequence observations pair independent common prior distribution deltaj 
directly observed prior parameter estimates obtained marginal density xj defined xj xj xj 
maximum likelihood estimation xj appears difficult 
alleviate problem choose simpler optimization criterion maximizing joint maximizing marginal 
starting initial estimate hill climbing procedure obtained alternate maximization 
argmax argmax procedure provides sequence estimates non decreasing values 
solution map estimate current prior parameter obtained applying forward backward map reestimation formulas observation sequence solution maximum likelihood estimate current values hmm parameters 
noted procedure gives estimate prior parameters map estimates hmm parameters independent observation sequence finding solution equation poses problems 
due wishart dirichlet components maximum likelihood estimation density defined trivial 
second parameters needed prior density hmm problem number pairs small 
way simplify estimation article submitted ieee trans 
speech audio published april 
problem moment estimates approximate ml estimates 
problem possible reduce size prior family adding constraints prior parameters 
example prior family limited family kernel density complete data likelihood posterior density family complete data model prior information available 
doing easy show constraints prior parameters hold ik ik ff ik ik parameter tying reduce size prior family useful parameter smoothing purposes 
practical constraint impose prior mode equal parameters hmm resulting scheme model adaptation 
approach classes applications parameter smoothing adaptive learning 
parameter smoothing goal estimate abovementioned algorithm offers direct solution smooth different estimates assuming common prior density models 
adaptive learning observe new sequence observations associated unobserved vector parameter value required specification prior parameters finding map estimate obtained point estimate computed proposed iterative algorithm 
training process seen adaptation specific priori model argmax training data available specific conditions match new observation sequence experimental results applications section 
experimental results bayesian learning gaussian densities widely sequential learning mean vectors feature template recognizers see example class stern 
bayesian estimation mean vectors build codebooks hmm framework 
cases precision parameter assumed known prior density limited gaussian 
brown bayesian estimation speaker adaptation parameters connected digit recognizer 
lee investigated various training schemes gaussian mean variance parameters prior densities speaker adaptation 
showed alpha digit vocabulary small amount speaker specific data utterances word map estimates gave better results ml estimates 
theoretical developments bayesian estimation successfully applied gaussian mixture observation densities speech recognition applications parameter smoothing speaker adaptation speaker group modeling corrective training 
previously reported experimental results applications 
order demonstrate effectiveness bayesian estimation applications results article submitted ieee trans 
speech audio published april 

cases hmm parameters estimated segmental map algorithm 
prior parameters subject conditions obtained forcing prior mode equal parameters hmm 
constraints leave free parameters ik estimated algorithm described section arbitrarily fixed 
model adaptation ik regarded weight associated th gaussian state shown equations 
weight large prior density sharply peaked values seed hmm parameters slightly modified adaptation process 
conversely ik small adaptation fast map estimates depend mainly observed data 
applications discussed parameter smoothing speaker adaptation 
known hmm training requires smoothing tying particularly large number context dependent cd phone models limited amounts training data 
solutions investigated smooth discrete hmms model interpolation occurrence smoothing fuzzy vq variance smoothing proposed continuous density hmms 
shown map estimation solve problem tying parameters prior density 
performance improvement reported tying prior parameters ways 
cd model smoothing prior density cd models corresponding phone smoothing marginal prior density components mixture 
experiments darpa naval resource management rm ti connected digit corpora map estimation outperformed ml estimation error rate reductions order 
case model adaptation map estimation may viewed process adjusting seed models form specific ones small amount adaptation data 
seed models estimate parameters prior densities serve initial estimate em algorithm 
experimental results speaker adaptation example model adaptation bayesian learning demonstrated scheme sex dependent training 
experiments set context independent ci phone models model left hmm gaussian mixture state observation densities maximum mixture components state 
diagonal covariance matrices transition probabilities assumed fixed known 
details recognition system basic assumptions acoustic modeling subword units 
described dimensional feature vector composed lpc derived cepstrum coefficients second order time derivatives computed data sampled khz simulate telephone bandwidth 
table speaker adaptation map estimation compared ml training sd models set ci phone models 
map estimation speaker independent si sex dependent seed models trained standard rm si training set consisting utterances native american talkers females males providing utterances 
test material consisted rm feb sd test data testing utterances testing speakers males females 
results reported utterances equivalently minutes speech material data taken rm sd data training adaptation 
mle sd map si word article submitted ieee trans 
speech audio published april 
training min min min min mle map si map table summary sd sa si sa results feb sd test 
results word error rate 
error rates standard rm word pair grammar rows table 
mle sd word error rate minutes training data 
si word error rate minutes adaptation data somewhat comparable mle result minutes speaker specific training data 
map models seen outperform mle models relatively small amounts data training adaptation map mle results comparable available training data 
result consistent bayesian formulation map estimate mle asymptotically similar demonstrated equations 
compared si results word error reduction minutes adaptation data 
larger improvement observed female speakers male speakers presumably fewer female speakers si training data 
speaker adaptation done sex dependent seed models gender new speaker known estimated prior adaptation process 
case estimation gender dependent model set best matches gender new speaker seed model set si seed models 
results speaker adaptation sex dependent seed models third row table 
word error rate speaker adaptation 
error rate reduced minutes minutes adaptation data 
comparing rows table seen speaker adaptation effective sex dependent seed models 
error reduction minutes training data compared sex dependent model results compared si model results 
details experimental results map estimation parameter smoothing model adaptation including application speaker clustering corrective training 
map estimation applied task adaptation 
case task independent si models trained utterance general english corpus served seed models speaker task adaptation 
map estimation proposed speaker identification small amount speaker specific training data 
theoretical framework map estimation multivariate gaussian mixture density hmm gaussian mixture state observation densities 
extending known ml estimation algorithms map estimation corresponding map training algorithms article submitted ieee trans 
speech audio published april 
forward backward map estimation segmental map estimation formulated 
proposed bayesian estimation approach provides framework solve various hmm estimation problems posed sparse training data 
applied successfully acoustic modeling automatic speech recognition bayesian learning serves unified approach speaker adaptation speaker group modeling parameter smoothing corrective training 
framework adopted smoothing adaptation discrete tied mixture hidden markov models known semicontinuous hidden markov models gram stochastic language models 
baum petrie soules weiss maximization technique occurring statistical analysis probabilistic functions markov chains ann 
math 
stat vol 
pp 
baum inequality associated maximization technique statistical estimation functions markov processes inequalities vol 
pp 

brown 
lee spohrer bayesian adaptation speech recognition proc 
icassp pp 

sur les lois de estimation exhaustive acad 
sci pp 

degroot optimal statistical decisions mcgraw hill 
dempster laird rubin maximum likelihood incomplete data em algorithm roy 
statist 
soc 
ser 
pp 

duda hart pattern classification scene analysis john wiley sons new york 
large vocabulary speech recognition speaker adapted codebook hmm parameters proc 
eurospeech pp 

forney viterbi algorithm proc 
ieee vol 
pp march 

gauvain 
lee bayesian learning gaussian mixture densities hidden markov models proc 
darpa speech natural language workshop pacific grove february 

gauvain 
lee map estimation continuous density hmm theory applications proc 
darpa speech natural language workshop arden house february 

gauvain 
lee bayesian learning hidden markov model gaussian mixture state observation densities speech communication vol 
nos 
june 
article submitted ieee trans 
speech audio published april 

hon vocabulary independent speech recognition system ph 
thesis school computer science carnegie mellon university pittsburgh pa march 
johnson kotz distribution statistics john wiley sons new york 

juang maximum likelihood estimation mixture multivariate stochastic observations markov chains technical journal vol 
july august 

juang rabiner segmental means algorithm estimating parameters hidden markov models ieee trans 
acoust speech signal processing vol 
assp september 
koopman distributions admitting sufficient statistic trans 
am 
math 
soc vol 
pp 

lamel 
gauvain cross lingual experiments phone recognition appear proc 
ieee icassp 

lee rabiner pieraccini acoustic modeling large vocabulary speech recognition computer speech language pp 


lee 
lin 
juang study speaker adaptation parameters continuous density hidden markov models ieee trans 
acoust speech signal processing vol 
assp pp 
april 

lee rabiner pieraccini rosenberg improved acoustic modeling large vocabulary continuous speech recognition computer speech language pp 


lee 
gauvain speaker adaptation map estimation hmm parameters appear proc 
ieee icassp 
maximum likelihood estimation multivariate observations markov sources ieee trans 
inform 
theory vol 
pp 
september 
price fisher bernstein pallett database recognition word domain proc 
icassp new york pp 
april 
probability theory springer verlag 
rabiner 
juang segmental means training procedure connected word recognition tech 
vol 
pp 
may 
rao linear statistical inference applications nd edition john wiley sons new york 
article submitted ieee trans 
speech audio published april 
redner walker mixture densities maximum likelihood em algorithm siam review vol 
pp 
april 
robbins empirical bayes approach statistical decision problems ann 
math 
statist vol 
pp 

stern dynamic speaker adaptation feature isolated word recognition ieee trans 
assp vol 
assp june 
class learning procedure speaker dependent word recognition systems sequential processing input tokens proc 
icassp pp 
boston 
article submitted ieee trans 
speech audio published april 

