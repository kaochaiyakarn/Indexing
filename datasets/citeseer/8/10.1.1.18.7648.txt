investigations joint multigram models grapheme phoneme conversion ney lehrstuhl fur informatik vi computer science department rwth aachen university technology aachen germany informatik rwth aachen de fully data driven language independent way building grapheme phoneme converter 
apply joint multigram approach alignment problem standard language modelling techniques model transcription probabilities 
study model parameters training procedures effects corpus size detail 
experiments conducted english german pronunciation lexica 
proposed training scheme performs better previously published ones 
phoneme error rates low english german achieved 

task grapheme phoneme conversion phonetic transcription formalized bayes decision rule argmax means orthographic form sequence letters seek pronunciation phoneme sequence grapheme phoneme conversion neglected alignment problem 
popular approach handcrafted rules align letters phonemes 
alignment produced machine learning techniques applied perform actual mapping 
developing grapheme phoneme conversion system new language inconvenient write alignment rules hand 
doing just alignment give acceptable results 
alignments inferred joint multigram models approach pioneered deligne bimbot 

joint multigram models convenience reader provide brief review joint multigram model context grapheme conversion 
grapheme phoneme joint multigram short pair letter sequence phoneme sequence possibly different length 
expressions gq refer second component respectively 
joint multigram model assume word orthographic form pronunciation generated common sequence 
example pronunciation speaking may regarded sequence speaking spi kin ea ing segmentation may unique 
joint probability determined summing matching sequences ql set joint segmentations 


ql joint probability distribution reduced probability distribution sequences model standard gram jq positions virtually understood contain special boundary symbol allows modelling characteristic phenomena word starts ends 

jq 

training training sample gn parameter estimation performed separate phases 
phase set inferred unigram statistics 
resulting unigram model corpus stream argmax segmented corpus second phase train gram model jq standard techniques 
bi trigram models absolute discounting estimating discount parameters leaving 
integrated optimization gram probabilities possible principle tried 
focus inference multigram set training unigram probabilities 

maximum likelihood training maximum likelihood training performed expectation maximization em algorithm 
case unigrams identify model uni 
re estimation equations updated parameters jqj nq nq nq number occurences quantity call evidence expected number occurences training sample current set parameters 
evidence calculated efficiently forward backward procedure 
obviously equations permit new emerge probability zero 
initialize model parameters assigning uniform distribution satisfying certain manually set length constraints 
notation jg min max indicate min max letters considered rmin rmax likewise number phonemes 

evidence trimming satisfying length constraints helpful transcription task 
contrary receive negligibly small probabilities see smaller inventories generally yield better results 
obtain reasonably sized models apply thresholding evidence values equation call procedure evidence trimming find causes gradually die iteration process 
implicit trimming caused limited machine precision 
evidence trimming superior model trimming similar thresholding applied probability estimates low probabilities conditional probability certain words trimming leave training sample representable model 

training maximum approximation earlier experiment joint multigram approach maximum approximation training 
tried strategy 
earlier called viterbi training sensitive initialization careful selection trimming thresholds 
particular necessary initialize unigram probabilities proportional occurence counts equivalent setting equation 

transcription producing phonemic transcription orthographic form restrict maximum approximation max ql means look sequence matching spelling project phonemes 
performed straight forward implementation zero rest cost term 

experiments conducted experiments german english transcription task constructed available pronunciation dictionaries 
english celex lexical database english version 
phrases abbreviations removed 
words converted lower case resulting usual grapheme symbols 
phoneme set consists symbols vowels vowels consonants syllabic consonants extremely rare 
preprocessed database contains word forms 
german bielefeld lexicon database vm ii version 
preprocessing steps included removal hyphenated compounds abbreviations pronunciation variants 
words converted lower case resulting grapheme symbols including sz 
phoneme set consists symbols vowels consonants 
preprocessing word forms 
database randomly selected evaluation test set words training set words disjoint course 
details corpus sizes table 
performance measured phoneme error rate levenshtein distance automatic transcription result pronunciation divided number phonemes pronunciation 
table 
statistics corpora german celex english train eval train eval words graphemes phonemes minimum length letter phoneme experiments 
maximum length tried minimum number insert delete substitute operations required transform sequence 
table 
selected results trimming words training sets english length constraints jqj phoneme error rate jg german length constraints jqj phoneme error rate jg combinations length constraints symbols sides 
experimented setting trimming threshold resulting model affected value couple iterations 
iterations increased speed convergence changing result significantly 
series tests cf 
table conducted call marginal trimming starting small values increased gradually factor iterations maximum value 
additional test higher constant thresholds cf 
table 
see performance affected amount training data available repeated experiments training sets words cf 
table 

results discussion summary phoneme error rates lower german task spelling closer pronunciation english 
interestingly number inferred multigrams jqj smaller german 
apart result structurally similar 
best phoneme error rate obtained marginal trimming german english quite competitive simplicity model 
large error rates experiments provide direct comparison methods get rough idea reports mapping accuracy english task words training 
reports phoneme error rate german task words training 
please keep mind conditions studies possibly harder 
table 
results differently sized training sets trimming 
best unigram trigram results shown english training length constraints set jqj german training length constraints set jqj length restricted letter proves importance proper alignment model 
unigram model error rates decrease longer longer considered 
find unigram case marginal trimming yields best results cases 
higher gram model picture clear hand longer cover larger context 
hand larger allowed sizes imply model handle larger number symbols naturally leads sparseness problems 
bigram trigram error rates go lengths increased respectively 
applying stronger trimming generally negative effect unigram error rate effective restricting size model consequently keeping bi trigram error rates low cf 
fig 

optimizing trigram phoneme error rate improve best results marginal trimming strategy cases cf 
table 
reducing amount training data observe longer harder estimate reliably 
optimal length restrictions decrease cf 
table 

maximum approximation training causes infrequent die quickly quickly making algorithm prone local optima 
careful evidence trimming needed te achieve performance 
unigram case true em summation consistently superior viterbi training cf 
table additional having optimize trimming parameters 
trigram case em algorithm summation slightly superior looses additional advantage cf 
table 
apply strong trimming cases avoid sparseness problems 
gram training resort maximum approximation anyway 
phoneme error rate number evidence trimming threshold model size unigram error rate bigram error rate trigram error rate fig 

effect evidence trimming threshold model size error rates different gram models english training set length constraints table 
comparison unigram results viterbi em training training set trimming optimized viterbi english length constraints viterbi em jg jqj jqj german length constraints viterbi em jg jqj jqj table 
comparison trigram results viterbi em training training sets trimming optimized viterbi em training english length constraints viterbi em jg jqj jqj german length constraints viterbi em jqj jqj table 
selected results trimming optimized trigram model training sets english length constraints phoneme opt jqj error rate german length constraints phoneme opt jqj error rate 
summary outlook investigated variations multigram approach grapheme phoneme conversion 
experiments german english demonstrate performance achieved relatively simple models 
shown evaluating sum em training algorithm yields consistently better results maximum approximation allows get fewer empirical parameters 
currently train gram models separate step time resorting maximum approximation 
results indicate integrated training procedure optimizes gram probabilities boundaries simultaneously beneficial 
acknowledgments partially funded european commission human language technologies project ist 

torkkola efficient way learn grapheme rules automatically proc 
icassp minneapolis mn usa apr vol 
pp 

deligne bimbot variable length sequence matching phonetic transcription joint multigrams proc 
eurospeech madrid sep pp 

deligne bimbot inference variable length acoustic units continuous speech recognition speech communication vol 
pp 

ney martin wessel statistical language modeling leaving corpus methods language speech processing young eds pp 

kluwer 
celex lexical database www kun nl celex 
gibbon und verbmobil phase ii tech 
rep issn universitat bielefeld november 
statistical methods phoneme conversion zur sprache vienna austria sep pp 

