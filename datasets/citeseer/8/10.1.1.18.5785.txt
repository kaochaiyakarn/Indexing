neural computation vol nov 
minimax entropy principle application texture modeling song chun zhu ing wu david mumford article proposes general theory methodology called minimax entropy principle building statistical models images signals variety applications 
principle consists parts 
maximum entropy principle feature binding fusion certain set feature statistics distribution built bind feature statistics maximizing entropy distributions reproduce feature statistics 
second part minimum entropy principle feature selection plausible sets feature statistics choose set maximum entropy distribution minimum entropy 
computational inferential issues parts addressed particular feature pursuit procedure proposed approximately selecting optimal set features 
model complexity restricted sample variation observed feature statistics 
minimax entropy principle applied texture modeling novel markov random field mrf model called frame filter random field minimax entropy derived encouraging results obtained experiments variety texture images 
relationship theory mechanisms neural computation discussed 
division applied mathematics brown university providence ri department statistics harvard university cambridge ma 
article proposes general theory methodology called minimax entropy principle statistical modeling variety applications 
section intro duces basic concepts minimax entropy principle discussion motivation theory brief review relevant theories methods previously studied literature 
motivation goal variety disciplines ranging computational vision pattern recognition image coding psychophysics important theme pursue probability model characterize set images signals posed statistical inference problem assumed exists joint probability distribution density image space concentrate subspace corresponds ensemble images application objective estimate set observed training images 
plays significant roles areas visual coding goal take advantage regularity re input images produce compact coding scheme 
involves measuring efficiency coding schemes terms entropy watson barlow computation entropy choice optimal coding schemes depend estimation underlying probability distribution 
example kinds coding schemes compared field compact coding sparse coding 
assumes gaussian distributions assumes non gaussian ones 

pattern recognition neural networks statistical decision theory needs find probability model category images similar patterns 
accurate estimation key factor successful classification recognition 
computational vision adopted prior model terms bayesian theory provides language visual computation ranging images segmentation scene understanding zhu 
example image restoration surface reconstruction geman geman blake zis simple model embodies common features statistics natural looking images instance natural images adjacent pixels similar intensity values bias vision algorithm able features noises 
texture modeling objective estimate probability model set texture images perceptually similar texture appearances 
important texture analysis texture seg mentation texture classification plays role texture synthesis texture images synthesized drawing samples 
furthermore find ing simple distributions characterize textures helps understand mechanisms human texture perception julesz 
making inference challenging learning problems neural networks dayan xu reasons 
dimension image space overwhelmingly large compared number available training examples 
texture modeling instance size images pixels probability distribution function variables access training images 
inappropriate non parametric inference methods kernel methods radial basis functions see ripley mixture gaussian models jordan jacobs 
secondly far gaussian popular dimen sion reduction techniques principal component analysis jolliffe histogram intensity difference adjacent pixels gaussian curve dashed mean variance domain 
histogram filtered texton image solid curve histogram filtered noise image dotted curve 
spectral analysis priestley appear directly applicable 
illustration non gaussian property shows empirical marginal distribution histogram intensity differences horizontally adjacent pix els natural images zhu mumford 
comparison gaus sian distribution mean variance plotted dashed curve 
similar non gaussian properties observed field 
example shown solid curve histogram texton image shown filter texton see section details 
clear solid curve far gaussian comparison dotted curve histogram white noise image 
key issue building statistical model balance generality simplicity model include rich structures adequately describe real world images capable modeling complexities due high dimension ality non gaussian property time simple computationally feasible give simple explanation observe 
reduce complexity necessary impose structures distribution 
previous methods past mainly methods adopted applications 
method adopts parametric markov random field mrf models forms gibbs distributions 
example general smoothness models image restoration geman geman mumford shah conditional auto regression models texture modeling besag cross jain 
method involves small number parameters constructs concise distributions images 
achieve adequate generality reasons 
mrf models afford small cliques number parameters explode small cliques hardly capture image features relatively large scales 
second potential functions limited prespecified forms practice desirable forms distributions determined learned observed images 
second method widely visual coding image reconstruction high dimensionality problem avoided representing images relatively small set feature statistics usually extracted set selected filters 
examples filters include frequency orientation selective gabor filters daugman designed model cells mammalian visual cortex wavelet pyramids various coding criteria mallat simoncelli adelson coifman wickerhauser donoho johnstone 
feature statistics extracted certain filter usually histogram filtered images 
histograms pattern classifi cation recognition visual coding watson donoho johnstone 
despite excellent performances method major problems solved 
feature binding feature fusion problem set filters histograms integrate single probability distribution 
problem difficult filters linear independent 
second problem feature selection model complexity choose set filters features best characterize images modeled 
theory methodology minimax entropy principle proposed building statistical models provides new strategy balance model generality model simplicity seemingly contrary criteria maximizing entropy minimizing entropy 

maximum entropy principle jaynes 
loss generality features image expressed dex features vector valued functions image intensities 
statistic feature ef expectation respect estimated sample mean feature computed training images 
model constructed re produce feature statistics observed ep ef model satisfying constraints maximum entropy principle favors simplest sense maximum entropy 
entropy measure randomness maximum entropy model considered simplest fusion binding features statistics 
ii 
minimum entropy principle 
distribution constructed depends features selected goodness measured kullback leibler divergence kullback leibler 
show section divergence constant equal entropy estimate closely need minimize entropy distribution means features possible specify 
sense minimum entropy principle favors model generality 
cases model complexity number features fixed computational reasons minimum entropy principle provides criterion selecting features best characterize 
computational procedures proposed parameter estimation feature selection model complexity studied presence sample variations feature statistics 
example application minimax entropy principle applied texture modeling features extracted filters selected general filter bank feature statistics empirical marginal distributions usu ally reduced histograms filtered images 
resulting model called frame filters random fields minimax entropy new class mrf model 
compared previous mrf models frame model employs enriched vocabulary enjoys stronger descriptive ability time model complexity check small set filters modeling certain texture 
texture images synthesized drawing samples estimated models correctness estimated models verified checking synthesized texture images similar visual appearances observed images 
rest arranged follows 
section devoted formal study minimax entropy principle greedy algorithm feature se lection proposed 
section applies minimax entropy principle texture modeling 
section consists experiments modeling variety textures 
section concludes brief discussion 
minimax entropy principle fix notation image defined domain lattice point interval real line set integers 
assumed observed images rs random sample probability distribution density defined image space vl size image domain 
objective estimate observed images 
maximum entropy principle initial stage studying regularity variability observed images rs starts exploring essential features characteristic observations 
loss generality features defined index features vector valued function intensities image statistics features estimated sample means large sample effect takes place usually necessary condition reasonable esti modeling sample averages mates expectations ef ef denotes ex respect 
call observed statistics expected statistics 
approximate probability model restricted reproduce observed statistics ep obs ep qb set distributions reproduce observed features need select provided 
concerned far observed feature statistics distributions explain equally distinguishable 
maximum entropy principle jaynes suggests choose achieves maximum entropy obtain purest simplest fusion observed features statistics 
underlying philosophy satisfies constraints dimensions random smooth possible unconstrained dimensions represent information available sense principle called minimum prejudice principle 
problem constrained optimization problem arg max logp di subject ep di di 
application lagrange multipliers known solution gibbs distribution form exp vector dimension denotes inner product parameter exp di partition function normalizes probability distribution 
equation specifies simple parametric model parameter solved satisfies constraints obs 
estimation computation computed equation maximum likelihood estimate logp ivs log likelihood function mle properties 
property 

ep 
vc 
property 
obs gradient ascent maximizing log likelihood gives equation solving iteratively da dt ep ors converges equation follows property 
property means hessian matrix covariance matrix positive definite condition usually satisfied 
strictly concave respect solution uniquely exists 
step equation computation ep general difficult adopt stochastic gradient method younes approxi sy marion 
fixed synthesize typical images drawing samples gibbs sampler geman geman markov chain monte carlo mcmc methods see 

ta approximate ep sample means ep iterative equation computing da dt accuracy approximation equation sample size large 
data flow parameter estimation shown details algorithm zhu wu mumford 
minimum entropy principle suppose sample size large expected feature statistics ef qb estimated exactly neglecting estimation errors observed statistics 
distribution computed reproduces expected feature statistics ep ef goal inference underlying distribution goodness model measured kullback leibler kullback leibler divergence kl log fi di ei log ei logp 
kl 
theorem zn notation kl entropy entropy 
see appendix proof 
result entropy fixed entropy de set features qb 
included distribution 
minimizing kl equivalent minimizing entropy 
call minimum entropy principle intuitive interpretations 
information theory defines optimal coding scheme assigned coding length logp shannon entropy ep logp stands expected coding length 
minimum entropy principle chooses coding system short est average coding length 
second statistics entropy negative kullback leibler divergence constant uniform distribu tion model random noise images 
minimizing entropy orderly regular possible 
phi entropy minimization information statistics observable specify 
maximum entropy principle favors simplicity minimum entropy principle emphasizes generality model 
keep model complexity check needs fix number features precise set possible features arbitrary set features 
entropy minimization provides criterion choosing optimal set features arg min entropy ps ps denotes fitted model features ep set probability distributions reproduce expected features statistics maximum entropy principle ps arg max entropy 
combining arg min max entropy 
isi pll call equation minimax entropy principle demonstrated principle consistent goal modeling finding best estimate underlying distribution 
feature pursuit enumerating possible sets features comparing entropies certainly impractical 
propose greedy procedure pursue features start empty feature set uniform word pursuit represent stepwise method distinguish selection 
distribution add model feature time added feature leads maximum decrease entropy model keep doing entropy decrease smaller certain value 
precise qb currently selected set features exp qb distribution fitted omit notational simplicity subsection 
new feature qb qb new feature set 
new distribution exp qb 
general discussion choose feature qb maximize entropy decrease remaining features qb arg max qb qb kl kl entropy entropy kl entropy decrease expressed quadratic form second order taylor expansion 
proposition notation ep ei vp ep el qb vp conditional variance qb qb distribution expected feature statistics 
see appendix proof discussion 
proposition drive feature pur suit procedure intuitive interpretation 
current model new feature qb ep qb observe ensemble governed ep qb close el qb adding new feature leads little improvement estimating 
look salient new feature qb el qb different ep qb including qb new model better approximation 
saliency new feature measured qb ep qb el qb scaled vp vp variance new feature compensated dependence new feature old ones 
practically approximately compute vp replacing current model furthermore feature statistics qb scale histograms texture modeling section may simplify measure saliency norm distance practice estimated ii respectively sample statistics averaged observed obs syn images synthesized images discussed equation 
measure texture experiments section 
summary illustrates data flow computation model pursuit features 
estimation error model complexity subsection concerns corrections minimum entropy principle feature pursuit procedure presence estimation error training images 
reader interested technical details may skip subsection 
bs sampling 

tobs 


obs obs estimate 

yn 

image analysis 
true distribution 
image synthesis estimate syn syn syn 
sampling model mcmc data flow algorithm model estimation feature selection 
previous subsections set features qb stud ied distributions 
reproduces observed feature statistics ep qb obs reproduces expected feature statistics ep qb ef qb previous derivations assume ef qb esti mated exactly observed statistics true practice finite sample observed 
estimation errors account need correct minimum entropy principle feature pursuit procedure 
consider minimum entropy principle relates kullback leibler divergence kl entropy model 
practice estimated goodness model measured kl kl shown proposition notation kl kl kl 
see appendix proof 
estimation error come close extra noise measured kl increases model complexity 
fact model random variable depending random sample ivs kl 
stands expectation respect training images applying eo sides equation eo kl kl kl entropy entropy kl 
proposition relates entropy entropy 
proposition notation entropy eo entropy eo kl 
see appendix proof 
proposition entropy average smaller entropy estimated specific training data better job fitting training data 
combining equation equation eo kl eo entropy entropy 
correction terms kl eo kl fi 
ripley section trace vf var vp 
vp estimated observed images synthesized images respec tively 
vp approximately number free parameters model model complexity divided 
form akaike information criterion akaike eo eo entropy entropy trace vp drop higher order term 
equation leads correction minimum entropy principle 
entropy ps fi trace chooses optimal set features possible decrease entropy including features balanced second term mea sures model complexity 
provides reason restricting model complexity scientific parsimony computational efficiency 
issue minimum description length mdl principle rissanen 
corrected version minimum entropy principle leads corrected version feature pursuit procedure step decrease entropy introducing new feature penalized increase model complexity 
entropy decrease approximated quadratic form simply norm distance increase model complexity roughly measured number free parameters brought new feature 
feature pursuit procedure stops soon entropy decrease compensate increase model complexity 
homogeneous image features qb ex pressed average local features pixels function defined local windows centered iv small large sample effects take place image large asymptotic studies conducted situation 
complicated phase transition shall pursue article 
application texture modeling section applies minimax entropy principle texture modeling 
general problem texture important characteristic surface property visual scenes power cue visual perception 
general model textures long sought computational vision psychology model far achieved vast diversity physical chemical processes generate textures large number attributes need considered 
illustration diversity textures displays typical texture images 
typical texture images 
existing models textures roughly classified categories 
dynamic equations replacement rules simulate specific physical chem ical processes generate textures witkin kass picard 
kth order statistics model texture perception famous julesz conjecture julesz 
markov random field models 
discussion previous models methods reader referred zhu wu mumford 
method texture considered ensemble images similar texture appearances texture ensemble governed probability distribution defined random field discussed section 
objective texture modeling estimate building model set observed images 
model consistent human texture perception sense estimates closely sample images perceptually similar training images 
choosing features statistics apply minimax entropy principle texture modeling step choose features statistics obs loss generality features texture images extracted filters linear nonlinear function intensities image denote filter response point iw function depending intensities inside window centered compute histogram filtered image features texture modeling notation replaced idi dirac point mass function concentrated 
correspondingly observed statistics ob defined luo theory continuous functions practice approximated piecewise constant functions finite number bins taken dimensional vectors rest 
definitions considered obs vectors infinite dimensions 
sample size large images 
bs large large sample effect takes place ergodicity obs close estimate marginal distributions es 
motivations choose histograms filtered images feature statistics 
comes mathematical result 
theorem continuous probability distribution texture image determined marginal distributions 
see appendix proof 
choose lo es observed statis tics distribution ep ey possible 
involve uncountable number filters filter big image psychophysical research human texture perception suggests homogeneous textures difficult discriminate pro duce similar marginal distributions responses bank filters bergen adelson landy 
means plausible ignore statistical properties important human texture dis 
motivated psychophysical research assumptions limit number filters window size filter computational reason assumptions necessary conditions theory hold true 

limit model homogeneous textures stationary respect location 
features concern texture perception captured locally supported filters 
locally mean sizes filters smaller size image 
example size image pixels window sizes filters limited pixels 

finite set filters 
access training texture images assumption enable ergodicity takes effects observed histograms filter images reasonable estimates marginal distributions 
substituting equation obtain exp call frame filter random field minimax entropy model 
angle brackets indicates sum bin computation parameters selection filters proceed described section 
detailed analysis texture modeling algorithm referred zhu wu mumford 
frame new class mrf models section derive continuous form frame model equation compare existing mrf models 
histograms image continuous functions con straint optimization problem ep ff 
application lagrange multipliers maximizing entropy constraints gives exp dz exp 
continuous variable infinite number constraints lagrange multipliers take form dimensional potential functions 
specifically filters linear rewrite equation exp clearly equation markov random field mrf models equivalently gibbs distributions 
previous mrf models po built directly filter response cliques forms potential functions learnt training images incorporate high order statistics model non gaussian properties images 
frame model stronger expressive power existing mrf models pair cliques time complexity model check filter introduces number parameters regard window size enables explore structures large scales pixel filters modeling fabric texture section 
easy show existing mrf models texture special cases frame model filters potential functions specified 
detailed comparison frame model mrf models referred zhu wu mumford 
designing filter bank describe wide variety textures need specify general filter bank filters selected describing certain texture 
filter bank serves vocabulary selected filters considered words analogy language 
shall discuss rules constructing optimal filter bank kinds filters motivated multi channel filtering mechanism discovered generally accepted neurophysiology silverman 
intensity filter capturing dc component 
laplacian gaussian filters isotropic center surrounded model retinal ganglion cells 
impulse response functions form lg const 
rr controls scales filters 
choose scales 
filter scale denoted lg 
gabor filters models frequency orientation sensitive simple cells 
impulse response functions form gabor const ysi 
xsi 
controls scales controls orientations 
choose scales orientations notice filters nearly orthogonal overlap information captured 
sine cosine components denoted respectively 
non linear gabor filters models complex cells responses powers responses pair gabor filters gabor 
fact local spectrum smoothed gaussian function filters serve local spectrum analyzers 
specially designed filters texton primitives see subsection 
experiments texture modeling section describes modeling natural textures algorithm studied section texture image described details order illustrate filter pursuit algorithm works 
observed image animal fur 
starts uniform white noise image displayed 
algorithm picks filter pixels laplacian gaussian filter scale largest entropy decrease filters filters bank 
texture image synthesized matching histogram filter response shown 
comparing evident filter captures local smoothness features observed texture image 
continuing algorithm filters sequentially added respectively intensity captures features various scales orientations 
mea sure entropy decrease filters respectively texture images synthesized filters displayed 
obviously filters added synthesized texture image gets closer observed 
appears filters chosen steps contributions confirms early assumption marginal distributions small number filtered images adequate capturing essential features underlying probability distribution 
scene mud ground scattered animal footprints filled water get brighter 
texture image shows sparse features 
synthesized texture image filters 
image taken skin cheetah displays synthesized texture filters 
may notice original observed texture image homogeneous shapes blobs vary systematically spatial locations left upper corner darker right lower 
synthesized texture shown elongated blobs introduced different filters bright pixels spread uniformly image due effect entropy maximization 
shows texture fabric clear periods horizon tal vertical directions 
want texture test non linear filters choose spectrum analyzers horizontal direction synthesis fur texture 
observed image synthesized images filters respectively 
observed texture mud synthesized filters observed texture image cheetah blob 
filters synthesized input image fabric synthesized image pairs gabor filters plus laplacian gaussian filter 
images sampled different steps gibbs sampler 
vertical direction periods tuned periods texture window sizes filters pixels 
intensity filter laplacian gaussian filter lg window size pixels take care intensity histogram smoothness features 
syn texture images displayed different sampling steps 
experiment shows markov chain stationary gets close stationary sampled images perceptually similar appearances different details 






typical texton images pixels 
circle cross 
synthesized images pixels 
show special binary texture images formed iden tical circles crosses studied extensively psychologists purpose understanding human texture perception 
interest see class textures modeled frame 
linear filter impulse response function pixels mask correspond ing primitive center 
filter selected frame algorithm starts uniform white noise image gradually matches histogram filter responses simulated image histogram obtained observed image 
examination plots histograms obtained observed image solid curve uniform noise im age dotted curve observe isolated peaks observed histogram set potential wells change filter response peak frame algorithm flips pixel time take long time algorithm match histograms 
experiments proposed annealing approach smooth histograms matching smoothed histogram gradually target histogram smoothed 
details heuristics referred zhu wu mumford 
synthesized images 
texture modeling various artificial categories textures respect various attributes fourier non fourier deterministic stochastic macro micro textures 
frame erases artificial boundaries characterizes unified model different filters parameter values 
recognized traditional mrf models special cases frame model stochastic non fourier micro textures 
textures synthesized evident frame capable modeling periodic deterministic textures fabric textures large scale elements fur cheetah blob textures distinguishable circles cross bars frame realizes full potential mrf models 
method texture modeling inspired bears similarities heeger bergen texture synthesis nat ural looking texture images successfully synthesized matching histograms filter responses organized form pyramid 
compared heeger bergen algorithm frame model advantages 
obtain probability model merely synthesizing texture images 
secondly monte carlo markov chain model estimation texture sampling guaranteed converge stationary process follows estimated dis tribution geman geman observed histograms matched closely 
thirdly theoretical proof provided show marginal distributions filter responses linear filters matched eventually obtain underlying model 
frame model computationally expensive 
approaches facilitating computa tion developed discussion aspect reader referred zhu wu mumford 
challenging texture images 
textures difficult model human synthesized cloth textures shown 
appears synthesizing textures requires far sophisticated high level features high level features may correspond high level visual process geometrical properties object shape 
choose filters fixed set filters general understood design set features structures arbitrary applications 
discussion proposes minimax entropy principle building probability models variety applications 
theory answers major questions 
feature binding feature fusion integrate image features statistics single joint probability distribution limiting forms features 
second feature selection choose set features best characterize observed images 
algorithms proposed parameter estimation stochastic simulation 
greedy algorithm developed feature pursuit model complexity studied presence sample variations 
minimax entropy principle applied texture modeling feature extracted images empirical marginal distributions histograms filtered images 
new mrf model frame derived experiments described section demonstrate method capable modeling wide variety textures previously considered belonging different categories 
results texture modeling support psychological experiments suggest texture images difficult discriminate produce similar empirical marginal distributions filter responses bank filters bergen adelson landy 
theory methodology contributes possible image representing strategy brain biologically plausible gabor filter analysis 
impor tant issue minimax entropy principle model inference ically plausible considered model method natural constructing models classes images 
maximum entropy phase algorithm computational standpoint consists mainly approx values lagrange multipliers done hill climbing respect log likelihood 
specifically monte carlo methods sample distributions plugged sampled statistics gradient log likelihood 
authors conjectured feedback pathways cortex may serve function forming mental images basis learned models distribution images mumford 
mechanism sample monte carlo algorithm 
theory postulated cortex seeks residuals features observed image different mental image 
algorithm shows residuals drive learning process lagrange multipliers gradually improved increase log likelihood 
conjecture lagrange multipliers stored suitable synaptic weights higher vi area top pathway 
massively parallel architecture apparent stochastic component neural firing huge amount observed images processed day computational load algorithm may excessive cortical implementation 
minimum entropy phase algorithm direct experimental evidence favor 
extensive psychophysical experimentation phenomenon pre attentive texture discrimination 
propose textures pre discriminated exactly suitable filters incorporated minimum entropy cortical model process subjects train pre discriminate new sets textures exactly incorporating new filter feature model 
evidence texture pairs pre naive subjects practice reported groups notably sagi 
remarkable specificity reported texture discrimination learning suggests specific new filters incorporated cortical texture model theory 
minimax entropy principle applied model general nat ural images zhu mumford 
hope simulate research efforts direction 
appendix mathematical details proof theorem ep qb ef qb ef logp ef logz el qb logz ef efa logz ep aq qb ep og entropy result follows 
proof proposition 
clearly ep ep ei 

taylor expansion argument kullback page entropy decrease ep ep ep ep ei qb ep qb vpt ei ep qb distribution expected feature statistics vii qb vp vv 
conditional variance 
result follows 
conditional variance vp interpreted follows 
vv qb qb linear combination qb shown qb uncorrelated qb orthogonalization qb respect 
vp qs variance qb dependence eliminated 
proof proposition proof theorem know ef logp ep logp similar derivation ef logp ep logp kl ef ef logp ef ep logp el log logp ep logp ep logp kl kl 
result follows setting 
derivation shows best approximates possible 
proof proposition subsection logp bs log likelihood function subscript obs emphasize fact random variable depending observed images 
show similar derivation prove entropy kl 
applying sides equation entropy entropy kl result follows 
proof theorem connected fourier transform 
application inverse fourier transform gives vector size characteristic function ai fe zf ai di 
inner product ai marginal distribution specific linear filter index filters 
result follows 
acknowledgments grateful anonymous referees insightful comments greatly im proved presentation article 
supported nsf dms david mumford 
second author supported donald rubin 
akaike entropy maximization principle applications statistics ed 
krishnaiah amsterdam north holland 
barlow mitchison finding minimum entropy codes 
neural computation 
vol 
pp 
bergen adelson theories visual texture perception 
spatial vision regan eds crc press 
besag spatial interaction statistical analysis lattice systems discussion 
royal star 
soc 

blake zisserman visual reconstruction 
mit press 
landy orthogonal distribution analysis new approach study texture perception 
comp 
models visual proc 
landy 
ed 
mit press 
coifman wickerhauser 
entropy algorithms best basis selection 
ieee trans 
information theory vol pp 
cross jain markov random field texture models 
ieee pami 

daugman uncertainty relation resolution space spatial frequency orientation optimized dimensional visual cortical filters 
journal optical soc 
amer 
vol 
dayan hinton neal zemel helmholtz machine neural computation 

donoho johnstone ideal de noising orthonormal basis chosen library bases 
acad 
sci 
paris set vol pp 
field goal sensory coding neural computation 

gabor theory communication 
iee proc vol 
geman geman stochastic relaxation gibbs distribution bayesian restoration images 
ieee trans 
pami 

heeger bergen pyramid texture analysis synthesis 
com puter graphics press 

jaynes information theory statistical mechanics 
physical review 
pp 
jolliffe principle components analysis 
new york springer 
jordan jacobs hierarchical mixtures experts em algo rithm neural computation 

julesz visual pattern discrimination 
ire transactions information theory pp 
julesz dialogues perception 
sagi practice perfect texture discrimination evidence primary visual cortex plasticity proc 
nat 
acad 
sci 
vol 

kullback leibler information sufficiency annual math 
star 
vol pp 
kullback information theory statistics 
john wiley sons new york 
mallat multi resolution approximations wavelet orthonormal bases 
trans 
amer 
math 
soc 


mumford shah optimal approximations piecewise smooth func tions associated variational problems 
comm 
pure appl 
math pp 

mumford computational architecture neocortex ii role cortico cortical loops biological cybernetics pp 

picard 
society models video image libraries mit media lab 
technical report 
priestley 
spectral analysis time series academic press 
rissanen stochastic complexity statistical inquiry 
singapore world scientific 
silverman grosof de valois spatial frequency organization primate striate cortex 
proc 
natl 
acad 
sci 


simoncelli freeman adelson heeger shiftable multi scale transforms 
ieee trans 
information theory 
vol 
pp 
winkler image analysis random fields dynamic monte carlo methods springer verlag 
witkin kass reaction diffusion textures 
computer graphics 

xu ying yang machine bayesian kullback scheme unified new results vector quantization proc 
int conf 
neural info 
proc 

younes 
estimation annealing fields 
annales de institut henri poincare section calcul des probabilities statis 
zhu mumford learning generic prior models visual computation 
harvard robotics lab 
technique report 

zhu statistical theories image segmentation texture modeling object recognition 
ph thesis harvard university 
zhu wu mumford frame filters random fields maximum entropy unified theory texture modeling harvard robotics lab 
technique report 


