massachusetts institute technology artificial intelligence laboratory memo august mean field theory sigmoid belief networks lawrence saul tommi jaakkola michael jordan publication retrieved anonymous ftp publications ai mit edu 
develop mean field theory sigmoid belief networks ideas statistical mechanics 
mean field theory provides tractable approximation true probability distribution networks yields lower bound likelihood evidence 
demonstrate utility framework benchmark problem statistical pattern recognition classification handwritten digits 
copyright fl massachusetts institute technology report describes research done artificial intelligence laboratory massachusetts institute technology 
support research provided part advanced research projects agency department defense office naval research contract 
authors supported nsf cda atr research laboratories siemens 
bayesian belief networks pearl lauritzen spiegelhalter provide rich graphical representation probabilistic models 
nodes networks represent random variables links represent causal influences 
associations endow directed acyclic graphs dags precise probabilistic semantics 
ease interpretation afforded semantics explains growing appeal belief networks widely models planning reasoning uncertainty 
inference learning belief networks possible insofar efficiently compute approximate likelihood observed patterns evidence buntine russell binder koller kanazawa 
exist provably efficient algorithms computing likelihoods belief networks tree chain architectures 
practice algorithms tend perform general sparse networks 
networks nodes parents exact algorithms slow jensen kong 
large networks dense layered connectivity exact methods intractable require summing exponentially large number hidden states 
approach dealing networks gibbs sampling pearl stochastic simulation methodology roots statistical mechanics geman geman 
approach relies different tool statistical mechanics mean field theory parisi 
mean field approximation known probabilistic models represented undirected graphs called markov networks 
example boltzmann machines ackley hinton sejnowski mean field learning rules shown yield tremendous savings time computation sampling methods peterson anderson 
main motivation extend mean field approximation undirected graphical models directed counterparts 
belief networks transformed markov networks mean field theories markov networks known natural ask new framework required 
reason probabilistic models compact representations dags may unwieldy representations undirected graphs 
shall see avoiding complexity working directly dags requires extension existing methods 
focus sigmoid belief networks neal resulting mean field theory straightforward 
networks binary random variables local conditional distributions log linear models 
develop mean field approximation networks compute lower bound likelihood evidence 
method applies arbitrary partial instantiations variables networks restrictions network topology 
note lower bound available learning procedure maximize lower bound useful true likelihood computed efficiently 
similar approximation models continous random variables discussed jaakkola 
idea bounding likelihood sigmoid belief networks introduced related architecture known helmholtz machine hinton dayan frey neal 
fundamental advance establish framework approximation especially conducive learning parameters layered belief networks 
close connection idea mean field approximation statistical mechanics developed 
hope elucidate connection convey sense approximations generate useful lower bounds time remaining analytically tractable 
develop simplest approximation belief networks noting sophisticated methods jaakkola jordan saul jordan available 
emphasized approximations form required handle multilayer neural networks statistical pattern recognition 
networks exact algorithms hopelessly intractable gibbs sampling methods impractically slow 
organization follows 
section introduces problems inference learning sigmoid belief networks 
section contains main contribution tractable mean field theory 
mean field approximation sigmoid belief networks derive lower bound likelihood instantiated patterns evidence 
section looks mean field algorithm learning parameters sigmoid belief networks 
algorithm give results benchmark problem pattern recognition classification handwritten digits 
section presents issues research 
sigmoid belief networks great virtue belief networks clearly exhibit conditional dependencies underlying probability model 
consider belief network defined binary random variables sn 
denote parents pa fs gamma smallest set nodes js gamma jpa sigmoid belief networks neal conditional distributions attached node loglinear models 
particular probability ith node activated jpa oe ij ij weights biases network oe gammaz sigmoid function oe gammaz gamma sum weighted inputs node jz oe conditional probability node activated 
sigmoid function shown 
sigmoid belief networks ij pa ij network structure directed acyclic graph 
sigmoid function eq 
provides compact parametrization conditional probability distributions eq 
propagate beliefs 
particular jpa depends pa sum weighted inputs weights may viewed parameters logistic regression mccullagh nelder 
conditional probability distribution may summarized jpa exp hi ij exp ij note substituting eq 
recovers result eq 

combining eqs 
may write joint probability distribution variables network jpa exp hi ij exp ij denominator eq 
ensures probability distribution normalized unity 
turn problem inference sigmoid belief networks 
absorbing evidence divides units belief network types visible hidden 
visible units evidence nodes instantiated values hidden units 
possible ambiguity denote subsets hidden visible units 
bayes rule inference done conditional distribution hjv relation noisy models discussed appendix likelihood evidence principle likelihood may computed jhj configurations hidden units 
unfortunately calculation intractable large densely connected networks 
intractability presents major obstacle learning parameters networks nearly procedures statistical estimation require frequent estimates likelihood 
calculations exact probabilistic inference difficulties 
unable compute directly hjv resort approximation statistical physics known mean field theory 
mean field theory mean field approximation appears multitude guises physics literature old statistical mechanics 
briefly explain acquired name ubiquitous 
physical models described markov networks variables represent localized magnetic moments sites crystal lattice sums ij represent local magnetic fields 
roughly speaking certain cases central limit theorem may applied sums useful approximation ignore fluctuations fields replace mean value name mean field theory 
models excellent approximation poor 
simplicity widely step understanding types physical phenomena 
explains origins mean field theory fact ways derive amounts approximation parisi 
formulation appropriate inference learning graphical models 
particular view mean field theory principled method approximating intractable graphical model tractable 
done variational principle chooses parameters tractable model minimize entropic measure error 
basic framework mean field theory remains directed graphs necessary introduce extra mean field parameters addition usual ones 
markov networks finds set nonlinear equations mean field parameters solved iteration 
practice iteration converge fairly quickly scale large networks 
return problem posed section 
belief networks intractable decompose joint distribution hjv likelihood evidence purposes probabilistic modeling mean field theory main virtues 
provides tractable approximation hjv hjv conditional distributions re quired inference 
second provides lower bound likelihoods required learning 
consider origin lower bound 
clearly approximating distribution hjv equality ln ln ln hjv delta hjv obtain lower bound apply jensen inequality cover thomas pushing logarithm sum hidden states expectation ln hjv ln hjv straightforward verify difference left right hand side eq 
divergence cover thomas kl qjjp hjv ln hjv hjv better approximation hjv tighter bound ln 
anticipating connection statistical mechanics refer hjv mean field distribution 
natural divide calculation bound components particular averages approximating distribution 
components mean field entropy energy bound difference ln gamma hjv ln hjv gamma hjv ln terms physical interpretations 
measures amount uncertainty mean field distribution follows standard definition entropy 
second measures average value gamma ln name energy arises interpreting probability distributions belief networks boltzmann distributions unit temperature 
case energy network configuration constant minus logarithm probability similar average performed step em algorithm dempster laird rubin difference average performed mean field distribution hjv true posterior hjv 
related discussion see neal hinton 
terminology follows 
denote degrees freedom statistical mechanical system 
energy system real valued function degrees freedom boltzmann distribution gammafie gammafie defines probability distribution possible boltzmann distribution 
sigmoid belief networks energy form gamma ln gamma ij ij gamma ln exp ij follows eq 

terms equation familiar markov networks pairwise interactions hertz krogh palmer term peculiar sigmoid belief networks 
note energy linear function weights polynomial function units 
price pay sigmoid belief networks identifying hjv boltzmann distribution log likelihood partition function 
note identification implicitly form eqs 

bound eq 
valid probability distribution hjv 
choose distribution enables evaluate right hand side eq 

consider factorized distribution hjv gamma gammas binary hidden units fs appear independent bernoulli variables adjustable means mean field approximation obtained substituting factorized distribution eq 
true boltzmann distribution eq 

may approximation replaces rich probabilistic dependencies hjv impoverished assumption complete 
true degree reader keep mind values choose statistics hidden units depend evidence best approximation form eq 
choosing mean values minimize kullback leibler divergence kl qjjp 
equivalent minimizing gap true loglikelihood ln lower bound obtained mean field theory 
mean field bound loglikelihood may calculated substituting eq 
right hand side eq 

result calculation ln ij ij gamma ln ij ae gamma ln gamma ln gamma deltai indicates expectation value mean field distribution eq 

terms rations parameter fi inverse temperature serves calibrate energy scale fixed unity discussion belief networks 
sum denominator known partition function ensures boltzmann distribution normalized unity 
lines eq 
represent mean field energy derived eq 
third represent mean field entropy 
slight abuse notation defined mean values visible units course set instantiated values 
note compute average energy mean field approximation find expected value hln ij sum weighted inputs ith unit belief network 
unfortunately mean field assumption hidden units uncorrelated average simple closed form 
term arise mean field theory markov networks pairwise interactions peculiar sigmoid belief networks 
principal average may performed enumerating possible states pa 
result calculation extremely unwieldy function parameters belief network 
reflects fact general sigmoid belief network defined weights ij equivalent markov network nth order interactions pairwise ones 
avoid complexity develop mean field theory works directly dags 
handle expected value hln distinguishes mean field theory previous ones 
unable compute term exactly resort bound 
note random variable real number equality hln omega ln theta gammaz ff ln gammaz gamma upper bound right hand side applying jensen inequality opposite direction pulling logarithm outside expectation hln ln gammaz gamma setting eq 
gives standard bound hln tighter bound seung obtained allowing non zero values illustrated special case gaussian distributed random variable zero mean unit variance 
bound eq 
useful properties state proof right hand side convex function ii value minimizes function occurs interval 
provided possible evaluate eq 
different values tightest bound form simple dimensional minimization 
bound put immediate attaching extra mean field parameter unit belief network 
upper bound intractable terms mean field energy ln ij ae ij ln gamma gamma exact bound bound eq 
case normally distributed zero mean unit variance 
case exact result hln bound gives min ln gamma 
standard bound jensen inequality occurs gives 
ij expectations inside logarithm evaluated exactly factorial distribution eq 
example gamma gamma gamma gamma gamma ij delta similar result holds gamma averages tractable tend write follows 
reader keep mind averages difficulty simply averages products independent random variables opposed sums 
assembling terms eqs 
gives lower bound ln lv lv ij ij gamma ij gamma ln gamma gamma ln gamma ln gamma log likelihood evidence far specified parameters particular bound eq 
valid choice parameters 
naturally seek values maximize right hand side eq 

suppose fix mean values ask parameters yield tightest possible bound 
note right hand side eq 
couple terms belong different units network 
minimization reduces independent minimizations interval 
done number standard methods press flannery teukolsky vetterling 
choose means set gradients bound respect equal zero 
markov blanket unit includes parents children parents children 
define intermediate matrix ij gamma ln gamma gamma weighted sum inputs ith unit 
note ij zero parent words connectivity weight matrix ij mean field approximation ij measures parental influence instantiated evidence degree correlation positive negative measured relative parents matrix elements may evaluated expanding expectations eq 
full derivation appendix setting gradient equal zero gives final mean field equation oe ij ji gamma ji oe delta sigmoid function 
argument sigmoid function may viewed effective input ith unit belief network 
effective input composed terms unit markov blanket pearl shown particular terms take account unit internal bias values parents children matrix ji values children parents 
solving equations iteration values instantiated units propagated entire network 
analogous propagation information occurs exact algorithms lauritzen spiegelhalter compute likelihoods belief networks 
factorized approximation true posterior exact mean field equations set parameters values approximation accurate possible 
turn translates tightest mean field bound log likelihood 
procedure bounding log likelihood consists alternating steps update fixed ii update fixed step involves independent minimizations interval second done iterating mean field equations 
practice steps repeated mean field bound log likelihood converges desired degree accuracy 
shown updates mean layer belief network topdown propagation beliefs 
model images handwritten digits section networks units bottom layer encoded pixel values bitmaps 
quality bound depends approximations complete mean field distribution eq 
logarithm bound eq 

reliable approximations belief networks study question performed numerical experiments layer belief network shown 
advantage working small network true likelihoods computed exact enumeration 
considered particular event units bottom layer instantiated zero 
event compared mean field bound likelihood true value obtained enumerating states top layers 
done random networks weights biases uniformly distributed 
left shows histogram relative error log likelihood computed ln gamma networks mean relative error 
right shows histogram results assuming states bottom layer occur equal probability case relative error computed ln gamma ln gamma 
uniform approximation root mean square relative error 
large discrepancy results suggests mean field theory provide useful lower bound likelihood certain belief networks 
course ultimately matters behavior mean field theory networks solve meaningful problems 
subject section 
learning attractive sigmoid belief networks perform density estimation high dimensional input spaces 
problem parameter estimation set patterns particular units belief network find set weights ij biases assign high probability patterns 
clearly ability compute likelihoods lies crux algorithm learning parameters belief networks 
mean field algorithms provide strategy discovering appropriate values ij resort field parameters lead monotonic increases lower bound just case markov networks 
relative error log likelihood mean field approximation relative error log likelihood uniform approximation histograms relative error log likelihood randomly generated layer networks 
top relative error mean field approximation bottom relative error states bottom layer assumed occur equal probability 
log likelihood computed event nodes bottom layer instantiated zero 
gibbs sampling 
consider instance procedure 
pattern training set solve mean field equations compute associated bound log likelihood adapt weights belief network gradient ascent mean field bound deltaj ij lv ij deltah suitably chosen learning rate 
cycle patterns training set maximizing likelihoods fixed number iterations detects onset overfitting crossvalidation procedure uses lower bound loglikelihood cost function training belief networks hinton dayan frey neal 
fact lower bound log likelihood upper bound course crucial success learning algorithm 
adjusting weights maximize lower bound affect true log likelihood ways see 
true log likelihood expressions gradients lv appendix course incorporate prior distributions weights biases maximize approximation log posterior probability training set 
training time true log likelihood lower bound training time true log likelihood lower bound relationship true log likelihood lower bound learning 
possibility left increase 
true log likelihood decreases closing gap bound 
viewed form regularization 
binary images handwritten digits 
increases moving direction bound true log likelihood decreases closing gap quantities 
purposes maximum likelihood estimation outcome clearly desirable second desirable viewed positive light 
case mean field approximation acting regularizer steering network simple factorial solutions expense lower likelihood estimates 
tested algorithm building maximumlikelihood classifier images handwritten digits 
data consisted examples handwritten digits compiled postal service office advanced technology 
examples preprocessed produce binary images shown 
digit divided available data training set examples test set examples 
trained layer network see digit sweeping training set times learning rate 
networks units top layers units middle layer units bottom layer making far large treated exact methods 
training classified digits test set network assigned highest likelihood 
table shows confusion matrix entry counts number times digit classified digit errors classification possible architectures chosen purpose density estimation layered networks permit comparison previous benchmarks data set 
table confusion matrix digit classification 
entry ith row jth column counts number times digit classified digit algorithm classification error nearest neighbor back propagation wake sleep mean field table classification error rates data set handwritten digits 
reported hinton 
possible yielding error rate 
table gives performance various algorithms partition data set 
table shows average log likelihood score network digits test set 
note scores lower bounds 
scores normalized network zero weights biases patterns equally receive score 
expected digits relatively simple constructions zeros ones easily modeled rest 
measures performance error rate loglikelihood score competitive previously published results hinton dayan frey neal data set 
success algorithm affirms strategy maximizing lower bound utility mean field approximation 
similar results obtained gibbs sampling require considerably computation methods maximizing lower bound frey dayan hinton 
discussion endowing networks probabilistic semantics provides unified framework incorporating prior knowledge handling missing data performing inference uncertainty 
probabilistic calculations quickly intractable important develop techniques approximate probability distributions flexible manner 
especially true networks multilayer architectures large numbers hidden units 
exact algorithms gibbs sampling methods generally practical networks digit log likelihood score table normalized log likelihood score network digits test set 
obtain raw score multiply theta theta ln 
row shows score averaged digits 
approximations required 
developed mean field approximation sigmoid belief networks 
computational tool mean field theory main virtues provides tractable approximation conditional distributions required inference second provides lower bound likelihoods required learning 
problem computing exact likelihoods belief networks np hard cooper true approximating likelihoods guaranteed degree accuracy dagum luby 
follows establish universal guarantees accuracy mean field approximation 
certain networks clearly mean field approximation bound fail capture logical constraints strong correlations fluctuating units 
preliminary results suggest worst case results apply belief networks 
worth noting qualifications apply markov networks domain mean field methods established 
idea bounding likelihood sigmoid belief networks introduced related architecture known helmholtz machine hinton dayan neal zemel 
formalism differs number respects helmholtz machine 
importantly enables compute rigorous lower bound likelihood 
said wake sleep algorithm frey hinton dayan relies sampling methods heuristic approximation dayan guarantee rigorous lower bound 
mean field theory takes place recognition model helmholtz machine applies generally sigmoid belief networks layered structure 
places restrictions locations visible units may occur network important feature handling problems missing data 
course advantages accrued extra computational demands complicated learning rules 
builds theory begun relax assumption complete eq 

general expect sophisticated approximations boltzmann distribution yield tighter bounds loglikelihood 
challenge find distributions allow correlations hidden units remaining computationally tractable 
tractable mean choice hjv enable evaluate upper bound right hand side eq 

extensions kind include mixture models jaakkola jordan partially factorized distributions saul jordan exploit presence tractable substructures original network 
approach simplest mean field theory computationally tractable clearly better results obtained tailoring approximation problem hand 
sigmoid versus noisy semantics sigmoid function similar identical noisy gates pearl commonly belief network literature 
gates weights network represent independent causal events 
case probability unit activated jpa gamma gamma ij ij probability causes absence causal events 
define weights noisy belief network ij gamma ln gamma ij follows jpa ae ij ae gamma gammaz noisy gating function 
comparing sigmoid function eq 
see model jpa monotonically increasing function sum weighted inputs 
main difference noisy networks weights ij constrained positive underlying set probabilities ij jaakkola jordan developed mean field approximation noisy belief networks 
gradients provide expressions gradients appear eqs 

usual ij denote sum inputs unit factorial distribution eq 
compute averages gamma gamma theta gamma gamma ij gamma gamma gamma gamma ij unit network define quantity oe gamma gamma gamma note oe lies zero 
definition write matrix elements eq 
ij gamma oe gamma gamma ij gamma gamma ij oe gamma gamma ij gamma gamma ij gradients eqs 
similar means 
weights ij gamma gamma gamma oe gamma ij gamma gamma ij gamma oe gamma gamma ij gamma gamma ij likewise biases lv gamma oe note may obtain simpler gradients expense introducing weaker bound eq 

advantageous speed computation important quality bound 
experiments bound eq 

especially grateful dayan hinton frey neal seung sharing early versions manuscripts providing stimulating discussions 
improved greatly comments anonymous reviewers 
facilitate comparisons similar methods results reported images preprocessed university toronto 
authors acknowledge support nsf cda onr atr research laboratories siemens 
ackley hinton sejnowski 
learning algorithm boltzmann machines 
cognitive science 
buntine 
operations learning graphical models 
journal artificial intelligence research 
cooper 
computational complexity probabilistic inference bayesian belief networks 
artificial intelligence 
cover thomas 
elements information theory 
new york john wiley sons 
dagum luby 
approximately probabilistic reasoning bayesian belief networks np hard 
artificial intelligence 
dayan hinton neal zemel 
helmholtz machine 
neural computation 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
frey hinton dayan 
algorithm learn density estimators 
touretzky mozer hasselmo eds 
advances neural information processing systems proceedings conference 
geman geman 
stochastic relaxation gibbs distributions bayesian restoration images 
ieee transactions pattern analysis machine intelligence 
hertz krogh palmer 
theory neural computation 
redwood city ca addison wesley 
hinton dayan frey neal 
wake sleep algorithm unsupervised neural networks 
science 


statistical field theory 
cambridge cambridge university press 
jaakkola saul jordan 
fast learning bounding likelihoods sigmoid type belief networks 
touretzky mozer hasselmo eds 
advances neural information processing systems proceedings conference 
jaakkola jordan 
mixture model approximations belief networks 
manuscript preparation 
jaakkola jordan 
computing upper lower bounds likelihoods intractable networks 
submitted 
jensen kong kjaerulff 
blocking gibbs sampling large probabilistic expert systems 
international journal human computer studies 
special issue real world applications uncertain reasoning 
lauritzen spiegelhalter 

local computations probabilities graphical structures application expert systems 
journal royal statistical society 
mccullagh nelder 
generalized linear models 
london chapman hall 
neal 
connectionist learning belief networks 
artificial intelligence 
neal hinton 
new view em algorithm justifies incremental variants 
submitted publication 
parisi 
statistical field theory 
redwood city ca addison wesley 
pearl 
probabilistic reasoning intelligent systems 
san mateo ca morgan kaufmann 
peterson anderson 
mean field theory learning algorithm neural networks 
complex systems 
press flannery teukolsky vetterling 
numerical recipes 
cambridge university press 
russell binder koller kanazawa 

local learning probabilistic networks hidden variables 
proceedings ijcai 
saul jordan 
exploiting tractable substructures intractable networks 
touretzky mozer hasselmo eds 
advances neural information processing systems proceedings conference 
seung 

annealed theories learning 
oh kwon cho eds 
neural networks statistical mechanics perspective proceedings ctp joint workshop theoretical physics 
singapore world scientific 

