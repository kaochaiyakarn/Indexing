generalization ability line learning algorithms nicol cesa bianchi dti university milan italy cesa bianchi dti unimi alex dti university milan italy dti unimi claudio gentile dsi university milan milano italy gentile dsi unimi show line algorithms classification regression naturally obtain hypotheses tail bounds risk 
results proven requiring complicated concentration measure arguments hold arbitrary line learning algorithms 
furthermore applied concrete line algorithms results yield tail bounds cases comparable better best known bounds 
main contributions statistical theories regression classification problems derivation functionals certain empirical quantities sample error sample margin provide uniform risk bounds hypotheses certain class :10.1.1.33.8995
approach known weak points 
obtaining tight uniform risk bounds terms meaningful empirical quantities generally difficult task 
second searching hypothesis minimizing empirical functional computationally expensive furthermore minimizing algorithm seldom incremental new data added training set algorithm needs run scratch 
line learning algorithms perceptron algorithm winnow algorithm variants general methods solving classification regression problems fully incremental fashion :10.1.1.37.1595
need cases short time process new training example adjust current hypothesis 
behavior algorithms understood called mistake bound model assumptions way training sequence generated fewer results concerning algorithms obtain hypotheses small statistical risk 
littlestone proposed method obtaining small risk hypotheses run arbitrary line algorithm cross validation set test hypotheses generated run 
method require convergence property online algorithm provides risk tail bounds sharper obtainable choosing instance hypothesis run survived longest 
helmbold warmuth showed cross validation sets obtain expected risk bounds opposed informative tail bounds hypothesis randomly drawn generated run 
prove refinements extensions previous analyses online algorithms naturally lead data dependent tail bounds employing complicated concentration measure machinery needed frameworks 
particular show obtain arbitrary line algorithm hypotheses risk close high probability theorems amount training data data dependent quantity measuring cumulative loss online algorithm actual training data 
applied concrete algorithms loss bound translates function meaningful data dependent quantities 
classification problems mistake bound norm perceptron algorithm yields tail risk bound terms empirical distribution margins see 
regression problems square loss bound ridge regression yields tail risk bound terms eigenvalues gram matrix see 
preliminaries notation arbitrary sets example pair instance belonging label associated random variables denoted upper case realizations lower case 
pair random variables take values respectively 
assume data generated unknown probability distribution probabilities expectations understood respect underlying distribution 
short hand denote random variable 
hypothesis measurable mapping instances predictions decision space 
risk defined er nonnegative loss function 
specified assume takes values known 
line algorithms investigate defined known mathematical model generalization learning model introduced littlestone angluin 
training sequence fixed 
learning model line algorithm processes examples time trials generating sequence hypotheses th trial algorithm receives instance uses current hypothesis compute prediction label associated true value label disclosed algorithm suffers loss measuring bad prediction label trial begins algorithm generates new hypothesis may may equal measure algorithm performance cumulative loss analysis write want stress fact cumulative loss hypotheses line algorithm functions random sample particular denote deterministic initial hypothesis arbitrary line algorithm random variable denoting th hypothesis line algorithm value change changes values goal relate risk hypotheses produced line algorithm running sequence cumulative loss algorithm sequence 
cumulative loss key empirical data dependent quantity 
analysis obtain bounds form er ln specific function sequence hypotheses produced algorithm suitable positive constant 
see specific line algorithms ratio bounded terms meaningful empirical quantities 
method centers simple concentration lemma bounded losses 
lemma arbitrary bounded loss satisfying arbitrary line algorithm output necessarily distinct hypotheses run er ln proof 
set er 
er furthermore takes values 
er denotes algebra generated direct application hoeffding azuma inequality bounded random variables proves lemma 
concentration convex losses section investigate risk average hypothesis def hypotheses generated line algorithm run training examples 
average hypothesis generates valid predictions decision space convex 
theorem convex convex argument 
arbitrary line algorithm output necessarily distinct hypotheses algorithm run holds er ln notice hypothesis average 
proof 
convex argument jensen inequality expectation respect yields er er 
inequality lemma yields thesis 
theorem viewed tail bound version expected bound implies risk average hypothesis close samples hand note er concentrates strong assumptions underlying line algorithm 
application theorem shown section 
just note applying theorem weighted majority algorithm prove version theorem absolute loss resorting sophisticated concentration inequalities details full 
penalized risk estimation general losses loss function nonconvex loss risk average hypothesis bounded way shown previous section 
risk best hypothesis generated line algorithm higher average risk hypotheses 
lemma immediately tells conditions loss function boundedness samples hypotheses generated risk close section give technique lemma penalized risk estimate finds high probability hypothesis 
argument refinement littlestone method 
littlestone technique require cross validation set 
able obtain bounds risk main term size set examples available learning algorithm training set plus validation set littlestone 
similar observations analysis refer randomized hypotheses loss absolute loss 
define penalized risk estimate hypothesis length suffix training sequence line algorithm seen generated cumulative loss suffix ln algorithm chooses hypothesis argmin sake simplicity restrict losses range 
clear losses values arbitrary bounded real interval handled techniques similar shown section 
prove result 
theorem arbitrary line algorithm output necessarily distinct hypotheses run hypothesis chosen penalized risk estimate satisfies er ln proof theorem technical lemmas 
lemma arbitrary line algorithm output necessarily distinct hypotheses run holds er min er proof 
argmin er set brevity fixed er er er er holds er er er er hold 
fixed write er er er er er er er er er er er er er er er er er er probability zero 
plugging write er er er er er inequalities applied chernoff hoeffding bounds 
lemma arbitrary line algorithm output necessarily distinct hypotheses run holds min er ln ln proof 
min er er er ln er ln er ln inequality follows min er ln ln er ln lemma 
proof theorem 
proof follows combining lemma lemma square root terms 
applications sake concreteness sketch generalization bounds obtained direct application techniques 
norm perceptron algorithm linear threshold algorithm keeps th trial weight vector instance fx jjxjj algorithm predicts sign 
jj jj 
algorithm prediction wrong algorithm performs weight update notice yields classical perceptron algorithm 
hand log gets algorithm performs multiplicative algorithm normalized winnow algorithm 
applying theorem bound number mistakes norm perceptron algorithm shown immediately obtain probability respect draw training sample risk er penalized estimator ln jjujj 
margin quantity maxf 
called soft margin accounts distribution margin values achieved examples respect hyperplane traditional data dependent bounds uniform convergence methods typically expressed terms sample margin jfi 
gj terms fraction training points margin ratio occurring similar flavor ratios general incomparable 
bound extra log factors appearing analyses uniform convergence 
furthermore significantly better bound constant typically occurs data sequence linearly separable 
second application consider ridge regression algorithm square loss 
assume 
algorithm computes th trial vector minimizes jjwjj 

instance algorithm predicts 
clipping function losses bounded apply theorem bound cumulative loss ridge regression see obtain probability respect draw training sample risk er average hypothesis estimator jjujj ln ai ln ln 
jaj denotes determinant matrix dimensional identity matrix transpose denote matrix columns data vectors simple linear algebra shows ln ai ln ln ai ln ln eigenvalues nonzero eigenvalues nonzero eigenvalues gram matrix risk bounds terms eigenvalues gram matrix derived defer full comparison results 
bound applies kernel ridge regression replacing eigenvalues eigenvalues kernel gram matrix kernel considered 
angluin queries concept learning machine learning 
warmuth relative loss bounds line density estimation exponential family distributions machine learning 
azuma 
weighted sum certain random variables 
mathematical journal 
slightly different linear regression algorithm forster warmuth proven sharper bound expected relative loss 
particular exhibited algorithm computing hypothesis expectation relative risk er minu 
bounded ny blum kalai langford 
beating hold bounds fold progressive cross validation 
th colt pages 
lugosi massart 
sharp concentration inequality applications 
random structures algorithms 
cesa bianchi freund haussler helmbold schapire warmuth 
expert advice 
journal acm 
forster warmuth 
relative expected instantaneous loss bounds 
th colt 
freund schapire 
large margin classification perceptron algorithm 
machine learning 
gentile robustness norm algorithms 
manuscript 
extended authored littlestone appeared th colt 
grove littlestone schuurmans 
general convergence results linear discriminant updates machine learning 
helmbold warmuth 
weak learning 
jcss june 
ridge regression biased estimation nonorthogonal problems 
technometrics 
kivinen warmuth 
additive versus exponentiated gradient updates linear prediction 
information computation 
littlestone 
learning quickly irrelevant attributes abound new algorithm 
machine learning 
littlestone 
line batch learning 
nd colt 
littlestone warmuth 
weighted majority algorithm 
information computation 
rosenblatt 
principles neurodynamics perceptrons theory brain mechanisms 
spartan books washington 
saunders gammerman vovk 
ridge regression learning algorithm dual variables th icml 
shawe taylor bartlett williamson anthony structural risk minimization data dependent hierarchies 
ieee trans 

shawe taylor cristianini generalization soft margin algorithms 
neurocolt tech 
rep www neurocolt org 
vapnik statistical learning theory 
wiley sons ny 
vovk competitive line linear regression 
nips 
tech 
rep department computer science royal holloway university london csd tr 
williamson shawe taylor scholkopf smola sample generalization bounds ieee trans 
appear 
