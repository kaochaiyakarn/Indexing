relational representations reinforcement learning review open problems martijn van cs utwente nl department computer science ae enschede representation rl 
discuss concepts representation generalization reinforcement learning argue higher order representations commonly propositional representations 
contains small review current reinforcement learning systems higher order representations followed brief discussion 
ends research directions open problems 
keywords reinforcement learning generalization relational representations review 
general agreement nowadays intelligent agents adaptive capable learning 
learning agents able proper generalizations reuse learned knowledge apply new situations similar encountered ones 
generalization inductive learning difficult examples 
generalize examples needs form bias representing organizing possible hypotheses hypothesis space form operators go hypothesis space find suitable hypothesis supports experienced evidence 
set modelling assumptions representational language hypothesis operators termed inductive bias mitchell 
guides search hypotheses consistent evidence restricts number structure possible hypotheses 
important bias language bias determines concepts domain represented 
roughly choices propositional attribute value av representations relational order representations 
av approach dominated field reinforcement learning rl sutton barto 
standard rl methods atomic av propositional representation current state learner standard machine learning ml mitchell techniques generalizing experience 
popular approaches neural networks decision trees naturally propositional av representations 
type representation suffices applications argue need relational representations 
relational representations reason objects properties possible relations objects types objects 
give simple example imagine want express basic fact state book table 
represent means states book table 
case enumerate states learn 
propositional representation express fact means 
bn 
order language express state set states just simple formula 
representation change number domain objects increased 
interest abstraction different levels generalization statistical terms 
abstraction time sutton primitive actions dietterich useful ways specific sub actions time 
currently interest higher order representation languages rl kaelbling dzeroski 
subsets language firstorder logic fol desirable representational power creates additional problems concerning tractability learnability 
discuss concepts issues underly higher order representation languages rl 
specifically interested rl agents interacting humans agents multi modal virtual worlds 
types agents necessary represent reason world terms cognitive notions beliefs desires intentions emotions 
natural language communication central notion means speech written text 
agents dealing rich information context able represent subjective view world terms objects relations objects 
beliefs ways expressing beliefs experience terms looks facial animation language expressions rational behavior 
highly desirable learn level representation terms objects kaelbling 
outline follows 
section discuss briefly concepts rl 
section discuss generalization representational issues rl 
section gives review main existing approaches higher order representations rl 
preliminary comparisons discussion approaches section 
section mentions directions research 

reinforcement learning reinforcement learning sutton barto rl powerful technique learning domains instructive feedback supervised learning evaluative feedback 
learning agents trained rewarding scaffolding expressed terms real numbers rewards 
short agent perceives state st decides action transition st st receives reward rt 
task agent maximize total reward gets doing actions 
agents learn policy maps states actions 
usually rl done learning value functions 
state value function returns expected total reward agent starts acting state 
state action value function returns expected total reward agent takes action state continues follow policy 
state values useful model environment available 
iterative algorithms learning value functions exist sutton barto 

representation generalization interesting problems large infinite state space 
case storing values feasible 
possible remains problem states visited times approximate value feasible states 
form generalization needed generalizing experience multiple state action pairs 
literature methods value function approximation 
popular methods include neural networks grossmann decision regression trees mitchell sutton barto 
rl learning value function difficult 
just supervised learning task correct fea tures induced learning algorithm value function appears unstable learning algorithm capable tracking moving target 
problem general proved converge optimal value function 
table case rl algorithms proved converge limit 
large majority generalization mechanisms rl av representations statistical phenomena geometrical distances group states 
typical av learners carve input space means geometrical structures hyperplanes group data points basis distances input space 
intuitive name learning mechanisms fence fill learner thornton 
mechanisms put fences input space fill resulting parts virtually similarly labeled data points 
underlying assumption methods regularities shared data points area input space 
problem problems regularity relation attributes data point 
simple example parity mapping 
classify data set nearest neighbor method create areas similarly labeled data points data points 
regularity data points helpful 
solve problem efficiently look regularities data point relations attributes 
problems solvable exploitation observable statistical effects input data probabilities termed type problems solvable efficiently exploiting relations attributes type clark thornton 
type problems learned av learners cost resources 
example value function tic tac toe stored means typical av learner multi layer perceptron clear problem type 
game state judged looking relations board items 
parity task extreme example type problem 
deal intrinsically relational type problems thornton mentions possible methods av learners 
called virtual essentially sparse coding sutton 
input dimensionality input severely enlarged easier find clusters data geometrical distances 
second method called allows learning algorithm create additional fences learning 
general term constructive algorithms grossmann 
methods specifically designed reinforcement learning problems grossmann chapman kaelbling utgoff precup containing special methods deal concept drift 
deal directly relational type problems need consider relational learning algorithms mitchell generally known inductive logic programming dzeroski lavrac 
relational learning hypothesis space contains logical formulae 
general relation hypotheses partial order space 
examples stated relational language learning consists searching hypothesis space proper predicate definitions 
search process guided providing bias form background knowledge search restrictions mode declarations 
intrinsic properties data reason relational learning representation learning number reasons opting higher order representations rl interesting problems simply large form abstraction 
done means av representations fol natural compact lookup tables av propositional representations able represent structural aspects states actions 
tic tac toe board spatial structure special board positions making fork expressed relation parts board 
structural aspects useful representation represent explicitly favorable 
relational representations enables knowledge transfer 
learned knowledge reused related tasks higher complexity part larger task operational knowledge represented explicitly manipulated 
relational representations allow general intuitive way specifying knowledge 
common knowledge representation terms fo knowledge suitable doing 
standard agent architectures theories subsets extensions fol 
modal logics common agent modelling design 
wooldridge argue kaelbling representation enable representing reasoning objects 
argue evident want connect existing agent architectures programming languages 
able represent objects relations language want connect higher order cognitive notions beliefs desires intentions emotions 
argument technical nature 
necessary se representation language supports objects relations 
domain representing state finite number propositions 
problems occur van laer de raedt fix number objects order objects representation possible relation proposition representation large small domains 

review relational rl deictic representations perspective able generalize relational representations 
mentioned section experience exists statistical learning propositional representations 
finney 
studied intermediate representations deictic representations dr 
drs ordinary language book corridor left semantics relative speaker kaelbling 
drs difficulties associated relational representations capability generalizing objects 
happen thing drop objects hand falls 
main advantage drs avoid arbitrary naming objects 
method uses called markers placed objects moved 
study uses types drs differ amount information agent receives markers 
agents dr normal action set acting environment additionally actions moving markers direction marker focus marker 
drs substantial degree partial observability exchange focusing things agent loses ability see rest 
propositional representations yield large observation spaces full observability drs yield small observation spaces partial observability 
drs longer possible observe state history included 
experimentally study possible advantages drs rl experiments conducted blocks world comparing types drs representation 
task pick green block 
types propositional learning algorithms 
multi layer perceptron mlp representation action trained sarsa algorithm chapman kaelbling incremental tree learner 
experimental results show mlps full propositional representation outperforms drs terms total reward trial opposite true algorithm 
algorithm representations reaches performance level mlp approach trees allowed grow large 
authors idea explained longer action sequence needed drs experimentation shows actual cause exploration process 
dr location focus crucial 
larger domain locations marker placed creates large exploration space 
solved degree adapting action set specifics task solved degrades flexibility dr approach 
action set includes ability agent control attentional focus inherently increases difficulty exploration problem allowing easily spend lot time exploring useless part state space 
authors experiments analysis show drs shown useful extent approaches converting inherently relational problem propositional succesful long run 
naive grows size redundant 
dr problem results inherently long early trials exploration involving markers 
furthermore partial observability drs involves extra problems 
authors experiments show inevitable move relational representations 
relational reinforcement learning relational reinforcement learning rrl dzeroski combination rl inductive logic programming dzeroski lavrac 
combines standard rl algorithm learning relational regression algorithm tilde rt blockeel 
tilde rt seen order extension decision tree algorithm mitchell 
tilde rt representational tool store function order logic decision tree called tree 
rrl collects experience form state action pairs corresponding 
episode actions taken current policy current tree 
newly encountered state action pairs stored values encountered pairs updated learning algorithm 
episode tilde rt induce firstorder decision tree example set 
resulting tree contains nodes basically prolog queries 
different nodes tree share variables 
find value construct prolog kb tree facts state action goal 
running query qvalue return desired value 
example part tree tic tac toe left part action move sq 
qvalue sq 
sq 
qvalue sq 
sq 
qvalue 
decision trees transformed deterministically normal prolog program right part function 
tests nodes depend declarative bias tests nodes higher tree 
individuals referred tree directly variables goal 
note decision tree represents tree containing partitions 
induced tree larger uses predicates simply different experience acquired complex predicates background knowledge 
rrl algorithm extension rrl algorithm abstracts values represents just policy 
rrl episode tree induced called tree induced represents optimality stateaction pairs 
essentially tree maps state action pairs values tree map state action pairs optimal non optimal 
experimentally verified blocks world rrl capable doing rl relational representation able transfer learned knowledge related tasks higher complexity dzeroski 
rrl effective tool relational rl suffers number problems 
episode new tree induced examples clearly efficient 
second set examples constantly growing state action pairs memorized 
third updating values experienced state action pairs requires searching example set 
fourth generalizing done efficient way 
updating value state action pair values pairs leaf updated standard rrl pairs updated experienced exactly 
solve problems incremental algorithm proposed driessens 
algorithm order extension constructive propositional tree building algorithm algorithm chapman kaelbling 
new algorithm called tg incrementally builds kind trees tilde rt 
additionally trees leaf contains statistics concerning values number positive matches examples 
node split seen examples statistical test node statistics significant high confidence 
tg faster original rrl problem set minimal sample size determines split node 
larger value means smaller representation slower convergence 
rrl learning computer game driessens blockeel 
game player controls dig tunnels predators collect valuable things 
learning task difficult natural subtasks distinguished 
avoid monsters shoot collect gold 
usually hierarchical rl 
dietterich hierarchical task decomposition consists sequence subtasks subtasks performed parallel 
learning rrl essentially step process 
subtasks tree induced learning isolated subtask 
learning play complete game done means rrl trees subtasks background knowledge 
way learning knowledge subtasks 
rrl add subtract compare negate values way able learn actions suboptimal subtask optimal task 
learning task rrl features game expected prefers knowledge trees 
symbolic dynamic programming situation calculus sc reiter system specifying implementing dynamical systems fol 
logic typical constructs language actions situations fluents 
actions order terms consisting action symbol arguments 
examples actions example open door goto 
situation order term denoting sequence actions involving operator 
sc uses special situation initial situation empty sequence actions 
example situation term action action action denotes sequence actions action action action third ingredient fluents relations function symbols extensions vary different situations 
relational functional fluents situation dependent therefor argument situation term 
fluents viewed variables system 
example open door true door denoted door open situation president bush denotes january 
formalizing domain consists specifying called action precondition axioms apa successor state axioms ssa 
form oss gives preconditions action form characterizes completely truth values fluent situation 
regression formula action formula holds prior performed iff holds successor state axioms naturally support regression way defined 
outcome regression 
symbolic dynamic programming sdp algorithm developed boutilier 
method solves order mdp specified sc produces logical description optimal value function policy 
language sc extended stochastic actions mapped number normal sc actions probability distribution 
example stochastic action primitive actions af ail corresponding probabilities 
way represented cases action succeeds 
expressed case statement af ail formalization domain probabilities primitive action action effect successor state axioms specified 
algorithm uses fact general function induce function knows model environment vice versa 
short algorithm starts initial partition state space determined goal states values 
number new partitions created logical regression actions 
algorithm ends symbolic description value function 
value function states classical expression pr fo definition means regression nj regr nj important notion derives formula treating stochastic action deterministic nature choices allowed regression directly derive properties pre action state determine value relevant properties post action state 
logical description derive logical description formulas case statement form partition case statement new partitioning derive new partitioning best actions 
start example partitioning goal states partition value case xw ow draw take shortcuts resulting partition case resembles tree previous section 
sdp algorithm capable generating logical description value function fo mdp explicit state enumeration 
scale better methods logical simplification needed method relies heavily simplification partition descriptions 
learning background knowledge previous sections seen complete algorithms relational representations rl domains 
background knowledge rrl sdp specified priori knowledge learned 
hamilton modification decision tree algorithm induce rules class specifies value variable action value 
lorenzo modified version progol ilp algorithm learn action theories situation calculus domain 
action effect learning robotic soccer domain means ilp considered matsui 


previous section seen efforts move propositional representations rl 
deictic representations intermediate step smart available knowledge rl propositional representations shown limited compared relational representations 
relational representations discussed model method sdp model free method rrl 
methods considered core field relational rl today 
rrl sdp partition state space 
sdp explicitly handles case statement containing formulae partition rrl delivers tree implicitly represents partition 
partition rrl built constructively operation split part partition parts 
problem unnecessary splits learning undone 
sdp hand manipulates partitions means logical conjunction simplification 
combines partitions example stochastic action decompositions value partitions create new partition state space 
sdp merge parts state space unnecessary splits occur 
mainly due model 
dr rrl algorithm rrl uses fo extension 
combination exploration problems algorithm generates massive growth decision tree 
due unnecessary splitting nodes 
leafs recreated 
possible problem occurs tg 
general decision split leaf difficult split reversed 
rrl sdp relational language representational power differs 
rrl uses restricted set fol containing horn clauses 
sdp uses sorted order language including quantification variables 
representational power sdp dominates rrl require model 
methods background knowledge form predicate definitions 
sdp additionally requires successor state action effect axioms 
new problems enter rl process turn relational representations 
rrl called query packs order process query generation efficient 
sdp special care needed logical simplification module 
formulae making partition complex syntactically logical simplification needed order keep small possible 
research research performed role representation rl 
research rl focuses performance issues algorithms insight needed interplay representation value functions rational behavior 
interesting conceptual framework psychological notion cognitive economy proposed 
need higher order languages relational learning algorithms rl operationalized model rl boutilier model free rl dzeroski 
systems developed done meet demands kaelbling 
fortunately done traditional rl sutton barto upgraded higher order representations van laer de raedt 
strategies outlined systems discussed briefly 
traditional rl algorithms relational rrl dzeroski way existing specification languages executable logics extending deal uncertainties value functions sdp boutilier 
upgrading traditional rl algorithms new representational tools yields large open space research 
ilp methods applied rrl model learning see section upgraded version algorithm incremental extension rrl tg 
upgraded versions representational tools traditionally propositional part relational rl system 
example fo extensions neural networks bayesian networks getoor rl 
direction upgrading traditional learning mechanisms fo case rl part general machine learning dzeroski lavrac 
field ilp mature learning algorithms capable knowledge representation schemes xml er diagrams relational databases common practice non learning systems 
second direction start fo methods programming specifying non learning agents extend methods probability measures value functions 
done sdp algorithm section situation calculus extended means specifying stochastic actions value functions modified version value iteration concepts sc regression 
agent literature wooldridge find executable logics sc framework reiter reiter possibly extend rl contexts 
problem semantics extending standard logics probability measures halpern relatively understood finding efficient computational methods logics largely open problem 
investigate detail semantics relational representations rl 
strive incorporate rl belief desires intentions bdi reasoning agents rao georgeff wooldridge vr environments 
van 

reinforcement learning agent minutiae extraction fingerprints 
proceedings belgium netherlands artificial intelligence conference pp 

blockeel de raedt ramon 

top induction clustering trees 
proceedings th international conference machine learning pp 

morgan kaufmann 
boutilier reiter price 

symbolic dynamic programming order mdp proceedings th international joint conference artificial intelligence ijcai pp 

san francisco ca morgan kaufmann publishers chapman kaelbling 

input generalization delayed reinforcement learning algorithm performance comparisons 
proceedings twelfth international joint conference artificial intelligence ijcai pp 

san mateo ca morgan kaufmann 
clark thornton 

trading spaces computation representation limits uninformed learning 
behavioral brain sciences 
dietterich 

hierarchical reinforcement learning maxq value function decomposition 
journal artificial intelligence research 
driessens blockeel 

learning hierarchical reinforcement learning concurrent goals 
proceedings fifth european workshop reinforcement learning utrecht netherlands 
driessens ramon blockeel 

speeding relational reinforcement learning incremental order decision tree algorithm 
proceedings ecml european conference machine learning pp 

springer verlag 
dzeroski de raedt driessens 

relational reinforcement learning 
machine learning 
dzeroski lavrac 

inductive logic programming 
dzeroski lavrac eds relational data mining 
springer verlag 
dzeroski lavrac 

relational data mining 
berlin springer 
finney kaelbling oates 
learning deictic report ai memo 
mit ai lab cambridge massachusetts 
finney kaelbling oates 
thing tried didn deictic representations reinforcement learning 
proceedings th international conference uncertainty artificial intelligence edmonton uai 


cognitive economy role representation line learning 
doctoral dissertation university wisconsin madison 
getoor friedman koller taskar 

learning probabilistic models relational structure 
proc 
th international conf 
machine learning icml pp 

morgan kaufmann san francisco ca 
grossmann 

continual learning mobile robots 
doctoral dissertation school computer science university birmingham uk 
halpern 

analysis order logics probability 
artificial intelligence 
nijholt 

embodied agents virtual environments project 
proceedings european symposium intelligent technologies hybrid systems implementation smart adaptive systems spain 
kaelbling oates hernandez finney 

learning worlds objects 
aaai spring symposium 
hamilton 

learning situation calculus domain 
twentieth international conference knowledge systems applied artificial intelligence es 


approximate match rules backpropagation neural networks 
machine learning 
lorenzo 

ilp algorithm learn logic programs reasoning actions 
proceedings progress track th international conference inductive logic programming pp 

matsui seki 

proposal inductive learning agent order logic 
progress reports ilp pp 

london uk 
mitchell 

machine learning 
new york mcgraw hill 
rao georgeff 

modeling agents bdi architecture 
proceedings nd international conference principles knowledge representation reasoning kr pp 

cambridge ma usa morgan kaufmann publishers san mateo ca usa 
reiter 

knowledge action logical foundations specifying implementing dynamical systems 
cambridge massachusetts mit press 
sutton barto 

reinforcement learning 
cambridge mit press 
sutton precup singh 

mdps semi mdps framework temporal abstraction reinforcement learning 
artificial intelligence 
sutton 

generalization reinforcement learning successful examples sparse coarse coding 
advances neural information processing systems pp 

mit press 
thornton 

truth trash learning sense 
cambridge massachusetts mit press 
utgoff precup 

constructive function approximation 
liu motoda eds feature extraction construction selection data mining perspective vol 
kluwer international series engineering computer science chapter 
kluwer academic publishers 
van laer de raedt 

upgrade propositional learners logic case study vol 
lecture notes articial intelligence 
springer verlag 
wooldridge 

multiagent systems 
west sussex england john wiley sons 
