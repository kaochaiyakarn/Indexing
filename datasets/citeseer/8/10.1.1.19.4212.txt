thesis submitted degree doctor australian national university pr imum declaration doctoral studies conducted dr peter bartlett dr jonathan baxter supervisors professor robert advisor 
contents thesis results original research carried collaboration submitted higher degree university institution 
mason september department systems engineering information sciences engineering australian national university canberra act australia iii firstly supervisors peter bartlett jonathan baxter professional guidance enthusiastic support optimistic outlook years 
importantly want friendship making feel colleague student 
want advisor bob input thesis 
gratefully acknowledge nancial support provided australian postgraduate award scholarship cooperative research centre robust adaptive systems 
want yoav freund rob schapire yoram singer opportunity research labs month break studies 
research conducted yoav freund forms part thesis want patience dedicated supervision 
discussions researchers visits anu conferences australia overseas helped increase understanding issues involved thesis 
particular want shai ben david leo breiman yoav freund jerry friedman gunnar rob schapire bernhard sch olkopf dale schuurmans john shawe taylor yoram singer alex smola bob williamson 
sta students past systems engineering life department enjoyable 
particular want de dey wayne jason ford jochen david jung andrew lim gareth loy jeremy louis 
louis deserves special mention sharing oce rst 
support family friends especially light confusion doing past years invaluable 
especially want mates barker making apartment feel 
lastly want 
support encouragement past year worthwhile 
orts proof reading thesis greatly appreciated 
binary classi cation algorithms produce real valued predictions thresholded produce binary classi cation 
real valued predictions viewed measure con dence classi cation 
theoretical analysis algorithms terms classi cation con dence margin dubbed margins analysis 
form analysis shown yield re ned measures classi er complexity realistic measures algorithm performance provided classical vc theory 
thesis concerned application margins analysis classi ers represented thresholded convex combinations 
firstly representing decision trees thresholded convex combinations leaf functions apply margins analysis prove theoretical upper bounds generalization error decision trees combinations decision trees terms proportion training examples small margin 
measures classi er complexity suggested results signi cantly smaller suggested theory 
related describe new type classi er alternating decision tree generalizes decision trees voted decision trees voted decision stumps 
classi er combines accuracy voting methods interpretability decision trees 
simple algorithm constructing alternating decision trees boosting show algorithm decision tree algorithms produces classi ers far simpler easier interpret 
secondly examine impressive empirical performance voting methods suchas popular adaboost algorithm terms margins analysis 
proving theoretical upper bound generalization error thresholded convex combinations classi ers involving general cost functions margin 
describe algorithm doom adjusts weights voted classi er order optimize cost functions suggested theoretical analysis 
experimental results show procedure typically improves generalization error 
develop framework understanding voting methods 
framework existing algorithms viewed gradient descent optimizers margin cost functions 
general class algorithms suggested analysis provide convergence bounds showing converge optimal cost 
combining framework theoretically motivated cost functions obtain new algorithm doom ii 
experimental results show doom ii generally outperforms adaboost particularly presence label noise 
vii list publications functional gradient techniques combining hypotheses improved generalization explicit optimization margins generalization error combined classi ers boosting algorithms gradient descent alternating decision tree learning algorithm error bounds voting classi ers margin cost functions direct optimization margins improves generalization combined classi ers generalization threshold networks combined decision trees combined mask perceptrons generalization decision trees dnf size matter 
contents declaration iii list publications vii list algorithms xiii background material 
classi cation model 
vc theory 
voted classi ers 
decision tree classi ers 
contributions thesis 
threshold margin bounds decision trees combinations classi ers 
decision trees 
optimizing bound 
experiments results 
discussion 
threshold margin bounds combinations decision trees contents combinations combinations 
combinations decision trees 
proofs 
proof theorem 
proof theorem 
discussion 
alternating decision tree generalizing decision trees 
algorithm 
interpreting alternating decision trees 
experiments results 
general margin bounds combined classi ers main result 
cost functions 
discussion 
doom direct optimization margins algorithm 
experiments results 
discussion 
gradient descent view voting methods optimizing cost functions margin 
anyboost 
anyboost 
anyboost 
anyboost margin cost functionals 
understanding existing voting methods 
convergence results 
convergence anyboost 
contents xi convergence anyboost 
discussion 
doom ii algorithm 
experiments results 
discussion 
discussion 
bibliography list algorithms adaboost 
adtree 
doom 
anyboost 
anyboost 

marginboost 
marginboost 
doom ii 
systems learn example applications elds human endeavour 
nancial prediction face recognition practical importance systems adapt speci environment 
machine learning concerned study systems 
thesis concerned simplest problem machine learning classi cation 
classi cation problem stated set objects distinct classes measurements objects produce function new object measurements decides class object belongs 
examples classi cation problems include face recognition automated security system linked surveillance cameras 
system learns recognize personnel trained sample camera images known personnel 
measurements image person face represented dimensional array pixel values classes known personnel intruder 
fraud detection automated customer analysis system decide customers may committing fraud 
system trained transaction records set customers known 
measurements transaction records customer classes fraudulent fraudulent 
medical diagnosis diagnosis support system decides patient details height weight medical observations blood pressure person risk 
developing heart disease 
measurements patient details medical observations classes high risk low risk 
times considerable interest eld due largely impressive practical performance techniques neural networks decision trees support vector machines voting methods 
understanding empirical success algorithms driving force theoretical machine learning decade 
tools theoretical analysis algorithms reached level maturity applied directly development new algorithms exhibit signi cantly improved performance 
current state art analysis described books 
thesis concerned furthering development theory practice algorithms 
background material order able apply quantitative analysis need formalize classi cation problem described 
classical framework problem assumes examples independently identically distributed xed unknown probability distribution measurement space label space 
simple assumption restrictive appear rst glance 
example time series data fails satisfy condition observations usually dependent 
note model assume necessarily deterministic function generating class labels 
general model allows measurements class labels subject noise 
possible models classi cation whichwe consider 
include models assume exists target function belonging particular class functions relax assumption independently generated examples allow drift generating distribution relax assumption xed relationship measurements class labels 
typically measurement space taken subset sake simplicity exclusively deal problem binary classi cation 
analysis signi cantly elegant binary case multi class case results obtained binary case easily extended handle labels 
shall see shortly class labels simpler notation 
general classi cation learning algorithm provided sequence labelled 
examples sequence generally called training set 
examples assumed distributed training set assumed distributed product probability distribution learning algorithm uses information contained produce function hopefully classify new unseen examples measurements 
reason function known classi er 
formally want nd pd small 
pd denote example satis es predicate probability new example generated correctly classi ed small 
quantity known generalization error ideally wewant nd known bayes classi er de ned argmin pd problem nding known bayes problem optimal error pd called bayes error 
unknown necessary infer properties training set practice accomplished nding classi er xed class classi ers denote proportion satis es predicate known training error question guarantees classi er generalization error trying minimize training error 
example consider classi ers classi er chosen class possible classi ers classi er chosen class consisting constant classi ers simply return examples 
particular training set chosen minimize training error 
suppose nite training set zero measure pd pd 
clearly training set chosen training error zero class possible classi ers includes classi er training example corresponding class label 
generalization error classi er 
classi er generalization error training error converge size training set increases 
di erence relationship training generalization error classi ers due di erence complexity classes chosen 
classi er essentially memorizes training set increases size training set complexity classi er constant 
intuitively classi er chosen class complex expect training error close approximation generalization error provided 
training set suciently large 
formalize intuition rstly need de ne precisely mean complexity class classi ers 
rst part section give technical results relate complexity general classes functions 
see results applied quantify complexity classes classi ers 
de nition growth function growth function class functions de ned max xm 
note de nition growth function slightly unusual allow functions real valued 
functions restricted obtain usual de nition growth function special case 
state powerful result due vapnik chervonenkis slightly general form 
result shows grows sub exponentially high probability choice probability expected value function larger sample average small 
pd denote sequence examples satis es predicate lemma class functions pd ed proof result essentially mirrors existing proofs case functions binary valued 
example proof theorem trivially extended handle functions bounded range 
real di erence lies application hoe ding inequality range replaced replacing constant nal result 
hoe ding 
lemma hoe ding inequality independent bounded random variables falls interval probability 
sn 
sn sn sn returning vapnik chervonenkis result speci case functions binary valued result obtained 
result gives upper bound faster rate convergence probability expected value function larger twice sample average 
lemma class functions pd ed proof result follows result theorem due vapnik chervonenkis 
class functions pd ed ed example proof corollary iii suppose ed ed consider separate cases ed ed case ed results terms general classes functions 
remainder section return considering classes binary classi ers 
class classi ers growth function measures maximum possible number unique classi cations set examples size classi ers clearly nite classi ers produce possible classi cations example set wesay shatters de nition vc dimension vc dimension vcdim class classi ers de ned vcdim max shatters maximum exists 

vc dimension largest value size largest set examples arbitrarily classi ed classi ers clearly nite vcdim log 
vc dimension de ned terms growth function quantities clearly related 
result gives relationship growth function vc dimension 
result independently discovered chervonenkis sauer shelah generally referred sauer lemma 
lemma sauer lemma vcdim em second inequality holds result main theoretical motivation nding classi ers training error 
provides high probability upper bound generalization error binary classi er class terms classi er training error complexity term depends growth function class decreases training set size theorem vapnik chervonenkis exists constant probability random choice satis es pd ln ln proof result obtained directly lemma choosing denotes indicator function gives 
noticing result obtained setting upper bound lemma solving 
applying sauer lemma bound growth function terms vc dimension immediately obtain result involving vc dimension growth function theorem vapnik chervonenkis exists constant probability random choice 
vcdim satis es pd ln ln 
results hold value training set size practical performance exhibited classi cation algorithms generally better theoretical results suggest 
results asymptotically close best possible lower bounds form 
reason theory practice generality results 
allowing distribution generating examples means results necessarily considering worst case distributions occur practice 
reason theory ignores important fact classi ers produce real values thresholded generate classi cation 
real valued output interpreted measure con dence classi cation 
classi ers type include neural networks support vector machines combined classi ers produced methods 
notion classi cation con dence formalized margin example 
de nition margin binary classi er form sign margin example de ned yf 
sign function sign maps 
easy see example correctly classi ed yf magnitude con dence classi cation larger margin con dent classi cation 
vapnik rst suggested margins training examples generalization error related 
classi ers represented thresholded real valued functions theoretical results authors giving bounds generalization error terms margins training examples 
results support vector machines neural networks voted classi ers give bounds derived vc theory training examples classi ed large margin 
analysis results thresholded real valued function classi es training examples large margin approximate classi er function simpler class larger margin simpler class 
assume continuous parameters determine classi ers exhibit large margins training examples reliable 
senses slightly perturbing training example large margin cause change classi cation 
slightly perturbing classi er change classi cation training examples large margins 
intuitively classi er reliable training examples generalize 
section see formal analysis terms margins applied voted classi ers 
machine learning concerned idea combining classi ers order improve performance 
way classi ers combined 
binary case voted classi er generally form sign classi er combination called base classi er restricted come xed class class linear combinations classi ers denoted lin lin denote class linear combinations arbitrary possibly countably nite number classi ers usually restriction combination convex 
class convex combinations classi ers denoted denote class convex combinations arbitrary possibly countably nite number classi ers nearly algorithms producing voted classi ers referred voting methods iteratively construct combination classi er time 
denote combination rst classi ers nal combination classi ers simply denoted remainder section devoted describing popular successful voting method adaboost 

adaboost freund schapire adaboost algorithm arguably important developments practical machine learning past decade 
studies demonstrated adaboost produce extremely accurate classi ers base classi ers simple decision stumps complex neural networks decision trees 
adaboost originally developed method solving boosting problem rst posed kearns valiant problem essentially asks weak learning algorithm performs slightly better random guessing boosted strong learning algorithm performs arbitrarily valiant pac learning model 
pseudo code adaboost shown algorithm 
adaboost iteratively combines base classi ers class repeatedly calling base learner returns classi ers goal base learner return classi er small weighted error relative distribution training examples 
choose classi er small 
adaboost starts uniform distribution training examples subsequent rounds adjusts distribution emphasize examples misclassi ed classi ers returned 
weight returned classi er combination weighted error freund schapire proved result bounding training error classi ers produced adaboost terms weighted errors base classi ers round 
theorem freund schapire adaboost generates combined classi er base classi er having weighted training error sign xed constant easy see product term 
provided base classi ers perform xed amount better random guessing relative weighted training set round training error decrease zero exponential rate boosting proceeds 
result gives impressive performance guarantees training error says generalization error 
theorem directly applied 
algorithm adaboost require class base classi ers containing functions 
training set 
base learner accepts training set distribution training set returns base classi ers small weighted error 
return ln exp return bounding vc dimension class combinations classi ers terms vc dimension class base classi ers 
freund schapire apply technique prove result 
theorem freund schapire exists constant probability random choice lin satis es pd sign sign ln ln ln vcdim 
important thing notice result term inside square root grows linearly number classi ers combination 
generalization error increase classi er achieves zero error training set guaranteed happen quickly theorem 
practice seen case 
behaviour illustrated compares training error error independent test set adaboost decision trees base classi ers synthetic data set see 
error rounds training error test error adaboost training test error boosting rounds data set decision trees base classi ers 
notice test error continues decrease training error converged zero 
de nition data set 
notice test error adaboost decreases signi cantly entire run training error converges zero rst rounds 
earlier due bartlett schapire gave rst theoretical justi cation behaviour 
applying techniques bartlett analyze neural networks terms margin proved result 
theorem schapire exists constant probability random choice satis es pd sign yf ln ln vcdim 
result theorem bounds generalization error terms 
rst term depending training error depends proportion training set margin value 
second term depends vc dimension class base classi ers time penalty size prevents small 
note second term dependence number classi ers combination 
bound holds obtain best result chosen best balance terms 
intuitively combined classi er attains large margins training set yf won larger yf large values 
large value means second term roughly vcdim 
vcdim theorem 
adaboost produces large training margins behaviour exhibited explained theorem 
schapire proved generalization theorem showing adaboost fact produce large margins training set 
theorem schapire adaboost generates combined classi er base classi er having weighted training error yf xed constant shown product term proportion training examples margin larger decreases zero exponential rate boosting proceeds 
decision tree classi ers extremely popular years particularly commercial implementations classi cation systems research systems cart sgi ibm intelligent miner 
popularity due largely factors eciency interpretability accuracy 
decision tree classi er partitions measurement space assume subset regions associated classi cation label 
decision tree consists root node set internal nodes including root node set leaf terminal nodes set directed edges connecting nodes 
node tree associated region internal node connected edges child nodes satisfy property 
internal node associated region child nodes associated regions respectively thena child nodes regions form partition parent region 
root node associated entire measurement space proceeding root tree node recursively subdivides successively smaller regions 
associating leaf node classi cation label associated unique classi cation 
depth leaf number edges root node leaf 
order classi ers practical process nding region example falls fast 
typically accomplished 
simple splitting rule de ne partitions node 
common splitting rule test form axis orthogonal hyperplanes partitions hyperrectangles 
decision trees type consist internal nodes exactly corresponding generally known binary decision trees 
example binary decision tree corresponding decision regions shown 
note example 
decision tree corresponding decision regions 
experimental results thesis restricted form binary decision tree decision stump 
simply binary decision tree consisting single internal node split form child nodes corresponding decision tree partitions single axis orthogonal hyperplane decision stumps referred axis orthogonal hyperplane classi ers 
returning general decision trees case measurements lie discrete set real interval 
case associating partitions possible value subsets values discrete measurement useful 
case internal nodes 
common splitting rule test form 
partitions arbitrary convex polyhedra formed hyperplanes 
course splits expensive evaluate axis orthogonal hyperplanes 
vast body literature design decision trees description available methods advantages disadvantages scope thesis 
seminal works mentioned 
breiman describe cart classi cation regression trees algorithm quinlan describes id algorithm evolved algorithms version adaboost 

algorithms top induction construct decision trees 
top induction phase procedure 
rst phase starting root node splitting criterion applied recursively 
criterion determines node corresponding region partitioned 
reviews popular splitting criteria 
recursive splitting process generally applied leaves tree contain examples single class useful splits possible order nodes split irrelevant 
tree constructed rst phase typically zero training error second phase prunes tree reduce tting improve generalization performance 
comparative analysis popular methods 
general idea pruning methods prune nodes tree reduce estimate generalization error 
typically estimates performance held pruning set cross validation estimate penalized version training error 
contributions thesis chapter show existing techniques analyzing thresholded convex combinations classi ers terms margin widely applied 
firstly prove slight generalization theorem allowing base classi ers come di erent classes single class 
representing decision tree thresholded convex combination leaf functions derive upper bound generalization error decision trees 
existing bounds derived vc theory indicate complexity decision tree depends total number leaves tree 
bound uses di erent notion complexity involving distribution training examples leaves 
decision trees classify training examples leaves bound considerably smaller suggested vc theory 
experimental results selection data sets uc irvine repository demonstrate decision trees generated algorithms small numbers leaves classify examples 
chapter derive upper bound generalization error classi ers represented thresholded convex combinations functions thresholded convex combinations 
classi ers include single hidden layer threshold networks voted combinations decision trees 
derived bound depends proportion training examples margin threshold average complexity combined classi ers average weights assigned classi er convex combination 
complexity individual classi ers combination depends closeness threshold 
representation decision trees described previous chapter apply result bound 
generalization error combinations decision trees terms margin classi er average complexity individual decision trees 
decision tree depends similar notion complexity described previous chapter 
previous bounds depend complexity complex decision tree combination decision tree depends total number leaves 
combinations decision trees constructed voting methods shown produce accurate classi ers 
unfortunately classi ers large complex dicult interpret 
chapter describes new type classi er alternating decision tree generalization decision trees voted decision trees voted decision stumps 
simple algorithm constructing alternating decision trees adaboost 
experiments data sets uc irvine repository show alternating decision tree algorithm competitive boosted decision tree algorithms generates classi ers usually smaller size far easier interpret 
chapter upper bound generalization error thresholded convex combinations classi ers terms sample average general cost functions margin 
result seen generalization theorem uses threshold cost function margin 
particular choice cost function upper bounds obtained improve existing results considerably smaller suggested theory 
chapter new algorithm doom directly optimizing family cost functions satisfying conditions main result previous chapter 
experiments data sets uc irvine repository adaboost generate set base classi ers doom nd optimal convex combination classi ers 
generally convex combination generated doom improves test error adaboost convex combination 
cases doom achieves lower test errors sacri cing training error interests reducing new cost function 
authors interpreted theorem mean success voting methods due ability maximize value minimum training margin provided class base classi ers xed 
graphs training margin distributions generated doom suggest value minimum margin critical factor determining generalization performance 
chapter describe general class gradient descent algorithms anyboost choosing linear combinations elements inner product space minimize cost functional 
component linear combination chosen maximize certain inner product 
provide convergence results class algorithms convex cost functional showing particular choice step size 
component linear combination maximizes inner product algorithm converges global minimum cost functional 
examine special case anyboost marginboost combination classi ers optimize sample average cost function margin 
marginboost performs gradient descent function space iteration choosing base classi er include combination maximally reduce cost function 
idea performing gradient descent function space way due breiman 
turns adaboost choice base classi er corresponds minimization problem involving weighted classi cation error 
certain weighting training data base classi er learning algorithm attempts return classi er minimizes weight misclassi ed training examples 
general class algorithms includes special cases number popular successful voting methods including freund schapire adaboost schapire singer extension adaboost combinations real valued functions breiman arc friedman hastie tibshirani logitboost 
algorithms implicitly minimize cost function margin gradient descent 
chapter examine restriction marginboost algorithm doom ii designed iteratively construct convex combination classi ers minimize sample average cost function margin qualitatively similar theoretically motivated cost functions derived chapter 
experiments data sets uc irvine repository demonstrate doom ii generally outperforms adaboost especially high noise situations 
graphs training margin distributions generated doom ii verify algorithm willing give examples hard order avoid tting 
show tting behavior exhibited adaboost quanti ed terms proposed cost function 
outline potential chapter 
threshold margin bounds decision trees decision tree classi ers results vc theory suggest amount training data grow linearly size tree 
empirical results suggest necessary 
example murphy pazzani observed generalization error monotonically increasing function tree size 
see size tree measure complexity consider trees ta leaves tb leaves tb ta size wise complex case classi cation carried leaves classi cation equally distributed leaves intuition suggests simpler tree approximated small tree leaves 
chapter formalize intuition deriving upper bounds generalization error decision trees terms new complexity measure depends distribution leaves induced set training examples 
bounds qualitatively di erent bounds obtained vc theory depend total number leaves tree considerably smaller 
results build previous results bounds generalization error convex combinations classi ers terms margin 
fact section derive re nement theorem voted combinations classi ers base classi ers come classes di erent complexity vc dimension class base classi ers replaced voting weights vc dimensions classes 
section extend results decision trees considering decision tree thresholded convex combination leaf functions 
threshold margin bounds decision trees boolean functions specify leaf examples reach leaf 
apply generalization error bounds classi ers 
resulting bounds depend distribution training examples tree leaves 
section making certain assumptions form distribution result considerably smaller results theory 
section demonstrate assumptions hold practice examining decision trees generated selection data sets uc irvine repository 
combinations classi ers consider voted combination classi ers de ned andf 
theorem gives upper bound generalization error classi ers terms proportion training examples margin threshold complexity term vc dimension class base classi ers 
bound particular convex combination classi ers complexity term necessarily depends complexity complex classi er classi er low voting weight little ect combination 
due fact base classi ers convex combination elements single class bounded vc dimension 
theorem generalizes result case base classi ers classes di erent vc dimensions 

bound theorem depends proportion training examples margin threshold complexity term involves average vc dimension classes base classi ers average terms voting weights assigned base classi ers convex combination 
theorem exists constant probability random choice satis es pd yf yf ln ln ln 

threshold margin bounds decision trees applying sauer lemma bound growth function terms vc dimension immediately obtain result involving vc dimension growth function 
theorem exists constant probability random choice satis es pd yf yf ln ln ln vcdim 
proof theorem proof closely follows proof theorem 
de ne approximating class de ne distribution note probability randomly choosing distribution satis es predicate denoted pg expected value random choice denoted order reduce inequality theorem standard structural risk minimization result require lemma 
denotes indicator function predicate lemma yf yg yg yf proof clearly inequality holds yf 
yf pg yg yf pg yg pg yg yf turn bounded hoe ding inequality 

threshold margin bounds decision trees clearly inequality holds yf 
yf pg yg yf pg yg pg yf yg turn bounded hoe ding inequality 
going bound pd yf terms yf complexity term suitable choice obtain result 
pd pd yf yf pd yg yf applying lemma pd yg ege yg applying lemma pd pd yg yg pd pd yg yg bounded max pd pd yg yg repeated application union bound sum valued functions values di erence probability 
values chosen 
bound probability apply lemma yg 
choice bounded product growth functions choices functions bounded max mc 
threshold margin bounds decision trees letting ln ln ln ln ln ln implies equal application jensen inequality ln ln ln ln ln ln substituting ln ln simplifying gives ln ln constant 
decision trees theorem gives bounds generalization error classi ers represented thresholded convex combinations binary functions 
key technique remainder chapter represent decision tree convex combination 
shall see wehave freedom choosing convex coecients choice ects error estimate yf average growth function 
attempt choose coecients margin optimize resulting bound generalization error 
decision tree leaves denote label associated leaf bethe valued function producing reaches leaf sequence output tree example written sign 
threshold margin bounds decision trees setting gives output tree example written sign de ned 
denoting depth leaf maximum depth leaves tree de ne average depth tree wenow de ne weighting leaves 
simply proportion training examples ed correctly leaf note probability distribution leaves considering examples correctly classi ed 
ready apply theorem 
result bounds generalization error decision tree terms training error complexity terms depend weighting average depth vc dimension class node decision functions 
theorem exists constant probability random choice decision tree represented sign de ned satis es pd sign sign ln ln ln vcdim sequence 
proof setting recalling maximum depth leaves tree de ne class class leaf functions leaves depth de ned denotes class node decision functions 
separating leaf 
threshold margin bounds decision trees functions classes depth 
max max theorem satis es ln ln ln vcdim ln em rst inequality follows denotes depth leaf inequality follows application sauer lemma 
note yf sign sign sign denotes indicator function 
second fact values corresponding leaf classify nal result follows substituting statement theorem simplifying 
wenow consider result compares existing vc bounds decision trees 
bounding vc dimension class decision trees see example chapter apply theorem directly show roughly ignoring constants 
threshold margin bounds decision trees log terms pd vcdim comparison result roughly ignoring constants log terms pd vcdim result large possible minimize third term right hand side ensuring close zero 
assume decision tree asymmetric depth tree roughly logarithm number leaves 
case comparing vc bound ln result 
decision tree uses small number leaves classify training examples weighting skewed possible choose large value small large result signi cantly smaller vc bound 
optimizing bound di erent choices theorem yield di erent upper bounds generalization error tree 
see making assumptions optimize choice sand minimize resulting upper bound 
bound theorem bounded pd sign sign ln ln ln constant 
notice choosing amounts choosing included sum second term right hand side 
rstly assume loss generality ordered case minimize choice choose value set set constant arbitrarily close zero 
rst leaves set set remaining leaves 
leaves set choices reduces arbitrarily 
threshold margin bounds decision trees close pd sign sign reduced problem choosing minimize problem choosing leaf index minimize 
making assumptions form cumulative correct leaf weight function nd closed form solutions new minimization problem 
instance assume bounded function constant obtain result optimizing choice section see unreasonable assumption satis ed practice 
theorem exists constant probability random choice decision tree cumulative correct leaf weight function constant satis es pd sign sign ln ln ln vcdim 
proof bounded equation bounded pd sign sign 
optimize choice continuous parameter simply di erentiating respect equating zero 
gives optimal upper bound pd sign sign cb substitution gives required result 
provided decision tree asymmetric roughly ln 
threshold margin bounds decision trees result theorem roughly ignoring constants log terms pd ln vcdim comparing result vc bound see result worse asymptotically rate compared considerably smaller vc result small stronger assumptions form cumulative correct leaf weight function exponential obtain stronger upper bounds generalization error rates closer vc result 
experimental results section suggest stronger assumptions may dif cult guarantee practice traditional decision tree algorithms 
experiments results examine practical form cumulative correct leaf weight function ran experiments collection data sets taken uc irvine repository 
restricted attention binary classi cation problems 
data sets examined listed table 
data set examples features ionosphere vote credit breast cancer pima indians hypothyroid sick euthyroid splice kr vs kp dis summary examined data sets 
build decision trees data sets quinlan algorithm default options pruning enabled 
leaf weights function training examples interested generalization error entire data set training purposes case 
data sets obtained decision trees cumulative correct leaf weight functions tted upper bounds form distributions data 
threshold margin bounds decision trees sets corresponding upper bounds constant minimized illustrated 
notice data sets optimal constants upper bounds quite small upper bound close cumulative correct leaf weight function 
case remaining data sets 
cumulative correct leaf weight functions decision trees breast cancer cumulative leaf weight upper bound sick euthyroid cumulative leaf weight upper bound credit cumulative leaf weight upper bound hypothyroid cumulative leaf weight upper bound dis cumulative leaf weight upper bound vote cumulative leaf weight upper bound cumulative correct leaf weight functions decision trees generated data sets 
upper bounds cumulative functions form shown comparison 
constant chosen case minimize upper bound 

threshold margin bounds decision trees kr vs kp splice pima indians ionosphere cumulative correct leaf weight functions decision trees generated data sets 
cumulative functions trees generated data sets tted functions form generated data sets illustrated 
cumulative correct leaf weight functions data sets increase slowly functions form data sets upper bounds form prohibitively large constants 
discussion theoretical bounds derived chapter indicate generalization performance decision tree small training error cumulative correct leaf weight function grows slowly 
experimental results chapter demonstrate cases build decision trees cumulative correct leaf weight functions satisfy property 
involve detailed experimental study relationship training error generalization error rate growth cumulative correct leaf weight function 
goal develop new algorithms attempt construct decision trees balance minimization training error minimization 
threshold margin bounds decision trees cumulative correct leaf weight function 
threshold margin bounds combinations decision trees chapter existing techniques analysis thresholded real valued classi ers bound generalization error decision tree representing tree thresholded convex combination leaf functions 
resulting bounds depended training error complexity term related distribution training examples leaves 
section chapter similar techniques derive upper bound generalization error thresholded convex combination functions thresholded convex combinations functions terms margin average complexity combined functions 
complexity combined functions result depends proportion examples close threshold 
section representation decision tree chapter apply general result section derive upper bound generalization error voted combination decision trees terms proportion training examples small margin average complexity decision trees average terms voting weights 
improves theorem special case voted decision trees ways 
firstly complexity single tree combination depends quantity signi cantly smaller vc dimension class decision trees 
secondly depending complexity complex decision tree combination bound depends average complexity combined decision trees 
result considered ways combination theorems proof considerably involved 
proofs section 

threshold margin bounds combinations decision trees combinations combinations chapter restrict attention binary classi ers form sign sign ts ts ts ts ts ts classes real valued functions map 
expressing functions way allows calculate intermediate functions re ned manner 
dealing single complex class functions choose consider simpler classes 
class functions form de ned recall classi er whichis thresholded function margin example 
theorem bounds generalization error function terms proportion training examples small margin average complexity functions complexity function related proportion training examples near threshold threshold 
theorem exists constant probability random choice sample set satis es pd yf yf ln min ln ln ln rt ts ln ln ln rm ln ln tb ln min 
threshold margin bounds combinations decision trees due involved nature proof result deferred section 
general idea random approximating functions approximating random combination approximating rst approximation approximating random combination ts random approximations bound probability large generalization error probability depending second approximation whichwe bound standard structural risk minimization result 
theorem provides bound generalization error depends terms 
rst term depends yf proportion training examples margin threshold whichwe free choose 
second term depends average complexity combination average terms weight assigned divided means choose large possible 
minimize bound choice want choose large possible ensuring yf small possible close possible training error yf 
training examples classi ed large margin choose large value yf close yf 
provided classi er attains large margins training examples optimize give reasonable result 
turning complexity see essentially minimum alternative measures complexity dealing simpler case rst value roughly proportional sum complexities functions ts complexities functions measured growth function 
complexity measure depends terms 
term depends proportion training examples distance whichwe threshold term roughly proportional average complexities functions ts divided means choose large possible 
minimize value choice wewant choose large possible ensuring close zero possible 
output far threshold training examples choose large value small 
case considerably smaller hand output close threshold training examples irrespective choice smaller course bound depends average complexity values relevant signi cant weight combination 
obtain bounds choice need classi er satisfy conditions 
firstly classi er classify examples large margin 
secondly output functions large weight combination far threshold training examples 
section 
threshold margin bounds combinations decision trees shall see conditions satis ed bound applied give signi cant improvements existing results voted combinations decision trees 
similar result theorem obtained yf replaced yf expense exponent half term decreasing considering theorems general convex combinations classi ers dependence number classi ers combination logarithmic dependence result artifact analysis 
combinations decision trees section apply theorem obtained section problem bounding generalization error convex combination decision trees 
rst represent decision tree thresholded convex combination boolean functions 
chapter accomplished manner 
decision tree leaves ts denote label associated leaf ts valued function producing reaches leaf sequence ts ts ts output tree example written sign ts ts ts setting ts ts ts gives output tree example written sign de ned 
de ne tree probability distribution ts leaves ts ts 
ts simply proportion training examples classi ed leaf tree denoting depth leaf tree ts maximum depth leaves tree de ne average depth tree ts ts ts theorem extends main result chapter handle case multiple decision trees combined thresholded convex combination 
case bound terms proportion examples small margin average complexity individual decision trees 

threshold margin bounds combinations decision trees theorem exists constant probability random choice sample set convex combination decision trees satis es pd yf yf ln min ln ln vcdim lnm ln max ln ts ts vcdim lnm ln max ln ln min denotes class node decision functions ts ts 
proof result section 
understand signi cance result rst consider existing results voted combinations decision trees 
bounding vc dimension class convex combinations decision trees see example chapter apply standard vc result comparable rate theorem show roughly ignoring constants log terms pd yf yf vcdim max applying theorem speci case combined decision trees obtain result roughly ignoring constants log terms pd yf yf vcdim note theorem implies similar result replaced max bound large possible minimize second term right hand side ensuring yf larger yf 
choose way comparing max vc bound bound 
combined classi er attains large margins training examples possible choose large value case bound signi cantly smaller vc bound 

threshold margin bounds combinations decision trees returning theorem consider case case result roughly ignoring constants log terms pd yf yf ts ts vcdim lnt minimize bound exactly way bound 
large possible minimize third term right hand side ensuring ts ts close zero 
assume decision trees asymmetric depth tree roughly logarithm number leaves 
case comparing bound ln result 
decision tree combination uses small number leaves classify training examples distribution ts skewed possible choose large values ts ts small large result signi cantly smaller bound 
trees combination satisfy assumption choose large case roughly trees bound 
proofs proof follows random choices independent 
de ne random approximating function sign chosen randomly class functions particular denoted 
de ne random approximating function sign 
threshold margin bounds combinations decision trees ij chosen randomly ij 
allow specify approximated randomly 
class functions particular denoted 
randomly satis es predicate denoted pg similarly ph denote random satisfying similarly eh denote expectation respect random choice respectively 
order simplify follows de ne indicator functions 
denotes indicator function predicate yf yg yh yh yg result require lemma allows relate probability event theorem standard structural risk minimization result 
lemma eh eh yf proof need show ph yh yg 
threshold margin bounds combinations decision trees clearly inequality holds rst term right hand side zero 
consider separately cases 
yg ph yh ph yh ph yh ph yh ph yh yg yg ph yh ph sign yh sign yf ph sign yh yf ph sign yh yf applying union bound ph applying hoe ding inequality 
need hold 
ph yh ph ph ph ph ph 
threshold margin bounds combinations decision trees applying union bound hoe ding inequality 
need show ph yh yg need consider case rst term right hand side zero bound trivial 
yg need ph yh order bound apply union bound bound resulting probabilities separately 
yg yg yh ph yh ph yg yh argument rst case part fact second ph ph ph argument second case part 
need pg yg yf yf need pg yg 
threshold margin bounds combinations decision trees pg yg pg yg pg yg pg yg yf yf 
hoe ding inequality implies pg yg yf need pg yg yf yf need pg yg bound applying union bound bounding resulting probabilities separately 
rst probability bounded pg yf yg hoe ding inequality implies pg yf yg second pg pg hoe ding probability going bound pd yf terms complexity term de ned suitable choice obtain result 
pd pd yf pd 
threshold margin bounds combinations decision trees applying lemma pd applying lemma pd ed eh pd eh applying lemma pd eh eh applying lemma pd ed pd ij ed xed ij denotes class functions form sign 
thing varying class particular choices classes chosen xed restricted 
probability turn bounded max rt max pd ed 
threshold margin bounds combinations decision trees equation follows repeated application union bound ij argument number possible choices variables follows 
values choose respectively 
ways 
suchthat set ij chosen ways set chosen ways 
set ways 
rest ij di erence 
sum valued variables values di erence de nitions order bound time result uniform choice quantization trick similar proposition 
suppose non increasing function nal choice ensure true 
note denotes set non 
pd ed pd ed yh yh pd ed yh yh notice value 
value important 
cases 
fact non increasing function probability pd ed yh 
threshold margin bounds combinations decision trees yh pd ed yh yh order bound apply lemma yh choice certainly yh furthermore bounded product growth functions thresholded suchthat growth function bounded product growth functions choices functions suchthat growth function bounded product growth function xed threshold perceptron inputs product growth functions choices functions gives em theorem sauer lemma bound growth function xed threshold perceptron 
bounded similarly depends product growth functions thresholded 
bound growth function em 
threshold margin bounds combinations decision trees combining equations lemma gives pd pd yf yf max max em mc ln ln ln ln ln ln rt ln ln ln ln ln em ln ln lnn ln mc rt em pd 
threshold margin bounds combinations decision trees pd yf yf setting ln ln nm max gives probability choice sample set satis es pd yf yf notice ln ln ln rst inequality follows fact sum sequence valued variables larger product variables 
observing ln ln ln rt ln ln rt ln ln ln em ln ts ln ln em ln 
threshold margin bounds combinations decision trees obtain ln ln ln ln ln ln rt ln ln ln ts ln ln em ln ln lnn ln substituting collecting terms simplifying gives ln ln tb ln min ln ln rt ts ln ln rm ln ln constant 
simpli cations applied obtain means result holds 
trivial restrictions 
decomposing rst term right hand side union bound gives yf yf yf wehave markov inequality nonnegative random variable second term 
exists constant random choice sample set satis es pd yf yf 
threshold margin bounds combinations decision trees ln ln ln ln rt ts ln ln ln rm ln ln tb ln min order obtain smallest bound choice set 
substituting gives required result 
things note proof 
firstly order simplify details argument little ort optimizing choice constants 
particular choice constants improved uses arbitrarily close factor expense worse constants exponential terms lemma 
secondly readability nal result log terms larger need 
proof recall ts ts ts ts ts ts 
setting max recalling depth tree de ne class class leaf functions leaves depth de ned 
threshold margin bounds combinations decision trees denotes class node decision functions 
separating leaf functions classes depth 
max max ts ln ts ts ln ln ts denotes depth leaf tree proof follows theorem give upper bound yh special case decision tree considerably better 
note bounded yh bounded product growth functions thresholded growth function bounded product growth functions choices functions suchthat notice growth function thresholded simply growth function decision tree leaves 
gives denotes class node decision functions 
bounded growth function decision tree product growth functions nodes valued leaves internal nodes decisions come class 
gives upper bound growth function represent 
threshold margin bounds combinations decision trees decision tree listing breadth rst order node functions leaf labels 
applying upper bound gives growth function bounded wenow proceed exactly proof theorem ln em ln replaced ln choose obtain result 
exists constant probability random choice sample set convex combination decision trees satis es pd yf yf ln min ln ln ln max ts ln ln ln max ln ln ln min obtain result wehave set follows de nition substituted max lastly note de nition ts ts ts ts ts ts ts predicate true 
second equality follows fact value ts values 
threshold margin bounds combinations decision trees corresponding leaf classify complete proof substitute apply sauer lemma bound terms vcdim simplify 
discussion applied general result section give upper bounds generalization error voted combinations decision trees classes classi ers result easily applied 
mention classes 
considering single hidden layer threshold combination single perceptrons obtain bound generalization error terms margin average complexity perceptrons average terms perceptron weights network 
complexity perceptron result related proportion training examples close perceptron threshold 
measure complexity suggested existing vc bounds threshold networks see example related number weights network 
network classi es examples large margin network perceptrons examples close threshold measure complexity considerably smaller 
details see 
binary mask perceptron introduced thresholded combination functions consist products valued decision functions class apply general result obtain bound generalization error voted combination binary mask perceptrons terms margin average complexity mask perceptrons average terms voting weights assigned mask perceptrons 
mask perceptron result related proportion training examples close mask perceptron threshold 
details see 
alternating decision tree part popularity algorithms constructing decision trees cart due fact produce classi ers highly accurate admit simple interpretation 
application voting methods particularly adaboost algorithm construction voted combinations decision trees proven remarkably successful 
classi ers generally signi cantly accurate single decision tree typically far complex dicult interpret 
chapters showed existing techniques analysis thresholded real valued classi ers applied eld 
representing decision tree thresholded convex combination leaf functions derived upper bounds generalization error decision trees voted combinations decision trees 
section chapter show representing decision tree general sort voted combination obtain new type classi er alternating decision tree provides natural generalization decision trees voted combinations decision trees 
alternating decision trees virtue representation voted combination constructed directly existing voting methods 
particular section algorithm learning alternating decision trees schapire singer real valued extension adaboost 
despite generality alternating decision trees interpreted simple techniques applied interpret single decision trees voted combinations decision stumps 
section demonstrate techniques analyzing alternating decision tree generated algorithm data set uc irvine repository 
section experiments algorithm selection data sets taken uc irvine repository 

alternating decision tree experiments show algorithm building alternating decision trees competitive algorithms building voted combinations decision trees 
generalizing decision trees section describing new representation decision trees show representation extended describe alternating decision trees 
chapters showed decision tree represented voted combination leaf functions 
similar manner show alternating decision trees represented voted combination simple functions 
tree classi ers decision tree left decision tree represented alternating decision tree middle general alternating decision tree right 
consider simple decision tree shown left 
tree decision nodes leaves 
call leaf nodes prediction nodes give output prediction tree notion generalize alternating decision trees 
tree maps measurement 
alternative representation decision tree shown middle 
tree decision nodes prediction nodes leaves associated real valued number 
sort diagrams squares denote decision nodes ellipses denote prediction nodes 
tree shown middle measurement classi ed decision tree path tree root leaves corresponding evaluation decision nodes way 
decision tree classi cation label leaf 
classi cation sign sum values prediction nodes 
alternating decision tree path root leaf 
example classi cation sign sign 
easily veri ed tree gives output previous decision tree 
clear di erent trees new type represent decision tree 
call new type tree alternating decision tree consists alternating layers prediction nodes decision nodes 
stated chapter decision tree de nes partition measurement space disjoint regions 
decision tree algorithms recursively subdivide measurement space splitting existing nodes 
decision tree nodes split 
requirement exists alternating decision tree 
multiple splitting corresponds alternating decision tree prediction node child decision node 
consider example alternating decision tree shown middle 
prediction node tree child decision node 
adding extra decision nodes tree obtain general alternating decision tree shown right 
general alternating decision tree measurement classi ed multipath tree root leaves 
decision tree measurement reaches decision node passes child node corresponding outcome decision associated node 
measurement reaches prediction node passes child decision nodes 
path union set paths multi path 
nal classi cation sign sum values nodes visited 
example tree shown right classi cation sign sign 
formal de nition alternating decision tree voted combination decision node functions 
condition boolean predicate elements denote conjunction denote negation denote constant predicate evaluates true 
denote set base conditions 
purposes consider consist tests form 
tests associated decision nodes correspond base conditions 
de ne precondition conjunction base conditions negations base conditions 
sequences tests path tree correspond preconditions 
alternating decision tree classi er function sign real de ned satis es satis es satis es 
alternating decision tree real constants precondition base condition 
constant corresponds value associated root prediction node 
corresponds decision node tree represents sequence tests path root node represents test associated node 
constants values associated true false child prediction nodes respectively 
alternating decision tree de ned decision node satisfy property corresponds decision node root prediction node 
corresponds decision node child true child prediction node decision node corresponding similarly thenf corresponds decision node false child prediction node decision node corresponding example consider alternating decision tree shown middle 
tree decision nodes represented de nitions seen 
output tree sign 
description clear alternating decision tree generalization decision tree 
alternating decision tree generalizes voted combinations decision stumps voted combinations decision trees 
voted combinations decision stumps represented alternating decision tree exactly layers root prediction node set decision nodes prediction nodes associated decision nodes 
voted combinations decision trees obtained restricting alternating decision tree root prediction node child decision node 
alternating decision tree described section similar option tree rst described buntine developed kohavi 
option trees shown provide signi cant improvements generalization error compared 
alternating decision tree single decision trees 
results reported comparable voted combinations decision trees generated bagging 
shall see section algorithm describe section building alternating decision trees achieves better performance levels comparable voted combinations decision trees generated adaboost 
algorithm previous section gave formal de nition alternating decision tree voted combination decision node functions 
representation simple matter modify existing voting method construct alternating decision trees 
unusual feature resulting algorithm class base classi ers consists possible decision nodes child predictions added tree 
means class base classi ers grows tree constructed 
predictions real valued original adaboost algorithm described chapter schapire singer real valued extension adaboost 
original real valued version maintains probability distribution training examples round 
selecting base classi ers round minimize weighted training error algorithm chooses base classi ers round minimize shown previous section decision node tree viewed realvalued classi er partitions measurement space regions corresponding examples true decision node examples evaluate false decision node examples reach decision node 
rst regions associated real valued classi cation corresponding true false child prediction node values respectively region associated prediction zero 
base classi ers partition measurement space schapire singer calculating best predictions partition order minimize 
case formulas reduce choosing round minimize 
alternating decision tree choosing predictions regions corresponding ln ln respectively 
denote total weight relative training examples satisfy predicate similarly denote total weight training examples satisfy labelled respectively 

algorithm adtree require training set 
set base conditions tests form 
ln exp normalizes argmin ln satis es ln satis es satis es return 
alternating decision tree resulting algorithm building alternating decision trees adtree shown algorithm 
algorithm starts nding best constant prediction entire training set 
prediction placed root tree 
algorithm iteratively adds decision nodes child predictions tree 
algorithm maintains set preconditions round 
preconditions set represent paths prediction nodes tree round round algorithm evaluates possible split represented elements prediction node currently tree represented elements chooses split smallest value 
decision node corresponding split prediction node added tree 
distribution training examples updated emphasize examples newly added decision node classi es incorrectly 
kearns mansour analyze decision tree learning algorithms terms boosting 
analysis suggests algorithm similar 
interpreting alternating decision trees section demonstrate alternating decision trees interpreted techniques applied interpret decision trees voted decision stumps 
question improving comprehensibility trained classi ers previously studied authors 
particular friedman describes techniques interpreting voted combinations decision trees 
section refer alternating decision tree shown generated data set 
data set consists medical diagnostics selection patients tested heart disease 
positive output corresponds healthy negative sick 
tree generated adtree algorithm described previous section 
alternating decision tree consists decision nodes average test error 
shall see voted combination decision trees generated data set average test error consists decision nodes 
fact alternating decision tree smaller easier interpretation 
main reason alternating decision trees easy interpret contribution decision node tree understood isolation 
sum contributions gives prediction classi cation 
example tree decision node indicates female strong indicator heart disease gives large positive contribution prediction 
hand decision node indicates having colored vessels strong indicator heart disease 
alternating decision tree normal number vessels colored chest pain type asymptomatic sex female alternating decision tree data set negative numbers correspond sick positive healthy 
large prediction 
decision nodes tree similarly interpreted 
understanding contribution decision nodes isolation analyze interactions decision nodes 
parallel decision nodes nodes rst level essentially independent 
example decision node evaluates true gives nal prediction irrespective decision node evaluates true false 
relationship represented nodes voted combination decision stumps 
sort relationship represented standard decision tree tree need decision nodes 
contrast parallel decision nodes rst level decision nodes second level depend evaluation ancestral nodes 
example decision node depends decision node evaluating true 
indicates worthwhile check patient level colored vessels 
reasoning holds decision node relation decision node 
relationship represented nodes decision tree 
sort relationship represented combination decision stumps 
example root node contributes 
small positive number indicates training set slightly healthy cases examined sick cases 
implies testing predict healthy low con dence 
associate lowlevel con dence prediction 
alternating decision tree absolute value prediction small 
evaluated node tree contribute small negative value classi cation change 
nal prediction sum contributions decision nodes prediction thresholded give classi cation 
testing conditions decision node tree serial fashion view process accumulation evidence heart disease proceed 
point process wehave sum absolute value large total contribution untested nodes small don need continue current sign sum change 
situation occur small example tree common occurrence larger trees 
possible sign change may require contributions opposite sign 
fact decision node limited uence exploited practice 
situations evaluate decision nodes due feature values unknown common occurrence real data due time restrictions 
situations simply consider reachable decision nodes associated predictions large 
reduce classi er accuracy signi cantly nodes ignored little uence classi cation 
indices printed left side decision nodes indicate boosting iteration nodes added 
general lower indices correspond uential nodes added earlier boosting process 
experiments results examine practical performance adtree algorithm described section ran experiments collection data sets taken uc irvine repository 
restricted attention binary classi cation problems 
compared performance types classi ers single decision trees voted combinations decision trees voted combinations decision stumps alternating decision trees 
single decision trees voted combinations decision trees constructed algorithm 
voted combinations decision trees restricted rounds boosting 
voted combinations decision stumps alternating decision trees constructed adtree algorithm section 
voted combinations decision stumps adtree simply restricted generate alternating decision trees single layer decision nodes 
seen adtree quickly ts smaller data sets data set 
occurs adtree build voted combinations 
alternating decision tree decision stumps full alternating decision trees 
reason carefully choose boosting process 
choose number boosting rounds fold cross validation training set experiments 
training set randomly subdivided subsets subset individually validation purposes remaining subsets training purposes 
results averaged 
chose stopping time round average error validation set minimized 
reran algorithm training set stopped chosen iteration 
time consuming method choosing stopping iteration tested adtree runs fold cross validation 
data set randomly subdivided subsets subset individually test purposes remaining subsets training purposes 
repeated times results averaged 
tested runs fold cross validation 
shows typical situations variation training test error function boosting round adtree adaboost decision stumps 
data set algorithms reach best performance boosting rounds 
number rounds increases training error continues decrease test error increases 
overcome tting behavior early stopping boosting algorithm critical 
data set larger probably reason di erence training test errors 
see adtree reaches small error rounds error adaboost decision stumps remains large iterations 
case general structure alternating decision tree gives advantage 
table summarizes average size generated classi er error rounds cleve adtree training adtree test adaboost stumps training adaboost stumps test error rounds kr vs kp adtree training adtree test adaboost stumps training adaboost stumps test average training test error rounds adtree adaboost decision stumps data sets 
average stopping points adtree adaboost decision stumps marked solid vertical lines dashed vertical lines respectively 

alternating decision tree examples adaboost data set decision adtree features boosting stumps labor promoters hepatitis sonar cleve ionosphere house votes vote credit breast cancer pima indians hypo hypothyroid sick euthyroid splice kr vs kp dis summary average classi er sizes boosting adaboost decision stumps adtree examined data sets 
adtree classifier size boosting classifier size adtree classifier size adaboost decision stumps classifier size comparisons average classi er sizes boosting adaboost decision stumps compared adtree examined data sets 
algorithms data sets tested 
data set number examples features listed average size generated classi er algorithm tested 
boosting size average total number decision nodes boosting size average total number decision nodes summed voted decision trees 
adaboost decision stumps 
alternating decision tree adtree size simply average number boosting rounds equal number decision nodes 
comparing size classi ers nd cases classi ers generated adtree smaller generated boosting 
fact half data sets tested classi er generated adtree times smaller classi er generated boosting 
graphical comparison average classi er sizes boosting adaboost decision stumps adtree summarized scatter plots 
table summarizes average test error standard error algorithms data sets tested 
interesting feature experimental results general performed signi cantly better adaboost decision stumps adtree adaboost decision stumps performed signi cantly better adtree 
demonstrates decision trees combinations decision stumps decision trees 
surprisingly data sets best method boosting 
true data sets data sets tree size small 
wehave explanation behaviour 
adaboost data set decision adtree boosting stumps labor promoters hepatitis sonar cleve ionosphere house votes vote credit breast cancer pima indians hypo hypothyroid sick euthyroid splice kr vs kp dis summary test errors standard error boosting adaboost decision stumps adtree examined data sets 

alternating decision tree easier comparison average test errors boosting adaboost decision stumps adtree summarized scatter plots 
plots seen algorithms competitive boosting having 
larger data sets boosting adtree outperform adaboost decision stumps 
smaller data sets adaboost decision stumps generally outperforms adtree probably prone tting 
adtree test error boosting test error adtree test error adaboost decision stumps test error comparisons average test errors boosting adaboost decision stumps compared adtree examined data sets 
general margin bounds combined classi ers previous chapters seen generalization error voted combinations classi ers bounded terms proportion training examples margin value yf complexity term depending vc dimension class base classi ers 
way think results technique adjusting ective classi ers adjusting parameter 
large values correspond low complexity small values high complexity 
large value yf ne distinctions convex combinations class consequently ective complexity class reduced 
nd convex combination class small value yf large value complexity term bound small obtain bound generalization error forced small value order reduce yf complexity term large small vc bounds give better results 
yf yf bounds stated terms sample average yf threshold cost function training margins 
practical standpoint view bounds depending parameterized family cost functions training margins 
family cost functions yf parameterized illustrated 
obtain best possible bound particular convex combination choose cost function optimizing choice order minimize bound 
chapter concerned upper bounds generalization error involving sample average general cost functions training margins 
section give general conditions parametrized families cost functions ensure give error bounds 
general margin bounds combined classi ers cost margin cost functions compared function 
smaller values correspond closer approximations 
convex combinations classi ers 
cost functions de ned sample average function margin example 
prove high probability generalization error convex combination classi ers sample average cost function plus complexity term involving complexity parameter vc dimension class base classi ers 
proof uses similar ideas previous proofs especially simpler 
section examine particular family cost functions improved upper bounds compared existing results convex combinations classi ers 
main result section derive upper bounds generalization error thresholded convex combination classi ers terms sample average certain functions margin whichwe call margin cost functions 
functions mapping interval general result derived section involves family margin cost functions indexed integer valued parameter measures resolution examine margins 
large value corresponding high resolution high ective complexity convex combination gives margin cost function close threshold function yf takes value margin 
case sample average cost function close approximation training error 
small value corresponding low resolution low ective complexity convex combination gives larger margin cost function 
trade ective complexity measured 
general margin bounds combined classi ers larger margin cost function function yf 
de nition gives suitable conditions family margin cost functions ensure trade violated 
particular form de nition important arises proof technique main theorem 
particular functions analysis concern 
de nition family cn margin cost functions admissible function satis es ez cn ez denotes expectation chosen randomly 
section show family threshold cost functions de ned cn example suitable parametrized family growing roughly particular family main result chapter reduces theorem 
shall see better choices possible giving upper bounds improve theorem 
theorem main result chapter provides admissible family margin cost functions upper bound generalization error combination classi ers 
margin cost function cn family bound depends sample average chosen margin cost function complexity term increases theorem admissible family cn margin cost functions exists constant probability random satis es pd yf cn yf nb vcdim lnm ln ln proof de ne approximating class de ne distribution 
general margin bounds combined classi ers probability randomly choosing distribution satis es predicate denoted pg expected value random choice denoted distribution average independent draws de nition 
distribution yg yf chosen randomly distribution de ned 
going bound pd yf terms cn yf complexity term suitable choice obtain result 
pd pd yf cn yf pd cn yf admissibility condition pd yg cn yf pd yg ez admissibility condition pd yg ege yg pd ed yg yg bounded pd ed yg yg application union bound bound probability lemma yg 
choice xed function bounded product growth functions choices functions bounded mc letting ln ln ln ln implies equal application jensen inequality ln ln ln ln 
general margin bounds combined classi ers bounding growth function sauer lemma simplifying gives required result 
cost functions mentioned earlier appropriate choice margin cost function theorem speci case theorem 
consider margin cost function cn straightforward show family admissible proof admissibility essentially reduces proof lemma 
identical requirement proof theorem 
theorem directly obtained theorem admissible family setting ln vcdim obtain immediate improvement result theorem choosing cn ez de ned 
motivation wenow consider family cn ez ln essentially equivalent chosen roughly ln drop exponential term absorbed complexity term nal result 
compares threshold cost function cost functions cn de ned 
notice larger values cost functions cn closer threshold function measures training error 
decreased cost function cn moves away step function compare functions ne distinctions blurred 

general margin bounds combined classi ers cost margin cost functions compared function 
larger values correspond closer approximations 
discussion chapter derived upper bound generalization convex combinations classi ers depends sample average general cost functions margin complexity term involving complexity cost function vc dimension class base classi ers 
chapters investigate practical utility bounds developing evaluating new voting methods attempt produce voted classi ers optimize theoretically motivated cost functions margin suggested bythe analysis chapter 
doom direct optimization margins theorems give upper bounds generalization error voted combinations classi ers terms yf threshold cost function training margins 
voting methods explicitly minimize yf theorem shows adaboost implicitly minimizes cost function 
recall theorem guarantees yf decreases zero exponentially quickly boosting proceeds provided base classi er generated adaboost weighted error smaller positive constant chapter upper bound generalization error voted combinations classi ers terms general cost functions training margins 
adaboost implicitly optimizes cost function training margins generalizes natural consider better explicitly optimizing cost function margin 
main subject 
particular address question useful theoretically motivated cost functions chapter error estimates 
chapter problem minimizing cost functions optimal set classi er weights existing voted combination classi ers 
overcome severe computational dif culties considering family described chapter consider related family piecewise linear cost functions 
section describes doom gradient descent algorithm purpose 
section describe experimental results algorithm selection data sets uc irvine repository 
results show doom adjust weights voted classi er generated adaboost typically improves generalization performance 
cumulative margin distribution plots show cases algorithm achieves lower errors sacri cing training error interests 
doom direct optimization margins reducing new cost function 
margin distributions suggest value minimum margin critical factor determining generalization performance 
algorithm wenow consider select convex coecients sequence binary classi ers combined classi er sign small generalization error 
point supply procedure selecting base classi ers 
experiments simply classi ers provided adaboost aim experiments investigate useful error estimates provided cost functions previous chapter 
chapter considering voting methods general framework procedures choosing base classi ers weights iterative fashion 
theorem face value ignore log terms best error bound obtained weights complexity parameter chosen minimize cn constant cn family cost functions shown left 
main diculties optimizing directly 
firstly cost functions cn negative margins things dicult local methods gradient descent 
secondly theorem provides expression constant practical problems certainly overestimate penalty moderately complex models great 
solve rst problem bound cn monotone decreasing function upper bound margin cost function 
considered sigmoid bounding cost function existence local minima caused diculties gradient descent approaches 
piecewise linear family cost functions de ned shown right 
note parameter plays role complexity parameter theorem case smaller values correspond higher complexity classes 
experiments xed 
second problem optimizing average cost margins plus 
doom direct optimization margins cost margin cost margin theoretically motivated cost functions chapter compared piecewise linear upper bounds onthe functions 
larger values smaller values correspond closer approximations 
values parameter estimate optimal value set 
xed values discrete fairly dense set minimizing choose solution smallest error set 
unfortunately restriction piecewise linear cost functions problem optimizing dicult 
fortunately nature cost function possible nd successful heuristics 
algorithm wehave devised optimize called doom optimization margins 
doom basically form complications take account fact cost function di erentiable wehave ensure lies unit ball problems addressed doom pseudo code algorithm 
order help avoid problems local minima allow weight vector lie ball optimization ball 
increase norm weight vector generally corresponds decrease value cost function weight vector tends naturally approach surface ball optimization proceeds 
weight vector reaches surface ball update direction points ball simply projected back surface ball 
understand algorithm operation rstly observe gradient constant function weights provided example crosses discontinuities provided margin example 
doom direct optimization margins cross 
examples cross discontinuities weight vector constrained lie ball 
central operation doom step negative gradient direction step margin examples hits discontinuities 
update direction projected necessary step ensure lies ball 
margin examples step multi valued generally valued point hits discontinuity simultaneously valued number points 
possible gradient directions tested small step direction steps 
algorithm doom require set base classi ers 
training set 
complexity parameter 
maximum number active constraints machine precision 
procedure fori call random subset size means take left example call continued 
doom direct optimization margins algorithm continued doom argmax call call return argmin distance closest face ball direction ignoring faces intersect call min procedure project project faces ball intersecting gradient directions tested random subset evaluated steps 
gradient direction corresponding maximum decrease cost step update direction directions tested lead decrease cost step examples margins lie discontinuities cost function added step 
subsequent iterations stepping procedure followed step direction modi ed ensure examples move remain discontinuity points cost function 
achieved constraint set projecting update direction orthogonal subspace space spanned vectors base classi ers 
procedure performs projection step whilst ensuring update direction lies face ball step 
progress 
doom direct optimization margins iteration reset zero 
progress procedure terminates 
experiments reported section increase chance nding global minimum doom algorithm called random initial weight vectors solution minimum cost selected 
experiments results experiments section selection data sets uc irvine repository 
binary classi cation problems considered due computational complexity doom algorithm restricted attention smaller uci data sets 
experiments repeated times examples randomly selected training validation test purposes respectively 
results averaged repeats 
experiments base learner exhaustively produces decision stumps 
ensured complexity base classi ers constant 
experiment consisted steps 
firstly adaboost run training data produce sequence base classi ers corresponding weights 
number classi ers combination chosen running adaboost large number iterations choosing combined classi er iteration corresponding minimum error validation set 
doom run classi ers produced adaboost large range values random initial 
value minimum error validation set chosen nal solution 
shows cumulative training margin distribution graphs data sets adaboost doom optimal chosen set 
agiven margin value curve corresponds proportion training examples margin equal value 
test errors algorithms shown comparison short horizontal lines margin 
things worth noting margin distributions generated doom compared generated adaboost 
firstly show value minimum training margin real impact generalization performance 
idea maximizing minimum margin examined breiman grove schuurmans 
breiman voting methods provably maximize minimum margin grove schuurmans maximized minimum margin explicitly linear programming 
cases maximization minimum margin expense margins generally gave worse generalization perfor 
doom direct optimization margins margin pima indians adaboost doom margin hepatitis adaboost doom margin credit adaboost doom margin cleve adaboost doom cumulative training margin distributions adaboost doom data sets 
test errors adaboost doom marked short horizontal lines margin 
mance adaboost 
generalization performance combined classi er produced doom signi cantly better classi er produced adaboost despite having dramatically worse minimum training margin 
clearly demonstrates minimum margin critical factor determining generalization performance 
cost hepatitis adaboost doom error hepatitis adaboost train adaboost test doom train doom test plot cost training test error adaboost doom data set 

doom direct optimization margins secondly margin distributions show balance training error complexity measured 
doom willing sacri ce training error order reduce complexity obtain better training margin distribution 
instance doom training error twice adaboost data set 
despite sacri cing training error doom test error lower adaboost 
reason success seen illustrates changes cost function training error test error function 
error validation set optimal value 
choice large value indicates optimal complexity data set low 
data set reduction complexity important generalization error reduction training error 
relationship cost function training error test error data set shown 
cost cleve adaboost doom error cleve adaboost train adaboost test doom train doom test plot cost training test error adaboost doom data set 
graphical representation di erence test error adaboost doom shown 
improvement test error exhibited adaboost standard error bars shown data set 
results show doom generally outperforms adaboost 
discussion chapter described doom algorithm adjusts classi er weights existing convex combination classi ers order minimize sample average certain piecewise linear family margin cost functions 
experiments data sets uc irvine repository demonstrated doom adjust weights existing voted classi er generated adaboost generally improved generalization performance 
family margin cost functions optimized doom qualitatively similar family suggested theoretical result chapter 
demonstrated improvement 
doom direct optimization margins labor promoters hepatitis sonar cleve ionosphere vote credit breast cancer pima indians examples attributes stump adaboost doom summary test errors single decision stump adaboost decision stumps doom decision stumps uci data sets 
best test error data set displayed bold face 
note doom uses set choose cost function parameter comparing version adaboost modi ed independent validation set early stopping 
error advantage data set labor promoters hepatitis sonar cleve ionosphere vote credit breast cancer pima indians summary average test error advantage standard error bars doom adaboost uci data sets 
results averaged repeats 
generalization performance obtained suggests cost functions chapter useful error estimates 
doom provides method choosing classi er weights base classi ers 
experiments set base classi ers provided adaboost 
set base classi ers generated adaboost suboptimal insofar minimizing cost function concerned 
chapter existing voting methods viewed gradient descent optimizers margin cost functions suitable inner product space 
part new understanding existing voting methods general algorithm iteratively constructing voted classi er choosing base classi er 
doom direct optimization margins iteration minimize sample average cost function training margin 
chapter describe evaluate new algorithm doom ii general algorithm chapter 
doom ii iteratively constructs voted combination classi ers order minimize sample average family margin cost functions qualitatively similar suggested analysis chapter 
gradient descent view voting methods chapter improved upper bounds generalization error voted classi er terms average training data certain cost function margins 
chapter described experiments algorithm directly minimizes similar cost function choice weights associated base classi er 
algorithm exhibited performance improvements adaboost suggests margin cost functions appropriate quantities optimize 
chapter general algorithm marginboost choosing combination classi ers optimize sample average cost function margin 
marginboost performs gradient descent function space choosing base classi er include combination iteration maximally reduce cost function 
idea performing gradient descent function space way due breiman 
turns adaboost choice base classi er corresponds minimization problem involving weighted classi cation error 
certain weighting training data base classi er learning algorithm attempts return classi er minimizes weight misclassi ed training examples 
simpler way view marginboost algorithm 
section describe class algorithms called anyboost gradient descent algorithms choosing linear combinations elements inner product space minimize cost functional 
component linear combination chosen maximize certain inner product 
marginboost inner product corresponds weighted training error base classi er 
section general class algorithms includes special cases number popular successful voting methods including freund schapire adaboost schapire singer extension 
descent view voting methods adaboost combinations real valued functions friedman hastie tibshirani logitboost 
algorithms implicitly minimize margin cost function gradient descent 
section give convergence results anyboost derivatives 
marginboost convex cost function results show particular choice step size base classi er minimizes appropriate weighted error algorithm converges global minimum cost function 
optimizing cost functions margin algorithms section apply di erent machine learning settings primary interest chapter construction voted combinations classi ers form sign base classi ers xed class classi er weights 
training set labelled examples generated wish construct voted combination classi ers form described generalization error pd sign small 
chapter take classi ers minimize sample average cost function margin 
training set suchthat minimized suitable cost function 
note symbol denote cost function real margin yf cost functional function intended interpretation clear context 
choice optimization problem motivated main theorem chapter experimental results chapter 
main theorem chapter provided upper bound generalization error convex combinations classi ers terms sample average cost function margin 
experimental results chapter showed generalization error typically improved doom algorithm modify weights existing voted classi er minimize sample average cost function margin 

gradient descent view voting methods way produce weighted combination classi ers optimizes gradient descent function space idea rst proposed breiman 
treatment shows existing voting methods may viewed gradient descent suitable inner product space 
level view base classi ers combinations elements inner product space 
case linear space functions contains lin set linear combinations functions inner product de ned lin 
anyboost algorithms de ned section convergence properties studied section valid inner product 
example hold case dp wherep marginal distribution input space generated suppose wehave function lin wish nd new add cost decreases small value 
viewed function space terms asking direction decreases rapidly 
viewing cost functional lin desired direction simply negative functional unique function trivial translations suchthat assume di erentiable indicator function restricted choosing new function general possible choose greatest inner product 
choose maximize motivated observing implies rst order greatest reduction cost occur maximizing 

descent view voting methods preceding discussion motivates algorithm iterative algorithm nding linear combinations base classi ers minimize cost 
note allowed base classi ers take values arbitrary set restricted form cost inner product speci ed step sizes 
appropriate choices apply algorithm concrete situations 
note algorithm terminates 
base learner returns base classi er longer points downhill direction cost function 
algorithm terminates rst order step function space direction base classi er returned increase cost 
corresponds reaching local minimum cost functional 
point note gradient descent viewpoint strictly choosing maximize choosing appropriate step size inner product induced norm constant classi cation case 
regression case general inner products convergence proofs section require induced norm bounded choosing maximize 
algorithm anyboost require inner product space containing functions mapping set bounded induced norm 
class base classi ers lin 
di erentiable cost functional lin 
base learner accepts lin returns large value 
return choose return 
gradient descent view voting methods anyboost algorithm return arbitrary linear combination elements class base classi ers 
exibility potential cause tting 
theorem provides guaranteed generalization performance certain classes cost functions provided algorithm returns elements convex combinations elements class base classi ers 
consideration motivates algorithm anyboost normalized version anyboost returns functions convex hull class base classi ers convenience assume class contains zero function equivalently denotes convex cone containing convex combinations functions zero function 
notice stopping criterion anyboost 
see notice iteration lie 
incorporating new component 
corresponds stepping direction corresponding geometrically implies change associated addition 
algorithm anyboost require inner product space containing functions mapping set bounded induced norm 
class base classi ers lin 
di erentiable cost functional 
base learner accepts returns large value 
return choose return 
descent view voting methods anyboost enforces constraint size combined classi ers returned algorithm 
certain classes cost functionals theoretical guarantees generalization performance algorithms see chapter aesthetic perspective constraint natural inner product space setting 
particular ask algorithm perform gradient regularized cost functional form regularization parameter needing refer individual weights combination contrast anyboost 
constraint freedom allow base learner return general linear combinations lin single classi ers modify anyboost base learner allowed return general convex combinations optimal direction anyboost fact pure direction lemma shows provided negation closed element closer negative gradient direction closest elementof lemma negation closed iteration anyboost forany sup proof write suchthat 
inf sup base learner produces combinations powerful base learner returning single classi er case 
true case 
general linear combination lin closer negative gradient direction single base classi er stepping direction lead greater reduction cost function ensuring classi er constructed 
gradient descent view voting methods 
base learner accepts direction attempts choose maximizing easily converted base learner attempts choose lin maximizing procedure call details algorithm 
algorithm essentially works iteratively combining classi ers order residual di erence current combination target direction output substituted output anyboost algorithm 
algorithm require inner product space induced norm containing functions mapping set class base classi ers lin 
di erentiable cost functional lin 
base learner accepts direction returns large value 
starting function lin 
constraints maximal return return main aim chapter optimization margin cost functionals section specialize anyboost anyboost algorithms previous sections restricting attention inner product cost 
case 
descent view voting methods derivative margin cost function respect sensible cost function margin monotonically decreasing positive 
dividing see nding maximizing equivalent nding minimizing iff rewritten distribution de ned nding maximizing equivalent nding minimizing weighted error making appropriate substitutions anyboost yields algorithm marginboost 
similarly making appropriate substitutions anyboost yields algorithm marginboost understanding existing voting methods successful voting methods appropriate choice cost function step size speci cases anyboost algorithm described derivatives 
interpretation adaboost algorithm performs gradient sample average cost function margins examined authors 
see adaboost algorithm shown algorithm fact marginboost cost function need verify distributions stopping criteria identical 
algorithm 
gradient descent view voting methods distribution adaboost written probability distribution clearly substituting gives marginboost distribution cost function algorithm stopping criterion adaboost algorithm marginboost require di erentiable cost function 
class base classi ers containing functions 
training set 
base learner accepts training set distribution training set returns base classi ers small weighted error 
return choose return 
descent view voting methods identical stopping criterion marginboost 
wish choose minimize di erentiating respect setting solving gives ln exactly setting adaboost algorithm 
choice cost function possible nd closed form solution line search optimal step size round 
adaboost performing gradient descent cost functional algorithm marginboost require di erentiable cost function 
class base classi ers containing functions 
training set 
base learner accepts training set distribution training set returns base classi ers small weighted error 
return choose return 
gradient descent view voting methods step size chosen line search 
schapire singer examine adaboost general setting classi ers produce real values indicating con dence valued classi cation 
general algorithm anyboost cost function yf yf base classi ers 
base learning algorithm decision trees directly optimizes exponential cost function margin iteration 
variant boosting reduce gradient descent optimization 
breiman introduced arc algorithm alternative adaboost uses entirely di erent distribution training examples 
combined classi er update arc distribution training examples approximately additive constant normalization factor ensure probability distribution 
rewrite distribution appropriately rede ned step normalize resulting distribution 
exactly distribution assigned marginboost cost function arc essentially marginboost cost function constant step size 
friedman adaboost approximation maximum likelihood 
direct approximation logitboost exhibits similar performance 
logitboost essentially anyboost cost function log step size chosen single newton raphson step 
lee describe iterative algorithm constructing convex combinations basis functions minimize quadratic cost function 
constructive approximation result prove rate convergence algorithm optimal convex combination 
algorithm similar special case anyboost quadratic cost function step size decreasing rate di erence lies fact adding basis function corresponding 
descent view voting methods algorithm cost function step size adaboost yf line search arc yf con dence rated adaboost yf line search logitboost ln yf newton raphson constructive nn algorithm yf summary existing voting methods essentially identical special cases anyboost algorithm derivatives 
gradient direction round lee algorithm adds basis function minimizes quadratic cost directly xed step size 
table summarizes cost function step size choices derivatives reduce algorithms essentially identical existing voting methods 
convergence results section prove convergence results algorithms anyboost anyboost quite weak conditions cost functional prescriptions step sizes results convergence guarantees practice smaller necessary xed small steps form line search 
section interested limiting behaviour anyboost derivatives assume algorithms terminate xed number iterations algorithms terminate due internal termination conditions 
theorem supplies speci step size anyboost characterizes limiting behaviour step size 
theorem lin lower bounded lipschitz di erentiable cost functional lin 
sequence combined classi ers generated anyboost algorithm 
gradient descent view voting methods step sizes anyboost terminates iteration orc converges nite value case lim proof need general lemma 
lemma norm di erentiable functional 
wg lw proof de ne wg 
wg wg wg cauchy schwartz lw lw lw lw substituting wg left hand side gives result 
write 
descent view voting methods lw lemma anyboost terminate 
greatest reduction occurs right hand side maximized 
step size statement theorem 
stated step size terminates 
bounded implies 
norm bounded implies 
theorem shows base learner nd best base classi er round anyboost cost functional convex accumulation point sequence generated anyboost step sizes guaranteed global minimum 
convenient assume class base classi ers negation closed means implies case function maximizes satis es 
ease exposition assumed terminating anyboost simply continues return subsequent iterations theorem lin cost functional properties theorem sequence combined classi ers generated anyboost algorithm step sizes 
assume class base classi ers negation closed round anyboost algorithm nds function maximizing 
sequence satis es lim sup accumulation point satis es inf proof equation follows immediately theorem 
proof general lemma 
gradient descent view voting methods lemma di erentiable convex cost functional inner product space norm 
linear denote perpendicular subspace 
satis es inf proof consider 
forall rewritten implies de nition dividing limit yields implies 
accumulation lipschitz sup negation closure implies lin lin lin lemma implies 
theorem supplies speci step size anyboost characterizes limiting behaviour step size regime 
theorem cost function theorem 
sequence combined 
descent view voting methods classi ers generated anyboost algorithm step sizes anyboost terminates iteration converges nite value case lim proof note step sizes positive 
addition lim clearly second case apply 
loss generality assume 
applying lemma wehave algorithm terminates 
right hand side maximized step size statement theorem 
stated step size lower boundedness implies 
norm bounded implies 
theorem shows base learner nd best base classi er round anyboost cost function convex anyboost guaranteed converge global minimum cost 
theorem wehave assumed terminating anyboost simply continues return subsequent iterations theorem convex cost function properties theorem classi ers generated anyboost algorithm 
gradient descent view voting methods step sizes 
assume class base classi ers negation closed round anyboost nds function maximizing 
lim sup accumulation point sequence satis es inf set convex combinations classi ers proof equation follows immediately theorem 
accumulation equivalently 
proof lemma negation closed write positive 
follows 
discussion chapter gradient descent algorithm anyboost generating linear combinations elements inner product space optimize cost functional related algorithm anyboost generating convex combinations 
convex cost functionals gave prescriptions step sizes algorithms guaranteeing convergence linear convex combination corresponding global minimum cost functional 
cost functionals nonconvex motivated main result chapter results guarantee convergence local minimum 
cost functions depending margins classi er training 
descent view voting methods set anyboost anyboost marginboost marginboost showed existing algorithms combining classi ers viewed special cases marginboost algorithm di ering choice margin cost function step size 
particular adaboost marginboost cost function margin step size equal line search 
chapter examine performance new algorithm doom ii combines margin cost functions suggested theoretical result chapter derivative marginboost algorithm 
doom ii chapter result bounding error convex combination classi ers terms sample average certain cost functions margin 
cost functions suggested result signi cantly di erent cost functions implicitly minimized methods described section 
chapter describe new algorithm doom ii gradient descent optimization procedures chapter cost functions suggested main theoretical result chapter 
details doom ii algorithm section section experimental results showing doom ii typically outperforms adaboost especially true label noise 
addition show theoretically motivated cost functions provide estimates error adaboost sense predict tting behaviour 
algorithm theoretical motivations described chapters propose new algorithm doom ii details algorithm 
doom ii essentially derivative marginboost cost function tanh adjustable parameter cost function 
family cost functions parameterized qualitatively similar family cn parameterized suggested main theoretical result chapter 
families shown 
functions cn directly cost functions practice may cause diculties gradient descent procedure negative margins 
family sigmoid cost functions alleviates problem 
doom ii viewed procedure gradient descent optimiza 
doom ii tion tanh restricted convex combination classi ers base class henceforth refer normalized sigmoid cost function normalized weights normalized convex combination 
choosing value corresponds choosing value complexity parameter theorem 
data dependent parameter measures resolution examine margins 
large value corresponds high resolution high ective complexity convex combination 
choosing large value amounts belief high complexity classi er tting 
conversely choosing small value corresponds belief high complexity classi er avoid tting large margins 
cost margin cost margin theoretically motivated cost functions chapter compared sigmoid cost functions tanh doom ii 
larger values correspond closer approximations 
adaboost perceived resistant tting produce combinations involving large numbers classi ers 
studies shown case base classi ers simple decision stumps 
demonstrated running adaboost hundreds thousands rounds lead signi tting 
number authors showed adding label noise tting induced adaboost relatively classi ers combination 
compares sigmoid cost function doom ii exponential cost function adaboost logit cost function logitboost 
notice exponential logit functions sigmoid function nonconvex large negative margins signi cantly smaller 
value sigmoid cost function increase signi cantly example large negative margin margin decreased 

doom ii cost margin sigmoid exponential logit comparison sigmoid exponential logit cost functions 
allows doom ii adaboost logitboost give examples large negative margins robust label noise 
supported experimental results section 
implementation doom ii described xed small step size experiments 
practice xed line search optimal step size 
worth noting norm classi er weights xed iteration cost function property choice equivalent choosing norm weights cost function tanh 
normalized sigmoid cost function nonconvex doom ii algorithm su er problems local minima 
fact result shows cost functions satisfying marginboost algorithm local minimum rst step 
lemma cost function satisfying 
marginboost nd optimal base classi er rst time step terminate time step returning proof assume loss generality 
assumption satisfy inf takes 
doom ii algorithm doom ii require class base classi ers containing functions 
training set 
base learner accepts training set distribution training set returns base classi ers small weighted error 
xed small step size 
complexity parameter 
tanh tanh return return values wehave marginboost terminate returning simple technique avoiding local minimum apply notion randomized initial conditions gradient descent procedure 
initial margins randomized random initial classi er chosen initial experiments showed techniques somewhat successful guarantee avoidance single classi er local minimum random initial conditions tried computationally intensive prospect 
principled way avoiding local minimum remove rst round continue algorithm returning cost goes rst round 
local minimum cost guaranteed increase rst round 
continue step best available 
doom ii direction attest uphill direction eventually crest hill de ned basin attraction rst classi er start decrease cost 
cost decreases rst classi er safely return rst classi er class available base classi ers 
course guarantee cost decrease rst classi er round rst 
practice problem small values cost function linear case rst classi er corresponds global minimum anyway 
experiments results compare performance doom ii adaboost series experiments carried selection data sets taken uc irvine repository 
considered binary classi cation problems 
experiments repeated times examples randomly selected training validation test purposes respectively 
results averaged repeats 
experiments base learner exhaustively produces decision stumps 
xed class base classi ers avoided problems combined classi er actual classi ers produced base learner 
adaboost validation set perform early stopping 
adaboost run rounds combined classi er chosen round corresponding minimum error validation set 
doom ii validation set set data dependent complexity parameter 
doom ii run rounds optimal chosen correspond minimum error validation set rounds 
typical behaviour test error doom ii proceeds shown various values 
small values test error converges value worse adaboost test error 
increased optimal value test errors decrease 
case data set test errors adaboost doom ii optimal similar 
course adaboost adaptive step size converges faster doom ii uses xed step size 
adaboost doom ii run data sets levels label noise applied 
summary experimental results provided table 
attained test errors shown data set single decision stump adaboost applied decision stumps doom ii applied decision stumps label noise 
graphical representation di erence test error adaboost doom ii shown 
improvement test error exhibited doom ii adaboost standard error bars shown data set 
doom ii test error rounds lambda lambda lambda adaboost test error data set rounds adaboost doom ii 
noise level 
results show doom ii generally outperforms adaboost improvement generally pronounced presence label noise 
sonar cleve ionosphere vote credit breast cancer pima indians hypo splice examples attributes stump label adaboost noise doom ii stump label adaboost noise doom ii stump label adaboost noise doom ii summary test errors single decision stump adaboost decision stumps doom ii decision stumps varying levels label noise uci data sets 
best test error data set displayed bold face 
note doom ii uses set choose cost function parameter comparing version adaboost modi ed set early stopping 
ect normalized sigmoid cost function exponential cost function best illustrated comparing cumulative margin distributions generated adaboost doom ii 
shows comparisons data sets label noise applied 
margin value curve corresponds proportion training examples margin equal value 
curves show trying increase margins negative examples adaboost willing sacri ce margin positive examples signi cantly 
contrast doom ii 
doom ii error advantage data set sonar cleve ionosphere vote credit breast cancer pima indians hypo splice noise noise noise summary average test error advantage standard error bars doom ii adaboost varying levels noise uci data sets 
results averaged repeats 
gives examples large negative margin order reduce value cost function 
margin breast cancer wisconsin noise adaboost noise doom ii noise adaboost noise doom ii margin splice noise adaboost noise doom ii noise adaboost noise doom ii cumulative training margin distributions adaboost doom ii label noise data sets 
adaboost su ers tting minimizes exponential cost function margins cost function certainly relate test error 
value proposed cost function correlate adaboost test error 
theoretical bound suggests right value data dependent cost function test error closely correlated 
shows variation normalized sigmoid cost function exponential cost function test error adaboost uci data sets rounds 
values curves averaged random train validation test splits 
value chosen running doom ii various values choosing 
doom ii corresponding minimum error validation set 
curves show strong correlation normalized sigmoid cost adaboost test error 
data sets minimum adaboost test error minimum normalized sigmoid cost nearly coincide 
data sets adaboost test error converges tting occur 
data sets normalized sigmoid cost exponential cost converge case data set exponential cost converges signi cantly test error 
data sets adaboost initially decreases test error increases test error tting occurs 
data sets normalized sigmoid cost mirrors behaviour exponential cost converges zero 
rounds sonar adaboost test error exponential cost normalized sigmoid cost rounds cleve adaboost test error exponential cost normalized sigmoid cost rounds labor adaboost test error exponential cost normalized sigmoid cost rounds vote adaboost test error exponential cost normalized sigmoid cost adaboost test error exponential cost normalized sigmoid cost rounds adaboost data sets 
costs scaled case easier comparison test error 
verify choice step size little ect generalization performance gradient descent procedures compare adaboost modi ed version xed step sizes called adaboost 
adaboost rst classi er weight weight 
comparison test errors algorithms various values shown 
expected changing value xed step size simply translates test error curve log scale signi cantly alter minimum test error 

doom ii test error rounds adaboost epsilon epsilon epsilon epsilon test error data set rounds adaboost adaboost 
discussion authors described techniques directly optimizing margins related quantities 
onoda uller show versions adaboost modi ed regularization robust noisy data 
friedman hastie tibshirani friedman describe general additive models regression classi cation various cost functions speci cases voted decision trees 
du helmbold algorithms attempt produce combined classi ers uniformly large margins training data 
freund describes boosting algorithm uses example weights similar suggested main theoretical result chapter 
developed algorithms arc adaboost breiman arc gv respectively attempt optimize margin cost function xed proportion training data 
discussion thesis divided main areas 
firstly wehave derived new upper bounds generalization error popular classes classi ers represented convex combinations including general voted combinations classi ers decision trees voted combinations decision trees 
bounds qualitatively di erent follow existing vc theory depend distribution margins training examples 
bounds considerably smaller existing results training examples classi ed large margin 
shows existing structural complexity measures classi ers number nodes decision tree number classi ers voted combination appropriate determining generalization error measures complexity depend training data provide accurate measure classi er performance 
bounds may help explain qualitative performance classi ers represented convex combinations 
secondly wehave new understanding voting methods developed new algorithms outperform existing techniques producing voted combinations classi ers 
viewing voting methods framework performing gradient descent cost functionals inner product space wehave seen existing voting methods gradient descent optimizers margin cost functions 
particular popular adaboost algorithm viewed procedure producing voted classi ers minimize sample average exponential cost function training margins 
adaboost remarkably successful practice su er tting particularly high noise situations 
adaboost exponential cost function places emphasis examples large negative margins 
new algorithms attempt optimize sensible cost functions motivated new bounds 
discussion generalization error convex combinations classi ers 
experimental results demonstrated algorithms generally outperform adaboost particularly presence label noise 
leaves interesting questions unanswered 
bounds generalization error combined classi ers theoretical bounds derived help explain empirical performance voting methods actual performance better bounds suggest 
proving tighter bounds important open problem 
techniques chapter derive upper bounds convex combinations classi ers terms general cost functions margin applied speci classes base classi ers 
particular results chapters decision trees combinations decision trees simpli ed generalized terms general cost functions margin 
obtaining lower bounds generalization error convex combinations classi ers terms general cost functions margin provide evidence bounds capturing important factors generalization error 
multi class classi cation regression theoretical results discussed chapters deal binary classi cation 
extensions results multi class case obtained techniques similar described 
results generalized handle general cost functions distribution generalization error general cost functions training set 
general anyboost algorithm described chapter restrictions label set applied multi class classi cation regression 
suitable choices cost functions general settings open problem 
interesting area see 
decision trees better understanding complexity decision trees required ultimate goal development new algorithms advantage new measures 
discussion complexity 
analysis distribution training examples leaves practical experiments may provide new insights 
theoretically analysis weaker 
representation decision trees thresholded convex combinations leaf functions convenient somewhat arti cial 
alternative approach understanding decision trees terms boosting 
alternating decision trees done alternating decision trees considered practical alternative existing methods 
firstly method construction currently computationally intensive 
friedman describe technique called weight trimming reducing size training set iteration adaboost algorithm 
technique combined methods sub optimal splits cached splits easily applied algorithm give signi cant speed improvements 
secondly generalization performance alternating decision trees expected 
voting method may full advantage power class alternating decision trees 
applying marginboost algorithm alternative cost functions building alternating decision trees may give improved results 
anyboost optimization nonconvex margin cost functions suggested theoretical analysis chapter proven dicult 
ort worthwhile unclear 
extensive experimental comparison marginboost various cost functions convex nonconvex regularized voting methods needed answer question 
anyboost algorithm brie discussed 
obtaining theoretical results case combination classi ers worthwhile constraint natural inner product space setting 
important practical advantage constraint allows base learners return general linear combinations base classi ers iteration 
chapter described algorithm converts base learner returning single base classi er base learner returning linear combinations base classi ers 
experimental analysis needed evaluate gives signi time 
generality anyboost algorithm means applied inner product space just spaces functions 
opens possibility applying 
discussion boosting techniques machine learning problems 
complexity base classi ers voted combinations voting method experiments thesis decision stumps base classi ers 
results complex base classi ers decision trees signi cantly ect relative performance voting methods 
understanding possible interactions voting method class base classi ers important open problem 
bibliography aldous vazirani 
extension valiant learning model 
proceedings st symposium foundations computer science pages 
ieee computer society press los alamitos ca 

bartlett 
neural network learning theoretical foundations 
cambridge university press 
anthony shawe taylor 
result vapnik applications 
discrete applied mathematics 
bartlett 
learning slowly changing distribution 
proceedings th annual workshop computational learning theory pages 
acm press 
bartlett 
sample complexity pattern classi cation neural networks size weights important size network 
ieee transactions information theory 
bartlett ben david kulkarni 
learning changing concepts exploiting structure change 
proceedings th annual conference computational learning theory pages 
acm press 
bartlett fischer gen exploiting random walks learning 
proceedings th annual acm workshop computational learning theory pages 
acm press 
long 
complexity learning drifting distributions 
information computation 
bauer kohavi 
empirical comparison voting classi cation algorithms bagging boosting variants 
machine learning 
baum haussler 
size net gives valid generalization 
neural computation pages 
bibliography blake keogh merz 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
blum chalasani 
learning switching concepts 
proceedings th annual workshop computational learning theory pages 
acm press 
breiman 
bagging predictors 
machine learning 
breiman 
arcing classi ers 
annals statistics 
breiman 
prediction games arcing algorithms 
neural computation 
appear 
breiman friedman olshen stone 
classi cation regression trees 
wadsworth belmont 
buntine 
learning classi cation trees 
statistics computing 
buntine niblett 
comparison splitting rules induction 
machine learning 
craven 
extracting comprehensible models trained neural networks 
phd thesis university wisconsin madison 
appears uw technical report cs tr 
devroye gy lugosi 
theory pattern recognition 
applications mathematics stochastic modelling applied probability 
springer new york 
devroye lugosi 
lower bounds pattern recognition learning 
pattern recognition 
dietterich 
experimental comparison methods constructing ensembles decision trees bagging boosting randomization 
machine learning 
appear 
dietterich kearns mansour 
applying weak learning framework understand improve 
proceedings th international conference machine learning pages 
morgan kaufmann 
domingos 
knowledge acquisition examples multiple models 
machine learning proceedings fourteenth international conference pages 
bibliography drucker cortes 
boosting decision trees 
advances neural information processing systems pages 
morgan kaufmann 
du helmbold 
geometric approach leveraging weak learners 
computational learning theory th european conference 
appear 
ehrenfeucht haussler 
learning decision trees random examples 
information computation 
esposito malerba semeraro 
comparative analysis methods pruning decision trees 
ieee transactions pattern analysis machine intelligence may 
fayyad irani 
minimized decision tree 
proceedings seventh national conference arti cial intelligence pages 
fayyad irani 
attribute selection problem decision tree generation 
proceedings ninth national conference arti cial intelligence pages 
frean downs 
simple cost function boosting 
technical report department computer science electrical engineering university queensland 
freund 
boost majority algorithm 
proceedings twelfth annual conference computational learning theory 
appear 
freund mansour 
learning persistent drift 
computational learning theory nd european conference pages 
springer verlag 
freund schapire 
experiments new boosting algorithm 
machine learning proceedings thirteenth international conference pages 
freund schapire 
decision theoretic generalization line learning application boosting 
journal computer system sciences 
friedman 
greedy function approximation 
technical report stanford university 
friedman hastie tibshirani 
additive logistic regression statistical view boosting 
technical report stanford university 
bibliography grove schuurmans 
boosting limit maximizing margin learned ensembles 
proceedings fifteenth national cial intelligence pages 
helmbold long tracking drifting concepts minimizing disagreements 
machine learning 
hoe ding 
probability inequalities sums bounded random variables 
journal american statistical association 
kearns 
computational complexity machine learning 
mit press 
kearns mansour 
boosting ability top decision tree learning algorithms 
proceedings th acm symposium theory computing pages 
acm press 
kearns valiant 
cryptographic limitations learning boolean formulae nite automata 
journal association computing machinery january 
kohavi kunz 
option decision trees majority votes 
machine learning proceedings fourteenth international conference pages 
kowalczyk developing higher order networks empirically selected units 
ieee transactions neural networks september 
lee bartlett williamson 
ecient agnostic learning neural networks bounded fan 
ieee transactions information theory november 
maclin opitz 
empirical evaluation bagging boosting 
proceedings fourteenth national conference arti cial intelligence pages 
dietterich 
pruning adaptive boosting 
machine learning proceedings fourteenth international conference pages 
mason bartlett 
generalization threshold networks combined decision trees combined mask perceptrons 
proceedings ninth australian conference neural networks pages 
mason bartlett 
generalization error combined classi ers 
journal computer system sciences 
appear 
bibliography mingers 
empirical comparison selection measures decision tree induction 
machine learning 
murphy pazzani 
exploring decision forest empirical investigation occam razor decision tree induction 
journal arti cial intelligence research 
quinlan 
induction decision trees 
machine learning 
quinlan 
programs machine learning 
morgan kaufmann 
quinlan 
bagging boosting 
proceedings thirteenth national conference arti cial intelligence pages 

ensemble learning methods classi cation 
master thesis department computer science april 
german 
onoda 
uller 
soft margins adaboost 
technical report nc tr department computer science royal holloway university london egham uk 
sch smola mika onoda 
uller 
arc ensemble learning presence outliers 
smola bartlett sch olkopf schuurmans editors advances large margin classi ers 
mit press cambridge ma 
appear 
sauer 
density families sets 
journal combinatorial theory series 
schapire freund bartlett lee 
boosting margin new explanation ectiveness voting methods 
annals statistics october 
schapire singer 
improved boosting algorithms con dence rated predictions 
proceedings eleventh annual conference computational learning theory pages 
schwenk bengio 
training methods adaptive boosting neural networks 
advances neural information processing systems pages 
mit press 
shawe taylor bartlett williamson anthony 
framework structural risk minimisation 
proc 
th colt pages 
acm press new york ny 
shelah 
combinatorial problem stability order models theories nitary languages 
paci journal mathematics 
bibliography simon 
general bounds number examples needed learning probabilistic concepts 
journal computer system sciences 
valiant 
theory learnable 
communications acm november 
vapnik 
estimation dependences empirical data 
springer verlag 
vapnik 
nature statistical learning theory 
springer 
vapnik 
statistical learning theory 
john wiley 
vapnik chervonenkis 
uniform convergence relative frequencies events probabilities 
theory probability applications 
vapnik chervonenkis 
theory pattern recognition 
russian nauka moscow 
german translation theorie der verlag berlin 

theory learning generalization applications neural networks control systems 
springer new york 
webb 
experimental evidence utility occam razor 
journal arti cial intelligence research 
