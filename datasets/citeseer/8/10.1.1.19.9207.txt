information maximization approach overcomplete recurrent representations oren haim sompolinsky racah institute physics center neural computation hebrew university jerusalem israel daniel lee bell laboratories lucent technologies murray hill nj principle maximizing mutual information applied learning overcomplete recurrent representations 
underlying model consists network input units driving larger number output units recurrent interactions 
limit zero noise network deterministic mutual information related entropy output units 
maximizing entropy respect feedforward connections recurrent interactions results simple learning rules sets parameters 
conventional independent components ica learning algorithm recovered special case equal number output units recurrent connections 
application new learning rules illustrated simple dimensional input example 
unsupervised learning algorithms principal component analysis vector quantization self organizing feature maps principle minimizing reconstruction error learn appropriate features multivariate data 
independent components analysis ica similarly understood maximizing likelihood data non gaussian generative model related minimizing reconstruction cost 
hand ica algorithm derived regard particular generative model maximizing mutual information data nonlinearly transformed version data 
principle information maximization previously applied explain optimal properties single units linear networks symplectic transformations 
proceedings show principle maximizing mutual information generalized overcomplete recurrent representations 
limit zero noise derive gradient descent learning rules feedforward recurrent weights 
show application learning rules simple illustrative examples 
output variables input variables mn feedforward weights recurrent weights network diagram overcomplete recurrent representation 
input data influence output signals feedforward connections signals interact recurrent interactions information maximization infomax formulation ica considers problem maximizing mutual information dimensional data observations fxg input network resulting dimensional output signals fsg 
consider general problem signals dimensional representation overcomplete signal components data components 
consider situation signal component influence component recurrent interaction ji network diagrammed fig 
feedforward connections described matrix recurrent connections matrix network response deterministic function input ij ik nonlinear squashing function 
case mutual information inputs outputs functionally dependent entropy outputs sjx distribution dimensional manifold embedded dimensional vector space nominally negatively divergent entropy 
shown appendix probability density related input distribution relation det susceptibility jacobian matrix defined ij result understood terms singular value decomposition svd matrix 
transformation performed decomposed series transformations orthogonal transformation rotates axes diagonal transformation scales axis followed orthogonal transformation 
volume element input space mapped volume element output space volume change described diagonal scaling operation 
scale change product square roots eigenvalues 
relationship probability distribution input output spaces includes proportionality factor det formally derived appendix 
get expression entropy outputs log det log det brackets indicate averaging input distribution 
learning rules eq 
see minimizing cost function tr hlog equivalent maximizing mutual information 
note susceptibility satisfies recursion relation ij 
ij ik kj gw gk ij ij ij ij ik solving eq 
yields result ij interpreted sensitivity recurrent network ith unit output changes total input jth unit 
derive learning rules network parameters gradient descent shown detail appendix 
resulting expression learning rule feedforward weights 
learning rate matrix defined vector ii multiplying gradient eq 
matrix ww yields expression analogous natural gradient learning rule 

similarly learning rule recurrent interactions 
case equal numbers input output units recurrent interactions previous expressions simplify 
susceptibility matrix diagonal substituting back eq 
learning rule results update rule 
zx known infomax ica learning rule recovered special case eq 

results fitting filters dimensional hexagon distribution sample points 
examples apply preceding learning algorithms simple dimensional input example 
input point generated linear combination twodimensional unit vectors angles coefficients taken uniform distribution unit interval 
resulting distribution shape unit hexagon slightly dense close origin boundaries 
samples input distribution shown fig 

second order cross correlations vanish structure data described higher order correlations 
fix sigmoidal nonlinearity tanh 
feedforward weights set overcomplete filters learned applying update rule eq 
random normalized initial conditions keeping recurrent interactions fixed 
length rows constrained identical filters projections certain directions dimensional space 
algorithm converged iterations 
examples resulting learned filters shown plotting rows vectors fig 

shown different local minimum solutions 
lengths rows left unconstrained slight deviations solutions occur relative orientation differences various filters preserved 
recurrent interactions investigate effect recurrent interactions representation fixed feedforward weights point directions shown fig 
learned optimal recurrent interactions eq 

depending length rows scaled input patterns different optimal values seen recurrent connections 
shown fig 
plotting value cost function strength uniform recurrent interaction 
small scaled inputs optimal recurrent strength negative effectively amplifies output signals signals negatively correlated 
large scaled inputs optimal recurrent strength positive tend decrease outputs 
example optimizing recurrent connections performs gain control inputs 
effect adding recurrent interactions representation 
cost function plotted function recurrent interaction strength different input scaling parameters 
discussion learned feedforward weights similar results ica model learn overcomplete representations 
algorithm need perform approximate inference generative model 
directly maximizes mutual information outputs inputs nonlinear network 
method advantage able learn recurrent connections enhance representational power network 
note approach easily generalized representations simply changing order matrix product cost function 
needs done order understand technical issues regarding speed convergence local minima larger applications 
possible extensions optimize nonlinearity adaptively change number output units best match input distribution 
acknowledge financial support bell laboratories lucent technologies israel binational science foundation 
appendix relationship input output distributions general relation input output distributions sjx deterministic mapping conditional distribution response input sjx wx ks 
adding independent gaussian noise responses output units considering limit variance noise goes zero write term sjx lim 


ks wx ks output space partitioned points belong image input space 
points outside image input space 
consider point inside image 
means exists wx ks 
small 
expand wx ks defined eq 
get sjx lim 


det lim 

det expression square brackets delta function eq 
get det characteristic function belongs image input space zero 
note case square matrix expression reduces relation det appendix derivation learning rules derive appropriate learning rules need calculate derivatives respect set parameters 
general derivatives obtained expression tr tr feedforward weights order derive learning rule weights calculate ab wlm ac cb lm ac wlm cb bm ac lm cb definition see ac wlm ij ai ij wlm jc ij wlm ij wlm ij lm ij ik derivatives satisfy recursion relation similar eq 
wlm 
il xm ij lm solution wlm il xm putting results eq 
trace get gradient descent rule eq 

recurrent interactions derive learning rules recurrent weights calculate derivatives ab respect lm ab klm ac klm cb ai ij klm jc cb definition obtain ij klm ij lm il jm derivatives obtained relations klm klm klm il results recursion relation similar eq 

combining results calculating trace get gradient descent learning rule eq 

jolliffe 
principal component analysis 
new york springer verlag 
haykin 
neural networks comprehensive foundation 
nd ed prentice hall upper saddle river nj 
jutten herault 
blind separation sources part adaptive algorithm neuromimetic architecture 
signal processing 
hinton ghahramani 
generative models discovering sparse distributed representations 
philosophical transactions royal society 
pearlmutter parra 
context sensitive generalization ica 
iconip 
bell aj sejnowski tj 
information maximization approach blind separation blind deconvolution 
neural comput 

barlow hb 
unsupervised learning 
neural comput 

linsker 
local synaptic learning rules suffice maximize mutual information linear network 
neural comput 

parra deco 
statistical independence novelty detection information preserving nonlinear maps 
neural comput 

amari cichocki yang 
new learning algorithm blind signal separation 
advances neural information processing systems 
lewicki ms sejnowski tj 
learning overcomplete representations 
neural computation 
