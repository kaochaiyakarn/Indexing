reactive scheduling parallel systems robert ross dissertation submitted clemson university partial fulfillment requirements degree doctor philosophy computer engineering december walter iii december graduate school dissertation entitled reactive scheduling parallel systems written robert ross graduate school clemson university 
recommend accepted partial fulfillment requirements degree doctor philosophy major computer engineering 
walter iii advisor reviewed dissertation recommend acceptance robert geist adam hoover ron sass accepted graduate school parallel computing integral high performance computing uniquely sufficient 
adoption parallel computing additional supporting technologies required 
parallel supporting technology providing high speed data storage parallel computing environments 
parallel systems emerged see main stream research optimizing systems open area 
particular techniques optimizing parallel focused disk performance optimization resources equal greater impact performance 
looked adaptive techniques optimizing systems focused caching prefetching 
reactive scheduling rs combines adaptive methods scheduling algorithms optimize service order workload system state 
parallel virtual file system pvfs parallel file system developed researching issues parallel described 
second selection additional scheduling algorithms added pvfs study workloads performed system 
third model developed matches pvfs performance workloads studied 
fourth model scheduling techniques combined create implementation rs pvfs 
implementation tested compared best case performance seen workload study showing improvement task service time original pvfs system 
drawn viability rs technique directions study described 
table contents list tables vi list figures vii disk arrays raid 
parallel 
optimization techniques 
optimization problem 
reactive scheduling 
contributions 
outline 
related workload studies 
concurrent file system analysis 
project 
scalable initiative characterization 
caching prefetching writeback strategies 
transfer mechanisms 
data sieving 
phase 
disk directed 
stream 
adaptive systems 
adaptive policy selection 
portable parallel file system ii 
parallel virtual file system related 
user libraries 
user level file systems 
kernel supported file systems 
galley parallel file system 
pvfs design 
pvfs manager metadata 
daemons data storage 
application programming interfaces apis 
iv trapping unix calls 
pvfs request servicing 
performance results 
scaling nodes 
read performance limitations 
lessons learned 
workload study test system 
algorithms 
test applications 
single block accesses 
strided accesses 
random block accesses 
observations 
reactive scheduling baseline model 
matching model data 
algorithm efficiency functions 
comparing final model data 
implementing reactive scheduling pvfs 
results utilizing reactive scheduling 


appendices original system model single request model 
multiple request model 
improved resource performance modeling 
system model implementation 
model parameter values calculating baseline parameters 
integrating workload effects 
accounting scheduling algorithms 
list tables metadata example 
model variables 
parameter values 
optimization efficiency values 
network model parameters 
disk model parameters 
model variables 
vi list figures configurations 
example workload 
optimization alternatives 
performance measurements 
nested strided example 
file striping example 
stream example 
file access example 
partitioning parameters 
specifying blocking matrix 
trapping system calls 
creating accesses request 
jobs service 
pvfs read performance 
pvfs write performance 
read performance dropoff 
test workloads 
single block read performance 
single block write performance 
single block read performance small uncached accesses 
single block read performance small cached accesses 
strided read performance 
strided write performance 
strided read performance small uncached accesses 
strided read performance small cached accesses 
random block read performance 
random block write performance 
random block read performance small uncached accesses 
random block read performance small cached accesses 
random block read performance 
random block write performance 
random block read performance small uncached accesses 
random block read performance small cached accesses 
vii comparing opt caching baseline model small accesses 
comparing opt caching baseline model large accesses 
comparing opt caching baseline model small accesses 
comparing opt caching baseline model large accesses 
comparing observed behavior model 
comparing observed behavior model small accesses 
rs performance caching small accesses 
rs performance caching large accesses 
rs performance caching small large accesses 
rs performance small accesses assuming cache 
rs performance large accesses assuming cache 
best case rs performance 
model parameters 
viii chapter performance improvements computing technology vastly paced improvements storage technology 
computing power depends raw processing power input output capability 
applications utilizing high performance computing significant input output requirements disparity advances computation hardware forced new approaches providing applications 
examples applications include satellite image processing involves large detailed multi band images genome research centers matching sequences large databases 
disk arrays raid important advancement high performance disk arrays redundant arrays inexpensive disks raids 
disk arrays simply collections smaller expensive disks replace larger ones 
connected single host disks concert provide higher bandwidth potentially higher capacity storage element 
raid extends account decrease mean time failure mtbf array parity mirroring scheme provide data redundancy 
disk arrays raid provide higher performance storage single machine parallel computing adds dimension problem 
parallel computing involves utilization multiple computers processors simultaneously solve problem 
typically processors communicate network share data solution process 
illustrates bottlenecks single disks disk array configurations 
compute nodes cns processors perform computation parallel system nodes ions data storage 
single disk case see cns access data single disk typically biggest performance bottleneck 
disk array case multiple disk resources cns access resources single host 
leads connection single host cns bottleneck 
avoid problem method spreading file data collection processors providing multiple points access devised 
parallel solutions solve problem utilizing multiple nodes seen 
ion cn cn cn 
ion 
cn cn cn 
cn cn cn 
ion ion ion single disk parallel disk array configurations parallel parallel concept direct operations parallel machine multiple resources connected multiple links service 
achieved number ways including separate networks additional nodes attached existing network attachment storage systems directly compute processors 
particular area study parallel parallel file systems multiple resources manner similar traditional file systems 
presenting resources way semantics access traditional sequential files may maintained 
generation parallel file systems constructed high performance commercial parallel machines ibm sp intel paragon 
machines primarily designed parallel computation high performance communication systems severely outperformed disk systems attached machine 
research groups machines parallel studies techniques improving performance tailored environment 
optimization techniques part environments research place early techniques tended focus optimizing disk performance 
techniques commonly discussed 
data sieving technique reduces number disk requests cost increased network traffic phase technique 
disk directed technique reorders data transfer force blocks accessed disk optimal order may negative effects utilization network 
techniques perform tested systems part gap performance disk network throughput systems 
new parallel computing platforms emerged including clusters workstations piles pcs beowulf computers 
constructed commodity components commonly includes fast ethernet ide disk interfaces 
components roughly order magnitude performance commercial parallel machines network typically lower latency higher throughput storage system 
coupled commodity software obvious resources consistently outperform components system 
emergence new different platforms created situation traditional parallel optimizations useful contexts flexible applied cases 
needed general approach optimization parallel approach recognize components system parameters workloads determine performance disks network cache 
data set view data stored server 
example workload provide mechanism determining resources currently limiting performance system system parameters workload characteristics 
information schedule operations apply technique best utilize available resources 
new approach cornerstone generation parallel systems added existing systems order provide better performance wider range situations 
optimization problem discuss example workload attempt optimize server side improved performance 
workload dimensional data set stored set servers row major order 
entire data set written application tasks application simultaneously traffic seen file system time 
shows dimensional data set distribution blocks server stored dimensional sequence 
disk oriented optimization network oriented optimization service order service order optimization alternatives approach optimization attempt access disk blocks sequentially 
lead highest possible disk performance 
shows order blocks serviced optimization 
alternative approach attempt optimize network traffic 
common approach service requests simultaneously assumption provide network layer opportunities transfer data 
protocols tcp example difficult attain peak network utilization request streams 
approach shown 
approaches advantages disk bottleneck optimally accessing blocks win 
hand network bottleneck network optimization right choice 
useful effects optimizations order really know approaches affect performance observe actual behavior system 
figures see performance measurements similar workload run real system 
servers serve fourteen client tasks 
axis graph tracks amount data accessed single server 
compare performance algorithms default network oriented net opt services ready requests disk oriented algorithm disk opt services disk location order 
application service time shown 
application service time mean time takes tasks collection complete operation 
means time shown maximum time required task complete 
vertical bar indicates amount physical memory available user processes system serves upper bound amount cache available run time 
see large accesses disk optimization provides lower application service times 
focuses small accesses 
see small consistent advantage network optimization 
results expected mean task service time variance figures respectively entire range access sizes disk optimization provides lower mean service times substantially higher variance tasks 
workload disk optimization services request time means tasks serviced finished quickly wait long time service starts 
see performance data small accesses network optimization best constraints larger ones disk optimization preferred 
expected availability cache effect transition takes place 
noting effects optimizations leads create scheduling algorithms try apply optimizations 
chapters attempt closely examine correlation resources workload performance apply reactive scheduling automatically choose appropriate scheduling algorithms 
access size kbytes application write service time net opt disk opt system memory access size kbytes application write service time small accesses net opt disk opt access size kbytes task write service time net opt disk opt access size kbytes task write variance net opt disk opt performance measurements reactive scheduling reactive scheduling rs approach method selecting applying correct optimization workload 
reactive scheduling decentralized server side approach optimization parallel focus utilizing system state workload characteristic data available locally servers potentially application hints passed application requests decisions schedule service application requests 
approach resource oriented focuses relationship workload state system resources order affect improvements system performance 
opposed common file oriented approach seen adaptive cache policy systems apply rules looking behavior file level 
allows effectively focus multiple accesses affect system doing allow rs provide benefits complicated workloads 
rs approach consists components set scheduling algorithms system model selection mechanism 
scheduling algorithms give options terms service collection requests system time allowing tailor service optimize disk access network access cache utilization 
system model uses system specific known values network disk bandwidth system state cache availability information describing workload progress predict available optimizations perform utilized 
selection mechanism maps available system workload information parameters model 
uses model predict performance avail able optimizations chooses predicts perform best 
components provide means increasing performance parallel systems adapting scheduling service best fit workload system 
describes new approach optimization parallel model performance accounts disk network memory resources 
validate model beowulf parallel computer adding reactive scheduling parallel virtual file system pvfs parallel file system designed implemented testing parallel techniques cluster workstation environments 
model develop technique predicting scheduling algorithm performance uses system parameters described measured workload characteristics 
technique select scheduling algorithm tailored workload system question 
reactive scheduling approach validated number workloads areas development discussed 
contributions describe reactive scheduling technique allows parallel server react changes workloads system state changing method schedules service 
number contributions result 
parallel file system pvfs designed implemented facilitate parallel research 
pvfs combines conventional wisdom application patterns new scheduling technique stream described create working parallel file system test bed parallel re search 
open source freely available parallel file system possible 
second contribution workload study presents evidence correlation workload characteristics performance various scheduling algorithms 
previous indicated disk directed approaches described appropriate situations 
contribute reactive scheduling technique combines system modeling scheduling system increases system performance real systems 
technique extended include new scheduling approaches thorough models order increase effectiveness 
outline chapter discuss related including previous workload studies strategies caching prefetching transfer mechanisms adaptive systems 
chapter describe design performance pvfs 
include discussion issues involved designing parallel system description related parallel file system overview components system outline interfaces available application designers study system performance node pc cluster 
chapter describes scheduling algorithms added pvfs order facilitate implementing rs 
cover algorithms operate cover performance system workloads scheduling algorithms 
provides data necessary validate system model developed chapter 
chapter presents design reactive scheduling system validation 
build model component rs system developing simple algorithmic model predicts performance round robin scheduling algorithm 
model supplemented additional component account scheduling changes 
model tuned data collected workload study shown accurate indicator performance 
completed cover implementation selection mechanism integration model pvfs file system constraints placed system input data available run time 
performance data compare rs implementation best case data obtained chapter 
point system features facilitate effective implementation rs 
chapter draws point areas study 
cover effectiveness new scheduling algorithms learned workload study accuracy system model practical implications integrating rs real parallel file system 
chapter related number research projects influenced development 
particular workload characterization studies previous optimization techniques attempts adaptive systems directly impacted described 
previous efforts covered chapter 
workload studies workload studies helped researchers parallel better understand behavior applications parallel systems 
knowledge influenced direction field 
section cover studies particular importance point commonalities access patterns lead greater understanding applications parallel systems 
concurrent file system analysis intel concurrent file system cfs subject number early performance studies parallel study cfs performed touchstone delta machine 
simple tests characterize performance system 
noted throughput single node limited fast node generate requests small requests adequate obtain performance 
read performance single node dropped large number disks 
indicating limiting number disks accessed simultaneously single client beneficial 
nitzberg examine performance characteristics cfs ipsc compute nodes nodes 
set simple read write tests characterize performance 
file pre allocation double buffering large block sizes necessary obtain maximum performance 
addition necessary split compute nodes groups approximately nodes restrict file system access group time reading order place limit number concurrent accesses 
allowed maintain performance allowing system better prefetch cache data 
allowing nodes simultaneously access system reduced performance 
tests particular interesting point necessity prefetching caching maintaining high performance parallel systems time show prefetching caching adequate maintaining performance workloads clients environment 
project results project described 
project designed characterize behavior production parallel workloads level individual reads writes 
authors traced workloads machines cm ipsc numerous scientists running applications 
hoping discover commonalities parallel applications machines terms number files read written size files typical read write request sizes requests spaced ordered 
differentiate sequential consecutive requests sequential accesses higher file offset point previous request process ended consecutive ones exactly point request ended 
write files accessed sequentially readonly files 
write files written consecutively 
applications process write data separate file 
read files accessed consecutively indicating read multiple applications 
third files accessed single request 
examining sizes intervals requests files accessed interval sizes 
request sizes regular applications distinct request sizes 
traces showed simple strided access pattern common 
simple strided pattern requests separated regular spacing stride 
files accessed pattern segments groups requests simple strided innermost strided pattern reading records skipping records second strided pattern reading records skipping records outermost strided pattern reading records skipping corresponding nested strided patterns dimensional dataset access nested strided example pattern cases 
segments usually consisted separate requests 
addition simple strided patterns nested strided patterns common 
nested strided pattern simply application multiple simple strided patterns allowing user build complex descriptions stored data 
shows example nested strided access case utilizing strided patterns order access block data set 
simple strided pattern describe data retrieve row case records 
pattern inside second strided pattern describe data retrieve plane records records case rows 
regions described inner strided pattern denoted dashed boxes 
third strided pattern utilizes previous describe data volume access unit case records skipping 
conclude parallel consists wide variety request sizes requests occur sequential consecutive patterns great deal interprocess spatial locality nodes 
believe strided request support programmer interface node important parallel systems effectively increase request sizes lowers overhead provides opportunities low level optimization 
results directly impacted design number parallel file systems highlight importance non contiguous request capability 
scalable initiative characterization parallel study performed crandall scientific applications designed paragon 
selected scalable initiative applications group applications intensive provided variety access patterns 
premise examine patterns access applications determine generalizations patterns access high performance applications access patterns discuss authors reasons decisions regarding patterns order determine chose certain approaches 
instrumented applications pablo performance environment order obtain information parameters duration calls 
applications exhibited variety access patterns requests sizes simple characterization 
conversations application programmers capabilities system match desired ones resulted complications application code reduced scope feasible problems 
example couple applications programmers easier read data single node distribute remaining nodes parallel access modes available modes allow particular pattern access needed 
addition small requests common 
application programmers aggregate requests application providing capabilities file system transforming requests prefetching caching preferred 
leads idea file system policies fixed adapt patterns access currently executing applications 
name approaches providing adaptation hints applications access patterns pattern identification file system matching access characteristics known patterns 
furthermore point small temporary files high performance systems leads change file system priorities general system attempting maximize utilization bandwidth system trying reduce volume disk common interactive systems 
emphasizes pattern matching altering policies prefetching caching 
conjunction reactive scheduling aid maintaining peak performance tuning policies addition scheduling match current workload 
caching prefetching writeback strategies early studies indicate double buffering adequate high performance parallel studies indicate availability proper memory prefetching caching paramount performance 
kotz ellis discuss test various caching prefetching strategies parallel file systems 
associate caches files caching logical blocks physical disk blocks 
notice called delayed writeback policies typical multiuser systems tailored type write traffic usually seen parallel promote scheme call writes buffer back disk number bytes written buffer equal size buffer bytes 
aging system push blocks disk eventually bytes written effectively falling back delayed writeback 
scheme tailored sequential access patterns common parallel workloads seen 
tests processes disks blocks buffers mbyte write size 
test single file accessed single application run 
tests caches blocks performed best blocks equivalent double buffering 
note particular application disks written order processes simultaneously large cache buffers allowed application achieve higher bandwidth 
test policies ffl forces disk write write request ffl writeback forces disk write buffer needed block ffl forces disk write buffer replaceable processor note choice writeback algorithms addition choice number buffers process large impact performance 
wrote back 
writeback delayed writes long resulting poor performance disk tried catch write stream 
compromise making mistakes delaying writes long writeback 
policy consistently performed near best performance 
somewhat limited scope clearly demonstrates effects caching writeback policies parallel conclude large caches useful cases high disk contention case occurs simply processes writing disks time 
unfortunately common occurrence parallel applications sequentially accessing data sets strided patterns disk performance simply match rest system 
importantly policies show advantages appropriately scheduling disks 
led directly disk directed discussed section 
adaptive caching writeback covered section 
transfer mechanisms investigation performance problems systems led researchers reconsider data moved applications servers 
new approaches request ing data ordering accesses optimally better utilizing network bandwidth devised 
techniques build basic concepts non contiguous requests collective non contiguous requests mean request mechanism extended allow number disjoint regions requested single unit example column records matrix stored row major order requested single non contiguous request 
collective approach focuses relationships task requests parallel application 
specifically collective approaches combine organize access groups tasks application order attain higher performance 
section describe techniques optimally performing data transfer system architectures 
data sieving data sieving technique efficiently accessing noncontiguous regions data files 
define direct read method naive approach perform single call contiguous region 
note result large number calls low granularity data transfer typically result poor performance 
data sieving technique number disjoint regions accessed reading block data containing regions regions interest extracted 
note advantage single call additional data read disk stored memory sieving process 
find system question intel touchstone delta advantages outweigh disadvantages large percentage accesses 
technique fact benefit systems slow networks reduces number requests significant startup time 
percentage data transferred desired high pay 
appropriate technique reducing overhead multiple requests network bound systems descriptive requests allow server perform non contiguous reads capability available perform sieving server side reducing network traffic 
phase phase access strategy described 
attempt avoid performance penalties incurred directly mapping distribution data disks distribution processor memories 
technique data read disks number processors redistribute data processors final processor memory distribution 
strategy tested intel touchstone delta concurrent file system cfs 
processor delta limited number nodes substantial network bandwidth processors arranged mesh topology 
direct access method defined discussed essentially direct read method covered number processors 
approach processor reads data nodes directly single requests contiguous region 
test performance row block column block cyclic distributions note cases transposition necessary performance direct access method orders magnitude worse cases data accessed large chunks 
phase technique obviously breaks stages 
phase number processors involved chosen match nodes 
single processor typically request needed data disk 
reduces number requests hitting disk avoiding poor performance seen direct access tests large number processors hit smaller number systems 
second phase processors previously read data disks calculate final destination block data perform necessary transfers 
takes advantage additional bandwidth compute processors quickly complete process 
order technique compute processes communicate organize transfer means collective available 
authors promote interface allows user declare data decomposition processor memories 
decomposition chosen number popular distributions changed dynamically 
run time system provided perform necessary utilizing underlying parallel file system case cfs handle actual physical storage 
technique excellent example assumption disks bottleneck 
data transfer organized way single request streams hit disk fewer larger requests may 
turn help disk accesses cost second transfer network potentially large amounts data 
clearly network significantly outperform disks tested systems 
number ideas useful systems resources bottleneck 
example reducing number requests file system stream large requests reduce total number requests tend create larger packets 
systems may paramount network traffic consist large packets optimal performance 
additionally systems number simultaneous independent accesses hitting server impact performance discussed section reduction clients accessing servers benefit systems 
disk directed disk directed combination number techniques data transfer parallel systems 
developed data sieving phase techniques study designed assuming non contiguous request capabilities collective available 
addition constraints number requirements met ffl interface application interface system pass information system ffl system direct access disk store data ffl system accurate method predicting optimal disk access orders requirements met disk directed technique uses information passed data requested collective request order determine list physical blocks retrieve disk contain data 
sorts blocks optimal access ordering uses double buffering transfer data disk buffer transferring data compute nodes 
potential advantages phase disk permutation steps overlapped time technique 
phase technique permutation occurs application nodes disk occurred 
second data items move network pass network twice phase communication nodes spread thoroughly disk access process phase network isn second permutation phase 
situations mainly network performance order disk performance perform better reduced total communication single phase operation 
systems network performance severely disk performance phase technique better performer 
far contributions rounded transfer method number things offer 
non contiguous requests result fewer larger packets 
second promote ordering scheme optimization server side 
sense optimize disk access static scheme examples help complex system sense system capable determining cost transferring particular packets ordering transfers accordingly 
derivative disk directed called server directed proposed implemented 
part panda system technique utilizes high level multidimensional data set interface performs array chunking uses disk directed techniques logical level 
determining physical block locations logical file locations determine optimal ordering file data stored underlying local file systems 
characteristics combined proved capable utilizing full capacity disk subsystems test system range array sizes numbers nodes 
stream stream approach attempts address network bottlenecks parallel systems 
stream technique developed part parallel virtual file system pvfs project described detail chapter 
stream concept combining small accesses efficient large ones applied data transfer network 
data transferred considered stream packetized manner best fits protocol 
similar technique known message coalescing interprocessor communication integral part stream removal control messages stream 
reduces total amount data transferred accomplished calculating data ordering side stream 
strictly technique optimizing network traffic 
coupled server focuses network network directed peak performance maintained variety workloads particularly network performance lags disk performance data servers cached 
adaptive systems previous closely matches performed pablo scalable performance tools group university illinois urbana champaign 
group developed number techniques analyzing workloads choosing policies analysis 
technique integrated parallel file system 
selection system file system covered 
adaptive policy selection focus artificial neural network ann approach classification access patterns parallel systems 
improve approach hidden markov models hmms perform classification provides thorough control policies neural network 
system added portable parallel file system ppfs order test performance benefits real system :10.1.1.40.3622
works concentrate patterns file access caching prefetching policies tuned optimize 
system server client levels 
continue discuss approach tuning prefetching caching 
add component performance steering uses number performance sensors monitoring things mean disk service time cache hits help optimize prefetching caching 
system addition reactive scheduling help maximize benefits caching prefetching 
additionally methods classification eventually applied problem reactive scheduling improve model 
promotion performance steering supports reactive scheduling concept showing example dynamic tuning action 
authors discuss adaptive prefetching alternative collective compare performance disk directed number workloads point prefetching performed user level opposed disk directed part parallel system proper 
implementation prefetching easier especially commercial machines system modifications feasible 
portable parallel file system ii implementation portable parallel file system ii ppfs ii described 
implementation includes advances pablo group adaptive cache policy selection adaptive disk striping 
ppfs ii system includes set new managers handle reception input data state distributed machine policy decisions data direct remaining components system implement desired policies 
addition implementation adaptive caching include system adaptive disk striping 
scheme alters striping previous indicating optimal striping units dependent resource contention 
performed group shares characteristics significant differences 
approach ppfs ii centralized sensor data gathered point global decisions 
system hand rely local data decision making decentralized system 
second ppfs ii system uses input strictly caching striping decisions system modify ordering network disk transfer 
provides potentially direct control performance possibly require effort implement 
third ppfs ii algorithms concentrate data file basis attempts identify specific patterns access reactive scheduling approach resource oriented focuses general characteristics workload 
sense coarse grain rely fixed set patterns 
fact techniques cooperatively single system 
pablo group policy selection system effectively cache reactive scheduling system works optimize data transfer 
potential investigated left 
chapter parallel virtual file system cluster computing emerged mainstream method parallel computing application domains linux leading pack popular operating system clusters 
researchers continue push limits capabilities clusters new hardware software developed meet cluster computing needs 
particular hardware software message passing matured great deal early days linux cluster computing cases cluster networks rival networks commercial parallel machines 
advances broadened range problems effectively solved clusters 
area commercial machines maintain great advantage parallel file systems 
production quality high performance parallel file system available linux clusters file system linux clusters large intensive parallel applications 
fill need developed parallel file system linux clusters called parallel virtual file system pvfs 
pvfs number groups including ones argonne national laboratory nasa goddard space flight center 
researchers pvfs system research tool 
designed pvfs goals mind ffl provide high bandwidth concurrent read write operations multiple processes threads common file 
ffl support multiple apis native pvfs api unix posix api apis mpi io :10.1.1.39.8219
ffl common unix shell commands ls cp rm pvfs files 
ffl applications developed unix api able access pvfs files recompiling 
ffl robust scalable 
ffl easy install 
addition firmly committed distributing software open source 
chapter describe designed implemented pvfs meet goals 
performance results linux cluster nasa goddard space flight center detailed performance study progress reported 
outline strengths weaknesses system discuss areas improvement 
rest chapter organized follows 
section discuss related area parallel file systems focusing basic approaches implementing software support parallel concentrating parallel file system galley 
section describe design pvfs 
performance results section 
section discuss lessons learned experiences design point areas improvement releases 
related basic approaches providing software support parallel ffl user libraries ffl user level file systems ffl kernel supported file systems architecture chosen underlying software constrains capabilities software handle complex requests take best advantage system resources 
time software complexity increases attempts directly control disk network cache 
subsections cover general approaches parallel software discuss implications 
user libraries user libraries fairly straightforward simple method providing parallel applications 
library calls created parallel application perform usually disks local application tasks 
libraries portable easy implement modify making excellent tools research 
require application wishing access data stored link library common file system tools useless accessing parallel files 
typically support interface tailored single class application suffer poor performance due generic message passing interfaces storage data local node file systems 
general libraries parallel best conjunction set applications access data sets similar manner latency tolerant 
type support groups developing compilers support parallel need library functions perform application 
user level paging mechanism example library implementation 
paging mechanism designed allows data pages remote processes accessed 
simple polling mechanism built library handles requests pages application processes need additional processes kernel support 
disks local application process data storage 
library hybrid implementation library routines application processes perform separate set processes known coalescing processes exist duration application execution order handle coalescing processes cps receive requests application processes determine blocks need read data files sort combine blocks optimally access disk local cp 
coalescing processes additional advantage allowing disks located nodes involved computation accessed 
furthermore executing coalescing processes lifetime application design coalescing process greatly simplified 
user level file systems user level file systems complicated method implementing parallel involve creating set processes handle tasks accessing file data performing permission checks 
processes subset constantly available handle requests application system 
requests applications generic interface opposed domain specific ones discussed previous section 
number advantages method 
low level interface communicating file system processes allows number application level interfaces developed interact file system reusing underlying file system code 
second flexible interface allow common file utilities interact file system rewriting utilities 
third processes store file data allows additional optimization file storage data transfer methods 
processes allow nodes compute nodes case user libraries 
time type system portable user library times processes data transfer mechanisms optimized target platform 
development implementation system takes time process code developed addition user interfaces 
portable parallel file system ppfs user level file system designed rapid experimentation variety platforms 
implements various caching prefetching data placement striping data sharing policies 
called caching agents provide shared caches application tasks data servers handle underlying unix file systems 
mpi pass messages processes 
parallel file system developed emory university clusters workstations 
consists data servers service coordinator psc library functions access file system 
psc involved major system events opening closing files general file accesses 
supports call files multiple zero bytes 
modes accessing files ffl global file seen linear sequence bytes shared file pointer ffl independent file seen linear sequence bytes private file pointers ffl segmented process sees separate segment 
global independent modes mapped single linear address space fixed stride round robin ordering 
uses transaction approach writes guaranteed completely succeed completely fail 
logging guarantee case system failure logging turned volatile option 
mainly fault tolerance 
vesta file system implemented user level file system 
implementation ibm sp dedicated nodes run vesta server code store data local disk partition aix file system underlying file system 
preliminary measurements indicate suffering small 
percent penalty implementing device drivers obviously number vary depending access patterns 
kernel supported file systems complex implementation built kernel machine direct access devices similar manner sfs file system cm 
number advantages implementation choice 
usually implementation kernel allows existing programs interact new file system modification recompilation 
access kernel data structures devices avoids extra data copies affect performance file system characteristics caching prefetching directly controlled needed 
said little focused performance benefits kernel support 
modifying kernel add new file system especially communicate kernels provide parallel trivial task 
source kernel order modify debugging troublesome user level code 
resulting code kernel dependent making porting operating systems difficult 
galley parallel file system galley parallel file system developed dartmouth university designed operate supercomputers clusters workstations 
authors assume nodes computation design precludes overlap compute nodes 
focus galley implementation low level support disk directed 
allows servers specify order data transferred order optimize traffic disk 
section discuss file format galley system design performance system 
declustering striping common parallel file systems galley allows files created call maximum node 
number location fixed file creation 
consists independent named forks linear sequences bytes 
forks added removed time 
explicit connection name requirement forks 
created higher level library handle declustering manner desired 
example single node hold stripes file data 
system file storage extremely flexible places larger burden application interfaces underlying client side implementations perform mapping 
galley multi threaded 
threads perform number distinct tasks ffl cache manager thread ffl disk manager thread ffl name space manager thread ffl incoming connection thread ffl service threads cache manager handles cache allocation lru policy replacement 
disk manager thread handles local disk prefetching done system support included storing data existing file systems raw devices 
name space manager handles metadata operations 
metadata files stored processors hashing function locate data specific file 
service thread started compute process tcp ip moving data clients 
interface supports strided batch accesses 
collective support presumably implemented higher level 
application tasks separately open close files 
performance system tested simulated disk giving level control disk file system policies available inside kernel 
separate situations run problems regards network utilization eventually note need better algorithm handle network flow control account situations 
galley system approaches problem providing parallel support clusters different angle pvfs 
systems differ way view data local processors manner metadata stored degree rely client side libraries resource optimize 
section detail pvfs design including approaches issues 
pvfs design parallel file system primary goal pvfs provide high speed access file data parallel applications 
addition pvfs provides cluster wide consistent name space enables user controlled striping data disks different nodes allows existing binaries operate pvfs files need recompiling 
pvfs client server file system multiple servers called daemons 
daemons typically run separate nodes cluster called nodes disks attached 
pvfs file striped disks nodes 
application processes interact pvfs client library 
pvfs manager daemon handles metadata operations permission checking file creation open close remove operations 
manager participate read write operations client library daemons handle file intervention manager 
necessary clients daemons manager run different machines 
running different machines may result higher performance alleviates competition compute processes network cpu memory resources 
pvfs implemented user level implementation kernel modifications necessary install operate file system 
pvfs uses tcp ip sockets internal communication 
result dependent particular message passing library 
pvfs kernel module development 
pvfs manager metadata single manager daemon responsible storage access metadata pvfs file system 
metadata context file system refers information describing characteristics file permissions owner group importantly description physical distribution file data 
case parallel file system distribution information include file locations disk disk locations cluster 
traditional file system metadata file data stored raw blocks single device parallel file systems distribute data physical devices 
pvfs simplicity chose store file data metadata files existing local file systems directly raw devices 
pvfs files striped set nodes order facilitate parallel access 
specifics file distribution described metadata parameters base node number number nodes stripe size 
parameters ordering nodes file system allow file distribution completely specified 
example metadata fields file pvfs foo table 
field specifies data spread nodes base specifies base node node specifies stripe size unit file divided nodes kbytes 
user set parameters file created specified pvfs default set values 
table metadata example file pvfs foo inode 
base application processes communicate directly pvfs manager tcp ip performing operations opening creating closing removing files 
application opens file manager returns application locations nodes file data stored 
information allows applications communicate directly nodes file data accessed 
words manager contacted read write operations 
issue development pvfs directory hierarchy pvfs files application processes 
implement directory access functions simply nfs export metadata directory nodes applications run 
provided global name space nodes applications change directories access files name space 
method drawbacks 
forced system administrators mount nfs file system nodes cluster 
pvfs emerged candidate file system large clusters order nodes note nfs store metadata actual file data 
actual data striped local disks nodes 
restriction problematic limitations nfs scaling 
second default caching nfs caused problems certain metadata operations 
drawbacks forced reexamine implementation strategy eliminate dependence nfs metadata storage 
done latest version pvfs result nfs longer requirement 
specifically dependence removed additionally trapping system calls related directory access 
mapping routine determines pvfs directory accessed operations redirected pvfs manager 
trapping mechanism extensively pvfs client library described section 
daemons data storage ordered set daemons run nodes cluster 
nodes specified user file system installed 
daemons responsible local disks node storing data pvfs files 
daemons utilize local file system store data 
pvfs file handled daemon local file created existing local file system 
files accessed standard unix read write mmap operations 
means data transfer occurs kernel block page caches scheduled kernel subsystem 
shows example file pvfs foo distributed pvfs metadata table 
note nodes example file striped nodes starting node metadata file specifies distribution 
daemon stores portion pvfs file file bytes local local local file striping example local file system node 
name file inode number manager assigned pvfs file example 
mentioned application tasks clients open pvfs file pvfs manager informs locations daemons 
clients establish connections daemons directly 
connections strictly data requests 
client wishes access file data client library sends descriptor file region accessed daemons holding data region 
daemons determine portions requested region locally perform necessary data transfers tcp ip 
shows example regions case regularly strided logical partition mapped data available single node 
logical partitions discussed section intersection regions defines call stream 
stream data transferred logical file order logical partitioning application physical stripe daemon resulting stream intersection stripe partition stream example network connection 
retaining ordering implicit request allowing underlying stream protocol handle packetization additional overhead incurred control messages application layer 
shows greater detail happens client accesses data pvfs daemons 
black see data requested corresponds portion logical partition described partitioning covered detail section 
shades gray see portions data stored nodes file striped 
region accessed daemons send back stream containing requested data possess 
streams merged final contiguous data buffer client 
data logical partition data daemon data daemon stream stream resulting data buffer file access example application programming interfaces apis pvfs multiple apis native api unix posix api array interface called multi dimensional block interface :10.1.1.39.8219
native api pvfs heavily influenced research projects progress time vesta 
project focused characterizing workloads parallel machines 
observations large fraction accesses occurred called simple strided patterns 
patterns characterized fixed size accesses spread apart fixed distance file 
vesta parallel file system interesting api allowed processes partition file logically process see subset file data 
combined concepts partitioned file interface forms basic api pvfs 
api provides functionality regular unix file functions provides partitioning mechanism 
api applications gsize stride offset partitioning parameters specify partitions files allow access simple strided regions file single read write calls reducing number calls common applications 
idea similar ways file view concept mpi io mpi io technique considerably flexible :10.1.1.39.8219
partitioning allows noncontiguous file regions accessed single function call 
special ioctl call set partition 
important parameters involved partitioning file offset gsize stride shown 
offset parameter defines far file partition begins relative byte file 
gsize parameter defines size simple strided regions data accessed stride parameter defines distance start consecutive regions 
noted significant difference semantics strided access pvfs compared galley 
galley servers information ordering file data strided requests received serviced relative data available node 
contrast pvfs strided accesses relative file 
better fit access patterns seen previous studies looking files entirety easier apply practice 
parallel programming parallel matured obvious certain applications benefit interfaces tailored native views file data 
rs ne nb ne nb bf bf block record superblock dimension dimension ne ne specifying blocking matrix popular application specific interface multidimensional array block interface 
interfaces tailored operation data sets consist records stored array format 
interface library calls designed help development core algorithms operating multi dimensional datasets making easier manage movement data core 
allows programmer describe open file dimensional matrix elements specified size partition matrix set blocks read write blocks specifying indices correct number dimensions 
addition supports buffering read ahead blocks definition superblocks poor choice terms retrospect 
programmer specifies superblocks giving dimension terms previously defined blocks file 
time block accessed blocks superblock read held transparent user space buffer compute node 
set parameters passed library describe logical layout superblocks file ffl number dimensions ffl size record rs ffl block size block count pairs giving number elements ne block number blocks nb file ffl blocking factors bf defining size superblocks dimension 
example shows file described library dimensional matrix containing theta array byte records stored row major order 
matrix grouped theta array blocks theta array records 
file described library accesses blocks specified simply giving coordinates 
blocks read placed multidimensional array records compute node size ne theta ne theta delta delta delta theta ned addition blocks residing superblock placed user space buffer 
example block accessed matrix block read user space buffer process resides superblock defined blocking parameters 
application library uses coordinates request blocking values buffer factors partitioning mechanism supported pvfs minimize number requests file system 
dimension case accesses converted strided access 
allows entire block superblock read single request 
data defined dimensions multiple requests batch request nested strided request mechanism 
mpi io interface implemented top pvfs 
purpose implementation mpi io developed argonne national laboratory 
designed ported easily new file systems implementing small set basic functions called device interface new file system 
feature key porting pvfs 
pvfs supports regular unix functions read write common unix shell commands ls cp rm 
furthermore existing binaries unix api operate pvfs files recompiling 
section describes feature implemented 
trapping unix calls system calls low level methods applications interacting kernel example disk network 
calls typically calling wrapper functions implemented standard library handle details passing parameters kernel 
trapping calls unix system calls previous works allow users new capabilities applications modifying code 
straightforward way trap system calls provide separate library users relink code 
approach condor system help provide checkpointing applications 
method requires relinking application new facilities 
libc syscall wrappers library pvfs library kernel pvfs library loaded standard operation application kernel library application pvfs syscall wrappers trapping system calls compiling applications common practice dynamic linking order reduce size executable shared libraries common functions 
side effect type linking executables take advantage new libraries supporting functions recompilation relinking 
method linking pvfs client library trap system calls passed kernel 
take advantage linux environment variable ld preload allow existing binaries operate pvfs files 
provide library system call wrappers loaded standard library environment variable 
method allows catch calls pass operating system relinking recompiling application 
shows organization system call mechanism library loaded 
applications call functions library libc turn call system calls wrapper functions implemented libc 
calls pass appropriate values kernel performs desired operations 
shows organization system call mechanism time pvfs client library place 
case libc system call wrappers replaced pvfs wrappers determine type file operation performed 
file pvfs file pvfs library handle function 
parameters passed actual kernel call 
method trapping unix calls limitations 
call exec destroy state save user space new process able file descriptors referred open pvfs files exec called 
second porting feature new architectures operating systems nontrivial 
appropriate system library calls identified included library 
process repeated system libraries change 
example gnu library linux glibc constantly changing result constantly change code 
pvfs request servicing pvfs request processing service mechanisms building blocks build rs implementation 
section provide short overview pvfs implementation 
cover pvfs receives requests processed structure stored service 
discuss system handles multiple simultaneous requests 
concentrate read operations discussion actual system calls servers perform local file system socket accesses described specifically 
data logical partition data daemon job daemon access creating accesses request receiving queuing requests pvfs currently performed tcp pvfs communication unix sockets interface 
pvfs servers maintain set open sockets checked activity loop 
sockets accept socket clients establish connections service 
possible states open socket connected outstanding request active servicing request 
pvfs servers single threaded entities rely select call identify connections ready service 
sockets server selects accept socket 
socket open socket involved request ready reading server attempts receive request 
job ack job ack job 
jobs service request received parsed request requires data transfer job created perform necessary shows job data structure created service request 
job associated socket file attached job list accesses data transfers performed service request 
typical job may accesses 
server allocates job structure breaks request contiguous accesses intersection requested data data available locally server 
process diagrammed 
acknowledgment prepended access list passing status information back client 
job added collection jobs server processing 
shows multiple jobs service simultaneously 
server processes jobs accesses associated job updated removed completed 
example job acknowledgment sent longer access list 
servicing requests typically task parallel application create job server performing operation 
easy imagine large parallel application large number jobs service server time 
large number jobs definition inter job dependencies provide opportunity optimize selecting jobs serviced order 
server perform part number accesses selected job selecting job service 
number jobs processed concurrently 
usually want server block waiting single job complete jobs serviced server normally performs operations selected job cause server block 
selection jobs service level apply scheduling optimization 
servers idle sit loop servicing requests job list empty select ready job service complete accesses job block selection job performed means wish including examining characteristics job size access type position 
gives flexibility implement scheduling algorithms 
connection selected service server refers access list order determine operation performed 
case read operation server uses mmap map data region address space 
performed region kbytes implementation tested previously reasonable trade mapping large region performing mapping operations 
region mapped memory send send data desired region remote host 
flag set socket prior sending data server block socket 
new region file needed connection old region unmapped new region mapped 
performance results performance results pvfs parallel test application beowulf cluster nasa goddard space flight center 
time performed experiments cluster configured follows 
nodes node mhz pentium pro processors mbytes ram gbyte seagate ide drive mbit sec intel pro network card operate full duplex mode 
network ii switch connected nodes 
separate front node connected switch gbit sec full duplex connection 
nodes running linux mpich 
seagate pro disks system kbyte caches rpm spindle speeds advertised sustained transfer rates mbytes sec 
performance disks measured bonnie file system benchmark showed write bandwidth mbytes sec cpu utilization read bandwidth mbytes sec cpu utilization operating mbyte file sequential manner 
write performance measured bonnie slightly higher advertised sustained rates test accessed file sequentially allowing file system caching read ahead write better organize disk accesses 
write performance higher read performance due write effects 
pvfs currently uses tcp communication measured performance tcp 
ttcp version tcp throughput mbytes sec 
measure pvfs performance performed experiments concurrent writes reads native pvfs calls 
varied number compute nodes cases shown compute nodes distinct 
test data removed disks synchronized test iteration 
scaling nodes set test runs designed test performance pvfs number nodes ions scaled 
amount data accessed node held constant number application processes compute nodes mbytes process 
example case application processes accessed nodes application task wrote mbytes data total mbytes stored parallel file 
shows maximum read bandwidth obtained mbytes sec nodes mbytes sec nodes mbytes sec nodes aggregate bandwidth mbytes sec number compute nodes ion mb ion mb ion mb ion mb ion mb pvfs read performance mbytes sec nodes 
values closely match maximum performance expect get disks node points peaks occur seeing benefits caching done local file system nodes 
cases find network performance bottleneck small numbers application tasks appears performance bottleneck larger numbers application tasks larger amounts data 
writing observed maximum bandwidths mbytes sec mbytes sec mbytes sec mbytes sec different numbers nodes shown 
read performance limitations shows read performance pvfs daemons varying number compute nodes 
see size aggregate requested region increases reach point performance drops significantly irrespective number clients compute nodes 
point occurs approximately clients reading mbytes clients reading mbytes clients reading mbytes 
aggregate bandwidth mbytes sec number compute nodes ion mb ion mb ion mb ion mb pvfs write performance aggregate bandwidth mbytes sec number compute nodes mb reads mb reads mb reads read performance dropoff point corresponds approximately mbytes total file data mbytes file data server 
believe drop related size available cache node due network centric design server routines 
issue discussed section 
working detailed performance study pvfs node cluster argonne called chiba city reported 
lessons learned design pvfs number decisions ease implementation single metadata manager local files data metadata storage implementing directory calls initially matching pvfs request types system calls 
cases identified problems decisions implemented changes inclusion directory calls pvfs system eliminate nfs dependency 
decisions turned acceptable example single metadata manager local files opposed raw devices storing metadata file data far caused problems performance configurability 
decisions appropriate time turned nontrivial detrimental implications 
realized improvements needed areas plan incorporate releases pvfs 
ffl flexible description format clients daemons needed 
ffl data transfer system implemented replaceable module 
ffl better low level client side interface needed 
ffl low level pvfs operations implemented way easily integrated vfs 
ffl disk optimizations needed daemons 
pvfs support simple strided accesses partitioning step right direction terms supporting common patterns powerful request mech needed take advantage descriptive capabilities new interfaces scalable sio low level api mpi io :10.1.1.39.8219
parallel libraries indicates providing library implementers native scatter gather support highly desirable obtaining highest possible performance 
existing implementation tightly coupled tcp data transfer 
time original design choice benign advent mainstream highperformance data transfer mechanisms gm st need internal communication abstraction effectively variety networking technologies apparent 
pvfs originally intended mainly linux clusters demand porting clusters run operating systems 
implementation client side interfaces pvfs needs better organized ease porting operating systems 
integration system call wrapper technique library currently calls client library difficult port 
inclusion partitioning system directly low level library resulted complications implementation 
retrospect created different lower level interface built partitioned file interface top included system call wrappers separate library 
separate important functionality base client library performing basic operations administrative tasks keeping partitioning information determining file types 
requests client library communicate pvfs daemons modeled system calls help perform 
implementing system pvfs user level obvious method choosing request types 
calls map types operations seen vfs system 
created problems respect implementing client side kernel interface 
mapping vfs operations system call operations simpler task 
design daemon focuses extracting maximum performance network 
reading pvfs file daemon code attempts fill available network buffers outgoing connections 
discussed section approach may lead performance drop offs 
better approach plan implement reorder read operations daemons better disk read performance 
pvfs brings high performance parallel file systems linux clusters testing tuning needed production ready available 
software downloaded freely www parl clemson edu pvfs 
inclusion pvfs support mpi io implementation easier applications written portably mpi io standard take advantage available disk subsystems lying dormant linux clusters :10.1.1.39.8219
working actively developing generation pvfs system account lessons learned experiences existing system feedback pvfs users 
new version improvements client server interface description format data transfer mechanism scheduling algorithms 
kernel vfs implementation works allow applications access pvfs files need system call wrappers 
chapter workload study chapter covered pvfs parallel file system experiments 
chapter discuss workloads examined order ascertain effectiveness scheduling algorithms servicing different test patterns ffl single block accesses ffl strided accesses ffl random block accesses describe system tests scheduling algorithms added pvfs purpose 
discuss test applications gather data 
discuss results test patterns 
observations various algorithms appropriate 
test system beowulf machine performed node cluster connected intel fast ethernet switch 
node specifications ffl pentium mhz cpu ffl mbytes dram ffl mbytes local swap space ffl gbyte ide disk ffl tulip mbit fast ethernet card node runs pvfs manager daemon handles interactive connections nodes compute nodes nodes 
node runs linux tulip driver 
ide disks provide approximately mbytes sec sustained writes mbytes sec sustained reads reported bonnie popular unix file system performance benchmark 
idle approximately mbytes memory node kernel various system processes including pvfs leaving approximately mbytes space system buffers 
tests nodes nodes nodes computation 
combination allowed separate nodes compute ones provide number simultaneous requests nodes schedule ensure single disk optimizations nodes mapping pvfs file locations local file ones simpler single disk case 
important understand scheduling refer request level disk level network level 
scheduling requests actual devices network disk performed kernel 
simply choosing hand kernel user level choosing order socket operations local file operations perform 
algorithms base configuration pvfs system optimizes network technique servicing requests ready time regardless data location 
algorithm maintained network limited behavior detected called opt purposes discussion 
similar come served fcfs algorithm readiness receivers accept data read case incoming data write case 
selection increasingly disk oriented approaches implemented better address disk limited workloads 
cases data ordering request remain modifying order service requests 
important note cases utilize file position indication spatial locality opposed disk block position 
shown effective previous works avoid additional modification parallel file system 
perform testing single file maximize reliability measurement 
new approach algorithm select buffers service modified utilize data location information retaining data transfer ordering 
data order servicing regions similar manner circular scan cscan 
regions ordered serviced order attempt better match disk file system read ahead 
algorithm denoted opt discussion 
second new approach extend selecting subset disk service requests time limiting number requests service simultaneously depending size distribution requests 
specifically logical window defined size near size physical memory machine 
file position serviced ready requests fitting inside window 
having starting file position half window size bytes position serviced serviced 
requests fit requirement nearest ready request serviced 
allow system better exploit caching spatial locality expense network buffer utilization 
known opt 
similar window scan algorithm 
final new approach order service strictly location servicing request closest access waiting request ready service 
mimic disk directed workloads referenced opt 
ways analogous greedy shortest seek time sstf algorithm 
test applications test applications total configurations workload study 
workloads represent number access patterns seen traditional applications particularly ones operating dense multidimensional matrices 
include contiguous strided random test workloads single contiguous block access strided access multiple random block access workloads order cover wide range possible workloads 
shows general pattern access workloads 
cases mpi application create set application tasks independently accessed pvfs file system native pvfs libraries 
mpich version 
mentioned tests single pvfs file store data 
single block access tests contiguous regions data file simultaneously accessed task 
tasks synchronize access times complete access recorded 
consider longest service time task application service time 
calculate mean service time tasks variance task service time 
patterns seen applications accessing dense matrices block manner checkpoint applications core applications 
strided access tests multiple regions data file simultaneously accessed task single operation 
tasks synchronize access times complete operation recorded 
consider longest task service time application service time calculate mean service time variance 
strided accesses seen result row cyclic distribution data sets access portions records fixed size 
purpose random block access tests observe system serving application irregular access pattern 
file logically divided number blocks blocks randomly assigned tasks application task access equal number blocks 
tasks synchronized accesses begins tasks access blocks random order native pvfs operation access block block time 
time access blocks recorded task largest total times considered application service time mean service time variance calculated 
pattern created application accessing pieces multidimensional data set reading arbitrary records large database 
cases goal analyze effects algorithms performance system metrics ffl application service time ffl mean task service time ffl task service time variance metrics important depends nature applications run system policies place system 
system serving single parallel applications application service time appropriate metric 
system running multiple parallel applications serial ones mean task service time appropriate metric 
groups interested fairness access desire minimize variance 
write tests data file removed test run disks synchronized 
second delay test runs imposed allow update process write metadata disk nodes 
read tests run local data file larger size node memory read entirety node remove pvfs file data cache 
separate run read tests additionally performed allowed file data remain cache 
read local data file runs 
results compared small accesses cache sections 
graphs metrics provided tests 
cases output algorithms described earlier 
total data accessed single node shown axis service time variance provided axis 
data average test runs 
single block accesses results uncached single block accesses shown 
opt provides lowest average task service time due servicing single tasks time opt provide lowest variance cycling requests fair manner 
opt provides compromise allowing smaller service time retaining lower variance opt due added flexibility servicing tasks 
single block writes shown observe benefit picking best algorithm 
opt appears provide benefit terms task write service time due servicing task time 
interesting effect seen read access caching compared 
file data flushed opt provides far best task read service time beating worst performers opt slightly higher variance similar application read service time 
recall opt algorithm similar disk directed request closest accessed file position serviced 
apparent effectively utilizing disk resources opt case 
case cached data available see opt results slowest application read service time highest variance task read service time beating worst performer 
opt appropriate choice case 
recall opt relaxes strict ordering requests allowing requests position window serviced allowing nearest ready request serviced 
cached data available opt provides better application service time resulting competitive mean task service time lower variance opt 
strided accesses strided access pattern arbitrarily chose access disjoint regions task access 
size disjoint regions varied tests 
service time sec access size mbytes application read service time opt opt opt opt service time sec access size mbytes task read service time opt opt opt opt variance access size mbytes task read service time variance opt opt opt opt single block read performance service time sec access size mbytes application write service time opt opt opt opt service time sec access size mbytes task write service time opt opt opt opt variance access size mbytes task write service time variance opt opt opt opt single block write performance service time sec access size mbytes application read service time opt opt opt opt service time sec access size mbytes task read service time opt opt opt opt variance access size mbytes task read service time variance opt opt opt opt single block read performance small uncached accesses service time sec access size mbytes application read service time caching opt opt opt opt service time sec access size mbytes task read service time caching opt opt opt opt variance access size mbytes task read service time variance caching opt opt opt opt single block read performance small cached accesses servicing strided read requests see opt provides lowest mean task service time due fact file data interleaved application tasks resulting opt performing similarly opt ordering restrictions 
algorithms result approximately application read service time wide range access sizes 
writes see opt provides best application service time variance single algorithm stands terms task service time performance 
small reads see figures cached data availability impact algorithm selection 
case cached data available see opt provides gains task service time cost higher variance reducing service time worst performing algorithm 
data reside cache see opt provides competitive application task service times resulting low variance 
data cache service times consistent service order disk optimal 
random block accesses addition tests focusing known patterns chose study random block access 
interesting characteristic tests fraction total data accessed requested time multiple operations required access randomly distributed blocks native pvfs calls 
contrast previous tests data accessed requested single calls 
result total size requests point time tot blks tot service time sec access size mbytes application read service time opt opt opt opt service time sec access size mbytes task read service time opt opt opt opt variance access size mbytes task read service time variance opt opt opt opt strided read performance service time sec access size mbytes application write service time opt opt opt opt service time sec access size mbytes task write service time opt opt opt opt variance access size mbytes task write service time variance opt opt opt opt strided write performance service time sec access size mbytes application read service time opt opt opt opt service time sec access size mbytes task read service time opt opt opt opt variance access size mbytes task read service time variance opt opt opt opt strided read performance small uncached accesses service time sec access size mbytes application read service time caching opt opt opt opt service time sec access size mbytes task read service time caching opt opt opt opt variance access size mbytes task read service time variance caching opt opt opt opt strided read performance small cached accesses total amount data accessed blks number blocks data split task 
figures show performance data split blocks task 
random block write tests see opt provides slight application performance benefit mbytes data accessed daemon 
addition opt gives far lowest variance bunch especially data sizes increase 
consistent ordering request service opt allows 
note opt opt extremely high variance read test opt variance quite low 
difference task service times fairly small opt consistently outperforms opt 
focus small read accesses see opt provides slightly higher performance rest cache available expense particularly high variance 
opt helpful situations created random block tests blocks contiguous data potentially separated unwanted data accessed simultaneously avoids traversing unwanted data regions necessary 
expected opt provides best performance data cached low service times providing modest performance gain opt variance 
figures see random block test results 
opt provides consistently low variance conjunction competitive service times writes 
performance slightly lower block case consistent fact twice requests perform transfer data 
reads see opt opt providing slightly better performance expense high variance 
service time sec access size mbytes application read service time opt opt opt opt service time sec access size mbytes task read service time opt opt opt opt variance access size mbytes task read service time variance opt opt opt opt random block read performance service time sec access size mbytes application write service time opt opt opt opt service time sec access size mbytes task write service time opt opt opt opt variance access size mbytes task write service time variance opt opt opt opt random block write performance service time sec access size mbytes application read service time opt opt opt opt service time sec access size mbytes task read service time opt opt opt opt variance access size mbytes task read service time variance opt opt opt opt random block read performance small uncached accesses service time sec access size mbytes application read service time caching opt opt opt opt service time sec access size mbytes task read service time caching opt opt opt opt variance access size mbytes task read service time variance caching opt opt opt opt random block read performance small cached accesses service time sec access size mbytes application read service time opt opt opt opt service time sec access size mbytes task read service time opt opt opt opt variance access size mbytes task read service time variance opt opt opt opt random block read performance service time sec access size mbytes application write service time opt opt opt opt service time sec access size mbytes task write service time opt opt opt opt variance access size mbytes task write service time variance opt opt opt opt random block write performance figures focus read performance small accesses cached data 
see opt performing best uncached files advantage opt expense high variance 
hand data cached opt appears perform best outperforming opt second best variance 
observations see able affect application service time small number cases general scheduling changes little effect write workloads 
see benefits applying certain algorithms read cases respect task service time 
particular situations uncached contiguous regions serviced opt opt show best performance 
hand cases significant fractions data cached see opt performs best 
strided read workload see opt performs best 
partially due implicit interleaving requests 
predefined ordering request data factor 
mean order request data returned requesting task pvfs order monotonically increasing file byte offset 
constrains order service pieces strided requests doing limits ability disk oriented algorithms best access disk 
service time sec access size mbytes application read service time opt opt opt opt service time sec access size mbytes task read service time opt opt opt opt variance access size mbytes task read service time variance opt opt opt opt random block read performance small uncached accesses service time sec access size mbytes application read service time caching opt opt opt opt service time sec access size mbytes task read service time caching opt opt opt opt variance access size mbytes task read service time variance caching opt opt opt opt random block read performance small cached accesses chapter reactive scheduling chapter discussed performance test system utilizing different scheduling algorithms 
algorithms component reactive scheduling implementation 
second components system model 
model predict scheduling algorithm appropriate choice specific system state workload progress 
chapter develop part empirical model serve system model reactive scheduling implementation 
simple system model characterize behavior system various workloads 
model tuned match behavior system algorithm effects included 
model parameters calculated specific system order accurately estimate behavior 
develop model read traffic allow correctly select algorithm workload system state available algorithms 
constrain discussion accesses single file observe file access locations 
description cover physical blocks single device 
intent model extendable include better modeling specific components new choices algorithms 
reason model split components baseline model algorithm efficiency functions 
baseline model allows estimate performance system workload parameters algorithm specific effects included 
algorithm efficiency functions allow calculate effects algorithm system performance specific workload system state 
separating behavior system effects algorithms behavior reduce complexity modeling system 
model developed cover implementation rs pvfs 
section particulars selection implementation pvfs implications adding component real system 
cover performance pvfs rs automatically select scheduling algorithms section 
section discuss effectiveness rs environment 
cover capabilities aid effectively applying rs approach 
baseline model original intention develop system model modeling components system including network devices disks cache 
model include factors caching dirty buffer flushing described appendix unfortunately due complexity system working unable map model seeing tests actual system 
failure led take alternative approach modeling system applying empirical approach 
performed workload study shown chapter data built model felt matched behavior system including hardware components 
disks network devices software involved particularly parallel file system 
led model result observing system number workloads 
model relies set parameter values selected system administrator observed system behavior operate correctly 
parameters selected series benchmark tests discussion selecting values appendix parameters selected model accurately predict performance system number workloads 
system components changed including limited processor disk available memory network components parameters recalculated 
model system parameters recalculated 
model estimate task service time read requests assumes round robin disk ordered servicing requests opt 
parameters model listed table discussed 
basic equation describing system written term constant value describing overhead servicing request 
time spent performing operations processing request sending 
term function workload complicated model 
second term represents time spent performing disk network order service request 
term function number parameters 
discuss calculation important understand parameters describe workloads system 
tested workloads needed parameters accurately describe workloads progress ffl total size requests req ffl number requests req ffl range request locations req ffl total number disjoint regions requests req values describe total number requests service total size data requested 
range number bytes included smallest contiguous region include bytes requested point time 
simple single file case calculated subtracting lowest byte position file request highest byte position request denoted req interested number disjoint regions requests 
example strided test cases sixteen disjoint regions request 
denote total number disjoint regions request set req noted appeared effective bandwidths seen tests rate occurred caching effective second occurred 
able characterize behavior parameters ffl bandwidth caching effective drc ffl bandwidth caching effective dr ffl effective size cache cache ffl cache drop coefficient cache parameters characterize drc req cache dr req cache cache dr gammas cache cache cache drc gamma gammas cache cache cache cache cache values allow match observed transition cached non cached performance 
allows calculate task service time characterization parameters total size requests 
reasonable due assumption service round robin fashion type scheduling number requests important total size servicing simultaneously service complete requests time 
characterization behavior take non workload account acknowledged model hold range workloads 
add new factors help account req dist new factor req take account efficiency requests 
case cover inefficiency requests specify disjoint regions 
aren saying requests bad simply efficient service contiguous requests 
efficient multiple requests access disjoint regions 
second factor introduce dist takes account efficiency distribution requests 
enable take sparse data access account 
adding factors enables accurately tune model match observed behavior 
equations show calculation new factors workload characteristic values parameters 
req dist control degree particular inefficiency affects performance req dist help describe type relationship 
linear 
req gamma req req dreq preq req dist gamma dist dist dreq dist req observations random block access performance degradation caused increasing sparseness data linear effect 
motivation addition exponential parameter dist equation describing dist table model variables predetermined constants dr bandwidth system cached data unavailable drc bandwidth system cached data available dist coefficient determining effect sparse data distribution dist exponent component sparse data distribution function req coefficient determining effect strided requests req exponent component sparse data distribution function cache coefficient determining range effect caching service time spent performing system state values req number disjoint regions requests service req number requests service req range file positions requests service req total size requests service cache memory available caching derived values dist efficiency access data distribution req efficiency access request characteristics expected time service read request task completeness parameter added equation relating disjoint requests req testing sufficient determine increasing number disjoint regions linear exponential effect performance 
matching model data mentioned earlier attempt match baseline performance opt round robin disk ordered scheduling algorithm 
opt provides consistent performance algorithms behavior easiest characterize 
focus calculating constants baseline model accurate prediction wide range sizes 
calculate dr drc data single block tests consider ideal workload case 
calculate req data strided tests dist dist data sets random block tests 
assume req set data points calculate req req table summarizes values calculated parameters 
interesting note dr fact closely match observed disk bandwidth test system drc approximately expect bandwidth network 
dist dist values indicate nonlinear relationship performance drop ratio requested data requested data range 
intuitive data spread wider range file locations spread wider range disk blocks caching effective read ahead read desired block disk access efficient disk head seek disk cache hold data 
figures show comparison model assuming caching data tests cache flushed test runs 
focuses small accesses shows results larger ones 
single block strided accesses accurate small random block underestimate service time somewhat 
due limited number variables model desire match wide range values order match caching large accesses table parameter values dr mbytes sec drc mbytes sec dist dist req req cache sec cases accuracy sacrificed cases 
see sections affect ability model predict appropriate scheduling algorithm 
figures see model compared observed performance caching effect small large accesses 
model closely matches tested cases deviation point request sizes exceed cache size 
limited ability match workloads case single set parameters model transition heavy disk access deviation great taken account section 
algorithm efficiency functions simple model able effectively account caching effects effects non ideal workloads system performance take complicated system model account changes scheduling result application alternative algorithms 
order requests serviced access size mbytes task service time single block reads model opt access size mbytes task service time strided reads model opt access size mbytes task service time block random reads model opt access size mbytes task service time block random reads model opt comparing opt caching baseline model small accesses access size mbytes task service time single block reads model opt access size mbytes task service time strided reads model opt access size mbytes task service time block random reads model opt access size mbytes task service time block random reads model opt comparing opt caching baseline model large accesses access size mbytes task service time single block reads model opt access size mbytes task service time strided reads model opt access size mbytes task service time block random reads model opt access size mbytes task service time block random reads model opt comparing opt caching baseline model small accesses access size mbytes task service time single block reads model opt access size mbytes task service time strided reads model opt access size mbytes task service time block random reads model opt access size mbytes task service time block random reads model opt comparing opt caching baseline model large accesses table optimization efficiency values uncached cached optimization ideal sparse disjoint ideal sparse disjoint opt opt opt opt tion cache easily predicted 
apply rule changes function choice algorithm 
add additional coefficient function opt req dist coefficient represent effect algorithms performance vary workload algorithm 
baseline model tuned match opt opt cases opt 
table presents values calculated previous workload study 
workloads characterized ideal sparse disjoint workload parameters 
req cache coefficients cached columns uncached ones 
comparing final model data section compare complete model data collected workload tests 
goal establish model correctly predict fastest performing algo rithm majority ideally tested cases 
data cache enabled behavior results similar cache case 
shows side side comparison model output observed workload behavior including caching range tested sizes 
data separately increase readability line type graph corresponding data model output 
comparing graphs see model predict appropriate algorithm shows algorithm giving minimum service time large accesses notable exception 
case strided reads model incorrectly predicts opt best sizes fact region mbytes mbytes opt better 
focus small access sizes 
graphs workload section reproduced allow direct comparison 
see model accurately predict correct scheduling algorithm cases 
notable exception strided accesses mbytes opt practice performed slightly better predicted opt 
fluctuation observed behavior inconsistent case performance difference point minimal 
simple model provides accurate estimation system performance wide range access sizes workloads 
baseline model consists concise set intuitive equations parameters model performance roundrobin scheduling system 
top set rule modifiers describe effects various algorithms system performance 
access size mbytes task service time single block reads opt opt opt opt access size mbytes task service time model single block reads opt opt opt opt access size mbytes task service time strided reads opt opt opt opt access size mbytes task service time model strided reads opt opt opt opt access size mbytes task service time block random reads opt opt opt opt access size mbytes task service time model block random reads opt opt opt opt access size mbytes task service time block random reads opt opt opt opt access size mbytes task service time model block random reads opt opt opt opt comparing observed behavior model access size mbytes task read service time caching opt opt opt opt access size mbytes task service time model single block reads opt opt opt opt access size mbytes task read service time caching opt opt opt opt access size mbytes task service time model strided reads opt opt opt opt access size mbytes task read service time caching opt opt opt opt access size mbytes task service time model block random reads opt opt opt opt access size mbytes task read service time caching opt opt opt opt access size mbytes task service time model block random reads opt opt opt opt comparing observed behavior model small accesses implementing reactive scheduling pvfs mentioned previously reactive scheduling rs system consists components 
system model set scheduling algorithms discussed 
remaining component selection mechanism 
component supply model necessary system state predetermined constants model select best scheduling algorithm 
user space process pvfs daemon limited access knowledge system state outside process space 
problem respect workload information number size characteristics requests service readily available 
impossible process reliably know data cache 
limits ability implement rs approach available cached data important input model 
address issue study ways 
supply system accurate knowledge set tests order ascertain system ability operate ideal case 
worst case assumption cases cached data available 
practical assumption systems workloads considering inability gather accurate information cache state 
system re tested assumption 
separate concern accurately determining number tasks requesting request data time 
recall model takes input number requests parallel operation total data size 
requests independent tasks arrive simultaneously pvfs system results period time near start collection requests current request statistics truly represent operation take place 
likewise requests completed state information solely requests service accurately describe set operations completed 
better indicator number tasks perform point time needed input 
recognizing workloads parallel ones note number tasks holding file open indicator number tasks perform simultaneously 
assumption perform small number calculations available workload data predict accurate model values 
define number observed values ffl ile number tasks holding file open ffl number observed requests ffl total size observed requests ffl number disjoint regions observed requests ffl total range file positions observed requests observed values calculate model parameters follows req ile req ile req ile req max fr values passed model order perform necessary calculations 
expected task service time calculated algorithms returning smallest service time chosen 
process repeated new request 
results utilizing reactive scheduling section detail results tests utilizing reactive scheduling pvfs 
test applications system chapter 
set tests fixed reported available cache mbytes value model testing chapter 
repeated original workload tests allowing data remain cache runs 
results shown figures separately small large accesses compared best performance seen workload study 
see rs implementation able adapt previously studied workloads providing performance comparable best seen study 
exception seen case small block random accesses 
studying effect determined request sizes small model reporting zero value time algorithms defaulting opt implementation detail 
effect eliminated choosing default practical algorithm situation opt choice small accesses encountered 
second set tests fixed reported available cache mbytes repeated workload tests flushing cache runs 
results shown access size mbytes task service time single block reads rs opt access size mbytes task service time strided reads rs opt access size mbytes task service time block random reads rs opt access size mbytes task service time block random reads rs opt rs performance caching small accesses access size mbytes task service time single block reads rs opt access size mbytes task service time strided reads rs opt access size mbytes task service time block random reads rs opt access size mbytes task service time block random reads rs opt rs performance caching large accesses access size mbytes task service time single block reads rs opt access size mbytes task service time strided reads rs opt access size mbytes task service time block random reads rs opt access size mbytes task service time block random reads rs opt rs performance caching small large accesses compared best algorithm seen workload study 
rs able correct algorithm selection 
final set tests fixed reported available cache mbytes reran workload tests allowing cached data remain runs 
simulates performance real environment cache active cache utilization data unavailable 
performance rs conditions compared best case run workloads figures small large accesses respectively 
see large sizes perform comparably small random block accesses perform 
opt best performing algorithm cached random access size mbytes task service time single block reads rs opt access size mbytes task service time strided reads rs opt access size mbytes task service time block random reads rs opt access size mbytes task service time block random reads rs opt rs performance small accesses assuming cache blocks opt best uncached data selection mechanism thinks state 
model combines simple behavioral model system different workloads set heuristics account effects varying scheduling algorithms 
encapsulated important workload characteristics especially sparse data access disjoint requests model 
model accurately predicts system access size mbytes task service time single block reads rs opt access size mbytes task service time strided reads rs opt access size mbytes task service time block random reads rs opt access size mbytes task service time block random reads rs opt rs performance large accesses assuming cache performance workloads tested provides solid basis rs implementation 
building system model scheduling algorithms previously implemented selection mechanism complete rs implementation pvfs 
case necessary selection mechanism perform estimation model parameters system state 
shown provided accurate cache values rs system able apply appropriate algorithm tested workloads 
furthermore workloads cover range important characteristics including disjoint access sparse requests expect performance optimal similar workloads 
accurate cache information conservative assumptions assuming access data cache 
assumption result optimal behavior large requests know system accurately choose algorithms range 
workload study realize difference file system performance simply due scheduling algorithm selection 
applying rs able avoid situations single algorithm fail advantages specific algorithms 
done automatically system 
means users see task service times drop changes system components applications 
rs system uses general workload characteristics basis operation operate wide variety workloads 
fact ability rs system adapt changing workloads means adapt collections requests progress increasing potential performance gains 
obviously useful capability production implementation ability query system cache information file disk block basis 
allow obtain needed data run time improving accuracy 
physical block locations logical file ones scheduling result improved performance particularly allowed requested data ordering chosen dynamically opposed fixed logical ordering imposed current system 
combining rs hint mechanism improve accuracy inputs passed model assuming trust hints 
hints provide exact values req req req req parallel operation directly 
chapter processor speeds cluster sizes increase important parallel systems extract maximum performance underlying hardware 
reactive scheduling piece performance puzzle provides novel technique improving performance parallel systems 
rs complementary existing optimization methods concentrate caching prefetching writeback strategies 
parallel virtual file system parallel file system build ground parallel research cluster environments 
pvfs grown research toy popular tool parallel computing number facilities world 
pvfs rapidly de facto standard parallel file systems linux clusters continued development strengthen position 
examined performance set workloads pvfs collection scheduling algorithms 
noted best algorithm situation depended goal system 
fastest performance lowest variance state system 
available cache workload progress 
scheduling algorithm study performed parallel environment knowledge disk directed optimal tested cases making interesting result 
quantified algorithm performed best workloads conditions caching available conditions 
noted particular workload characteristics presence requests multiple disjoint regions sparse data access noticeable impact system performance 
characteristics explicitly integrated system model 
workload study basis designed tuned system model represent system performance 
started simple behavioral model represented system performance round robin scheduling algorithm incorporated workload system effects 
account complexities algorithm changes set heuristics predict expected performance 
compared model output collected data showed accurate 
model rs implementation predict appropriate scheduling algorithm 
approach different adaptive parallel systems complicated markov models neural networks 
brought file system model algorithms single rs implementation pvfs 
implementation correctly predicted best scheduling algorithm tested workloads real time supplied input cache availability available accurately pvfs environment 
value able operate optimally cases assuming cache available 
best case shown rs approach provided access size mbytes task service time single block reads pvfs pvfs rs best case rs performance reduction mean service times original pvfs system requested sizes mbyte larger 
due correct application disk oriented scheduling techniques block accesses 
combines theoretical modeling scheduling practical issues real time operation data availability create solution important problem providing high performance parallel rs new technique performance improvement environment real time scheduling algorithm selection shown researched 
solution applicable variety systems model tuned match number networking disk technologies heuristic system accounting scheduling algorithms tuned match new algorithms 
additionally enumerated number workload characteristics partially determine system performance pointed characteristics impact algorithm selection 
issue workloads effect algorithm selection central point ripe area study parallel serves starting point research area undoubtedly help quantify characteristics workloads effect algorithm selection range workloads algorithms appropriate 
practical side increasing knowledge system state pvfs allow accurate prediction 
excellent area study lead reliable rs implementation 
appendices appendix original system model chapter discuss thorough system model serve basis effective system model 
model straight forward considers system point view server includes disk network memory components 
suffice simple network storage topologies means meant model possible systems 
section developing simple model single requests utilizes constant values disk network performance 
section extend model account multiple requests 
section relax assumption constant values resource performance build equations allow calculate expected performance workload 
single request model parallel file system resources usually potential bottlenecks network devices disks 
denote expected value raw network bandwidth seen file system server net expected value raw disk write bandwidth dw expected value raw disk read bandwidth dr assume normal distribution variables 
values roughly estimate effective bandwidth file system net dw read net dr simply saying effective bandwidth minimum system effective bandwidth resources 
assumes completely overlap communication assuming request size req similarly estimate time single write read data transfer rio respectively req rio req read setup time transfer request prepare data transfer tws rs writes reads respectively factor 
define new times include setup time data time rx read transfers write transfers tws rx rs rio obtain accurate model term included adjust fraction time disk network occur simultaneously overlap time 
case indicate fractional value denoted write transfers read read ones 
discuss estimation values section 
req net dw gamma req net gamma req dw req net dw gamma net dw req net dw rio read req net dr gamma read net dr req net dr says transfer request size req fractional overlap network disk transfer writes reads calculate expected time portion transfer function effective bandwidths network disk 
overlap communication disk related current state available memory memory available hold buffers order overlap possible 
categorize memory distinct types interest ffl free unallocated memory memory holding useless file data free ffl dirty buffers dirty ffl clean buffers holding useful file data clean free memory useful reads writes allowing prefetching case buffering data 
dirty buffers sense detriment having free memory buffers need written time performing data transfer 
clean buffers file data especially useful avoid disk reads cases converted free memory buffering writes 
discussion terms buffers cache interchangeably refer physical memory holding file data 
define new sizes disk cache correspond data accessed disk cache respectively 
cases req disk cache writes available cache free clean req free clean reads available cache data fraction number buffers holding file data req hit theta clean dirty hit fraction indicating probability byte cached 
defined look overlap read write transfers 
assume overlap behavior dominated available cache minf req read minf req point time dirty buffers written disk 
systems daemon responsible activity 
model behavior parameters amount dirty buffers flushed periodically bf interval writes bf really lower effective disk bandwidth stealing time active disk requests 
calculate expected amount time spend flushing buffers request twf dw minfm dirty bf bf rf dr minfm dirty rio bf bf simply take account average number times flushing occur size data written time maximum amount data dirty written due dirty buffers time transfer place 
assume time overlapped equations 
revisit transfer time equations including new term tws twf rx rs rf rio estimate read write transfer times single requests account caching dirty buffer write back effects 
multiple request model case multiple transfers type occurring simultaneously obvious effect increase start time increase linearly number connections assuming independent requests fixed startup time 
consider req previous requests aggregate size new equation simply twf rx nt rs rf rio expression time required service requests totalling req bytes data account time receive parse requests amount memory available determine overlap amount dirty blocks written transfer 
extend model simultaneous read write traffic 
improved resource performance modeling model system described examine relative performance resources predict resources limit performance 
assuming network disk read disk write bandwidths normally distributed expected values net dr dw respectively 
define expected network transmit time net net req net likewise predict disk transfer times writing reading account availability cache overhead flushing dw gamma req dw twf dr gamma read req dr rf basic components need order determine resource bottleneck system directly comparing expected service times 
importance acknowledging roles net dw dr performance values constant interrelated 
true number reasons including cpu utilization access characteristics 
traditionally application scheduling algorithms resulted improvements effective bandwidth numerous situations 
model multiple resources attempt table network model parameters size network performance reaches maximum bandwidth size network performance reaches minimum bandwidth maximum network bandwidth minimum network bandwidth ing create optimal schedules complex endeavor 
optimize slowest resource improve throughput 
consider disk network bandwidth point time denoted net dw dr approximate parameters transfer knowledge system architecture state 
architectures require different equations represent behavior resources certainly complex accurate models developed ones 
network model assume performance improve minimum peak value solely size request 
parameters characterize network performance described table shown 
net req gammas gammab gammas req req disk model utilize total request size determining performance 
performance modeled degrading certain aggregate size minimum performance 
parameters disk model described table table disk model parameters size disk performance reaches maximum bandwidth size disk performance reaches minimum bandwidth maximum disk bandwidth minimum disk bandwidth shown 
disk req req gamma gamma gamma req req assume time ffl independent requests reads writes ffl req total data size ffl free clean dirty units free clean dirty buffers respectively 
equations derived previously chapter obtain expected disk network bandwidth values 
values select number transfer algorithms 
algorithms ordered concentration certain resource 
ratio disk network performance select algorithm 
operation repeated necessary new requests received requests serviced time passes 
request size bytes network model parameters request size bytes disk model parameters model parameters table model variables predetermined constants bf amount data written pass dirty buffer flushing bf time dirty buffer flush operations tws setup time write operation rs setup time read operation size disk performance reaches maximum bandwidth size disk performance reaches minimum bandwidth maximum disk bandwidth minimum disk bandwidth size network performance reaches maximum bandwidth size network performance reaches minimum bandwidth maximum network bandwidth minimum network bandwidth system state values free unused memory clean memory holding buffers need written dirty memory holding buffers need written disk req total size requests service req number requests service system model implementation order implement system model integrate system state information estimated disk network performance values equations previously derived 
table summarizes values process 
calculation net follows model net req net function include component representing time acknowledge requests time impacts disk network performance equally 
value net equations section 
pure write workload estimate dw dw gamma req dw twf gamma req dw dw minfm dirty bf bf dw gamma req min dirty req bf bf net dw dw gamma min req req min dirty req bf bf net dw dw req gamma req min dirty req bf bf net dw dw req gamma req free clean req min dirty bf bf dw oj dw req gamma req free clean min dirty req bf bf net dw terms parenthesis represent amount data fit cache final term parenthesis represents amount data written due dirty buffer flushing 
pure read workload dr gamma read req dr rf gamma read req dr dr minfm dirty rio bf bf dr gamma read req minfm dirty req bf read bf dr gamma read req min dirty req bf bf net dr calculations estimate time complete set transfers calculating written net dw appendix model parameter values chapter discuss select parameters system model 
discussion broken parts 
discuss find baseline parameters parameters define ideal behavior system opt 
discuss calculate workload effect parameters account disjoint access sparse data access 
discuss build table algorithm efficiency values account varying scheduling algorithms 
process selecting parameters informal methodology precisely calculating values developed part 
text serves guide selecting reasonable values 
order calculate parameters large number workload tests performed installed pvfs system 
utilize test programs discussed earlier text create contiguous strided random block access patterns 
range kbytes node mbytes node approximately data points test series 
chose kbytes somewhat arbitrarily simply wanted fairly small size start 
upper bound chosen times size physical memory allowed see accesses greatly exceed cache size handled system 
repeated run times average runs data points finding parameter values 
basic approach finding parameter values simple 
smallest data point calculate constant 
attempted match slope remaining data points visual inspection words plotted output model set parameters side actual data points tweaked parameters best match data points 
calculating baseline parameters set parameters calculate drc dr cache cache parameters define ideal conditions system performs cached uncached data 
utilize single block workload test program opt turned 
ensure cached data flushed nodes test runs reading large data file runs 
run tests range values perform best fit obtain dr fixed overhead value uncached bandwidth values 
note workload case req dr repeat test allowing caching occur 
observe data set size caching appears lose effectiveness cache data points point perform best fit obtain drc region req drc note data set size effects caching disappeared entirely 
denoting point calculate cache cache cache cache cache gamma integrating workload effects select parameters model workload effects req req dist dist perform additional testing 
utilize strided workload test program opt 
workload isolates effect disjoint requests sparse access 
test cached uncached data 
data select req req simple method selecting values 
assumed req fit req cases starting point 
note uncached cases gamma req req dreq preq req dr req varies value indicating performance affected parameter 
plotting observed data alongside equation output able obtain final values 
selecting dist dist performed similarly case random block workload gather data 
utilizing random block workload isolate effect sparse access disjoint requests 
set algorithm opt perform tests numbers blocks 
gives data estimate values 
presence piece wise function exponential value difficult algorithmic ally calculate values matching cases substitution arrive reasonable values 
accounting scheduling algorithms build table values model effect scheduling algorithm 
observations appears model effect simple coefficient case 
re run workload tests available optimizations matching cases uncached tests obtain values uncached case 
data points near cache obtain values cached case 
bibliography robert bennett kelvin bryant alan sussman raja das joel saltz 
framework optimizing parallel proceedings scalable parallel libraries conference pages mississippi state ms october 
ieee computer society press 
rajesh choudhary juan miguel del rosario 
experimental performance evaluation touchstone delta concurrent file system 
proceedings th acm international conference supercomputing pages 
acm press 
rajesh juan miguel del rosario choudhary 
design evaluation primitives parallel proceedings supercomputing pages portland 
ieee computer society press 
tim bray 
bonnie file system benchmark 
www com bonnie 
bruce macdonald 
support portable parallel programming 
technical report tr edinburgh parallel computing center march 
karen cheng rod edward hook bill kramer craig manning john charles william douglas sheppard merritt smith ian welch rita williams david yip 
clustered workstations potential role high speed compute processors 
technical report rns nas systems division nasa ames research center april 
peter chen david patterson 
maximizing performance striped disk array 
proceedings th annual international symposium computer architecture pages 
chiba city argonne scalable cluster 
www mcs anl gov chiba 
choudhary rajesh michael harry rakesh ravi singh rajeev thakur 
parallel scalable software input output 
technical report sccs ece dept npac case center syracuse university september 
peter corbett dror feitelson 
vesta parallel file system 
acm transactions computer systems august 
peter corbett dror feitelson jean pierre sandra johnson 
parallel access files vesta file system 
proceedings supercomputing pages portland 
ieee computer society press 
peter corbett dror feitelson jean pierre marc snir 
user friendly efficient parallel os vesta parallel file system 
transputers advanced research industrial applications pages 
ios press september 
peter corbett jean pierre chris garth gibson erik reidel jim zelenka chen ed felten kai li john hartman larry peterson brian bershad alec wolman ruth 
proposal common parallel file system programming interface 
www cs arizona edu sio api ps september 
version 
crandall ruth andrew chien daniel reed 
input output characteristics scalable parallel applications 
proceedings supercomputing san diego ca december 
ieee computer society press 
denning 
effects scheduling file memory operations 
afips spring joint computer conference april 
dave dunning greg regnier gary don cameron bill frank berry anne marie merritt ed chris dodd 
virtual interface architecture 
ieee micro march april 
william gropp ewing lusk rajeev thakur 
mpi advanced features message passing interface 
mit press cambridge ma 
john hennessy david patterson 
computer architecture quantitative approach 
morgan kaufmann publishers 
jay huber christopher daniel reed andrew chien david blumenthal :10.1.1.40.3622
ppfs high performance portable parallel file system 
proceed ings th acm international conference supercomputing pages barcelona july 
acm press 
ieee ansi std 

portable operating system interface posix part system application program interface api language edition 
david kotz 
disk directed mimd multiprocessors 
acm transactions computer systems february 
david kotz carla ellis 
caching writeback policies parallel file systems 
journal parallel distributed computing january february 
john bill nitzberg 
performance characteristics ipsc cm systems 
proceedings seventh international parallel processing symposium pages newport beach ca 
ieee computer society press 

small disk arrays approach high performance 
presentation compcon march 
ross 
implementation performance parallel file system high performance distributed applications 
proceedings fifth ieee international symposium high performance distributed computing pages 
ieee computer society press august 
michael litzkow todd jim miron livny 
checkpoint migration unix processes condor distributed processing system 
tech nical report computer sciences technical report university april 
susan marshall andy nanopoulos william milne richard wheeler 
sfs parallel file system cm 
proceedings summer usenix technical conference pages 
tara christopher daniel reed 
optimizing input output adaptive file system policies 
proceedings fifth nasa goddard conference mass storage systems pages ii september 
tara garth gibson christos faloutsos 
informed prefetching collective input output requests 
proceedings acm ieee sc conference november 
tara daniel reed 
intelligent adaptive file system policy selection 
proceedings sixth symposium frontiers massively parallel computation pages 
ieee computer society press october 
tara daniel reed 
input output access pattern classification hidden markov models 
proceedings fifth workshop input output parallel distributed systems pages san jose ca november 
acm press 
message passing interface forum 
mpi extensions message passing interface july 
www mpi forum org docs docs html 
steven moyer sunderam 
parallel system high performance distributed computing 
proceedings ifip wg working conference programming environments massively parallel distributed systems 
steven moyer sunderam 
scalable parallel system distributed computing environments 
proceedings scalable high performance computing conference pages 
myrinet software documentation 
www com scs 
nils david kotz 
galley parallel file system 
proceedings th acm international conference supercomputing pages philadelphia pa may 
acm press 
nils david kotz 
galley parallel file system 
parallel computing june 
nils david kotz carla ellis michael best 
file access characteristics parallel scientific workloads 
ieee transactions parallel distributed systems october 
bill nitzberg 
performance ipsc concurrent file system 
technical report rnd nas systems division nasa ames december 
david patterson garth gibson randy katz 
case redundant arrays inexpensive disks raid 
proceedings acm sigmod international conference management data pages chicago il june 
acm press 
daniel reed ruth roger noe philip roth keith shields bradley schwartz luis 
scalable performance analysis pablo performance analysis environment 
proceedings scalable parallel libraries conference october 
daniel ridge donald becker phillip thomas sterling 
beowulf harnessing power parallelism pile pcs 
proceedings ieee aerospace conference 
high performance portable mpi io implementation 
www mcs anl gov 
john salmon michael warren 
parallel core methods body simulation 
proceedings eighth siam conference parallel processing scientific computing march 
scheduled transfer api mappings 
www org html 
peter scheuermann gerhard weikum peter 
data partitioning load balancing parallel disk systems 
technical report eth zurich january 
lind wilson 
analysis auxiliary storage activity 
ibm system journal 
seamons chen jones winslett 
server directed collective panda 
proceedings supercomputing san diego ca december 
ieee computer society press 
seamons winslett 
efficient interface multidimensional array proceedings supercomputing pages washington dc november 
ieee computer society press 
daniel reed ryan fox mario medina james nancy tran wang 
framework adaptive storage input output computational grids 
proceedings third workshop runtime systems parallel programming 
hal stern 
managing nfs nis 
reilly associates 
gil 
mpi io parallel file system cluster workstations 
proceedings ieee international workshop cluster computing 
test tcp 
ftp ftp arl mil pub ttcp 
rajeev thakur william gropp ewing lusk 
device interface implementing portable parallel interfaces 
proceedings th symposium frontiers massively parallel computation pages 
ieee computer society press october 
rajeev thakur william gropp ewing lusk 
implementing mpi io portably high performance 
proceedings th workshop parallel distributed systems pages 
acm press may 

unix internals new frontiers 
prentice hall upper saddle river nj 

