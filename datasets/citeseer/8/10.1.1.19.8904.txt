new machine learning methods prediction protein topologies pierre baldi pollastri paolo frasconi alessandro department information computer science institute genomics bioinformatics university california irvine irvine ca usa ics uci edu fax dipartimento di sistemi informatica universit di firenze di santa marta firenze italy paolo dsi fax 
protein structures translation rotation invariant 
protein structure prediction important able assess predict intermediary topological representations distance contact maps translation rotation invariant 
develop new machine learning methods prediction assessment fine grained coarse topological representations proteins 
particular introduce general class graphical model architectures corresponding neural network implementations 
architectures viewed bayesian network generalizations input output hidden markov models involving input layer output layer hidden layer supported directed acyclic graphs 
corresponding generalized recursive neural network architectures derived preserving graphical structures replacing conditional probability tables learnable deterministic functions 
methods proposed prediction protein topological structures 
method uses organized horizontal layers input plane hidden planes output plane directly represents adjacency matrix contact map distance matrix 
hidden plane associated cardinal corners edges corresponding lattice oriented 
corresponding construct fine grained contact map predictor 
second method uses approach learn graph scoring function turn efficiently search space possible configurations 
corresponding construct coarse grained contact map predictor 
computer simulations show predictors tasks achieve stateof art performance 
keywords graphical models bayesian networks recurrent neural networks 
new machine learning methods prediction protein topologies predicting structure chains amino acids primary sequence fundamental open problem computational molecular biology 
approach problem deal fundamental property protein structures invariant translations rotations 
effect proposed machine learning approach protein structure decomposes problem steps computes intermediate topological representation protein contact map translation rotation invariant 
precisely step starts primary sequence possibly conjunction multiple alignments leverage evolutionary information predicts number structural features classification amino acids sequence secondary classes alpha helices beta strands coils relative exposure classes surface buried 
second step starts primary sequence structural features attempts predict contact map protein 
contact map representation neighborhood relationships consisting adjacency matrix distance cutoff typically range euclidean distance matrix 
finegrained contact maps derived amino acid atomic level 
coarse contact maps derived looking secondary structure elements instance centers gravity 
third step prediction actual coordinates contact maps 
topological representations obtained terms local relative angle coordinates 
predictors step described methods third steps developed nmr literature distance geometry stochastic optimization techniques 
focus second difficult step 
various algorithms prediction contacts distances contact maps developed particular neural networks 
best contact map predictor literature casp prediction experiment reports average accuracy correct prediction 
result encouraging chance level factor greater far providing sufficient accuracy reliable structure prediction 
key issue area amount noise tolerated contact map prediction compromising reconstruction step 
best knowledge systematic tests area published preliminary tests appear indicate recovery distant contacts distance cutoff ought suffice proper reconstruction proteins amino acid long rita private communication oral presentation casp 
chapter describe new machine learning methods prediction assessment protein topologies particular fine grained coarse contact maps 
algorithms connectionist architectures viewed noncausal generalizations iohmms input output hidden markov models process data structures richer sequences including spatial structures undirected graphs 
class methods described introduce new general class graphical model architectures associated implementations terms recurrent neural network architectures 
dimensional case originally introduced address bioinformatics sequence analysis problems particular prediction protein structural features protein secondary structure 
key contribution generalization dimensions opens door generalizations higher dimensions non necessarily spatial data structures 
new machine learning methods prediction protein topologies pipeline strategy machine learning protein structures 
example scj complex protein 
stage predicts structural features including secondary structure contacts relative solvent accessibility 
second stage predicts topology protein primary sequence structural features 
coarse topology represented cartoon providing relative proximity secondary structure elements alpha helices beta strands 
high resolution topology represented contact map residues protein 
final stage prediction actual coordinates residues atoms structure 
step pipeline review dimensional version architectures second step introduce key generalization 
second class methods learning task consists predicting scoring function associated hypothetical contact maps purpose guiding graph search algorithm 
case recursive neural networks extended handle undirected possibly cyclic graphs 
achieved advantage peculiar property protein contact maps vertices uniquely ordered protein terminus 
getting intricacies protein structure prediction suffices say prediction contact maps probably challenging essential step strategy 
example choose discussion entirely terms processing architectures inputs outputs 
obvious reader concepts produced similar architectures inputs outputs hmms inputs outputs markov chains 
new machine learning methods prediction protein topologies dimensional case bidirectional iohmms temporal data modeled processed markov models markov chains hmms factorial hmms iohmms kalman filters forth 
models represented graphical models bayesian networks characteristic left right architecture successfully domains speech bioinformatics instance model protein sequences protein families 
biological sequences temporal objects spatial objects 
observation crucial bioinformatics applications led left right markov models chains run opposite direction right left past form bidirectional iohmms 
plain iohmms bidirectional iohmms contain directed path connecting input output 
simple form bidirectional iohmms bayesian network consisting set inputs ii outputs oi discrete hidden states forward backward 
length sequence processed 
hidden variable states integer controls model complexity 
parameters model correspond conditional probability distributions output distribution forward transition distribution iohmm transition distribution associated backward chain oi ii ii ii conditional distributions stationary depend sequence index form parameters sharing 
bayesian network graphical model underlying consisting input units output units forward backward markov chains hidden states 
inference bidirectional iohmms takes time polynomial state size sequence length deterministic version model neural networks preferred real world applications computationally demanding 
details architecture section 
neural network version bidirectional iohmms extensively bioinformatics particular stage protein structure prediction pipeline described giving rise best predictors secondary structure solvent accessibility new machine learning methods prediction protein topologies coordination number 
corresponding predictor servers accessible promoter ics uci edu brnn pred 
general case topological generalized iohmms predict contact maps fundamental question idea bidirectional iohmms generalized dimensional dimensional objects 
turns canonical generalization described figures 
basic version generalization consists bayesian network organized horizontal layers planes input plane hidden planes output plane 
plane contains nodes arranged vertices square lattice 
vertical column input unit ii hidden units ne nw sw se associated cardinal corners output unit oi 

hidden planes edges oriented corresponding cardinal corner 
ne plane instance edges oriented north east 
parameters dimensional conditional probability distributions oi ii ne nw sw se ne ii ne nw ii nw sw ii sw se ii se hne obvious adjustments boundaries 
easy check proven directed graph cycles properly defines support bayesian network 
variation approach appendix 
ne nw sw se output plane hidden planes input plane general layout bayesian network processing dimensional objects contact maps nodes regularly arranged input plane output plane hidden planes 
plane nodes arranged square lattice 
hidden planes contain directed edges associated square lattices 
edges square lattice hidden plane oriented direction possible cardinal corners ne nw sw se 
additional directed edges run vertically column input plane hidden plane hidden plane output plane 
new machine learning methods prediction protein topologies ne se ne nw ne nw sw se details connections column 
input unit connected hidden units hidden plane 
input unit hidden units connected output unit 
ii vector inputs position 
oi corresponding output 
connections hidden unit lattice neighbors plane shown 
dimensional case clear build canonical dimension instance input cube units ii cubes hidden units 
output cube oi ranging hidden cubic lattice edges oriented corners 
generally dimensions hidden lattices 
lattice connections directed corners corresponding hypercube 
version architectures useful protein structure prediction problems simple form architecture address problems translation rotation invariance 
main reason current decomposition problems stages 
additional details 
short general definition graphical model describe probabilistic input output mapping data structures consisting set input nodes output nodes dag hidden layer possibly multiple connected components additional connections running input output nodes input nodes hidden nodes :10.1.1.33.7666
generalized hmms defined similar way just removing input nodes 
sw se nw sw new machine learning methods prediction protein topologies employed density estimation structured spaces trees graphs 
models specialized trees applied classification documents 
contact map prediction graph search problem second set methods developed basic ideas architectures learn graph scoring function graph scoring function efficiently search space possible contact maps 
methods general applied different classes graphs focus analysis simulations coarse contact map proteins contact map derived level secondary structure elements 
denote target contact map undirected graph 
candidate map scoring function values having properties 
iff 
iff null graph 
max known easily conceive heuristic search procedure finding correct contact map 
search algorithm takes vertex set input starts assigning null graph main loop essentially consists basic operations generation successor generated applying allowable graph edit operators evaluation successors scored function update replaced new graph closer solution 
algorithm terminates note property type edit operator required operator adds edge hill climbing version algorithm pseudo code see 
algorithm maintains loop invariant main loop lines subset invariant follows assumptions scoring function easily proved induction 
loop property scoring function analyze algorithm observe iteration requires evaluation candidate edge times 
loop search cmap executed times evaluated times 
evaluation takes time linear assume follows search cmap polynomial algorithm takes time 
apparent simplicity procedure finding clearly due existence oracle compute candidate map 
suggest suitable scoring function propose neural network model capable learning examples successful searches 
scoring function map introduce precision recall relative follows new machine learning methods prediction protein topologies search cmap repeat score return algorithm finding correct contact map perfect scoring function 
precision eq 
fraction edges predicted set correctly assigned 
recall eq 
fraction edges correctly discovered easily verified function satisfies properties suitable metric guiding heuristic search 
note function known metric information retrieval 
learning scoring function scoring function eq 
depends unknown 
introduce machine learning method approximating 
approach suggest input nodes elements possibly enriched attributes hidden layer topology reflects candidate map case fine grained maps elements amino acid symbols 
case coarse maps elements secondary structure elements described set numerical categorical attributes length position sequence secondary structure category chemical properties related average exposure solvent 
fundamental issue generation training set 
sequence length possible distinct contact maps including null complete graph 
clearly graphs training examples realistic subsampling strategy required 
training set generation static examples selected training begins dynamic examples inserted deleted training proceeds 
static case note random selection graphs protein extremely yield balanced dataset probability guessing random graph new machine learning methods prediction protein topologies high score decreases exponentially 
simple strategy guarantees reasonable balance high low score graphs run algorithm search cmap guided eq 
collect training examples graphs generated search 
disadvantage static strategy training learner specialized relatively narrow region search space repeated errors may drive search algorithm far away goal 
order mitigate problem hill climbing procedure algorithm search cmap changed beam search 
hill climbing keeps stage best candidate beam search maintains bounded open list size open list filled stage best candidates selected possible successors graphs open list previous stage 
alternative static selection examples propose online strategy examples generated fly network trained 
methods implement state space exploration suggested reinforcement learning literature practical ways dynamically sampling training instances 
methods costly implementing dynamic training scheme contest 
propose simplification 
learning phase sequence learner asked follow particular trajectory null graph final graph policy 
policy mapping states actions state undirected graph candidate contact map action edit operator adds edge 
various exploration strategies implemented order investigate effects exploration exploitation trade known reinforcement learning 
random exploration choose successor graph randomly uniform probability 
pure exploitation select successor graph maximum score predicted current network 
semi uniform exploration known greedy policy probability random uniform choice set successors probability choose maximum score graph 
see different strategies may significantly vary tradeoff precision recall eqs 

bi recursive topology framework data structures applied directly requires input graph directed ordered acyclic contact maps undirected unordered possibly cyclic graphs :10.1.1.33.7666
extension propose forwardbackward state space factorization bidirectional topologies described earlier sections order avoid message propagation directed cycles 
factorization possible serial order relation defined vertices allowing interpret pair directed acyclic graphs forward terminus graph gf having ef transpose backward graph gb 
shows example architecture 
node input graph labeled fixed length tuple 
forward backward hidden states hf hb respectively linked topologies gf gb 
architecture shown single output connected extreme hidden variables terminus hidden layer terminus hidden layer 
network supposed trained regression mode output approximates closely possible 
solution necessarily effective input nodes connected output long paths may introduce new machine learning methods prediction protein topologies sample graph graphical model bi recursive network consisting input nodes output node sets hidden states linked forward backward graphs gf gb bi recursive network output node problems associated long term dependencies 
suggest introduce output node position decompose scoring function set local scoring functions see 
precisely ev denote subsets edges incident vertex respectively local precision recall measure ev relative defined follows ev ev ev ev ev ev ev ev ev ev formula computes score weighted sum local scores computed node level evi locally representation pearson correlation coefficient close 
inference process model computes sequence output values 
vn predicts score vi new machine learning methods prediction protein topologies approach output unit trained regression mode approximate evi 
neural network architectures replacing recursive bayesian networks recursive neural networks described bayesian networks principle general propagation learning algorithms bayesian networks applied 
practice approach convenient feasible excessive computational demands computational intractability inference step 
learning algorithms bayesian networks presence hidden variables require iterative approaches em gradient descent 
algorithms inference called subroutine computational cost particularly critical 
example consider bidirectional iohmms 
inference case computationally tractable algorithms easily derived special forms belief propagation 
junction tree associated network cliques triplets state variables easily seen moralization triangulation network 
consequence complexity inference scales nn number forward backward discrete states sequence length 
order store information upstream downstream regions respect position large number states leading excessive computational burden training 
case dimensional described section unfortunately worse belief propagation intractable example easily seen triangulation applied regular grid yields cliques exponential size 
approximate inference algorithms variational methods help case 
totally different approach adopt devise efficient neural network versions graphical formalism retained different semantics 
missing arcs bayesian networks encode probabilistic conditional independence existing arcs corresponding neural network encode deterministic functional dependencies 
general method bayesian network corresponding neural network steps described graph 
discrete variables replaced real vectors non input node variable conditional probability table pa deterministic adaptive function pa implemented feedforward neural network multilayered perceptron 
stationarity maintained conditional probability tables identical corresponding neural networks shared weights 
doing computational complexity dramatically reduced inference consists forward propagation input signals outputs topological sort dag 
learning achieved gradient descent amounting back propagation unfolded space structure :10.1.1.33.7666
new machine learning methods prediction protein topologies bidirectional rnns precise consider example case dimensional bidirectional iohmms problem protein secondary structure prediction 
letting denote position sequence model outputs probability vector oi representing membership probability residue position classes alpha beta coil 
output implemented normalized exponential output units 
output prediction functional form oi ii denotes vector activities associated hidden node similarly output depends local input ii position forward upstream hidden context backward downstream hidden context vector ii ir encodes external input position simple case input limited single amino acid orthogonal encoding 
larger input windows extending amino acids possible 
learnable output function realized neural network see center top connections 
regression task performance model assessed usual mean square error 
multinomial classification task secondary structure prediction performance model better assessed relative entropy estimated target distribution 
contextual information contained vectors ir ir usually 
satisfy recurrent bidirectional equations nf ii fi nb ii bi nf nb learnable non linear state transition functions implemented nns nf nb left right subnetworks 
boundary conditions hf hb set hf hb length sequence examined 
intuitively think hf hb wheels rolled protein 
predict class position roll wheels opposite directions terminus position combine read wheels ii calculate proper output 
weights brnn architecture including weights recurrent wheels trained supervised fashion examples generalized form gradient descent backpropagation time unfolding wheels time space 
architectural variations obtained changing size input windows size window hidden states considered determine output number hidden layers number hidden units layer forth 
general architectures sequence translation translation prediction position depends combination local information provided standard feedforward neural network distant context information 
learning backpropagation space time 
lattice rnns clear immediately apply ideas case lattice 
output hidden layer propagations parameterized neural networks form new machine learning methods prediction protein topologies brnn architecture left forward right backward context associated recurrent networks wheels 
connections input wheels shown 
oij iij hne hne ii hne hne ii nsw ii nse ii ne plane instance boundary conditions set ne ij 
activity vector associated hidden unit ne ij depends local input iij activity vectors units ne ne 
activity ne plane propagated row row west east row south north column column south north column 
updates schemes easy code continuous requires jump row column toroidal architecture may considered contain directed cycles 
continuous update scheme zig zag scheme run successive diagonal lines oriented se nw 
depending software hardware implementation embodiment details scheme slightly faster smoother online learning 
worth noting homogeneous described far activity propagates simultaneously hidden dags source nodes sink nodes nodes center ones propagation hidden dags converges 
words correct output value stabilize center periphery 
bi recursive topology neural network implementation dependencies implied graphical model shown equations describe inference dynamics hf nf hf pa hv nb hb pa new machine learning methods prediction protein topologies pa denotes parents gf pa denotes parents gb note models upper bound indegree hidden node assumed :10.1.1.33.7666
means data structures processed network bounded connectivity 
node parents subgraph corresponding entries pa pa set zero 
total order parent sets pa pa necessary properly construct argument list functions nf nb 
case total order inherited serial order propagation algorithms straightforward recognize architecture cycles 
generalization generally consider connected dag hidden layer node inputs 
nodes strictly inputs called boundary nodes 
particular dag source node outgoing edges source node boundary node 
boundary node inputs add distinct input nodes called boundary condition nodes 
source nodes boundary condition nodes added 
pre processing step hidden dag regular sense nodes exactly inputs exception boundary condition nodes source nodes 
parameterize corresponding bayesian network neural network shared nodes 
network single output vector corresponding activity node input vectors 
dimension vector vary 
vectors associated boundary nodes set vector matching dimensions properly 
propagation activity proceeds boundary nodes sink nodes 
may multiple sink sources evidenced propagation hidden trees case tree structures 
fact graph dag boundary conditions ensures consistent order update nodes 
topological order may unique case lattice tree structures 
learning gradient descent recurrent networks extended learning recursive neural networks 
particular unfolding procedure associated gradient computation described easily extends enabling propagation algorithms gradient computation 
practice trivial get gradient descent learning procedures recurrent networks error gradients vanish rapidly function time learning procedures stuck poor local minima 
important factor architectures considering competition collaboration tradeoff hidden dags nn equivalents 
especially homogeneous hidden components equivalent system initialized small weights inherent symmetry needs broken 
algorithmic details address issues 
new machine learning methods prediction protein topologies simulations data fine grained maps training testing data sets extracted pdb select list february containing proteins 
list structures additional information obtained ftp site ftp ftp embl heidelberg de pub databases 
avoid biases set redundancy reduced identity threshold distance derived corresponds sequence identity roughly long alignments higher shorter ones 
set reduced excluding chains backbone interrupted 
extract coordinates secondary structure solvent accessibility information partners run sander program version pdb files pdb select list excluded ones crashed due instance missing entries erroneous entries format errors 
final set consists proteins 
training testing purposes subset containing proteins length constructed 
set contains proteins pairs amino acids 
essential notice contact maps depend strongly selection distance cutoff 
furthermore composition data general strongly biased favor 
typically contact map size contains number contacts linear test effect various distance thresholds contacts thresholds selected contact classification yielding different classification tasks 
number pairs amino acid class contact cutoff table 
table data set composition number pairs amino acids separated close far distance thresholds 
far close total coarse grained maps contacts coarse maps associated intuitive spatial neighborhood concept elements secondary structure 
neighborhood relation uniquely defined 
preliminary studies considered alternative definitions 
definition secondary structure elements contact distance centers gravity falls cutoff 
second definition elements contact atoms belonging element distance cutoff 
addition tested possible definition considering average distance projections principal axes secondary structure segments convex hulls 
preliminary attempts find major differences 
noted segment contact threshold fixed ais related amino acids segment contact 
experiments reported adopted definition contact distance atoms segment case coarse maps significant fraction current representative set non homologous protein data bank chains pdb select 
extracted chains new machine learning methods prediction protein topologies file sep accessible ftp embl heidelberg de pub databases listing proteins chains percentage homology identity 
set retained high quality proteins program version crash determined ray diffraction multiple nmr models physical chain breaks resolution threshold filtered set retained proteins sequence length amino acids resulting database proteins 
program assign secondary structure categories 
automatic assignments projected secondary structure states alpha beta gamma criteria maps alpha maps beta rest gamma 
resulting segments amino acid discarded 
previously noted working segment level results significant dimensionality reduction 
example subset pdb sequences low similarity average segment length residues size coarse map average roughly size residue resolution map 
inputs contact map prediction obvious input location pair corresponding amino acids 
amino acids represented orthogonal encoding vectors length single component set zero 
case input components 
structural inputs added 
reasonable expect relative solvent accessibility percentage indicator residue surface buried hydrophobic core globular protein relevant 
likewise secondary structure categories included 
value indicators close exact obtained pdb training data noisier estimated secondary structure accessibility predictor 
second type input consideration profile correlated mutation ideas 
profiles essentially form alignment homologous proteins implicitly contain evolutionary structural information related proteins 
information relatively easy collect known alignment algorithms ignore structure applied large data sets proteins including proteins unknown structure 
profiles improves prediction secondary structures percentage points probably secondary structure conserved amino acid sequence 
case secondary structure input modified include profile vector position profile vector position yielding dimensional probability vectors input numerical components 
distant pair positions multiple alignment considered horizontal correlations sequences may exist result entirely structural constraints 
horizontal correlations entirely lost profile entered independently 
expanded input retains information consists generally sparse matrix corresponding probability distribution pairs amino acids observed corresponding columns alignment 
typical alignment contain dozen sequences matrix general sparse 
unobserved entries set regularized small values standard dirichlet prior approach 
attempted larger input considered new machine learning methods prediction protein topologies tions extracted positions respect neighborhoods including instance 
compensate small alignment errors rapidly lead intractably large inputs scale size neighborhood considered 
compression techniques weight sharing higher order neural networks conjunction expanded inputs 
experiments reported inputs size just amino acids profiles size correlated mutation profile plus secondary structure solvent accessibility position 
architectures simulations consider problem predicting protein contact maps amino acid level 
approach hidden plane lattices diagonal edges associated similar independent neural networks 
neural network single hidden layer 
architecture described key parameters number hidden units output neural network number hidden units hidden neural networks associated lateral propagation planes number noh units output layer hidden neural networks corresponding dimension vector encoding hidden state hidden planes 
experimented architectures results report noh corresponding parameters input size 
learning initialization training implemented line adjusting weights complete presentation protein 
stronger version line training updates weights presentation unnecessary inefficient 
experiments reported trained networks half data tested remaining half 
protein sequences length training set sequences test set 
piecewise linear learning step learning rate equal divided number protein examples 
prior learning weights unit various neural networks randomly initialized 
standard deviations controlled flexible way avoid bias ensure expected total input unit roughly range 
results amino acid contact maps results contact map predictions distance cutoffs provided table 
experiment system trained half set proteins length tested half 
results obtained plain sequence inputs amino acid pairs information profiles correlated mutations structural features 
cutoff instance system able recovering contacts 
course essential able predict contact maps longer proteins 
attempted training recurrent neural network architectures larger data sets new machine learning methods prediction protein topologies example exact left predicted contact map protein prior symmetrization prediction 
color code blue non contact red contact 
table percentages correct predictions different contact cutoffs validation set 
model trained tested proteins length 
inputs correspond simple pairs amino acids sequence profiles correlated profiles structural features 
far close containing long proteins 
experiments progress noted systems developed accommodate inputs arbitrary lengths system trained short proteins produce predictions longer proteins 
fact overwhelming majority contacts proteins linear distances shorter amino acids reasonable expect decent performance system 
observe table 
cutoff percentage correctly predicted contacts proteins length 
table percentages correct predictions different contact cutoffs validation set 
model trained proteins length tested proteins length 
inputs correspond simple pairs amino acids sequence 
far close typical example prediction reported 
example display raw output network symmetric symmetry constraints enforced learning 
symmetric output easy derive non symmetric output averaging output values positions 
application averaging procedure yields small improvement prediction performance seen table 
new machine learning methods prediction protein topologies table table symmetric prediction constraints 
far close possible alternative enforce symmetry training phase 
table percentages correct predictions different contact cutoffs validation set 
model trained tested proteins length 
inputs size correspond correlated profiles multiple alignments derived psi blast program 
far near table percentages correct predictions different contact cutoffs validation set 
model trained tested proteins length 
table inputs include secondary structure relative solvent accessibility threshold derived program 
row represents standard deviations protein basis 
far near std results additional experiments conducted larger inputs displayed tables 
inputs size corresponding correlated profiles performance increases marginally roughly contacts instance table 
secondary structure relative solvent accessibility threshold added input performance shows remarkable improvement range contacts 
example contacts predicted accuracy 
row table provides standard deviations accuracy protein basis 
standard deviations reasonably small proteins predicted levels close average 
results support view secondary structure relative solvent accessibility important prediction contact maps useful profiles correlated profiles 
confirmed results obtained model trained short proteins correlated profile inputs augmented structural features tested proteins length table 
cutoff model predicts contacts correctly achieving state art performance previously reported results 
terms diagonal prediction sensitivity amino acids satisfying contrasted reported 
small improvement derived combining output new machine learning methods prediction protein topologies predictors similar architecture noh table trained data overfitting detected 
global improvement visible improvement table 
non trivial improvement prediction contacts 
table percentages correct predictions different contact cutoffs validation set 
model trained proteins length tested proteins length 
inputs include correlated profiles secondary structure relative solvent accessibility 
far near table percentages correct predictions different contact cutoffs validation set obtained network combining predictors trained distance cutoff 
model trained proteins length tested proteins length 
inputs include correlated profiles secondary structure relative solvent accessibility 
far near results coarse contact maps method detailed section tested prediction protein contact maps coarse level 
numerical features encode input label node comprise hot encoding secondary structure type normalized linear distances terminus average maximum minimum hydrophobic character segment scale moving length window centered residues positions segment 
note network model implementation uses input minimal amount biologically significant information 
table graph search dynamic sampling summary experimental results 
report micro averaged precision recall denoted mp mf respectively 
corresponding macro averages denoted mp mf mp nc mp nc micro macro averaged precisions predicting non contacts 
sampling strategy mp mp nc mf mp mp nc mf random exploration semi uniform exploration pure exploitation hybrid preliminary experiments carried tune prediction system choose best architectural parameters 
splitting proteins training test validation sets selected bi recursive nn architecture state vectors dimension new machine learning methods prediction protein topologies forward backward dynamics hidden layers 
hidden layers greater dimension state vectors model particularly sensitive overfitting phenomena 
performed set experiments trying effect static training set generation explained section 
protein generated sample graphs means search procedure target graph guided perfect evaluation function collecting valid successors search total graphs 
dataset split training set proteins graphs test set proteins graphs estimate prediction accuracy validation set early stopping 
adapted version bi recursive model training procedure 
trained network replaced heuristic evaluation function subsequent topology search algorithm beam search procedure beam size 
search algorithm scales bw number weights network 
experiments compare graph search algorithms architecture section 
results summarized plot macro micro averaged precision vs recall 
graph similar roc curve plots true positive rate false positive rate different cutoffs diagnostic test 
macro averages computed averaging precision recall set proteins 
measure tends weight performance short sequences 
micro averages obtained computing precision recall flattened set segment pairs 
precision search algorithm consistently better recall reaches 
obtained significant expense cpu time training phase prediction 
second experiment investigated effects dynamic sampling exploration training 
applied exploration strategies described section random exploration pure exploitation semi uniform exploration 
applied trying find optimal balance exploration exploitation 
probability uniform random exploration set 
addition strategies tried different approach network agent proceed greedily pure exploitation step best possible successor time network updated representative sample current state set successors 
main purpose exploration scheme guide gradient descent region parameters optimized wide possible spectrum values candidate alternatives 
experiments fold cross validation procedure splitting representative set proteins subsets routinely testing subset training remaining 
cross validation exploration scheme obtained results indicated table 
column labeled different updating schemes applied 
strategy report performances measured indices micro macro averaged precision mp mp recall measure mf mf 
report percentage correct prediction non contacts averaged set proteins mp nc segments pairs mp nc 
row hybrid provides indices obtained example selection strategy described 
new machine learning methods predicting protein topologies form contacts amino acids secondary structure elements 
new machine learning methods prediction protein topologies methods general class bayesian networks call process data structures variable size associated particular graphical supports sequences lattices trees general graphs case coarse contact maps 
efficiency architectures replaced recursive neural network versions call trained examples generalized gradient descent methods 
case coarse contact maps ideas learn scoring function turn efficiently search space possible topological configurations 
simulations shown lattice perform significantly better method prediction fine grained contact maps 
coarse contact maps combination graph search methods far yielded promising results 
direction include integration predictions different level granularity computationally efficient extension graph search methods fine grained contact maps 
pb gp part supported laurel faculty innovation award nih sun microsystems award pb uci 
pf av partially supported murst 

aligned sequences share fold 
mol 
biol 
taylor 
global fold determination small number distance restraints 
mol 
biol 
baker 
protein structure prediction structural genomics 
science 
baldi 
gradient descent learning algorithms overview general dynamical systems perspective 
ieee transactions neural networks 
baldi brunak 
bioinformatics machine learning approach 
mit press cambridge ma 
second edition 
baldi brunak frasconi soda pollastri 
exploiting past protein secondary structure prediction 
bioinformatics 
baldi brunak frasconi pollastri soda 
bidirectional dynamics protein secondary structure prediction sun giles eds sequence learning lnai pp 

baldi chauvin 
hybrid modeling hmm nn architectures protein applications 
neural computation 
baldi pollastri 
generalized iohmms recurrent neural network architectures 

submitted 
baldi pollastri 
machine learning structural functional proteomics 
ieee intelligent systems 
special issue intelligent systems biology 
new machine learning methods prediction protein topologies bengio simard frasconi 
learning long term dependencies gradient descent difficult 
ieee transactions neural networks 
bengio frasconi 
input output hmm sequence processing 
ieee trans 
neural networks 
gori 
problem local minima recurrent neural networks 
ieee transactions neural networks 
diligenti frasconi gori document categorization hidden tree markov models structured representations 
singh eds 
advances pattern recognition proc 
pages 
lncs springer 

neural network predictor residue contacts proteins 
protein engineering 

prediction number residue contacts proteins 
proceedings conference intelligent systems molecular biology ismb la jolla ca pages 
aaai press menlo park ca 
valencia 
prediction contact maps neural networks correlated mutations 
protein engineering 
frasconi gori sperduti :10.1.1.33.7666
general framework adaptive processing data structures 
ieee trans 
neural networks 
frasconi gori sperduti sequences data structures theory applications 
kolen kremer eds 
field guide dynamic recurrent networks ieee press 
frasconi 
prediction protein coarse contact maps recursive neural networks 
proc 
ieee embs conference molecular cellular tissue engineering press 
gobel sander schneider valencia 
correlated mutations residue contacts proteins 
proteins structure function genetics 
lund andersen brunak 
sequence motifs enhanced neural network prediction protein distance constraints 
proceedings seventh international conference intelligent systems molecular biology ismb la jolla ca pages 
aaai press menlo park ca 
heckerman 
bayesian networks data mining 
data mining knowl 
discov 
heckerman 
tutorial learning bayesian networks 
jordan editor learning graphical models 
kluwer dordrecht 
schneider sander 
selection representative data sets 
prot 
sci 
ghahramani jordan 
factorial hidden markov models machine learning 
sander 
dictionary protein secondary structure pattern recognition geometrical features 
biopolymers 
lauritzen 
graphical models 
oxford university press oxford uk 
lesk lo conte hubbard 
assessment novel fold targets casp predictions dimensional structures secondary structures function genetics 
proteins 
submitted 
lund bohr bohr hansen brunak 
protein distance constraints predicted neural networks probability density functions 
prot 
eng 
rost valencia 
effective sequence correlation conservation fold recognition 
mol 
biol 
valencia 
improving contact predictions combination correlated mutations sources sequence information 
fold 
des 
new machine learning methods prediction protein topologies ausiello valencia 
correlated mutations contain information protein protein interactions 
mol 
biol 
pearl 
probabilistic reasoning intelligent systems 
morgan kaufmann san mateo ca 
pollastri baldi 
contact maps recurrent neural network architectures 

ismb conference 
submitted 
pollastri baldi 
prediction coordination number relative solvent accessibility proteins 
proteins 
press 
pollastri rost baldi 
improving prediction protein secondary classes recurrent neural networks profiles 
proteins 
press 
sander 
dimensional contacts proteins predicted analysis correlated mutations 
protein engineering 
simons strauss baker 
prospects ab initio protein structural genomics 
mol 
biol 
sutton barto 
reinforcement learning 
mit press 
thrun 
role exploration learning control 
white editors handbook intelligent control neural fuzzy adaptive approaches 
van nostrand reinhold florence kentucky 
domany 
recovery protein structure contact maps 
folding design 

protein folding contact maps contact vectors 
volume 
new machine learning methods prediction protein topologies precision recall graph search micro graph search macro micro macro precision recall plots comparing search algorithm 

