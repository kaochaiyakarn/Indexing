constrained independent component analysis wei lu school computer engineering nanyang technological university singapore email ntu edu sg presents novel technique constrained independent component analysis cica introduce constraints classical ica solve constrained optimization problem lagrange multiplier methods 
shows cica order resulted independent components speci manner normalize demixing matrix signal separation procedure 
systematically eliminate ica indeterminacy permutation dilation 
experiments demonstrate cica ordering independent components providing normalized demixing processes 
keywords independent component analysis constrained independent component analysis constrained optimization lagrange multiplier methods independent component analysis ica technique transform multivariate random signal signal components mutually independent complete statistical sense 
growing interest research ecient realization ica neural networks 
neural algorithms provide adaptive solutions satisfy independent conditions convergence learning 
ica de nes directions independent components 
magnitudes independent components norms demixing matrix may varied 
order resulted components arbitrary 
general ica inherent indeterminacy dilation permutation 
reduced additional assumptions constraints 
constrained independent component analysis cica proposed way provide unique ica solution certain characteristics output introducing constraints avoid arbitrary ordering output components statistical measures give indices sort order evenly highlight salient signals 
produce unity transform operators normalization demixing channels reduces dilation ect resulted components 
may recover exact original sources 
conditions applied ica problem constrained optimization problem 
lagrange multiplier methods adopted provide adaptive solution problem 
implemented iterative updating system neural networks referred 
section brie gives problem analysis solution lagrange multiplier methods 
basic concept ica stated 
lagrange multiplier methods utilized develop systematic approach cica 
simulations performed demonstrate usefulness analytical results indicate improvements due constraints 
lagrange multiplier methods lagrange multiplier methods introduce lagrange multipliers resolve constrained optimization iteratively 
penalty parameter introduced condition local convexity assumption holds solution 
lagrange multiplier methods handle problems equality inequality constraints 
constrained nonlinear optimization problems lagrange multiplier methods deal take general form minimize subject matrix vector problem arguments objective function 
gm de nes set inequality constraints 
hn de nes set equality constraints 
lagrangian methods directly deal inequality constraints possible transform inequality constraints equality constraints introducing vector slack variables 
result equality constraints 
transformation corresponding simpli ed augmented lagrangian function problem de ned maxf jjh jj 

sets lagrange multipliers scalar penalty parameter equals jj 
jj denotes euclidean norm jj 
jj penalty term ensure optimization problem held condition local convexity assumption xx 
augmented lagrangian function gives wider applicability provides better stability 
discrete problems changes augmented lagrangian function de ned 
xl achieve saddle point discrete variable space 
iterative equations solve problem eq follows 
xl maxf denotes iterative index 
unconstrained ica time varying input signal xn interested signal consisting independent components ics generally signal considered linear mixture independent components ac mixing matrix full column rank 
goal general ica obtain linear demixing matrix recover independent components minimal knowledge normally recovered components wx contrast function mutual information output signal de ned sense variable entropy measure independence xm marginal entropy component output joint entropy 
non negative value equals zero components completely independent 
minimizing learning equation demixing matrix perform ica 
transpose inverse matrix nonlinear function depending activation functions neurons sources 
assumptions exact components possible dilation permutation 
independent components columns rows estimated multiplicative constant 
de nitions normal ica imply ordering independent components 
constrained ica practice ordering independent components quite important separate non stationary signals interested signals signi cant statistical characters 
eliminating indeterminacy permutation dilation useful produce unique ica solution systematically ordered signals normalized demixing matrix 
section presents approach cica enhancing classical ica procedure lagrange multiplier methods obtain unique ics 
ordering independent components independent components ordered descent manner certain statistical measure de ned index 
constrained optimization problem cica de ned follows minimize mutual information subject 
gm set inequality constraints de nes descent order index statistical measures output components variance normalized kurtosis 
lagrange multiplier methods augmented lagrangian function de ned eq maxf discrete solutions applied changes individual element ij formulated minimizing eq 
ij 
ij min ij maxf maxf 
rst derivative index measure 
iterative equation nding individual multipliers maxf learning equation normal icnn multiplier iterative equation iterative procedure determine demixing matrix follows 

wl 
um um um apply measures variance kurtosis examples emerge ordering signals 
functions corresponding rst derivative 
variance var var kurtosis kur kur gu signal variance shows majority information input signals consist 
ordering variance sorts components information magnitude needs reconstruct original signals 
accompanying preprocessing constraints pca normalization normal ica indeterminacy dilation demixing matrix may cause variance output components ampli ed reduced 
normalized kurtosis kind th order statistical measure 
kurtosis stationary signal extracted constant situation indeterminacy signals amplitudes 
kurtosis shows high order statistical character 
signal categorized super gaussian gaussian sub distributed ones kurtosis 
components ordered distribution sparseness super gaussian sub gaussian 
kurtosis widely produce unit ica 
contrast sequential extraction approach extract order components parallel 
normalization demixing matrix de nition ica implies indeterminacy norm mixing demixing matrix contrast pca 
unknown mixing matrix estimated rows demixing matrix normalized applying constraint term ica energy function establish normalized demixing channel 
constrained ica problem de ned follows minimize mutual information subject 
hm de nes set equality constraints 
de ne row norms demixing matrix equal 
lagrange multiplier methods augmented lagrangian function de ned eq diag ww ww jj diag 
denotes operation select diagonal elements square matrix vector 
applying discrete lagrange multiplier method iterative equation minimizing augmented function individual multiplier iterative equation demixing matrix follows 

wl assume normalized source unit variance input signal processed matrix px obeys normalized demixing matrix network output contains exact independent components unit magnitude contains non assignment experiments results cica algorithms simulated matlab version 
learning procedure ran iterations certain learning rate 
signals preprocessed whitening process zero mean uniform variance 
accuracy recovered components compared source components measured signal noise ratio snr db signal power measured variance source component noise mean square error sources recovered ones 
performance network separating signals ics measured individual performance index ipi permutation error ith output xn jp ij max jp ik ij elements permutation matrix wa 
ipi close zero corresponding output closely independent components 
ordering ics signal separation independent random signals distributed gaussian sub super gaussian manner simulated 
statistical con gurations similar 
source signals mixed random matrix derive inputs network 
networks trained obtain demixing matrix algorithm kurtosis constraint cica eq separate independent components complete ica manner 
source components mixed input signals resulted output waveforms shown gure respectively 
network separated samples time series samples time series samples time series result extraction super gaussian gaussian signals kurtosis descent order 
normalized kurtosis measurements 
source components input mixtures resulted components 
sorted output components decreasing manner kurtosis values component kurtosis super gaussian gaussian sub gaussian 
nal performance index value output components average snr value db show independent components separated 
demixing matrix normalization deterministic signals gaussian noise simulated experiment 
signals independently generated unit variance mixed random mixing matrix 
input mixtures preprocessed whitening process zero mean unit variance 
signals separated unconstrained ica constrained ica eq respectively 
table compares resulted demixing matrix row norms variances separated components snr values 
dilation ect seen di erence demixing matrix norms variance snr ica cons ica table comparison demixing matrix elements row norms output variances resulted components snr values ica cica normalization 
components variances caused non normalized demixing matrix unconstrained ica 
cica algorithm normalization constraint normalized rows demixing matrix separated components variances remained unit 
source signals exactly recovered dilation 
increment separated components snr values cica seen table 
source components input mixture separated components normalization gure 
shows resulted signals cica exactly match source signals sense waveforms amplitudes 
samples time series samples time series samples time series source deterministic components unit variances mixture inputs resulted components normalized demixing channel approach constrained ica lagrange multiplier methods eliminate indeterminacy permutation dilation classical ica 
results provide technique systematically enhancing ica usability performance constraints restricted conditions treated 
useful constraints considered similar manners improve outputs ica practical applications 
simulation results demonstrate accuracy usefulness proposed algorithms 
wei lu 
uni ed approach independent component networks 
second international icsc symposium neural computation nc 
bell sejnowski 
information maximization approach blind separation blind deconvolution 
neurocomputing 
amari yang 
new learning algorithm blind signal separation 
advances neural information processing systems 

lee girolami sejnowski 
independent component analysis extended algorithm mixed sub gaussian super gaussian sources 
neural computation 
comon 
independent component analysis new concept 
signal processing 
dimitri bertsekas 
constrained optimization lagrange multiplier methods 
new york academic press 
hyv erkki oja 
simple neuron models independent component analysis 
neural systems december 
