means extending means efficient estimation number clusters dan pelleg andrew moore school computer science carnegie mellon university pittsburgh pa usa cmu edu cmu edu despite popularity general clustering means suffers major shortcomings scales poorly computationally number clusters supplied user search prone local minima 
propose solutions problems partial remedy third 
building prior algorithmic acceleration approximation introduce new algorithm efficiently searches space cluster locations number clusters optimize bayesian information criterion bic akaike information criterion aic measure 
innovations include new ways exploiting cached sufficient statistics new efficient test means sweep selects promising subset classes refinement 
gives rise fast statistically founded algorithm outputs number classes parameters 
experiments show technique reveals true number classes underlying distribution faster repeatedly accelerated means dif ferent values 
means duda hart bishop long workhorse metric data 
attractive ness lies simplicity local minimum convergence properties 
main shortcomings 
slow scales poorly respect time takes complete iteration 
number clusters supplied user 
confined run fixed value empirically finds worse local optima dynamically alter offer solutions problems 
speed greatly improved embedding dataset multiresolution kd tree storing sufficient statistics nodes 
careful analysis centroid locations allows ric proofs voronoi boundaries deng moore zhang moore absolutely approximation computation 
additional geometric computation blacklisting maintains list just centroids need considered region pelleg moore 
blacklisting extremely fast scales number centroids allowing tractable means algorithms 
fast algorithm building block means new algorithm quickly estimates 
goes action run means making local decisions subset current split order better fit data 
splitting decision done computing bayesian information criterion bic 
show blacklisting method naturally extends ensure obtaining bic values current centers tentative offspring costs single means iteration 
enhance computation caching stable state information eliminating need re compute 
experimented means traditional method estimates number clus ters guessing 
means consistently produced better clustering synthetic real life data respect bic 
runs faster baseline accelerated blacklisting means 

definitions describe naive means algorithm producing clustering points input clusters 
partitions data points subsets points subset belong center 
algorithm keeps track subsets proceeds iterations 
iteration initialized random values 
algorithm terminates locations stay fixed iteration 
iteration performed 
point find closest associate 
re estimate locations centroid center mass points associated 
means algorithm known converge lo cal minimum distortion measure av erage squared distance points class cen 
known slow practical databases 
related attempt confront algorithmic issues directly 
different methods subsampling approximation proposed 
way obtain small balanced sample points sampling leaves tree shown ester 

ng hah suggested simulated annealing approach direct search space possible partitions input points 
zhang 
tree structure sufficient statistics 
iden tify outliers speed computations 
calculated clusters approximations depend parameters 
note starting centers selected arbitrarily means fully deterministic starting centers 
bad choice initial centers great impact performance tion 
bradley fayyad discuss ways refine selection starting centers repeated subsampling smoothing 
remainder denote pj coordinates th centroid 
tation denote index closest th data point 
example associated th point iteration 
input set points di set points closest 
number di gaussian covariance matrix diag cr 

estimation algorithm described point perform means fixed supplied user 
proceed demonstrate efficiently search best framework changes user specifies range true reasonably lies output set centroids value range scores best model selection cri bic see section 
describe process conceptually paying attention algorithmic details 
derive statistical tests scoring different structures 
come back high level description algorithm show implemented efficiently ideas deriving blacklisting sufficient statistics stored kd tree nodes 
model searching essence algorithm starts equal lower bound range continues add needed upper bound reached 
process set achieves best score recorded output 
algorithm consists operations repeated completion 

improve params 
structure 
report model search 
goto 
improve params operation simple consists running conventional means convergence 
improve structure operation finds new appear 
achieved letting split 
decide split 
describing obvious strategies combine strengths avoid weaknesses means strategy 
idea oe time idea pick produce new nearby run means completion see resulting model scores better 
accept new 
doesn return previous structure 
need 
improve structure steps means complete 
begs question choose centroid deserving give birth 
doesn improve score tried 
tested way stick best test needs run means extremely expensive operation adding centroid 
splitting idea trg half 
second idea system gaus sian mixture model identification wasserman moore press 
simply choose say half centroids heuristic criterion promising split 
split run means see resulting model scores better original 
accept split 
aggressive structure im provement requiring 
improve ure steps means completes 
heuristic criterion 
size region owned centroid 
distortion due centroid furthermore chance improve cases centroids need split rest 
solution achieves benefits ideas avoids drawbacks see section turned extremely fast operation 
explain means example 
shows stable means solution centroids 
boundaries regions owned centroid shown 
structure improvement operation begins splitting centroid children 
moved distance pro size region opposite directions randomly chosen vector 
parent region run local means pair children 
local children fighting points parent region 
shows step local means runs 
shows children eventually local means terminated 
point model selection test performed pairs children 
case test asks evidence children modeling real structure original parent model distribution equally 
section gives details test means 
outcome test parent offspring killed 
hope centroids set points form cluster true underlying distribution modified process outlive children 
hand regions space represented 
result running means cen 

original centroid splits children 
current centroids receive attention increasing number centroids 
shows happens test applied pairs children 
search space covers possible post splitting configurations determines explore improving bic locally region 
compared ideas allows automatic choice increase number centroids case current number close true number current model severely underestimates 
ically regional means runs just centers tend sensitive local minima 
continue oscillating improve params improve ure upper bound attained 
bic scoring assume data family models mj case different mod els correspond solutions different values step parallel local means 
line coming centroid shows moves 
bic bic 
bic bic bc 
result parallel means terminated 
surviving centroids local model scoring tests 
choose best 
probabilities pr score models 
case models type assumed means spherical gaussians 
approxi mate posteriors normalization formula fi om kass wasserman log log likelihood data accord ing th model taken maximum likelihood point number parameters 
known schwarz criterion 
maximum likelihood estimate mle vari ance identical spherical gaussian assumption point probabilities exp ii log likelihood data fix belong plugging maximum likelihood estimates yields log log number free parameters simply sum class probabilities centroid coordinates variance estimate 
extend formula fact log likelihood points belong centroids question sum log likelihood individual centroids replace total number points belong centroids consideration 
bic formula globally means chooses best model encountered locally split tests 
acceleration means algorithm described far implemented small datasets 
far neglected important feature 
invented subject design constraint possible cached statistics scale datasets massive numbers records 
accelerating means concentrating single means iteration 
task determine data point owns 
compute center mass points belong new location 
immediately observe showing subset points belong just informative doing single point sufficient statistics subset case sufficient statistics number points vector sum 
clearly may save lot computation provided doing significantly expensive demonstrating ownership single point 
kd tree imposes hierarchical structure dataset easily compute sufficient statistics nodes construction time natural selec tion partition points 
kd node represents subset data set 
bounding box minimal axis parallel hyper rectangle includes points subset 
addition contains pointers children nodes represent bisection points parent owns 
consider set counters centroid store running total number points belong vector sum 
show update counters scanning kd tree just 
update procedure recursive accepts parameters node list may points 
task update counters nodes question appropriate values points node 
ini tial invocation root node list 
returns new locations may calculated counters 
procedure considers geometry bounding box current centroid locations eliminate centroids list proving possibly point current node 
name blacklisting 
complete details proofs pelleg moore 
point remember shrinking list procedure recurses children current node 
halting condition list contains just centroid centroid counters incremented statistics stored kd node 
frequently happens shallow level kd tree eliminates needed traverse descendants 
accelerating improve structure procedure described works update centroid locations global means iteration 
apply procedure carry forward improve structure step 
recall improve structure perform means voronoi region current structure 
list parent input update procedure 
difference global iteration action taken list reduces single 
event signifies fact points current node belong single cen 
divide children 
simply list containing just children recurse short list current node 
rest done update procedure 
full scan kd tree carried possibly pruning away nodes counters child centroids final values new locations computed 
new iteration take place pairs settled 
additional acceleration interesting outcome local decision making regions space tend active lot splitting re arrangement takes place regions centroids true classes appear dormant 
translate pattern acceleration caching statistics previous iterations 
consider kd node contains boundary centroids centroids may node points 
recurse tree order update counters centers reason iteration provided centroids move centroid moved position node points 
cache con tribution node centroids counters node subsequent iterations need traverse tree current node list competing centroids matches 
enable fast comparison centroid locations position previous iterations employ data structure permit alteration centroid coordinates initial insertion 
case centroid location changes new element inserted unique identifier 
way comparison identifiers needed 
clearly old accessed need keep storing data structure 
allows fast implementation original memory plus hash table identifier lookup 
extension idea cache children centroid regional iteration 
killed moved state regional iteration 
owing fact identifiers change caching mechanism immediately recalls outcome local iteration may reached re positioning steps 

experimental results experiment tested quality means solution means 
define quality bic value solution unfair means algorithm tries optimize distortion average squared distance points centroids 
compared algorithms distortion output 
gaussian datasets generated pelleg moore algorithms 
means true number classes oo oooo ooo points 
distortion means means average distortion point 
results average runs data classes 
means variant search range tq see 
interestingly distortion values means solutions lower meaning higher quality solutions 
may attribute gradual way means adds new areas needed 
contrasts placement initial means 
interesting question means revealing true number classes 
comparison variant means simply tries different values reports configuration resulted best bic 
permissible range means means equally distant values 
averaged results table detailed results class case 
show means outputs configuration true number classes 
see means better respect average deviation 
results show means tends estimate number classes output classes number records increases means usually estimates true general insensitive slightly different picture arises examine bic score output configurations 
note means variant chooses best configuration bic score fair comparison 
see means scores better means respect outperforms underlying distribution generate data 
may explained random deviations table 
mean absolute error vs number classes output algorithms data points 
classes error means means means means tru 








number output classes function input size data true classes averaged randomly generated data sets 
data cause better modeled fewer classes 
example class centers chosen random may fall extremely close approximate single class 
far speed concerned means scales better iterated means 
shown means runs twice fast large problems 
note competition accelerated version means described pelleg moore 
compared naive means compute distances point cen pick minimal means fares bet ter 
dataset galaxies means completed seconds traditional means choosing values took seconds 
algorithms set perform just iter ation improve params stage means programmed iterate stage augment split global iteration 
quality means solution superior terms bic distortion 
means means 
true 

bic means means 
average bic point shown 
results average multiple runs data classes 
abe true stands bic score centroids generate data 
interesting application means arises astrophysics domain 
dataset composed galaxies coordinates ask typical size cluster galaxies 
selected brightest galaxies sloan digital sky survey data 
input set approximately sky objects divided grid ranges data proportional axis 
cells approximately number objects 
cell objects ran means iterated values range means searching ic range recorded resulting number clusters equivalently average cluster size 
average cluster size means means 
hard validate manually tend believe means free choose number clus ters wide range means validate small number specified fact reflected smaller variance means put 
early experiments range large number sample points small effect noticeable early stages scientific research guesses tend educated expect recurring theme 
terms run time means faster increases advantage number points increases similarly way syn datasets 
means run full data survey progresses tve expect number galaxies significantly 
lo ooo 


run times means means 
average run times dimensions classes mhz pentium 
set points resulting centroids takes hours mhz dec alpha 
similar means invocation ran hard coded limit running twice long 
similar experiment important task clustering galaxies las redshift survey compared means highly optimized traditional implementation means 
traditional means tried different val ues 
algorithms solutions identical bic scores chose larger value means completed search times faster means 

new ix means algorithm incorporates model selection 
adopting extending algorithmic improvements means efficient extent running cheaper looping fixed model algorithm 
uses statistically criteria local decisions maximize model posterior probabilities 
experimental results synthetic reallife data show performing faster better means 
choice bic splitting criterion possible 
bic perform test sets applications criteria aic mdl may sense areas 
incorporating measures algorithm straightforward 
direct extension application bic similar criteria direct model search unrestricted gaussian em algorithm blacklisting assuming hard membership non trivial 
vein currently progress wasserman moore 
think ways con duct search model means assumption removing adding 
fast algorithms statistical analysis millions data points thousands classes performed matter hours 
consequently able test astrophysical theories observations larger scale available past 
hinted opens opportunity large class algorithms aid endeavors 
need consider question data 
empirically demonstrated means dimensional data simpler algorithms pelleg moore give significant accelerations di 
dimensions interesting 
say reasons 
big science disciplines need cluster data sets millions billions low dimensional records quickly 
spatial galaxy color space sky objects protein gel clustering just examples collaborating natural scientists 
second high dimensional data sets frequently preferable model pdf factored represen tation meila bayesian network node distributions represented clusters 
means step fast inner loop expensive algorithms 
larry wasserman invaluable help statistical foundations 
support provided nsf award andrew moore dms 
bishop 

neural networks pattern recognition 
oxford clarendon press 
bradley fayyad 

refining ini tial points means clustering 
fifteenth international learning pp 

morgan kaufmann san fran cisco ca 
deny moore 

multiresolution instance learning 
proceedings twelfth international joint conference artificial intel pp 

san francisco morgan kaufmann 
duda hart 

pattern classifi cation scene analysis 
john wiley sons 
ester kriegel xu 

database interface clustering large spatial databases 
proceedings international conference discovery data mining 
menlo park aaai 
kass wasserman 

bayesian test nested hypotheses relationship schwarz criterion 
journal american statistical association 
las redshift survey 
astro utoronto ca lin html 
meila 

efficient tree learning 
doctoral dissertation massachusetts institute technology department computer science cambridge ma 
moore 

fast mixture model clustering multiresolution kd trees 
advances neural information processing systems pp 

morgan kaufmann 
ng ham 

efficient effective clustering methods spatial data mining 
proceedings vldb 
pelleg moore 

accelerating exact means geometric reasoning technical report cmu cs 
carnegie mellon uni versity pittsburgh pa available www 
cs 
cmu 
sloan digital sky survey 
www 
sdss 
org 
wasserman moore density estimation accelerated exact mixture models 
press 
zhang ramakrishnan livny 

birch efficient data clustering method large dat 
proceedings cm pp 

