approximate frequency counts data streams singh manku stanford university manku cs stanford edu algorithms computing frequency counts exceeding user specified threshold data streams 
algorithms simple provably small memory footprints 
output approximate error guaranteed exceed user specified parameter 
algorithms easily deployed streams singleton items ip network monitoring 
handle streams variable sized sets items exemplified sequence market basket transactions retail store 
streams describe optimized implementation compute frequent itemsets single pass 
emerging applications data takes form continuous data streams opposed finite stored datasets 
examples include stock network traffic measurements web server logs click streams data feeds sensor networks telecom call records 
stream processing differs computation traditional stored datasets important aspects sheer volume stream lifetime huge queries require timely answers response times small 
consequence possible store stream entirety secondary storage scan query arrives 
motivates design summary data structures small memory footprints support time continuous queries 
supported nsf iis stanford graduate fellowship supported nsf iis foundation re search microsoft veritas 
permission copy fee part material granted provided copies distributed direct commercial advantage vldb copyright notice title publication date appear notice copying permission large data base endowment 
copy republish requires fee special permission endowment 
proceedings th vldb conference hong kong china rajeev motwani stanford university rajeev cs stanford edu aggregate query widely applicable select count group relation models stream events attribute denoting type event 
applications groups low frequencies uninteresting user interested groups frequency exceeds certain threshold usually expressed percentage size relation seen far 
modified query select count group having count user specified percentage length stream seen far 
related aggregate query input stream relation tuples sets items individual items 
goal compute subsets items called itemsets occur fraction stream seen far 
far challenging counting singleton tuples 
motivating examples problems drawn databases data mining computer networks frequency counts exceeding user specified threshold computed 

iceberg query performs aggregate function attribute set attributes relation eliminates aggregate values user specified threshold 

association rules dataset consisting sets items require computation frequent itemsets itemset frequent occurs user specified fraction dataset 

iceberg br compute group cube operator aggregate frequency exceeds user specified threshold 

traffic measurement accounting ip packets requires identification flows exceed certain fraction total traffic ev 
existing algorithms iceberg queries association rules iceberg cubes optimized finite stored data 
compute exact results attempting minimize number passes entire dataset 
best algorithms take passes 
adapted streams pass allowed results expected available short response times algorithms fail provide priori guarantees quality output 
algorithms computing frequency counts single pass priori error guarantees 
algorithms variable sized transactions 
compute frequent sets items single pass 
offline streams data warehousing environment bulk updates occur regular intervals time daily weekly monthly 
desirable data structures storing aggregates frequent itemsets iceberg cubes maintained incrementally 
complete entire database update prohibitively costly multi terabyte warehouses 
call sequence updates warehouses backup devices offline stream distinguish online streams stock network measurements sensor data 
offline streams characterized regular bulk arrivals 
queries streams allowed processed offline 
time scan entire database update comes 
considerations motivate design summary data structures agp allow incremental maintenance 
summaries significantly smaller base datasets need necessarily fit main memory 
query processing offline online streams similar summary data structures need incrementally maintained possible entire input 
difference lies time scale size updates 
truly incremental algorithm maintaining frequent itemsets data warehouse setting 
algorithm maintains small summary structure require repeated entire database 
roadmap section review applications compute frequent itemsets identifying problems existing approaches 
section formally state problem solve 
section describe algorithm singleton items 
section describe basic algorithm extended handle sets items 
describe implementation detail 
section report experiments 
related section 
frequency counting applications section review existing algorithms different applications identifying shortcomings surface adapt stream scenarios 
iceberg queries idea iceberg queries identify aggregates group sql query exceed user specified threshold prototypical query ck rest threshold select ck count rest group ck having count rest parameter equivalent tos percentage size algorithm uses repeated hashing multiple passes 
builds park 
basic idea pass set counters maintained 
incoming item hashed counter incremented 
counters compressed bitmap denoting large counter value 
second pass exact frequencies elements maintained hash value corresponding bitmap value variations improvements basic idea explored difficult adapt algorithm streams pass frequencies available 
iceberg cubes iceberg cubes br take iceberg queries step proposing variant cube operator ck select ck count sum cube ck having count algorithms iceberg cubes take passes number dimensions cube computed 
algorithms applied preprocessing step problem computing iceberg cubes 
clause sum removed algorithm enable computation approximate iceberg cubes single pass 
currently exploring application algorithms iceberg cubes 
frequent itemsets association rules transaction defined subset items drawn universe items 
collection transactions itemset said support occurs subset fraction transactions 
association rules set transactions rules form subsets support exceeding user specified threshold confidence rule value usually rules produced confidence exceeds user specified thresh old 
apriori algorithm successful solutions association rules 
problem reduces computing frequent itemsets 
frequent itemsets identified identifying rules confidence exceeds stipulated confidence threshold straightforward 
fastest algorithms date employ passes 
pass techniques proposed savasere son input partitioned chunks fit main memory 
pass computes set candidate frequent itemsets 
second pass computes exact supports candidates 
clear algorithm adapted streaming data pass guarantees accuracy frequency counts candidates 
pass approach sampling toi 
briefly idea produce sample pass compute frequent itemsets negative border main memory samples verify validity negative border 
negative border defined parameter small probability negative border invalid happens frequency count highly frequent element sample true frequency count entire dataset shown want probability value size sample constant toivonen algorithm provides strong probabilistic guarantees accuracy frequency counts 
scheme adapted data streams reservoir sampling vit 
problems false negatives occur error frequency counts sided small values number samples enormous need samples 
second problem ameliorated employing concise samples idea studied gibbons matias gm 
essentially repeated occurrences element compressed element count pair 
adapting concise samples handle variable sized transactions amount designing data structure similar fp trees periodically frequencies singleton items change 
idea explored data mining community 
problem false negatives toivonen approach remains 
algorithms produce false negatives 
hid pass algorithm gives loose guarantees quality output produces pass 
upper lower bound frequency itemset displayed user continuously 
guarantee sizes intervals small 
provides interval information best effort basis goal providing user coarse feedback running 
online incremental maintenance association rules offline streams data warehouses important practical problem 
provide solution guaranteed require pass entire database 
network flow identification measurement monitoring network traffic required management complex internet backbones 
measurement essential short term monitoring hot spot denial service attack detection longer term traffic engineering rerouting traffic upgrading selected links accounting usage pricing 
identifying flows network traffic important applications 
flow defined sequence transport layer tcp udp packets share source destination addresses 
estan verghese ev proposed algorithms identifying flows exceed certain threshold say algorithms combination repeated hashing sampling similar iceberg queries 
algorithm directly applicable problem network flow identification 
beats algorithm ev terms space requirements 
problem definition section describe notation definition approximation goals algorithms 
algorithm accept user specified parame error parameter ters support threshold denote current length stream number tuples seen far 
applications tuple treated single item 
denotes set items 
term item set short hand item itemset 
point time algorithm asked produce list item set estimated frequencies 
answers produced algorithm guarantees 
item set true frequency exceeds output 
false negatives 

item set true frequency output 

estimated frequencies true frequencies imagine user interested identifying items frequency entire stream seen far 
user free set feels comfortable margin error 
rule thumb set tenth twentieth value algorithm 
assume chooses tenth 
property elements frequency exceeding output false negatives 
property element frequency output 
leaves elements frequencies form part output 
way output false positives 
property individual frequencies true frequencies approximation algorithms aspects high frequency false positives small errors individual frequencies 
kinds errors tolerable applications outlined section 
say algorithm maintains deficient synopsis output satisfies aforementioned properties 
goal devise algorithms support deficient synopses little main memory possible 
algorithms frequent items section describe algorithms computing deficient synopses singleton items 
extensions sets items described section 
devised algorithms frequency counts 
provide approximation guarantees outlined section 
algorithm sticky sampling probabilistic 
fails provide correct answers probability failure 
second algorithm lossy counting deterministic 
interestingly experimentally show lossy counting performs better practice theoretically worse worst case bound 
sticky sampling algorithm section describe sampling algorithm computing deficient synopsis data stream singleton items 
algorithm probabilistic said fail properties section satisfied 
user specifies parameters support error probability failure data structure set entries form estimates frequency element belonging stream 
initially empty sampling rate set sampling element rate means select element probability incoming element entry exists increment corresponding frequency sample element rate element selected sam pling add entry ignore move element stream 
sampling rate varies lifetime stream follows elements sampled rate elements sampled rate elements sampled rate 
sampling rate changes scan entries update follows entry repeatedly toss unbiased coin coin toss successful diminishing unsuccessful outcome process delete entry number unsuccessful coin tosses follows geometric distribution 
value efficiently computed vit 
effectively transformed exactly state sampling new rate 
user requests list items threshold output entries theorem sticky sampling computes deficient synopsis probability expected number entries 
proof sampling rate elements find way follows error frequency count element corresponds sequence unsuccessful coin tosses occurrences length sequence follows geometric distribution 
probability length exceeds turn number elements frequency probability estimate deficient probability failure yields space bound follows 
call algorithm sticky sampling space requirements sweeps stream magnet attracting elements entry note space complexity sticky sampling independent current length stream 
idea maintaining counts samples gibbons matias gm compress samples solve top problem frequent items need identified 
algorithm different sampling rate increases logarithmically proportional size stream guarantee produce items frequency exceeds just top lossy counting algorithm section describe deterministic algorithm computes frequency counts stream single item transactions satisfying guarantees outlined sec tion space denotes current length stream 
user specifies param eters support error algorithm performs better sticky sampling practice theoretically worst case space complexity worse 
definitions incoming stream conceptually divided buckets width transactions 
buckets labeled bucket ids starting denote current bucket id value element denote true frequency stream seen far note fixed running variables values change stream progresses 
data structure set entries form element stream inte ger representing estimated frequency maximum possible error algorithm initially empty 
new ele ment arrives lookup see entry exists 
lookup succeeds update entry incrementing frequency 
create new entry form prune deleting entries bucket boundaries rule deletion simple entry deleted user requests list items thresh old output entries represents exact frequency entry count entry inserted value assigned new entry maximum number times occurred buckets 
value exactly inserted value remains unchanged 
lemma deletions occur lemma entry entry gets deleted proof prove induction 
base case entry deleted true frequency gets deleted entry inserted induction step consider entry bucket processed 
entry possibly deleted late time bucket full 
induction true frequency deletion occurred true frequency inserted 
fol lows true frequency buckets combined deletion rule get lemma appear proof lemma true element gets deleted true 
lemma lemma infer gets deleted 
lemma proof possibly deleted buckets 
lemma infer exact frequency deletion took place conclude consider elements true frequency exceeds elements 
lemma shows entries lemma shows estimated frequencies elements accurate correctly maintains deficient synopsis 
theorem shows number low frequency elements occur times large 
theorem lossy counting computes deficient synopsis entries 
proof current bucket id denote number entries bucket id element corresponding entry occur times buckets deleted 
size bucket get constraints claim prove inequality induction 
base case inequality true follows inequality directly 
assume show true 
adding inequality instances inequality varying gives get readily simplifies inequality completing induction step 
inequality get rear assumption real world datasets elements low frequency tend oc cur uniformly random lossy counting requires space proved appendix 
note ibm test data generator data mining generating synthetic datasets association rules chooses low frequency items uniformly randomly fixed distribution 
sticky sampling vs lossy counting section compare sticky sampling lossy counting 
show lossy counting performs significantly better streams random arrival distributions 
glance theorems suggests sticky sampling require constant space lossy counting require space grows logarithmically length stream 
theorems tell story bound worst case space complexities 
sticky sampling worst case amounts sequence duplicates arriving order 
lossy counting worst case corresponds pathological sequence suspect occur practice 
show arrival order elements uniformly random lossy counting superior large factor 
studied streams 
duplicates 
highly skewed followed zipfian distribution 
streams practice terms frequency distributions 
streams assumed uniformly random arrival order 
table compares algorithms varying values value consistently chosen tenth worst case space complexities obtained plugging values theorems 
ss lc ss lc ss lc worst worst zipf zipf uniq uniq table memory requirements terms number entries 
lc denotes lossy counting 
ss denotes sticky sampling 
worst denotes worst case bound 
zipf denotes zipfian distribution parameter uniq denotes stream duplicates 
length stream entries entries stream length memory requirements terms number entries support error probability failure zipf denotes zipfian distribution parameter uniq denotes stream duplicates 
bottom magnifies section barely visible lines upper graph 
sticky sampling performs worse tendency remember unique element gets sampled 
lossy counting hand pruning low frequency elements quickly high frequency elements survive 
highly skewed data algorithms require space worst case bounds 
comparison alternative approaches known technique estimating frequency counts employs uniform random sampling 
sample size relative frequency single element accurate fraction probability basic idea toivonen toi devise sampling algorithm association rules 
sticky sampling beats approach roughly factor algorithm maintaining deficient synopsis employ approximate quantiles mrl 
key idea element frequency exceeding recur times small relative follows set compute approximate histograms high frequency elements deduced error algorithm due greenwald probability failure shows amount space required streams function support error probability failure khanna gk worst case space requirement curve sticky sampling correspond re sampling 
units apart axis 
lossy counting correspond bucket boundaries deletions occur 
comparison memory requirements sticky sampling uniq sticky sampling zipf lossy counting uniq lossy counting zipf log stream length memory requirement profile lossy counting lossy counting uniq lossy counting zipf worse lossy counting theorem 
obvious quantile algorithms efficiently extended handle variable sized sets items problem consider section 
learned unpublished algorithm kps identifying elements frequency exceeds fraction stream 
algorithm computes exact counts passes 
essential ideas maintain deficient synopsis exactly space 
pass algorithm maintains elements counters 
initially counters free 
new element arrives check counter element exists 
simply increment 
counter free assigned element initial value counters repeatedly diminish counters counter free value drops zero 
assign newly arrived element counter initial value algorithm works streams singleton items 
appear straightforward adaptation scenario stream variable sized transactions analyzed distribution transaction sizes known 
input stream zipfian number elements exceed table ing threshold significantly smaller shows lossy counting takes space 
example roughly entries suffice skew cies consider problem identifying frequent sets items section 
example assume transactions known fixed size frequent subsets size computed 
adaptation algorithm kps maintain counters 
lossy counting require significantly space experiments section show 
frequent sets items theory practice section develop lossy counting algorithm computing frequency counts streams consist sets items 
section theoretical nature preceding 
focus system level issues implementation optimizing memory speed 
frequent itemsets algorithm input algorithm stream transactions transaction set items drawn denote current length stream user specifies parameters support error challenge lies handling variable sized transactions avoiding explicit enumeration subsets transaction 
data structure set entries form subset items inte ger representing approximate frequency maximum possible error initially empty 
imagine dividing incoming transaction stream buckets bucket consists transactions 
buckets labeled bucket ids starting denote current bucket id process input stream transaction transaction 
try fill available main memory transactions possible process batch transactions 
algorithm differs section 
time amount main memory available increase decrease 
denote number buckets main memory current batch processed 
update follows update set entry date counting occurrences current entry satisfies batch 
updated delete entry 
new set set frequency current batch occur create new entry easy see set true fre entry entry quency true frequency satisfies inequality user requests list items threshold output entries important large number 
reason subset occurs times contributes entry smaller spurious subsets find way section show represented compactly update set new set implemented efficiently 
data structures implementation modules buffer trie 
buffer repeatedly reads batch transactions available main memory 
trie maintains data structure described earlier 
operates current batch transactions 
enumerates subsets transactions frequencies 
trie implements update set new set operations 
challenge lies designing space efficient representation trie fast algorithm avoids generating possible subsets itemsets 
buffer module repeatedly reads batch transactions available main memory 
transactions sets item id laid big array 
bitmap remember transaction boundaries 
bit item id denotes item id member transaction 
reading batch buffer sorts transaction item id trie module maintains data structure outlined section 
conceptually forest set trees consisting labeled nodes 
labels form estimated frequency maximum possible error distance node root tree belongs 
root nodes level level node parent 
children node ordered item id root nodes forest ordered item id node tree represents itemset consisting item id node ancestors 
mapping entries nodes trie 
tries association rules algorithms 
hash tries popular choice 
usual implementations trie require pointers memory segments number children node changes time 
trie different traditional implementations 
tries bottleneck far space concerned designed compact possi item id ble 
maintain trie array entries form corresponding pre order traversal underlying trees 
note equivalent lexicographic ordering subsets encodes 
pointers node children siblings 
compactly encode underlying tree structure 
representation okay tries scanned sequentially show 
module generates subsets item id frequencies current batch transactions lexicographic order 
possible subsets need generated 
glance description date set new set operations reveals subset enumerated iff occurs trie fre quency current batch exceeds uses pruning rule subset way trie application update set new set supersets considered 
similar apriori pruning rule 
describe efficient implementation greater detail 
algorithm buffer repeatedly fills available main memory transactions possible sorts 
operates current batch transactions 
generates sets itemsets frequency counts lexicographic order 
limits number subsets pruning rule 
trie implement update set new set operations 
trie stores updated data structure buffer gets ready read batch 
efficient implementations buffer item id successive integers small say maintain exact frequency counts singleton sets 
need array size ex act frequency counts maintained buffer prunes away item id frequency sorts transactions 
note length stream including current batch transactions 
trie generates sequence sets associated frequencies trie needs updated 
adding deleting trie nodes situ difficult fact trie compact array 
take advantage fact sets produced sequence additions deletions lexicographically ordered 
recall compact trie stores constituent subsets lexicographic order 
lets trie hand hand 
maintain trie huge array set fairly large sized chunks memory 
modifying original trie create new trie afresh 
chunks old trie freed soon required 
overhead maintaining tries significant 
time finishes chunks original trie completely discarded 
finite streams important trie optimization tains batch transactions value number buckets buffer small 
applying rules section prune nodes trie aggressively setting threshold deletion frequency nodes contribute final output 
lower module bottleneck terms time algorithm 
optimizing fairly complex 
describe salient features implementation 
employs priority queue called heap initially contains pointers smallest item id transactions buffer 
duplicate members pointers pointing item id maintained constitute single entry heap 
fact chain pointers deriving space chain buffer 
item id buffer inserted heap byte integer represent item id converted byte pointer 
heap entry removed pointers restored back item id repeatedly processes smallest item id heap generate singleton sets 
singleton belongs trie update set new set rules applied try generate set lexicographic sequence extending current singleton set 
done invoking recursively new heap created successors pointers item id just removed processed 
successors item id item id transaction 
item id transactions successors 
recursive call returns smallest entry heap removed successors currently smallest item id added heap chain pointers described earlier 
system issues optimizations buffer scans incoming stream memory mapping input file 
saves time getting rid double copying file blocks 
unix system call memory mapping files mmap 
accompanying interface allows process inform operating systems intent read file sequentially 
sort transactions 
time taken read sort transactions comparison time taken obviating need custom sort routine 
threading buffer help significantly slower 
tries written read sequentially 
operational buffer processed 
time disk idle 
rate tries scanned read written smaller rate sequential disk done 
possible maintain trie disk loss performance 
important advantages size trie limited size main memory available case algorithms 
means algorithm function amount main memory available quite small 
available memory devoted buffer smaller values algorithms handle 
big win 
trie currently implemented pair anonymous memory mapped segments 
associated actual files user desires 
tries read written sequentially accessed randomly possible compress decompress fly sections read written disk 
current implementation attempt compression node labels 
writing trie disk violates definition single pass algorithms 
note term single pass meaningful disk bound applications 
program cpu bound 
memory requirements heap modest 
available main memory consumed primarily buffer assuming trie disk 
implementation allows user specify size buffer 
novel features technique implementation differs apriori variants important aspect candidate generation phase 
apriori finds frequent itemsets size finding frequent itemsets size amounts breadth search frequent itemsets lattice 
algorithm carries depth search 
incidentally buc br uses repeated depth traversals iceberg cube computation 
passes entire data number dimensions cube 
idea compact disk tries novel 
allows compute frequent itemsets low memory conditions 
enables algorithm handle smaller values support threshold previously possible 
experimental results experimented kinds datasets streams market basket transactions text document sequences 
frequent itemsets streams transactions experiments carried ibm test data generator 
study data streams size transactions 
average transaction size average large itemset size average transaction size average large itemset size conventions set forth names datasets numbers denote average transaction size average large itemset size number transactions respectively 
items drawn universe unique items 
raw sizes streams mb mb respectively 
experiments carried linux kernel version 
experiments fix tenth 
amount main memory required programs dominated buffer size stipulated user 
left parameters study experiments support number transactions size buffer total time taken 
measure wall clock time 
plot times taken algorithm values support ranging buffer size ranging mb mb 
leftmost graphs show decreasing leads increases running time 
middle graphs interesting explanation 
graphs plot running time varying buffer sizes 
fixed value support running time increases buffer size increases 
happens due trie optimization described section 
finite streams batch transactions processed threshold raised leads considerable savings run pentium iii processor running ning time batch 
buffer size input split equal sized batches 
buffer size increases batch increases size leading increase running time 
buffer size reaches mb big accommodate entire input batch rapidly processed 
leads sharp decrease running time 
increasing buffer size effect running time 
rightmost graphs show running time linearly proportional length stream 
curve flattens processing batch faster owing trie optimization mentioned section 
disk bandwidth required read input file mbps 
low rate compared modern day disks 
single high performance scsi disk deliver confirms frequent itemset computation cpu bound 
interesting fact emerged experiments error output zero 
itemsets reported output error 
happens highly frequent itemset invariably occurs batch transactions 
enters data structure get deleted 
mbps 
rarely false positives reason fre elements range accurate 
suggests able set higher value get accurate results 
higher error rate observed highly skewed data global data characteristics change stream sorted 
comparison apriori comparison known apriori algorithm loaded publicly available package written christian borgelt pretty fast implementation apriori prefix trees commercial data mining package 
version linux support re linked borgelt program widely available library doug lea just program termination memory requirements 
algorithm algorithm apriori mb buffer mb buffer support time memory time memory time memory mb mb mb mb mb mb mb mb mb mb mb mb mb mb mb mb mb mb table performance comparison transactions unique items average transaction size table compare apriori algorithm dataset varying support error set ran algorithm twice buffer set mb fuzzy cs uni magdeburg de borgelt software html edu dl html malloc html time taken seconds time taken seconds time taken seconds time taken seconds varying buffer sizes support buffer mb buffer mb buffer mb buffer mb support threshold time taken seconds varying support buffer sizes support support support support buffer size mbytes time taken seconds time taken stream progresses support support support length stream thousands times taken ibm test dataset items 
transactions 
varying buffer sizes support buffer mb buffer mb buffer mb buffer mb support threshold time taken seconds varying support buffer sizes support support support support buffer size mbytes times taken ibm test dataset experimental results algorithm ibm test datasets 
support threshold varying buffer sizes support buffer mb buffer mb buffer mb buffer mb varying buffer sizes support buffer mb buffer mb buffer mb buffer mb support threshold time taken seconds time taken seconds time taken stream progresses support support support length stream thousands items 
transactions 
varying support buffer sizes support support support support support buffer size mbytes time taken seconds times taken frequent word pairs web pages 
time taken seconds varying support buffer sizes support support support support support buffer size mbytes times taken frequent word pairs reuters documents times taken iceberg queries web pages reuters articles 
time taken seconds time taken stream progresses support support support length stream thousands time taken stream progresses support support support length stream thousands buffer set mb 
table shows total memory required programs 
program includes maximum cost heap trie runtime 
value support increases memory required trie decreases 
fewer itemsets higher support 
size trie decreases buffer changes mb mb 
value see section increases 
fewer low frequency subsets frequency creep trie 
interesting observe buffer set small value mb algorithm able compute frequent itemsets memory apriori time 
buffer size mb entire input fits main memory 
program beats apriori factor showing main memory implementation faster 
iceberg queries iceberg query studied identification pairs words repository web documents occur times gether 
note relation query explicitly materialized 
query equivalent identifying documents 
word pairs occur ran query different datasets 
dataset collection web pages crawled webbase web crawler developed stanford university 
words document identified 
common words sb removed 
resulting input file mb 
experiments dataset carried mhz pentium iii machine running linux kernel version 
second dataset known reuters newswire dataset containing news articles 
input file resulting dataset removing words roughly mb 
experiments dataset carried mhz pentium iii machine running linux kernel version 
study interplay length stream support time taken size buffer 
shape graphs similar frequent itemsets ibm test datasets studied previous section 
sake comparison algorithm original iceberg queries ran program web documents support settings corresponds query studied see 
ran program exactly machine mhz sun ultra ii mb ram running sunos 
fixed buffer mb 
program processed input batches producing frequent word pairs 
buffer heap auxiliary data structures required mb 
maximum size trie mb 
program took seconds complete 
fang re port query required seconds roughly mb main memory 
algorithm faster 
interesting duality emerges approach algorithm 
program scans input just repeatedly scans temporary file disk memory mapped trie 
iceberg algorithm scans input multiple times uses temporary storage 
advantage approach require lookahead data stream 
related problems related frequency counting studied context data streams include approximate frequency moments ams differences distinct values estimation fm bit counting top queries gm 
algorithms data streams pertain aggregation include approximate quantiles mrl gk histograms gks wavelet aggregate queries correlated aggregate queries gks 
currently exploring application basic techniques sliding windows data cubes pass algorithms frequent itemsets 
proposed novel algorithms computing approximate frequency counts elements data stream 
algorithms require provably small main memory footprints 
problem identifying frequent elements heart important problems iceberg queries frequent itemsets association rules packet flow identification 
solve streaming data 
described highly optimized implementation identifying frequent itemsets 
general algorithm produces approximate results 
datasets studied algorithm runs pass produces exact results beating previous algorithms terms time 
frequent itemsets algorithm handle smaller values support threshold previously possible 
remains practical environments moderate main memory 
believe algorithm provides practical solution problem maintaining association rules incrementally warehouse setting 
agp acharya gibbons poosala 
aqua fast decision support system approximate query answers 
proc 
th intl 
conf 
large data bases pages 
ams alon matias szegedy 
space complexity approximating frequency moments 
proc 
th annual acm symp 
theory computing pages may 
agrawal srikant 
fast algorithms mining association rules 
proc 
th intl 
conf 
large data bases pages 
br beyer ramakrishnan 
bottom computation sparse iceberg cubes 
proc 
acm sigmod pages 
charikar chen farach colton 
finding frequent items data streams 
proc 
th intl 
colloq 
automata languages programming 
datar gionis indyk motwani 
maintaining stream statistics sliding windows 
proc 
th annual acm siam symp 
discrete algorithms january 
ev estan verghese 
new directions traffic measurement accounting 
acm sigcomm internet measurement workshop november 
feigenbaum kannan strauss viswanathan 
approximate difference algorithm massive data streams 
proc 
th annual symp 
foundations computer science pages 
fm flajolet martin 
probabilistic counting algorithms 
comp 
sys 
sci 
fang shivakumar garcia molina motwani ullman 
computing iceberg queries efficiently 
proc 
th intl 
conf 
large data bases pages 
gk greenwald khanna 
space efficient online computation quantile summaries 
proc 
acm sigmod pages 
gilbert kotidis muthukrishnan strauss 
surfing wavelets streams pass summaries approximate aggregate queries 
proc 
th intl 
conf 
large data bases 
gks gehrke korn srivastava 
computing correlated aggregates continual data streams 
proc 
acm sigmod pages 
gks guha koudas shim 
data streams histograms 
proc 
rd annual acm symp 
theory computing pages july 
gm gibbons matias 
new sampling summary statistics improving approximate query answers 
proc 
acm sigmod pages 
hid 
online association rule mining 
proc 
acm sigmod pages 
han pei dong wang 
efficient computation iceberg cubes complex measures 
proc 
acm sigmod pages 
han pei yin 
mining frequent patterns candidate generation 
proc 
acm sig mod pages 
hirai raghavan garcia molina paepcke 
webbase repository web pages 
computer networks 
kps karp papadimitriou shenker 
personal communication 
motwani raghavan 
randomized algorithms 
cambridge university press edition 
mrl manku rajagopalan lind say 
random sampling techniques space efficient online computation order statistics large datasets 
proc 
acm sigmod pages 
matias vitter wang 
dynamic maintenance wavelet histograms 
proc 
th intl 
conf 
large data bases pages 
park chen yu 
effective hash algorithm mining association rules 
proc 
acm sigmod pages 
sb salton buckley 
term weighting approaches automatic text retrieval 
information processing management 
son savasere omiecinski navathe 
efficient algorithm mining association rules large databases 
proc 
st intl 
conf 
large data bases pages 
toi toivonen 
sampling large database association rules 
proc 
nd intl 
conf 
large data bases pages 
vit vitter 
random sampling reservoir 
acm tran 
math 
software 

whang vander zanden taylor 
linear time probabilistic counting algorithm database applications 
acm trans 
database systems 
appendix theorem lossy counting stream elements drawn independently fixed probability distribution proof element probability chosen element stream 
consider elements number entries contributed elements members bucket contribute entry entries 
remaining entries elements inserted current bucket survived deletion phase 
show fewer entries 
prove bound claimed lemma 
current bucket id denote number entries consider element con tributes arrival remaining elements buckets looked sequence poisson trials probability denote number successful trials number remaining occur element trials get contribute require chernoff bound tech niques see theorem yield inequality write get follows theorem true positions high frequency elements chosen adversary low frequency elements required drawn fixed distribution 
