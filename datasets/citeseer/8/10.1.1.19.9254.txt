krylov projection methods model reduction eric james ohio state university university illinois urbana champaign thesis submitted partial fulfillment requirements degree doctor philosophy electrical engineering graduate college university illinois urbana champaign urbana illinois dissertation focuses efficiently forming reduced order models large linear dynamic systems 
projections unions krylov subspaces lead class models known rational interpolants 
cornerstone dissertation collection theory relating krylov projection rational interpolation 
theoretical framework algorithms model reduction proposed 
algorithm dual rational arnoldi numerically reliable approach involving orthogonal projection matrices 
second rational lanczos efficient generalization existing lanczos methods 
third rational power krylov avoids orthogonalization suited parallel approximate computations 
performance algorithms compared combination theory examples 
independent precise algorithm host supporting tools developed form complete model reduction package 
techniques choosing matching frequencies estimating modeling error insuring model stability treating multiple input multiple output systems implementing parallelism avoiding need exact factors large matrix pencils examined various degrees 
iii dedication wife kimberly 
iv acknowledgments supported doctoral studies 
foremost list advisors professors kyle gallivan paul van 
paul deserves credit starting numerical linear algebra 
insights showed interesting field 
go kyle support years doctoral research 
encouragement humor appreciated 
drs 
danny sorensen steve ashby eli working different times past years 
excellent host contributed significantly education 
danny played key role iterative krylov methods 
steve pointed direction davidson method approach implicitly touches parts dissertation 
eli patiently led basics circuit analysis asked stimulating questions 
gratitude goes examination committee professors pai time comments 
professors deserve providing hours classroom instruction 
department energy financial support 
dissertation supported part computational science graduate fellowship program office scientific computing department energy 
table contents chapter page motivating trends problem overview dissertation goals notation krylov model reduction problem statement solution techniques implementation techniques projection preconditioning existing approaches history explicit moment matching lanczos moment matching projection framework rational interpolation rational interpolation theory interpretations theory limits theory singular large scale pencil rank deficient projection matrices singular reduced order pencil issues stable models multiple input multiple output models projection methods rational interpolation rational krylov method rational power krylov algorithm dual rational arnoldi algorithm initial rational lanczos algorithm practical rational lanczos algorithm comparisons vi model error complementary approximations residual expressions comparisons model interpolation points analysis tools point placement imaginary interpolation points real interpolation points multiple interpolation points point selection adaptive termination adaptive selection adaptive placement comparisons parallel rational interpolation overview parallel dual rational arnoldi algorithm parallel rational power algorithm approximate rational interpolation conversions approximate solves approximate solve algorithms approximate rational lanczos approximate rational power methods comparisons relating inner outer recursions iterative eigenvalue solvers existing techniques preconditioned matrices reduced order pencils existing implementations approaches eigenvalues arriving pies model reduction summary results possibilities mimo systems rank deficiencies multilevel parallelism vii approximate solvers related problems appendix lemma proofs appendix selected matlab implementations vita viii list tables table page matrix notation abbreviations character notation moment choices model reduction modeling choices lanczos algorithm orthogonalization choices rational krylov algorithm rational lanczos parameters example rational lanczos parameters example computational costs rk implementations memory requirements rk implementations modeling error estimates example interpolation point strategies example convergence interpolation point strategies example interpolation point strategies example convergence interpolation point strategies example convergence digits precision example convergence digits precision example convergence digits precision example convergence digits precision example convergence digits precision example convergence digits precision example convergence digits precision example convergence digits precision example convergence approximate solutions example convergence approximate solutions example improved starting vector results example classifying existing pies ix list figures page partial realization pad approximation example shifted pad approximation rational interpolation example sparsity structure example sparsity structure example interconnect segment example frequency response example convergence example loss bi orthogonality example computational costs example interpolation point interlacing complementary models frequency response example frequency response example error estimate ffl example error estimate ffl example error estimate ffl example error estimate ffl example error estimate example error estimate example error estimate example error estimate example frequency response example error estimate ffl example error estimate delta example eigenvalue mapping imaginary interpolation point eigenvalue mapping real interpolation point frequency response example eigenvalue spectrum example eigenvalue spectrum example approximate frequency response example approximate frequency response example approximate frequency response example approximate frequency response example approximate frequency response example approximate frequency response example approximate frequency response example approximate frequency response example approximate frequency response example approximate frequency response example eigenvalue mapping oe discretized pde grid theta case discretized pde sparsity pattern theta case coupled interconnects theta case mna sparsity pattern coupled interconnects theta case frequency response example finite precision dual ra results example frequency response example finite precision dual ra results example xi chapter iterative projection methods solution large scale frequency dependent problems introduced chapter 
problems growing interest analyses simulations linear dynamic systems 
outline dissertation provided highlights goals addressing issues 
notation utilized dissertation summarized 
motivating trends surprisingly large variety physical phenomena modeled linear lti dynamic systems 
advantages approach include relative ease initial model development eventual mathematical treatment achieved 
models frequently acquired discretizations common finite difference finite element approaches 
range techniques backward euler method multistep methods exists solving ordinary differential equations ode describe system 
stable understood numerical linear algebra algorithms reduction schur form orthogonal transformations dominate low level mathematical operations 
combined various fashions techniques enable robust analysis control simulation large class physical applications 
trends suggest need novel iterative approaches treating lti dynamic systems particularly respect linear algebra algorithms 
physical models complex due increased system size increased desire detail 
discretizations dimensional behavior common 
sources applications include modeling chip increasingly chip interconnects high speed circuit designs 
simple dimensional extractions resistance capacitance may longer sufficient minimum feature sizes drop clock speeds exceed ghz 
second example large scale systems model north american power grid arising planning problems increasingly power industry 
models tend accurately describe behavior underlying physical system complexity leads high analysis simulation costs traditional numerical techniques electrical engineering 
dense numerical linear algebra techniques computationally feasible limited number variables rough order hundreds time writing 
popular orthogonal transformation approaches control eigenvalue computations typically grow cost quadratically memory 
direct sparse factorizations simulation may impractical due highly variable step size sparsity pattern 
growth problem complexity certainly limited applications electrical engineering 
surprising second trend proliferation iterative algorithms particular krylov iterative algorithms appeared numerical linear algebra 
suited sparse structured problems iterative methods frequently lead approximate numerical solutions low computational effort 
part existing krylov research focuses fixed problems 
example numerous implementations exist solving fully specified system linear equations finding eigenvalue closest fixed point 
unfortunately approaches extend frequency dependent time dependent problems limited fashion 
regards dynamic systems may desired find system response range frequencies check unstable eigenvalues entire right half plane 
time dependent frequency dependent variables presents exciting relatively unexplored challenge krylov approaches 
reasons dissertation explores extension development iterative implementations krylov projection analyses approximations lti dynamic systems 
certainly endeavor area see historical survey chapter believed represents comprehensive treatment topic date 
simply adapting existing iteration dynamic systems concentrate deriving theoretical framework utilized develop complete spectrum novel powerful krylov methods dynamic systems 
problem overview dissertation emphasizes approximate solution techniques dynamic system problems involving matrix pencil se gamma 
matrices ir thetan ir thetan assumed large sparse typically contain lumped parameters largescale system 
scalar denotes complex frequency 
matrix pencil arises problems linear system theory 
example finding poles dynamic system entails computing generalized eigenvalues finding values vectors theta gamma computing frequency response single input single output siso dynamic system requires solution dual systems shifted linear equations se gamma se gamma imaginary values model reduction finding low order approximation original dynamic system problem involving se gamma see chapter detailed discussion problem 
dissertation focuses model reduction increasingly important problem right studied context iterative methods encompasses facets problems 
traditional techniques problems involving se gamma transform upper triangular form 
generalized schur decomposition example determines orthogonal tq tz upper triangular 
eigenvalues upper triangular pencil appear immediately 
solving systems equations upper hessenberg form dominated backwards substitutions 
pencil rapidly treated frequencies initial transformation 
cost initial transformation prohibitive large scale problems 
turn iterative versions projection methods 
family methods interested iteratively acquiring low order approximation matrix pencil denoted gamma 
low order approximation treated conventional algorithms yield estimates certain eigenvalues approximate simulated responses various inputs low order controllers original large scale system 
part dissertation concentrates efficiently determining accurate low order approximation dynamic system 
examples efforts study insertion approximation design simulation 
iterative construction low order approximation depends combination procedures known preconditioning projection see section formal definitions terms 
roughly speaking context problems preconditioning speeds approximation convergence chosen frequency regions 
frequency dependent problems preconditioning entails exact evaluation original system discrete points points denoted interpolation points 
second component projection thought forming approximation extrapolation limited exact information regions computing meaningful preconditioners projectors leads problems underlying model reduction 
acquiring accurate low order approximation requires choosing appropriate interpolation points acquiring sufficient excessive exact information points 
computing exact information original large scale pencil significant constraint 
additionally measure quality low order approximation required 
formal treatment subproblems constitutes successful approach iteratively approximating dynamic systems bulk dissertation 
dissertation goals treatment krylov projection methods dynamic systems strives comprehensive 
solid statement problems examined rigorous development theory spectrum iterative methods range tools implementing iterative methods sought 
novel techniques developed dissertation comprehensive treatment main difference existing 
solution approach direct transfer arbitrary existing iterative method associated constraints problem hand 
theoretical understanding complement intuition developed hopefully provide bigger picture 
framework existing methods avenues entirely new approaches result 
multiple levels sophistication balance solution accuracy computational effort large variety problems 
cornerstone dissertation collection new theory relates model reduction topics krylov projection multiple interpolation points 
core theory novel algorithms model reduction proposed 
algorithms related ability approximate information multiple frequencies 
convergence costs differ 
algorithm denoted dual rational arnoldi sided technique constructs orthogonal bases unions krylov subspaces 
utilized krylov subspaces method adaptations seen eigenvalue technique 
due emphasis orthogonalization method extremely robust important contribution krylov model reduction 
robustness cheap 
second algorithm denoted rational lanczos proposed constructs biorthogonal bases unions krylov subspaces sided fashion 
rational lanczos requires short recursions cost competitive existing methods lanczos algorithm 
approach particularly suited parallelism perturbations constructed subspaces 
third algorithm denoted rational power krylov method developed 
rational power krylov method implemented sided approach constructs union krylov subspaces sort orthogonalization 
rigorously understood slightly slower converge novel third algorithm low cost highly parallel amenable approximations constructed krylov subspaces 
performance algorithms compared examples 
trade offs reliability speed exist may affected properties dynamic system availability computing resources 
independent precise algorithm host supporting tools developed chapters aid implementation complete model reduction package 
techniques choosing interpolation points matching frequencies estimating modeling error insuring model stability treating multiple input multiple output mimo systems implementing parallelism avoiding need exact factors pencil gamma se considered 
conclude section summaries material remaining chapters 
ffl background 
chapter describes motivates model reduction problem dynamic system approximation problem considered dissertation 
important tools projection preconditioning explained context dynamic systems 
needed survey existing literature existing solution approaches provided 
ffl projection framework 
chapter provides clear framework understanding existing krylov modeling methods 
sufficient conditions krylov projection documented order achieve model reduction rational interpolation 
treatment unstable mimo models considered projection framework 
ffl projection method implementations 
chapter utilizes projection framework complete spectrum iterative algorithms achieve rational interpolation 
algorithms analyzed efficiency numerical stability 
rational lanczos algorithm fast low memory iterative algorithm implementing rational interpolation derived 
ffl error analysis 
chapter develops different schemes approximating error reduced order model 
performance schemes monitoring modeling algorithms analyzed experimentally verified 
ffl interpolation point analysis 
chapter provides theoretical understanding impact interpolation point placement usage quality model 
suggestions utilizing error analysis methods chapter adapt interpolation points specific problem qualities 
performance various interpolation strategies experimentally verified 
welldefined multipoint interpolation schemes demonstrated robustly handle various situations 
ffl parallelism 
chapter studies parallelism speeding construction reduced order models 
parallelism utilized conjunction multiple interpolation points 
version rational power method proposed avoids large scale communications processors 
interpolation point scheme chapter utilized balance processors 
ffl approximate solves 
chapter allows inexact rational interpolation relaxing need precise factorizations large scale matrix pencils 
reduction involved sought significant drops accuracy 
approximation techniques solving linear systems equations connections drawn techniques model reduction process 
examples provided illustrate possibilities approximate solves 
ffl eigenvalue problems 
chapter surveys preconditioned iterative eigenvalue solvers relates proposed model reduction techniques 
providing initial insight iterative model reduction techniques dissertation existing iterative eigenvalue methods active area research 
primary aim chapter links eigenvalue model reduction techniques exploited 
notation section summarizes notation chapters 
selected symbols attempt balance notation areas system theory numerical linear algebra 
common matrix definitions operations summarized table respect generic matrix generic vector commonly abbreviations dissertation summarized table 
notation nearly letter greek standard alphabets summarized table 
relatively concrete table exact definition symbol taken context surrounding text 
general rule uppercase letters matrices lowercase letters vectors functions lowercase greek letters scalars calligraphic letters subspaces 
exceptions rules attempt match standard practice 
particular letters correspond indices 
regard functions laplace transform time dependent function indicated bold roman italicized type transforms 
explicit dependency functions dropped meaning obvious may simply denoted confusion avoided 
table matrix notation ir thetaj thetaj sets real complex matrices size th column matrix columns matrix transpose complex conjugate spectrum colsp fgg column space span fg span vectors table abbreviations awe asymptotic waveform evaluation cd compact disc complex frequency hopping ds dynamic system gmres generalized minimum residual method lti linear time invariant mimo multiple input multiple output mna modified nodal admittance partial element equivalent circuit pies preconditioned iterative eigenvalue solvers qmr quasi minimal residual method ra rational arnoldi rc resistor capacitor rl rational lanczos rk rational krylov rp rational power siso single input single output svd singular value decomposition table character notation uppercase letters left pencil matrix initial system dimension right matrix order magnitude left matrix preconditioner matrix feed matrix projection matrix right pencil matrix residual matrix generic matrix structured matrix transfer matrix transformation matrix identity matrix projection matrices upper bounds solution matrix reduced order model size projection matrix lowercase letters right vector projection vector left vector residual vector feed term complex frequency exponential time function input generic vector projection vector transfer function solution vector column identity output indices projection vector greek letters ff fl matrix elements eigenvalue ffi perturbation moment ffl error evaluation point ae residue generic scalar oe interpolation point imaginary gamma oe polynomial coefficients condition number real frequency calligraphic letters hardy norm search subspace krylov subspace constraint subspace chapter krylov model reduction primary problem interest dissertation model reduction efficiently computing accurate low order approximation dynamic system 
techniques model reduction retain certain invariant features original system briefly surveyed chapter 
solution approaches utilized chapters retain moments original system yield reduced order model known rational interpolant 
preconditioning krylov projection tools fundamental chapter computing rational interpolants explained detail 
chapter concludes history overview existing krylov projection methods model reduction 
problem statement dissertation primarily devoted computing low order approximations linear dynamic systems 
assumed original system described generalized state space equations ax bu du vector ir theta vector state variables ir theta input vector system ir theta output vector system 
simplicity assumed section system single input single output input output scalar functions time 
case nearly large scale problems assumed system matrix ir thetan descriptor matrix ir thetan large sparse structured 
reduced order approximation takes corresponding form ax bu du dimension reduced order model designated output approximates true output 
general simple relation exists state vector 
instance tenth element need directly related tenth element 
generalized state space expressions merely possible representation linear dynamic systems 
second important representation transfer function system 
resulting laplace transform transfer function original system se gamma gamma represents complex frequency 
loss generality feed term original model assumed zero feed term simply original system needs treatment model reduction 
function maps laplace transform input laplace transform output 
transfer function reduced order model defined manner similar 
preliminaries way state prime problem considered large scale lti dynamic system rapidly compute accurate reduced order model 
clearly statement vague different solution approaches possible 
fact different model reduction techniques exist literature see section 
paragraphs formalize problem statement various model reduction methods compared 
key terms problem statement reduced order rapidly accurate 
hoped dimension reduced order model significantly original model analyzed simulated relative ease conventional techniques 
course reduction expensive 
cost generating reduced order model comparable directly analyzing little saved working low order approximation 
desires reduced order model reasonably accurate approximation original model 
behavior original model interest uses reduced order model place reduced order system match original sense 
conditions accuracy speed order conflicting goals 
typically expects example accuracy model increases larger order measures accuracy reduced order model possible 
formally tends interest difference actual low order outputs gamma set inputs 
difference characterized system norm 
popular error norm example defined kh gamma max ku ky gamma ku sup 
gamma 
gamma time domain norm measures worst ratio output error energy input energy 
equivalently frequency domain norm represents largest magnitude frequency response error 
weighting norm emphasize error due specific input class interest 
second measure accuracy approximation assess properties original model retained reduced order 
properties interest said invariant independent respect similarity transform 
retaining certain original properties system reduced order model hopes resulting approximation error small 
course error depends selection pertinence retained invariant properties 
leaving problem statement note model reduction connected generalized eigenvalue problem problem shifted systems linear equations 
chapter explains accuracy reduced order model partially connected quality pole eigenvalue approximations 
eigenvalue problem important areas system analysis noteworthy stability problem 
variety approaches exists computing nontrivial solutions eigenvalues eigenvectors region see example 
variations approaches appear model reduction techniques proposed dissertation 
survey pertinent eigenvalue techniques connections model reduction algorithms provided section 
shifted systems linear equations arise writing transfer function se gamma gamma se gamma se gamma gamma se gamma solutions dual system shifted linear equations 
demonstrated section model reduction approaches taken dissertation phrased terms finding approximate solutions values model reduction problem ability efficiently solve shifted systems equations desirable certain ode solvers 
techniques exist iteratively solving shifted systems equations single krylov subspace 
methods restricted case limited choices preconditioner 
suitable preconditioners various regions shifted problem considered symmetric diagonally dominant 
evaluates treating frequency independently effort share information solves multiple points 
solution techniques model reduction methods retention invariant properties 
common choice invariant properties called modal properties system 
modal properties system poles eigenvalues residues ae arise partial fraction expansion frequency response ae gamma assumed simplicity poles system unique 
modal components summation contributes quantity ae zero state impulse response original system 
reduced order model matches approximately matches specific modal components original model retains certain time dependent features original system response 
potentially iterative eigenvalue techniques find specific components modal retention approach feasible large scale problems 
drawbacks model reduction 
difficult identify priori modes truly dominant modal components original system 
response system depends interaction poles residues locating poles near imaginary axis may sufficient 
alternative invariant properties may retained model reduction hankel singular values 
hankel singular values related controllability observability properties system 
constructing reduced order model retain largest hankel singular values known balanced truncation 
balanced truncation possesses desirable feature norm modeling error bounded sum hankel singular values retained reduced order model 
unfortunately implementing balanced truncation involves solution lyapunov equations cost operations 
invariant properties importance coefficients power series expansion 
solution techniques proposed determine reduced order model accurately matches leading coefficients arising chosen power series 
expansion infinity takes form gamma gamma gamma gamma gamma gamma coefficients known markov parameters case shown satisfy gammaj gamma gamma gamma making neumann expansion gamma jg gamma jg markov parameters values zero state impulse response subsequent derivatives impulse response 
reduced order model markov parameters gammaj equal gammaj known partial realization 
partial realization emphasizes behavior model may dominated extremely rapidly decaying dynamics system 
extensions partial realizations accurately reproduce behavior time apparent 
reason power series expansion typically favored literature assuming loss generality feed term absent coefficients referred moments expansion shown satisfy gamma gammac gamma gamma gamma 
moments value subsequent derivatives transfer function evaluated 
reduced order model moments gamma gammac gamma gamma gamma equal gamma known pad approximant 
replacing expansion shifted variable gamma oe gamma oe gamma gamma led shifted moments gamma gammac gamma oee gamma gamma gamma oee gamma shifted moments value subsequent derivatives user specified interpolation point oe 
reduced order model typically matches moments oe free parameters available numerator denominator 
single interpolation point may interested reduced order model interpolates frequency response derivatives multiple points 
possible interpolation points foe oe oe differentiated superscripts 
moments matched oe moments matched oe jk model meeting constraints denoted multipoint pad approximation rational interpolant 
varying location number interpolation points utilized underlying problem mind construct accurate reduced order models variety situations 
quick various moments matched summarized table 
case moments computed matrix vector multiplies matrix inversions solving systems linear equations involving relative simplicity required operations favors moment matching sparse large scale problems 
table moment choices model reduction approximation power series th names expansion coefficient partial realization gammaj gammaj gamma gamma gamma pad pad gamma gamma gammac gamma gamma gamma shifted pad gamma gamma oe gamma gammac gamma oee gamma gamma gamma oee gamma rational interpolant gamma gamma oe gamma gammac gamma oe gamma gamma gamma oe gamma multipoint pad example understand various moment matching possibilities conclude examination different th order models th order siso system 
original system describes dynamics lens actuator radial arm position portable compact disc player 
frequency response corresponding system shown solid line figures 
frequency responses partial realization dotted line pad approximation dashed line 
frequency responses shifted pad approximation dashed line oe frequency true response partial realization approximation partial realization pad approximation example frequency true response shifted approximation rational interpolant shifted pad approximation rational interpolation example rational interpolation dotted line oe oe oe shown 
note partial realization captures higher frequency behavior accuracy single point pad approximations directly related choice oe 
best results acquired general rational interpolant frequency response rational interpolant nearly indistinguishable original high frequencies 
example clearly exhibits behaviors expected moment matching methods 
particular rational interpolation matches information range frequencies 
mean larger number interpolation points necessary wise 
rational interpolation requires inversion triangular factorization gamma oe interpolation points see table 
extra fixed costs involved going multiple interpolation points costs may offset resulting improvements convergence 
balancing number interpolation points placement points order crucial efficient calculation accurate low order model 
implementation techniques previous section defined model reduction techniques including family moment matching methods 
implementing algorithms yield desired reduced order models remains 
moment matching fact different avenues implementation 
path explicit approach section 
dissertation concentrates utilization projection preconditioning proposed implementations 
obtainable benefits path include numerical stability opportunity iterative implementations 
projection primary tool chapters projection 
projection extracts approximate solution dimension search subspace order precisely defined approximation chosen constraints satisfied 
subspace associated constraints 
example typically require approximate solution chosen residual orthogonal specified constraints known petrov galerkin conditions 
equals projection orthogonal projection said oblique 
detailed review projection technique refer 
subspaces represented rectangular matrices ir thetam ir thetam columns form bases respective subspaces 
important note infinitely columns acceptable bases necessary know satisfy colsp fv colsp fzg fact knowledge sufficient theory possible choices bases lead identical results similarity transformation 
chapter analyze specific choices light issues numerical efficiency accuracy 
terms linear dynamic systems projection technique associated transform truncate operations 
nonsingular left right transformation matrices ir thetan ir thetan partitioned applied original model expressed ev ev ev ev av av av av du reduced order model determined retaining leading subsystem transformed original system 
transformation truncation operations denoted model reduction projection lead reduced order model components av ev quantities said restrictions original system matrices concept model reduction projection connected formal concept projection 
analogous transfer function model written gamma gamma gamma gamma gamma defining solutions dual reduced order shifted systems equations gamma gamma transfer function written se gamma comparing sees transfer functions original models differ approximates fact approximations satisfy petrov galerkin conditions chosen model reduction projection computes approximations computed projection petrov galerkin constraints 
approximate solution vector lies colsp fv residual gamma se gamma orthogonal colsp fzg gamma se gamma gamma gamma column spaces play reversed role acquiring approximate solution approximate solution vector lies colsp fzg residual gamma se gamma orthogonal colsp fv connection formal definition projection method model reduction projection 
popular model reduction methods utilize projection 
balanced truncation chosen correspond called hankel singular vectors associated largest hankel singular values system 
modal model reduction methods chosen correspond left right eigenvectors pencil relative ordering eigenvalues 
reduced order model retains exactly modal components original system 
rational interpolation method choice dissertation phrased terms projection 
section proves detail rational interpolation achieved column spaces span unions krylov subspaces 
th dimensional krylov subspace corresponding matrix vector denoted defined span gg gamma basis krylov subspace quickly computed rapidly applied due sparsity 
fact gives krylov model reduction potential cost savings 
specifying krylov subspace desirable best reduced order model remains 
extremely simple particularly effective choices colsp fv km gamma se span gamma se gamma se gamma colsp fz km gamma se span gamma se gamma se gamma time overlook dependence subspaces think fixing value 
content individual krylov subspaces classic choices approximately solving dual systems equations 
example biconjugate gradient method known iterative krylov solver lead applied fixed 
second motivation form consistent model reduction history simplicity 
model reduction emphasizes simple sparse matrix vector products 
structure krylov subspaces consistent dual input output structure lti dynamic system 
identity matrix subspaces closely related controllability observability spaces dynamic lti system 
chapters motivate evaluate alternative choices krylov subspaces composing cost approximation quality ability handle frequency dependence problem significant concerns addressed forms preconditioning preconditioning important partner projection iterative solution techniques 
fact frequently credited important component efficiently computing accurate solution 
broad terms goal preconditioning generate better projection subspaces yield faster model convergence drastically complicating construction krylov subspaces 
preconditioner ir thetan traditionally introduced fixed left right transformation 
solving problem ax example consider left transformed problem pax exactly equals gamma solution trivially general matrix transforms original problem description hopefully easier iteratively solve 
frequency dependent problems approaches similar traditional preconditioning 
example transform matrix pencil gamma se gamma se 
fixed approximate gamma se gamma general 
may helpful certain frequency ranges 
emphasize difference traditional fixed case term dynamic system ds preconditioner describe matrix left transformations utilized ds preconditioned system pe pax du stressed describe system 
generalized eigenvalues pa pe identical 
starting new description utilizing mapping seen going consistent define new reduced order model pav du matrix ir thetam satisfies colsp fv km gamma se matrix ir thetam satisfies colsp fw km gamma se describe original system describes system different previously constructed 
presence modifications projection matrices lead new reduced order model 
particular definition changed section 
model reduction ds preconditioned dynamic systems generally derived fashion existing literature 
matrices explicitly computed applied transformed version original dynamic system 
possible obtain new reduced order model directly original description 
defines new reduced order model written desired form 
associating new projection subspaces new reduced order models possible 
obtain reduced order model choose colsp fv span gamma se fp gamma se gamma colsp fz span gamma se fp gamma se gamma apply matrices 
ds preconditioning concisely defined context matrix takes old choices 
definition ds preconditioning equivalent left transformation followed application 
favor point view 
focusing avoids need left transformations original description 
additional benefits point view apparent section 
clearly provides additional freedoms specifying contents constraint solution subspaces 
section shows chosen acquire reduced order model matches moments single interpolation point 
choice ds preconditioner determines frequency response original reduced order systems agrees 
noteworthy class ds preconditioners frequency dependent problems called exact ds preconditioners 
exact ds preconditioner exact inverse matrix pencil gamma oee gamma fixed scalar oe 
see chapter exact ds preconditioners required rational interpolation achieved 
important property exact ds preconditioning lemma 
proof lemmas dissertation may appendix lemma value oe gamma oee gamma gamma se oe gamma gamma oee gamma applying gamma oee gamma pencil gamma se leads simpler transformed pencil consists scaled matrix pe shifted identity matrix 
scalings shifts identity matrix important krylov subspaces 
lemma krylov subspace shift invariance matrix vector nonzero scalar jg combining lemmas leads equivalences gamma oee gamma gamma se gamma oee gamma gamma oee gamma gamma oee gamma gamma oee gammat gamma se gamma oee gammat gamma oee gammat gamma oee gammat exact ds preconditioner utilized preconditioned projection subspaces equivalent frequency independent subspaces right sides 
exact ds preconditioning causes invariant respect fact important frequency dependent generally lead lti reduced order models 
exact preconditioning utilized option fix value specifying frequency dependent tends favor accurate solution frequencies 
issue discussed section 
ds preconditioning especially exact ds preconditioning significantly improve accuracy reduced order model 
significant limitations preconditioning frequency dependent problem 
ds preconditioners significantly complicate computation multiplying sparse pencil gamma se step gamma se 
exact ds preconditioner inverse gamma oee appears generation solving large scale systems linear equations implicitly enact inverse may costly 
second may wonder choose ds preconditioners 
petrov galerkin constraints insure reduced order model converges steps able specify ds preconditioners achieve significantly faster results 
poorly chosen ds preconditioners hardly better ds preconditioners 
cost choice ds preconditioners addressed subsequent chapters 
existing approaches number papers proposing exploring utilizing krylov projection model reduction approaching 
section hopefully complete history related works 
subsections typical examples methods utilized existing efforts 
certainly claimed subsections precisely capture variations literature 
history methods forming foundation relatively old 
history pad approximation example spans years 
algorithm lanczos important krylov iteration nearing anniversary 
evident dissertation understanding application concepts certainly closed topic 
large number moment matching methods particularly early ones form reduced order model explicit knowledge desired moments original system see example section 
explicit methods utilized construct pad approximants area control early 
extensions techniques multiple interpolation points followed 
interest circa class explicit moment matching methods known asymptotic waveform evaluation awe 
awe methods vary little basic concept earlier control implementations awe techniques applied interconnect model reduction area circuits 
methods received attention ability reduce rc interconnect models involving tens thousands variables 
multipoint version awe complex frequency hopping available 
unfortunately explicit moment matching methods known exhibit numerical instabilities particularly dimension reduced order model grows 
source difficulties pointed independent 
efforts point moment matching lanczos method generally bi orthogonalized krylov projection preferred numerical implementation 
significant mathematical connection lanczos algorithm krylov technique model reduction occurred early 
shown partial realizations generated lanczos algorithm 
adaptations krylov subspaces proposed generate pad approximations shifted pad approximations 
mathematical connections lanczos method utilized model reduction application areas 
areas chronologically apparently structural dynamics 
prior knowledge moment matching connections lanczos method utilized structural dynamics model reduction eigenvalue analysis 
field utilized lanczos method pad approximation including mimo systems 
wave application took place control literature 
large amount existing repeated new results appear areas error analysis stability retention 
lanczos model reduction popular topic area high speed circuits 
existing lanczos algorithms applied standard mimo symmetric problems 
new algorithms proposed stability retention 
application areas approaches remained closely tied classical lanczos algorithm 
approaches emphasize exploit fundamental structure projection techniques rational interpolation 
explicit moment matching explicit moment matching straightforward approach constructing pad approximations 
typically step process 
selected moments original system explicitly computed 
moments frequently leading coefficients power series expansion see column table assorted moment definitions 
second step reduced order frequency response oe gamma gamma oe oe gamma gamma forced correspond selected moments 
numerator parameters oe denominator parameters chosen moments reduced order system equal original system parameter selection requires solution linear systems equations involving hankel matrices 
partial realization problem example solves equation gamma gamma gammam gamma 
gammam gamma 
gammam gammam gamma gamma 
gamma gammam gamma gammam gamma 
gamma determine coefficients 
equation solved oe coefficients 
similar hankel equations arise cases pad shifted pad approximations 
rational interpolation problem equations involving general matrix solved 
cases important note system matrices vectors enter modeling problem moments 
definition moments table enter problem sparse matrix vector multiplies sparse linear system solves 
unfortunately numerical implementations explicit moment matching experience difficulties 
consider serious problems ill conditioned hankel matrices example 
reader referred discussion shortcomings explicit moment matching 
example consider simple order dynamic system defined gamma delta delta delta 


delta delta delta 
identity matrix 
difficult see digits finite precision computed gamma equal gammaj 
moderate values change consecutive moments determined largest eigenvalue finite precision 
information corresponding eigenvalues rapidly lost practice computation higher order moments 
condition number hankel matrix order 
repetitive multiplications fixed matrix longer possible finite precision introduce additional new information reduced order model 
loss information due repetitive multiplications manifests hankel matrices explicit moment matching equations 
regardless number moments matched computed model converges actual 
partial realization example difficulties occur various pad schemes 
practice bounds placed number moments computed expansion point explicit moment matching utilized 
lanczos moment matching due relative numerical elegance reliability nonsymmetric lanczos algorithm popular choice moment matching model reduction methods 
nonsymmetric lanczos method due lanczos originally proposed method solving linear systems equations eigenvalue problems 
focus nonsymmetric matrices nonsymmetric designation lanczos method versus symmetric lanczos method dropped assumed 
algorithm lanczos computes rectangular matrices ir thetam restrict specified matrix tridiagonal form gv ff fi delta delta delta fl ff 


fi delta delta delta fl ff satisfy colsp fv km colsp fwg km vectors user specified starting vectors lie direction columns alternatively equivalently lanczos method viewed approach constructing biorthogonal satisfy krylov subspace conditions 
columns satisfying constraints iteratively computed term recursions fl vm gvm gamma ff vm gamma fi vm gamma fi wm wm gamma ff mwm gamma fl mwm gamma choosing ff fi fl parameters leads tridiagonal gv vice versa 
implementation appropriate parameter selections nonsymmetric lanczos algorithm algorithm 
interested reader referred detailed study nonsymmetric lanczos method 
algorithm nonsymmetric lanczos initialize vm vm fl fl vm wm wm fi fi sign vm fl ff avm vm gvm gamma ff vm gamma fi vm gamma wm wm gamma ff mwm gamma fl mwm gamma actual implementations lanczos method may encounter numerical difficulties including loss biorthogonality called serious breakdowns 
breakdowns drastic rarer breakdowns occurring explicit moment matching 
additionally remedies possible discussed section 
noted section lanczos method utilized realize pad approximants partial realizations 
proof statements follows section 
table summarizes appropriate lanczos input choices order achieve various reduced order models 
constructed lanczos method lead reduced order model 
differences reduced order models table due entirely choice exact ds preconditioners partial realizations utilize gamma pad approximations involve gamma shifted pad approximants utilize gamma oee gamma important observe reduced order models table restricted moment matching single interpolation point 
lanczos method generate rational interpolant 
table modeling choices lanczos algorithm model type lanczos quantities model quantities partial realize gamma gamma gammat pad gamma gamma gammat shifted pad gamma oee gamma gamma oee gamma gamma oee gammat oes table defines terms lanczos matrix model generated 
practice reduced order system matrices directly generated tridiagonal matrix columns table 
case partial realization example equals av gamma av gv equals ev constructing reduced order model explicit projection 
lanczos method implicitly constructs reduced order model assumed biorthogonality validity assumption studied section 
reader note involvement inverses combinations formation various moment matching models 
particular problem model may unrealizable due singularities 
possible inclusion inverses lanczos method standard 
traditionally lanczos algorithm assumes matrix vector products easily accessible matrices 
modeling approaches table inverses treated implemented solving systems linear equations 
inverses fact examples exact ds preconditioner discussed section 
mentioned lanczos method avoids difficulties encountered explicit moment matching 
storing modeling information biorthogonal matrices moments 
recall explicit moment matching eventually fails certain direction corresponding largest eigenvalue quickly dominates generated moments 
hand biorthogonality condition lanczos insures new information introduced projector step 
directions vm wm theoretically kept orthogonal wm vm dominate new information 
unfortunately lanczos method maintain precise biorthogonality limited numerical precision 
convergence reduced order model practice slightly 
frequently discuss role biorthogonality orthogonality model reduction see sections 
chapter projection framework rational interpolation chapter set sufficient conditions projection technique derived guarantee rational interpolation 
conditions intuitively meaningful points view 
projection approach far general unraveling preselected iterative implementation 
frequently emphasized existing literature projection approach provides general framework deriving contrasting new existing implementations 
validity projection framework corresponding algorithms depend certain assumptions covered detail 
extension projection techniques cases stable mimo systems requires receives special attention 
rational interpolation theory mild assumptions dual conditions projection matrices sufficient produce rational interpolant reduced order model 
concise summary conditions pertinent intuition section 
section formally connects krylov subspace projection rational interpolation 
statements arguments working goal reminiscent 
analysis rational interpolation projector point view apparently completely general allow arbitrary number interpolation points arbitrary number values derivatives interpolation point direct projection involving restrictions placed bi orthogonality assumed section large scale matrix pencil gamma oee reduced order pencil gamma oe nonsingular oe interpolation point 
assumption gamma oee hold moments corresponding oe exist 
gamma oe considered detail section 
sufficient know assumptions typically hold 
connection rational interpolation projection begins simple property oblique projection 
lemma shows conditions projection applied direction changing 
behavior generalized subsequent lemmas model reduction 
desires applied original system modify moments retained 
proof lemma lemmas dissertation may appendix lemma colsp fv ir thetam biorthogonal 
moments original system model essentially sided containing input output direction 
proceed fundamental lemma corresponding directions combine desired result 
lemma gamma oee gamma gamma oee gamma colsp fv gamma oee gamma gamma gamma oee gamma gamma oe gamma gamma gamma oe gamma lemma jc gamma oee gammat gamma oee gammat colsp fzg gamma oee gamma gamma oee gamma gamma gamma oe gamma gamma oe gamma gamma theorem gamma oe gamma gamma oe gamma colsp fv jc gamma oe gammat gamma oe gammat colsp fzg moments satisfy gammac gamma oe gamma gamma gamma oe gamma gammac gamma oe gamma gamma gamma oe gamma proof develop case case follows trivially lemma 
case nonnegative integers satisfy gamma oe gamma gamma gamma oe gamma gamma oe gamma gamma oe gamma jc gamma gamma oe gamma gamma gamma oe gamma lemmas expression equivalent gamma oe gamma gamma oe gamma jc gamma ev gamma oe gamma gamma gamma oe gamma may simplified gamma oe gamma gamma gamma oe gamma quantity corresponding moment reduced order model 
relations hold value contain krylov subspaces corresponding range 
interpretations theory acquire rational interpolant matches moments interpolation points oe propose selecting 
assuming pencils gammaoe points see section theorem guarantees desired rational interpolant acquired 
stress pair projection bases satisfying sufficient achieve desired rational interpolant 
restrictions biorthogonality orthogonality purely implementation specific choices 
question special forms 
particular forms suggest family projection approaches computing reduced order model 
clear connections desired moments original system gamma oe gamma gamma oe gamma gamma oe gamma gamma oe gamma obvious correlation exists repetitive multiplication gamma oe gamma krylov spaces moments 
sum dimensions krylov subspaces corresponding oe exactly equal number moments matched oe reduced order model 
direct connection pair krylov subspaces values derivatives matched frequency suggests great deal potential parallelism forming reduced order model 
independent rest basis gamma oe gamma gamma oe gamma pertinent moments oe matching moments multiple points requires multiple krylov subspaces 
constructing multiple subspaces parallel addressed chapter 
addition moment matching demonstrated section proposed model reduction approach phrased terms solving dual system equations 
recall reduced order frequency response written se gamma zx approximate solutions sense 
rational interpolation connected approximate solutions concept varying ds preconditioners 
understanding meant varying ds preconditioner follows lemmas obtain equality gamma oe gamma gamma oe gamma gamma oe gamma gamma se gamma oe gamma dual result obtained subspace left 
subspace right simply union preconditioned krylov subspaces type 
fact subspaces making union vary value matrix gamma oe gamma denote exact varying ds preconditioner choices leading rational interpolation simply combine exactly preconditioned krylov subspaces 
introducing multiple ds preconditioners hopes obtain accurate solutions dual equations neighborhoods surrounding interpolation points oe impact various ds preconditioners various choices oe considered chapter 
multiple varying preconditioners appeared literature solving fixed systems linear equations 
similarities seen algorithms model reduction algorithms developed chapter 
problem frequency dependent adapting multiple preconditioners cover range frequencies novel 
may question advantages phrasing rational interpolation problem terms shifted systems linear equations varying ds preconditioners 
strength new point view apparent relaxes constraint equals gamma oe gamma iterative method converged working precision 
avoiding need exact inverses matrix pencil may significantly cut costs involved generating unfortunately concept rational interpolation exact inverses inexact ds preconditioners exactly match moments 
hand inexact ds preconditioners accepted fact standard treating systems linear equations 
array inexact ds preconditioning techniques iterative solvers considered solving dual systems linear equations 
approximations gamma oe gamma model reduction considered chapter 
finish section noting matrices theorem computed existing krylov model reduction methods 
existing literature consistently employs matrices seen sections columns matrices satisfy simple relation gamma oe gammat wm value oe theoretically approach rational interpolation problem terms computing appropriate choice naturally suited lanczos implementations krylov projection 
decided choice yields clearer presentation 
column spaces exact duals 
reduced order model follows trivially 
lastly appearance varying ds preconditioners straightforward 
choice fixed left transformation associated longer clear ds preconditioners vary 
limits theory connections developed section rational interpolation projection depend assumed gammaoe singularities occur matrices gamma oe singular 
individual components nonsingular gamma oe may ill conditioned 
section explores various sources remedies singular gamma oe regards notation recall matrix zm consists columns ir thetam likewise vm columns columns form bases arbitrarily chosen dimensional subspaces contained respectively 
define reduced order model size simply replacing vm zm note vm zm singular large scale pencil matrix inverses form gamma oee gamma dominant presence construction krylov subspaces colsp fv colsp fzg 
oe generalized eigenvalue inverse exist gamma oee singular 
eigenvalue original system chosen interpolation point 
restriction insignificant constraint finite number discrete points continuous plane need avoided 
may wonder interpolation points neighborhood discrete singularity points 
interpolation point nears eigenvalue conditioning gamma oee worsens 
fortunately surprisingly poorly conditioned gamma oee lead catastrophic results attempting match information oe 
conditioning issue examined respect related concerns method inverse iteration eigenvalue problems 
quoting period inverse iteration considered notable exaggerated fears concerning instability direct methods solving linear systems ill conditioned systems source particular anxiety 
reason widely held oe accurate thought eigenvalue little resulting matrix gamma oei ill conditioned 
generally recognized necessary recommended 
ill conditioned gamma oee utilized accurate interpolation oe due form error gamma oee gamma short reader referred rigorous analysis error dominated eigenvector oe approaches corresponding eigenvalue eigenvector direction desired solution 
desired vector gamma oee gamma computed vector ffl gamma oee gamma ffl may large 
scaling error unimportant projection technique requires accurate subspace basis acquire desired reduced order model 
scaling basis vector ffl perturb resulting subspace 
existing theory practical experience show accurate interpolation occur frequency oe gamma oee ill conditioned care taken matching higher order moments oe 
directions corresponding oe significantly damped gamma oee gamma multiplication 
expect loss precision frequencies away oe directly proportional number digits shared oe acquiring information away better achieved moving different interpolation point 
ill conditioned gamma oee fact rare concern applications practical reason 
original lti system dynamically stable eigenvalues obviously restricted left half plane 
see chapter positive real purely imaginary interpolation points preferred model reduction 
unstable lightly damped modes occur dynamic system ill conditioned gamma oee occur 
rank deficient projection matrices full rank leads singular gamma oe fact case leads singular gamma multiple sources rank deficient theoretically loss full rank corresponds occurrence invariant subspace 
example consider letting go fixed interpolation point 
column space krylov subspace kn gamma oee gamma gamma oee gamma subspace fact controllability subspace eigenvectors gamma oee gamma shown identical 
original system completely controllable dimension controllability subspace column rank course fact full rank case really disturbing 
original system implies existence completely accurate reduced order model order known minimal realization 
need size context lanczos method loss rank due computation invariant subspace aptly termed fortuitous breakdown 
alarming theoretical possibility invariant subspace rank loss finite machine precision 
seen section finite precision lead problems naive implementations moment matching 
desired constraint search subspaces may theoretically acceptable computed may possess full rank 
similar section avoid forming vm example simply multiplying vm gamma oe gamma explicitly constructed columns quickly dependent finite precision 
fashion inverse iteration repeated multiplications matrix gamma oe gamma emphasize single example 
explicitly constructing columns common construct vm force orthogonal biorthogonal previous directions vm zm columns may handled similarly 
manner new columns kept independent old 
sure new information added size grows 
stressed placement orthogonality biorthogonality type constraints purely implementational decision 
various biorthogonality orthogonality possibilities explored table chapter 
orthogonalization choices way fundamental model reduction projection 
orthogonality biorthogonality possible tool avoiding rank deficient finite precision 
choice orthogonality biorthogonality variety numerical approaches exist enforcement 
order increasing numerical robustness techniques include selective classical gram schmidt classical gram schmidt modified gram schmidt classical gram schmidt reorthogonalization householder reflectors 
version classic gram schmidt common choice 
vector orthogonalized orthogonal matrix gm classical computes gamma component summation column gm cases certain past directions columns gm known orthogonal due structure problem 
lanczos type methods example knows priori zero theory gamma 
utilizing knowledge avoiding computation zero terms leads simple updates short recursions 
round error perturbs away zero practice 
short term recursions accurate explicitly computing terms summation classical gram schmidt 
computing terms classical gram schmidt 
computing certain terms summation assuming zero known selective orthogonalization 
robust classic gram schmidt passes gram schmidt known reorthogonalization 
approach computes sets computes left hand side 
singular reduced order pencil matrices gamma oe nonsingular gamma oe may ill conditioned 
final subsection explores situation detail assumes gamma oe nonsingular 
key insight situation stated proven theorem 
singular gamma oe arises forming order model matches moments oe exists reduced order model size matches gamma moments oe theorem consider gamma oe singular gamma oe gamma gamma oe gamma colsp fv jc gamma oe gammat gamma oe gammat colsp fzg vm gamma zm gamma ir theta gamma full rank matrices satisfying conditions colsp vm gamma ae colsp fv colsp zm gamma ae colsp fzg gamma oe gamma gamma gamma oe gamma colsp vm gamma gamma oe gammat jc gamma gamma oe gammat colsp zm gamma gamma gamma oe vm gamma singular gamma st moment dimension gamma reduced order model gamma vm gamma gamma vm gamma gamma vm gamma du oe equals gamma st moment original system oe proof due conditions vectors vm gamma vm gamma gamma gamma oe vm gamma gamma gamma gamma oe gamma zm gamma gamma gamma oe vm gamma gammat gamma gamma oe form completed matrices vm gamma vm zm gamma satisfying colsp fv colsp colsp fzg colsp inclusion classical gram schmidt relation gamma gamma oe vm gamma vm gamma gamma oe vm gamma ff value ff gammaoe zero due assumed singularity gamma oe gamma gamma oe ff written gamma oe gamma gamma oe vm gamma gamma gamma oe vm gamma gamma gamma gamma oe expression simplifies gamma oe gamma jc gamma gamma oe gamma gammac gamma oe gamma jc gamma vm gamma gamma gamma oe vm gamma gamma gamma fe gamma oe gamma gamma due 
gamma gammaoe vm gamma nonsingular lemmas write expression ff jc gamma gammac vm gamma gamma gamma oe vm gamma gamma gamma vm gamma jc gamma gamma gamma oe vm gamma gamma delta gamma vm gamma gamma gamma oe vm gamma gamma gamma quantity ff difference gamma st moments original reduced order models 
ff known zero 
certainly conditions involved statement theorem 
theorem typical general description singular gamma oe assuming gamma gamma oe vm gamma nonsingular guarantees rank gamma oe gamma 
treating conditions cases detail simply reiterate main concept provide example 
reader simply keep mind singular gamma oe implies existence system order matches gamma desired moments 
singular gamma oe implies existence lesser approximation nearly matching desired moments original system 
behavior illustrated example 
example consider third order system gamma gamma gamma gamma gamma gamma system stable controllable observable 
attempt obtain second order model matches moments oe oe follow theorem choose gamma gamma gamma gammat gamma gammat unfortunately matrix gamma oe singular 
agreement theorem check order model described fz av ev matches second moments original system oe expected moment original system oe expected 
note order model match second moment original system oe presence singular gamma oe fortuitous matching extra moments tied phenomenon 
example reduced order model size implies presence th order approximation matches moments th order model 
reduced order model singular gamma oe case concern generation reduced order models minimal wasted effort 
common scenario singular gamma oe lack th order system meets specified moment constraints 
rational interpolant size meets specified constraints singular gamma oe consistent situation 
example approximation exists matches moments oe order system zeros 
approximation match moment agreeing theorem 
similarly example check reduced order model order equal exists matches specified moments gamma oe gamma oe 
fortunately theorem suggests environment surrounding singular nearly singular gamma oe common practice 
say difficulties arise 
cause singular gamma oe better understood possible characterize various approaches working problem 
remedies modifies size moment constraints reduced order model hopes valid rational interpolant realized 
remedies assumes gamma oe nonsingular 
modified model dimension 
possible remedy simply increase model size gamma oe nonsingular model large meet required constraints 
model size lacking simply skips augments valid reduced order model 
process known look ahead lanczos literature 
unfortunately rarely knows priori required increase model size valid approximation 
usually increase iterations 
approach appropriate fast implementations required implementation may complicated heuristic 
revised moment constraints 
second remedy choose interpolation points avoid difficulties 
fixing number moments matched priori adaptively selects interpolation points model reduction proceeds 
example assume valid model size gamma 
interpolation point utilized moments matched selected avoid singular gamma oe 
theorem intuition tell avoid matching new moments interpolation point oe new moments matched model size gamma 
unfortunately known priori interpolation points inspected valid 
usually number 
approach suited implementations match moments multiple interpolation points 
consistent choosing interpolation points attempt generate accurate reduced order model possible 
avoids choosing interpolation point match data included existing reduced order model 
adaptively choosing interpolation points modeling error considered chapter 
reduced moment constraints 
final approach avoiding singular gamma oe construct reduced order model matches fewer moment constraints 
reduced order model size meet specified moment constraints drop constraints 
known algorithm class arnoldi method 
arnoldi methods guaranteed avoid singular pencils choosing gamma oee gamma long gamma oee nonsingular assumed section nonsingular 
choice value theorem zero arnoldi type approach generally meets moment constraints 
advantages arnoldi type approach immunity certain breakdowns 
disadvantage approach inability match moments reduced order model 
noted approaches matching moments need studied 
issues dissertation focus computation rational interpolant siso system 
system related issues exist 
issues dynamic stability mimo systems considered projection framework 
stable models introduced projection approach matches moments original system different set invariant quantities system eigenvalues regulated 
known eigenvalues known ritz values may lie right half complex plane original system stable 
discussion instabilities partial realizations 
unstable reduced order models stable systems frequently unacceptable approximation simulations 
address stability concerns author proposed techniques discard unstable modes reduced order model 
finite ritz values right half plane correspond true eigenvalues stable system eliminating unstable ritz values reasonable approach 
similar strategies appeared 
explicitly need find order orthogonal left right matrices transform reduced order model form du eigenvalues stable ones initial reduced order model 
leading subsystem retained final reduced order model 
straightforward approach acquiring form discussed 
lanczos algorithm efficient implementation hyperbolic rotations produce operations 
approach known implicitly restarted lanczos algorithm 
implicitly edits lanczos iteration rapidly acquire new lanczos iteration generates purely stable reduced order subsystem 
special conditions normal matrices equals stable reduced order models obtained 
conditions relation field values convex hull exists bounds spectrum 
guaranteed stability partial realizations normal identity matrix equals discussed 
projection involving cholesky factorization utilized acquire guaranteed stable pad approximations symmetric 
certainly results desirable require normal matrices limit choices multiple input multiple output models mimo system rectangular matrices ir thetal ir thetal take place vectors corresponding new input output matrices corresponding reduced order model av ev similarly moments original system matrices gamma oe gamma gamma gamma oe gamma trivially element position th moment siso system input vector th column denoted output vector th column denoted 
corollary theorem follows readily mimo case 
corollary gamma oe gamma gamma oe gamma colsp fv lc jc gamma oe gammat gamma oe gammat colsp fzg moments original reduced order model satisfy gammac gamma oe gamma gamma gamma oe gamma gammac gamma oe gamma gamma gamma oe gamma important points arise corollary 
number scalar elements grows product changes size versus versus linear respect understand difference consider special uncommon case ae 
number subsystems problem mimo version simplifies 
size reduced order model formed guaranteed match moments subsystems 
case possible match moments subsystems projection matrices size problem sided times cost effective concentrate sides 
second point importance mimo problem number moments need matched subsystem 
standard krylov approaches mimo problem assume fix 
approach known block method individual vectors treated identically construction projection matrices 
block approach straightforward implement situations frequently arise complexity subsystems varies significantly 
corollary provides tremendous amount flexibility treating mimo systems 
projection matrices may weighted achieve greater accuracy specific subsystems 
corollary natural extension rational interpolation mimo case user avoid going extremes 
allowing moderately large automatically precludes small model size may large amount overlap various individual krylov subspaces particularly respect variations indices 
example number theoretically required subspaces may grow large subspaces may come close covering entire union 
unclear time locate globally appropriate individual subspaces 
experiments replacing kl subspaces krylov subspaces independent replace random starting vector replace summed vector led limited success practice 
research needed case large determine appropriate reduced order model practical size 
chapter projection methods rational interpolation chapter demonstrates rational interpolant acquired straightforward conditions column spaces rational krylov algorithm developed section provides great deal freedom computing bases subspaces 
specifically type orthogonality imposed columns projection matrices differentiates algorithms proposed chapter 
significant amount attention impact various orthogonalization schemes theory practice 
particularly elegant version rational krylov method rational lanczos method leads generalization lanczos method treating multiple interpolation points 
development rational lanczos significant compute rational interpolants short iterative recursions 
chapter concludes example demonstrating behavior various rational krylov approaches 
rational krylov method theorem provides simple conditions column spaces acquiring reduced order model rational interpolation 
specific choices implementing meet conditions remains 
goal compute efficient numerically stable manner 
section general method denoted rational krylov method introduced computing infinite number appropriate fact generated method simply varying defined parameters 
broad method available specific implementations follow readily 
additionally presence broad method provides clear framework comparing version particular attributes 
rational krylov rk method algorithm 
name follows description subspaces rational krylov subspaces 
additionally accident title fitting perspective rk algorithm generates rational interpolants 
algorithm rational krylov general version initialize fl gamma fi gamma input oe interpolation point th iteration vm gamma oe gamma pm gamma oe gammat pm fl vm vm gamma vm gamma vm fi gamma zm gamma evm wm fl gamma qm fi wm wm gamma wm wm simplifying assumptions going theorem rk algorithm 
column spaces constructed equal contain union krylov subspaces left sides 
assumption prevent rational interpolation krylov subspaces contain desired moment information 
second assumed dimensions dual krylov subspaces consistent choices allow matching maximum number possible moments model size immediately notice presence wm addition vm algorithm 
constructed related direct fashions 
choice primarily working versus mainly ease notation point view 
initially incorporating matrices rk algorithm options apparent 
iteration rk algorithm consists executing 
user specified interpolation point oe associated iteration 
value oe possible interpolation points oe oe ordering interpolation points rk algorithm arbitrary number times oe chosen oe iterations determines number moments matched oe final reduced order model 
mainly shown number moments matched oe twice number times oe chosen oe iterations 
steps rk algorithm generate new columns projection matrices 
step introduces new information column spaces introduces new information column spaces actual bases represent columns spaces determined 
updates steps correspond classical gram schmidt procedure described 
choices vectors vm wm updates determine type biorthogonality orthogonality produced furthermore vectors primarily distinguish specific implementations 
important options certainly summarized table 
second fourth cases table implemented detail sections 
study specific cases clarifies breadth rk algorithm 
additional component bi orthogonalization specification scaling parameters fi fi fl fl column table titled fi fl restriction lists conditions met case choice fl fl fi fi parameters 
second row example orthogonal required choose fi fl fl gamma vm gamma fl gamma zm gamma mk table orthogonalization choices rational krylov algorithm case vm wm fi fl restrictions orthogonal gamma vm gamma biorthogonal gamma vm gamma vm biorthogonal gamma vm gamma wm vm insure vm 
cases fewer constraints fi fl parameters 
parameter interest rk algorithm mainly previous index pm scalar subscript appears 
value pm locates iteration prior iteration employed interpolation point th iteration 
oe interpolation point gamma iterations sets pm 
oe oe gamma oe gamma st iteration value pm gamma 
new directions generated current iteration founded directions computed time oe 
example sheds light role pm rk algorithm 
example consider th order model oe oe oe oe oe oe oe oe oe oe iteration said associated oe iteration associated oe definition pm 
value example follows fact prior oe utilized second iteration 
choosing fi fi fl fl vm wm check constructed algorithm takes form gamma oe gamma dual result holds pm leads directly correspond desired krylov subspaces order determined selection oe new columns correspond krylov subspaces involving oe added oe chosen oe example demonstrates rk algorithm special parameter choices leads desired column spaces prior leaving general version rk algorithm important show computed lead desired rational interpolant cases 
results prove fact demonstrating column spaces fit required form theorem 
key proof lemma proven appendix lemma oe oe arbitrary distinct interpolation points gamma oe gamma ef gamma oe gamma gamma gamma oe gamma spanf gamma oe gamma bg gamma oe gamma gamma oe gamma jo gamma oe gammat gamma oe gammat gamma gamma oe gammat spanf gamma oe gammat cg gamma oe gammat gamma oe gammat jo value 
theorem assume results steps general rk algorithm nonzero scaling parameters fl fl fi fi relations colsp gamma oe gamma gamma oe gamma colsp gamma oe gammat gamma oe gammat hold equals number times oe chosen oe iterations 
proof inductively proving colsp colsp ae evm oe case straightforward fl initialized assume holds gamma 
due steps rk algorithm fl evm gamma qm combining expression inductive assumption demonstrates holds 
holds induction 
show relation vm ff gamma oe gamma gamma gamma oe gamma vm gamma holds ff nonzero oe equals oe subscript identifies interpolation points th iteration 
equality follows trivially 
induction utilized prove 
desired relation fl gamma oe gamma follows directly rk algorithm 
assume holds iterations gamma 
vm written vm fl gamma gamma vm gamma fl gamma gamma oe gamma pm gamma vm gamma pm equals zero pm lies direction case follows directly 
greater utilized express vm fl gamma gamma oe gamma fl pm gamma pm gamma qm gamma vm gamma vm fl gamma gamma oe gamma fl pm gamma fev pm gamma qm gamma vm gamma vm expression rewritten vm fl gamma fl pm gamma gamma oe gamma ev pm gamma oe gamma evm gamma vm gamma ff pm fl gamma fl pm gamma gamma oe gamma gamma oe gamma gamma oe gamma evm gamma vm gamma vectors 
second equality follows inductive assumption fact pm gamma 
lemma leads desired result 
value ff simply product inverses fl fl terms assumption nonzero ff ff pm fl pm gamma fl pm gamma expression follows directly 
portion proof corresponding dual 
side note interesting fact proof theorem strongly dependent specific value pm subscript 
replacement pm value gamma arises section 
insights alternatives pm provided example 
example consider construction orthogonal see second row table interpolation point ordering oe oe oe oe oe oe columns fl gamma oe gamma fl gamma oe gamma ff parameters fl fl ff chosen orthogonal 
prescribed subscript pm step third rk iteration obtains new direction gamma oe gamma gamma oe gamma ev fl gamma oe gamma gamma oe gamma new direction acceptable augmenting subspace 
subscript pm replaced subscript third direction gamma oe gamma gamma oe gamma ev fl gamma oe gamma gamma oe gamma ff gamma oe gamma gamma oe gamma due lemma gamma oe gamma gamma oe gamma vector combination vectors gamma oe gamma gamma oe gamma takes form ff gamma oe gamma gamma oe gamma ff ff vector acceptable third direction long ff nonzero 
gamma oe gamma gamma oe gamma nearly orthogonal ff extremely small fails introduce new direction 
consistent example proof theorem claim satisfy colsp gamma oe gamma gamma oe gamma colsp gamma oe gammat gamma oe gammat value substituted pm greater pm equality hold theorem pm utilized rank deficient 
case substituted index pm appropriate 
approach handling rank deficient section 
rational power krylov algorithm various orthogonalization parameters vectors rk algorithm provide great deal flexibility computing valid consider section simple case fl fl fi fi vm wm 
orthogonalization approach krylov subspaces generated directly multiplying previous vectors gamma oe gamma choices simplified version rk algorithm rational power algorithm algorithm results 
denote approach rp rational power algorithm consistency abbreviations 
algorithm rational krylov rp version initialize vm gamma oe gamma gamma oe gammat vm gamma oe gamma evm gamma gamma oe gammat gamma vm vm mk mk 
choices going algorithm concrete implementation algorithm 
interpolation points iterations denoted oe utilized consecutive fashion 
iterations involved oe iterations utilized oe course oe theoretically selected order 
different possibilities ordering interpolation points illustrated algorithms developed sections 
due interpolation point ordering rp algorithm pm takes value new interpolation point interpolation point previous iteration 
fact leads possible decision branches 
note rp implementation lacks wm vectors general rk algorithm 
vectors buried rp algorithm due choice wm 
simplification leads wm wm general version yields evm gamma wm gamma expressions substituted general rk algorithm remove explicit presence wm rp implementation 
step need store sequences vectors memory 
general vm wm equals corresponding vectors vm wm see table 
equalities occur possible corresponding vector sequence need stored memory 
compute projection matrices general nonsymmetric problems obtain rational interpolant 
relatively brief analysis rp implementation reveals constructed takes explicit form gamma gamma gamma oe gamma matrix takes dual form 
rp algorithm realizes appropriate bases colsp fv colsp fzg direct manner possible 
course pointed section direct approach straightforward repeated multiplications gamma oe gamma may lead numerical difficulties practice 
lack sort bi orthogonalization algorithm immediately causes concern 
extremely small problems sure arise 
numerous repeated multiplications fixed matrix finite precision longer introduce new information hope arises rp algorithm values oe varied 
interpolation point frequently altered kept small 
changing new interpolation point problem converge guarantees new information added 
places information projection matrices repeated changes oe repeated multiplications 
approach similar spirit complex frequency hopping improvements awe 
successful rp implementation tends require factors gamma se numerous points 
projection technique rp algorithm generates single reduced order model rational interpolant 
projection approach may enable techniques approximate solves parallelism reduce matrix factorization costs see chapters 
frequent change oe unacceptable dependent columns appear consider example redundancy columns results different interpolation points near 
unfortunately constitutes nearness problem dependent difficult characterize 
practice may try adapt point placement problem model reduction proceeds 
situation rule appearance dependencies columns rp generated long values kept small expect number dependencies small fraction dependent portion discarded singular value decomposition 
assume ranks gamma ffi gamma ffi respectively rank gamma lesser gamma ffi gamma ffi singular value decomposition exist orthogonal matrices jt jt matrix gamma gamma sigma matrix sigma nonsingular square rank equal min gamma ffi gamma ffi 
theorem lower order model consisting appropriate model reduction 
theorem left right singular vectors gamma lead matrices zt full rank 
proof singular value decomposition columns known form bases null spaces gamma gamma respectively 
equals av equals ev null spaces respectively contained column spaces vector clearly av ev zero 
orthogonal implying zt full rank 
matrices zt full rank serve suitable projection matrices 
fact gamma ffi column spaces equivalent 
dual result holds colsp ae colsp fv column spaces differ slightly manner depending choice typically interpolation points 
case reduced order model involving differs slightly desired rational interpolant 
additionally matrices typically exactly singular practice 
result lower right corner rightmost matrix differs slightly zero 
reasons projection yields better conditioned slightly perturbed reduced order models 
summary rp implementation simple version rk framework avoids orthogonalization 
due simplicity issue dependent projection directions addressed multiple interpolation points possibly 
rp approach certainly promote level understanding elegance follows inclusion orthogonalization 
simplicity rational power krylov approach allows interesting possibilities chapter 
dual rational arnoldi algorithm arguably best approach avoiding difficulties construction insure quantities orthogonal matrices 
technique implemented algorithm 
algorithm denoted dual rational arnoldi ra version steps taken independently construct similar steps rational arnoldi method 
algorithm interpolation points interspersed 
iteration uses oe second iteration uses oe th iteration involves oe st iteration uses oe alternating strategy fixes pm gamma determines decision indices 
stressed ordering interpolation points theoretically factor resulting model 
approach algorithm simply provides example interpolation point selection strategy 
algorithm rational krylov dual ra version initialize vm gamma oe gamma gamma oe gammat vm gamma oe gamma evm gammak gamma oe gammat gammak vm vm gamma vm gamma gamma vm gamma zm gamma gamma vm vm kv kz mk long singular gamma oe invariant subspaces avoided dual ra implementation guaranteed yield desired guarantee resulting gamma oe nonsingular see section 
ability construct chance breakdowns important point 
long orthogonality maintained stable fashion dual ra implementation generates completely stable fashion 
krylov implementation rational interpolation pad approximation possessing stability property 
recall construction orthogonal basis trivial process 
householder gram schmidt cases gradual loss orthogonality observed computed versions exact orthogonality condition theorem significant losses orthogonality impair quality reduced order model 
sort postprocessing required severely ill conditioned 
appearance dependent columns classical gram schmidt approach implemented algorithm rare possible 
situation postprocessing approach discussed section possible remedy 
summary dual ra method tends better behaved algorithm rk variants seen chapter 
unfortunately dual ra implementation fast rational krylov version derived section 
fact particularly surprising trade algorithm speed robustness common numerical linear algebra 
understand spectrum possibilities proper implementation may application 
general rk method section provides great flexibility balancing computational effort stability 
initial rational lanczos algorithm rational lanczos version rk algorithm maintains biorthogonal simple develop initial version rational lanczos rl method algorithm general rk algorithm 
duals steps rl implementation particularly symmetric respect 
rational lanczos potentially fast implementation krylov model reduction 
shares name similar algorithm derived 
beauty algorithm rational krylov initial rl version initialize fi gamma input oe interpolation point th iteration vm gamma oe gamma pm gamma oe gammat pm fl vm vm gamma vm gamma gamma vm evm wm fi wm wm gamma wm oe oe rl algorithm lies speed computed 
fact paths generating valid reduced order model rational lanczos algorithm 
approach emphasizes expression permutes order columns produce model form similarity transformation 
approach developed section starts expression leads model takes exact form 
paths differ significantly duals paths algorithm lead similar final results 
spend time reviewing older version 
implementation superior ease development form resulting reduced order model 
implementations section rapidly lead rational interpolation newly proposed implementations follow clearly projection framework utilized dissertation 
considering form ev generated algorithm 
value th column appears computation wm th iteration fi wm wm gamma wm gamma wm second equality follows definition wm th iteration 
th column arises directly rightmost quantity 
multiplying left recalling biorthogonality matrices yields results fi results hold value upper hessenberg matrix 
fact said 
case algorithm step th iteration leads relation gamma oe gammat pm specific values quantity zero 
matrix ev generated rl algorithm extremely sparse 
theorem columns generated rational lanczos pm proof determine values quantity zero note pm pm equals biorthogonality question zero rephrased gamma oe gamma ev lie column space pm answer question follows fact pm index iteration interpolation point oe due fact theorem colsp fv pm gamma gamma oe gamma gamma oe gamma gamma oe gamma gamma oe gamma index oe oe values set th iteration 
definition oe th iteration known relation colsp fv pm gamma gamma gamma oe gamma gamma oe gamma gamma oe gamma gamma oe gamma holds 
lemma expressions colsp gamma oe gamma ev pm gamma colsp fv pm obtain gamma oe gamma ev colsp fv pm pm zero pm development particular sense greater 
generally nonzero 
greater nonzero pm 
presence nonzero pm generalization term recursion standard lanczos method 
lanczos method equals pm equals gamma nonzero terms exist values gamma 
behavior rl algorithm demonstrated example 
example consider executing rl algorithm iterations oe iterations oe iterations oe iterations 
values parameters oe pm varies table 
structure matrix shown 
structure particularly elegant interpolation points utilized alternating fashion 
case pm equals gamma lower bandwidth upper bandwidth column sparsity structure example table rational lanczos parameters example oe oe oe oe oe oe oe oe oe oe oe oe pm example consider executing rl algorithm example interpolation points alternating order indicated table 
iterations performed oe oe new reduced order model equivalent similarity transform results example 
alternating interpolation points leads different 
note banded structure structure surprisingly sparse 
fact follows direct fashion see fact note biorthogonality th row av gamma oe oe ev pm oe ev pm oe ev th row oe times th row plus standard unit vector 
single interpolation point oe matrix follows trivial fashion rational lanczos 
know simple compute 
providing pleasing structure sparsity rl algorithm significantly reduce memory computational requirements algorithm 
seen lanczos algorithm banded matrices correspond shortened recursions computation vectors 
example computed fi wm wm gamma components summation drop trivially 
special case interpolation points alternated regular fashion example scalars zero gamma fi wm wm gamma max gammak column sparsity structure example table rational lanczos parameters example oe oe oe oe oe oe oe oe oe oe oe oe pm special case 
computation wm length recursion requires knowledge vectors vm gammak vm wm gammak wm earlier vectors need stored memory 
interpolation points regularly alternated reasonably small rl algorithm comparable memory standpoint standard lanczos method 
regularly alternated interpolation points important 
interpolation point selection schemes necessarily introducing higher total number nonzero elements lead larger values gamma pm lengthened vm wm recursions 
trade flexibility interpolation point selection length iterative recursions exists 
practical rational lanczos algorithm version rl method utilizes regularly alternated interpolation points provided algorithm 
slight reordering steps version easier implementation practice 
reordering change results 
discussing rl algorithm remaining chapters referring algorithm noted 
shortened recursion wm follows 
validity truncating vm recursion follows values fl gamma oe gamma evm gamma dual proof theorem expression lemma indicates vector gamma oe gammat lies column space wm gamma gamma gamma 
biorthogonality forces fl zero gamma gamma 
matrices generated algorithm follow readily developed results 
example comparing yields descriptor algorithm rational krylov banded rl version initialize gamma oe gamma vm gamma oe gamma evm gamma vm vm gamma gamma max gammak gamma fl fl vm wm wm gamma gamma max gammak gamma fi fi wm vm vm fl fl vm wm wm fi fi sign vm fl wm gamma oe gammat wm matrix reduced order model fi fi 
fi 
fi 
fi jk gammak gammak fi fi jk gammak jk gammak 
fi jk jk fi jk jk gammak delta delta delta fi jk jk computing column requires execution st iteration 
fact consistent behavior standard lanczos method 
columns follow readily 
lies direction biorthogonality yields fi expression reduced order input vector results relation gamma oe gamma oe gamma gamma oe fl vector simply column gamma oe times scalar 
due vector zero elements 
sparse 
rational interpolant achieved short recurrences 
efficiency short recurrences come introducing additional pitfalls 
remainder section explore potential difficulties corresponding remedies arise practical rl implementation 
analogies pitfalls remedies exist standard lanczos method 
harsh reality rl method employed short recursions fail keep biorthogonal finite precision 
fact observed practice biorthogonality lost rapidly significant extent 
issue concern reduced order model explicitly constructed 
rl method constructs reduced order model biorthogonality assumption 
validity banded structure intertwined assumption biorthogonality 
proposed remedy similar problems literature simple 
ignores loss biorthogonality continues short recursions blindly chooses reduced order model 
course implemented reduced order model longer exact rational interpolant 
surprisingly approximation generated falsely assumed biorthogonality converges practice albeit slightly slower rational interpolant 
symmetric case finite precision eventual convergence resulting approximation assured stressed steps may required achieve desired level convergence practice 
comments behavior area iterative solvers linear systems equations may 
bottom line loss bi orthogonality tends slow destroy convergence approximations assume bi orthogonality avoid explicit 
behavior perfectly understood nonsymmetric lanczos method 
certainly scope study attempt address rl algorithm rigor 
intuition rl results converge follows local satisfaction petrov galerkin constraint 
shown section biorthogonality assumptions output residual gamma se gamma resulting rational lanczos iterations form fi wm fi gamma gammat cg orthogonal wm satisfying guaranteed gamma implemented short recursions zero rl algorithm may enforce full biorthogonality practice guarantee current output residual orthogonal directions 
petrov galerkin constraints hold respect directions 
observation important regarding convergence rl algorithm light biorthogonality loss 
repeatedly observed practice eventual convergence rl results occurs pm replaced value gamma 
approach introduced just prior section contradicts standard practice choosing pm index iteration employing oe replacing pm gamma implemented rl algorithm 
superiority gamma choice methods construct approximations assuming bi orthogonality observed 
related comments topic pm choice may 
second practical concern rl implementation called serious breakdown 
breakdown occurs lanczos type method vm 
assumptions theorem violated event fi fl take value zero 
case biorthogonality maintained algorithm proceed 
theoretical background breakdown closely related theory section 
fact dot product vm proportional error st moment order reduced order model corresponding vm wm 
recall theorem fortuitous matching st moment leads singularity order approximation 
requirements section need concerned singularities order approximation singular vm leads breakdown rational lanczos 
subsequent step rational lanczos relies assumed biorthogonality existing projection matrices 
solutions serious breakdowns include look ahead approach interpolation point changes discussed section 
remedies longer simply postprocessing events may need avoid breakdown iteration 
interested reader may find intricate details look ahead lanczos 
editing interpolation order difficult implement require maintenance longer biorthogonality recursions gamma pm grows 
way avoidance breakdown results fill outside band elements rational stressed issue serious breakdowns concern rl version considered rk implementations 
final pertinent issue implementing rational lanczos choice fl fi parameters 
care taken scaling vectors vm wm obtain biorthogonal vectors vm wm standard approach taken algorithm scale vm wm fi stable approach occasionally seen lanczos literature select fl jw vm delta fi sign vm jw vm delta norms vm wm identical 
comparisons variety numerical approaches implementing rational interpolation projection obvious question method best 
simple answer exists 
problem dependent factors eigenvalue spectrum sparsity pattern large scale matrix pencil desired accuracy reduced order model favor different variations general rk approach 
handling varying factors numerically efficient robust manner requires flexibility 
computational costs methods sections follow relatively simple analyses algorithms 
results analyses iterations algorithm summarized table 
column indicates number floating point operations flops required generate projection matrices column lists number flops required form model projection matrices formed 
data columns accurate order dominant terms 
table utilizes special notation represent cost specific matrix operations utilized rk method 
cost acquire triangular factors gamma oe denoted cost solve system equations factors cost multiplying dense vector cost multiplying dense vector determining precise costs floating point operations flops matrix operations requires knowledge practice sparsity patterns specific techniques exploit sparsity patterns 
example rc models circuit interconnects involve order order table computational costs rk implementations method projection matrix generation model generation rp kf nm dual ra kf nm nm rl kf methods requires kf operations form factors gamma oe 
remaining difference algorithms due amount orthogonalization performed effort required generate reduced order model 
rp algorithm requires orthogonality recursions rl method requires length recursions dual ra method requires length recursions 
fixed cost kf dominates matrix factorizations cost large due lack easily exploitable sparsity differences negligible 
hand matrices extremely sparse large cost different implementations varies significantly 
case rl algorithm desirable cost grows linearly km 
rl algorithm employs short recursions automatically lead reduced order model 
dual ra method far expensive fixed costs negligible cost orthogonalize effort compute reduced order model grow quadratically rp implementation requires linearly increasing compute quadratically increasing effort form reduced order model 
cost rp method order dual ra approach robustness sequential rp implementation algorithm rarely recommended 
memory requirements different variations related costs 
summary memory requirements algorithms table 
second column lists memory needed iteratively storing columns third column lists additional memory required store reduced order model 
analogous table variable denotes space store matrix factorization denote number nonzero elements memory requirements exactly specified depend sparsity values assuming memory required store factorizations dominant rl implementation requires fixed amount memory memory requirements dual ra rp implementation grow linearly table memory requirements rk implementations method projection matrix generation model generation rp kf mn dual ra kf mn rl kf kn memory cost standpoint rl method preferred dual ra method 
section suggests numerical accuracy favors methods exactly opposite order 
rl implementation gains speed reliable short recursions 
rational lanczos generated model may eventually converge convergence may delayed relative results dual rational arnoldi implementation 
behavior explored example 
example compare behavior dual rational arnoldi rational lanczos implementations consider generated problem intended mimic behavior packaging interconnect circuit 
generated problem consists fifteen identical segments connected series see structure segment 
modified nodal analysis mna set equations size formulated describe interconnect 
frequency response interconnect hertz shown 
input system voltage source placed left segment output current source 
ph pf interconnect segment example iterations dual ra rational lanczos algorithms executed matlab utilized code available appendix cases iterations alternated interpolation points oe oe figures display convergence loss biorthogonality orthogonality computational costs approaches function iteration presents computed estimate relative error norm weighted hz original system respective reduced order models 
identical selection interpolation points cases suggests identical convergence infinite precision convergence rl method dotted line clearly slower 
behavior finite precision agrees practical observations limited theory literature see discussion section 
delay convergence correlated loss biorthogonality rl case see dotted line denoting ki gamma 
orthogonality dual ra case eventually lost see dashed line denoting ki gamma vm ki gamma 
dual ra implementation employs classical gram schmidt orthogonalization 
approach unstable robust shortened recursion rational lanczos 
stress difference orthogonalization schemes determine convergence difference 
difference arises fact dual ra employs construct reduced order model depend orthogonality rl approach constructs banded reduced order pencil depend biorthogonality 
assumption associated undesirable convergence delay rational lanczos offset reduced computational effort 
presents cumulative number floating point operations required modeling implementations function involved rl case grows linearly dotted line dual ra case grows quadratically dashed line 
interesting note approaches require number total operations compute reduced order models errors gamma requiring iterations rl approach need involve 
behavior tends problem dependent contributes long standing arguments lanczos type versus arnoldi type approaches numerical linear algebra literature 
frequency hz frequency response example iteration dual rational arnoldi rational lanczos convergence example iteration dual rational arnoldi rational lanczos loss bi orthogonality example iteration flops dual rational arnoldi rational lanczos computational costs example statements summarize choice rk implementation 
rp algorithm appropriate cases exceeds 
dual ra algorithm preferred complexity factorizing gamma oe dominates 
extremes balance convergence reliability dual rational arnoldi shortened recursions rational lanczos 
additionally research determine desirable versions general rk algorithm exist 
chapter model error knowledge error original system computed rational interpolant important reasons 
monitor number iterations required convergence reduced order model 
simulation needs know response reduced order model sufficiently close original system 
control hopes construct controller reduced order model performs acceptably original system 
applications unnecessarily large models avoided due computational cost 
measure error feedback adapt modeling procedure 
example attempt select interpolation points steps focus errors earlier iterations 
chapter approaches estimating error reduced order model developed 
merits compared particularly examples chapter 
unfortunately proposed techniques guaranteed completely inaccuracies 
reality follows naturally fact combined knowledge reduced order model modeling error implies total knowledge original system 
complete analysis original system avoided resort approximations measure gap original reduced order models 
complementary approximations simple approach estimating frequency response error ffl gamma original reduced order models compute difference reduced order models ffl gamma 
transfer function 
corresponds second completely different low order approximation original system 
approximations generated necessarily krylov projection algorithm 
suitable achievable error estimate previously discussed modeling techniques 
low order approximations contrast approximations original system difference estimates modeling error 
points view original system sought designed complementary 
drastically different viewpoints typically suggests 
agree consistently frequencies approximations accurate 
different viewpoints agree ffl small assumes ffl small 
different viewpoints diverge approximations inaccurate ffl assumed significant 
note assumption errs conservative side lack converged 
frequency directly imply large ffl frequency 
generation distinct reduced order models requires construction different projection pairs dimension previously seen second pair 

note subscript denotes complimentary necessarily orthogonal directions colsp fv colsp fv 

flexibility forming pairs projection matrices resides choice interpolation points 
different sets interpolation points sought lead distinct reduced order models 
frequency responses multiple pad approximations obviously utilizes single distinct interpolation point compared estimate convergence 
generalization rational interpolation propose interlaced sets interpolation points interlaced moment matching 
example point distribution seen black dots correspond approximation white dots 
interlaced interpolation points versus distinct clusters points provides reduced order models reasonable opportunity converge entire frequency range 
recall viewpoints converge estimated error diminishes 
keeping individual interpolation points models oe oe separated leads approximations desired local complementary viewpoints 
min max interpolation point interlacing complementary models viewpoints approximations ffl designed complementary possible may feature original dynamic model 
situation ffl indicate missing feature error estimate incorrect 
difficulties tend associated lightly damped poles identified reduced order models 
issue taken section section shows ffl appropriate circumstances 
cost generating error estimate involves computation second reduced order model evaluation 
generating additional approximation simply doubles cost appropriate algorithm table 
various possibilities associated costs exist evaluating ffl 
computing norm conventional eigenvalue analyses hamiltonian matrices costs minimum flops 
typically total cost order flops error estimated iteration algorithm find cost scaled back slightly evaluating error iteration 
alternative approach computing ffl simply evaluate multiple points 
practice approximately placed points fashion frequency response algorithms give reasonable results purposes 
transforming matrices upper hessenberg form flops prior evaluation cost computing ffl fixed points kept roughly flops iteration 
total cost estimate error algorithm order flops 
table cost evaluate ffl typically exceed cost generating reduced order model dual rational arnoldi approach 
possible easily exploitable sparsity ffl evaluation may eclipse cost generating reduced order model rational lanczos 
model difference approach may appropriate rl implementation 
assuming cost generating second model dominates computation ffl may wonder apparent doubling required effort worthwhile 
possible savings gained error estimate may offset doubling computation 
sense modeling error adapt utilization interpolation points dynamics absent model see section 
adaptive interpolation point placement selection lead acceptable models smaller values save storage 
error estimate suggests stopping criterion model size assumed sufficient error drops certain level 
stopping criterion allows avoid excessive wasteful iterations 
error estimate may increase iteration knowledge ffl reduce total number iterations required 
implementations effort grows quadratically dual rational arnoldi reduction total iterations especially significant 
error estimate oftentimes improved simple modification 
recall ultimately interested acceptable low order model represented 
second low order function 
serves estimate true frequency response 
better approximation combining information sets projection matrices 

obtain th order model 
updated error estimate ffl gamma similar hopes new error estimate large converged due presence 

directions reduced order model 
new estimate superior perspective new error approximation expected drop zero converges 

includes original directions tends converge 
cost standpoint generating ffl may require times effort ffl 
projection matrices 

bi orthogonalized generating cost evaluating ffl larger 
advantages ffl significant preferred 
residual expressions alternative approach quantifying modeling error previously discussed residual expressions gamma se gamma gamma se gamma zx residual expressions significant tool quantifying error iterative linear system solving 
known simple relations residuals arise arnoldi lanczos contexts 
role dual systems model reduction surprising residuals pertinent modeling error 
residuals utilized partial realization problem 
formalize new fundamental relationship residuals modeling error result 
theorem difference frequency responses original reduced order systems gamma se gamma proof starting frequency response definitions modeling error ffl se gamma gamma gamma gamma gamma se gamma gamma gamma se gamma gamma fb gamma se gamma se gamma gamma petrov galerkin conditions section expanded ffl fc gamma se gamma se gamma gamma se gamma gamma desired result 
evaluating error expression entirety remains difficult task 
sufficiently small typically implies small error frequency 
exception behavior occurs near eigenvalue elements gamma gamma grow large 
analogous section large errors due presence weak poles imaginary axis may adequately reflected residual 
stressed monitoring directly lead estimate modeling error 
acquiring modeling error requires inverse se gamma possessed 
concentrate trends residual behavior vary 
attempting gauge trends demands evaluation numerous values residual expressions implementations chapter simplify result 
lemma matrices general rk algorithm satisfy gamma se vm qm delta delta delta delta delta delta pm fl delta delta delta fl vm 
fl gamma delta delta delta fl 
fl oe gamma 
oe gamma gamma se zm wm delta delta delta delta delta delta pm fi delta delta delta fi 
fi gamma delta delta delta fi wm 
fi oe gamma 
oe gamma 
lemma writes se gamma se gamma matrices residual expression product fixed matrix size theta low order frequency dependent matrix 
ability extremely beneficial rational lanczos versions vm recalling definition rational lanczos reduces gamma se zm wm em oe gamma 
oe gamma wm fi oe gamma wm gamma wm fi oe gamma rl output residual vector iterations gamma gamma gamma gammat fi oe gamma wm fi gamma fi oe gamma wm gamma gammat wm gamma gammat cfi oe gamma due definition 
norm output residual computed norm wm times frequency dependent function composed low order matrices 
norm wm need computed regardless number times evaluated 
nonzero element function gamma gammat rapidly evaluated arbitrary frequency need find left transformation place upper hessenberg matrix gamma upper triangular form perform single step back substitution find element 
upper triangular transformations step easily updated flops iteration gamma gamma leading minors upper hessenberg matrices assuming residual evaluated points cost iteratively updating norm approximately flops iteration 
cost roughly mn flops value compares favorably expense computing reduced order model rational lanczos 
versions rk algorithm expressions generate residuals 
updating residual norms iteration involves mn operations iteratively update qm wm additional operations frequency point solve low order upper hessenberg system equations 
general case residual evaluation iterations involves operations cost comparable treatment ffl previous section 
comparisons computation modeling error involves common theme balancing computational expense quality results 
methods chapter provide error estimates costs larger modeling expense 
residual tends slightly cheaper estimation particularly case efficient rational lanczos algorithm 
evaluate performance versus complementary model comparison analyze examples 
example considers effectiveness error estimates predicting worst case error reduced order model example example consider model plasma dynamics originally 
frequency response original model shown 
response relatively simple due presence eigenvalues negative real axis 
frequency responses type quite common applications 
dual ra algorithm applied problem iterations oe 
actual error frequency response indicated second column table varies 
difference maximum relative difference frequencies range 
estimates complementary approximations third column various third column compares reduced order models centered respectively oe oe 
provides reasonable error estimate example 
maximum product relative residuals indicated fourth column table 
included residual error estimate possible take sided properties theorem account 
fourth column provides excellent estimate worst case frequency response error 
approaches sections provides acceptable measure modeling error problem 
worst case bound user may interested error estimate frequency range 
estimate may useful example modify selection interpolation points 
example problem arises partial element equivalent circuit model patch antenna structure see experimentation 
containing capacitances mutual circuit realized system dimension 
magnitude frequency response circuit shown 
note presence multiple sharp peaks resulting lightly damped poles imaginary axis 
rational lanczos algorithm applied model iterations 
generated approximation alternated iterations oe ghz iterations oe ghz 
figures true modeling error dashed line ffl estimate 
acquiring ffl reduced order model compared second complementary model generated single interpolation point oe ghz 
occasionally missing peaks ghz occasionally overestimating peak ghz ffl provides surprisingly accurate measurement true error ffl entire frequency range 
figures compare true error ffl dashed line output residual dotted line 
recall rational lanczos residual value readily evaluated 
ease analysis results residual computation scaled amount frequencies means magnitudes ffl identical figures 
practice concentrate relative changes values variables vary 
clearly residual results precise figures 
keep mind residual estimate may generated effort 
frequency rad frequency response example table modeling error estimates example max jh gamma hm jh max hm gamma max kck delta jr bm kbk frequency ghz frequency response example residual plots able capture general shape error curve point locations spikes frequency response 
examples suggest proposed error estimation techniques effective situations 
error estimates model differences residuals adequately treat sharp spikes frequency response 
assumption small ffl residual guarantees small error necessarily valid sharp peaks valid neighborhoods poles imaginary axis 
difficulty demonstrated example 
point example completely invalidate proposed error estimation methods 
demonstrate care treating systems lightly damped poles 
frequency ghz true error model difference error estimate ffl example frequency ghz true error model difference error estimate ffl example frequency ghz true error model difference error estimate ffl example frequency ghz true error model difference error estimate ffl example frequency ghz true error output residual error estimate example frequency ghz true error output residual error estimate example frequency ghz true error output residual error estimate example frequency ghz true error output residual error estimate example example consider simple seventh order system defined gamma gamma gamma gamma gamma gamma gamma gamma gamma frequency response system shown 
model interpolation point oe true error shown dashed line figures 
note reduced order model fails capture peak original frequency response rad frequency rad frequency response example second model associated 
constructed interpolation point oe 
error estimate comparing 
shown dotted line 
unfortunately error estimate ffl falsely suggests convergence estimates digits accuracy misses peak rad note estimate conservative low frequencies values convergence 
behavior 
error estimate residuals displayed dotted line 
dotted curve indicates scaling products relative output input residuals kbk kck frequency 
error estimate fails indicate absent peak rad residual provide better estimate true errors low frequencies 
discussions examples qualities proposed techniques summarized 
ffl estimate 
provides direct approximation ffl 
accurate frequencies corresponding complementary interpolation points oe 
overestimate error frequencies corresponding primary interpolation points oe 
may underestimate errors corresponding sharp frequency peaks residual error estimate 
scaled ffl particularly residuals computed 
accurate frequencies corresponding interpolation points model 
may fail indicate errors corresponding sharp frequency peaks 
typically requires particularly function rational lanczos features complementary suggesting robust implementation incorporate approaches possible 
frequency rad actual error model difference error estimate ffl example frequency rad actual error residual product error estimate delta example chapter model interpolation points krylov projection methods chapter interpolate value consecutive derivatives frequency response original system points 
knowledge interpolating frequency response reveals little concerning quality resulting reduced order model 
example extract combination poles eigenvalues original system appropriate interpolation choices 
precise location interpolation points amount data matched interpolation point central factors determining accuracy dimension reduced order model 
placement selection interpolation points studied chapter 
connections locations interpolation points convergence behavior model 
particular effort concentrated popular choices purely real imaginary interpolation points 
analysis provides insights relations model convergence placement interpolation points dynamics system 
insights sufficient dynamics original system rarely known priori 
portion chapter focuses implementation point placement selection practice nontrivial problem dynamic behavior original system rarely known prior model convergence 
analysis tools implement rational interpolation location interpolation points number moments matched interpolation points specified 
decisions determine size accuracy reduced order model 
understanding impact choices resulting reduced order straightforward 
relating choices interpolation points convergence eigenvalues residuals past results theorem turn connect quantities convergence reduced order model 
understand convergence analyses performed chapter tend consist general trends precise mathematical derivations 
unfortunately current understanding convergence krylov projection methods limited particularly nonsymmetric matrices multiple krylov subspaces involved 
level rigor sufficient providing intuition interpolation point placement selection 
insights section serve important practical implementation decisions sections 
initially assume number interpolation points small convergence frequency range depends primarily single pair dual krylov subspaces corresponding interpolation point region 
associating frequency range single interpolation point oe simplifies analysis reasonable applications 
additionally assumption errs conservative side considering interpolation points considering union information projector typically improves convergence frequency 
role eigenvalues behavior system grounded bode 
relations exist poles original system peaks system frequency response 
considered krylov projection methods convergence eigenvalues governed spectrum matrix gamma oee gamma neighborhood oe 
simple show eigenvalue eigenvector gamma oe eigenvalue eigenvector gamma oee gamma tendency eigenvalue appear eigenvalue pencil depends extent gamma oe 
positioned outer edge spectrum gamma oee gamma particular eigenvalues original system closest oe mapped outside spectrum gamma oee gamma 
separated eigenvalues gamma oee gamma say gamma oe separated distance gamma oe closest neighbor order gamma oe 
strengthened due presence large eigenvector components vectors residue ae corresponding measure strength 
residue arises partial fraction expansion 
motivate observations note construction utilized krylov subspace involves multiplication vectors gamma oee gamma vector expanded terms eigenvectors multiplied gamma oee gamma result assuming distinct eigenvalues simplicity gamma oee gamma gamma oee gamma ff ff gamma oe eigenvectors strong ff large near oe positioned outside spectrum gamma oee gamma emphasized multiplication gamma oee gamma case scaling ff gamma oe grows large 
may emphasized eigenvectors corresponding eigenvalues cluster separated emphasized extent making difficult discern individual directions cluster 
point analysis similar spirit power method 
stress care taken limiting comparison krylov projection involves entire subspaces simply single directions 
additional relations interpolation points convergence reduced order model follow point view approximately solving dual systems linear equations 
error frequency response model se gamma gamma explained section speed residuals driven zero depends choice ds preconditioners krylov subspaces 
exact ds preconditioner gamma oe gamma turn determined choice interpolation point 
certainly interest know properties ds preconditioner relate interpolation point 
analysis eigenvalue convergence residuals associated approximate solutions defined pertinent values matrix gamma oee gamma gamma se center linear system solver point view considered section 
recall gamma oee gamma plays role exact preconditioner krylov solver linear systems equations involving gamma oee 
possible path discuss ds preconditioning associated matrix gamma oee gamma gamma se concept clustering 
ds preconditioner may evaluated number distinct eigenvalue clusters appearing spectrum gamma se 
small number tightly packed clusters preferred spectrum gamma se values frequency range interest 
eigenvalues gamma oee gamma gamma se said clustered relative difference gamma min sufficiently small 
cluster particularly desirable 
motivate concept clustering consider matrix equals gamma je generic matrix 
assume matrix fl clusters 
solving system equations gx projection technique leads solution lies krylov space 
clusters tight eigenvalues cluster lie atop dimension exceed fl 
example dimension scaled identity matrix eigenvalues case identical lies 
number steps required iterative solver find exceed fl gamma 
eigenvalues exist points example exact solution arises iterations 
clustering reduces number directions considered iterative method finding solution 
course clusters tight clustered elements longer exactly top say rank increasingly perturbed version equals fl 
rough rule thumb expect relative error solution fl gamma iterations proportional relative distance clustered eigenvalues 
impact choice oe matrices gamma oee gamma gamma oee gamma gamma se key analyses interpolation points 
surprisingly matrices fact related simple scaling shift expression 
point placement desired features gamma oee gamma gamma oee gamma gamma se various strategies locating interpolation points may evaluated 
simpler single point approaches 
imaginary interpolation points imaginary interpolation point logical starting points interested minimizing frequency response error gamma imaginary axis 
aid analysis choice example mapping gamma oe displayed imaginary interpolation point 
recall gamma oe eigenvalue gamma oee gamma eigenvalue 
evident expression gamma oe imaginary interpolation point maps eigenvalues near oe separated positions outer edge spectrum gamma oee gamma discussion section expect poles original system nearest oe rapidly appear 
practical experience confirms observation 
poles near oe tend appear regardless strength 
imaginary interpolation point powerful tool finding information neighborhood near oe 
advantage imaginary interpolation point locally unfortunately globally 
eigenvalue strength separation come play away oe spectrum spectrum eigenvalue mapping imaginary interpolation point roughly expect convergence tendency eigenvalue inversely proportional distance oe 
desired eigenvalue near imaginary axis away oe wait convergence eigenvalues closer oe including eigenvalues possibly distance left half plane 
strong eigenvalues far left half plane impact 
weak eigenvalue near imaginary axis leads peak frequency response 
convergence may forced wait undetermined number eigenvalues far left half plane nonessential model 
situation stagnation observed reduction modeling error non contributing eigenvalues identified 
stagnation amplified lies cluster eigenvalues weak 
slowing convergence stagnation leads model unnecessarily large size 
ds preconditioner point view previous discussion remains pertinent 
due lemma eigenvalues gamma oee gamma gamma se simply matrix gamma oee gamma shifted scaled gamma oe 
deviation eigenvalues ds preconditioned matrix cluster directly proportional distance oe 
deviates oe numerous eigenvalues escape cluster effectiveness ds preconditioner diminishes 
convergence zx true solutions values away oe slows iterative solver capture scattered eigenvalues outside cluster 
order scattered eigenvalues depends issues touched previous paragraph 
summarize imaginary interpolation points lead excellent results locally result extremely slow convergence frequencies away oe 
example behavior provided section 
leaving topic imaginary points reader note poles near complex conjugate interpolation point oe typically near oe mapped gamma oee gamma cluster origin 
convergence poles original system occur complex conjugate pairs interpolation oe oe required 
results interpolating oe simply dual oe 
comments implementational aspects complex arithmetic order 
allows complex oe avoidance complex desirable 
potential cost savings keeping purely real real reduced order model preferred consistency real description original system 
approach retain real operations complex oe developed rational arnoldi algorithm 
thrust development treat complex points oe oe pairwise 
iteration oe executed perform simultaneous iteration conjugate oe relation gamma oe gamma gamma oe gamma vg knowledge direction automatically implies knowledge conjugate 
execution simultaneous conjugate iterations need introduce real directions gammaoe gamma vg gammaoe gamma vg complex ones 
halves extra effort typically increase factor involved working complex matrix gamma oee 
details reader referred 
real interpolation points interpolation points prior examples dissertation real 
utility positive real interpolation point follows mapping generalized eigenvalues eigenvalues matrix gamma oee gamma mapping displayed determined result 
lemma initial system stable oe positive real number eigenvalues gamma oee gamma contained circle radius oe centered gamma oe spectrum spectrum io eigenvalue mapping real interpolation point lemma value left half complex plane mapped gamma oe position inside circle 
imaginary axis edge left half plane compressed edge circle 
simple analysis mapping gamma oe reveals poles initial system magnitudes oe mapped cluster gamma oe poles magnitudes greater oe mapped cluster 
poles magnitudes order oe avoid squeezed cluster gamma oe poles separated spectrum gamma oee gamma tend approximated single compressed pole model 
expect poles oe distinguishable reduced order model 
particular expect strong poles magnitudes order oe appear 
poles lie shaded regions 
real positive interpolation points provide distant view dynamics stable initial system 
position mapped eigenvalue gamma oe convergence approximate eigenvalue typically sensitive value real oe real 
practical experience verifies pole away imaginary axis appears model real interpolation point pole separated strong 
example consider eigenvalue gamma perturbed version eigenvalue ffi gamma 
change resulting mapped eigenvalue relative perturbation pole gamma oe gamma gamma ffi gamma oe gamma gamma ffi oe oe 
difference mapped versions ffi significant imaginary interpolation point negligible real interpolation point 
eigenvalue convergence interested approximate system solutions turn ds preconditioned matrix gamma oee gamma gamma se varies 
distance exists real oe system poles small relative perturbations barely impact results ds preconditioned matrix 
result helps formalize statement 
lemma eigenvalue gammas gammaoe eigenvalue matrix gamma oee gamma gamma se 
assuming stable initial system positive real oe difference gamma oej large oe 
observation lemma eigenvalues gamma oee gamma gamma se gamma oee gamma gamma ffi vary ffi oe oe 
compare fact imaginary interpolation case change spectrum ds preconditioned matrix directly proportional perturbation ffi interpolation point 
ds preconditioner real interpolation point opportunity suitable wider regions potential real interpolation point effective wide regions effective broad regions different things 
examples section real interpolation point leads convergence wide neighborhood oe finite number weak eigenvalues imaginary axis 
insight observation return 
spectrum ds preconditioned matrix simply shifted scaled version clear real interpolation point leave large number separated eigenvalues spectrum gamma oee gamma gamma se corresponding eigenvalues magnitudes near oe 
previous discussions indicate strong scattered eigenvalues ones tending appear model 
eigenvectors corresponding strong eigenvalues play important part solutions ones definition dominate significance directions arises role fixed vectors importance depends variations frequency 
points directions overshadowed weaker poles crop near imaginary axis 
eigenvector corresponding weak predominantly imaginary pole dominates solution gamma se nears krylov projection emphasis strong poles weak direction absent reduced order model converge zx delayed 
summary real interpolation point tends yield broader convergence true frequency response 
multiple interpolation points better understand strengths weaknesses single interpolation point appropriate turn general rational interpolation problem 
goal remains rapid efficient convergence accurate reduced order model 
combinations interpolation points real imaginary complex achieve goal 
tend concentrate purely real imaginary points keep development manageable 
proposed placement strategy favored previous examples dissertation logarithmically space interpolation points min max real axis 
large spacing real points appropriate due broad convergence regions real interpolation points discussed section 
interpolation point order frequency magnitude recommended 
large class problems strategy involves single interpolation point 
general small number interpolation points required technique 
strategy appropriate cost factorizing gamma oe large 
real interpolation points preferred rational lanczos algorithm allows small bandwidth drawback real interpolation points previously mentioned difficulties lightly damped poles sharp peaks frequency response 
numerous iterations may required points 
spaced interpolation points imaginary axis second possibility 
rapid convergence frequency response expected strategy neighborhoods centered oe linear logarithmic spacing utilized adjust behavior frequency range problem 
accurate model expected sufficient number imaginary interpolation points value number rarely known priori 
underestimating number required imaginary interpolation points lead convergence away oe frequency response peaks due weak eigenvalues imaginary axis may untouched peaks located away preset imaginary interpolation points 
problems may arise local convergence regions oe readily overlap 
avoiding problems requires adaptive extra interpolation points modeling process fine grid closely spaced points 
strategy especially undoubtedly costly factorizations gamma oe involved 
possible algorithmic approaches overcoming expenses proposed chapters 
details adaptive point placement selection provided section 
course combination real imaginary interpolation points possible 
real points produce general features frequency response 
imaginary interpolation points may introduced needed capture exact behavior user specified frequency ranges 
imaginary interpolation points weigh desirable application specific features known user 
implemented approach selecting multiple interpolation points interest 
approach due leads optimal reduced order model certain sense 
shown norm inverse laplace transform gamma minimized matches values oe gamma poles reduced order model 
unfortunately values gamma interpolation proposed known priori 
algorithm proposed iteratively locating points convergence efficiency procedure questionable large scale problems 
regardless feasibility implementation approach suggest combination real imaginary interpolation points preferred single interpolation point 
point selection values interpolation points set ordering values need specified 
schemes simply alternating interpolation points iteration seen chapter 
alternating points particularly important rational lanczos algorithm promoted small bandwidth reduced order matrices 
simple scheme may wish match number moments interpolation point 
example error certain frequency range observed drop faster 
frequently information arises picture system develops modeling process 
simplest schemes adaptive control interpolation point selection may worthwhile 
logical base adaptive interpolation decisions error estimates developed chapter 
interpolation points new iterations specified grows goal reducing remaining error approximation 
various degrees effort placed controlling modeling steps error estimate ffl 
adaptive termination simplest approaches utilizes ffl specify number moments matched interpolation point 
placement ordering interpolation points alternated consecutive blocks specified prior execution 
oe utilized prescribed ordering ffl drops acceptable level frequency ranges correspond oe imaginary points example ceases oe modeling error small frequencies nearest neighbors 
approach pertinent corresponding frequency range simply runs min max case 
adaptive selection approach utilizes error estimate determine ordering oe values predetermined set interpolation points 
simply chooses interpolation point st iteration closest frequency ffl largest 
intuitively approach pleasing strives reduce maximum error step 
adaptive placement previous approach step farther adaptively place completely new interpolation point th iteration frequency ffl largest 
set interpolation points fixed priori grows process 
limit growth may choose perform multiple iterations changing new point 
iteration new interpolation point chosen rule thumb persist chosen interpolation point frequency response change iteration hm gamma hm gamma drops hm gamma hm gamma 
remain interpolation point signs stagnation arise 
manner attempts obtain full benefits interpolation point suffering costs involved moving 
note adaptive placement appropriate imaginary interpolation points real ones possess broader range suited previously mentioned adaptive selection 
example consider applying adaptive schemes find approximation rad adaptive termination section start oe odd iterations oe iterations 
stops utilizing oe interpolation point soon ffl small rad likewise oe longer ffl small rad adaptive selection section interpolation point utilized th iteration oe depends ffl gamma ffl gamma largest rad oe oe set 
adaptive placement section oe chosen gamma times frequency range ffl gamma 
largest 
adaptation increases going point selection approaches sections performance need increase accordingly 
practical theoretical reasons adaptive selection scheme may preferred simpler combination regularly alternating interpolation points adaptive termination 
assuming exact ds preconditioners infinite precision values interpolation point ordering determine reduced order model 
theory approaches sections yield similar results 
furthermore error estimate ffl adaptive selection placement approximation 
regularly alternating interpolation points section insures information matched entire frequency range 
cost standpoint care taken abandoning simpler adaptive termination scheme 
interpolation point ordering ffl section destroys banded structure rational lanczos method 
adaptive placement points section may lead large number imaginary interpolation points 
points may speed convergence large impractical algorithms chapter algorithms seen point 
comparisons compare various interpolation point placement selection techniques generate reduced order models strategies strategy 
single real interpolation point max strategy 
imaginary interpolation points sigma 
min strategy 
real interpolation point order magnitude strategy 
conjugate pairs linearly spaced imaginary interpolation points strategy 
adaptively placed imaginary interpolation points max ffl strategy real point max endorsed strategy imaginary point min quite common literature 
strategies discussed section 
strategy adaptive strategy places new interpolation point iterations examples current modeling error greatest 
strategy implementable practical situations error estimate ffl commonly available 
strategy serves rough bound best achievable error useful comparisons 
expected behavior strategy follows section 
recall real interpolation point oe tends induce course convergence approximate range oe 
oe imaginary interpolation point oe tends induce precise convergence neighborhood oe example return compact disc player considered example 
example cd subsystem approximated relates position player focusing lens inputs radial arm actuator 
frequency response subsystem shown 
rk iterations executed interpolation point strategies listed section 
specific interpolation points corresponding strategies listed table 
model reduction implemented version dual rational arnoldi insure quality approximation depended solely interpolation point placement selection 
relative errors reduced order model interpolation point strategy number iterations varies table 
error measured computed norm uniformly weighted frequency range min max rad exception single real interpolation point case strategies table perform comparably 
difficulties single interpolation point follow table interpolation point strategies example strategy interpolation point sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma delta sigma delta sigma delta sigma table convergence interpolation point strategies example strategy strategy strategy strategy strategy frequency rad frequency response example portion spectrum 
moves way origin vertical spacing eigenvalues grows proportionally horizontal spacing gradually increases 
vertical spacing eigenvalues top order drops order approaches origin 
real interpolation point vertical spacings order significant 
eigenvalues top appear separated eigenvalues imaginary parts appear large cluster origin 
unfortunately strategy individual eigenvalues cluster dominate frequency response 
strategy emphasizes eigenvalues top 
strategy hand focuses eigenvalues nearest origin 
eigenvalues imaginary axis appear cluster interpolation point 
strategy able continue converging higher frequencies grows eigenvalues lie single path imaginary axis 
think circles increasingly larger radii centered origin 
circles negative real axis eigenvalue spectrum example grow size additional eigenvalues enclose lie higher frequencies imaginary axis 
eigenvalues large real small imaginary components induce stagnation strategy 
observe empty space lower left corner 
strategies utilize interpolation point magnitude order 
strategies share desirable convergence properties strategy 
cd player possesses eigenvalue spectrum favors low magnitude interpolation points 
example shows interpolation points near origin cause difficulties problems 
example example consider problem introduced example 
example strategies section utilized dual rational arnoldi iterations 
specific interpolation points strategy listed table 
convergence results strategies table 
frequency range interest spans order magnitude single real interpolation point appropriate strategies 
example strategy single imaginary interpolation point suffers example 
understand occurrence consider portion spectrum appears 
note axes scaled hz easy comparison frequency response 
cluster eigenvalues near origin leads stagnation observed second column table 
imaginary interpolation points sigma strategy emphasizes cluster origin line eigenvalues running imaginary axis 
line eigenvalues dominates frequency response note correspondence locations spikes 
weak times eigenvalues easily captured strategies particularly numbers 
complicated strategies show speed ups single real point strategy unreasonable economical implementation 
fact strategy carried iterations problem due memory limitations machine executing algorithm 
negative imaginary axis pi scale eigenvalue spectrum example table interpolation point strategies example strategy interpolation point sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma sigma table convergence interpolation point strategies example strategy strategy strategy strategy strategy single interpolation point suitable applications 
actual location optimal point varies problem problem 
placement single point may fact little frequently known system response prior model reduction 
hand examples suggest systematically placed interpolation points insure fast convergence variety situations 
strategy example provides robust competitive convergence relative single point strategy attempted problems 
adaptive strategies section results mixed 
convergence improvements factor times observed adaptive placement selection 
convergence improvements tend accompanied significant increases cost 
accuracy adaptations tends limited quality error estimates ffl 
research needed area 
true value adaptation may realized methods avoid sequential implementation exact ds preconditioners 
alternatives exact case studied chapters 
chapter parallel rational interpolation significant amount high level parallelism appears exist rational interpolation 
multiple interpolation points corresponding krylov subspaces involved 
treating points concurrently significantly reduce time required construct reduced order model 
possible complications arise bi orthogonalization generation approximation interesting version rp algorithm devised avoids difficulties provides impressive preliminary experimental results 
overview way enhance performance krylov model reduction execute algorithm multiple processors 
breaks algorithm portions treated parallel fashion 
interaction typically required sub portions communications kept minimum 
types exploitable parallelism model reduction projection 
parallelism exists various degrees basic matrix operations matrix vector products matrix factorizations second type parallelism arises fact column spaces projection matrices composed multiple krylov subspaces differentiated choice oe hopes concurrently construct subspaces making unions accordingly interpolation points scattered processors 
theory second type parallelism combined basic matrix operations 
strategy levels parallelism results 
needs utilize levels meet memory requirements imposed larger problems 
section structure column spaces exploited 
reader referred parallel versions basic matrix operations 
corresponding assignment distinct interpolation point processor assumptions required 
assume sufficient memory exists store sparse factorizations gamma oe interpolation points 
distributed network processors assume memory local processor store sparse factorizations corresponding assigned interpolation point 
second reasons scalability allow number interpolation points meet exceed number moments matched interpolation point 
interpolation points treated parallel 
matching higher moments large point sequential procedure parallelism available matrix operations 
assumption tends invalidate parallel version rl algorithm section 
large rational lanczos leads large bandwidth long recurrences eliminates rl method edge dual rational arnoldi algorithm 
assumption large runs contrary tendencies rest dissertation 
proposed parallel approach interpolation points utilized capture dynamic behavior locality 
highly parallel implementation appears suited imaginary real interpolation points recall section 
moderate parallelism processors combinations real imaginary shifts appropriate consider case section 
bases individual subspaces unions bases gamma oe gamma gamma oe gamma gamma oe gammat gamma oe gammat constructed concurrently 
consider case interpolation point assigned processor 
matrices constructed th processor yield bases subspaces evaluated oe completion projection matrices stored processors 
unfortunately simple approach incomplete 
reduced order model computed requiring interaction processors 
related limiting may place orthogonality conditions orthogonalization requires processor access entire matrices 
broadcasting dense columns processors significant communication bottleneck 
parallel dual rational arnoldi algorithm parallel variants sided rational arnoldi method developed eigenvalue problem 
techniques trivially extended dual case cover details 
summarize factorizations gamma oe multiple points scattered processors 
step algorithm new directions column spaces concurrently constructed 
achieve orthogonal bases processors needs access existing directions mentioned amount communication involved orthogonalization undesirable 
parallel rational power algorithm avoid orthogonalization associated processor interactions turn rational power krylov method proposed section 
rp method orthogonalization requires numerous interpolation points 
qualities suited parallel implementation 
particular computed processor interactions orthogonalization avoided 
significant processor interaction may required form ev av consider example avm formed columns scattered processors 
matters significantly simplified processor automatically knew entire matrix start execution 
matrices generated parallel 
processor compute columns avm evm correspond columns possessed 
communication required form automatic knowledge communications possible form prespecified manner known processors 
prespecified matrix random elements example processor directly access simply knowing seed value random number generator 
similarly contains columns identity matrix processor trivially generate 
course versions fail satisfy desired rational interpolation form 
moment matching sided satisfies leads reduced order model matches moments 
specify ease computation approximation accuracy 
moments form loss model reduction quality practice harmful fear 
arbitrary matches moments insures reduced order model converges steps 
examples demonstrate convergence sided tends reasonably competitive previous sided algorithms 
sided approach highly parallel 
avoiding communication parallel rp algorithm carefully assign interpolation points processors 
interpolation point placement crucial load balancing making sure amount performed processor fairly identical 
interpolation points different processors close processors compute nearly duplicate directions wasted 
processors efficiently utilized 
hand insufficient number processors assigned frequency range large modeling error exists error gradually declines iterations required single processor advantages parallelism lost 
load balancing placement multiple interpolation points fundamental rp method 
recall section rp algorithm relies frequent changes interpolation points small introduce sufficient information luckily rp approach placed interpolation points consistent load balanced parallel implementation 
effectively address point placement suggest combination techniques proposed previous chapters 
error estimate complementary models section 
determining appropriate locations interpolation points requires sense dynamics original system 
model reduction proceeds error estimate utilized obtain sense important behavior absent model 
comparison complementary models particularly appropriate parallel algorithms construct different viewpoints 
concurrently 

adaptive point placement section 
difficult impossible quantify convergence behavior reduced order model priori 
insure processor contributing new information step old interpolation points discarded convergence occurred new interpolation points introduced modeling error remains large 
decisions error estimate 

dependency postprocessing section 
adaptive point placement error estimates rp algorithm may occasionally compute nearly redundant information 
postprocessing singular value decomposition employed remove redundancies projection matrices 
algorithm combines techniques 
algorithm computes directions points updates set interpolation points updated error estimate returns step needed 
computation new directions may executed parallel 
steps include matrix factorizations reduced order model updates 
note complementary reduced order models computed 
parallel rp algorithm completely replaces set interpolation points iteration outer loop practice want interpolation point iterations reduce required 
moment value frequency response matched interpolation point 
outer iteration error estimate computed comparison distinct reduced order models 
mainly difference gamma 
computed points frequency range min max scaled version difference discrete probability distribution function random placement set interpolation points 
probability new interpolation point located frequency proportional degree modeling error estimated frequency 
operations outside dependent loops require communications processors sequential operations 
communications require messages length practice sequential involves operations 
communications sequential operations involving order quantities avoided 
communications sequential operations occur times value expected small 
algorithm designed clarity details intentionally overlooked 
amount effort associated quite suggested 
need compute send entire matrices outer iteration 
leading minors matrices remain iteration 
bottom rows rightmost columns need algorithm rational krylov parallel rp version initialize ffl min 
max choose oe randomly min max ffl distribution gamma oe gamma kv gamma lk av gamma lk ev 
gamma lk av gammak 
gamma lk ev gammak send 

processors postprocess approximations rank deficiencies compute ffl gamma 

min gamma max 
min max send ffl processors treated outer iteration 
output input vectors handled similarly 
effort involved vectors minor compared forming communicating details implemented parallel rp algorithm 
edit ffl interpolation point chosen 
new interpolation point placed set ffl zero near proximity point 
small trick nearly duplicate interpolation points avoided 
implementing optional ffl modification require executed sequentially real concern step simple 
second outer iteration terminated new interpolation points cease chosen ffl sufficiently small 
third treatment complex quantities algorithm nebulous 
practice augmented directions gamma oe gamma bg gamma oe gamma bg incorporate oe complex conjugate reduced order model 
reliability proposed parallel methods questioned point 
terms random nonorthogonal exactly inspire confidence approach 
rigorous understanding method convergence practice claimed 
rely generality theorem impressive initial experimental results 
tests sequential quasi parallel version parallel rp algorithm examples 
matlab code tests may appendix stress reduced order models generated quasi parallel algorithm exactly equal formed true processor machine 
point goal obviously indepth analysis approach efficiency execution time desired neighborhood sparse linear system solves 
simply emphasize potential approach 
examples details overlooked 
random generator seed set zero test 
second plots associated case utilize dotted line denote true frequency response dashed line denote frequency response reduced order model solid line denote error estimate ffl 
third expected reduced order models augmented additional states corresponding interpolation points complex conjugates outer matlab iteration 
actual model size denoted grow quite quickly dependent directions corresponding singular values relatives sizes gamma discarded svd postprocessing section 
example problem reconsider packaging interconnect example 
quasi parallel rp algorithm executed outer iterations 
results iteration shown figures 
quality models error estimates high 
accuracy th order model competitive results sequential dual rational arnoldi algorithm 
dual ra approach matches twice moments dual ra aggressively utilize multiple interpolation points 
example problem reconsider cd player subsystem example 
results iterations displayed figures 
model accuracy comparable dual ra algorithm 
error high frequencies due primarily measure norm modeling error shortcoming parallel rp method 
example problem reconsider model example 
results iterations displayed figures 
reduced order model accuracy parallel rp algorithm superior previously seen dual ra results 
large number spikes problem frequency response rapidly terms model size large number interpolation points 
interesting note number discarded projection directions large degree convergence significant model size changes 
difference original reduced order models longer large readily support multiple new directions additional interpolation points 
parallel model reduction algorithm involving random projection matrix nonorthogonal projection matrix proposed 
random reduces number moments matched reduced order model 
random allows parallel construction reduced order model minimal communications 
long satisfies moments matched 
error estimates adaptive point placement aimed acquiring efficient balanced convergence 
avoiding gram schmidt recurrences construction parallelism increased 
lack orthogonality lead dependences problems apparently overcome numerous interpolation points svd postprocessing reduced order model 
study svd postprocessing required see chapter results examples noteworthy 
frequency magnitude approximate frequency response example frequency magnitude approximate frequency response example frequency approximate frequency response example frequency approximate frequency response example frequency approximate frequency response example frequency approximate frequency response example frequency approximate frequency response example frequency approximate frequency response example frequency approximate frequency response example frequency approximate frequency response example chapter approximate rational interpolation power model reduction techniques previous sections arises primarily construction bases exact ds preconditioned krylov subspaces 
doubtful bases generated stored size complexity original system grows 
example direct sparse factorizations matrix pencil arising dimensional discretization frequently impractical 
chapter considers approaches reducing computational effort required rational interpolation 
explicit implicit approximations gamma oee gamma incorporated projection subspaces 
exact relations chapters lost reasonable results possible 
efforts represent attempts relax requirements ds preconditioners krylov algorithms order reduce 
chapter starting point final solution reducing computational effort model reduction 
conversions approximate solves construction exact rational interpolant requires exact ds preconditioners subspaces 
lemmas subspaces take form colsp fv gamma oe gamma gamma se gamma oe gamma colsp fzg gamma oe gammat gamma se gamma oe gammat gamma oe gamma exact ds preconditioner choice superfluous 
emphasizing iterative solution dual systems equations recall section suggests need find acceptable approximate solutions frequency range interest 
adopting view abandoning strict moment matching conditions inexact ds preconditioners appropriate 
replace gamma oe gamma approximations 
adaptation rk algorithm utilizing approximations gamma oe gamma algorithm 
differences algorithms 
matrix gamma oe gamma replaced phi gamma oe gamma replaced gamma 
scalar corresponds fixing value 
noted section choice relevant exact ds preconditioner case due shift invariance property krylov subspaces 
previously assumed exact cases computational sense simply replace gamma se think allowing grow larger 
algorithm rational krylov approximate general version initialize fl gamma fi gamma input oe interpolation point th iteration vm phi pm phi pm fl vm vm gamma vm gamma vm fi gamma zm gamma gamma vm wm gamma fl gamma qm fi wm wm gamma wm wm generated approximate rk algorithm longer form bases unions krylov subspaces lemmas carry inexact ds preconditioning case projection matrices lead rational interpolants 
reduced order model formed approximately ds preconditioned satisfy petrov galerkin constraints meeting petrov galerkin constraints restricted moment matching theorem holds 
long reasonable approximations acquired reduced order model achievable 
interesting note exactly ds preconditioned necessarily lead optimal approximations rational interpolation leads exact ds preconditioners optimally suited discrete points interpolation points 
frequencies away interpolation points uncertain gamma oe gamma necessarily better ds preconditioner gamma oe gamma new parameters phi appear algorithm specified 
denote phi ds preconditioning operator th iteration 
operator phi approximates action gamma oe gamma vector 
phi matrix assumed past member set fp pk approximation gamma oe gamma choice consistent previous notation regarding ds preconditioner 
numerous possibilities exist finding fixed ds preconditioner approximates gamma oe gamma 
typically constructs sparse matrix utilized approximate action gamma oe gamma incomplete lu approach example approximate sparse lu factorization gamma oe computed 
elements appearing factorization correspond certain level fill possess sizes certain tolerance dropped 
second technique approximate inverse approach 
constructs sparsity pattern gamma oe gamma minimized respect norm frobenius norm 
alternatively generally think phi operation takes vectors pm pm outputs vectors vm phi represent iterative system solver computes approximate solutions equations gamma oe pm gamma oe pm note exact ideal solutions underlined 
vector computed approximation indicated standard fashion underlining 
iterative solver utilized phi represents nonlinear operation longer associated fixed matrix iterative solvers implicitly perform ds preconditioning common linear solver literature 
methods type known inner outer iterations 
outer iteration constructs search subspace solution original problem 
case outer iteration constructs projection matrices outer step entire loop inner iterations executed necessarily number steps time generate ds preconditioner current outer step 
case inner iteration consists iteratively constructing approximate solutions 
operator phi implicitly defined inner iteration 
different iterations possible constructing approximations vm gamma oe gamma pm gamma oe gammat pm inner iterative solvers generally krylov methods 
approximate solutions vm typically chosen inner krylov subspaces petrov galerkin minimal residual constraints 
examples iterative approaches satisfying respective constraints biconjugate gradient bicg residual qmr methods 
particular methods mentioned sided simultaneously generate approximation solutions dual systems equations involving gamma oe gamma oe noteworthy sided approach generalized minimal residual gmres method 
new parameter interest approximate rk algorithm phi inexact ds preconditioner specific choice contributes specification column spaces 
exact case gamma replaced convenience 
denote exact case certainly compute gamma 
inexact case possible tune improved results 
possible choice interpolation point oe motivation selection matching moments oe closely connected solving dual equations oe traditional subspaces involved solving system equations fixed oe oe second motivation choice follows mapping eigenvalues eigenvalues phi gamma 
mapping case phi exact 
mapping sought desired eigenvalues stand spectrum ds preconditioned matrix 
properties exact ds preconditioner drove desired poles outer edge spectrum 
example mapping oe case displayed 
mapping studied great extent eigenvalue literature technique known davidson method 
transforms eigenvalue near oe position spectrum phi gamma oe close origin 
behavior follows fact eigenvalue zero eigenvalue phi gamma matrix phi hand phi approximation gamma oe hoped eigenvalues phi gamma oe close 
mapping occurs desired eigenvalues near oe mapped origin separated cluster 
unfortunately argument choice oe breaks phi approximation gamma oe gamma case phi gamma oe identity eigenvalues mapped top 
furthermore multiplication old directions identity contributes new information subsequently computed directions 
experiments examples indicate choice lead significant oftentimes unpredictable differences convergence reduced order model 
practice tuned oe available information ds preconditioner quality convergence behavior previous solves 
alternatively sophisticated approaches implementing approximate solvers discussed section de emphasizes approximate solve algorithms choices phi trivial straightforward discussions adapt algorithms chapter approximate system solves 
eigenvalue mapping oe required repeat modifications seen approximate rk algorithm 
approximate dual ra algorithm algorithm 
long orthogonality nearly maintained convergence steps guaranteed approximate dual ra version 
course effective ds preconditioners utilized obtain effective model size approximate rational lanczos rk variants seen chapter rational lanczos algorithm relies properties exact ds preconditioner 
reliance efficient algorithm section approach questionable inexact ds preconditioning 
rl algorithm relied exact ds preconditioning theorem achieve banded matrices 
approximations gamma oe gamma utilized rl algorithm reduced order systems matrices dense 
nonzero diagonal terms computed error incurred 
computing elements corresponds full length biorthogonality recursions algorithm rational krylov approximate dual ra version initialize vm phi phi vm phi gamma vm gammak phi gamma gammak vm vm gamma vm gamma gamma vm gamma zm gamma gamma vm vm kv mk kz length recursions 
approach dealing approximate ds preconditioning increases cost approximate rl algorithm levels comparable dual ra approach 
approximate dual ra algorithm numerically reliable version rl algorithm little value 
apparent option dealing approximate rl algorithm simply ignore error phi gamma oe gamma case simply edit rl algorithm 
approximate rl method algorithm 
banded assumed algorithm formed fi terms length recursions 
error theoretically unsupported approach characterized theorem 
theorem output residual expression approximate rl algorithm fi wm gamma gammat oe gamma gamma gamma gammat algorithm rational krylov approximate banded rl version initialize phi vm phi gamma vm vm gamma gamma max gammak gamma fl fl vm wm wm gamma gamma max gammak gamma fi fi wm vm vm fl fl vm wm wm fi fi sign vm fl wm phi wm th column wm gamma gamma oe phi wm residual associated approximate system solve th iteration 
proof due residual definition approximate system solution th iteration written phi wm gamma oe gammat wm gamma multiplying expression left gamma oe gamma oe wm gamma shift se yields equivalent expressions gamma se wm gamma oe gamma wm gamma oe gamma wm wm fi expression involves relations 
placing expressions iterations produces matrix equality gamma se gamma wm 
fi wm 
fi oe gamma 
oe gamma due definitions expression gamma se gamma oe gamma fi wm gamma equality substituted acquire output residual expression fw gamma oe gamma fi wm gamma rg gamma gamma fi wm gamma gammat oe gamma desired expression follows trivially definition rl algorithm 
exact case output residual resulting rl algorithm scaled version wm vector wm stays biorthogonal directions scaling wm drops zero regions interpolation point 
approximations gamma oe gamma employed output residual corrupted error phi residual associated approximation vector gamma oe gammat wm appears things noted concerning corruption 
corruption proportional error phi 
exact case residual approximate rl generally forced zero regions interpolation point 

errors computation gamma oe gamma th iteration continue appear reduced order models iterations entire matrix appears 

total corruption behaves sum product previous errors computation gamma oe gamma matrix appears matrix vector product 
fact news exactly ds preconditioned versions rl algorithm implemented finite precision 
machine level precision errors linear system solvers blown steps 
summary significant errors gamma oe gamma phi appear acceptable iteration approximate rational lanczos algorithm 
limited numerical experience supports result 
approximate rational power methods dual ra algorithm appropriate approximate version rational power algorithm follows readily modifications seen algorithm approximate rk algorithm 
appears little value approach advantages dual ra approach sided rp method remain 
collapse elegant rational lanczos theory approximate case loses low memory short recursion projection technique 
dual ra approach reliable storing mn elements may exceed available memory 
fact especially true approximate solves utilized expects model dimension grow slightly compensation solve inaccuracies 
approach requiring storage achievable sided rp method 
turn idea prespecified matrix section 
completely known prior columns associated columns computed full knowledge rp approach incorporate orthogonalization 
th step sequential approximate rp algorithm computes vm phi gamma vm phi oe new interpolation point 
scaling vm computes entire th column am em avm evm low order columns acquired vm computed vectors vm discarded memory 
length vectors required storage iteration 
prespecified matrix meet conditions approach successful 
matrix size theta known ahead time size great proposed low memory rp approach parallel version section th columns am em computed entirety th iteration 
computing entire column iteration requires knowledge immediately prefer specify final value reduced order model size priori hopefully adapted complexity dynamic system 
avoid priori specification choose initial larger estimate algorithm 
conservative guess prior model reduction choose times 
course significantly larger eventual true value wasted 
iteration computes avm evm fact avm evm eventually needed final reduced order pencil cut size theta theta 
reason product vector relatively cheap operation wasted expensive 
additionally hoped compactly stored available entirety 
long satisfies value leads reduced order model matches moments 
section suggested simply choose random random compactly stored matrix vector product involving mn operation 
reason consider going composed random integers sparse 
choosing columns identity matrix leads extremely low storage computations 
sparsity patterns better suited structure problem may improve convergence 
appropriate choice topic additional research 
algorithm implementing sided approximate rational power method algorithm 
values algorithm kept small 
attempt reduce dependencies columns dependencies may arise require singular value postprocessing discussed section 
small correspondingly large significant concern approximate version 
approximate rp algorithm complete factorizations longer computed interpolation point 
iterative linear system solvers example may significant cost advantage staying interpolation points 
algorithm rational krylov approximate rp version initialize vm phi vm phi gamma vm gamma vm vm mk am avm evm implementation algorithm requires approximate solve product mat vec mat vec mat vec hopefully chosen way reduce expenses 
sided rp algorithm cuts cost avoiding operations involving iteration divided half 
assuming chosen compact storage phi involves sparse operations memory required algorithm mm elements 
value represents order reduction versus approximate dual ra algorithm 
comparisons successful approximate solves model reduction techniques chapter trivial task 
theory governing determination modeling error interpolation points longer exact 
fast implementations may longer elegant short recursions alarming debate successful iterative methods preconditioners generating approximate solutions far settled numerical linear algebra literature 
determining right level preconditioning fixed system linear equations oftentimes task involving trial error 
possible payoff approximate solutions great 
remainder section impact approximations model reduction moderately sized problems considered 
breakdown lanczos type approaches approximate solves demonstrated 
show approximate rational interpolation possible exact matrix factorizations 
initial insights working approximate solves related 
problem considered examples arises discretization partial differential equation pde function time vertical position horizontal position 
boundaries interest problem lie square opposite corners 
function zero boundaries 
pde discretized centered difference approximations grid theta points 
discretization grid shown 
state space equation dimension results discretization 
sparsity pattern resulting matrix shown 
input vector system corresponds composed random elements 
output vector system equated input vector reason simplicity 
second test problem consider interconnects running parallel horizontally coupled mutual inductance vertically 
example parallel interconnects consisting segments resistors capacitors 
elements interconnects randomly generated 
variance resistance values variance capacitance values gamma variance inductance values gamma input circuit current source leftmost segment top interconnect 
output circuit voltage rightmost segment bottom interconnect 
system mna equations dimension gamma result 
quantities state vector node voltages remaining ones currents 
sparsity pattern resulting se gamma matrix shown 
sparsity pattern examples obtained matlab 
example example explores impact errors linear equation solutions various model reduction implementations 
specifically equations form gamma oe gamma oe wm arise various rk implementations solved assorted degrees numerical precision digits accuracy 
variations solver accuracy different model reduction implementations considered dual ra real interpolation point rl real interpolation point dual ra imaginary interpolation points rl imaginary interpolation points dual ra numerous adaptively placed imaginary interpolation points 
combinations model reduction algorithms solver accuracies applied discretization theta grid 
frequency response discretization 
plots modeling error resulting dual ra iterations real shift solver accuracy varies 
line corresponds digits solver discretized pde grid theta case row discretized pde sparsity pattern theta case coupled interconnects theta case row mna sparsity pattern coupled interconnects theta case frequency hz frequency response example iterations finite precision dual ra results example digits digits digits digits precision line corresponds digits precision line digits line digits 
quick analysis plots shows convergence reduced order model tends level related solver precision nonlinear fashion 
noise level reached case continued iterations stagnation period add random directions directions particularly suited reducing modeling error 
stagnation continues number iterations nearly modeling error guaranteed drop machine precision orthogonal maintained cases orthogonality maintained limit machine precision 
dual ra model simply different realization original model 
shown line drops sharply zero gamma 
results model reduction algorithm varies tables 
table linear equations model reduction algorithm solved different degree numerical precision 
various dual ra implementations continue converge reasonable fashion degree numerical precision drops 
course difference columns due varying interpolation point schemes 
numerous imaginary interpolation points tuned modeling error consistently yield best results 
performance single real interpolation point imaginary points acceptable dual ra significant levels solver noise 
results rl algorithms columns tables 
significant digits precision table rl results slightly inferior rational arnoldi implementations 
slight discrepancy consistent rl approach dependence biorthogonality recall example 
convergence rl approaches rapidly worsens fewer digits accuracy achieved linear equation solutions 
fact convergence rl results level immediately fewer digits accuracy maintained tables behavior consistent observations section table convergence digits precision example modeling error dual ra dual ra dual ra rl rl real oe imag oe adapted oe real oe imag oe table convergence digits precision example modeling error dual ra dual ra dual ra rl rl real oe imag oe adapted oe real oe imag oe table convergence digits precision example modeling error dual ra dual ra dual ra rl rl real oe imag oe adapted oe real oe imag oe table convergence digits precision example modeling error dual ra dual ra dual ra rl rl real oe imag oe adapted oe real oe imag oe numerical errors rational lanczos dissipate additional modeling iterations 
errors rl approximations continue large surpasses example repeated experiments previously described interconnect problem 
frequency response system displayed 
convergence dual ra implementation real shift plotted different levels solver precision 
line corresponds digits solver precision line digits line digits line digits 
results plotted iterations problem 
lines easily distinguishable 
high solver accuracy digits precision convergence gradual problem dynamics easily captured single interpolation point 
solver accuracy digits sufficient reproduce gradual convergence 
results figures suggest level solver accuracy st iteration need consistent degree model convergence iterations 
relation current model error required solver accuracy th iteration exist clearly nonlinear 
characterizing minimal level solver accuracy near ideal reduced order model convergence interesting question requiring investigation 
example consider convergence behavior various rk implementations 
dependence behavior solver precision indicated tables 
important feature plots stagnation rl implementations solver accuracy diminishes 
case digits precision probably closest reality actual approximate solution techniques implemented rl approaches completely useless 
noted results various ra implementations barely affected variations solver precision 
practice loss numerical precision solver follows naturally approximations existing inexact ds preconditioners inner solver iterations 
frequency hz frequency response example iterations finite precision dual ra results example digits digits digits digits table convergence digits precision example modeling error dual ra dual ra dual ra rl rl real oe imag oe adapted oe real oe imag oe table convergence digits precision example modeling error dual ra dual ra dual ra rl rl real oe imag oe adapted oe real oe imag oe table convergence digits precision example modeling error dual ra dual ra dual ra rl rl real oe imag oe adapted oe real oe imag oe table convergence digits precision example modeling error dual ra dual ra dual ra rl rl real oe imag oe adapted oe real oe imag oe regardless exact source inaccuracy approximate dual ra approach lead valid reduced order models 
fact demonstrated examples larger versions problems interest 
results experimentation preliminary intended reflect final effort incorporating approximate solves 
merely demonstrating exact factorizations gamma se required apparently novel result 
example consider discretization pde theta grid 
grid leads matrix dimension nonzero elements 
approximate dual ra algorithm applied problem real shift 
systems linear equations approximately solved gmres method 
inner gmres iteration inner preconditioner formed method 
preconditioner applied prior execution gmres method 
operator phi combines fixed inner preconditioner iterative solver 
results dual ra iterations gmres inner iterations table 
total inner iterations take place case second iterations involve matrix vector products different values oe table 
note oe case performs ds preconditioner poorer fewer gmres steps unacceptable ds preconditioner accurate 
opposite behavior occurs gamma replaced best results obtained second column results chosen gmres iterations column reasonably 
example consider parallel interconnects consist segments 
problem size matrix nonzero elements matrix nonzero elements 
approximate dual ra algorithm utilized ds preconditioner phi speed convergence reduced order model outer iteration range frequencies 
inner preconditioner utilized inner solver iteration speed determination solution 
model reduction fifteen gmres inner iterations oe example vary inner preconditioner inner iteration 
fill allowed inner preconditioner case 
results modeling iterations table 
amount inner preconditioning critical problem significant differences result moderate changes phi computing reasonably accurate solutions dual system equations demands ds preconditioners 
iterative krylov techniques involved solution inner iteration inner preconditioner example 
felt improved inner preconditioners inner iterations crucial successful implementation faster approximate model reduction method section 
relating inner outer recursions prior portions chapter generation phi considered independent subproblem model reduction procedure 
choice phi depends properties logical utilize piece information linear system solves 
search constraint subspaces iteratively generated past model reduction steps provide useful information consider computation direction outer modeling iteration 
ideally solves equation gamma oe gamma pm pm exactly 
possible sort approximation required 
reduced order model provides guess solution vm am gamma oe em gamma gamma pm pm initial guess finds approximate solution column space vm satisfies petrov galerkin constraint respect zm projection associated table convergence approximate solutions example modeling error oe gmres steps gmres steps gmres steps gmres steps table convergence approximate solutions example modeling error model reduction generate initial guess lead better inner solver results simply choosing random vector vector zeros 
sense existing trying solve scratch 
unfortunately 
purpose solving compute new direction vm approximate solution lies entirely existing directions vm acceptable 
initial guess improved 
correction accomplished incorporating initial guess form problem gamma oe gamma gamma pm pm gamma gamma oe approximation vm follows approximately solving gamma oe gamma pm pm gamma gamma oe adding result 
computed vector um update hopefully leads better approximation vm um update fact new direction introduced vm vector um satisfies colsp ae vm vm oe colsp ae vm um oe lies column space vm update approximated um phi gamma pm pm gamma gamma oe phi approximates gamma oe gamma phi fixed ds preconditioner iterative solver preconditioned iterative solver 
rightmost term incorporates existing information outer modeling iteration approximate solve 
try avoid recomputing known information formation um computation um simplified rewriting initial guess vm am gamma oe gamma gamma oe pm oe gamma pm vm am gamma oe gamma ev pm pm oe gamma pm vm am gamma oe gamma ev pm expression initial guess update um oe gamma pm phi ev pm gamma oe gamma pm phi gamma oe vm am gamma oe gamma ev pm oe gamma pm phi fev pm gamma gamma oe vm am gamma oe gamma ev pm important things note 
update um consists vector phi ev pm naive approximation ideal new direction gamma oe gamma ev pm correction vector phi gamma oe vm am gamma oe gamma ev pm incorporates information existing reduced order model 
phi nears gamma oe gamma vector phi ev pm nears ideal direction correction vm am gamma oe gamma ev pm contained colsp irrelevant 
hand reduced order model accurate fortunate event vm gamma oe gamma nears gamma oe gamma update um small 
second important feature parameter pm comes play scaling 
scalings matter constructing subspaces unscaled vector phi ev pm gamma gamma oe vm am gamma oe em gamma ev pm perfectly appropriate new direction augmenting vm phi may exact ds preconditioner choice starting guess leads new direction independent second way exact ds preconditioning generating directions independent evaluation point side note claimed derivation provides alternative path obtaining davidson method eigenvalue problem 
return topic section 
example discretized pde example solved improved initial guess modeling iterations data table result 
improved starting vector leads convergence slightly improved best cases example 
best results example required careful choices issue longer concern 
results example independent table improved starting vector results example modeling error gmres steps gmres steps approach outer modeling projection subspaces aid solution linear systems equations 
thrust approach iteratively solve equation gamma vmv gamma oe gamma gamma pm pm 
approximate solution vm iteratively constructed subspace gamma vmv gamma oe gamma gamma pm pm orthogonal outer search subspace vm manner effort wasted computing vm directions vm dual approach obtained concept keeping inner iterations orthogonal outer developed solving fixed systems linear equations 
modifications davidson method exist eigenvalue problem keep vm orthogonal initial guess 
drawback approaches 
entire outer matrices available utilized iteration 
utilization increases costs prevents fast iterations suggested section 
course compromises possible involve directions dominant directions scope dissertation single completely characterize issues pertinent preconditioning initial solution guesses relations 
topics run iterative approaches applications continue active area research 
hopefully chapter demonstrated solver techniques applicable interest model reduction large scale dynamic systems 
chapter iterative eigenvalue solvers chapter focuses preconditioned iterative eigenvalue solvers pies class methods closely tied model reduction techniques chapters 
relations pies proposed model reduction techniques explained section 
prior discussion pies surveyed classified defined choices 
say worthwhile iterative utilize preconditioners fall proposed classification 
implicitly restarted arnoldi algorithm example notable approach eigenvalue computations 
section assumed problem solved standard symmetric concentrating commonly occurring problem allows straightforward eigenvalue themes 
extensions developed techniques general pencil gamma oftentimes straightforward comprehensive treatment symmetric problem significant task 
existing techniques approaches solving generalized problem ax developed dimension problem small 
computing eigenvalues eigenvectors large sparse typically difficult task day 
elegant relatively inexpensive classical iterative methods arnoldi lanczos oftentimes fall short particularly target interior portion spectrum 
result numerous authors apparently davidson developed significant modifications extensions arnoldi lanczos methods 
part modifications incorporate preconditioning fashion yield improved rates convergence 
nearly pies implement rayleigh ritz procedure respect subspace transformed matrix ta 
previous terminology search subspace corresponding orthogonal projection 
acquired eigenvector approximations lie search subspace 
approximate eigenvalues acquired low order pencil tay ty column space ir thetam model reduction eigenvalues tay ty depend selection fact standard forms characterized defined choices 
assuming single eigenvalue corresponding eigenvector desired column space typically takes form colsp fy gamma gamma eigenvalue literature 
matrix ym contains columns column specified user typically chosen random vector vector 
matrices pm play role varying es preconditioners es stands eigenvalue solver playing role ds previous chapters 
vector dm eigenvector approximation iterations 
approximation column space sought 
similar model reduction problem approximation arises low order approximation 
assume dm eigenvalue perceived closest reduced order eigenvector dm corresponding leads approximation dm approximate eigenvector dm satisfies galerkin type condition gamma dm dm tm gamma dm ym dm residual associated eigenvalue estimate dm pies iteration augmented multiplying previous eigenvector approximation ym gamma dm gamma pm gamma pm gamma oe gamma scalars oe pm gamma selected attempt emphasize desired vector new direction pm gamma ym dm specifics pm gamma considered detail section 
choice oe closely related specification model reduction interpolation point chapter 
preconditioned matrices matrix pm gamma appeared model reduction methods previous chapters plays central role 
model reduction considering case pm exact es preconditioner gamma oe gamma eigenvalues pm gamma recall lemma gamma gamma oe going pm gamma mapped recall section eigenvalues mapped separated positions spectrum pm gamma converge rapidly 
pies attempt choose oe map separated position map rest eigenvalues tight cluster 
mapping achieved multiplication pm gamma separates rest cluster 
parameter oe chosen estimate desired eigenvalue pies 
manner recall role oe interpolation point portion complex plane oe emphasized 
parameter typically chosen acquire mappings case oe case 
derivation mappings model reduction 
case relies properties gamma oe gamma map desired eigenvalue extreme outer edge spectrum 
oe case relies heavily properties gamma oe map desired eigenvalue origin 
surprising learn choice matter pm exactly gamma oe gamma result follows lemma consistent independence pm gammaoe gamma mappings pm gammai equally terms resulting eigenvalue separation 
important pm exact 
choice oe favored exact pm literature apparently mapping relies gamma oe gamma pies scalars oe updated iteration reflect approximations advantage fixing values pm start algorithm initial estimate approach column space krylov subspace colsp fy gamma ii gamma oei gamma need computed 
symmetric lanczos type methods compute 
form reduced order pencil tay ty implicitly generated lanczos type algorithm 
practice lanczos method restarted multiple times 
lanczos method executed iterations new approximations values oe updated starting vector updated process repeated 
manner oe occasionally updated 
infrequent update may slow convergence costs iteration may drop new needed iteration 
reduced order pencils matrix ym fixed pies iterations new eigenvector approximation ym dm 
vector dm depends choice transformation tm low order problem 
stressed choice tm effects eigenvectors values reduced order approximation eigenvectors values original problem 
goal choosing tm find best possible eigenvector approximation colsp reasonable amount computational effort 
possibilities tm appear literature 
strategy tm simple approach leads reduced order pencil ym 
tm trivial implement making common choice existing implementations 
ym exterior approximate eigenvalues vectors ym tend optimal approximations corresponding eigenvalues 
better approximations interior eigenvalue ym may achievable different tm 
strategy tm gamma oe gamma choice tm leads best sense possible approximations eigenvalues near possibly interior value oe observation follows relations gamma oe gamma gamma oe gamma ym gamma oe gamma gamma oe oe ym gamma oe gamma ym ym oe gamma oe gamma ym gamma oe gamma ym gamma oe gamma ym gamma oe gamma ym eigenvectors values gamma oe gamma gamma oe gamma ym directly related eigenvectors values gamma oe gamma ym ym pencil simply strategy applied matrix gamma oe gamma eigenvalues exterior gamma oe gamma correspond near oe ones best approximated gamma oe gamma ym ym tm main shortcoming approach requires exact factorization gamma oe acquire tm cost acceptable methods utilizing exact es preconditioners pm gamma oe gamma appropriate general 
strategy tm gamma oe choice proposed efficient avenue treating interior eigenvalues 
evaluate transformation rewrite terms second equivalent eigenvalue problem 
define quantities gamma oe ym gamma oe ym sx reduced order eigenvalue problem tm gamma oe rewritten gamma oe gamma oe ym gamma oe ym gamma oe gamma oe ym gamma oe gammat gamma oe gamma gamma gamma oe gamma oe gamma ym gamma oe gamma ym gamma oe gamma approximation strategy equivalent finding approximation column space ym strategy 
strategy finds best sense possible eigenvector approximations ym eigenvalues near oe eigenvector approximations lying frequently referred harmonic ritz vectors 
strategy finds eigenvector approximations ym corrupted version ym quality results may suffer 
strategy cheaper strategy exact factors inverses need treated 
limited experimentation literature suggests tm gamma oe preferred interior eigenvalues 
strategy tm tmr gamma oe gamma final approach left right transformations utilized obtain pencil gamma oe gamma gamma oe gamma ym gamma oe gamma ym strategy thought sided version strategy 
similar desirable approximations eigenvalues near oe expected cost involving gamma oe gamma 
observations follow relations gamma oe gamma gamma oe gamma ym gamma oe gamma ym oe gamma oe gamma gamma oe gamma ym gamma oe gamma ym gamma oe gamma ym gamma oe gamma oe gamma ym gamma oe gamma ym gamma oe gamma gamma oe gamma ym eigenvector approximations strategy result straightforward restriction gamma oe gamma gamma oe gamma ym common existing eigenvalue literature strategy important section 
existing implementations array pies exists literature traced back techniques 
seminal techniques method davidson 
davidson method chooses tm see strategy section oe recall section 
second technique denoted lack consistent historical title shift invert pies 
originating shift invert pies chooses tm gamma oe gamma 
original form shift invert pies kept oe fixed multiple iterations allow symmetric lanczos method 
version shift invert pies provided algorithm 
shift rayleigh ritz method relatively straightforward extension known shift invert eigenvalue iteration 
direction approximate desired eigenvector shift invert pies incorporates previously computed directions search subspace colsp computed directions kept orthogonal improved numerical stability 
scalar oe chosen eigenvalue dm gamma oe gamma gamma oe gamma ym 
practice computes eigenvalue dm gamma oe gamma ym largest magnitude uses result obtain oe dm gamma oe gamma oe kept fixed updated symmetric lanczos algorithm algorithm vm wm ym gamma oei gamma compute tridiagonal matrix gamma oei gamma arises naturally lanczos algorithm shift invert pies initialize orthogonal vector eigenvalue guess oe compute dm dm rayleigh quotient gamma oe gamma ym oe dm gammaoe ym gamma oe gamma ym dm ym ym gamma ym ym ym ky method 
largest eigenvalue tridiagonal matrix turn leads eigenvalue estimate dm gamma oe gamma shift invert pies exact es preconditioners transformation strategy section rapid convergence oftentimes occurs 
unfortunately involvement gamma oe gamma explicitly implicitly practical problems 
reason davidson method foundation currently popular pies methods 
davidson method generalizations avoid exact es preconditioners 
version davidson method provided algorithm 
noted method utilizes left transformation sets equal oe orthogonal basis search subspace formed original method davidson chose inverse diagonal gamma oe es preconditioner pm extensions approach utilize general es preconditioners 
difficulties arise davidson method pm approximation gamma oe gamma case ym approximately dm new direction obtained 
avoid difficulty perturb slightly away oe popular choice literature select oe ffi algorithm davidson method initialize orthogonal vector compute dm dm gamma dm dm ym pm ym ym gamma ym ym ym perturbation ffi dm pm gamma oe ym dm dm pm ym dm computing directly relatively cheap 
efficient implementation correction possible method jacobi 
perturbation leads new search direction pm gamma oe ffi dm orthogonal previous eigenvector estimate ym dm manner new information added iteration 
apparently variations original shift invert davidson methods categorized choices tm table attempts sort methods contained certainly papers accordingly 
point table minimize contributions listed papers emphasize central themes exist pies literature involving tm interested reader examine papers varied contributions concerning es preconditioning oe selection eigenvalue convergence 
surveys iterative eigenvalue methods available 
papers listed table tend progress chronologically left right 
noted entry second row complementary third rows 
difference follows fact row table classifying existing pies oe oe ffi tm gamma oe gamma tm gamma oe second column table tend require exact es preconditioners 
long exact es preconditioners computed powerful transformation tm gamma oe gamma easily computed case 
approaches eigenvalues traditionally eigenvalues computed block extension algorithm 
simply replaces vector ym block vectors ym ir thetak ym pm gamma ym dm dm matrix far right contains eigenvector estimates desired eigenvalues 
alternatively individually update new direction ym gamma ym dm gamma ym dm hopefully improve resulting convergence 
parameters tuned individual 
approach developed davidson method 
interesting results obtained exact es preconditioners parameters oe fixed initial eigenvector guesses identical 
case column space ym popular rational krylov subspace gamma oe gamma stated earlier initial rational krylov subspaces associated ra algorithm directed eigenvalue problem 
desired subspace constructed sided rational arnoldi algorithm 
context rational arnoldi algorithm serves multiple eigenvalue generalization shift invert pies 
algorithm sided rational arnoldi algorithm algorithm 
algorithm works compute eigenvalue approximation time corresponding outer loop constructing krylov subspace time 
converging eigenvalue parameter oe remains fixed arnoldi type method results 
note matrix included algorithm consistency rk algorithms chapter 
consistency simplifying assumptions section simply think identity symmetric matrix 
algorithm sided rational arnoldi initialize orthogonal vector set oe estimate desired eigenvalue ey ym gamma oe gamma ym ym gamma fl fl ym ym ym fl fl ky point casual reader may choose proceed section 
spend remainder section demonstrating sided ra algorithm implicitly generate pencil tay ty involving strategy transformation gamma oe gamma approach computing approximate eigenvectors novel differs proposed consistent eigenvalue approximations generated shift invert pie single eigenvalue case 
algorithm computes orthogonal corresponding subspace 
applied matrix pencil determine approximate eigenvalues eigenvectors step 
desired low order pencil implicitly formed sided ra algorithm 
behavior derivation similar seen rl algorithm sections 
starting point matrix relationship em am derived holds iterations sided ra algorithm 
due matrices em am ir thetam matrices th columns take forms fl 
fl oe fl 
fl vector th column identity matrix matter discussion scalar oe foe oe specific interpolation point th iteration 
note similarity 
defining am am gamma oe em noting results gamma oe ym em am am equality holds st row am consists zeros column upper hessenberg am just 
multiplying left gamma oe gamma yields em gamma oe gamma am gamma oe gamma gamma ym am gamma oe gamma gamma oe oe gamma am oe gamma em am am gamma em eigenvalue eigenvector am em am eigenvalue eigenvector gamma oe gamma gamma oe gamma exactly pencil sought strategy section finding approximations eigenvalues near oe th iteration desired eigenvalue approximation dm nearest oe directly acquired matrices am em constructed algorithm 
occurrence ideal trying locate eigenvalue nearest oe th iteration 
desired low order pencil oe implicitly generated th iteration doing requires vector gamma oe gamma ym available 
information fact available vector ym formed multiplying ym gamma oe gamma eigenvector approximation ym am dm arising approximate eigenvector ym em dm suggested 
provide motivation choice write residual corresponding eigenvector gamma dm ym am dm gamma dm ym am gamma oe em dm gamma oe ym am gamma oe em dm oe gamma dm am gamma oe em dm continue noting writing gamma oe ym am gamma oe em dm oe gamma dm gamma oe ym em dm gamma oe ym am gamma dm em dm lastly fact am gamma dm em dm equals zero yields residual expression ff gamma oe ym ff dot product dm row am gamma dm em 
residual results choosing approximate eigenvector ym am dm th iteration gamma oe orthogonal column space ym orthogonality consistent galerkin condition 
approximate eigenvector choice ym em dm leads orthogonality residual 
arriving pies model reduction previous chapters connections proposed model reduction techniques eigenvalue methods section 
section techniques developed model reduction considered context finding eigenvalues 
variations themes section result 
performing exercise hoped new insights modifications existing pies result eventually obtained 
simplicity model reduction problem consistent eigenvalue assumptions symmetric identity matrix considered 
assumption restrictive model reduction techniques serve starting point known general case 
key model reduction pies form search subspaces 
symmetric case model reduction subspace column space model reduction direction search subspace computed phi gamma gamma new direction pies phi gamma ym gamma dm gamma section notes new directions take equivalent forms phi constant respect interpolation point krylov subspaces result shift invert pies 
happens phi inexact varying 
particular davidson method relate model reduction 
answer questions come section relations outer modeling inner solver iterations discussed 
recall update direction derived model reduction phi pm gamma gamma oe vm am gamma oe gamma pm vector update direction results outer iteration generate initial guess inner solve 
oe approaches eigenvalue dm set equal davidson method vector am gamma oe gamma pm orthogonal eigenvector dm times large scaling factor 
due large scaling dm phi pm term drops left new direction davidson method 
davidson method inner outer type iterative method implicitly computes phi gamma gamma utilizing initial solver guess outer iteration 
problem arise davidson method phi exact recall section 
case vector phi pm yields ideal new direction dropped davidson method 
corrections problem utilize orthogonality existing outer subspace computed new direction avoid difficulties 
noted section approach way relating inner outer iterations 
point difference model reduction eigenvalue subspaces choice vector subspace 
pies typically choose vector say starting vector column model reduction phi think simply improved guess starting vector gamma oe gamma better approximation desired eigenvector supposedly better starting guess assuming fixed shift selection eigenvalues ay av identical 
alternative description differences 
consider simplest case column space km gamma oei gamma gamma oei gamma column space km gamma oei gamma 
trivially equals gamma oei gamma low order pencil model reduction gamma gamma oei gamma gamma gamma oei gamma right side low order pencil model reduction result strategy approach section generating reduced order pencil 
expression hold exactly event inexact es preconditioners 
illustrate minor differences exist different strategies section 
chapter final chapter results previous chapters emphasis ds preconditioners bases projection subspaces 
suggestions provided improvements extensions results 
summary results rational interpolation readily achieved projection unions krylov subspaces 
projection algorithm implementing rational interpolation choose ds preconditioners approximately forming subspaces specific bases representing subspaces 
modulo assumptions conditioning reduced order pencil gamma oee bases lead rational interpolation 
decisions form biorthogonal orthogonal bases pertinent respect efficiency reliability rational interpolation 
biorthogonality respect certain matrix leads rational lanczos algorithm efficient approach generalizes existing single point lanczos techniques pad approximation 
theory biorthogonality maintained projection implicitly performed recursions lengths proportional number interpolation points 
unfortunately biorthogonality lost practice convergence rl results slightly slowed algorithm susceptible errors ds preconditioner 
difficulties avoided utilizing orthogonal bases 
choice leads dual rational arnoldi algorithm relatively robust costly approach 
dual ra approach suited inexact ds preconditioners 
orthogonality tends eliminate column dependencies arise finite precision particular moments matched single interpolation point 
novel approach low order singular value decomposition alternatively employed extract dependent columns 
technique associated proposed rational power method applied reduced order model avoids costly computations vectors large dimensional subspaces 
avoiding form bi orthogonalization rp algorithm particularly suited parallel implementations 
particular known priori approach sided reduced order model concurrently computed multiple processors nearly negligible communications 
error estimates avoid redundant processors 
addition type basis ds preconditioners selected 
ideally ds preconditioners exact inverses gamma oe leading moment matching interpolation point oe approximate ds preconditioners solves reasonable versions ra rp algorithms may reduce computational costs 
experiments suggest accuracy approximate solves consistent desired reduced order model significant deterioration modeling convergence avoided 
approximations implemented fixed preconditioners inner iterations krylov iterative solver combination inner iterations inner preconditioners 
injecting information outer modeling iteration solver improve solver performance 
regardless ds preconditioner formed contents depend choice interpolation point 
imaginary interpolation points oe lead reduced order model convergence frequencies locality joe real interpolation points hand lead convergence broader frequency ranges 
terms cost versus accuracy trade offs limited number real interpolation points typically preferred sequential rational krylov implementations 
purposes load balancing parallel implementations better served numerous dynamically placed imaginary interpolation points 
implementation placement interpolation points number moments matched point available information feedback error analyses 
error implicitly estimated residual computations explicitly estimated comparisons different reduced order models differences due complementary choices past interpolation points 
residuals times easier compute model comparison tends accurate 
possibilities aim dissertation provide strong foundation reduced order modeling lti siso dynamic systems krylov projection methods 
possible problem dependent issues advantage symmetry addressed simple modifications methods dissertation 
implementational details tuning adaptive interpolation point strategy addressed coded trials errors 
purer research standpoint remaining areas apparent time 
mimo systems noted section multiple input multiple output systems common frequently treated block versions projection algorithms 
block versions expensive moderate numbers inputs outputs 
new ideas acquiring fundamental directions subspaces needed 
certain poles system emphasized modal type approach 
rank deficiencies section proposes svd approach handling rank deficient matrices postprocessing reduced order model 
theoretical experimentation needed completely understand details reliability method 
particular choosing frequency frequencies svd svd evaluated requires additional insight 
furthermore acceptable condition numbers reduced order model examined 
multilevel parallelism version parallel algorithm planned 
implemented distributed architectures mpi message passing interface directives 
extremely large problems doubtful matrix pencil gamma oe stored treated single processor unit 
second level parallelism partitioning basic matrix operations required 
manner hundreds processors brought bear problem hopefully retaining efficiency 
approximate solvers nearly countless number iterative solvers inner preconditioners considered approximately solving 
simplicity known gmres algorithm utilized chapter 
risks benefits sided iterative solver qmr studied 
furthermore connections outer modeling inner solver recursions appear important considered detail 
lastly connections obtained approximate solves interpolation point placement 
conjectured approximate solves increasing number interpolation points feasible sequential implementations 
statement explored exploited chapter 
related problems issues arising directly dissertation classes problems possibly benefit developed lti model reduction techniques 
shifted systems linear equations 
model reduction techniques interpolation point placement residual calculations various rk implementations parallelism directly transferable solving arbitrary systems shifted linear equations 

eigenvalue problems 
eigenvalue problem discussed detail chapter connections model reduction 
general approaches computing eigenvalues regions needed 
issues interpolation point placement parallelism need explored 

nonlinear time varying problems 
extending model reduction lti dynamic systems provides new challenges 
distributed systems transmission line equations higher order moments system generally longer available simple repetitive multiplications matrix 
linear pencil gamma se replaced higher infinite order matrix polynomial explicitly computing approximation longer straightforward 
linearize model reduction process frequency regions attempt merge results 
similarly time dependent problems lti reduced order models constructed different regions time 
sensitivity model accuracy respect time determined approaches updating projection matrices time obtained 
appendix lemma proofs proof lemma result follows equalities gamma oee gamma gamma se gamma oee gamma gamma se oe gamma oe oe gamma gamma oee gamma proof lemma show jg induction 
dual relation jg follows similar fashion 
subspaces jg trivially identical 
assume holds 
shown jg 
assumption jg gamma 
vector jg jg jg lies half proof complete 
proof lemma assumption lies column space implies exists vector noting biorthogonality follows proof lemma defining matrix gamma oee gamma gamma oee satisfies proof follows induction 
gamma oee gamma gamma oee gamma gamma oee gamma equality follows lemma gamma oee gamma colsp fv assume desired result holds expression interest corresponding gamma oee gamma ev gamma gamma oee gamma gamma oee gamma gamma oee gamma gamma gamma oee gamma gamma oee gamma gamma oee gamma oee gamma gamma gamma oee gamma gamma oee gamma gamma gamma oee gamma gamma oee gamma gamma gamma oee gamma equality follows inductive assumption 
equality follows lemma gamma oee gamma gamma gamma oee gamma colsp fv induction desired result hold proof lemma desired result simply dual lemma 
reason case note balance argument proceeds fashion previous proof 
define 
gamma oee gamma gamma oee gamma gamma oee gamma equality follows lemma conjunction observation gamma oee zt nonsingular matrix ir thetam matrix ir thetam satisfying gamma oee gammat gammat gamma oee gammat colsp fzg gamma oee gammat colsp fwg 
see observation noting gamma oee gammat gamma gamma oee gammat colsp fzg assumed written zg vector multiplying left gamma oee yields gamma oee gamma oee gamma wt gamma gammat gamma oee gammat gamma column space basis gamma oee gammat gamma oee gammat lie colsp fwg 
proof lemma prove 
key note gamma oe gamma rewritten gamma oe gamma gamma oe gamma gamma oe gamma oe gamma gamma oe gamma gamma oe oe gamma oe gamma oe gamma yield oe gamma oe gamma oe gamma gamma oe gamma gamma oe gamma gamma gamma oe gamma expression follows induction 
multiplying right gives gamma oe gamma gamma oe gamma oe gamma oe gamma gamma oe gamma gamma gamma oe gamma gb satisfied 
assume holds gamma 
multiplying right gamma oe gamma gamma gamma oe gamma yields oe gamma oe gamma oe gamma gamma oe gamma gamma gamma oe gamma gamma oe gamma gamma oe gamma gamma gamma oe gamma gamma gamma oe gamma gamma gamma oe gamma assumption holds gamma shows holds induction step hold general 
proof dual provided 
proof lemma prove leave dual 
combining right sides general rk algorithm leads relation gamma oe zm fi pm right sides relation rewritten gamma se zm fi pm oe gamma fi pm oe gamma wm fi wm 
fi fi combining expressions matrix form multiplying right inverse fi delta delta delta 

fi yields 
proof lemma eigenvalue ax ex equivalent relation gamma oee gamma ex gammaoe follows lemma 
original system stable spectrum lies left imaginary axis spectrum gamma oe gamma bounded complex plane complex function 
gammaoe 
simply defines circle centered gamma oe radius oe relation oe oe gamma oe oe oe holds real gammaoe oe imag gamma 
oe proof lemma desired result follows relations ax ex gamma oee gamma ex gamma oe gamma oe gamma gamma oee gamma oe gamma gamma oe gamma oee gamma gamma oee oe gamma gamma gamma oe appendix selected matlab implementations development dual rational arnoldi rational lanczos algorithms important contributions 
provided implementations algorithms executed matlab examples 
dual rational arnoldi implementation function am em bm cm rational krylov method dual rational arnoldi version inputs system modeled number moments matched point column vector interpolation points outputs am em bm cm reduced order model parameter initialization round size size factorize sparse matrices minimal column ordering zeros lt ut pt lu lt ut pt sparse eye construct kk kk kk kk kk kk kk kk kk kk kk kk kk kk kk kk kk norm norm compute reduced order model am em bm cm rational lanczos implementation function am em bm cm rational krylov method rational lanczos version inputs system modeled number moments matched point column vector interpolation points outputs am em bm cm reduced order model parameter initialization round size size reduced order model initialization am zeros em zeros bm zeros cm zeros factorize sparse matrices minimal column ordering zeros lt ut pt lu lt ut pt sparse eye construct kk kk kk kk kk kk kk kk kk gamma max beta max max gamma max beta max max max max gamma sqrt abs norm norm beta sign sqrt abs norm norm beta beta gamma gamma gamma beta kk kk kk kk em max beta beta am sold em am am sold finish am em matrices beta max em beta am em am am compute reduced order input output vectors bm am em gamma cm beta quasi parallel rational power implementation function am em bm cm rational krylov method quasi parallel rational power version inputs system modeled number parallel iterations perform vector containing frequency point grid outputs am em bm cm reduced order model initialize pdf uniform distribution size pdf ones size round initialize projection matrices choose point model current pdf rand pdf pdf pdf pdf pdf pdf edit pdf keep current points max min pdf pdf pdf sum pdf compute new directions model real norm real imag norm imag size size real norm real imag norm imag choose point model current pdf rand pdf pdf pdf pdf pdf pdf edit pdf keep current rounds points max min pdf pdf pdf sum pdf compute new directions model real norm real imag norm imag size size real norm real imag norm imag compute model number junk size uc sc vc svd abs sc sc uc vc uc vc uc vc compute model number junk size uc sc vc svd abs sc sc uc vc uc vc uc vc compute difference models update pdf diff zeros size diff abs pdf diff sum diff am em bm cm reddy finite element method 
new york ny mcgraw hill 
moler nash numerical methods software 
englewood cliffs nj prentice hall 
golub van loan matrix computations nd ed baltimore md john hopkins university press 
van der accurate interconnect modeling multi transistor chips microwave circuits proc 
ieee acm int 
conf 
computer aided design san jose ca pp 

put shut grid control ieee spectrum vol 
pp 

freund golub nachtigal iterative solution linear systems acta numer vol 
pp 

saad numerical methods large eigenvalue problems 
manchester uk manchester university press 
saad iterative methods sparse linear systems 
boston ma pws publishing 
numerical linear algebra aspects control design computations ieee trans 
autom 
control vol 
pp 

van generalized problem linear system theory ieee trans 
autom 
control vol 
pp 

moler stewart algorithm generalized matrix eigenvalue problems siam numer 
anal vol 
pp 

lee rohrer numerical integration algorithms waveform evaluation awe proc 
ieee acm int 
conf 
computer aided design santa clara ca pp 

behavioral approach model reduction proc 
th ieee conf 
decision control new orleans la pp 

nguyen interconnect analysis timing simulation partitioning multipoint block arnoldi model reduction preparation 
ruhe rational krylov algorithms nonsymmetric eigenvalue problems ii matrix pairs linear 
appl vol 
pp 

gallo model reduction techniques applications electrical engineering 
london uk springer verlag 
doyle francis tannenbaum feedback control theory 
new york ny macmillan 
matrix transformations computing rightmost eigenvalues large sparse nonsymmetric eigenvalue problems ima numer 
anal vol 
pp 

morgan scott generalizations davidson method computing eigenvalues sparse symmetric matrices siam sci 
stat 
comput vol 
pp 

datta saad arnoldi methods large sylvester observer matrix equations linear 
vol 
pp 

freund solution shifted linear systems quasi minimal residual iterations numerical linear algebra reichel varga eds berlin de pp 

choudhary numerical solutions large sparse linear systems applications phd dissertation northern illinois university de il 
quantitative measure modal dominance continuous systems proc 
nd ieee conf 
decision control san antonio tx pp 

unified derivation critical review modal approaches model reduction int 
control vol 
pp 

dynamic system control 
new york ny john wiley sons 
franklin powell emami feedback control dynamic systems nd ed reading ma addison wesley 
viability methods computing stable reduced order models ieee trans 
autom 
control vol 
pp 

moore principal component analysis linear systems controllability observability model reduction ieee trans 
autom 
control vol 
pp 

model reduction balanced realizations error bound frequency weighted generalizations proc 
rd ieee conf 
decision control las vegas nv pp 

bartels stewart algorithm solution matrix equation ax xb commun 
acm vol 
pp 

chen linear system theory design 
new york ny holt rinehart winston 
partial realization problem linear 
appl vol 
pp 

baker jr essentials pad approximation 
new york ny academic press 
anderson rational interpolation state space variable realizations linear 
appl vol 
pp 

robust control compact disc player proc 
st ieee conf 
decision control tucson az pp 

frequency weighted balanced reduction closed loop mechanical servo systems phd dissertation technical university delft delft 
boley golub nonsymmetric lanczos algorithm controllability syst 
control lett vol 
pp 

pad type approximation general orthogonal polynomials 
basel birkhauser 
lanczos iteration method solution eigenvalue problem linear differential integral operators res 
natl 
bur 
stand vol 
pp 

van pad techniques model reduction linear system theory survey comput 
appl 
math vol 
pp 

linear system reduction pad approximation allow retention dominant modes int 
control vol 
pp 

hwang chen multi point continued fraction expansion linear system reduction ieee trans 
autom 
control vol 
pp 

frequency response computation rational interpolation ieee trans 
autom 
control vol 
pp 

ff pad method model reduction frequency domain ieee trans 
autom 
control vol 
pp 

rohrer waveform evaluation timing analysis ieee trans 
computer aided design vol 
pp 

asymptotic waveform evaluation moment matching interconnect analysis 
boston ma kluwer academic publishers 
gallivan van asymptotic waveform evaluation lanczos method appl 
math 
lett vol 
pp 

feldman freund efficient linear circuit pad approximation lanczos method ieee trans 
computer aided design vol 
pp 

model reduction projection formulation int 
control vol 
pp 

clough dynamic analysis structures lanczos coordinates 
eng 
struct 
dyn vol 
pp 

newman vibration modes large structures automatic matrix reduction method aiaa vol 
pp 

wilson yuan dynamic analysis superposition ritz vectors 
eng 
struct 
dyn vol 
pp 

su craig jr krylov vector methods model reduction control flexible structures adv 
control dynamic syst vol 
pp 

kim craig jr structural dynamics analysis unsymmetric block lanczos algorithm int 
numer 
methods eng vol 
pp 

kim craig jr computational enhancement unsymmetric block lanczos algorithm int 
numer 
methods eng vol 
pp 

vertical stabilisation proc 
th ieee conf 
decision control brighton uk pp 

boley krylov space methods state space control models circuits syst 
signal process vol 
pp 

van numerical linear algebra techniques large scale matrix problems systems control proc 
st ieee conf 
decision control tucson az pp 

oblique projection methods large scale model reduction siam matrix anal 
appl vol 
pp 

sorensen van model reduction state space systems implicitly restarted lanczos method numer 
algorithms vol 
pp 

feldman freund reduced order modeling large linear block lanczos algorithm proc 
nd acm ieee design automation conf san fransisco ca pp 

feldman freund reduced order modeling large passive linear circuits means algorithm proc 
ieee acm int 
conf 
computer aided design san jose ca pp 
yang stable efficient reduction substrate model networks transforms proc 
ieee acm int 
conf 
computer aided design san jose ca pp 

white coordinate transformed arnoldi algorithm generating guaranteed stable reduced order models rlc circuits proc 
ieee acm int 
conf 
computer aided design san jose ca pp 

lanczos solution systems linear equations minimized iterations res 
natl 
bur 
stand vol 
pp 

gutknecht completed theory unsymmetric lanczos process related algorithms part siam matrix anal 
appl vol 
pp 

gutknecht completed theory unsymmetric lanczos process related algorithms part ii siam matrix anal 
appl vol 
pp 

parlett taylor liu look ahead lanczos algorithm unsymmetric matrices math 
comp vol 
pp 

freund gutknecht nachtigal implementation look ahead lanczos algorithm non hermitian matrices siam sci 
comp vol 
pp 

saad flexible inner outer preconditioned gmres algorithm siam sci 
comput vol 
pp 

van der vorst family nested gmres methods numer 
linear 
appl vol 
pp 

peters wilkinson inverse iteration ill conditioned equations newton method siam review vol 
pp 

parlett scott lanczos algorithm selective orthogonalization math 
comput vol 
pp 

round error analysis gram schmidt method solution linear squares problems bit vol 
pp 

arnoldi principle minimized iterations solution matrix eigenvalue problem appl 
math vol 
pp 

stability instability partial realizations syst 
control lett vol 
pp 

implicitly restarted lanczos method model reduction stable large scale systems master thesis university illinois urbana il 
implicitly restarted subspace methods stable partial realisations appear siam matrix anal 
appl 
cullum donath block lanczos algorithm computing algebraically largest eigenvalues corresponding eigenspace large sparse real symmetric matrices proc 
th ieee conf 
decision control az pp 

ruhe rational krylov methods eigenvalue computation linear 
appl vol 
pp 

implementation parallel rational krylov algorithm phd dissertation goteborg university chalmers university technology goteborg 
gallivan van rational lanczos method model reduction numer 
algorithms vol 
pp 

error bounds simple lanczos procedure computing functions symmetric matrices eigenvalues comput 
maths 
math 
phys vol 
pp 

walker implementation gmres method householder transformations siam sci 
stat 
comput vol 
pp 

greenbaum behavior slightly perturbed lanczos conjugate gradient recurrences linear 
appl vol 
pp 

singhal computer methods circuit analysis design nd ed new york ny van nostrand reinhold 
zhang fast method frequency time domain simulation high speed vlsi interconnects ieee trans 

theory tech vol 
pp 

fast algorithm compute norm transfer function matrix syst 
control lett vol 
pp 

grace little thompson control system toolbox 
natick ma mathworks 
rohrer dimensional circuit oriented electromagnetic modeling vlsi interconnects proc 
ieee int 
conf 
computer design cambridge ma pp 

ruhe rational krylov algorithm nonsymmetric eigenvalue problems iii complex shifts real matrices bit vol 
pp 

longman best rational function approximation laplace transform inversion siam math 
anal vol 
pp 

lucas optimal model reduction multipoint pad approximation franklin inst vol 
pp 

kumar gupta karypis parallel computing design analysis algorithms 
redwood city ca benjamin cummings 
saad dual threshold incomplete lu factorization numer 
linear algebra appl vol 
pp 

grote parallel preconditioning sparse approximate inverses siam sci 
comp 
ashby minimax preconditioning hermitian linear systems siam matrix anal 
appl vol 
pp 

davidson iterative calculation lowest eigenvalues corresponding eigenvectors large real symmetric matrices comput 
phys vol 
pp 

saad projection deflation methods partial pole assignment linear state feedback ieee trans 
autom 
control vol 
pp 

de nested krylov methods preserving orthogonality proc 
th copper mountain multigrid conf copper mountain pp 

sorensen implicit application polynomial filters step arnoldi method siam matrix anal 
appl vol 
pp 

parlett symmetric eigenvalue problem 
englewood cliffs nj prenticehall 
ericsson ruhe spectral transformation method numerical solution large sparse generalized eigenvalue problems math 
comp vol 
pp 

morgan scott preconditioning lanczos algorithm sparse symmetric eigenvalue problems siam sci 
stat 
comput vol 
pp 

restarted arnoldi method applied iterative linear system solvers computation right eigenvalues siam matrix anal 
appl vol 
pp 

scott advantages inverted operators raleigh ritz approximations siam sci 
stat 
comput vol 
pp 

morgan computing interior eigenvalues large matrices linear 
appl vol 
pp 

paige parlett van der vorst approximate solutions eigenvalue bounds krylov spaces numer 
linear 
appl vol 
pp 

olsen jrgensen simons passing limit full configuration interaction fci calculations chem 
phys 
lett vol 
pp 

van der vorst jacobi davidson iteration method linear eigenvalue problems siam matrix anal 
appl vol 
pp 

van der vorst golub years old alive eigenproblems technical report stanford university palo alto ca 
morgan davidson method preconditioning generalized eigenvalue problems comput 
phys vol 
pp 

philippe davidson method siam sci 
comput vol 
pp 

saad fischer robust preconditioning large sparse symmetric eigenvalue problems comput 
appl 
math vol 
pp 

block arnoldi davidson methods unsymmetric large eigenvalue problems numer 
math vol 
pp 

vita eric james born cincinnati ohio september 
entered ohio state university national merit scholar graduated honors electrical engineering 
received master science degree electrical engineering university illinois urbana champaign university fellow 
doctoral studies university illinois worked department energy computational science fellow 
research interests lie area computational techniques systems controls circuits 
member eta kappa nu institute electrical electronics engineers 

