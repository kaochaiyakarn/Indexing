investigating lightly supervised acoustic model training lori lamel jean luc gauvain gilles adda spoken language processing group limsi cnrs bp orsay cedex france gauvain limsi fr decade witnessed substantial progress speech recognition technology todays state art systems able transcribe broadcast audio data word error 
acoustic model development recognizers requires large corpora manually transcribed training data 
obtaining data time consuming expensive requiring trained human annotators substantial amounts supervision 
describe experiments different levels supervision acoustic model training order reduce system development cost 
experiments carried darpa tdt corpus sdr sdr evaluations 
experiments demonstrate light supervision sufficient acoustic model development drastically reducing development cost 

despite rapid progress large vocabulary continuous speech recognition remain outstanding challenges 
main challenges reduce cost terms human effort financial needs required adapt recognition system new task language 
cited costs obtaining necessary transcribed acoustic training data expensive process terms manpower time 
certain audio sources radio television broadcasts provide essentially unlimited supply acoustic training data 
vast majority audio data sources corresponding accurate word transcriptions 
sources particular main american television channels broadcast manually derived closed captions 
closed captions close exact transcription spoken coarsely time aligned audio signal 
manual transcripts available certain radio broadcasts 
preliminary experiments partially financed european commission human language technologies project 
lightly supervised acoustic model training described basic idea speech recognizer automatically transcribe unannotated data generating approximately labeled training data 
iteratively increasing amount training data accurate acoustic models obtained transcribe set unannotated data 
straightforward approach training automatically annotated data compared closed captions filter hypothesized transcriptions removing words incorrect 
surprise somewhat comparable recognition results obtained filtering suggesting inclusion closed captions language model training material provided sufficient supervision 
idea data train acoustic models proposed see aware large scale experiments technique publicly available corpora 
investigate effects different levels supervision provided language model training texts accuracy acoustic models constructed automatically generated word transcriptions 
remainder follows 
section presents basic ideas lightly supervised training followed description corpora overview limsi broadcast news transcription system 
experimental results section 
acoustic model training hmm training requires alignment audio signal phone models usually relies perfect orthographic transcription speech data phonetic lexicon 
training acoustic models usually entails carrying sequence operations audio data transcription files loaded 
transcriptions need converted common format adjustment needed different corpora different conventions pronunciation lexicon salt lake city lamel gauvain adda rived 
orthographic transcriptions aligned signal existing models bootstrap models task language 
procedure rejects substantial portion data particularly long segments 
audio data available errors simply ignored principal transcription errors manually corrected 
alignments available standard em training procedure carried 
procedure usually iterated times refine acoustic models general iteration recovers portion rejected data 
imagine training acoustic models supervised manner iterative procedure manual transcriptions alignment iteration word transcription current models known information audio sample 
approach fits em training framework suited missing data training problems 
compared commonly training procedures manual considerably reduced generating annotated corpus training procedure longer need deal new words word fragments data need correct transcription errors 

system description limsi broadcast news transcription system main components audio partitioner word recognizer 
data partitioning serves divide continuous audio stream segments associating appropriate labels cluster gender bandwidth segments 
speech recognizer uses continuous density hmms gaussian mixture acoustic modeling gram statistics estimated large text corpora language modeling 
context dependent phone model left right cd hmm gaussian mixture observation densities tied states obtained means decision tree 
word recognition performed steps initial hypothesis generation word graph generation final hypothesis generation 
initial hypotheses cluster acoustic model adaptation mllr technique prior word graph generation 
gram lm decoding steps 
final hypotheses generated gram lm acoustic models adapted hypotheses step 
baseline system darpa evaluation tests acoustic models trained hours audio data darpa hub broadcast news corpus ldc broadcast news speech collections 
gender dependent acoustic models built map adaptation si seed models wideband telephone band speech 
models contain position dependent cross word triphone models tied states approximately gaussians 
baseline language models obtained interpolation models trained different data sets excluding test epochs words newspaper newswire texts word commercial broadcast news transcripts transcriptions hub acoustic data 
recognition vocabulary contains words 

experimental results section series experiments assessing recognition performance function available acoustic language model training data summarized 
recognition runs carried xrt stated 
particular investigate accuracy acoustic models obtained recognizing audio data different levels supervision language model 
exception baseline hub language models language models include component estimated transcriptions hub acoustic training data 
language model training texts come contemporaneous sources newswires commercial summaries transcripts closed captions 
sources indirect correspondence audio data provide supervision closed captions 
set lm training texts new word list selected word frequencies training data 
language models formed interpolating individual lms built text source 
interpolation coefficients chosen order minimize perplexity development set composed second set nov evaluation data portion tdt data jun included lm training data 
combinations investigated ffl lma baseline hub lm newspaper newswire news commercial transcripts com jun acoustic transcripts ffl lmn news com closed captions may ffl lmn news com may ffl lmn news closed captions may ffl lmn news may ffl lmn news may com dec noted conditions include newspaper newswire texts epoch audio data 
provide important source knowledge particularly respect vocabulary items 
conditions include closed captions lm training data provide additional supervision decoding process transcribing audio data epoch 
testing purposes hub evaluation data comprised minute data sets selected nist 
set extracted hours data broadcast june second set set broadcasts recorded august september 
limsi system obtained word error evaluation set combined scores penultimate row table lma 
word error reduced system running xrt entry table 
icassp salt lake city lamel gauvain adda training conditions bn bn average lmn lmn lmn lmn lmn lmn lmn lmn lma lma table word error rate various conditions acoustic models trained hub training data detailed manual transcriptions 
runs done xrt row 
designates set gender independant acoustic models designates sets gender bandwidth dependent acoustic models 
amount training data raw unfiltered filtered unfiltered filtered table word error rate increasing quantities automatically labeled training data evaluation test sets gender bandwidth independent acoustic models language model lmn runs done xrt 
seen table word error rates original hub language model lma transcriptions acoustic data lmn give comparable results acoustic models trained hours manually annotated data 
quality different language models listed compared table speaker independent acoustic models trained hub data 
observed removing text source leads degradation recognition performance 
appears important include commercial transcripts lmn old lmn closed captions lmn 
suggests commercial transcripts accurately represent spoken language closed captioning 
newspaper newswire texts available word error increases best configuration lmn 
basic idea align automatically generated word transcriptions hours audio broadcasts spoken document retrieval task nist sdr 
audio corpus comprised shows difference sources cnn abc pri broadcast january june 
lightly supervised training procedure follows 
order bootstrap training procedure initial set acoustic models trained minutes shows manually transcribed data ldc hub corpus 
acoustic models significantly fewer parameters standard hub models 
manually transcribed data bootstrap process building successive model sets 
recognition performance bootstrap models entry table 
small models transcribe broadcasts hours data 
table compares methods investigated automatically transcribed data acoustic model training 
method hypothesized transcriptions aligned closed captions story story regions automatic transcripts agreed closed captions kept training purposes 
second method consists simply training aligned data trying filter recognition errors 
cases closed story boundaries delimit audio segments automatic transcription 
automatically labeled data train substantially larger acoustic models turn transcribe additional shows 
shows processed hours data resulting hours aligned acoustic data 
data models sets close size baseline system built 
acoustic model sets trained subsets automatically transcribed data assess recognition performance function available data 
unfiltered model sets larger terms number triphone contexts covered total number gaussians built filtered data 
recognition results sets hub evaluation test shown table 
results compared rows table report results detailed manual transcriptions training data 
observations results 
expected training data word error rate decreases 
true filtered unfiltered training 
word error reduction saturate amount training data increases hope lower error rate continuing procedure 
filtering automatic transcripts closed captions reduces word error relative compared error rate obtained simply training available data 
including closed captions language model training data difference amounts data transcribed training due factors 
total duration includes non speech segments eliminated prior recognition partitioning 
secondly story boundaries closed captions eliminate irrelevant portions commercials 
thirdly remaining silence frames portion retained training 
icassp salt lake city lamel gauvain adda amount training data raw unfiltered lmn lmn lmn lmn lmn table word error rate different language models increasing quantities automatically labeled training data evaluation test sets gender bandwidth independent acoustic models 
provide supervision ensure proper convergence training procedure 
best word error rate obtained procedure higher obtained training hours detailed annotated transcriptions filtered unfiltered versus models 
part difference may due fact different corpora training conditions believe essentially due difference transcription qualities 
differences arise errors procedure word boundary problems incorrect labeling non speech events breath noises supervision available 
table word error rates different language models increasing quantities automatically labeled training data gender bandwidth independent acoustic models 
performance seen improve increasing amounts training data best lm trained text sources 
commercial transcripts lmn lmn data epoch seen important closed captions lmn supporting earlier observation closer spoken language 
news texts period lmn available provide adequate supervision lightly supervised acoustic model training 

summary discussion investigated low cost data train acoustic models broadcast news transcription supervision provided closed captions 
show recognition results obtained acoustic models trained large quantities automatically annotated data comparable relative increase word error results acoustic models trained large quantities data detailed manual annotations 
significantly higher cost detailed manual transcription substantially time consuming producing commercial transcripts expensive closed captions commercial transcripts produced purposes approaches promising require substantial computation time little manual effort 
advantage offered approach need extend pronunciation lexicon cover words word fragments occurring training data 
possible way improve method take advantage priori knowledge broadcast show type 
amount available data reduced training models subset shows better match unannotated data result better approximate transcription 
adda gauvain language modeling broadcast news transcription esca eurospeech budapest pp 
sept 
barras wu liberman transcriber development tool assisting speech corpora production speech communication pp 
jan 
graff liberman tdt text speech corpus darpa broadcast news workshop herndon 
see morph ldc upenn edu tdt 
gauvain adda lamel adda decker transcribing broadcast news limsi nov hub system arpa speech recognition workshop va pp 
feb 
gauvain lamel fast decoding indexation broadcast data icslp pp 
beijing oct 
gauvain lee maximum posteriori estimation multivariate mixture observation markov chains ieee trans 
sap pp 
april 
garofolo voorhees fisher trec spoken document retrieval track overview results th text retrieval conference trec nov 
graff broadcast news speech corpus arpa speech recognition workshop pp 
feb 
kemp waibel unsupervised training speech recognizer experiments esca eurospeech bu pp 
sept 
lamel gauvain adda lightly supervised acoustic model training isca asr pp 
paris sept 
woodland maximum likelihood linear regression speaker adaptation continuous density hidden markov models computer speech language pp 

pallett fiscus przybocki broadcast news test results nist nsa speech transcription workshop college park may 
utilizing training data improve performance darpa broadcast news transcription understanding workshop pp 
feb 
icassp salt lake city lamel gauvain adda 
