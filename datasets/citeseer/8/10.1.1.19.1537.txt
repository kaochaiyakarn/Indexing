partially supervised learning morphology stochastic transducers alexander clark issco tim university geneva uni mail boulevard du pont ch gen eve switzerland algorithm unsupervised learning morphology stochastic nite state transducers particular pair hidden markov models 
task viewed alignment problem sets words 
supervised model morphology acquisition converted unsupervised model treating alignment hidden variable 
expectation maximisation algorithm task studied leads calculations involving permanent matrix probabilities 
calculation permanent discussed various technical diculties addressed 
results english past tense arabic plural highly complex system morphology 
number di erent approaches unsupervised learning morphology past years goldsmith jurafsky 
achieve impressive results share common failing priori limitation form morphological transductions modelled restricted simple concatenation 
clearly undesirable european languages ones german processes precisely studied languages need automatic morphological inducers 
related limitation learn regular morphology 
systems perform limitations general language learning system sorts assumptions 
supervised learning algorithms hand capable learning irregularities morphology rumelhart mooney cali clark 
desirable means turning supervised acquisition model unsupervised acquisition model 
closely related yarowsky 
concerned integrating diverse sources information am concerned correct application single source information surface forms word 
say languages quite practical degree morphological analysis generation viewed primarily alignment task broad coverage word list 
approach taken stochastic model joint probability pair strings supervised model trained em algorithm treat alignment strings hidden variable modelled em algorithm 
clean treatment allow inte complex broader language acquisition systems 
sources information available aid alignment 
focus demonstrate rigorous method apply speci piece information surface forms words aligned 
shall particular form trainable stochastic nite state transducer called pair hidden markov model approach powerful types em trained transducers context free transducers wu 
shall solution general problem unsupervised learning morphology shall consider slightly easier task shall call partially supervised learning learner sets strings relationship 
pair hidden markov models pair hidden markov models introduced durbin 
prior bioinformatics learn morphology clark 
extension hidden markov models hmms model joint probability pair strings single string 
historically derived extension levenshtein edit distance related various kernels support vector machines cristianini 
trained modi cations em algorithm dempster suited learning range transductions natural language including grapheme phoneme conversion especially morphological transductions 
major advantage learn non concatenative transductions vowel changes stem example umlaut german 
di erence hmms output symbols streams sequences call left right streams 
having normal state conditional output functions sorts outputs output symbol streams symbol left stream symbol right stream called transitions respectively 
similar copy delete insert operations levenshtein distance 
results model joint probability distribution pair strings may di erent lengths 
discussed clark standard dynamic programming algorithms hmms modi ed deal 
particular em parameter estimation algorithm similar forward backward algorithm applied learn transductions 
related general algorithm eisner 
fact transitions output symbol streams randomly initialised models strong bias produce similar strings left right streams 
bias align similar strings 
perfect situation start arti cially simple situation 
suppose sets strings size 
wish align nd bijection simultaneously train model aligned data 
model stochastic process stages rst generate pairs strings generate permutation second set shu es 
model permutation hidden variable takes 
permutations value 
consider permutation matrix ij ith element aligned jth element zero 
jx reasons prefer permutation set 
probability alignment just product probabilities matching pairs jx consider matrix element sum 
permutations called linear algebra permanent matrix bhatia 
er def ranges permutations elements 
similar familiar determinant alternating signs 
problem possible calculate eciently 
great deal active research area possible encode various combinatorial problems appropriate matrix 
train model em algorithm need able calculate posterior expectation ij data model 
ij zero expectation equals probability aligned ij ju ij jx jx denominator fraction permanent matrix sum 
permutations 
numerator sum 
permutations ij 
product ij permanent ij minor write ij ju ij erm ij erm consider posterior probabilities matrix doubly stochastic 
map matrix probabilities matrix posteriors called bregman map bregman example nding number maximum matchings bipartite graph 
ij minor matrix matrix formed removing row column containing ij element form matrix 

intractable compute exactly approximate certain circumstances technique sinkhorn balancing sinkhorn advocated sullivan 
converges rapidly soules giving complexity tractable matrices dimensions 
method sinkhorn balancing intuitively quite straightforward want scale positive matrix doubly stochastic 
normalise row sums matrix row stochastic row sums equal unity necessarily column stochastic 
normalise column sums matrix column stochastic probably 
continue way alternating normalising rows normalising columns converge doubly stochastic matrix certain circumstances approximation bregman map 
basis algorithm 
choose random model 
calculate matrix estimate matrix posteriors sinkhorn balancing 
train model pair weighting value posterior probability 
repeat step posterior probability matrix close permutation matrix 
theoretically know converge em algorithm initial experiments framework showed matrix posteriors rapidly converges permutation matrix 
imperfection obviously highly arti cial situation 
interesting cases sets size subsets want align 
formally sets strings size respectively subsets size models 
model model model joint distribution hidden variable corresponds selection sets bijection value write total likelihood function jx certain degree exibility de ne depend size sets selected 
calculations tractable 
algorithm allows trade gain aligning aligning including models information individual sequences allison 
tweak need raising joint model probability power 
ect making align words align words little relation general related conversely values close mean model align words strong link 
tunable parameter allows adjust recall precision trade suppose elements elements matrix joint probabilities call pm de ne diagonal matrix corresponding probabilities model call pu diagonal matrix probabilities create matrix size element form square matrix pu pm permutation matrix corresponds particular choice alignment exactly techniques estimating posterior probabilities matrix 
substantive di erence block ones top right alignments align bonus factor corresponding 
paths submatrix generally want encourage algorithm align possible 
accomodate formally making generative model proportional 
train models weighting appropriate values posterior matrix 
take simple example suppose dog 
table shows resulting composite matrix 
permutation matrix correspond particular alignment probability alignment product appropriate elements matrix 
identity matrix correspond aligned probability equal product elements diagonal matrix table jx cat dog fox cats dogs cat dog aligned correctly matrices shown probability cat dog fox cat cats dog cats fox cats cats cat dogs dog dogs fox dogs dogs table matrix probabilities dog 
jx cat cats dog dogs fox experiments english experiments follow model joint probability hmms model left sets 
number states 
ran models iterations models ectively converged considered words aligned posterior probability greater fact values aligned pairs invariably close 
experiment approach chose contrasting morphological processes 
chose english past tense studied extensively 
advantages rst known understood correct analyses dispute secondly simple means trivial thirdly standard test sets available facilitate comparison techniques 
data set ling consisted set base forms past forms phonetic alphabet 
table summarizes results algorithm various data sets 
trained small state transducer algorithm produced highly accurate alignment 
analysis resulting transducer showed correctly selecting sux applied modelling process crude way 
table shows behaviour model applied unseen nonce word 
model sensitive phonology stem 
past form table regular suxes english past tense conditional probabilities nonce word 
note select correct sux base form ected form recover base form 
state model 
second experiment english noisier data set derived unsupervised technique 
slightly simpli ed version wall street journal corpus ed charniak 
unsupervised clustering technique described ney cluster words classes 
classes corresponded singular plural nouns manually selected producing test set words singular noun class words plural class 
data set matching pairs nouns singular plural form substantial amounts noise 
performance algorithm data set denoted ney summarised table 
experiments arabic arabic language complex system non concatenative morphology arabic broken plural particular complexity interest mccarthy prince 
space permit brief summary arabic general lexical roots consonants pattern vowels providing data states pairs correct incorrect precision recall ling mccarthy plunkett plunkett plunkett plunkett plunkett ney ney ney ney ney table comparison results various test sets 
second set results plunkett data set show ect exponent precision recall 
tactic information 
singular changed plural changes vowels insertions consonants sensitive prosodic outline root 
sound plural formed conventional process 
rst data set mccarthy mccarthy prince consists singular nouns plurals fully phonemic transcription taken dictionary modern standard arabic 
case di erence numbers arises singular forms multiple possible plurals 
data set quite noisy addition number cases singular plural clearly algorithms sort tend align 
complex morphological processes form plural plural 
trained state model data set mccarthy 
results quite poor 
error analysis revealed alignment correctly identi ed consonant root system de ning characteristics allow alignment proceed accurately 
experimented slightly simpli ed data set prepared plunkett substantially smaller consisting pairs drawn dictionary consisting mixture sound broken plurals plural form singular form 
performed experiments data set plunkett randomly removed half half plurals see algorithm correctly match presence missing data 
produced data set plunkett size possible pairs aligned 
results tests summarised table 
second data set ran various values exponent see ect precision recall 
expected algorithm performed initial data set aligning high accuracy precision 
imperfect data set plunkett ect exponent quite marked 
exponent precision poor exponent increased precision increases rapidly 
note possible repeatedly apply algorithm data removing time pairs aligned combining number high precision models high recall 
small number states employed eciency purposes clearly small allow correct modelling processes 
little directly comparable prior unsupervised learning arabic 
compare results english yarowsky 
number di erent sources information results ranging levenshtein distance iteration nal model combining sources information including frequency semantic information 
discussion motivation fold rst clearly desirable engineering reasons able extract morphology language automatically language having manually construct 
secondly key area cognitive science modelling language acquisition 
languages may possible learn alignments words semantic information languages highly ected individual token frequencies may low allow 
cases sort operation performed 
completely unsupervised algorithms unnecessary possible extract sets syntactically similar words corpora unsupervised clustering algorithms brown apply algorithms type result 
better understanding correct application techniques necessary allow correct treatment morphologically rich languages hungarian finnish 
de fares presents algorithm identifying arabic roots uses language speci distance function similarly yarowsky weighted edit distance component model performing viterbi approximation em reestimation particular languages able perform simpler algorithm prior knowledge language question 
clearly option cognitive modelling languages language speci information 
techniques fairly general algorithm satisfying combinatorial constraints related qap optimisation gold rangarajan 
algorithms comparatively slow naive form compute elements matrix ju jjv 
simple optimisation avoid having compute pairs obviously related 
course went obviously past tense go approach introduce errors 
step move completely unsupervised algorithm integrate components complete language acquisition system 
particular order handle complete stem sorts information required frequency semantic information obvious candidates 
tends occur infrequently frequent words suce 
may allow understanding prevalence phonological transparency natural languages 
am grateful alexander pointing am grateful john mccarthy ramin allowing data 
bill keller eric gaussier helpful comments 
part done part tmr network learning computational grammars 
allison powell dix 

compression approximate matching 
computer journal 


polynomial time algorithms approximate mixed discriminants simple exponential factor 
random structures algorithms 
isabel francis sullivan 

approximating permanent importance sampling wish provide information universal grammar 
application dimer covering problem 
journal computational physics 
bhatia 

matrix analysis 
springer 
bregman 

proof convergence method problem transportation constraints 
zh 

mat 
mat 

peter brown vincent della pietra peter de souza lai robert mercer 

class gram models natural language 
computational linguistics 
eugene charniak 

immediate head parsing language models 
proceedings th annual meeting acl pages toulouse france 
alexander clark 

learning morphology pair hidden markov models 
proceedings student workshop th annual meeting association computational linguistics pages toulouse france july 
nello cristianini john shawe taylor 

support vector machines 
cambridge university press 
anne de fares 

morphologically sensitive clustering algorithm identifying arabic roots 
coling 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
durbin eddy krogh mitchison 

biological sequence analysis probabilistic models proteins nucleic acids 
cambridge university press 
jason eisner 

expectation semi rings flexible em learning nite state transducers 
proceedings esslli workshop finite state methods nlp helsinki august 
steven gold anand rangarajan 

softmax softassign neural network algorithms combinatorial optimization 
journal arti cial neural networks aug john goldsmith 

unsupervised learning morphology natural language 
computational linguistics 
charles ling 

learning past tense english verbs symbolic pattern associator vs connectionist models 
journal arti cal intelligence research 
mccarthy prince 

foot word prosodic morphology arabic broken plural 
natural language linguistic theory 
raymond mooney mary elaine cali 
induction rst order decision lists results learning past tense english verbs 
journal arti cial intelligence research 
hermann ney ute essen reinhard kneser 

structuring probabilistic dependencies stochastic language modelling 
computer speech language 
kim plunkett ramin charles 

connectionist model arabic plural system 
language cognitive processes 
eric sven 

finite growth models 
technical report cs tr department computer science princeton university 
revised 
rumelhart 

learning past tenses english verbs 
rumelhart editors parallel distributed processing volume pages 
mit press cambridge ma 
patrick daniel jurafsky 

knowledge free induction morphology latent semantic analysis 
proceedings conll lll pages lisbon portugal 
sinkhorn 

relation arbitrary positive matrices doubly stochastic matrices 
annals mathematical statistics 
soules 

rate convergence sinkhorn balancing 
linear algebra applications 
wu 

stochastic inversion transduction grammars bilingual parsing parallel corpora 
computational linguistics september 
david yarowsky richard 

minimally supervised morphological analysis multimodal alignment 
proceedings acl pages hong kong 
