maximum likelihood density estimation sample criterion unsupervised learning complex models sepp hochreiter michael mozer department computer science university colorado boulder cs colorado edu goal unsupervised learning procedures bring probability distributions alignment 
generative models gaussian mixtures boltzmann machines cast light recoding models ica projection pursuit 
propose novel sample error measure classes models applies situations maximum likelihood ml probability density estimation formulations applied models nonlinear intractable posteriors 
furthermore sample error measure avoids diculties approximating density function 
prove unconstrained model approach converges correct solution number samples goes nity expected solution approach generative framework ml solution 
evaluate approach simulations linear nonlinear models mixture gaussians ica problems 
experiments show broad applicability generality approach 
unsupervised learning procedures viewed trying bring probability distributions alignment 
known classes unsupervised procedures cast manner generative recoding models 
generative unsupervised framework environment generates training examples refer observations sampling distribution distribution embodied model 
examples generative frameworks mixtures gaussians mog factor analysis boltzmann machines 
recoding unsupervised framework model transforms points obser vation space output space output distribution compared distribution distribution derived output distribution 
example independent component analysis ica method discovers representation vector valued observations statistical dependence vector elements output space minimized 
ica model observation vectors output distribution compared factorial distribution derived assumptions distribution supergaussian factorization output distribution 
examples recoding framework projection methods projection pursuit principal component analysis 
case described unsupervised learning model objective bring probability distributions produced model alignment 
improve model need de ne measure discrepancy distributions know model parameters uence discrepancy 
natural approach outputs model construct probability density estimator pde 
primary disadvantage approach accuracy learning procedure depends highly quality pde pdes face bias variance trade learning generative models maximum likelihood ml popular approach avoids pdes 
ml approach model generative distribution expressed analytically straightforward evaluate posterior data model adjust model parameters maximize likelihood data generated model 
limits ml approach models tractable posteriors true simplest models 
describe approach ml avoids construction explicit pde requiring analytic expression posterior 
approach call sample method assumes set samples distribution proposes error measure disagreement de ned directly terms samples 
second set samples drawn model serves place pde analytic expression model density 
sample method inspired theory electric elds describes interactions charged particles 
details metaphor see 
prove approach converges optimal solution sample size goes nity assuming unconstrained maximally exible model 
prove expected solution approach ml solution generative context 
empirical results showing sample approach works linear nonlinear models 
method consider model learned fw parameterized weights model maps input vector indexed output vector fw 
model inputs sampled distribution learning procedure calls adjusting model output distribution comes match target distribution 
unsupervised recoding models observation transformed representation speci es desired code properties 
unsupervised generative models xed distribution observations 
sample method intuitive story assume data points sampled di erent distributions labeled 
sample error measure speci es samples moved distributions brought alignment 
gure samples lower left upper right corners moved upper left lower right corners 
goal establish explicit correspondence sample sample 
method utilizes mass interactions samples introducing repelling force sam ples distribution attractive force samples different distributions allowing samples move forces 
sample method formal presentation conceiving problem terms samples attract repel natural think terms physical interactions charged particles 
consider set positively charged particles locations denoted set negatively charged particles locations denoted particles correspond data samples distributions 
interaction particles characterized coulomb energy nx nx 
nx ny 
ny ny 
distance measure green function results nearby particles having strong uence energy distant particles having weak uence 
green function de ned ka bk dimensionality space constant depending denotes euclidean distance 
ln ka bk 
coulomb energy low negative positive particles near positive particles far negative particles far 
exactly state achieve distributions samples bringing distributions alignment collapsing distribution trivial form 
consequently sample method proposes coulomb energy objective function minimized 
gradient respect sample location readily computed force acting sample gradient chained jacobian location respect model parameters obtain gradient update rule 
nx nx 
ny ny 
step size nx ny potential transposition jacobian fw time derivative fw 
depends notation analogous zero matrix 
turns advantage green function particle interactions basis possibilities gaussian function 
advantage stems fact green function force nearby points goes nity points pushed gaussian force goes zero 
consequently green function expect local optima clusters points collapse single location 
empirically simulations con rmed conjecture 
proof correctness update rule numbers samples go nity expressed db 
sample method moves data points moving data points method implicitly alters probability density gave rise data 
relation movement data points change density expressed operator vector analysis divergence 
divergence location number data points moving volume surrounding minus number data points moving volume 
negative divergence movements gives density change movement data points 
get div 
cartesian orthogonal coordinates divergence div vector eld de ned div laplace operator scalar function de ned div ra laplace operator allows important characterization green function dirac delta function 
characterization gives 
div gives ectiveness algorithm moving sample get exp 
integrated squared error ise distributions obtain ise da exp da exp ise ise independent ise distributions guaranteed decrease learning sample size goes nity 
proof expected generative solution ml solution case generative model constraints model distribution maximum likelihood solution distribution ny ny model produce observations equal probability 
case show sample method yield solution expectation ml 
sample method converges local minimum energy hr expectation model output 
equivalently hr ny ny hr dx ny 
equation holds obtain ny ny ml solution 
sample method viewed approximation ml gets exact number samples goes nity 
experiments illustrate sample approach common unsupervised learning problems mog ica 
cases demonstrate sample approach works linear case 
consider nonlinear case illustrate power sample approach 
mixture gaussians generative model framework denotes mixture component chosen probability vm components associated model parameters wm 
standard mog model choice component linear model output obtained drawn gaussian distribution zero mean identity covariance matrix 
nonlinear mixture model layer sigmoidal neural network 
update rule vm derived approach 
vm nx 
step size enforced 
trained linear mog model standard expected maximization em algorithm code linear nonlinear mog approach 
xed training set samples models models nonlinear model 
sample approach generated samples model training epoch 
nonlinear model trained backpropagation 
shows results 
linear ml model better sample model 
surprising ml computes model probability values analytically posterior tractable algorithm uses samples approximate model probability values 
model samples epoch linear sample model acceptable solution worse ml model 
nonlinear models better true ring distribution su er sharp corners edges 
linear nonlinear nonlinear set original ml linear upper panel left right training samples chosen ring density larger sample density solutions obtained linear model trained em lower panels models trained sample method left right linear model nonlinear model nonlinear model component 
independent component analysis recoding model tried source distributions supergaussian modes 
ica methods able sources 
shows results nearly perfect 
ideal result scaled permuted identity matrix mixing demixing matrices multiplied 
details see 
sources mixtures recovered sources dimensional linear mixture projections sources rst row mixtures second row sources recovered approach third row twodimensional plane shown 
demixing matrix multiplied mixing matrix yields second experiment tried recover sources nonlinear 
problem impossible standard ica methods designed linear 
result shown 
exact demixing expected nonlinear ica unique solution 
details see 
sources mixtures recovered sources dimensional nonlinear mixing functions upper row lower row complex variable sources mixtures recovered sources 
mixing function completely inverted sources recovered recognizable 
discussion sample approach intuitively straightforward implementation drawbacks cautious samples close lead unbounded gradients samples considered computing force data point approach computation intensive 
approximations proposed reduce computational complexity approach 
simulations showing generality power sample approach unsupervised learning problems proven important properties approach certain assumptions approach nd correct solution 
unconstrained model expected solution approach ml solution 
approach applied unsupervised learning complex models ml method avoids drawbacks pde approaches 
acknowledgments geo rey hinton suggestions regarding 
supported deutsche forschungsgemeinschaft ho mcdonnell pew award nsf award 
dayan hinton neal zemel 
helmholtz machine 
neural computation 
duda hart 
pattern classi cation scene analysis 
wiley 
principe 
comparision entropy mean square error criteria adaptive system training higher order statistics 
pajunen karhunen editors proceedings second international workshop independent component analysis blind signal separation helsinki finland pages 
espoo finland isbn 
everitt 
latent variable models 
chapman hall 
ghahramani hinton 
em algorithm mixtures factor analyzers 
technical report crg tr university toronto dept comp 
science 
ghahramani hinton 
hierachical non linear factor analysis topographic maps 
jordan kearns solla editors advances neural information processing systems pages 
mit press 
gray moore 
body problems statistical learning 
leen dietterich tresp editors advances neural information processing systems 
proceeding 
hinton sejnowski 
learning relearning boltzmann machines 
parallel distributed processing volume pages 
mit press 
hinton sejnowski 

hinton sejnowski editors unsupervised learning foundations neural computation pages vii xvi 
mit press cambridge ma london england 
hochreiter mozer 
electric eld approach independent component analysis 
pajunen karhunen editors proceedings second international workshop independent component analysis blind signal separation helsinki finland pages 
finland isbn 
hyv 
survey independent component analysis 
neural computing surveys 
marques almeida 
separation nonlinear mixtures pattern repulsion 

cardoso jutten editors proceedings international workshop independent component analysis signal separation france pages 
principe xu 
information theoretic learning renyi quadratic entropy 

cardoso jutten editors proceedings international workshop independent component analysis signal separation france pages 
zhao atkeson 
implementing projection pursuit learning 
ieee transactions neural networks 
