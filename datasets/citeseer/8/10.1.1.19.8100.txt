spectral clustering analysis algorithm andrew ng cs division berkeley ang cs berkeley edu michael jordan cs div 
dept stat 
berkeley jordan cs berkeley edu yair weiss school cs engr 
hebrew univ cs huji ac il despite empirical successes spectral clustering methods algorithms cluster points eigenvectors matrices derived data unresolved issues 
wide variety algorithms eigenvectors slightly di erent ways 
second algorithms proof compute reasonable clustering 
simple spectral clustering algorithm implemented lines matlab 
tools matrix perturbation theory analyze algorithm give conditions expected 
show surprisingly experimental results number challenging clustering problems 
task nding clusters focus considerable research machine learning pattern recognition 
clustering points main application focus standard approach generative models algorithms em learn mixture density 
approaches su er drawbacks 
parametric density estimators harsh simplifying assumptions usually need density cluster gaussian 
second log likelihood local minima multiple restarts required nd solution iterative algorithms 
algorithms means similar problems 
promising alternative emerged number elds spectral methods clustering 
uses top eigenvectors matrix derived distance points 
algorithms successfully applications including computer vision vlsi design :10.1.1.14.1476
despite empirical successes di erent authors disagree exactly eigenvectors derive clusters see review :10.1.1.43.7945
analysis algorithms brie review tended focus simpli ed algorithms eigenvector time 
line analysis link spectral graph partitioning sec ond eigenvector graph laplacian de ne semi optimal cut 
eigenvector seen solving relaxation np hard discrete graph partitioning problem shown cuts second eigenvector give guaranteed approximation optimal cut 
analysis extended clustering building weighted graph nodes correspond datapoints edges related distance points 
majority analyses spectral graph partitioning appear deal partitioning graph exactly parts methods typically applied recursively nd clusters 
experimentally observed eigenvectors directly computing way partitioning better :10.1.1.14.1476
build weiss meila shi analyzed algorithms eigenvectors simultaneously simple settings :10.1.1.43.7945:10.1.1.143.153
propose particular manner eigenvectors simultaneously give conditions algorithm expected 
algorithm set points fs want cluster subsets 
form anity matrix de ned ij exp jjs jj ii 
de ne diagonal matrix element sum th row construct matrix ad 
find xk largest eigenvectors chosen orthogonal case repeated eigenvalues form matrix xk stacking eigenvectors columns 

form matrix renormalizing rows unit length ij ij ij 

treating row point cluster clusters means algorithm attempts minimize distortion 

assign original point cluster row matrix assigned cluster scaling parameter controls rapidly anity ij falls distance describe method choosing automatically 
note large family possible algorithms discuss related methods :10.1.1.143.153
rst sight algorithm little sense 
run means step just apply means directly data 
shows example 
natural clusters correspond convex regions kmeans run directly nds unsatisfactory clustering 
map points rows form tight clusters method obtains clustering shown 
note clusters lie relative origin cf 

readers familiar spectral graph theory may familiar laplacian replacing complicate discussion changes eigenvalues eigenvectors analysis algorithm informal discussion ideal case understand algorithm instructive consider behavior ideal case points di erent clusters nitely far apart 
sake discussion suppose clusters sizes 
simplify exposition assume points fs ordered cluster rst points cluster shorthand moving clusters nitely far apart corresponds zeroing elements ij corresponding points di erent clusters 
precisely de ne ij di erent clusters ij ij 
de ned previous algorithm starting note block diagonal adopted convention parenthesized superscripts index subblocks vectors matrices ii ii ii ii ii ii matrix intra cluster cluster de ne vector containing ii diagonal elements contain diagonal elements :10.1.1.43.7945
construct nd rst eigenvectors 
block diagonal eigenvalues eigenvectors union eigenvalues eigenvectors blocks padded appropriately zeros 
straightforward show ii strictly positive principal eigenvector eigenvalue 
ii jk eigenvalue strictly 
see 
stacking eigenvectors columns obtain subtlety needs addressed 
repeated eigenvalue just easily picked orthogonal vectors spanning subspace columns de ned rst eigenvectors 
replaced xr orthogonal matrix rr 
note immediately suggests considerable caution attempting interpret individual eigenvectors choice columns arbitrary rotation easily change due small perturbations di erences implementation 
reasonably hope guarantee algorithm arrived considering unstable individual columns subspace spanned columns considerably stable 
renormalize rows unit length obtain denote th subblock letting denote th row see th row orthogonal matrix gives proposition 
proposition diagonal blocks ij zero 
assume cluster connected 
exist orthogonal vectors rows satisfy words mutually orthogonal points surface unit sphere rows cluster 
clusters correspond exactly true clustering original data 
general case general case diagonal blocks non zero hope recover guarantees similar proposition 
viewing perturbation ideal results ask expect resulting rows cluster similarly rows speci cally eigenvectors view perturbed version close 
matrix perturbation theory indicates stability eigenvectors matrix determined 
precisely subspace spanned rst eigenvectors stable small changes di erence rd th eigenvalues large 
discussed previously eigenvalues union eigenvalues :10.1.1.43.7945
letting th largest eigenvalue ii see max assumption large exactly assumption max bounded away 
assumption 
exists note depends ii turn depends ii ii matrix intra cluster similarities cluster assumption natural interpretation context clustering 
informally captures idea want algorithm nd clusters require sets really look tight cluster 
consider example separated clusters 
looks clusters unreasonable expect algorithm correctly guess partition clusters subsets mind 
connection cohesiveness individual clusters formalized number ways 
assumption 
de ne cheeger constant cluster min ii jk minf outer minimum index subsets assume exists condition satis ed ii jk true case 
standard result spectral graph theory shows assumption implies assumption 
recall ii jk characterizes connected similar point points cluster 
term min 
characterizes partitions subsets minimum picks best partition 
speci cally partition points weight edges partition small partitions moderately large volume sum cheeger constant small 
assumption cheeger constants large exactly clusters hard split subsets 
relate mixing time random walk de ned points cluster chance transitioning point proportional ij tend jump nearby points :10.1.1.143.153
assumption equivalent assuming walk de ned points clusters corresponding transition matrix second eigenvalue mixing time random walk governed second eigenvalue assumption exactly walks mix rapidly 
intuitively true tight fairly connected clusters untrue cluster consists separated sets points random walk takes long time transition half cluster 
assumption related existence multiple paths points cluster 
assumption 
xed kg jk dk gain intuition consider case dense clusters size 
measures connected point points cluster case sum terms turn divided 
long individual jk small sum small assumption hold small measures connected rest jk measures connected points clusters 
assumption points connected points cluster points clusters speci cally ratio quantities small 
assumption 
xed jk kl dk intuition assumption consider case densely connected clusters previously 
quantity parentheses right hand side equivalent demanding ratio small jk jk jk 
assumption 
constant cn 
assumption fairly benign points cluster connected points cluster 
theorem assumptions hold 
set exist orthogonal vectors rows satisfy jjy jj rows form tight clusters separated points surface sphere true cluster experiments test algorithm applied clustering problems 
note previously described human speci ed parameter analysis suggests particularly simple way choosing automatically right theorem predicts rows form tight clusters surface sphere 
simply search pick value clustering rows gives tightest smallest distortion clusters 
means step algorithm inexpensively initialized prior knowledge clusters apart 
results algorithm shown 
giving algorithm coordinates points di erent clusters shown di erent symbols colors available 
results surprisingly clusters form convex regions cleanly separated algorithm reliably nds clusterings consistent human chosen 
note related algorithms give results subset problems aware equally simple algorithm give results comparable 
example noted earlier means easily fails clusters correspond convex regions 
alternative may simple connected components algorithm threshold draws edge points jjs jj takes resulting connected components clusters 
parameter say optimized obtain desired number clusters result algorithm joined dataset shown 
clusters consists singleton point 
clear method non robust 
compare method algorithm meila shi see :10.1.1.143.153
method similar seemingly cosmetic di erence normalize rows sum eigenvectors renormalize rows unit length 
re nement analysis suggests method susceptible bad clusterings degree di erent clusters connected varies substantially clusters 
brie rst cluster centroid randomly chosen row repeatedly choose centroid row closest centroids formally worst case centroid picked 
resulting means run restarts give results 
means conventional random initialization small number restarts gave identical results 
contrast implementation meila shi algorithm restarts 
nips clusters clusters clusters clusters clusters joined clusters joined clusters rows jittered randomly subsampled circles clusters means joined clusters connected components clusters meila shi algorithm nips clusters kannan algorithm clustering examples clusters indicated di erent symbols colors available 
results algorithm parameter varied runs 
rows jittered subsampled dataset 
means 
connected components algorithm 
meila shi algorithm 
kannan spectral algorithm 
see text 
discussion intriguing similarities spectral clustering methods kernel pca empirically observed perform clustering 
main di erence rst steps algorithm kernel pca gaussian kernel normalization form normalizations improve performance algorithm straightforward extend analysis prove conditions kernel pca give clustering 
di erent detail kannan give analysis spectral clustering matrix perturbation theory case anity matrix row sums equal 
clustering algorithm singular vectors di ers identi es clusters individual singular vectors 
experiments algorithm frequently gave poor results 
acknowledgments marina meila helpful conversations 
alice zheng helpful comments 
ng supported microsoft research fellowship 
supported intel nsf iis onr muri 
alpert kahng yao 
spectral partitioning eigenvectors better 
discrete applied math 
shawe taylor kandola 
spectral kernel methods clustering 
neural information processing systems 
chung 
spectral graph theory 
number cbms regional conference series mathematics 
american mathematical society 
kannan vempala 
clusterings bad spectral 
proceedings st annual symposium foundations computer science 
malik belongie leung shi 
contour texture analysis image segmentation 
perceptual organization arti cial vision systems 
kluwer 
meila shi :10.1.1.143.153
learning segmentation random walks 
neural information processing systems 
sch olkopf smola uller 
nonlinear component analysis kernel eigenvalue problem 
neural computation 
scott longuet higgins 
feature grouping eigenvectors proximity matrix 
proc 
british machine vision conference 
spielman teng 
spectral partitioning works planar graphs nite element meshes 
proceedings th annual symposium foundations computer science 
stewart 
sun 
matrix perturbation theory 
academic press 
weiss :10.1.1.43.7945
segmentation eigenvectors unifying view 
international conference computer vision 
