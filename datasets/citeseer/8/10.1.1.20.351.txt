refines analysis cotraining defines evaluates new training algorithm theoretical justification gives theoretical justification yarowsky algorithm shows training yarowsky algorithm different independence assumptions 
overview proceedings th annual meeting association computational linguistics acl philadelphia july pp 

term bootstrapping refers problem setting small set labeled data large set unlabeled data task induce classifier 
unlabeled natural language data paucity labeled data bootstrapping topic interest computational linguistics 
current spurred papers yarowsky blum mitchell 
blum mitchell propose conditional independence assumption account efficacy algorithm called training give proof conditional independence assumption 
give intuitive explanation training works terms maximizing agreement unlabeled data classifiers different views data 
suggest yarowsky algorithm special case training algorithm 
blum mitchell influential shortcomings 
proof give apply directly training algorithm directly justify intuitive account terms classifier bootstrapping steven abney laboratories research park avenue florham park nj usa agreement unlabeled data matter training algorithm directly seek find classifiers agree unlabeled data 
suggestion yarowsky algorithm special case training incidental detail particular application yarowsky considers properties core algorithm 
dasgupta prove classifier low generalization error agrees unlabeled data second classifier different view data 
addresses shortcomings original training gives proof justifies searching classifiers agree unlabeled data 
extend ways 
dasgupta assume conditional independence assumption proposed blum mitchell 
show independence assumption remarkably powerful violated data show weaker assumption suffices 
second give algorithm finds classifiers agree unlabeled data report implementation empirical results 
consider question relation training algorithm yarowsky algorithm 
suggest yarowsky algorithm different independence assumption show independence assumption holds yarowsky algorithm effective finding high precision classifier 
problem setting notation bootstrapping problem consists space instances set labels function assigning labels instances space rules mapping instances labels 
rules may partial functions write prediction input classifier synonymous rule 
useful think rules labels sets instances 
binary rule thought characteristic function set instances 
multi class rules define useful sets particular target class understood 
rule write set instances ambiguously set characteristic function 
write complement set characteristic function 
note contains instances 
write 
abstain identical 
expressions pr square brackets pr functions random variables 
contrast expression parentheses set instances set instances 
view independence blum mitchell assume instance consists views 
take assumption functions 
propose views conditionally independent label 
definition pair views satisfy view independence just case pr pr pr pr classification problem instance satisfies view independence just case pairs satisfy view independence 
related independence assumption prove useful 
define consist rules functions define consist rules functions 
definition pair rules satisfies rule independence just case pr pr similarly 
classification problem instance satisfies rule independence just case opposing view rule pairs satisfy rule independence 
generating assume set features thought binary rules take rule independence reduces naive bayes independence assumption 
theorem difficult prove omit proof 
theorem view independence implies rule independence 
rule independence bootstrapping blum mitchell suggests rules agree unlabelled instances useful bootstrapping 
definition agreement rate rules pr note agreement rate rules labels determined unlabeled data 
algorithm blum mitchell describe explicitly search rules agreement agreement rate play direct role learnability proof blum mitchell 
second lack dasgupta 
show view independence satisfied agreement rate opposing view rules upper bounds error 
statement theorem simplified assumes non abstaining binary rules 
theorem satisfy rule independence nontrivial predictors sense minu pr pr inequalities holds pr pr pr pr agrees unlabelled instances predicts error greater 
small amount labelled data suffices choose give geometric proof sketch reader referred original formal proof 
consider figures 
diagrams area represents probability 
example leftmost box diagram represents instances area upper left quadrant represents pr 
typically diagram horizontal vertical line broken 
special case rule independence satisfied horizontal vertical lines unbroken 
theorem states disagreement upper bounds error 
consider lemma wit disagreement upper bounds minority probabilities 
define minority value value probability pr minority probability probability minority value 
note minority probabilities conditional probabilities distinct marginal probability minu pr mentioned theorem 
areas disagreement upper right lower left quadrants box marked 
areas minority values marked 
obvious area disagreement upper bounds area minority values 
error values values opposite values error value 
minority values error values disagreement upper bounds error theorem follows immediately 
cases possible 
possibility minority values opposite error values 
case minority values error values disagreement upper bounds error disagreement minority values disagreement upper bounds minority probabilities 
case admitted theorem 
final cases minority values regardless value cases predictors satisfy condition theorem requires minu pr greater disagreement rule independence rule independence strong assumption remarkable consequence show just strong precision rule defined pr 
continue assume non abstaining binary rules 
rule independence holds knowing precision rule allows exactly compute precision rule unlabeled data knowledge size target concept 
arbitrary rules independent views 
derive expression precision terms note second line derived rule independence 
gy gy gy compute expression righthand side line require 
value precision assumed known 
second value assumed known rate estimated small amount labeled data 
values computed unlabeled data 
precision arbitrary rule compute precision rule compute precision rules view precision view rule compute precision rule precision 
data empirical investigations described data set collins singer 
task classify names text person location organization 
unlabeled training set containing instances labeled test set containing persons locations organizations total instances 
instances represented lists features 
intrinsic features words making name contextual features features syntactic context name occurs 
example consider bruce kaplan president metals text snippet contains instances 
intrinsic features bruce kaplan bruce kaplan complete name contains contextual feature president modified 
second instance intrinsic features metals metals contextual feature president context 
define location instance 
estimate test sample contains location instances giving 
treat feature rule predicting 
precision 
internal feature new york precision 
permits compute precision various contextual features shown training column table 
note numbers look probabilities 
cause failure view independence hold data combined instability estimator 
yarowsky column uses seed rule estimate training yarowsky truth chairman court firm meeting positive correlation table data negative correlation deviation conditional independence 
done yarowsky algorithm truth column shows true value 
relaxing assumption view independence mean abandon theorem 
section introduce weaker assumption satisfied data show theorem holds weaker assumption 
ways data diverge conditional independence rules may positively negatively correlated class value 
illustrates positive correlation illustrates negative correlation 
rules negatively correlated disagreement shaded larger conditionally independent theorem maintained fortiori 
unfortunately data positively correlated theorem apply 
quantify amount deviation conditional independence 
define conditional dependence dy pr pr conditionally independent dy 
permits state weaker version rule independence definition rules satisfy weak rule dependence just case dy minu pr minu pr 
definition exceed 
weak rule dependence reduces independence weak rule dependence satisfied dy say conditionally independent 
decreases permissible amount conditional dependence increases 
state generalized version theorem theorem satisfy weak rule dependence nontrivial predictors sense minu pr pr inequalities holds pr pr pr pr consider 
illustrates relevant case positively correlated 
case shown case similar 
assume minority values error values cases handled discussion theorem 
minority value 
probability takes minority value probability takes majority value 
value difference 
note conditionally independent 
fact show positive correlation 
exactly measure dy conditional dependence dy particular may write dy observe minority probability weighted average 
combining equation dy allows express terms remaining variables wit dy dy 
order prove theorem need show area disagreement upper bounds area minority value 
true just case larger say bq ap 
substituting expressions inequality solving dy yields dy short disagreement upper bounds minority probability just case weak rule dependence satisfied proving theorem 
greedy agreement algorithm dasgupta littman mcallester suggest possible algorithm give suggestion implement evaluate 
give algorithm greedy agreement algorithm input seed rules loop atomic rule evaluate cost keep lowest cost worse quit swap greedy agreement algorithm constructs paired rules agree unlabeled data examine performance 
algorithm 
begins seed rules view 
iteration possible extension rules considered scored 
best kept attention shifts rule 
complex rule classifier list atomic rules associating single feature label 
feature 
atomic rule permitted appear multiple times list 
atomic rule occurrence gets vote classifier prediction label receives votes 
case tie prediction 
cost classifier pair general version theorem admits abstaining rules 
theorem dasgupta 
theorem view independence satisfied rules different views holds pr pr pr minu pr 
words binary rule pessimistic estimate number errors times number instances labeled plus number instances left unlabeled note cost sensitive choice cost respect necessarily cost respect get cost average cost respect respect precision recall performance greedy agreement algorithm shows performance greedy agreement algorithm iteration 
test instances labeled persons locations organizations classifiers label instances show precision recall single error rate 
contour lines show levels measure harmonic mean precision recall 
algorithm run convergence atomic rule decreases cost 
interesting note significant overtraining respect measure 
final values recall precision measure compare favorably performance yarowsky algorithm 
collins singer add special final round boost recall yielding yarowsky algorithm version original training algorithm 
algorithms essentially perform equally advantage greedy agreement algorithm explanation performs 
yarowsky algorithm yarowsky algorithm classifier consists list atomic rules 
prediction classifier prediction rule list applies 
algorithm constructs classifier iteratively seed rule 
variant consider atomic rule added iteration 
atomic rule chosen precision pr measured labels assigned current classifier exceeds fixed threshold 
yarowsky give explicit justification algorithm 
show algorithm justified basis independence assumptions 
follows represents atomic rule consideration represents current classifier 
recall set instances true label set instances 
write set instances labeled current classifier 
assumption precision independence 
definition candidate rule classifier satisfy precision independence just case bootstrapping problem instance satisfies precision independence just case rules atomic rules nontrivially overlap nonempty satisfy precision independence 
precision independence stated looks conditional independence assumption emphasize similarity analysis training 
fact half independence assumption precision independence necessary 
second assumption classifiers balanced errors 
consider concrete hypothetical example 
suppose initial classifier correctly labels instances mistakes 
initial precision yarowsky citing yarowsky uses superficially different score monotone transform precision equivalent precision sorting 
recall 
suppose add atomic rule correctly labels new instances incorrectly labels new instance 
rule precision 
precision new classifier old classifier plus new atomic rule 
note new precision lies old precision precision rule 
show case precision independence balanced errors 
need consider quantities precision current classifier precision rule consideration precision rule current labeled set precision rule measured estimated labels 
assumption balanced errors implies measured precision equals true precision labeled instances follows 
assume instances true labels combined precision independence implies precision measured labeled set equal true precision 
consider precision old new classifiers predicting 
instances old classifier labels number correctly labeled number incorrectly labeled 
defining nt precision old classifier qt nt 
number new instances rule consideration correctly labels number incorrectly labels 
defining precision rule precision new classifier qt nt written qt nt qt nt nt precision new classifier weighted average precision old classifier precision new rule 
precision recall performance yarowsky algorithm immediate consequence accept rules precision exceeds threshold precision new classifier exceeds 
measured precision equals true precision previous assumptions follows true precision final classifier exceeds measured precision accepted rule exceeds 
observe recall written nt qt number instances true label 
qt recall bounded nt grows nt grows 
proven theorem 
theorem assumptions precision independence balanced errors satisfied yarowsky algorithm threshold obtains final classifier precision 
recall bounded nt quantity increases round 
intuitively yarowsky algorithm increases recall holding precision threshold represents desired precision final classifier 
empirical behavior algorithm shown accordance analysis 
seen yarowsky algorithm training algorithm justified basis independence assumption precision independence 
important note yarowsky algorithm special case training 
precision independence view independence distinct assumptions implies 
sum refined previous analysis training new cotraining algorithm theoretically justified empirical performance 
theoretical analysis yarowsky algorithm time shown justified independence assumption quite distinct independence assumption training 
blum mitchell 

combining labeled unlabeled data training 
colt proceedings workshop computational learning theory 
morgan kaufmann publishers 
michael collins yoram singer 

unsupervised models named entity classification 
emnlp 
sanjoy dasgupta michael littman david mcallester 

pac generalization bounds training 
proceedings nips 
david yarowsky 

decision lists lexical ambiguity resolution 
proceedings acl 
david yarowsky 

unsupervised word sense disambiguation rivaling supervised methods 
proceedings rd annual meeting association computational linguistics pages 
see view independence imply precision consider example 
compatible rule independence implies violating precision independence 
