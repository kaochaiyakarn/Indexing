article submitted computer speech language published jan lightly supervised unsupervised acoustic model training lori lamel jean luc gauvain gilles adda spoken language processing group cnrs limsi bp orsay cedex france decade witnessed substantial progress speech recognition technology todays state art systems able transcribe unrestricted broadcast news audio data word error 
acoustic model development recognizers relies availability large amounts manually transcribed training data 
obtaining data time consuming expensive requiring trained human annotators substantial amounts supervision 
describes experiments lightly supervised unsupervised techniques acoustic model training order reduce system development cost 
approach uses speech recognizer transcribe unannotated broadcast news data darpa tdt corpus 
hypothesized transcription optionally aligned closed captions transcripts create labels training data 
experiments providing supervision language model training materials show including texts contemporaneous audio data crucial success approach acoustic models initialized little minutes manually annotated data 
experiments demonstrate lightly unsupervised supervision dramatically reduce cost building acoustic models 

despite rapid progress large vocabulary continuous speech recognition remain outstanding challenges 
main challenges reduce development costs required adapt recognition system new task language 
today technology adaptation recognition system new task language requires large amounts transcribed training data 
cited costs development obtaining necessary transcribed acoustic training data lamel gauvain adda lightly supervised unsupervised acoustic model training expensive process terms manpower time 
certain audio sources radio television news broadcasts provide essentially unlimited supply acoustic training data 
vast majority audio data sources corresponding accurate word transcriptions 
sources particular main american television channels broadcast manually derived closed captions 
closed captions close exact transcription spoken coarsely time aligned audio signal 
manual transcripts available certain radio broadcasts 
may exist sources information different levels completeness approximate transcriptions summaries keywords provide supervision 
describes series experiments aimed reducing level supervision required acoustic model training 
basic idea speech recognizer automatically transcribe raw audio data generating approximate transcriptions training data 
training automatically annotated data compared closed captions transcripts filter hypothesized transcriptions removing words potentially incorrect training words agree 
effects different levels supervision selection language model training texts assessed 
idea data train acoustic models proposed see kemp waibel aware large scale thorough experiments technique publicly available corpora 
experiments carried bbn completely unsupervised acoustic training conversational speech corpus switchboard callhome spanish corpora combined hours manually annotated data 
small improvements relative reported led conjecture order magnitude data needed achieve comparable levels performance transcribed data 
kemp waibel report significant word error reductions data german broadcast news transcription source 
show comparable levels performance obtained twice data transcribed data hours versus hours 
remainder follows 
section presents basic ideas lightly supervised training followed description corpora overview limsi broadcast news transcription system 
experimental results sections varying amount group types transcripts refer closed captions 
authors give little information data train language models difficult assess level supervision 
lamel gauvain adda lightly supervised unsupervised acoustic model training manually transcribed data estimate initial acoustic models quantity sources language model training texts 
possible extensions discussed 

lightly supervised acoustic model training hmm training requires alignment audio signal phone models usually relies orthographic transcription speech data phonemic lexicon 
orthographic transcription usually considered ground truth assumed word sequence speech recognizer hypothesize confronted speech segment 
general easier deal relatively short speech segments transcription errors propagate jeopardize alignment 
training acoustic models new corpus reflect change task language usually entails sequence operations audio data transcription files loaded 
normalize transcriptions common format adjustment needed different text sources different conventions 

produce word list transcriptions correct errors include typographical errors inconsistencies 

produce phonemic transcription words lexicon manually verified 

viterbi align orthographic transcriptions signal existing models bootstrapped task language pronunciation lexicon produce time aligned phone transcription 
transcriptions phonemic lexicon really perfect alignment procedure may succeed 
failure occurs complete viterbi alignment due beam pruning duration criteria respected maximum allowable phone duration 

correct transcription errors unaligned data simply ignore segments discarding corresponding data audio data available 

run standard em training procedure 
operations may iterated times refine acoustic models 
general iteration recovers portion rejected data 
imagine training acoustic models supervised manner 
fact related linguistic information available audio sample place manual transcriptions required alignment 
information training language model produce phone duration longer ms indicative error phones silence breath noise 
lamel gauvain adda lightly supervised unsupervised acoustic model training word transcription current models 
language model range simple left right word graph corresponding orthographic transcription open gram model encoding available linguistic content training data 
iterative procedure successively refine models transcription 
approach fits em training framework suited missing data training problems 
automatically produced orthographic transcriptions training data eventually filtered confidence measures kemp waibel approximate manual transcriptions closed captions keeping words correctly recognized 
detailed annotation requires order times real time manual effort manual verification final transcriptions exempt errors barras 
orthographic transcriptions produced times real time quite bit costly 
transcriptions added advantage available television channels 
problems faced dealing closed captions accurate speech transcriptions 
addition providing exact word level transcription said detailed speech transcriptions provide wealth additional information available closed captions 
includes marking non speech events respiration throat clearing indication speaker turns speaker identities gender indication acoustic conditions presence background music noise transmission channel annotation non speech segments music 
closed captions accurately reflecting meaning precise 
repetitions marked may word insertions changes word order 
nist disagreement closed captions manual transcripts hour subset tdt data order garofolo 
order closed captions training need automatically produce missing information audio segmentation speaker turns intra show speaker identifiers identifying nonspeech segments acoustic conditions 
word closed needs aligned audio signal allow transcription errors insertions deletions 
addition closed captions possible electronic sources text come newswires internet 
text data may contemporaneous audio data may case archives period 
sources indirect correspondence audio data evidently provide supervision orthographic transcriptions closed captions 
models partitioning process gaussian mixture trained small amount data required labeling costly 
lamel gauvain adda lightly supervised unsupervised acoustic model training training procedure different levels supervision schemes described 
normalize available text materials newspaper newswire commercially produced transcripts closed captions detailed transcripts acoustic training data train gram language model 
partition show homogeneous segments labeling acoustic attributes speaker gender bandwidth gauvain 
train acoustic models small amount manually annotated data hour 
automatically transcribe large amount raw training data 
optionally align closed captions automatic transcriptions dynamic programming algorithm removing speech segments transcripts disagree 

run standard acoustic model training procedure speech segments automatic transcripts 
reiterate step 
easy see manual considerably reduced generating annotated corpus training procedure longer need deal new words word fragments data errors detailed manual transcriptions need corrected 
way iterate procedure available training data new models 
costly choose process data chunks new chunk transcribed models trained previously transcribed chunks 
way training data transcribed quality transcription improving step 
exception chunked processing experiment reported section models iteration data 
approach produce automatic word transcriptions hours audio broadcasts trec spoken document retrieval task nist sdr 
sets experiments exploring lightly supervised unsupervised acoustic model training carried 
set experiments reported section aim assess recognition performance function available acoustic training data terms amount raw data quality transcription 
results supervised training light supervision closed filtering tables 
second set experiments described section investigates impact different levels supervision language model training materials 
language models see table estimated various combinations text sources epoch tdt audio data lamel gauvain adda lightly supervised unsupervised acoustic model training period 
mentioned earlier newswires sources indirect correspondence audio data provide supervision closed captions commercially generated transcripts 
section explores unsupervised acoustic model training 
third set experiments amount available acoustic language model training data severely reduced order better understand tradeoff cost performance 
initial bootstrap models estimated minutes manually transcribed data acoustic model training unsupervised 
exception baseline hub language models comparative purposes language models include component estimated transcriptions hub acoustic training data 
recognition runs carried xrt stated 

corpora unannotated audio data experiments taken darpa tdt corpus 
corpus consists hours data sources cnn headline news minute shows abc world news tonight minute shows public radio international world hour shows voice america today world report hour shows 
tdt collection contains data broadcast january june associated closed captions tv shows commercially produced transcripts radio shows data january experiments 
data divided stories identifying story average duration minute seconds story 
language model training data darpa broadcast news task exception manual transcriptions acoustic training data word list selection language model estimation 
data include words newspaper newswire texts distributed ldc jan may hub tdt corpora words commercial broadcast news transcripts distributed ldc years directly years closed captions june distributed part tdt corpus 
testing purposes hub evaluation data comprised minute data sets selected nist 
set extracted hours data broadcast june second set set broadcasts recorded august september pallett 
worked broadcast news transcription task years acquired fair amount knowledge task paid extra attention avoid inadvertently including information unsupervised training experiments 
source knowledge avoided concerns lexical pronunciations modified 
lamel gauvain adda lightly supervised unsupervised acoustic model training 
system description limsi broadcast news transcription system main components audio partitioner word recognizer 
data partitioning serves divide continuous stream acoustic data segments associating appropriate labels segments 
segmentation labeling process gauvain detects rejects non speech segments applies iterative maximum likelihood segmentation clustering procedure speech segments 
result partitioning process set speech segments cluster gender telephone wideband labels 
speech recognizer uses continuous density hmms gaussian mixture acoustic modeling gram statistics estimated large text corpora language modeling 
context dependent phone model tied state right cd hmm gaussian mixture observation densities tied states obtained means decision tree 
word recognition performed steps initial hypothesis generation word graph generation final hypothesis generation 
initial hypotheses cluster acoustic model adaptation mllr technique woodland prior word graph generation 
gram language model decoding passes 
final hypotheses generated gram language model acoustic models adapted hypotheses step 
baseline system darpa evaluation tests acoustic models trained hours manually transcribed audio data darpa hub broadcast news corpus ldc broadcast news speech collections graff tdt data unsupervised training 
carefully annotated data comes variety sources abc world news world news tonight cnn early prime headline news prime news world today early edition prime time live washington journal public policy npr things considered marketplace graff 
august february releases ldc transcriptions 
addition word transcriptions annotations include speech fragments non speech events speaker turns identities markers overlapping portions non english speech 
overlapping speech portions detected transcriptions removed training data 
acoustic feature vector components comprised cepstrum log energy second order derivatives gauvain 

gender dependent acoustic models built map adaptation gauvain lee si seed models wideband telephone band speech 
computational reasons smaller sets acoustic models decoding pass 
position dependent cross word triphone models cover contexts tied states gaussians state 
second third decoding passes larger set position lamel gauvain adda lightly supervised unsupervised acoustic model training dependent cross word triphone models tied states approximately gaussians gauvain lamel 
baseline language models obtained interpolation backoff gram language models trained different data sets commercial broadcast news transcripts word newspapers north american business news associated press ap texts words excluding test data epochs transcriptions broadcast news acoustic data 
baseline recognition vocabulary contains words phone transcriptions lexical coverage evaluation test sets years 
pronunciation graph associated word allow alternate pronunciations including optional phones 
lexical entries pronunciation having alternatives 
pronunciations set phones set phone units represent silence filler words breath noises 
filler breath phones model events transcribing lexical entries 
lexicon contains compound words frequent word sequences word entries common acronyms providing easy way allow reduced pronunciations gauvain 
limsi system obtained word error darpa nist evaluation set transcribe unrestricted broadcast data word error gauvain lamel 
word error reduced system running xrt 

impact amount acoustic training data mentioned standard broadcast news system language models built interpolating gram lms built sources texts adda large amounts newspaper newswire texts large amounts commercial bn transcriptions smaller amounts available detailed bn transcriptions 
investigates unannotated acoustic model training data component language model estimated detailed bn transcriptions replaced trained closed captions tdt corpus experiments 
word list selected word frequencies training texts excluding detailed transcriptions 
language model interpolation coefficients chosen order minimize perplexity word development set composed second set nov evaluation data portion tdt data jun included language model training data 
resulting language model news com cap interpolation language models trained newspaper newswires news commercially produced transcripts com closed captions cap may 
interpolation coefficients notation means entire transcription process including partitioning word recognition adaptation requires hours computation time process hour data 
targeted processing speed evaluation 
lamel gauvain adda lightly supervised unsupervised acoustic model training commercial transcript language model newspaper language model tdt closed language model 
amount training data conditions language model raw usable news com cap table supervised acoustic model training word error rate evaluation test data various conditions acoustic models trained hub training data detailed manual transcriptions 
runs done xrt final row 
designates set gender independant acoustic models designates sets gender bandwidth dependent acoustic models 
news com cap language model trained available text sources including tdt closed captions detailed transcriptions acoustic training data 
baseline table gives word error rate news com cap language model function amount manually annotated acoustic training data 
raw data reflects size audio data partitioning usable data amount data training acoustic models 
expected increasing amount training data large impact total amount small 
hour data word error rate hours word error relative error rate reduction 
successive doubling training data reduces error relative 
noted initially large increase model size accounting improved model accuracy 
system model size tied states gaussians obtained larger models give significant improvement desired operating range xrt decoding 
word error rate news com cap language model original hub language model eval test data acoustic models trained hours manually annotated data 
sets gender bandwidth dependent acoustic models reduces relative word error rate 
remaining experiments section run news com cap language model set gender bandwidth independent acoustic models 
automatically transcribed audio data acoustic model training investigated 
order bootstrap training procedure initial set acoustic models trained hour manually transcribed data ldc hub corpus models second entry table 
data consist shows abc cnn early prime npr things considered lamel gauvain adda lightly supervised unsupervised acoustic model training 
resulting acoustic models significantly fewer parameters standard hub models 
hour manually transcribed data bootstrap process building successive model sets 
amount training data average raw unfiltered filtered unfiltered filtered manual table lightly supervised acoustic model training word error rate increasing quantities automatically labeled training data evaluation test sets gender bandwidth independent acoustic models news com cap language model 
recognition results increasing amounts automatically transcribed audio data shown table hub evaluation test 
bootstrap models successively transcribe hours raw data 
data sets sets acoustic models built filtering transcripts closed captions see table 
yy closed filtering consists dynamic programming align hypothesized transcription closed captions story story basis retaining regions automatic transcripts agree closed captions 
words disagree considered incorrect corresponding audio segments discarded 
filtering seen remove third audio data compare unfiltered filtered durations 
second model set training transcribed data trying filter recognition errors 
amount data train acoustic models shown closed filtering 
consequence unfiltered model sets larger terms number triphone contexts covered total number gaussians built filtered data 
noted cases closed story pass models cover triphone contexts tied states gaussians second third pass models cover triphone contexts tied states gaussians respectively 
yy difference amounts raw data transcribed training due factors 
total duration includes non speech segments eliminated prior recognition partitioning 
secondly story boundaries closed captions eliminate irrelevant portions commercials 
thirdly remaining silence frames portion retained training 
lamel gauvain adda lightly supervised unsupervised acoustic model training boundaries determine limits retained audio segments automatic transcription information recognizer 
acoustic models trained hours automatically transcribed data turn transcribe additional hours raw data yielding total hours 
acoustic models trained data process remaining hours audio data 
total hours raw data processed acoustic models trained 
pass models cover triphones tied states gaussians second pass models cover triphone contexts tied states gaussians third pass models cover triphones sharing states gaussians 
observations results 
expected training data word error rate decreases 
true filtered unfiltered training data 
word error reduction saturate amount training data increases hope lower error rate continuing procedure 
filtering automatic transcripts closed captions reduces word error relative compared error rate obtained simply training available data 
implies including closed captions language model training data provide supervision ensure proper convergence training procedure 
best word error rate obtained procedure higher obtained training hours detailed annotated transcriptions versus models table 
part difference may due fact different corpora training believe essentially due differences transcription quality 
differences arise errors alignment procedure word boundary problems incorrect labeling non speech events breath noises supervision available recall closed captions 

impact language model training material results unfiltered data indicate language model experiments provided sufficient supervision lightly supervised approach successful 
conditions quite advantageous language model contains lot information unannotated acoustic training data 
current state art broadcast news transcription requirements developing system different language acquiring necessary audio textual resources 
costs obtaining resources evidently depends target language resources may accessible 
effort understand contribution different text sources importance epoch text data series experiments carried different combinations text materials train lamel gauvain adda lightly supervised unsupervised acoustic model training language models 
training text sources see section include newswires commercially produced summaries transcripts 
combinations investigated ffl lma baseline hub lm newspaper newswire news commercially produced transcripts com jun acoustic transcripts ffl news com cap newspaper newswire commercially produced transcripts closed captions cap may ffl news com newspaper newswire commercially produced transcripts may ffl news cap newspaper newswire closed captions may ffl news newspaper newswire may ffl news com newspaper newswire may commercially produced transcripts dec ffl news com cap newspaper newswire closed captions may commercially produced transcripts dec ffl news newspaper newswire dec noted exception language model news conditions include newspaper newswire texts epoch audio data 
provide important source knowledge particularly respect vocabulary items 
conditions include closed captions language model training data evidently provide closer supervision decoding process 
combination lm training texts word list obtained including frequent words training texts 
language models formed interpolating individual lms built text source 
interpolation coefficients chosen order minimize perplexity development described see section 
lm training word error rate lma news com cap news com news cap news news com news com cap news table supervised acoustic model training word error rate evaluation test sets various language model training conditions set gender independant acoustic models trained hub training data hours detailed manual transcriptions 
table compares value different language models lamel gauvain adda lightly supervised unsupervised acoustic model training listed speaker independent acoustic models trained baseline hub data 
entries reminder original hub language model lma light supervision language model news com cap word error acoustic models 
observed removing text source leads degradation recognition performance 
appears important include commercially produced transcripts com old com closed captions cap 
suggests commercial transcripts accurately represent spoken language closed captioning 
newspaper newswire texts available word error increases best configuration news com cap older newspaper newswire texts news substantially increase word error rate 
amount raw data news com news cap news news com news com cap news table lightly supervised acoustic model training training word error rate different language models increasing quantities automatically labeled training data evaluation test sets set gender bandwidth independent acoustic models 
language model approach previous section taken bootstrap acoustic models successively transcribe hours raw data 
automatically annotated data build acoustic models turn transcribe chunk data 
experiments filtering closed captions closed story boundaries delimit audio segments 
table gives word error rates different language models increasing quantities automatically labeled training data 
column shows language model word error rate acoustic models trained hour manually transcribed data 
word error rates range language models indicative anticipated word error rate raw data transcribed various configurations 
hours raw approximately labeled training data word error reduced lms compared training data carefully manual transcriptions 
expected best performance obtained language model trained data language models behave similar manner 
models including commercial bn transcripts news com news com data epoch seen perform better commercially produced transcripts lamel gauvain adda lightly supervised unsupervised acoustic model training replaced closed captions news cap supporting earlier observation commercial bn transcripts closer spoken language 
news texts news news available provide adequate supervision slightly better results texts period audio data 
relative improvements observed language models largest gain early iterations 
may imply additional data useful linked method transcription quality improving iteration portions data transcribed early models significantly higher word error rates data 
possible solution data best available system 

unsupervised acoustic model training preceding experiments indicated sufficient language model training texts exact source epoch critical success approach 
showed filtering hypotheses resulted slightly better acoustic models filtering required see table 
experiments reported section look drastically reducing amount acoustic training data quantity language model training texts order find minimal requirements bootstrapping procedure 
conditions investigated unsupervised acoustic model training ffl removing story boundary filtering 
ffl training acoustic models small amount manually annotated audio data case minutes taken single show sph zz ffl training language models substantially data short time period epoch acoustic training data words 
evaluate data needed train bootstrap models amount annotated acoustic training reduced hour minutes 
resulting acoustic models small covering phone contexts sharing tied states gaussians 
experiment news com cap language model see table column news com cap 
initial word error configuration high may question approach possibly 
high initial word error decided carry iterations processing smaller amounts data chunk 
shows processed train acoustic models 
difference performance zz looked entire show training initial model performances minutes decided smaller amount training data remaining experiments 
lamel gauvain adda lightly supervised unsupervised acoustic model training story boundary filtering relatively small procedure eliminated remaining steps 
successive iteration amount data processed roughly doubled relative error reductions order 
th iteration word error close obtained previously seed models trained hour manually annotated data see line table 
remaining difference performance may due removal story boundary filtering procedure 
confirms earlier hypothesis language model provides sufficient supervision training procedure converge rapidly 
wer iteration raw acoustic training data news cap com news bootstrap models min manual shows shows shows shows shows shows data st iteration models shows shows table unsupervised acoustic model training word error rate evaluation test sets set gender bandwidth independent acoustic models 
initial acoustic models trained minutes manually annotated data 
news com cap language model trained newspaper newswire texts commercially produced transcripts closed captions may 
news language model trained newspaper newswire texts may 
story boundary filtering applied 
acoustic data transcribed chunks set acoustic models built data range word errors 
assess potential improvement obtained iterating subset training data models iteration trained shows data 
seen lower part table word error reduced reprocessing hours data processing data third time 
word error rate quite close word error rate light supervision reported table hours data higher word error obtained supervised training data see table 
news com cap language model includes components estimated closely related manually produced text data similar experiment carried newspaper newswire sources 
results table column news show similar trend higher error rate 
noted lamel gauvain adda lightly supervised unsupervised acoustic model training hours data gives smaller improvement obtained news com cap language model 
experiment shows amount manually annotated data train bootstrap models crucial questioned effects dramatically reducing language model training data 
recall news com cap models trained words text 
experiment language models estimated words newspaper newswire texts december audio data 
corresponding lexicon contains words including frequent words text corpus american english master lexicon 
language models tested acoustic models trained standard hub training data hours hour minute training sets 
results summarized table word error rates news com cap language model 
word error minutes data 
condition chosen starting point exploration unsupervised acoustic model training 
raw acoustic training data language model hours hours minutes news com cap news words table supervised acoustic model training word error rates evaluation test data varying amounts manually annotated acoustic training data language model trained words text 
raw acoustic training data wer bootstrap models min manual shows shows shows shows shows shows table unsupervised acoustic model training word error rate evaluation test data varying amounts automatically transcribed acoustic training data language model trained words text 
acoustic training data chunked manner preceding experiment processing exactly files iteration 
observation recognizer word er lamel gauvain adda lightly supervised unsupervised acoustic model training ror procedure converging properly training acoustic models automatically labeled data 
surprising supervision language model trained small amount text data raw acoustic audio data 
conditions substantially compared initial experiments carried section 
amount automatically transcribed acoustic data successively doubled consistent reductions word error rate 
error rates quite bit higher reported previous section may expect shows reduce word error observed table 
reminder word error hub acoustic models trained hours data language model substantially higher word error obtained news com cap language model 

investigated low cost data train acoustic models broadcast news transcription 
shown detailed manual transcriptions requirement acoustic model training 
explored scheme approximate transcriptions closed captions provide light supervision 
closed captions available recognition results obtained acoustic models trained large quantity automatically annotated data comparable relative increase word error results acoustic models trained large amount manually annotated data 
different levels supervision provided language model training data investigated procedure converges tested configurations differences language models relatively small 
implies technique applied closely related texts available 
shown level supervision considerably reduced training done essentially manual transcripts minutes data construct bootstrap models need manually locate story boundaries 
show language model source supervision training process procedure converges poor language model 
method requires substantial computation time little manual effort 
advantage offered approach need extend pronunciation lexicon cover words word fragments occurring training data 
eliminating need manual transcription automated training applied essentially unlimited quantities task specific training data 
question remains unanswered better performance obtained large amounts automatically annotated data large lesser amount manually annotated data 
data needed 
shown basic idea proposed lamel gauvain adda lightly supervised unsupervised acoustic model training transparent method adapting generic models specific task achieving higher degree genericity improve acoustic models portability tasks 
approach reduce cost porting language 
reasonable amount language model training texts pronunciation lexicon available bootstrap models initialize transcription process large quantity audio data low cost 
partially financed european commission language engineering project ist 
adda gauvain language modeling broadcast news transcription proc 
esca eurospeech budapest hungary pp 
september 
barras wu liberman transcriber development tool assisting speech corpora production speech communication pp 
january 
graff liberman tdt text speech corpus proc 
darpa broadcast news workshop herndon va 
see morph ldc upenn edu tdt 
clarkson rosenfeld statistical language modelling toolkit eurospeech rhodes greece pp 
september 
garofolo voorhees fisher trec spoken document retrieval track overview results proc 
th text retrieval conference trec november 
gauvain adda lamel adda decker transcribing broadcast news limsi nov hub system proc 
arpa speech recognition workshop va pp 
february 
gauvain lamel adda partitioning transcription broadcast news data icslp pp 
sydney december 
gauvain lamel adda limsi broadcast news transcription system appear speech communication 
gauvain lamel fast decoding indexation broadcast data proc 
icslp pp 
beijing october 
lamel gauvain adda lightly supervised unsupervised acoustic model training gauvain lee maximum posteriori estimation multivariate mixture observation markov chains ieee trans 
sap pp 
april 
graff broadcast news speech language model corpus proc 
arpa speech recognition workshop va pp 
february 
kemp waibel unsupervised training speech recognizer experiments proc 
esca eurospeech budapest hungary pp 
september 
kubala cohen hub spoke paradigm csr evaluation proc 
arpa spoken language systems technology workshop nj pp 

lamel gauvain adda lightly supervised acoustic model training proc 
isca asr pp 
paris september 
gauvain lamel genericity adaptability issues speech recognition proc 
isca adaptation methods speech recognition sophia antipolis france august 
woodland maximum likelihood linear regression speaker adaptation continuous density hidden markov models computer speech language pp 

pallett fiscus broadcast news benchmark test results proc 
darpa broadcast news workshop pp 
herndon va february 
pallett fiscus przybocki broadcast news test results proc 
nist nsa speech transcription workshop college park maryland may 
waibel mayfield schultz multilinguality speech spoken language systems ieee special issue spoken language processing pp 
august 
utilizing training data improve performance proc 
darpa broadcast news transcription understanding workshop va pp 
february 
