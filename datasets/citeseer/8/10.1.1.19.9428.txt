multiagent planning factored mdps carlos guestrin computer science dept stanford university guestrin cs stanford edu daphne koller computer science dept stanford university koller cs stanford edu ronald parr computer science dept duke university parr cs duke edu principled efficient planning algorithm cooperative multiagent dynamic systems 
striking feature method coordination communication agents imposed derived directly system dynamics function approximation architecture 
view entire multiagent system single large markov decision process mdp assume represented factored way dynamic bayesian network dbn 
action space resulting mdp joint action space entire set agents 
approach factored linear value functions approximation joint value function 
factorization value function allows agents coordinate actions runtime natural message passing scheme 
provide simple efficient method computing approximate value function solving single linear program size determined interaction value function structure dbn 
avoid exponential blowup state action space 
show approach compares favorably approaches reward sharing 
show algorithm efficient alternative complicated algorithms single agent case 
consider system multiple agents set possible actions observations coordinate order achieve common goal 
want find mechanism coordinating agents actions maximize joint utility 
obvious approach problem represent system markov decision process mdp action joint action agents reward total reward agents 
unfortunately action space exponential number agents rendering approach impractical cases 
alternative approaches problem local optimization different agents reward value sharing direct policy search :10.1.1.27.9386:10.1.1.17.898
novel approach approximating joint value function linear combination local value functions relates parts system controlled small number agents 
show factored value functions allow agents find globally optimal joint action natural message passing scheme 
provide efficient algorithm computing factored approximation true value function linear programming approach 
approach independent interest significantly faster compares favorably previous approximate algorithms single agent mdps 
compare multiagent algorithm multiagent reward value sharing algorithms schneider showing algorithm achieves superior performance fact close achievable optimum class problems :10.1.1.17.898:10.1.1.17.898
aa aa aa coordination graph agent problem 
dbn agent mdp 
cooperative action selection considering simpler problem selecting globally optimal joint action order maximize joint immediate value achieved set agents 
suppose collection agents agent chooses action denote fa agent local function represents local contribution total utility function 
agents jointly trying maximize agent local function influenced action agents define scope scope ae set agents action influences may decomposed linear combination functions involve fewer agents case complexity algorithm may reduced 
task select joint action maximizes 
fact depend actions multiple agents forces agents coordinate action choices 
represent coordination requirements system coordination graph node agent edge agents directly coordinate actions optimize particular fig 
shows coordination graph example 
graph structure suggests cost network solved non serial dynamic programming variable elimination algorithm virtually identical variable elimination bayesian network 
key idea summing functions doing maximization maximize variables time 
specifically maximizing summands involving participate maximization 
optimization agent 
optimize functions irrelevant 
obtain max max see optimally choose agent know values effect computing conditional strategy possibly different action choice action choice agents 
agent summarize value brings system different circumstances new function value point value internal max expression 
note introduces new induced communication dependency agents dashed line fig 

problem reduces computing max having fewer agent 
agent decision giving max max 
agent decision giving max agent simply choose action maximizes max 
recover maximizing set actions performing entire process reverse maximizing choice selects action agent 
fulfill commitment agent agent choose value maximizes 
turn forces agent agent select actions appropriately 
general algorithm maintains set functions initially contains fq algorithm repeats steps select agent 
take scope contains 
define new function max introduce scope scope gamma fa maximizing action choices recovered sending messages reverse direction 
cost algorithm linear number new function values introduced exponential induced width coordination graph 
furthermore agent need communicate directly agent necessary communication bandwidth induced width graph simplifying coordination mechanism 
note algorithm essentially special case algorithm solve influence diagrams multiple parallel decisions section 
knowledge ideas applied problem coordinating decision making process multiple collaborating agents 
step lookahead consider elaborations action selection problem previous section 
assume agents acting space described set discrete state variables fx xn takes values finite domain dom 
state defines value dom variable scope local functions comprise value include action choices state variables 
assume agents full observability relevant state variables extension fairly trivial functions define conditional cost network 
particular state xn agents instantiate state variables solve cost network previous section 
note agents need access state variables agent needs observe variables scope local function decreasing considerably amount information agent needs observe 
second extension somewhat complicated assume agents trying maximize sum immediate reward value expect receive step 
describe dynamics system dynamic decision network ddn 
denote variable current time variable step 
transition graph ddn layer directed acyclic graph nodes fa xn nodes parents 
denote parents graph parents 
simplicity exposition assume parents 
assumption relaxed algorithm somewhat complex 
node associated conditional probability distribution cpd parents 
transition probability defined value variables parents 
immediate rewards set functions step values set functions assume functions depend small set variables 
fig 
shows ddn simple agent problem ovals represent variables rectangles agent actions 
diamond nodes time step represent immediate reward nodes second time step represent value associated subset state variables 
setting state variables agents aim maximize max ag immediate reward plus expected value state 
expectation summation exponential number states 
shown simplified substantially 
example consider function fig 
see expected value function generally define 
recall assumption scope small subset variables scope gamma parents 
specifically value gamma 
note cost computation depends linearly jdom gamma depends scope complexity process dynamics 
replacing expectation backprojection generate set local functions apply coordination graph algorithm unchanged 
markov decision processes turn attention substantially complex case agents acting dynamic environment jointly trying maximize expected longterm return 
markov decision process mdp framework formalizes problem 
mdp defined tuple finite set jxj states set actions reward function theta 
ir represents reward obtained state action markovian transition model represents probability going state state action assume mdp infinite horizon rewards discounted exponentially discount factor fl 
optimal value function defined value state maximal value achievable action state 
precisely define qv fl bellman operator max qv 
optimal value function fixed point stationary policy mdp mapping 
action agent takes state value function define policy obtained acting greedily relative greedy arg max qv 
greedy policy relative optimal value function optimal policy greedy 
algorithms computing optimal policy 
linear programming 
numbering states xn variables vn represents 
lp minimize ff subject fl state relevance weights ff convex weights ff ff 
setting state space exponentially large state assignment fx xn common approach restricting attention value functions compactly represented linear combination basis functions fh linear value function function written coefficients lp approach adapted value function representation variables minimize ff subject fl ff ff :10.1.1.17.898
transformation effect reducing number free variables lp number constraints remains jxj theta jaj 
general guarantee quality approximation de van roy provides analysis error relative best possible approximation subspace guidance selecting ff improve quality approximation 
factored mdps factored mdps allow representation large structured mdps dynamic bayesian network represent transition model 
representation step transition dynamics section precisely factored mdp factor states actions 
proposed factored linear value functions approximate value function factored mdp 
value functions weighted linear combination basis functions basis function restricted depend small subset state variables 
functions fig 
example 
value function represented way algorithm section implement greedy having agents message passing coordination algorithm step 
function agent approach extends trivially case multiple functions 
previous algorithms computing approximate value functions form factored mdps 
algorithms circumvent exponential blowup number state variables explicitly enumerate action space mdp making unsuitable exponentially large action space multiagent mdps 
provide novel algorithm lp previous section 
particular show solve lp exactly closed form explicitly enumerating exponentially constraints 
task compute coefficients ff objective function 
note ff ff ff basis scope restricted ff represents marginal state relevance weights ff coefficients ff pre computed efficiently ff represented compactly marginals ff 
experiments uniform weights ff jxj ff jc deal exponentially large constraint set 
backprojection section rewrite constraints fl 
note exponentially large set linear constraints replaced single equivalent non linear constraint max flg gamma factored mdp reward function represented sum local rewards furthermore basis backprojection functions depend small set variables 
right side constraint viewed sum restricted scope functions parameterized fixed compute maximum fx ag cost network section 
specified maximization induces family cost networks parameterized showed turn cost network compact set lp constraints free variable generally suppose wish enforce constraint max restricted scope 
superscript indicates multiplied weight dependency linear 
consider cost network maximize function network including original scope 
assignment introduce variable value represents linear program 
initial functions include constraint 
linear constraint linear lp variables 
consider new function introduced eliminating variable functions extracted scope zl ffl offline 
select set restricted scope basis functions fh 

apply efficient lp approximation algorithm offline section compute coefficients fw approximate value function 
step lookahead planning algorithm section value function estimate compute local functions agent 
ffl online state 
agent instantiates values state variables scope 
agents apply coordination graph algorithm section local functions coordinate approximately optimal global action 
algorithm multiagent planning factored mdps respectively 
cost network want max value instantiation 
enforce introducing set constraints lp function generated elimination empty domain 
introduce additional constraint em equivalent global constraint max 
case cooperative multiagent mdps actions individual agents variables cost network set simply functions simply local functions corresponding rewards bases backprojections write constraints enforce fl entire exponential state space joint action space number constraints exponential induced tree width cost network exponential number actions state variables problem 
traditional single agent course special case multiagent case 
lp approach described section provides attractive alternative methods described 
particular approach requires solve single lp size essentially size cost network 
approach substantially efficient requires solve lp step policy iteration lp contains constraints corresponding multiple cost networks number depends complexity policy representation 
furthermore lp approach eliminates restrictions action model 
algorithm multiagent planning factored mdps shown fig 

experimental results validate approximate lp approach comparing quality solution approximate policy iteration pi approach 
approximate pi algorithm able deal exponentially large action spaces multiagent problems compare approaches single agent sysadmin problem unidirectional ring network machines states 
shown fig 
new approximate lp algorithm factored mdps significantly faster approximate pi algorithm 
fact approximate pi single variable basis functions variables costly lp approach basis functions consecutive triples variables 
shown fig 
singleton basis functions approximate pi policy obtains slightly better performance problem sizes 
increase number basis functions approximate lp formulation value resulting policy better 
problem new approximate linear programming formulation allows basis functions obtain resulting policy higher value maintaining faster running time 
constructed multiagent version sysadmin problem applied various net network topologies experiments 
graphs approximate lp versus approximate pi single agent sysadmin unidirectional ring running time estimated value policy 
running time approximate lp increasing number agents 
policy performance approximate lp dr drf star ring rings 
architectures shown fig 

machine associated agent variables status faulty load loaded process 
dead machine increases probability neighbors faulty die 
system receives reward process terminates successfully 
status faulty processes take longer terminate 
machine dies process lost 
agent decide machine rebooted case status running process lost 
network machines number states mdp joint action space contains possible actions problem agents states possible actions 
implemented factored approximate linear programming message passing coordination algorithms matlab cplex lp solver 
experimented types basis functions single contains indicator basis function value pair addition contains indicators joint assignments status variables neighboring agents 
fl 
shown fig 
total running time algorithm grows linearly number agents fixed network basis type 
expected asymptotic behavior problem fixed induced tree width cost network 
induced tree width pair basis ring rings problem large 
comparison implemented distributed reward dr distributed value function drf algorithms schneider :10.1.1.17.898:10.1.1.17.898
learning iterations learning exploration rates starting respectively decaying schedule iterations observations agent status load machine 
results comparison shown fig 

computed upper bound value optimal policy removing negative effect neighbors status machines 
loose upper bound dead neighbor increases probability machine dying 
network topologies tested estimated value approximate lp solution single basis significantly higher dr drf algorithms 
note single basis solution requires coordination acting fair comparison dr drf communicate acting 
allow pair bases implies agent communication achieve improvement terms estimated value 
policies obtained tended intuitive star topology pair basis server faulty rebooted loaded 
clients agent waits process terminates machine dies rebooting 
provided principled efficient approach planning multiagent domains 
placing priori restrictions communication structure agents choose form approximate value function derive optimal communication structure value function architecture 
approach provides unified view value function approximation agent communication 
novel linear programming technique find approximately optimal value function 
inter agent communication lp avoid exponential blowup state action spaces having computational complexity dependent induced tree width coordination graph agents negotiate action selection 
exploiting structure state action spaces deal considerably larger mdps described previous 
family multiagent network administration problems states actions demonstrated near optimal performance superior priori reward value sharing schemes 
believe methods described significantly extend efficiency applicability general usability factored value functions models control dynamic systems 
acknowledgments supported onr muri decision making uncertainty sloan foundation author supported scholarship 

dynamic programming 
academic press 
boutilier dean hanks 
decision theoretic planning structural assumptions computational leverage 
journal artificial intelligence research 
de van roy 
linear programming approach approximate dynamic programming 
submitted ieee transactions automatic control january 
dean kanazawa 
model reasoning persistence causation 
computational intelligence 
dechter 
bucket elimination unifying framework reasoning 
artificial intelligence 
guestrin koller parr 
max norm projections factored mdps 
proc 
th ijcai 
jensen jensen 
influence diagrams junction trees 
uncertainty artificial intelligence proceedings tenth conference pages seattle washington july 
morgan kaufmann 
koller parr 
computing factored value functions policies structured mdps 
proceedings sixteenth international joint conference artificial intelligence ijcai morgan kaufmann 
koller parr 
policy iteration factored mdps 
proc 
th uai 
peshkin meuleau kim kaelbling :10.1.1.27.9386
learning cooperate policy search 
proc 
th uai 
schneider wong moore riedmiller :10.1.1.17.898
distributed value functions 
proc 
th icml 
schweitzer :10.1.1.17.898
generalized polynomial approximations markovian decision processes 
journal mathematical analysis applications 
wolpert tumer 
general principles learning multi agent systems 
proc 
rd agents conference 
