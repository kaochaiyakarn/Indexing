line cumulative learning hierarchical sparse grams karl pfleger computer science department stanford university cs stanford edu system line cumulative learning hierarchical collections frequent patterns unsegmented data streams 
learning critical long lived intelligent agents complex worlds 
learned patterns enable prediction unseen data serve building blocks higher level knowledge representation 
introduce novel sparse gram model pruned grams learns line stochastic search frequent tuple patterns 
adding patterns data arrives complicates probability calculations 
discuss em approach problem introduce hierarchical sparse grams model uses better solution new method combining information levels 
second new method combining information multiple granularities gram widths enables models effectively search frequent patterns line stochastic analog pruning association rule mining 
result example rare combination unsupervised line cumulative structure learning 
prediction suffix tree pst mixtures model learns size bound space data 
repeatedly iterate data maxent feature construction 
discovers repeated structure line psts uses learn larger patterns 
type repeated structure limited compared hierarchical hmms useful important steps learning repeated structure expressive representations seen little progress especially unsupervised line contexts 
purposes line learning means learner sees data little time remember data repeatedly iterate 
takes importance line learning dwell motivating 
suffices note line learning author performed research stanford presently affiliated google 
critical long lived autonomous agents complex environments specific applications data sizes overwhelm storage capacities vast array literature argued line learning :10.1.1.119.3124
quote broad sense online learning essential want obtain learning systems opposed merely learned ones 
assumes importance cumulative learning called layered hierarchical learning involves results prior learning facilitate learning building new knowledge structures experience combining previously learned structures 
discussion see 
model hierarchical sparse grams line cumulative learning frequently occurring patterns unstructured unsegmented data related subcomponent model sparse grams 
language music spatial configurations event chronologies action sequences types data exhibit repeated substructure 
intelligent autonomous agents identifying repeated substructure frequent patterns data benefits including prediction unseen information improving short term memory capacity information processing capability generally facilitating communication learning 
addition frequent patterns serve building blocks higher level knowledge representation 
general methods identifying frequent patterns greatly aid automated selection higher level representational units tuned environment 
frequent patterns unbounded unsegmented data streams identified line simply noticing remembering patterns occur search larger patterns difficult notice 
putting practice involves challenges explain 
emphasize models grams main goal model class specific investigated online learning frequent patterns data cumulative existing patterns help identify larger ones 
purpose tweak slight performance improvement standard nlp compression speech community gram benchmarks batch non cumulative 
organized follows 
section presents sparse grams component model 
section demonstrates benefits sparse representation explains method line structure learning presence sparseness introduces mathematical form novel probability estimates form basis inference basic hierarchical models 
section introduces hierarchical sparse grams explains hierarchical nature models dramatically improves probability estimates inference pattern selection structure learning 
experimental results demonstrate models impressive job finding frequent patterns demonstrated environments despite sparse sampling huge pattern spaces 
section discusses related diverse set research communities 
general related models share certain properties address different fundamental goals 
concludes section 
sparse grams grams considered state art problems involving discrete sequences 
exhaustive grams store occurrence count pattern width introduce sparse grams keep counts 
trade prediction quality space wider predict narrower exhaustive grams number patterns making sparse models useful data plentiful relative storage 
similar grams trained exhaustively pruned require increased storage training easily adaptable online learning 
goal merely save space training improve gram models investigate line hierarchical learning frequent patterns unsegmented data 
necessary subcomponent hierarchical models introduced interesting issues easily introduced 
sparse joint distributions inference purposes learning inference tasks unbounded sequential analogs standard fixed feature iid unsupervised learning 
query sequence width symbols positions specified targets predicted rest missing 
position symbol alphabet grams keep count jaj patterns estimate probabilities smoothed version jaj sums counts 
sng counts patterns called tracked patterns fixed gram 
estimates tracked patterns distributes remaining probability mass evenly patterns complete joint distribution conditioned arbitrary predictions 
demonstrate sparseness improve prediction trained sparse exhaustive grams book thomas hardy novel calgary corpus stripped non letters 
batch training just demonstrate potential 
examined inference patterns forward backward middle prediction variants missing values 
accuracy loss cross entropy measured held test set characters 
expected sparseness impair prediction expected hurt cross entropy accuracy fine distinctions events important expected increased width mitigate degradation 
cross entropy results mixed grams outperforming cases 
accuracy grams outperformed wider sng number patterns 
shows advantage extra predictor variable outweigh degradation caused sparseness 
shows little degradation sparseness severe 
increase storage increase sng performance increasing wider exhaustive model increasingly important jaj rise 
sparseness create inefficiencies useful circumstances 
sparseness degrades accuracy cross entropy accuracy depends correctness mode model conditional distribution targets fine distinctions 
detailed explanation 
predicting full set symbols conditional distribution derives renormalizing jaj entries joint distribution 
trained sng includes get correct mode 
absent accuracy suffer case model includes frequent patterns 
grams stored conditional form undirected 
interchangeable exhaustive sparse versions 
undirected simplicity flexible sparseness see section predicting equally backwards generalizability data longterm goal 
adtrees allow efficient prediction undirected representation 
experiments concentrated letters symbols standard compression community words 
believe structure letters phonemes musical notes appropriate studying identification frequent patterns play role discovering words place 
words longer range interactions 
despite word level success grams suited letters claimed 
models limited letter level word grams 
representation uses tree 
leaf stores count corresponding tuple lookup grams 
model width middle backward backward middle backward backward middle backward tracked patterns non zero possible forward prob 
mass marginalized 
gram accuracy vs backward middle prediction 
forward behaves similar backward omitted clarity 
random guessing class problem gets accuracy majority gram 
leftmost points equi lines exhaustive grams outperformed wider right 
limited number non zero counts exhaustive gram performance circle square exceeds exhaustive model 
gram accuracy total tracked probability mass vs forward marginalized forward gmt prediction missing target 
degradation severe small 
counts kept 
cross entropy degrade entries replaced average 
missing values conditional distribution derives renormalizing sums entries 
larger tracked entries usually dominate sums model contains larger set entries accuracy suffer 
shows marginalized prediction degrades relatively sparseness 
general loss function penalizes incorrect answers allows skipping questions smaller penalty sparseness hurts obvious model relevant counts guess 
concerns important accuracy general loss functions allowing abstention appropriate entropy measures situations 
line learning frequent patterns model know frequent patterns priori line learning requires structural selection counts include 
structure learning harder line 
batch structure learning works optimizing global metric bayesian posterior mdl score data 
done line identify frequent patterns line stochastic subset search incrementally adds removes patterns 
patterns added randomly appearing training add probability new instance small fixed value 
tracked 
model encodes sufficient statistics summarize past data possible structure changes grow new parameters 
sufficient statistics encoded changes shrink models segmented data allows direct data incorporation steps 
techniques depend segmented data 
note adding growth steps reversing bad merges breaks line nature algorithms 
patterns low observed frequencies discarded 
infrequent patterns shuffle model 
key learning feasible frequent patterns added identifiable 
period rarity cause truly frequent pattern discarded truly frequent added pattern tracked longer chance large anomalous drought trigger removal vanishingly small 
addition removal operate independently trigger keep fixed 
frequent pattern removed model converge 
unfortunately know true pattern frequencies slowly converging estimates 
created new version hoeffding races decide remove pattern remove 
maron moore introduced hoeffding races machine learning community model selection supervised learning domingos hulten races line decision tree construction 
adapt technique sparse gram pattern removal introduce slight improvement specific low probabilities application 
hoeffding races useful competing entities statistical evidence accumulating goal find extreme high low valued example 
context allows smoothly trade frequent frequent pattern converged estimates bigger gap frequencies acted error bars high close race require convergence 
traditional hoeffding races hoeffding bounds member broader class chernoff bounds 
tighter version chernoff bounds sec appropriate probabilities close zero grams 
difference traditional hoeffding races case race perpetual new competitors continually introduced race started 
exact rule sparse grams employ abstractly stated follows model removes pattern soon identify reasonably sure close frequent probably approximately frequent 
specifically model finds pattern upper bound highest possible probability confidence interval lowest 
finds different pattern lower bound lowest 
difference upper bound lower bound tolerance parameter model drops pattern 
continual addition removal process converges stochastic equilibrium frequent patterns included 
pattern pat model maintains count occurrences inclusion 
patterns added different times empirical frequencies different reliabilities model simply estimate pattern keeps total number instances seen adding pat see 
reliable new patterns probability estimates weighted age insure adding new pattern probability model slowly diverges average asymptotically approaching model keeps patterns sorted 
pat additional term estimating missed occurrences pat tracked called episode jaj estimates number non pat occurrences episode linear interpolation jaj adding pat weighted general formula adds terms episode 

jaj 
approach computed linear sweep 
investigated bayesian em approaches problem 
bayesian predictions derived slight problem variant assumes knowledge counts individual episode complex likelihood function involving nested interacting sums original problem difficult 
operationalized approximating count breakdowns episode 
em approach attempts improve uniform distribution equation right side solution distribution 
resulting equations solved algebraically requiring iteration fixed point 
modified problem solution equivalent bayesian solution 
implemented approach practice behave identically 
section improves 
see sec approaches relationships 
hierarchical sparse grams hierarchical sparse grams consist multiple sparse grams consecutive widths possibly exhaustive gram smallest dramatically improve aspects fixed width models probability calculations pattern selection 
single tree accommodates patterns storing counts non leaves patterns lookup 
provides variance advantages traditional exhaustive grams uses new method combining models elegant linear interpolation backoff models 
slowly growing versions incrementally add greater widths line training 
models smoothly surf bias variance curve fitting parameters widening joint probability distributions continually decreasing bias error widening joint variance error converging better parameter estimates improved probability estimation estimate occurrences em approach width distribution unreliable precisely needed little data seen distribution converged 
smaller widths converge exponentially faster level uses distribution estimate missing data 
equation dn pat tmax 

pat pat pat dn pat probability pat th oldest width pattern distribution 
pat rest probability mass oldest patterns level 
dn dn em approach uniform equation 
conditioning just renormalization dn pat pat pat dn pat reduces equation base case 
course distribution specify tuple probabilities directly 
trick estimates order markov assumption xn xn 
xn xn xn 
xn xn xn 
xn xn xn approximation second lines xn xn xn xn sense best estimate order knowledge 
gives way effectively widen gram lookups width lookup width determine width probability 
patterns kn kn number width patterns width falls back width directly dn pat dn pat kn kn equation probabilities patterns computed linear sweep 
works despite pattern probability involves sum linear number terms terms pattern true equation due rightmost term 
straightforward computation probabilities level require computation quadratic number patterns 
expansion equation portion rightmost term changes pattern pattern factored preserving ability compute probabilities patterns model linear sweep 
cached probabilities tracked patterns looking probability pattern requires lookups narrower submodels equations 
may turn absent requiring lookups 
querying widest model specific probability takes time exponential submodel lookups handled traversals tree 
lookup requires tree traversals steps 
typically small relative size model 
furthermore inference patterns require lookup patterns handled efficiently patterns accessible traversal 
looking possible single symbol extensions pattern requires traversal root 
worst case 
normally narrower level contains necessary patterns making typical lookups linear optimizations possible see sec optimizations detailed derivations equations 
predictions largest width 
information filters submodel calls 
smaller widths larger fully converged estimates 
dynamically adding new level jar distribution model having small values smoothly fall back robust narrow information identifies strong wide patterns data 
estimates widths naturally weighted reliabilities 
authority smoothly transitions greater widths data seen 
improved pattern selection fixed width models search large space blindly 
smaller known frequent patterns bias new instances accuracy improved prob 
est 
improved pat 
sel 
improved prob 
est 
improved pat 
sel 
improved prob 
est 
improved pat 
sel 
improved prob 
est 
improved pat 
sel 

learning curves sparse gram gram submodel probability estimation pattern selection book 
submodel yields benefit 
selections 
specifically probability adding new tuple small constant proportional estimate dn viewed refinement simple strategy combining existing frequent patterns yield larger frequent patterns 
refinement automatically takes account possibly ways parse new pattern existing patterns frequency estimates existing patterns 
level seeks frequent patterns bias probabilities functions bias compositional chunking known patterns 
key cumulative aspect learning learning narrower levels combined results learning wider level better prediction traditional gram combinations directly enables structure learning wider level critical see results 
fixed kn number patterns level kn number levels grow slowly data seen applying add remove rules independently considering addition new widest pattern creates new level 
wider patterns generally lower absolute probabilities vastly patterns higher levels converge slowly 
results shows submodels provides significant benefit greater combined similar results seen larger improvement probability estimation dominant fully converged submodels widening approximation accurate larger 
test frequent pattern identification hierar models trained large datasets slowly growing kn numbers levels 
tables shows kn top patterns model probability training letters reuters newswire text section north american news corpus non letters removed 
kn th tion ation nation ing said ofthe inthe er ion ther tions ations ent kn official president resident learned chunks reasonable substrings text 
spaces removed frequent word sub word super word patterns mixed 
table shows frequent patterns true corpus frequency learned model smaller widths larger widths far converging 
reuters kn top top unique similar results achieved speech data timit corpus dna data chromosome human genome base pairs 
dna results shown 
note purpose testing different data types show particular performance problems traditional importance domains demonstrate technique learning compositional patterns estimating prevalence general handle different kinds data 
chromosome kn top top unique mil mil models excellent job identifying frequent patterns 
reuters example model added patterns removed early convergence level included top unique tuples occurred possible dramatic improvement chance guessing 
half patterns added level top top 
impressive considering level contained fewer pattern number indexed added model true model 
comparison reuters trained hierarchical model estimate vs true probability th oldest grams 
model tracks true probabilities 
data unique tuples 
demonstrates narrower submodels impressive job enabling successful pattern selection wider levels just levels turn wider levels 
examined model probabilities matched true corpus frequencies 
standard summarizations relative entropy distributions inappropriate sparse representation 
metric dominated errors low probabilities specifically sacrificed sparse models 
just inspected values directly newest patterns matched quite 
shows small sample section patterns 
majority patterns older sample shown matched better shown 
related pruning batch trained grams investigated 
sparseness similar count cutoffs widely practice 
models grow scale back storage need excessive space training 
literature discusses tradeoffs space predictive performance research aware explicitly address tradeoff memory reduction increased predictive accuracy increased explain accuracy loss suffers missing counts cross entropy 
prediction suffix trees psts sparse models unbalanced trees 
node represents string stores conditional distribution symbol string preceding symbols 
psts grown adding nodes represent frequent strings distributions sufficiently different parent descendant property 
models differ ways 
pst learning contains forward directed inductive bias chooses patterns terms forward distributions 
psts equally information sides target 
second undirected representation allows greater flexible sparseness 
psts store full conditional distribution node 
store probability 
pst depth node store counts jaj tuple extensions represent subset 
counts saved improving predictions 
importantly pst learning utilize repeated substructure 
extend model existing sequences high counts tree 
pst mixtures incremental updates line structure learning sense 
depth bound add sufficiently small strings occur bound add strings occur 
simply reorganizes remembers data impractical 
case model grow recognize patterns wider bound may require memory exponential bound 
desire middle ground model grows slowly bound 
model able include pattern demonstrated strongly data 
adtrees structures efficient access discrete multivariate data similar complementary speed sng inference 
adtrees trade space speed trade prediction performance space 
combination balance 
addition adtrees sparse representation synergistic 
applying adtrees allows greater adtree pruning speed space compared application exhaustive grams patterns pruned 
models equivalent hierarchical bayesian priors 
moved sparseness prior model 
submodel bias pattern selection analogous pruning association rule mining section sequence mining 
heuristic online stochastic absolute 
apply threshold level flexible 
seeks frequent patterns level regardless absolute frequencies 
basic pruning ideas related incremental association rule different problem different assumptions clear job done 
pattern level 
rely probabilistic bias 
width distribution provides useful absolute bounds probabilities 
low frequency tuple model avoid extensions sparse models nature retain pattern selection bias amounts bias combinations existing frequent patterns similar compositional chunking sequitur similar systems finer statistical sensitivity sequitur greedy 
note sequitur remembers input order chunk 
true line algorithm sense 
maximum entropy related nlp techniques features general patterns 
uniform completion distribution consistent features added incrementally process seemingly similar different 
addition requires iterating training data 
transformation techniques related schemes require batch training clear adapt line learning models may suggest necessary ingredients 
uses features constraints fundamentally assumes constraints equally statistically reliable fine batch context new features introduced different amounts data 
introduced frequency estimates 
sort regularization introduced required 
hierarchical hmms represent sparse collections patterns expressive representation structure learning addressed batch training 
complete random access data identification repeated substructure problematic 
learning models representationally subsume models case learning problem address dramatically different due domain theory supervision batch training structured segmented data usually see sec sec ch 
case accomplish learning described 
hierarchical sparse grams viewed combining grams nlp text compression communities frequent itemset pruning kdd community line learning ml connectionist traditions 
suggests cross fertilization 
flow information help direct search key thing lacking pst 
difficulties combining sparseness line learning fundamental require subtle changes exhaustive batch techniques 
sparse grams useful alternatives exhaustive grams data overwhelms memory 
autonomous agents complex environments case confidently estimated low frequency patterns 
long run 
hierarchical sparse grams learn frequent patterns fewer parameters number potential patterns remembering data repeatedly iterating 
novel techniques falling back narrower distributions 
stochastic bias compositions smaller known frequent patterns facilitates line learning increasingly complex sparse representations 
result model capable finding frequent patterns huge search spaces cumulatively constructing larger representations frequent patterns demonstrated environment 
necessarily concentrated prediction primary way utilize knowledge frequent patterns chunks frequent chunks valuable ways larger computational system 
learned chunks act aids increase working memory capacity substitution recoding improves information processing capacity quite broadly frequent patterns important developing communication shared language 
frequent chunks serve important features types learning enable automatic formation associations impossible induce 
topics see sec 
point single general learning mechanism build representations enable useful predictions serve foundations improving aspects agent cognitive behavior 
sparseness fundamental complex knowledge representation 
semantic networks frames description logics ontologies knowledge real world domains 
continue investigate domain independent techniques choosing patterns exhibited environment include sparse representations 
extensive treatment material including motivations derivations formal description learning problem experimental results related comparisons explanations usefulness frequent patterns research landscape see 
pedro domingos geoff hulten 
mining high speed data streams 
knowledge discovery data mining pages 
richard sutton steven whitehead 
online learning random representations 
proceedings th international conference machine learning 
paul utgoff 
layered learning 
neural computation 
press 
weng mcclelland pentland sporns sur thelen 
autonomous mental development robots animals 
science 
karl pfleger 
line learning predictive hierarchies 
phd thesis stanford university june 
see ksl stanford edu thesis 
manning schutze 
foundations statistical natural language processing 

timothy bell john cleary ian witten 
text compression 
prentice hall 
joshua goodman gao 
language model size reduction pruning clustering 
international conference spoken language processing 
andrew moore mary soon lee 
cached sufficient statistics efficient machine learning large datasets 
journal artificial intelligence research 
schutze singer 
part speech tagging variable memory markov model 
proceedings annual meeting association computational linguistics 
stolcke 
bayesian learning probabilistic language models 
phd thesis berkeley 
oded maron andrew moore 
hoeffding races accelerating model selection search classification function approximation 
advances neural information processing 
ron singer tishby 
power amnesia 
machine learning 
fernando pereira yoram singer naftali tishby 
word grams 
proceedings third workshop large corpora 
nir friedman yoram singer 
efficient bayesian parameter estimation large discrete domains 
advances neural information processing systems 
hector garcia molina jeffrey ullman jennifer widom 
database systems complete book 
prentice hall 
agrawal mannila srikant toivonen verkamo 
fast discovery association rules 
advances knowledge discovery data mining 
aaai press 
mannila toivonen verkamo 
discovering frequent episodes sequences 
proceedings international conference knowledge discovery data mining 
craig nevill manning ian 
witten 
identifying hierarchical structure sequences linear time algorithm 
journal artificial intelligence research 
della pietra della pietra lafferty 
inducing features random fields 
ieee transactions pattern analysis machine intelligence 
kevin murphy mark 
linear time inference hierarchical hmms 
advances neural information processing systems 
george miller 
magic number plus minus limits capacity processing information 
psychological review 

