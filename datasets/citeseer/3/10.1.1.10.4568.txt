active learning drug discovery process manfred warmuth gunnar michael jun liao christian computer science dep univ calif santa cruz fhg 
berlin germany dupont pharmaceuticals california st san francisco 
cse ucsc edu gunnar anu edu au de investigate data mining problem computational chemistry large data set compounds find bind target molecule iterations biological testing possible 
iteration comparatively small batch compounds screened binding target 
apply active learning techniques selecting successive batches 
selection strategy picks unlabeled examples closest maximum margin hyperplane 
produces weight vectors running perceptrons multiple permutations data 
weight vector votes prediction pick unlabeled examples prediction evenly split third selection strategy note unlabeled example bisects version space consistent weight vectors 
estimate volume sides split bouncing billiard version space select unlabeled examples cause split version space 
demonstrate data sets provided dupont pharmaceuticals selection strategies perform comparably better selecting random batches testing 
important goals computational drug design find active compounds large databases quickly usually way obtain interpretable model specific subset compounds active 
activity typically defined author received partial support nsf ccr current address national university canberra 
partially supported dfg ja mu travel eu neurocolt ii 
current address gmbh der sankt augustin germany binding target molecule 
time iterative approach problem employed 
iteration batch unlabeled compounds screened target sort biological assay 
desired goal active hits show assays selected batches 
machine learning point view examples compounds initially unlabeled 
iteration learner selects batch un labeled examples labeled positive active negative inactive 
machine learning type problem called query learning ang selective sampling cal active learning tk 
round data set contains chemically diverse examples positive 
second round data set examples positives 
data set preselected basis chemistry intuition 
note classification problem fundamentally asymmetric data sets typically negative examples chemists interested positive hits compounds lead new drugs 
problem challenging compound described vector binary shape features 
vectors sparse average features set round compound round compound 
working retrospective data sets know labels 
simulate real life situation initially hiding labels giving algorithm labels requested batches examples virtual screening 
long term goal type research provide computer program chemists interactive job point new unlabeled examples may added 
test completed labels program 
new test needs set chemist asks program suggest batch unlabeled compounds 
suggested batch edited augmented invaluable knowledge intuition chemist 
hope computer assisted approach allows mining larger data sets quickly 
note compounds generated virtual combinatorial chemistry 
compound descriptors computed compounds synthesized 
words comparatively easy generate lots unlabeled data 
case round data set consists compounds vendor catalog corporate collec types compounds points active inactive unlabeled 
maximum margin hyperplane internal classifier 
tions 
design effort went harder round data set 
initial results encouraging 
selection strategies better choosing random batches indicating long term goal outlined may feasible 
machine learning point view fixed set points unlabeled labeled positive negative 
see 
binary descriptors compounds complete data linearly separable 
concentrate simple linear classifiers 
analyzed large number different ways produce hyperplanes combine hyperplanes 
section describe different selection strategies basis hyperplanes detail provide experimental comparison 
section give theoretical justification strategies effective 
data provided dupont pharmaceuticals 
current data sets kernels improve results shown 
different selection criteria performance selection algorithm specified parts batch size initialization selection strategy 
practice cost effective test single examples time 
chose total data set batch size matches reasonably typical experimental constraints 
initial batches chosen random positive negative example 
typically achieved batch 
batches chosen selection strategy 
mentioned selection strategies linear classifiers data labeled far 
examples normalized unit length consider homogeneous hyperplanes fx 
normal direction 
plane predicts sign 
example compound specify weight vector batch selecting unlabeled examples closest hyperplane 
simplest way obtain weight vector run perceptron labeled data produces consistent weight vector perc 
second selection strategy called svm uses maximum margin hyperplane produced support vector machine 
perceptron predict example handwritten characters shown voting predictions hyperplanes improves predictive performance fs 
start weight vector zero multiple passes data perceptron consistent 
processing example store weight vector 
remember weight vectors pass random permutations labeled examples 
weight vector gets vote 
prediction example positive total vote larger zero select unlabeled examples total vote closest zero call selection strategy 
dot product commutative 

point lies positive side hyperplane dual view point lies positive side hyperplane recall instances weight vectors unit length 
weight vector consistent labeled examples xn yn lie yn side plane xn set consistent weight vectors called version space section unit hypersphere bounded planes corresponding labeled examples 
unlabeled hyperplane xn bisects version space 
third selection strategy billiard bounced times inside version space fraction fn bounce points positive side xn computed 
prediction xn positive fn larger half strategy selects unlabeled points fraction closest half 
left plot true positives false positives data set perc showing performs slightly better 
lower variance right 
left shows averaged true positives false positives svm 
note perform similarly 
plotted roc curves batch added shown 
plots show strategies comparable 
strategies svm perform better corresponding strategies selection criterion select random unlabeled examples closest criterion 
example show svm significantly better svm rand 
surprisingly improvement larger easier round data set 
reason round smaller fraction positive examples 
recall surprisingly smart bookkeeping done essentially computational overhead 
fs voting predictions weight vectors average weight vectors normalizing select unlabeled examples closest resulting single weight vector 
way averaging leads slightly worse results shown 
fraction examples selected number examples perc true pos perc false pos true pos false pos fraction examples selected standard deviation perc true pos perc false pos true pos false pos left average runs true positives false positives entire round data set batch perc 
right standard deviation runs 
fraction examples selected number examples true pos false pos svm true pos svm false pos true pos false pos fraction examples selected total number hits batch size example batch size left average runs true false positives entire round data set batch svm 
right comparison batch size example batch size round data 
round data preselected chemists fraction raised 
suggest methods particularly suitable positive examples hidden large set negative examples 
simple strategy svm choosing unlabeled examples closest maximum margin hyperplane investigated authors ccs character recognition tk text categorization 
labeled points closest hyperplane called support vectors points removed maximum margin hyperplane remains unchanged 
visualize location points relation center hyperplane 
show location points projected normal direction hyperplane 
batch location points scattered thin stripe 
hyperplane crosses stripe middle 
left plot distances scaled support vectors distance 
right plot geometric distance hyperplane plotted 
recall pick unlabeled points closest hyperplane center stripe 
soon window support vectors cleaned positive examples compare svm curves left 
shrinking width geometric window corresponds improved generalization 
far selection strategies svm shown similar performance 
question performance criterion considered far suitable drug design application 
goal label verify positive compounds quickly 
think total number positives hits examples tested far best performance criterion 
note total number hits random selection strategy grows linearly number batches random batch fraction examples selected number examples random true pos random false pos closest true pos closest false pos fraction examples selected number examples random true pos random false pos closest true pos closest false pos comparisons svm random batch selection closest batch selection 
left round data 
right round data 
normalized distance hyperplane geometric distance hyperplane fraction examples selected left scatter plot distance examples maximum margin hyperplane normalized support vectors 
right scatter plot geometric distance examples hyperplane 
stripe shows location random sub sample points round data additional batch labeled svm 
selected examples black unselected positives red plus unselected negatives blue square 
expect hits 
contrast total number hits svm batch random faster see 
performs best 
positive examples valuable application changed selection strategy svm selecting unlabeled examples largest positive distance 
maximum margin hyperplane svm smallest distance jw 
xj 
correspondingly picks unlabeled example highest vote picks unlabeled example largest fraction fn total hit plots resulting modified strategies svm improved see versus 
generalization plots modified strategies curves left slightly worse new versions 
sense original strategies better exploration giving better generalization entire data set modified strategies better exploitation higher number total hits 
show trade svm svm trade occurs pairs strategies shown 
investigate effect batch size performance 
simplicity show total hit plots right 
note data batch size examples round performing worse experimentally unrealistic batch size example 
results batch size better means selecting right left fraction examples selected total number hits svm fraction examples selected total number hits svm total hit performance round left round right data sv batch size 
fraction examples selected total number hits svm fraction examples selected total number hits svm total hit performance round left round right data sv batch size 
results larger batch sizes sophisticated selection strategies worth exploring pick say batch close time diverse 
point data sets small able precompute dot products kernel matrix 
preprocessing pass perceptron nm number labeled examples number mistakes 
finding maximum margin hyperplane estimated time 
computation need spend bounce billiard 
implementations svm light joa billiard algorithm rm hgc 
internal hypothesis algorithm applying selection criterion need evaluate hypothesis unlabeled point 
cost proportional number support vectors svm methods proportional number mistakes perceptron methods 
case need time bounce number labeled points 
clearly slowest 
larger data sets simplest adaptable 
theoretical justifications see right geometric margin support vectors half width window shrinking examples labeled 
goal reasonable designing selection strategies pick unlabeled examples cause margin shrink 
simplest strategy pick examples closest maximum margin hyperplane example expected change maximum margin fraction examples selected svm svm fraction examples selected number examples svm true pos svm true pos svm false pos svm false pos exploitation versus exploration left total hit performance right true false positives performance right svm sv round data hyperplane tk ccs 
alternative goal reduce volume version space 
volume rough measure remaining uncertainty data 
recall weight vectors instances unit length 

distance point plane dual view distance point plane maximum margin hyperplane point version space largest sphere completely contained version space rm 
labeling side plane remains 
passes close point half largest sphere eliminated version space 
second justification selecting unlabeled examples closest maximum margin hyperplane 
selection strategy starts point inside version space bounces billiard times 
billiard ergodic see discussion 
fraction fn bounces positive side unlabeled hyperplane xn estimate fraction volume positive side xn unknown xn labeled best example split version space half 
select unlabeled points fn closest half 
thinking underlying strategy closely related committee machine random concepts version space asked vote random example label example requested vote close split sos 
tried improve estimate volume replacing fn fraction total trajectory located positive side xn data sets improve performance shown 
averaged bounce points 
resulting weight vector approximation center mass version space approximates called bayes point property unlabeled hyperplane passing bayes point cuts version space roughly half 
tested selection strategy picks unlabeled points closest estimated center mass strategy indistinguishable strategies bouncing billiard 
rigorous justification variants algorithms 
showed active learning paradigm ideally fits drug design cycle 
deliberations concluded total number positive examples hits tested examples best performance criterion drug design application 
dimension point exactly 
number different selection strategies comparable performance 
variants select unlabeled examples highest score variants perform better 
selection strategies voted perceptron versatile showed slightly better performance 
ang angluin 
queries concept learning 
machine learning 
boser guyon vapnik 
training algorithm optimal margin classifiers 
haussler editor proceedings th annual acm workshop computational learning theory pages 
cal cohn atlas ladner 
training connectionist networks queries selective sampling 
advances neural information processing systems 
ccs campbell cristianini smola 
query learning large margin classifiers 
proceedings icml page stanford ca 
fs freund schapire 
large margin classification perceptron algorithm 
proc 
th annu 
conf 
comput 
learning theory 
acm press new york ny july 
hgc ralf herbrich graepel colin campbell 
bayes point machines estimating bayes point kernel space 
proceedings ijcai workshop support vector machines pages 
joa joachims 
making large scale svm learning practical 
scholkopf burges smola editors advances kernel methods support vector learning pages cambridge ma 
mit press 
myers greene saunders 
rapid reliable drug discovery 
today chemist 
rm marchand 
computing bayes kernel classifier 
advances large margin classifiers volume pages 
mit press 

playing billiard version space 
neural computation 
sos seung opper sompolinsky 
query committee 
proceedings fifth workshop computational learning theory pages 
tk tong koller 
support vector machine active learning applications text classification 
proceedings seventeenth international conference machine learning san francisco ca 
morgan kaufmann 
