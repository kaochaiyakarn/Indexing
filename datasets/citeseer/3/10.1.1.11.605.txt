content publication coral michael freedman eric david mazi eres new york university www scs cs nyu edu coral coralcdn peer peer content distribution network allows user run web site offers high performance meets huge demand price cheap broadband internet connection 
volunteer sites run coralcdn automatically replicate content side effect users accessing 
publishing coralcdn simple making small change hostname object url peer peer dns layer transparently redirects browsers nearby participating cache nodes turn cooperate minimize load origin web server 
system key goals avoid creating hot spots volunteers hurt performance 
achieves coral latency optimized hierarchical indexing infrastructure novel abstraction called distributed sloppy hash table dsht 
availability content internet large degree function cost publisher 
funded web site reach huge numbers people combination load balanced servers fast network connections commercial content distribution networks cdns 
publishers afford amenities limited size audience type content serve 
sites risk sudden overload publicity phenomenon slashdot effect popular web site periodically links provisioned servers driving levels traffic 
struggling content providers forced expend significant resources content distribution 
fortunately static content easy way popular data reach people publishers afford serve volunteers mirror data servers networks 
internet long history organizations network connectivity mirroring data consider value 
peer peer file sharing demonstrated willingness individual broadband users dedicate upstream bandwidth redistribute content users enjoy 
additionally organizations mirror popular content reduce downstream bandwidth utilization improve latency local users accessing mirror 
describes coralcdn decentralized selforganizing peer peer web content distribution network 
coralcdn leverages aggregate bandwidth volunteers running software absorb dissipate traffic web sites system 
doing coralcdn replicates content proportion content popularity regardless publisher resources effect content publication 
coralcdn content publisher posting link high traffic portal simply appends net hostname url 
dns redirection oblivious clients unmodified web browsers transparently redirected nearby coral web caches 
caches cooperate transfer data nearby peers possible minimizing load origin web server latency experienced browsers 
coralcdn built top novel key value indexing infrastructure called coral 
properties coral ideal cdns 
coral allows nodes locate nearby cached copies web objects querying distant nodes 
second coral prevents hot spots infrastructure degenerate loads 
instance node repeatedly stores key rate requests heavily loaded machine logarithmic total number nodes 
coral exploits overlay routing techniques popularized number peer peer distributed hash tables dhts 
coral differs dhts ways 
coral locality hot spot prevention properties possible dhts 
second coral architecture clusters connected machines 
clusters exposed interface higherlevel software fact form crucial part dns redirection mechanism 
achieve goals coral provides weaker consistency traditional dhts 
reason call indexing abstraction distributed sloppy hash table dsht 
coralcdn number contributions 
enables people publish content previously distribution costs 
completely decentralized self organizing web content distribution network 
coral indexing infrastructure pro vides new abstraction potentially application needs locate nearby instances resources network 
coral introduces epidemic clustering algorithm exploits distributed network measurements 
furthermore coral peer peer key value index scale stores key hot spot congestion new rate limiting technique 
coralcdn contains peer peer dns redirection infrastructure allowing system inter operate unmodified web browsers 
measurements coralcdn demonstrate allows provisioned web sites achieve dramatically higher capacity clustering provides quantitatively better performance locality unaware systems 
remainder structured follows 
section provides high level description coralcdn section describes dns system web caching components 
section describe coral indexing infrastructure underlying dsht layers clustering algorithms 
section includes implementation overview section presents experimental results 
section describes related section discusses section concludes 
coral content distribution network coral content distribution network coralcdn composed main parts network cooperative proxies handle users requests network dns nameservers net map clients nearby coral proxies underlying coral indexing infrastructure clustering machinery applications built 
usage models enable immediate incremental deployment coralcdn transparent clients requires software plug installation 
coralcdn variety ways including publishers 
web site publisher com change selected urls web pages urls www com 
net jpg 
third parties 
interested third party poster web portal usenet group url publishing causing embedded relative links coralcdn 
users 
coral aware users manually construct urls surfing slow overloaded coral proxy definitely provides proxy functionality proxy strict rfc sense serves requests syntactically formatted ordinary server 
coral www com www com net dns srv prx coral dns srv prx coral dns srv prx coral dns srv prx coral dns srv prx coral resolver browser dns srv dns srv prx coral prx net coralcdn steps involved resolving url returning corresponding file section 
rounded boxes represent coralcdn nodes running coral dns servers 
solid arrows correspond coral rpcs dashed arrows dns traffic dotted dashed arrows network probes dotted arrows traffic 
web sites 
relative links redirects automatically 
system overview shows steps occur client accesses url www com 
net standard web browser 
main stages dns redirection request handling coral indexing infrastructure 

client sends dns request www com 
net local resolver 

client resolver attempts resolve hostname coral dns server possibly starting registered net domain 

receiving query coral dns server probes client determines round trip time network hops 

probe results dns server checks coral see known nameservers proxies near client resolver 

dns server replies returning servers coral previous step returns random set nameservers proxies 
case dns server close client returns nodes close see section 

client resolver returns address coral proxy www com net 

client sends request www 
com net specified proxy 
proxy caching file locally returns file stops 
process continues 

proxy looks web object url coral 

coral returns address node caching object proxy fetches object node 
proxy downloads object origin server www com shown 

proxy stores web object returns client browser 

proxy stores coral recording fact caching url 
coral indexing abstraction section introduces coral indexing infrastructure coralcdn 
coral provides distributed sloppy hash table dsht abstraction 
designed applications storing soft state key value pairs multiple values may stored key 
coralcdn uses mechanism map variety types key addresses coralcdn nodes 
particular uses find coral nameservers topologically close clients networks find proxies caching particular web objects locate nearby coral nodes purposes minimizing internal request latency 
global overlay coral node belongs distinct called clusters :10.1.1.110.5867:10.1.1.159.9358
cluster characterized maximum desired network round trip time rtt call diameter 
system parameterized fixed hierarchy diameters known levels 
node member dsht level 
group nodes form level cluster high fraction pair wise rtts level diameter threshold 
coral implementation allows arbitrarily deep dsht hierarchy describes level hierarchy thresholds msec msec level clusters respectively 
coral queries nodes higher level fast clusters lower level slower clusters 
reduces latency lookups increases chances returning values stored nearby nodes 
coral provides interface higher level applications put key val ttl levels inserts mapping key arbitrary value specifying live 
caller may optionally specify subset cluster hierarchy restrict operation certain levels 
get key levels retrieves subset values stored key 
optionally specify subset cluster hierarchy 
nodes level count target services returns count neighbors belonging node cluster specified level target supplied specifies ip address machine returned nodes ideally near 
coral probe target exploit network topology hints stored dsht satisfy request 
services specified coral return nodes running particular service proxy dns server 
levels returns number levels coral hierarchy corresponding rtt thresholds 
section describes design coralcdn dns redirector proxy especially regard coral dsht abstraction clustering hierarchy returning coral section 
application layer components coral dns server directs browsers fetching urls coral proxies attempting find ones near requesting client 
proxies exploit caches way minimize transfer latency load origin web servers 
coral dns server coral dns server returns ip addresses coral proxies browsers look hostnames urls 
improve locality attempts return proxies near requesting clients 
particular dns resolver client contacts nearby instance returns proxies appropriate cluster ensures dns requests client need leave cluster 
nodes function exploits coral thefly network measurement capabilities stored topology hints increase chances clients discovering nearby dns servers 
specifically instance authoritative nameserver domain net 
assuming level hierarchy coral generally configured maps domain name 
net coral proxies 
level hierarchy domain name extended ln obvious way 
names somewhat unwieldy established dns dname alias net target 
net 
domain name net equivalent name suffix net allowing urls concise form www com net 
assumes web browsers generally close resolvers network source ad dress dns query reflects browser network location 
assumption holds varying degrees akamai digital island mirror image successfully deployed commercial cdns dns redirection 
locality problem reduced returning proxies near source dns request 
order achieve locality measures round trip time resolver categorizes level 
level hierarchy resolver correspond level level level client depending rtt compares coral thresholds 
asked address hostname net reply contains sections interest set addresses name answers query set nameservers name domain known authority section dns reply 
returns addresses cluster level corresponds client level categorization 
words rtt dns client level threshold best return addresses coral nodes level cluster 
obtains list nodes nodes function 
note returns addresses short time live fields seconds levels level 
achieve better locality specifies client ip address target argument nodes 
causes coral probe addresses network hops client results look clustering hints 
avoid significantly delaying clients coral maps network hops fast built traceroute mechanism combines concurrent probes aggressive time outs minimize latency 
entire mapping process generally requires rtts bytes bandwidth 
coral node caches results avoid repeatedly probing client 
closer client better selection addresses client 
exploits authority section dns replies lock dns client cluster happens nearby 
answer section selects nameservers returns appropriate cluster level uses target argument exploit measurement network hints 
addresses answer section gives nameservers authority section long ttl hour 
nearby override inferior nameservers dns client may caching previous queries 
manipulating domain returned nameservers servers 
clients distant level timing threshold claims return nameservers domain net 
clients closer threshold returns nameservers net 
clients closer level threshold returns nameservers domain net 
dns resolvers query servers specific known domain scheme allows closer instances override results distant ones 
unfortunately resolvers tolerate fraction unavailable dns servers browsers handle bad servers gracefully 
reason returning addresses short ttl fields 
added precaution returns addresses verified hand 
means synchronously checking proxy status udp rpc prior replying dns query 
note people wish contribute upstream bandwidth flag proxy non recursive case return proxy clients local networks 
coral proxy coral proxy satisfies requests urls 
seeks provide reasonable request latency high system throughput serving data origin servers comparatively slow network links home broadband connections 
design space requires particular care minimizing load origin servers compared traditional cdns reasons 
coral origin servers slower network connections typical customers commercial cdns 
second commercial cdns collocate number machines deployment site select proxies part url requested effectively distributing urls proxies 
coral contrast selects proxies client locality 
coralcdn easier single proxy fetching particular url 
aggressively minimize load origin servers fetch web pages proxies possible 
proxy keeps local cache immediately fulfill requests 
client requests non resident url attempts locate cached copy referenced resource coral get resource indexed sha hash url 
discovers proxies data attempts fetch data proxy connects 
coral provides referrals referrals return data fetch resource directly origin 
fetching web object origin inserts time live seconds 
renew short lived completes download 
flash crowd suddenly fetches web page simultaneous requests naturally form kind multicast tree retrieving web page 
obtains full file inserts longer lived hour 
insertion algorithm accounts ttl longer lived overwrite shorter lived ones stored selected nodes high insertion load described section 
periodically renew referrals resources caches 
proxy evict web object cache may persist dsht 
ideally proxies adaptively set ttls cache capacity implemented 
coral hierarchical indexing system section describes coral indexing infrastructure coralcdn leverages achieve scalability selforganization efficient data retrieval 
describe coral implements put get operations form basis distributed sloppy hash table dsht abstraction underlying key routing layer dsht algorithms balance load changes enable latency data placement optimizations hierarchical set 
describe clustering mechanisms manage hierarchical structure 
coral key routing layer coral keys opaque bit id values nodes assigned ids bit identifier space 
node id sha hash ip address 
coral defines distance metric ids 
henceforth describe node close key distance key node id small 
coral put operation stores key value pair node close key 
get operation searches stored key value pairs nodes successively closer key 
support operations node requires mechanism discover nodes close arbitrary key 
dsht contains routing table 
key node routing table allows find node closer closest node 
routing tables kademlia defines distance values id space bitwise exclusive xor interpreted unsigned integer 
xor metric ids longer matching prefixes significant bits numerically closer 
size node routing table dsht logarithmic total number nodes comprising dsht 
node closest node key routing table contains clos nodeids rpc rpc target rpc target target distance nodeids xor example routing operations system containing nodes ids 
illustration node id looking node closest key sorted nodes distance top boxed row illustrates xor distances nodes initially known contacts known peer distance closest half distance illustration peer node zero distance 
data rpc requests responses shown parentheses braces respectively asks node zero peers half way closer distance 
inserts new routing table middle row 
repeats process contacting node distance closest contacts node distance completes search bottom row 
est node node distance bit shorter 
permits visit sequence nodes monotonically decreasing distances 
encoding binary number fewer bit result expected number iterations discover closest node logarithmic number nodes 
illustrates coral routing algorithm successively visits nodes distances key approximately halved iteration 
traditional routing layers attempt route directly node closest key possible resorting intermediate hops faced incomplete routing information :10.1.1.140.3129:10.1.1.10.1904:10.1.1.28.5987
caching additional routing state necessary log systems practice manage achieve routing constant number hops 
observe frequent key generate high levels traffic nodes close key 
congestion called tree saturation identified shared memory interconnection networks 
minimize tree saturation iteration coral search prefers correct bits time 
specifically splice designate significant bi bits followed significant bi bits node id wishes search key initializes variable iteration updates splice smallest value yields new value hop lookup path closest node exists routing table 
described limiting potentially closer known hops way coral avoid overloading node presence heavily accessed keys 
potential downside longer lookup paths higher lookup latency presence slow stale nodes 
order mitigate effects coral keeps window multiple outstanding rpcs lookup possibly contacting closest nodes intermediary target sloppy storage coral uses sloppy storage technique caches key value pairs nodes ids close key referenced 
cached values reduce hot spot congestion tree saturation indexing infrastructure frequently satisfy put get requests nodes closest key 
characteristic differs dhts put operations proceed nodes closest key 
insertion algorithm 
coral performs phase operation insert key value pair 
forward phase coral routes nodes successively closer key previously described 
avoid tree saturation insertion operation may terminate prior locating closest node key case key value pair stored distant node 
specifically forward phase terminates storing node happens node full loaded key 
node full respect key stores values ttls new value 

node loaded respect received maximum leakage rate requests past minute 
experiments meaning high load node claims loaded store attempt seconds 
prevents excessive numbers requests hitting key closest nodes allows requests propagate keep values nodes fresh 
experiments 
forward phase coral routing layer repeated rpcs contact nodes successively closer key 
remote nodes returns key loaded number values stores key minimum expiry time values 
client node uses information determine remote node accept store potentially evicting value shorter ttl 
forward phase terminates client node finds node closest key node full loaded respect key 
client node places contacted nodes full loaded stack ordered xor distance key 
reverse phase client node attempts insert value remote node referenced top stack element node closest key 
operation succeed due insertions client node pops stack tries insert new stack top 
process repeated store succeeds stack empty 
phase algorithm avoids tree saturation storing values progressively key 
eviction leakage rate ensure nodes close key retain long lived values live keys remain reachable nodes minute contact intermediate node including go contact nodes closer key 
perfectly balanced tree key closest node receives log store requests minute fixing bits iteration 
proof sketch 
node system nodes uniquely identified string log bits 
consider string bit digits 
node contact closest node key contacts node id differs key exactly digit 
log digits digit take values differ key 
node differs digit throttle requests minute 
closest node receives maximum rate log rpcs minute 
irregularities node id distribution may increase rate slightly rate traffic logarithmic traditional dhts linear 
section provides supporting experimental evidence 
retrieval algorithm 
retrieve value associated key node simply traverses id space rpcs 
finds peer storing remote peer returns corresponding list values 
node terminates search get returns 
requesting client application handles redundant application specific way contacts multiple sources parallel download cached content 
multiple stores key spread multiple nodes 
pointers retrieved application distributed stored providing load balancing coral servers coral 
hierarchical operations locality optimized routing data placement coral uses levels called clusters 
level cluster named randomly chosen bit cluster identifier level cluster id predefined recall set nodes form cluster average pair wise rtts threshold 
mentioned earlier describe level hierarchy thresholds msec msec level clusters respectively 
section experimental evidence client side benefit clustering 
illustrates coral hierarchical routing operations 
coral node node id clusters belongs view node projecting presence location clusters 
structure reflected coral basic routing infrastructure particular support switching node distinct midway lookup 
hierarchical retrieval algorithm 
requesting node specifies starting stopping levels coral search 
default initiates get query highest level cluster try take advantage network locality 
routing rpcs cluster hit node storing key rpc fig 
lookup halts returns corresponding stored value hit searching lower level clusters 
key lookup reach closest node cluster rpc signifying failure level 
node continues search level cluster 
clusters concentric exists identical location identifier space clusters shown 
begins searching onward level cluster rpc having traversed id space prefix 
search eventually switches global cluster rpc total number rpcs required single level lookup service lookup continues point left identifier space previous cluster 
lookups fast system tightly bound rpc timeouts pointers higherlevel clusters data local cluster 
hierarchical insertion algorithm 
node starts performing put level cluster section nearby nodes take advantage locality 
initially built coral chord routing layer block box difficulties maintaining distinct clusters complexity subsequent system caused scrap implementation 
bit id space level level level coral hierarchical routing structure 
nodes ids clusters higher level clusters naturally sparser 
note node identified cluster shortest unique id prefix level cluster nodes sharing id prefixes located common subtrees closer xor metric 
higher level neighbors usually share lower level clusters shown necessarily 
rpcs retrieval key sequentially numbered 
placement correct context local level cluster 
provided key loaded node continues insertion level cluster point key inserted level retrieval case 
coral traverses id space 
illustrated practice results loose hierarchical cache lower level cluster contains nearly data stored higher level clusters members belong 
enable cluster aware behavior headers coral rpc include sender cluster information identifier age size estimate clusters 
recipient uses information demultiplex requests properly recipient consider put get levels shares cluster sender 
additionally information drives routing table management nodes added removed local cluster specific routing tables ac cluster information accumulated drive cluster management described 
joining managing clusters peer peer system peer contacts existing node join system 
new node queries seed routing tables 
clusters coral adds important requirement node join acceptable cluster acceptability requires latency nodes cluster threshold 
node easily determine condition holds recording minimum round trip times rtts subset nodes belonging cluster 
nodes learn clusters side effect normal lookups coral exploits store hints 
coral starts uses built fast traceroute mechanism described section determine addresses routers hops 
excluding private rfc ip addresses coral uses router addresses keys index clustering hints 
specifically node stores mappings router address ip address udp port number 
new node sharing gateway joins network find hints quickly cluster assuming fact near addition nodes store mappings keys ip subnets directly connect bit prefixes gateway router addresses 
prefix hints coral level function traceroutes clients direction addresses forward reverse traceroute paths share bit prefixes 
nodes continuously collect clustering information peers rpcs include round trip times cluster membership estimates cluster size 
minutes node considers changing cluster membership collected data 
collected data indicates alternative candidate cluster desirable node validates collected data contacting nodes candidate cluster routing selected keys 
node form new singleton cluster accesses members cluster meet rtt constraints 
probes indicate cluster nodes acceptable ttls cluster larger replaces node current cluster 
multiple clusters acceptable coral chooses largest cluster 
unfortunately coral rough approximations cluster size routing table size 
nearby clusters similar sizes inaccurate estimations lead oscillation nodes flow back forth observed behavior 
perturb oscillating system stable state coral employs preference function shifts hour 
node selects larger cluster holds log size log size min age age age current time minus cluster creation time 
node simply selects cluster lower cluster id square wave function takes value number hours odd number 
clusters disproportionate size selection function immediately favors larger cluster 
transition perturbs clusters steady state 
case node switches clusters remains routing tables nodes old cluster 
old neighbors contact learn new potentially better cluster 
produces avalanche effect nodes switch larger cluster 
merging clusters beneficial 
small cluster diameter provides fast lookup large cluster capacity increases hit rate 
implementation coral indexing system composed client library stand daemon 
simple client library allows applications dns server proxy connect interface coral daemon 
coral lines dns server lines proxy additional lines 
components asynchronous library provided sfs toolkit structured asynchronous events callbacks 
coral network communication rpc udp 
successfully run coral linux openbsd freebsd mac os evaluation section provide experimental results support hypotheses 
coralcdn dramatically reduces load servers solving flash crowd problem 

clustering provides performance gains popular data resulting client performance 

coral naturally forms suitable clusters 

coral prevents hot spots indexing system 
clusters similar size continuously exchange members zero soon transitions nodes flow cluster lower cluster id clusters oscillate estimations hit times larger nodes flow larger returns zero 
examine claims wide area measurements synthetic load coralcdn nodes running planetlab internationally deployed test bed 
experimental setup traditional tests cdns web servers interesting evaluating coralcdn client side traces generally measure data client latencies 
mainly interested system handles load spikes 
benchmark tests specweb measure web server throughput disk bound access patterns coralcdn designed reduce load shelf web servers network bound 
basic structure experiments follows 
planetlab machines geographically distributed mainly north america europe launch coral daemon 
experiments referred multi level configure level hierarchy setting clustering rtt threshold level msec level msec 
experiments referred single level level global cluster 
objects evicted caches experiments 
simplicity nodes seeded known host 
network allowed stabilize minutes 
second run unmodified apache web server sitting dsl line kbit sec upstream bandwidth serving different kb files representing groups embedded images referenced web pages 
third launch client processes machine additional random delay seconds asynchrony making get requests urls 
client generates requests group files corresponding randomly selected web page period minutes 
recognize web traffic generally zipf distribution attempting merely simulate flash crowd popular web page multiple large embedded images slashdot effect 
clients generating requests sec resulting cumulative download rate approximately kb sec 
rate orders magnitude greater origin web server handle 
note rate chosen synthetically way suggests maximum system throughput 
experiment section run clients 
coral nodes generate requests high rates key examine dsht indexing infrastructure prevents nodes close target id overloaded 
stabilization time shorter reducing clustering period minutes 
additionally real applications clustering fact simpler task new nodes immediately join nearby large clusters join pre established system 
setup clusters develop initial network comprised entirely singletons 
time sec level level level origin server number client accesses origin server 
accesses reported relative cluster level data fetched include requests handled local caches 
server load plots number requests minute handled local cache 
initial minute requests hit origin web server unique files 
redundant lookups due simultaneity requests generated subsequently requests handled coralcdn wide area cooperative cache proxy local cache supporting hypothesis coralcdn migrate load web server 
minute equal numbers requests handled level level cluster caches 
files propagated caches requests quickly resolved faster level clusters 
minutes files replicated nearly server client requests went proxies local caches 
repeated runs experiment yielded variance relative magnitudes initial spikes requests different levels number origin server hits remained consistent 
client latency shows latency client fetch file coralcdn steps section 
top graph shows latency planetlab nodes experiment bottom graph includes data clients located nodes asia hong kong taiwan japan nodes located europe performance benefit clustering pronounced graph asian nodes 
recall latency includes time client dns request connect fraction requests single level multi level multi level traceroute fraction requests asia single level asia multi level asia multi level traceroute request latency sec nodes asian nodes single level multi level multi level traceroute client latency requests urls comparing effect single level vs multi level clusters traceroute dns redirection 
top graph includes nodes bottom nodes asia 
discovered 
proxy attempts fulfill client request local cache coral origin web server 
note implements cut routing forwarding data client prior receiving entire file 
figures report results distribution latency clients single level cluster solid line distribution latencies clients multi level clusters dashed hierarchical network traceroute dns resolution map clients nearby proxies dotted 
clients ran subnet host fact experimental setup 
case real deployment expect com latency sec fraction requests single level multi level latencies proxy get keys coral 
bination hosts sharing networks ip prefix registered coral hosts 
multi level network traceroute provides lowest latency percentiles multi level system traceroute performs better single level system 
clustering clear performance benefit clients benefit particularly apparent poorly connected hosts 
shows latency get operations seen lookup urls coral step section 
plot get latency single level system vs multi level systems 
multi level system times faster percentile 
percentile single level system faster heavy packet loss multi system requires timeouts traverses hierarchy levels 
clustering illustrates snapshot clusters previous experiments time clients began fetching urls minutes 
map meant provide qualitative feel organic nature cluster development opposed offering quantitative measurements 
maps unique non singleton cluster network assigned letter 
plotted location nodes latitude longitude coordinates 
nodes belong cluster represented letter 
planetlab site usually collocates servers size letter expresses number nodes site belong cluster 
example large world map map correspond nodes collocated berkeley 
include singleton clusters maps improve readability post run analysis showed nodes rtts surprisingly site coral thresholds 
nodes nodes node nodes nodes node world view level clusters msec threshold united states view level clusters msec threshold 
unique non singleton cluster assigned letter size letter corresponds collocated nodes cluster 
world map shows coral natural divisions sets nodes geospatial lines msec threshold 
map shows distinct regions dramatic eastern 
nodes western 
nodes europe nodes 
close correlation network physical distance suggests speed light delays dominate round 
note plot singleton clusters map include asian nodes japan taiwan respectively 
united states map shows level clusters roughly separated physical locality 
map shows distinct clusters obvious clusters include california nodes pacific northwest nodes south midwest northeast corridor cluster contains nodes stretching north carolina massachusetts 
interesting aspect map separate non singleton clusters san francisco bay area 
close examination individual rtts sites shows widely varying latencies coral clustered correctly underlying network topology 
load balancing shows extent dsht balances requests key id experiment ran nodes earlier hosts total nodes 
configured system single near far requests minute distance hotspot total number put rpcs hitting coral node minute sorted distance node id target key 
level cluster 
time planetlab nodes began issue back back put get requests maximum non concurrent rates 
operations referenced key values stored put requests randomized 
average node issued put get operation pairs second total approximately put get requests minute fraction hit network 
node storing key get requests satisfied locally 
loaded node allows leakage rate rpcs minute 
graphs show number put rpcs hit node steady state sorted xor distance node id key 
minute closest node received put rpcs 
second minute shown system reached steady state closest node receiving put rpcs minute 
recall equation section predicts receive log rpcs minute 
plot strongly emphasizes efficacy leakage rate number rpcs received majority nodes low multiple 
nodes far side graph received rpcs 
coral routing algorithm explains condition nodes routing flipping id bit match key subsequently contact node near side 
omitted graph get rpcs minute loaded node received rpcs subsequently key widely distributed system 
related coralcdn builds previous peer peer systems web content delivery 
dhts directory services distributed hash table dht exposes basic functions application put key value stores value specified key id get key returns stored value just normal hash table 
dhts key routing layer chord kademlia pastry tapestry store keys node id closest key 
keys distributed balance load nodes 
dhts replicate multiply fetched key value pairs scalability having peers replicate pair second peer contacted part get request 
dhts act actual data stores merely directory services storing pointers 
cfs past take approach build distributed file system require true read write consistency operations writes atomically replace previously stored values modify 
network directory service tapestry coral relax consistency operations network 
put key tapestry routes fast hops peers placing peer pointer back sending node reaches node closest key 
nearby nodes routing key follow similar paths discover cached pointers 
coral flexible clustering provides similar lookup data placement algorithms prevent multiple stores forming hot spots 
skipnet builds hierarchy lookup groups explicitly groups nodes domain name support organizational disconnect 
web caching content distribution web caching systems fit large class cdns handle high demand diverse replication 
prior interest peer peer systems projects proposed cooperative web caching :10.1.1.21.1584
systems multicast queries require caches know servers worsens scalability fault tolerance susceptibility hot spots 
cache hit rate cooperative web caching increases certain level corresponding moderate population size highly scalable cooperative systems increase total system throughput reducing server side load 
projects considered peer peer overlays web caching systems benefit participating clients require widespread adoption reduce server load 
dht cache replicas proofs uses randomized overlay distribute popular content 
systems focus solely mitigating flash crowds suffer high request latency 
squirrel proposes web caching traditional dht organization wide networks 
squirrel reported poor load balancing system stored pointers dht 
attribute dht inability handle values key squirrel stored pointers object coralcdn proxies storing different sets pointers different nodes 
scan examined replication policies data disseminated multicast tree dht deployed isps 
akamai commercial cdns dns redirection reroute client requests local clusters machines having built detailed maps internet combination bgp feeds measurements traceroutes numerous vantage points 
reaching cluster collocated machines hashing schemes map requests specific machines increase capacity 
systems require deploying large numbers highly provisioned servers typically result performance latency throughput customers 
centrally managed cdns appear offer benefits coralcdn 
coralcdn network measurements traceroute probing dns clients somewhat constrained comparison 
coralcdn nodes bgp feeds tight latency constraints avoid delaying dns replies probing 
additionally coral design assumes single node knows identity nodes system precise network location 
people adopt system build rich database neighboring networks 
coralcdn offers aggregate storage capacity cache management completely localized 
designed larger number machines vantage points coralcdn may provide better performance small organizations hosting nodes economically efficient commercial cdns deploy machines bottleneck links 
provided users set open web proxies 
users reconfigure browsers proxy subsequently enjoy better performance 
system deployed anecdotal evidence suggests successful distributing content efficiently 
earlier simulation results show certain policies achieve high system throughput low request latency 
specific details deployed system published including akamai service development 
gives participating users better performance web sites coralcdn goal gives users better performance participating web sites publishers urls 
design points pose somewhat dif ferent challenges 
instance coralcdn takes pains greatly minimize load provisioned origin servers tighter latency requirements critical path web requests 
suffered number administrative headaches problems apply coralcdn coralcdn allow post operations ssl tunneling barred accessing particular sites affecting users browsing experience 
security 
address coralcdn security issues 
probably important issue ensuring integrity cached data 
experience spam internet expect adversaries attempt replace cached data advertisements pornography prescription drugs 
solution breaks components 
honest coral nodes cache invalid data 
possible solution include embedding pathnames urls solution requires server buy 
second coral nodes able trace path cached data taken exclude data known bad systems 
third try prevent clients malicious proxies 
requires client buy offers additional incentives organizations run coral recall client access local proxy available administrators configure local dns resolver return specific coral instance 
alternatively ssl splitting provides security clients servers albeit higher overhead origin servers 
coralcdn may require additional mechanisms throttling bandwidth hogs restricting access address authenticated content 
leverage redundant resources considering efficient erasure coding large file transfers 
developed fly verification mechanisms limit malicious proxies abilities waste node downstream bandwidth 
leveraging clustering abstraction 
presents clustering mainly performance optimization lookup operations dns redirection 
clustering algorithms driven generic policies allow hierarchy creation variety criteria 
example provide clustering policy ip routing block name simple mechanism reflects administrative control performs network partition 
coral clusters explicitly encode web trust security model system especially useful standard open admissions policy 
clusters easily represent trust relationships allowing lookups resolve trustworthy hosts 
clustering may prove useful abstraction building interesting applications 
multi cast tree formation 
coralcdn may transmit multiple requests origin server flash crowd 
caused race condition key closest node eliminate extending store transactions provide return status information test set shared memory systems 
similar extensions store semantics may useful balancing dynamically formed dissemination trees 
handling heterogeneous proxies 
consider heterogeneity proxies performing dns redirection intra coral fetches 
type feedback allocation policy proxies return current load bandwidth availability probed determine liveness 
deployment scalability studies 
planning initial deployment coralcdn long lived planetlab port dns service 
doing hope gather measurements large active client population better quantify coralcdn scalability effectiveness client transparency achieving wide spread easier peer peer systems 
coralcdn peer peer web content distribution network harnesses people willingness redistribute data find useful 
indexes cached web content new distributed storage abstraction called dsht 
map key multiple values scale stores key hot spot congestion 
coral successfully clusters nodes network diameter ensuring nearby replicas data located retrieved querying distant nodes 
peer peer dns layer redirects clients nearby allowing unmodified web browsers benefit coralcdn importantly avoid overloading origin servers 
measurements coralcdn demonstrate allows provisioned web sites achieve dramatically higher capacity 
web server dsl line experiences hardly load hit flash crowd sustained aggregate transfer rate orders magnitude greater bandwidth 
coral clustering mechanism forms qualitatively sensible geographic clusters provides quantitatively better performance locality unaware systems 
coralcdn freely available people slow connections publish web sites capacity grows automatically popularity 
please visit www scs cs nyu edu coral 
acknowledgments 
grateful vijay karamcheti early conversations helped shape 
david andersen nick feamster daniel robert grimm shepherd marvin theimer helpful feedback drafts 
maymounkov max krohn provided access kademlia data structure parsing code respectively 
planetlab support team allowing udp port dns despite additional caused 
coral part project iris project iris net supported nsf cooperative agreement 
ani 
david mazi eres supported alfred sloan research fellowship 
michael freedman supported fellowship 
akamai technologies www akamai com 
chankhunthod danzig neerdaels schwartz worrell 
hierarchical internet object cache 
usenix jan 
chen katz kubiatowicz 
scan dynamic scalable efficient content distribution network 
proceedings international conference pervasive computing zurich switzerland aug 
crawford 
rfc non terminal dns name redirection aug 
dabek kaashoek karger morris ion stoica 
wide area cooperative storage cfs 
sosp banff canada oct 
digital island www com 
fan cao almeida broder 
summary cache scalable wide area web cache sharing protocol 
technical report cs dept madison feb 
gadde chase rabinovich 
taste squid 
workshop internet server perf madison wi jun 
harvey jones saroiu theimer wolman 
skipnet scalable overlay network practical locality properties 
usits seattle wa mar 
iyer rowstron druschel 
squirrel decentralized peer peer web cache 
podc monterey ca jul 
karger lehman leighton levine lewin panigrahy 
consistent hashing random trees distributed caching protocols relieving hot spots world wide web 
stoc may 
karger sherman kim 
web caching consistent hashing 
www computer networks 
krohn freedman mazi eres 
fly verification erasure codes efficient content distribution 
ieee symp 
security privacy oakland ca may 
kubiatowicz bindel chen czerwinski eaton geels gummadi rhea weatherspoon weimer wells zhao 
oceanstore architecture persistent storage 
asplos cambridge ma nov 
laas kaashoek 
ssl splitting securely serving data untrusted caches 
usenix security washington aug 
lorch berger 
making world wide web caching servers cooperate 
www apr 
maymounkov mazi eres 
kademlia peer peer information system xor metric 
iptps cambridge ma mar 
maymounkov mazi eres 
codes big downloads 
iptps berkeley ca feb 
mazi eres 
toolkit user level file systems 
usenix boston ma jun 
mazi eres kaashoek 
escaping centralized control self certifying pathnames 
acm sigops european workshop sep 
mirror image internet 
www mirror image com 
fips publication secure hash standard 
national institute standards technology nist apr 
pai wang park pang peterson 
dark side web open proxy view 
hotnets cambridge ma nov 
pfister norton 
hot spot contention combining multistage interconnection networks 
ieee trans 
computers oct 
ratnasamy francis handley karp shenker 
scalable content addressable network 
acm sigcomm san diego ca aug 
rowstron druschel 
pastry scalable distributed object location routing large scale peer peer systems 
proc 
ifip acm middleware nov 
rowstron druschel 
storage management caching past large scale persistent peer peer storage utility 
sosp banff canada oct 
spring mahajan wetherall 
measuring isp topologies 
sigcomm pittsburgh pa aug 
maniatis baker 
peer peer caching schemes address flash crowds 
iptps cambridge ma mar 
rubenstein sahu 
lightweight robust system handle flash crowds 
ieee icnp paris france nov 
stoica morris liben nowell karger kaashoek dabek balakrishnan 
chord scalable peer peer lookup protocol internet applications 
ieee acm trans 
networking 
thaler ravishankar 
name mappings increase hit rates 
ieee acm trans 
networking 
wang pai peterson 
effectiveness request redirection cdn robustness 
osdi boston ma dec 
wolman voelker sharma cardwell karlin levy 
scale performance cooperative web proxy caching 
sosp kiawah island sc dec 
zhao huang stribling rhea joseph kubiatowicz 
tapestry resilient global scale overlay service deployment 
ieee selected areas communications 

