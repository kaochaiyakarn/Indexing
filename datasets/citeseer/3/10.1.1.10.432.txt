submitted int computer vision short version appeared iccv 
modeling visual patterns integrating descriptive generative methods cheng en guo song chun zhu ying wu department computer science department statistics university california los angeles stat ucla edu presents class statistical models integrate statistical modeling paradigms literature 
descriptive methods markov random elds minimax entropy learning ii :10.1.1.18.5785
generative methods principal component analysis independent component analysis transformed component analysis wavelet coding sparse coding 
demonstrate integrated framework constructing class hierarchical models texton patterns term texton coined psychologist julesz early 
bottom level model assume observed texture image generated multiple hidden texton maps textons map translated scaled stretched oriented versions window function mini templates wavelet bases 
texton maps generate observed image occlusion linear superposition 
bottom level model generative nature 
top level model spatial arrangements textons texton maps characterized minimax entropy principle leads versions gibbs point process models 
top level model descriptive nature 
demonstrate integrated model set experiments 
keywords descriptive models generative models gibbs point processes markov chain monte carlo markov random elds minimax entropy learning perceptual organization texton models visual learning 
vision algorithm accomplish depends crucially knows contents visual scenes knowledge mathematically represented general parsimonious models realistically characterize visual patterns ensemble images 
due variations patterns scenes richness details scene models statistical nature 
existing methods statistical modeling generally divided categories 
call category descriptive methods category generative methods 
descriptive methods construct model visual pattern imposing statistical constraints features extracted signals 
descriptive methods include markov random elds minimax entropy learning deformable models example methods texture modeling fall category :10.1.1.18.5785
models built pixel intensities deterministic transforms original signals linear ltering 
shortcomings descriptive methods fold 
capture high level semantics visual patterns important human perception 
example descriptive model texture realize cheetah skin pattern impressive synthesis results explicit notion individual blobs 
second descriptive models built directly original signals resulting probability densities high dimensions sampling inference computationally expensive 
desirable dimension reduction sparse representation models built low dimensional space better re ects intrinsic complexity pattern 
contrast descriptive methods generative methods postulate hidden variables causes complicated dependencies raw signals models hierarchical 
generative methods widely vision image analysis 
example principle component analysis pca independent component analysis ica transformed compo third category methods called discriminative 
goal discriminative methods modeling visual patterns explicitly approximating posterior probabilities directly example pattern recognition feed forward neural networks classi cation trees choose discuss focus statistical modeling 
see tu zhu incorporates discriminative methods markov chain monte carlo posterior sampling 
nent analysis tca wavelet image representation sparse coding random collage model generic natural images 
hidden variables employed represent generate observed image usually follow simple models 
existing generative models appear su er simpli ed assumption hidden variables independent identically distributed 
result sophisticated model realistic visual patterns 
example wavelet image coding model easily reconstruct observed image synthesize texture pattern independent random sampling spatial relationships wavelet coecients captured 
modeling paradigms developed independently somewhat disjoint communities working di erent problems relationship explored 
class probabilistic models integrate descriptive generative methods algorithm computational inference 
proposed method viewed perspectives examples natural patterns layered structures 
perceive texture impression terms pixel intensities repeated texture elements 
combines advantages descriptive generative methods provides general scheme modeling sophisticated visual patterns 
computer vision fundamental observation stated marr primal sketch paradigm natural visual interested readers referred discussion problem existing generative models 
patterns consist multiple layers stochastic processes 
example displays natural images 
look ivy wall image perceive texture impression terms pixel intensities see repeated elements ivy bricks 
capture hierarchical notion propose multi layer generative model shown 
inspired seminal olshausen field assume image generated layers stochastic processes layer consists nite number distinct similar elements called textons terminology julesz 
experiments texton covers pixels average layered representation achieves nearly fold dimension reduction sparsity 
sparse representation step modeling spatial arrangements geometric features 
particular textons layer characterized markov random eld mrf models minimax entropy learning previous mrf texture models considered special cases models layer texton just pixel :10.1.1.18.5785
see directly built olshausen field geometry linear bases characterized causal sketch model 
feel integrated model natural step linear superposition models wavelet sparse coding 

generative model image consists multiple layers texton maps superimposed occlusion plus background texture image belief descriptive models precursors generative models ingredients integrated learning process 
visual learning model initially built image intensities features computed deterministically image intensities 
replace features hidden causes process incrementally discover elements concepts textons curves ows elements levels causes elements lower abstractions 
instance ows generate curves curves generate textons turn generate pixel intensities 
stage elements level hidden causes characterized descriptive model deterministic features models derived minimax entropy principle demonstrated 
new hidden level elements introduced replaces current descriptive model simpli ed 
learning process evolves descriptive model elements simple certain vision purpose 
second integrated scheme provides representational de nition textons 
texton important notion texture perception early vision 
unfortunately expressed vaguely psychology precise de nition texton 
argue de nition texton possible context generative model 
contrast constraint clustering method malik leung textons naturally embedded generative model inferred hidden variables generative model 
consistent philosophy ica tca sparse coding 
textons de ned terms image bases window functions 
related explored de nitions textons combinations linear bases local elements shape shading third gestalt ensemble characterize hidden texton maps attributed point processes 
gestalt ensemble corresponds grand canonical ensemble statistical physics di ers traditional gibbs models having unknown number textons neighborhood changes dynamically 
relationships neighboring textons captured gestalt laws proximity continuity fourth adapt stochastic gradient algorithm learning inference 
algorithm simplify original likelihood function solve simpli ed maximum likelihood problem rst 
starting initial solution stochastic gradient algorithm nd re ned solutions 
demonstrate proposed modeling method texture images 
input texture image learning algorithm achieve objectives 
learning appearance textons stochastic process 
textons stochastic process translated scaled stretched oriented versions window function mini templates wavelet bases 

inferring hidden texton maps consists unknown number similar textons related ane transformations 

learning minimax entropy models stochastic processes generate textons maps 

verifying learned window functions generative models stochastic sampling 
variety texture synthesis techniques proposed notably successful methods efros freeman xu guo shum rearranging local image patches 
concerned learning parsimonious sucient models texture patterns 
models useful image understanding computer vision may lead graphics applications models may capture visually meaningful dimensions 
organized follows 
section introduces background generative descriptive methods 
section discusses hierarchical model texture 
section studies gestalt ensembles modeling texton processes 
section presents integrated modeling scheme 
section presents algorithm inferential computation 
experiments shown section 
conclude discussion section 
background descriptive generative models set images fi obs obs obs considered realizations underlying stochastic process governed frequency distribution 
objective visual learning estimate parsimonious probabilistic model approaches minimizing kullback leibler divergence kl jjp kl jjp log di log log practice expectation log replaced sample average 
standard maximum likelihood estimator mle arg min kl jjp arg max log obs family distributions searched 
general procedure search sequence nested probability families increasing complexities 
indexes dimensionality space 
example number free parameters model 
increases probability family general approach arbitrary preset precision 
choices families literature 
rst choice exponential family derived descriptive method maximum entropy root statistical mechanics 
descriptive method extracts set feature statistics deterministic transforms image denoted constructs model imposing descriptive constraints reproduces observed statistics obs extracted obs obs may consider projected statistics large projected marginal statistics chosen dimensions 
maximum entropy principle leads gibbs model expf parameters lagrange multipliers computed solving constraint equations 
features chosen minimum entropy principle :10.1.1.18.5785
descriptive learning method augments dimension space increasing number feature statistics generates sequence exponential families 
family includes mrf minimax entropy models texture :10.1.1.18.5785
example type descriptive model texture chooses histograms responses gabor lters 
second choice mixture family derived integration summation hidden variables wk dw ijw dw parameters generative model include parts 
assumes joint probability distribution generates conditional model ijw parameters hidden variables characterized model 
inferred probabilistic manner contrast deterministic features descriptive models 
generative method incrementally adds hidden variables augment space generates sequence mixture families 
example principal component analysis wavelet image coding sparse coding assume linear additive model image result linear superposition window functions plus gaussian noise process coecients eigen vectors pca wavelet bases image coding complete basis sparse coding 
hidden variables coecients bases plus noise ak 
coecients assumed pca bases orthogonal ak computed transform complete basis ak inferred 
independently identically distributed expf ja normalizing factor 
norm sparse coding basis pursuit principal component analysis 
simple distribution expf ja expf example parameters bases plus parameters occlusion models randomly positioned discs called random collage models see refs 

model exponential family 
literature hidden variables assumed iid gaussian laplacian distributed 
concept descriptive models 
multi layered generative model texture texture images texton processes 
texton represented rectangle window 
focus multi layer generative model texture images believe modeling method applied patterns object shapes 
image assumed generated layers stochastic processes layer consists nite number distinct similar elements called textons 
shows typical examples texture images texton represented rectangular window 
layered model shown 
textons layer image patches transformed square template th texton layer identi ed transformation variables lj lj lj lj lj lj lj lj lj represents texton center location lj scale size lj stretch aspect ratio height versus width lj orientation lj photometric transforms lighting variability 
lj de nes ane transform denoted lj pixels covered texton lj denoted lj image patch lj texton lj lj lj denotes transformation operator 
texton examples circular template di erent scales stretches orientations shown 
template transformed copies 

template 
scaled copy 
stretched copy 
scaled stretched rotated copy 
de ne collection textons layer texton map ft lj number textons layer layer texton map template generate image deterministically 
texton patches overlap site pixel value taken average lj lj lj true 
image pixels covered texton patches transparent 
image generated way 
tl obs symbol denotes occlusion linear addition means occludes called reconstructed image assumed gaussian noise process general stochastic texture 
pixel value site image top layer image point uncovered pixels modeled noises 
generative model hidden variables lg indexes order relative depth th layer 
simplify computation assume stochastic layers called background foreground respectively 
texton process assumed independent 
assumption okay simple texture patterns studied sophisticated patterns certainly necessary levels consider dependencies levels 
likelihood observable image computed dt lo dt dt texton templates parameters texton processes shall discuss section variance noise 
generative part model conditional probability obs jt expf obs reconstructed image hidden layers noise see eq 

generative model simple texture pattern captured spatial arrangements textons models lo lower dimensional spaces semantically meaningful previous gibbs models pixels :10.1.1.18.5785
section discuss model lo texton processes 
descriptive model texton processes texton processes generated hidden layers model characterized descriptive models exponential families 
section rst review background physical ensembles introduce gestalt ensemble texton process 
show experiments realizing texton processes 
background physics foundation visual modeling main di erences texton process conventional texture de ned lattice texton process unknown number elements element attributes lj texture image xed number pixels pixel variable intensity 
neighborhood texton change depending relative positions scales orientations pixels xed neighborhoods 
texton process complicated texture image share common property large number elements global patterns arise simple local interactions elements 
suited theory studying patterns statistical physics subject studying macroscopic properties system involving huge number elements 
bath bath 
micro canonical ensemble 
canonical ensemble 
grand canonical ensemble typical ensembles statistical mechanics 
may introduce additional layers hidden variables curve processes render textons 
model stops texton level 
understand intuitive ideas various texture texton models nd revealing discuss physical ensembles shown 

micro canonical ensemble 
insulated system elements 
elements atoms molecules systems solid ferro magnetic material uid gas 
nearly nity say system decided con guration describes coordinates elements momenta 
system subject global constraints 
number elements total system energy total volume xed 
reaches equilibrium insulated system characterized called micro canonical ensemble mcn fs mcn jg microscopic state instance macroscopic summary system 
state assumed uniformly distributed mcn associated uniform probability 
system identi ed 
canonical ensemble 
illustrates small subsystem embedded ensemble 
subsystem elements xed volume energy exchanges energy wall remaining elements called heat bath reservoir 
thermodynamic equilibrium microscopic state small system characterized canonical ensemble gibbs model cn fs expf gg texture modeling micro canonical ensemble mapped julesz ensemble nite image plane collection gabor ltered histograms 
canonical ensemble mapped frame model image nite lattice :10.1.1.18.5785
intuitively small patch viewed window 
intrinsic relationship ensembles gibbs model cn derived conditional distribution mcn duality see refs 

grand canonical ensemble 
illustrates third system subsystem open exchange energy elements bath 
xed may vary 
models liquid gas materials 
equilibrium microscopic state small system governed distribution controlling density elements grand canonical ensemble gd fs grand canonical ensemble mathematical model visual patterns varying numbers elements lays foundation modeling texton processes 
subsection map grand canonical ensemble gestalt ensemble visual modeling 
gestalt ensemble loss generality represent spatial pattern set attributed elements called textons discussed section 
simplify notation consider texton layer lattice ft ng texton neighborhood 

texton neighbors 
measurements texton neighbor dm texton map de ne neighborhood system 
tg set neighboring textons texton nearest neighborhood 
texton covers patch average pair adjacent textons captures image features scale pixels 
di erent ways de ning 
may treat texton point compute voronoi diagram delaunay provides graph structures neighborhood 
example voronoi neighborhood ahuja tuceryan grouping dot patterns 
textons need consider attributes orientation de ning neighborhood 
shows texton plane separated quadrants relative axes rectangle 
quadrant nearest texton considered neighbor texton 
markov random eld image lattice texton neighborhood longer translation invariant 
neighborhood de ned deterministically 
general settings shall represented set hidden variables inferred texton may varying number neighbors referenced indexing address variables 
address variables decided probabilistically depending relative positions orientations scales intensities 
leads called mixed markov random eld scope 
mumford fridman discussed cases context see 
texton neighbor measure features shown capture various gestalt properties 
distance centers measures proximity 

dm gap textons measures connectedness continuation 

angle neighbor relative main axis texton 
useful quadrants iii 
measures curvature possible curves formed textons linearity circularity gestalt language 

relative orientations textons 
useful neighbors quadrants ii iv measures parallelism 

size ratio denotes similarity texton sizes 
width divided width neighbors quadrants iii length divided length neighbors quadrants ii iv 
total pairwise features computed texton plus features texton orientation dimensional feature consisting scale stretch 
notation descriptive models section denote features tj compute dimensional marginal histograms dimensional histogram averaged textons 
denote histograms vector length total number bins histograms 
may choose features high order statistics 
vision literature steven earliest attempt characterize spatial patterns histogram attributes see examples 
distribution characterized statistical ensemble correspondence grand canonical ensemble call gestalt ensemble nite lattice general representation various gestalt patterns gestalt ensemble gst ft gestalt ensemble governed gibbs distribution expf partition function parameter controlling texton density 
rewrite vector valued potential functions energy functions expf jt model provides rigorous way integrating multiple feature statistics probability model generalizes existing point processes 
probability derived appendix julesz ensemble ensemble 
rst de ne close system elements lattice assume density textons xed lim obtain julesz ensemble julesz ensemble ft macroscopic summary system state nite image texton process conditional density 
correspondence parameters see appendix details 
learn parameters select ective features descriptive method minimax entropy learning paradigm :10.1.1.18.5785
subsection discuss computational issues experiments learning simulating gestalt ensembles 
experiment learning sampling gestalt ensembles suppose set texton maps tm lattice assumed independent realizations texton processes 
section assume texton maps known manually drawn human observer 
section texton maps estimated bayesian inference step learning descriptive models texton maps shall integrated estimation hidden texton maps 
long observation large large estimate texton model standard lattice maximum likelihood estimator mle arg maxl log tm steepest ascent time steps nm tm due concavity log likelihood respect solution unique mild regularity conditions 
expectation estimated monte carlo simulations case texture learning :10.1.1.18.5785
observed 
observed image textons illustrated rectangular windows 
typical texton maps sampled gibbs model various stages learning procedure 
di erent methods simulating gestalt ensemble due fundamental link micro canonical julesz grand canonical gestalt ensembles 
rst method simulate julesz ensemble xed number textons large lattice 
markov chain monte carlo mcmc algorithm sampling julesz ensemble texture images zhu 
di erent patches large synthesized texton map samples 
second method samples directly markov chain death birth dynamics adjust number textons 
choose second method learn parameter simultaneously draw samples model 
brie stated markov chain process includes types dynamics 
death birth process simulated reversible jump deletes adds texton 

di usion process updates position orientation scale stretch observed simulation regular grid pattern various stages learning procedure 
textons gibbs sampler 
show typical examples learning sampling figures 
rst example cheetah skin pattern textons see rectangles blobs 
observed image textons illustrated rectangular windows 
typical texton maps sampled gibbs model various stages learning procedure 
step synthesized texton map statistics close observed error histograms 
spatial arrangements cheetah blobs random pattern easiest example 
shows regular point pattern 
harder simulate pattern extremely cold 
special annealing strategy employed sample pattern 
picture show neighbors texton 
strictly speaking wood pattern crack pattern point processes 
textons form lines curves trees random graphs cracks 
desirable introduce layer representation 
experiment intend demonstrate global curve graph patterns ectively 
observed 


markov chain monte carlo simulation woods pattern various stages learning procedure 

observed 
markov chain monte carlo simulation crack pattern various stages learning procedure 
characterized texton processes gestalt models 
simulated patterns woods cracks figures expose drawbacks current texton models 
rectangular window representation rigid leaves small gaps windows supposed aligned seamlessly 
solve problem introduce sophisticated texton representation linear superposition wavelet bases 
second vertices junctions crack pattern missing assume textons play role 
solve problem label textons edge textons vertex textons de ne neighborhood type textons respectively 
shall address problems research 
integrated model discussing descriptive models hidden texton layers return integrated framework section 
generative model observed image obs rewritten equation obs obs jt lo dt dt follow ml estimate equation arg max log obs parameters include texton templates lagrange multipliers lo gestalt ensembles variance gaussian noise maximize log likelihood take derivative respect set zero 
log obs log obs tji obs dt log obs jt log tji obs dt tji obs log obs jt log literature known methods solving equation 
em algorithm data augmentation bayesian context 
propose stochastic gradient algorithm ective problem 
stochastic gradient algorithm step 
initialize hidden texton maps templates simpli ed likelihood discussed section 
set 
repeat steps ii iteratively em algorithm 
step current obtain sample texton maps posterior probability syn tji obs obs jt bayesian inference 
sampling process realized monte carlo markov chain simulates random walk types dynamics 

di usion dynamics realized gibbs sampler sampling relaxing transform group texton 
example move textons update scales rotate 
jump dynamics adding removing texton death birth reversible jumps 
step ii 
treat syn observations estimate integration eq 

learn texton templates gibbs models respectively gradient ascent ii 
update texton templates maximizing log obs jt syn model tting process 
experiment texton templates represented windows unknowns 
size windows sampling process identical simulation gestalt ensemble section likelihood obs jt engaged posterior tji obs 
point window transparent shape texton change learning process 
adequate experiments textures larger local structures need increase window size 
transparency template learned 
pixel foreground template boolean variable indicates pixel transparent 
originally pixels foreground template transparency indicator equal 
set transparency equal pixel composing foreground 
gibbs sampler decide transparency indicators 
ii 
update lo maximizing log syn lo 
exactly maximum entropy learning process descriptive method see eq 
texton processes step ii 
update noise process 
step choose sample example time 
reasons choice 
images usually quite large stationary spatial averaging image large sample ect 
iterative algorithm cumulative 
learning rate steps ii ii slow long run behavior exhibits large sample ect 
proved statistics algorithm converges optimal step size step ii satis es mild conditions 
useful observations 

descriptive model part integrated learning framework terms representation computing step ii 

bayesian vision inference sub task step integrated learning process 
vision system machine biological evolves learning generative models inference world current imperfect knowledge bayesian view vision 
missing learning paradigm discovery process introduces new hidden variables 
separate learning templates learning computational eciency 
iterate steps ii xing lo control density textons 
learn sampled texton maps keeping learned xed 
ective inference simpli ed likelihood section address computational issues integrated model propose method initializing stochastic gradient algorithm step 
initialization likelihood simpli cation clustering stochastic algorithm section needs long burn period starts arbitrary condition 
accelerate computation simpli ed likelihood step stochastic gradient algorithm 
input image obs objective compute initial texton templates hidden texton maps iterative process steps ii 
close examination reveals computational complexity largely due complex coupling textons generative model descriptive models 
simplify models decoupling textons 
firstly decouple textons 
total number textons excessive number need simulate death birth process 
set lo uniform distribution texton elements decoupled spatial interactions 
secondly decouple textons obs jt 
image generating model eq 
implicitly imposes couplings texton elements eq 
adopt constraint model obs jt expf jji obs lj lj jj obs lj image patch domain lj observed image 
pixels obs covered textons uniform distribution assumed introduce penalty 
run stochastic gradient algorithm decoupled log likelihood reduces conventional clustering problem 
start random texton maps algorithm iterates steps 

algorithm runs gibbs sampler change texton lj respectively moving rotating scaling stretching rectangle changing cluster texton falls simpli ed model eq 

texton windows intend cover entire observed image time try form tight clusters ii 
algorithm updates texton averaging lj obs lj lj inverse transformation 
layer order needed simpli ed model 
initialization algorithm computing resembles transformed component analysis 
inspired clustering algorithm leung malik engage hidden variables compute variety textons di erent scales orientations 
see miller 
experimented representing texton template set gabor bases window 
results encouraging generative model 
experiment ii texton clustering subsection experiment initialization clustering method outlined section 
shows experiment initialization algorithm crack pattern 
textons template size 
number textons twice necessary cover image 
optimizing likelihood eq 
annealing scheme utilized temperature decreasing 
sampling process converges result shown 
input image texton maps respectively 
cluster centers shown rectangles respectively 
reconstructed image 
results demonstrate clustering method provides rough reasonable starting solution generative modeling 
result initial clustering algorithm provides rough reasonable starting solution generative modeling 
initial clustering algorithm simpli es models decoupling textons accelerate computation 
experiment iii integrated learning synthesis section show experimental results obtained integrated model 
input image rst clustering step section showed 
run stochastic gradient algorithm full models re ne clustering results 
shows result crack image obtained stochastic gradient algorithm took iterations steps step step ii initial solution step shown 
background foreground texton maps respectively 
learned textons respectively 
reconstructed image learned texton maps templates 
compared results results precise texton maps accurate texton templates due accurate generative model 
foreground texton bar pixel corner left top transparent 
integrated learning results cheetah skin image shown 
generative model learning result crack image 
input image background foreground textons discovered generative model templates generative model reconstructed image generative model 
due accurate generative model results learning precise texton maps accurate texton templates compared initial results 
seen foreground template surround pixels learned transparent blob exactly computed texton 
results brick image 
point template transparent gap lines bricks 
shows learning short crack patterns 
displays pine corn pattern 
seeds black intervals separated cleanly reconstructed image keeps pine structures 
pine corn seeds learnt background textons gaps pine treated foreground textons 
experiment bark image 
result shows details bark modeled 
patterns linear superposition templates better job 
shall investigate issue 
extend model layers experiment pattern text white background type letters foreground 
shows learning process 
templates white background letter letter inferred gradually 
generative model learning result cheetah skin image 
see explanations 
parameters generative model estimated new random samples drawn generative model 
proceeds steps texton maps generative model learning result brick image 
see explanations 
generative model learning result crack image 
see explanations 
generative model learning result pine corn image 
see explanations 
sampled gibbs models respectively 
second background foreground images synthesized texton maps texton templates 
third nal image generated combining images occlusion model 
show synthesis experiments patterns 

synthesis examples layered model cheetah skin pattern 
templates learned results 

shows texture synthesis crack pattern computed 

displays texture synthesis brick pattern 
capture vertical horizontal distances brick add neighbors addition nearest neighbors feature space 
new neighbors nearest neighbors orientation concerned texton 
junctions captured feature statistics 
note texture synthesis experiments markov chain operates meaningful textons pixels 
generative model learning result bark image 
details bark modeled current generative model 
text le foreground letters test model layers textons 
discussion class statistical models visual patterns 
models integrate extend descriptive generative methods provide mathematical de nition generative model learning result text image 
main steps shown illustrate improving textons templates learning 
textons perceptual organizations 
hierarchical model considered generalization hidden markov model hidden markov structure non causal model 
model advantages previous pure descriptive method markov random elds pixel intensities 
representational perspective neighborhood texton map smaller pixel neighborhood frame model :10.1.1.18.5785
generative method captures semantically meaningful elements texton maps 
second computational perspective markov chain operating texton maps move textons ane transforms add delete texton death birth dynamics ective markov chain traditional markov random elds ips pixel intensity time 
show integration descriptive generative methods natural path example randomly synthesized cheetah skin image 
background foreground texton maps respectively sampled lo synthesized background foreground images texton map templates nal random synthesized image generative model 
second example randomly synthesized cheetah skin image 
visual learning 
argue vision system evolve progressively replacing descriptive models generative models realizes transition empirical statistical models physical semantical models 
example randomly synthesized crack image 
see notation explanations 
example randomly synthesized brick image 
see notation explanations 
important issues addressed research 
gestalt model nearest neighbors simple spatial pat terns 
need introduce descriptive feature statistics descriptive modeling replace concepts curves graphs hierarchy generative model 
need explore ecient inference synthesis algorithms gestalt model 
second model local textons image windows quite limited 
explore combination linear bases local shape shading models 
explore motion elements 
done order nd local descriptors term generative models 
third texture patterns foliage intrinsically complex huge number leaves may exist low dimensional sparse representation terms textons 
patterns may modeled descriptive frame model :10.1.1.18.5785
hand patterns may contain clear textons stochastic background case noise generative part model replaced frame model :10.1.1.18.5785
acknowledgments authors reviewers helpful comments 
supported partially nsf iis iis dms onr 
ahuja tuceryan extraction early perceptual struct 
dot patterns cvgip 
bell sejnowski independent components natural images edge lters vision research 
bergen adelson early vision texture perception nature 
chandler modern statistical mechanics oxford university press 
chen donoho saunders atomic decomposition basis pursuit siam journal scienti computing 
cover thomas elements information theory 
de bonet viola non parametric multi scale statistical model natural images advances neural information processing 
dempster laird rubin maximum likelihood incomplete data em algorithm journal royal statistical society series 
duda hart stork pattern classi cation scene analysis second edition wiley sons 
efros freeman image quilting texture synthesis transfer siggraph 
frey jojic transformed component analysis joint estimation spatial transforms image components iccv 
fridman mixed markov models doctoral dissertation division applied math brown university 

geman geman 
stochastic relaxation gibbs distributions bayesian restoration images 
ieee trans 
pami 
pp 

gilks roberts strategies improving mcmc chapter gilks eds markov chain monte carlo practice chapman hall 
green reversible jump markov chain monte carlo computation bayesian model determination biometrika vol 

gu stochastic approximation algorithm mcmc method incomplete data estimation problems preprint dept math 
stat mcgill univ 
heeger bergen pyramid texture analysis synthesis 

jaynes information theory statistical mechanics physical review 
julesz textons elements texture perception interactions nature 
ko ka principles gestalt psychology 
lee mumford huang occlusion models natural images statistical study scale invariant dead leaves model int computer vision vol 
pp 
leung malik detecting localizing grouping repeated scene elements image proc 
th eccv cambridge uk 
leung malik recognizing surface dimensional textons proc 
th iccv corfu greece 
lewicki olshausen probabilistic framework adaptation comparison image codes 
malik perona preattentive texture discrimination early vision mechanisms optical society america vol 
may 
malik belongie shi leung textons contours regions cue integration image segmentation iccv 
mallat zhang matching pursuit time frequency dictionary ieee trans 
signal processing vol pp 
marr vision freeman 
miller learning example machine vision sharing probability densities 
ph thesis massachusetts institute technology 
olshausen field emergence simple cell receptive eld properties learning sparse code natural images nature 
roweis ghahramani unifying review linear gaussian models neural computation vol 

portilla simoncelli parametric texture model joint statistics complex wavelet coecients ijcv 
steven computation locally parallel structure biol 
cybernetics pp 
kendall stochastic geometry applications 
tanner tools statistical inference springer 
tu zhu image segmentation data driven markov chain monte carlo ieee trans 
pami vol 
may 
wu zhu liu equivalence julesz gibbs ensembles iccv 
wu zhu guo statistical modeling texture sketch eccv 
xu guo shum chaos mosaic fast memory ecient texture synthesis msr tr april 
zhu guo wu wang textons 
eccv zhu wu mumford :10.1.1.18.5785
minimax entropy principle application texture modeling 
neural computation vol 
nov 
zhu embedding gestalt laws markov random fields ieee trans 
pami 
vol 

zhu liu wu exploring julesz texture ensemble ective markov chain monte carlo ieee trans 
pami vol 
june 
zhu cheng en guo conceptualization modeling visual patterns proc 
rd int workshop perceptual organization computer vision vancouver canada july 
appendix deriving gibbs model texton process 
texton pattern large lattice summarized julesz ensemble ensemble ft hg large lattice rigorously texton map de ned lattice number textons collection histograms gestalt features 
parameters de nes julesz ensemble 
suppose look large texton maps julesz ensemble small window interested frequency distribution small texton maps see window 
frequency distribution called gestalt ensemble grand canonical ensemble 
probabilistic language random texton map sampled uniform distribution julesz ensemble part large small lattice interested probability distribution speci rest lattice 
clearly number large texton maps number textons maps 
frequency taylor expansion log gives log log log constant identi ed derivatives log volumes julesz ensemble respect gibbs form derived 

