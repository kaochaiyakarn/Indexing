hierarchical directed acyclic graph kernel methods structured natural language data jun suzuki sasaki maeda ntt communication science laboratories ntt seika cho soraku gun kyoto japan jun maeda sasaki cslab ntt jp proposes hierarchical directed acyclic graph hdag kernel structured natural language data 
hdag kernel directly accepts levels chunks relations efficiently computes weighed sum number common attribute sequences 
applied proposed method question classification sentence alignment tasks evaluate performance similarity measure kernel function 
results experiments demonstrate hdag kernel superior kernel functions baseline methods 
easy get structured corpora annotated texts researchers applied statistical machine learning techniques nlp tasks accuracies basic nlp tools pos taggers np named entities taggers dependency analyzers improved point realize practical applications nlp 
motivation identify richer information texts improve performance nlp applications contrast feature vectors constructed bagof words salton 
focusing methods numerical feature vectors represent features proceedings st annual meeting association computational linguistics july pp 

natural language data 
case original natural language data symbolic researchers convert symbolic data numeric data 
process feature extraction ad hoc nature differs nlp task neat formulation generating feature vectors semantic grammatical structures inside texts 
kernel methods vapnik cristianini shawe taylor suitable nlp devised 
convolution kernels haussler demonstrate build kernels discrete structures strings trees graphs 
remarkable properties kernel methodology retains original representation objects algorithms manipulate objects simply computing kernel functions inner products pairs objects 
means map texts feature vectors explicitly representing long efficient calculation inner products pair texts defined 
kernel method widely adopted machine learning methods support vector machine svm vapnik 
addition kernel function described similarity function satisfies certain properties cristianini shawe taylor 
similarity measure texts important factors tasks application areas nlp machine translation text categorization information retrieval question answering 
proposes hierarchical directed acyclic graph hdag kernel 
handle structures texts cal similarity regard structures practical cost time 
hdag kernel widely applied learning clustering similarity measures nlp tasks 
sections define hdag kernel introduce algorithm implements 
results applying hdag kernel tasks question classification sentence alignment discussed 
convolution kernels convolution kernels proposed concept kernels discrete structure 
framework defines kernel function input objects applying convolution sub kernels kernels decompositions parts objects 
positive integer nonempty separable metric spaces 
focuses special case countable sets 
start composite structure parts defined relation set true parts defined suppose parts similarity defined general parts ized convolution note convolution kernels concepts instances determined definition sub kernel tree kernel collins duffy string subsequence kernel ssk lodhi developed nlp field examples convolution kernels instances 
explicit definition tree kernel ssk written conceptually enumerate sub structures occurring represents total number possible sub structures objects 
feature mapping sample space feature space case tree kernel trees 
tree kernel computes number common subtrees trees defined number occurrences th enumerated subtree tree case ssk input objects string sequences kernel function computes sum occurrences th common subse quence weighted length subsequence 
kernels polynomialtime calculations efficient recursive calculation possible see equation 
proposed method uses framework convolution kernels 
hdag kernel definition hdag defines hdag directed acyclic graph dag hierarchical structures 
certain nodes contain dags 
basic nlp tasks chunking parsing analyze text semantically grammatically 
levels chunks phrases named entities sentences bound relation structures dependency structure anaphora coreference 
hdag designed enable representation structures inside texts hierarchical structures chunks dag structures relations chunks 
believe richer representation extremely useful improve performance similarity measure texts learning clustering tasks application areas nlp 
shows example text structures handled hdag 
contains simple examples hdag elucidate calculation similarity 
shown figures nodes allowed zero attributes nodes texts usually kinds attributes 
example attributes include words partof speech tags semantic information word tsujii chair acl 
famous researchers nlp field 
jun ichi tsujii general chair acl nnp nnp vbz dt jj nn nnp np person np org word named entity np chunk coreference sentence famous 
prp vbz cd dt rbs jj dependency structure np np node direct link attribute words part speech tags np chunk class ne example text structures handled hdag np np examples hdag structure net class named entity 
definition hdag kernel define set nodes respectively represent nodes graph defined respectively 
expression represent path define attribute sequence sequence attributes extracted nodes included subpath 
attribute sequence expressed represents chunk 
ba sic example extraction attribute sequences sub path contains attribute sequences combinations attributes section explains detail method extracting attribute sequences sub paths 
define terminated nodes contain graph nodes treat exact matching sub structures approximate matching allow node skips decay factor extracting attribute sequences sub paths 
framework similarity evaluation robust similar sub structures evaluated value similarity contrast exact matching evaluate similar substructure 
define parameter number attributes combined attribute sequence 
calculating similarity consider combination lengths discussion feature vector hdag written represents explicit feature mapping hdag represents number possible attribute combinations 
value number occurrences th attribute sequence hdag attribute sequence weighted node skip 
similarity definition hdag kernel follows equation input objects proach hdag kernel calculates inner product common attribute sequences weighted node skips occurrence note general dimension feature space high approaches infinity computationally infeasible gen respectively 
ap erate feature vector reader understanding hdag kernel calculates introduce efficient calculation method section details attribute sequences elements feature vector calculation explicit 
explicitly 
improve attribute sequences elements feature vector describe details attribute sequences elements feature vector hdag kernel 
framework node skip denote explicit representation node skip 
attribute sequences sub path node skip written 
costs skip terminated node 
cost skipping table attribute sequences values nodes sub path seq 
val 
np sub path seq 
val 
np non terminated node skipping graphs inside non terminated node 
introduce decay functions decay factor represents cost node skip example represents cost node skip cost just node represents sum multiplied skip cost node skips nodes path sum cost path represents sum multiplied cost node skips nodes path 
represents cost node skip path attribute sequences non terminated nodes define attributes non terminated node combinations attribute sequences including node skip 
table shows attribute sequences values details elements feature vector elements feature vector considered node skips 
means element element 
considering hierarchical structure natural assume different elements 
framework node skip attributes node treated element 
framework table similarity values att 
seq 
value att 
seq 
value np np achieves approximate matching structure automatically hdag kernel judges pairs attributes attribute sequence inside outside chunk 
pairs attributes attribute sequences condition inside outside chunk attribute sequences judge element 
table shows similarity values feature vectors ex represented 
show common elements feature vector appear number elements appear large 
note shown table attribute sequences non terminated node addressed features graph 
due hierarchical structure attribute sequences non terminated node come combination attributes terminated nodes 
case attribute sequence comes treat evaluate attribute sequence twice 
similarity value ta ble contain see table 
calculation determine returns sum common attribute sequences combination attributes nodes returns number common attributes nodes including attributes inside nodes function define returning set nodes inside non terminated node means node terminated node 
example define functions calculate boundary conditions 
function returns set nodes direct links node means nodes direct links define representing sum common attribute sequences combinations attributes extracted subpaths sinks respectively 
functions needed recursive calculation written form respectively boundary con dition written efficient similarity calculation formula written equation recursive definition similarity calculated time efficient calculation method elucidate efficient processing algorithm 
pre process nodes sorted condition nodes path focused node graph inside focused node set focused node 
get set ordered nodes treating hdag 
case get rewrite recursive calculation formula loops follow sorted order 
shows algorithm hdag kernel 
dynamic programming technique compute hdag kernel efficiently sorted order values needed calculate focused pair nodes calculated previous calculation 
calculate table order nodes left right top bottom 
normalize computed kernels algorithms 
normalization corresponds standard unit norm normalization easily rewrite equation calculate combinations attributes order calculation time algorithm hdag kernel combination foreach foreach foreach foreach foreach foreach return algorithm hdag kernel examples feature space corresponding kernel space lodhi 
experiments evaluated performance proposed method actual application nlp data set written japanese 
compared hdag dag hierarchy structure string subsequence kernel ssk word sequence dependency structure question george bush purchased small interest baseball team 
hierarchical dependency structure george bush purchased small interest baseball team nnp nnp vbd dt jj nn wdt nn nn person np dependency structure np pp np george bush purchased small interest baseball team person vbd dt jj nn wdt nn nn 
word order george bush purchased small interest baseball team person vbd dt jj nn wdt nn nn examples input object structure hdag dag dsk ssk kernel dsk collins duffy special case tree kernel cosine measure feature vectors consisting occurrence attributes boa boa attributes noun unknown word boa 
expanded ssk dsk improve total performance experiments 
denote ssk dsk respectively 
original ssk treats exact string combinations parameter consider string combinations ssk 
original dsk specifically constructed parse tree 
expanded able treat combinations nodes free order child node matching 
shows input objects evaluated kernel hdag dag dsk ssk 
note dag dsk treat input objects kernel calculation methods differ return values 
words semantic information similar wordnet english attributes node 
chunks relations texts analyzed kudo matsumoto named entities analyzed method :10.1.1.19.388
tested combination case changing parameter step 
best performance achieved parameter shown case 
table results performance similarity measure question classification hdag dag dsk ssk boa boa performance similarity measure question classification questions questions crl qa data assigned question types crl qa data 
evaluated classification performance step 
extracted question data 
second calculated similarity extracted question questions 
third ranked questions order descending similarity 
evaluated performance similarity measure mean reciprocal rank mrr voorhees tice question type ranked questions 
table shows results experiment 
sentence alignment data set taken formed sentences manually aligned sentences meaning sentence say thing 
experiment follows 
extracted sentence data set 
second calculated similarity extracted sentence sentences data set 
third ranked sentences descending order calculated similarity values 
evaluated performance similarity measure mrr measure 
table shows results experiment 
www nlp cs ac jp www cs nyu edu sekine project table results performance similarity measure sentence alignment hdag dag dsk ssk boa boa table results question classification svm comparison kernel functions hdag dag dsk ssk boa poly boa poly performance kernel function question classification comparison methods evaluated performance kernel function machine learning approach question classification 
chose svm kernel learning algorithm produces state art performance nlp tasks 
data set previous experiments difference question type fewer questions moved entries upper question type defined crl qa data provide training samples question type 
vs rest multi class classification method highest scoring question type 
case boa boa polynomial kernel vapnik consider attribute combinations 
table shows average accuracy question evaluated fold cross validation 
discussion experiments designed evaluated similarity measure reflects semantic information texts 
task question classification question classified ques tion type reflects intention question 
sentence alignment task evaluates sentence semantically similar sentence 
hdag kernel showed best performance experiments similarity measure kernel learning algorithm 
proves usefulness hdag kernel determining similarity measure texts providing svm kernel resolving classification problems nlp tasks 
results indicate approach incorporating richer structures texts suited tasks require evaluation semantical similarity texts 
potential hdag kernel wider nlp tasks believe adopted practical nlp applications text categorization question answering 
experiments indicate optimal parameters combination number decay factor depend task hand 
determined experiments 
original dsk requires exact matching tree structure expanded dsk flexible matching 
dsk showed worst performance 
sentence alignment task paraphrasing different expressions meaning common structures parse tree widely differ general 
dsk ssk hdag kernel offer approximate matching produces better performance 
structure hdag approaches dag consider hierarchical structure 
addition structure sequences strings entirely included dag 
framework hdag kernel covers dag kernel ssk 
proposed hdag kernel reflect richer information texts 
proposed method generalized framework handling structure inside text 
evaluated performance hdag kernel similarity measure kernel function 
experiments showed hdag kernel offers better performance ssk dsk baseline method cosine measure feature vectors hdag kernel better utilizes richer structure texts 
collins duffy 

parsing single neuron convolution kernels natural language problems 
technical report ucs crl 
uc santa cruz 
cristianini shawe taylor 

support vector machines kernel learning methods 
cambridge university press 
haussler 

convolution kernels discrete structures 
technical report ucs crl 
uc santa cruz 
maeda matsumoto 

machine learning approach multi document summarization 
journal natural language processing 
japanese 
miyazaki shirai yokoo hayashi editors 

semantic attribute system japanese lexicon volume 
publishing 
japanese 


efficient support vector classifiers named entity recognition 
proc 
th international conference computational linguistics coling pages 
kudo matsumoto 

japanese dependency analysis cascaded chunking 
proc 
th conference natural language learning conll pages 
lodhi saunders shawe taylor cristianini watkins 

text classification string kernel 
journal machine learning research 
salton wong yang 

vector space model automatic indexing 
communication acm 
vapnik 

nature statistical learning theory 
springer 
voorhees tice 

trec question answering track evaluation 
proc 
th text retrieval conference trec 
