ieee computer research feature published ieee computer society outperforming lru adaptive replacement cache algorithm fundamental metaphor modern computing finds wide application storage systems databases web servers middleware processors file systems disk drives redundant array independent disks controllers operating systems applications data compression list updating 
level memory hierarchy cache performs faster auxiliary storage expensive 
cost concerns usually limit cache size fraction auxiliary memory size 
cache auxiliary memory handle uniformly sized items called pages 
requests pages go cache 
page cache hit occurs cache happens request goes auxiliary memory 
case copy paged cache 
practice called demand paging rules prefetching pages auxiliary memory cache 
cache full system page new page page currently cached pages 
replacement policy determines page evicted 
commonly criterion evaluating replacement policy hit ratio frequency finds page cache 
course replacement policy implementation overhead exceed anticipated time savings 
discarding page policy choice cache management 
attempts outperform lru practice succeeded overhead issues need parameters 
adaptive replacement cache self tuning low overhead algorithm responds online changing access patterns 
arc continually balances recency frequency features workload demonstrating adaptation eliminates need workload specific plagued previous proposals improve lru 
arc online adaptation benefits real life workloads due richness variability time 
workloads contain long sequential os moving hot spots changing frequency scale temporal locality fluctuating stable repeating access patterns patterns transient clustered 
lru arc easy implement running time request essentially independent cache size 
real life implementation revealed arc low space overhead percent cache size 
lru arc allows time sequential requests pass polluting cache flushing pages temporal locality 
likewise arc effectively handles long periods low temporal locality 
arc leads sub self tuning low overhead scan resistant adaptive replacement cache algorithm outperforms algorithm dynamically responding changing access patterns continually balancing workload recency frequency features 
nimrod megiddo modha ibm almaden research center performance gains terms improved hit ratio compared lru wide range cache sizes 
arc intuition arc maintains lru page lists maintains pages seen maintains pages seen twice 
algorithm caches fraction pages lists 
pages seen twice short time may thought having high frequency having longer term reuse potential 
say captures recency captures frequency 
cache hold pages strive keep lists roughly size lists comprise cache directory holds pages 
arc caches variable number pages total number cached pages arc continually adapts precise number pages list cached 
contrast adaptive approach nonadaptive approach suppose frc provides policy attempts keep cache pages pages arc behaves frc vary adaptively 
introduce learning rule lets arc adapt quickly effectively variable workload 
algorithms recency frequency predictors likelihood pages reused 
arc acts adaptive filter detect track temporal locality 
recency frequency important time arc detect change adapt investment lists accordingly 
arc works policy policy uses hindsight choose best fixed respect particular workload cache size 
surprisingly arc operates completely online delivers performance comparable state art cache replacement policies hindsight policies choose best fixed values tuning parameters 
arc matches lru ease implementation requiring lru lists 
cache replacement algorithms laszlo belady min optimal offline policy replacing page cache greatest distance occurrence 
lru policy replaces page 
decades policy undergone numerous approximations improvements 
important related algorithms clock ws working set 
request stream drawn lru stack depth distribution lru offers optimal policy 
simple implement lru responds deviations underlying sdd model 
sdd captures recency capture frequency 
independent model captures notion page frequencies 
irm requests received different times stochastically independent 
lfu replaces frequently page optimal irm drawbacks lfu running time request logarithmic cache size oblivious history adapts poorly variable access patterns accumulating stale pages past high frequency counts may longer useful 
lru represents significant practical progress approximating original lfu working adaptively 
lru memorizes times cache page occurrences replaces page second occurrence 
irm lru maximum expected hit ratio online algorithm knows page works traces 
lru suffers practical drawbacks uses priority queue gives logarithmic complexity tune parameter correlated information period 
logarithmic complexity severe practical drawback improved method constant complexity alleviates 
resembles lru uses simple lru list priority queue 
arc low computational overhead resembles 
choice correlated information period crucially affects lru performance 
single priori fixed choice works uniformly various cache sizes workloads 
lru drawback persists 
low inter recency set design builds 
lirs maintains variable size lru stack potentially unbounded size serves cache directory 
stack lirs selects top pages depending parameters crucially affect performance certain choice works stable irm workloads choices sdd workloads 
due certain stack pruning operation lirs april arc acts adaptive filter detect track temporal locality 
computer average case worst case constant time overhead significant practical drawback 
frequency replacement maintains lru list partitions sections new middle old moves pages 
fbr maintains frequency counts individual pages 
idea factoring locality works theory hit page stored new section count increment 
cache system replaces page old section count 
fbr drawbacks include need rescale counts periodically tunable parameters 
frequently lrfu policy subsumes lru lfu 
assigns value page depending parameter cache access updates referenced 
approach resembles exponential smoothing statistical forecasting method 
lrfu replaces page value 
tends tends number occurrences lrfu collapses lfu 
tends emphasizes recency lrfu collapses lru 
performance depends crucially 
adaptive lrfu adjusts dynamically 
lrfu drawbacks 
lrfu require tunable parameter controlling correlated 
second lrfu complexity fluctuates constant logarithmic 
required calculations practical complexity significantly higher lru 
small lrfu times slower lru arc potentially wipe benefit high hit ratio 
replacement policy uses queues ith queue contains pages seen times times 
mq algorithm maintains history buffer 
hit page frequency increments page placed appropriate queue mru position page set currenttime lifetime lifetime tunable parameter 
access memory checks lru page queue currenttime moves page lower queue mru position 
estimate parameter lifetime mq assumes distribution temporal distances consecutive accesses single page certain hill shape 
arc assumption robust wider range workloads 
mq adjust workload evolution detect measurable change peak temporal distance arc track evolving workload adapts continually 
mq constant time overhead needs check lru page time stamps queues request higher overhead lru arc 
contrast lru lirs fbr lrfu algorithms require offline selection tunable parameters arc replacement policy functions online completely self tuning 
arc maintains frequency counts lfu fbr suffer periodic rescaling requirements 
lirs arc require potentially unbounded space overhead 
arc lirs fbr constant time implementation complexity lfu lru lrfu logarithmic implementation complexity 
cache history cache size pages 
introduce policy dbl memorizes pages manages imaginary cache size introduce class ii cache replacement policies 
dbl maintains lru lists contains pages seen contains pages seen twice 
precisely page resides requested exactly time removed requested removed similarly page resides requested time removed requested removed policy functions follows contains exactly pages replace lru page replace lru page initially lists empty 
requested page resides policy moves mru position moves mru position case policy removes lru member policy removes lru member constraints list sizes maintained 
propose class ii policies track items cache size managed dbl kept cache 
partitioned arc maintains frequency counts suffer periodic rescaling requirements 
contains top pages contains bottom pages 
similarly partitioned top bottom subject conditions 
empty lru page mru page contains exactly pages cached policy class 
pages reside cache directory cache history pages reside cache directory cache 
cache directory pages contain exactly pages 
arc leverage extra history information effect continual adaptation 
shown policy lru class ii 
conversely pages need dbl 
justifies choice maintain history pages 
adaptive replacement cache fixed replacement cache frc tunable parameter class ii attempts keep cache pages pages denote requested page 
replace lru page replace lru page roughly speaking current target size list arc behaves changes adaptively 
describes complete arc policy 
intuitively hit suggests increase size hit suggests increase size continual updates effect increases 
amount change important 
learning rates depend relative sizes arc attempts keep roughly size roughly size 
hit increments max exceed similarly hit decrements max drops zero 
taken numerous small increments decrements profound effect 
arc stops adapting responds workload changes irm sdd vice versa 
contains lru pages lru experience cache hits arc arc april arc 
requested page 
case hit arc move top case ii 
arc hit dbl adapt min max rep 
move top place cache 
case iii 
arc hit dbl adapt max max rep 
move top place cache 
case iv 
dbl case delete lru page 
delete lru page 
case ii delete lru page rep 
put op place cache 
move lru page top remove cache 
move lru page top remove cache 

arc policy 
adaptive replacement cache algorithm maintains lru pages lists maintains pages seen maintains pages seen twice 
arc time overhead request remains independent cache size space overhead marginally exceeds lru 
computer experience cache hits lru 
page system places top way lru position requested prior evicted enters long sequence read requests passes flushing possibly important pages sense arc scan resistant 
arguably scan begins fewer hits occur compared effect learn table 
comparison arc algorithms online transaction processing workload 
cache online hit ratios offline hit ratios byte pages arc lru lfu fbr lirs mq lru lrfu min table 
comparison arc algorithms trace 
cache online hit ratios offline hit ratios byte pages arc lru mq lru lrfu lirs table 
comparison arc algorithms trace 
cache online hit ratios offline hit ratios byte pages arc lru mq lru lrfu lirs ing law list grow expense list arc resistance scans 
experimental results compared performance various algorithms various traces 
oltp contains hour worth database 
collected months windows nt workstations obtained concat concatenating traces merged time stamps request obtain merge 
took ds trace commercial database server 
traces page size bytes 
captured trace storage performance council spc synthetic benchmark contains long sequential scans addition random accesses page size kbytes 
considered traces perform disk read accesses initiated large commercial search engine response various web search requests hours 
traces page size kbytes 
obtained trace merge merging traces time stamps request 
hit ratios cold starts reported percentages 
table compares arc hit ratios hit ratios algorithms oltp trace 
set tunable parameters fbr lirs original descriptions 
selected tunable parameters lru lrfu offline best result cache size 
arc requires user specified parameters 
tuned mq online 
lfu fbr lru lrfu min parameters exactly match lrfu policy 
arc outperforms lru lfu fbr lirs mq 
performs lru lrfu min respective offline values 
similar results db sprite file system traces 
tables compare arc lru mq lru lrfu lirs traces tunable parameters mq set online tunable parameters algorithms chosen offline optimized cache size workload 
arc outperforms lru performs nearly competitively lru lrfu lirs mq 
general similar results hold traces examined 
table compares arc lru traces practically relevant cache size 
spc trace contains long sequential scans inter april table 
comparison arc lru hit ratios percentages various workloads 
workload cache pages cache mbytes lru arc frc offline concat merge ds spc merge computer random requests 
due scan resistance arc outperforms lru quite dramatically 
arc working online performs closely better frc best offline fixed choice parameter traces 
adaptation parameter approaches zero arc emphasizes contents parameter approaches cache size arc emphasizes contents 
parameter fluctuates reaches extremes 
arc fluctuate frequency recency back single workload 
compares hit ratios arc lru traces spc merge 
arc substantially outperforms lru virtually traces cache sizes 
ur results show self tuning low overhead scan resistant arc cache replacement policy outperforms lru 
adaptation cache replacement policy produce considerable performance improvements modern caches 

mattson evaluation techniques storage hierarchies ibm systems vol 
pp 


sleator tarjan amortized efficiency list update paging rules comm 
acm vol 
pp 


belady study replacement algorithms virtual storage computers ibm systems vol 
pp 


paging experiment multics system honor morse mit press pp 


denning working sets past ieee trans 
software eng vol 
pp 


carr hennessy simple effective algorithm virtual memory management proc 
th symp 
operating system principles acm press pp 


coffman denning operating systems theory prentice hall 

aho denning ullman principles optimal page replacement acm vol 
pp 


neil neil weikum opti cache size number byte pages hit ratio percent spc arc lru cache size number byte pages arc lru cache size number byte pages merge arc lru hit ratio percent hit ratio percent 
arc lru hit ratios percentages versus cache size pages log log scale traces spc merge 
proof lru page replacement algorithm acm vol 
pp 


johnson shasha low overhead high performance buffer management replacement algorithm proc 
vldb conf morgan kaufmann pp 


jiang zhang lirs efficient low recency set replacement policy improve buffer cache performance proc 
acm sigmetrics conf acm press pp 


robinson data cache management frequency replacement proc 
acm sigmetrics conf acm press pp 


lee lrfu spectrum policies subsumes frequently policies ieee trans 
computers vol 
pp 


zhou philbin multi queue replacement algorithm second level buffer caches proc 
usenix ann 
tech 
conf 
usenix usenix pp 


hsu smith young automatic improvement locality storage systems tech 
report computer science division univ california berkeley 

megiddo modha arc self tuning low overhead replacement cache proc 
usenix conf 
file storage technologies fast usenix pp 

nimrod megiddo research staff member ibm almaden research center san jose calif research interests include optimization algorithm design analysis game theory machine learning 
megiddo received phd mathematics hebrew university jerusalem 
contact megiddo almaden ibm com 
modha research staff member ibm almaden research center san jose calif research interests include machine learning information theory algorithms 
modha received phd electrical computer engineering university california san diego 
senior member ieee 
contact almaden ibm com 
april visit computer magazine online current articles links online resources collection classics changed computing field 
www computer org computer 
