sparse principal component analysis hui zou trevor hastie robert tibshirani april principal component analysis pca widely data processing dimensionality reduction 
pca suffers fact principal component linear combi nation original variables difficult interpret results 
introduce new method called sparse principal component analysis spca lasso elastic net produce modified principal components sparse loadings 
show pca formulated regression type optimization problem sparse loadings obtained im posing lasso elastic net constraint regression coefficients 
efficient algorithms proposed realize spca regular multivariate data gene expression arrays 
give new formula compute total variance modified principal components 
illustrations spca applied real simulated data results encouraging 
keywords multivariate analysis gene expression arrays elastic net lasso singular value de composition thresholding hui zou ph student department statistics stanford university stanford ca 
email stat stanford edu 
trevor hastie professor department statistics department health research policy stanford university stanford ca 
email hastie stat stanford edu 
robert tibshirani professor department health research policy department statistics stanford university stanford ca 
email stat stanford edu 
principal component analysis pca popular data processing dimension reduction technique un supervised learning method pca numerous applications handwritten zip code classification hastie human face recognition hancock 
pca gene expression data analysis misra 
hastie 
propose called gene shaving techniques pca cluster high variable coherent genes microarray data 
pca seeks linear combinations original variables derived variables capture maximal variance 
pca done singular value decomposition svd data matrix 
detail data matrix number observations number variables respectively 
loss generality assume column means 
suppose svd means transpose 
principal components pcs unit length columns corresponding loadings principal components 
variance ith pc gene expression data pcs called eigen arrays eigen genes alter 
usually pcs chosen represent data great dimensionality reduction achieved 
success pca due important optimal properties 
principal components sequentially capture maximum variability minimal information loss 
principal components uncorrelated talk principal component referring 
pca obvious drawback pc linear combination variables loadings typically nonzero 
difficult interpret derived pcs 
rotation techniques commonly help practitioners interpret principal components jolliffe 
considered simple principal components restricting loadings take values small set allowable integers 
feel desirable achieve dimensionality reduction reduce size explicitly variables 
ad hoc way artificially set loadings absolute values smaller threshold zero 
informal thresholding approach frequently practice potentially misleading various respects jolliffe 
mccabe alternative pca subset principal variables 
jolliffe introduced get modified principal components possible zero loadings 
recall interpretation issue arising multiple linear regression response predicted linear combination predictors 
interpretable models obtained variable selection 
lasso tibshirani promising variable selection technique simultaneously producing accurate sparse models :10.1.1.35.7574
zou hastie propose elastic net generalization lasso improve lasso 
introduce new approach get modified pcs sparse loadings call sparse principal component analysis spca 
spca built fact pca written regression type optimization problem lasso elastic net directly integrated regression criterion resulting modified pca produces sparse loadings 
section briefly review lasso elastic net 
method details spca section 
discuss direct sparse approximation approach tic net useful exploratory tool 
show finding loadings principal components reformulated estimating coefficients regression type optimization prob lem 
imposing lasso elastic net constraint coefficients derive modified principal components sparse loadings 
efficient algorithm proposed realize spca 
give new formula justifies correlation effects calculate total variance modified principal components 
section consider special case spca algorithm efficiently handle gene expression arrays 
proposed methodology illustrated real data simulation examples section 
discussions section 
ends appendix summarizing technical details 
lasso elastic net consider linear regression model 
suppose data set observations predictors 

yn response xj 

predictors 
location transformation assume xj centered 
lasso penalized squares method imposing constraint norm regression coefficients 
lasso estimates lasso obtained minimizing lasso criterion lasso arg min xj non negative value 
lasso originally solved quadratic programming tibshirani :10.1.1.35.7574
efron 
proved lasso estimates function piece wise linear proposed algorithm called lars efficiently solve lasso solution path order computations single squares fit 
lasso continuously shrinks coefficients zero gaining prediction accuracy bias variance trade 
due nature penalty coefficients shrunk exact zero large 
lasso simultaneously produces accurate sparse model favorable variable selection method 
lasso limitations pointed zou hastie 
relevant number selected variables lasso limited number observations 
example applied microarray data thousands predictors genes samples lasso select genes clearly unsatisfactory 
elastic net zou hastie generalizes lasso overcome drawbacks enjoying similar optimal properties 
non negative elastic net estimates en follows en arg min xj 
elastic net penalty convex combination ridge penalty lasso penalty obviously lasso special case elastic net 
fixed lars en algorithm zou hastie efficiently solves elastic net problem computation cost single squares fit 
choose 
elastic net potentially include variables fitted model limitation lasso removed 
additional benefit offered elastic net grouping effect elastic net tends select group highly correlated variables variable selected 
contrast lasso tends select grouped variables care final model 
zou hastie compare elastic net lasso discuss application elastic net gene selection method microarray analysis 
motivation method details lasso elastic net sparse coefficients direct consequence penalty depending squared error loss function 
jolliffe proposed directly putting constraint pca get sparse loadings 
successively maximizes variance subject extra constraints xt ak ak ak ak tuning parameter sufficiently small yields exact zero loadings scot lack guidance choose appropriate value 
try values high computational cost impractical solution 
high compu tational cost due fact convex optimization problem 
examples jolliffe show obtained loadings sparse requiring high percentage explained variance 
consider different approach modify pca directly lasso 
light success lasso elastic net regression state strategy seek regression optimization framework pca done exactly 
addition regression framework allow direct modification lasso elastic net penalty derived loadings sparse 
direct sparse approximations discuss simple regression approach pca 
observe pc linear combination variables loadings recovered regressing pc variables 
theorem denote yi 
yi th principal component 
suppose ridge ridge estimates ridge vi 
ridge ridge arg min yi 
theme simple theorem show connection pca regression method possible 
regressing pcs variables discussed jolliffe focused approximating pcs subset variables 
extend general ridge regression order handle kinds data especially gene expression data 
obviously full rank matrix theorem require positive 
note ordinary multiple regression unique solution exactly vi 
story happens full rank matrix 
pca gives unique solution situations 
shown theorem discrepancy eliminated positive ridge penalty 
note normalization coefficients independent ridge penalty penalize regression coefficients ensure reconstruction principal components 
keep ridge penalty term 
add penalty consider optimization problem arg min yi 
call vi approximation vi vi ith approximated principal component 
called naive elastic net zou hastie differs elastic net scaling factor 
normalized fitted coefficients scaling factor affect vi 
clearly large gives sparse sparse vi 
fixed efficiently solved lars en algorithm zou hastie 
flexibly choose sparse approximation ith principal component 
sparse principal components spca criterion theorem depends results pca genuine alternative 
stage exploratory analysis perform pca find suitable sparse approximations 
self contained regression type criterion derive pcs 
consider leading principal component 
theorem xi denote ith row vector matrix 
arg min xi xi subject 
theorem extends theorem derive sequence pcs 
theorem suppose considering principal components 
matrices 
xi denote th row vector matrix arg min xi xi vi 
subject ik 
theorem effectively transforms pca problem regression type problem 
critical el ement object function xi xi xi xi restrict xi xi minimizer orthonormal constraint exactly loading vectors ordinary pca 
alternative derivation pca maximizing variance approach hastie 

theorem shows exact pca relaxing restriction adding ridge penalty term 
seen generalizations enable flexibly modify pca 
obtain sparse loadings add lasso penalty criterion consider optimization problem arg min xi xi subject ik 
components different js allowed penalizing loadings different principal components 
positive required order get exact pca sparsity constraint lasso penalty vanishes 
called spca criterion 
numerical solution propose alternatively minimization algorithm minimize spca criterion 
proof theorem see appendix details get xi xi xt xt 
amounts solve independent elastic net problems get 
hand details appendix xi xi tr tr 
fixed maximize tr subject ik solution theorem 
theorem matrices rank consider constrained maxi mization problem arg max tr suppose svd uv subject ik 
steps numerical algorithm derive sparse pcs 
general spca algorithm 
start loadings ordinary principal components 

fixed solve naive elastic net problem 
arg min 

fixed svd update uv 
repeat steps converges 

normalization vj 
remarks 
empirical evidence indicates outputs algorithm vary slowly changes 
data default choice zero 
practically small positive number overcome potential collinearity problems section discusses default choice data thousands variables gene expression arrays 

principle try combinations choice parameters algorithm converges quite fast 
shortcut provided direct sparse approximation 
lars en algorithm efficiently deliver sequence sparse approximations pc corresponding values pick gives compromise variance sparsity 
selection variance higher priority sparsity tend conservative pursuing sparsity 

pca spca depend note xt sample covariance matrix variables xi 
covariance matrix xi known replace population version pca spca 
standardized pca spca uses sample correlation matrix preferred scales variables different 
adjusted total variance ordinary principal components uncorrelated loadings orthogonal 
ik diagonal 
easy check loadings ordinary principal components satisfy conditions 
jolliffe loadings forced orthogonal uncorrelated property sacrificed 
spca explicitly impose uncorrelated components condition 
modified pcs 
usually total variance explained calculated trace 
uncorrelated 
correlated computed total variance optimistic 
propose new formula compute total variance explained takes account correlations 
suppose 
modified pcs method 
denote reminder adjusting effects 
projection matrix 

adjusted variance total explained variance modified pcs uncorrelated new formula agrees trace 
note computations depend order natural order pca ordering issue 
qr decomposition easily compute adjusted variance 
suppose qr orthonormal upper triangular 
straightforward see explained total variance equal computation complexity 
pca computationally efficient data 
separately discuss computational cost general spca algorithm 
traditional multivariate data fit category 
note spca criterion defined depends trick compute matrix requires np operations 
step loop 
computing costs svd order pk 
elastic net solution requires operations 
total computation cost np mo number iterations convergence 
spca algorithm able efficiently handle data huge long small say 

gene expression arrays typical examples category 
trick longer applicable huge matrix case 
consuming step solving elastic net cost order pj positive finite number nonzero coefficients 
generally speaking total cost order mo pj expensive large fortunately shown section exits special spca algorithm efficiently dealing data 
spca gene expression arrays gene expression arrays new type data number variables genes bigger number samples 
general spca algorithm fits situation positive 
computation cost expensive requiring large number nonzero loadings 
desirable simplify general spca algorithm boost computation 
observe theorem valid principle positive 
turns solution emerges 
precisely theorem 
theorem vi loadings derived criterion 
define solution optimization problem vi 
arg min tr subject ik 
statements section criterion solved algorithm special case general spca algorithm 
gene expression arrays spca algorithm replacing step general spca algorithm step fixed 
sign 
operation called soft thresholding 
gives illustration soft thresholding rule operates 
soft thresholding increasingly popular soft thresholding rule sign 
literature 
example nearest shrunken centroids tibshirani adopts soft thresholding rule simultaneously classify samples select important genes microarrays 
examples data data introduced observations measured variables 
classic example showing difficulty interpreting principal components 
tried interpret pcs 
jolliffe find modified pcs 
table presents results pca table presents modified pcs loadings adjusted variance computed 
demonstration considered principal components 
usual data set set 
chosen sparse approximation explained amount variance ordinary pc 
table shows obtained sparse loadings corresponding adjusted variance 
compared modified pcs pcs spca account nearly amount variance vs sparser loading structure 
important variables associated pcs overlap interpretations easier clearer 
interesting note table variance strictly monotonously decrease adjusted variance follows right order 
table shows true 
worthy mention computation spca done seconds implementation expensive jolliffe 
optimizing values difficult computational challenge 
informal thresholding method referred simple thresholding forth various drawbacks may serve benchmark testing sparse pcs methods 
variant simple thresholding soft thresholding 
pca soft thresholding performs similarly simple thresholding 
omitted results soft thresholding 
spca compared simple thresholding 
table presents loadings corresponding explained variance simple thresholding 
fair comparisons numbers nonzero loadings simple thresholding match results spca 
terms variance simple thresholding better worse spca 
variables non zero loadings spca different chosen simple thresholding pcs create similar sparseness pattern simple thresholding especially leading pc 
pc pc pc pc pc data sequences sparse approximations principal components 
plots show percentage explained variance function 
pc table data loadings principal components variable pc pc pc pc pc pc length clear knots variance cumulative variance table data loadings modified pcs variable pc pc pc pc pc pc length clear knots number nonzero loadings variance adjusted variance cumulative adjusted variance table data loadings sparse pcs spca variable pc pc pc pc pc pc length clear knots number nonzero loadings variance adjusted variance cumulative adjusted variance simulation example created hidden factors independent 
observed variables generated follows xi xi xi independent 
table data loadings modified pcs simple thresholding variable pc pc pc pc pc pc length clear knots number nonzero loadings variance adjusted variance cumulative adjusted variance variable pc pc pc pc pc pc length clear knots number nonzero loadings adjusted variance adjusted variance cumulative adjusted variance avoid simulation randomness exact covariance matrix 
perform pca spca simple thresholding 
words compared performances infinity amount data generated model 
variance underlying factors respectively 
numbers variables associated factors 
equally important important 
pcs explain total variance 
facts suggest need consider derived variables right sparse representations 
ideally derived variable recover factor second derived variable recover factor 
fact sequentially maximize variance derived variables orthonormal constraint restricting numbers nonzero loadings derived variable uniformly assigns nonzero loadings second derived variable uniformly assigns nonzero loadings 
spca simple thresholding carried oracle information ideal sparse representations variables 
table summarizes comparison results 
clearly spca correctly identifies sets important variables 
matter fact spca delivers ideal sparse representations principal components 
ically easy show able find sparse solution 
example spca produce ideal sparse pcs may explained fact methods explicitly lasso penalty 
contrast simple thresholding wrongly includes important variables 
explained variance simple thresholding lower spca relative difference small 
due high correlation variables gain loadings higher true important 
table results simulation example loadings variance pca spca simple thresholding pc pc pc pc pc pc pc adjusted variance truth disguised high correlation 
hand simple thresholding correctly discovers second factor low correlation 
ramaswamy data ramaswamy data ramaswamy genes samples 
principal component explains total variance 
typical microarray data appears practically useful 
applied spca find sparse leading pc 
sequence number nonzero loadings varied wide range 
displayed percentage explained variance decreases slow rate sparsity increase 
genes sufficiently construct leading principal component little loss explained variance 
simple thresholding applied data 
number genes simple thresholding explains slightly higher variance spca 
number selected genes spca simple thresholding different genes difference rate quite consistent 
ramaswamy data number nonzero loadings spca simple thresholding sparse leading principal component percentage explained variance versus sparsity 
simple thresholding spca similar performances 
exists consistent difference selected genes ones nonzero loadings 
discussion long standing interest formal approach derive principal components sparse loadings 
practical point view method achieve sparseness goal possess properties 
sparsity constraint method reduce pca 
computationally efficient small big data 
avoid mis identifying important variables 
frequently simple thresholding criterion 
informal ad hoc method properties listed 
explained variance sparsity concerns simple thresholding bad choice extremely convenient 
shown simple thresholding pretty gene expression arrays 
serious problem simple thresholding mis identify real important variables 
simple thresholding regarded benchmark potentially better method 
lasso constraint pca successfully derives sparse loadings 
computationally efficient lacks rule pick parameter 
addition feasible apply gene expression arrays pca quite popular tool 
developed spca spca criterion 
new spca criterion gives exact pca results sparsity lasso penalty term vanishes 
spca allows quite flexible control sparse structure resulting loadings 
unified efficient algorithms proposed realize spca regular multivariate data gene expression arrays 
principled procedure spca enjoys advantages aspects including computational efficiency high explained variance ability identifying important variables 
appendix proofs theorem proof vd ridge xvi vi 
vi theorem proof note xi xi scalars get xi tr xix tr xix tr tr tr 
xi xi fixed quantity minimized substituting gives 

xi xi 
arg max 
subject 
sv 

vt 
theorem proof steps proof theorem derive long ik 
xi xi tr tr xt xt fixed quantity minimized 
equivalently 
arg max tr subject ik 
eigen analysis problem solution sj 
eigenvectors gives sj 
vj theorem proof assumption ik ik 
constraint ik equivalent constraints lagrangian multipliers method define 


setting gives matrix form full rank invertible observe tr tr tr ik 
tr tr tra jj ik 
jj jj djj 
taken diagonal ajj jj av uv theorem proof observe vi hand means arg min xi xi subject ik 
xi xi arg min tr subject ik 
xt xt xt approaches 
follows 
tr 
alter brown botstein 
singular value decomposition genome wide ex pression data processing modeling proceedings national academy sciences 
jolliffe 
loadings correlations interpretation principal com ponents journal applied statistics 
efron hastie johnstone tibshirani 
angle regression annals statistics press 
hancock burton bruce 
face processing human perception principal components analysis memory cognition 
hastie tibshirani eisen brown ross weinstein alizadeh staudt botstein 
gene shaving method identifying distinct sets genes similar expression patterns genome biology 
hastie tibshirani friedman 
elements statistical learning data mining inference prediction springer verlag new york 

case studies application principal component applied statistics 

principal component analysis springer verlag new york 
jolliffe 
rotation principal components choice normalization constraints journal applied statistics 
jolliffe 
modified principal component technique lasso journal computational graphical statistics 
mccabe 
principal variables technometrics 
misra schmitt hwang hsiao 
exploration microarray gene expression patterns reduced dimensional space genome research 
ramaswamy tamayo rifkin angelo ladd reich mesirov poggio gerald lander golub 
multiclass cancer diagnosis tumor gene expression signature proceedings national academy sciences 
tibshirani 
regression shrinkage selection lasso journal royal statistical society series 
tibshirani hastie narasimhan chu 
diagnosis multiple cancer types shrunken centroids gene proceedings national academy sciences 

simple principal components applied statistics 
zou hastie 
regression shrinkage selection elastic net applica tions microarrays technical report statistic department stanford university 
available www stat stanford edu hastie pub htm 

