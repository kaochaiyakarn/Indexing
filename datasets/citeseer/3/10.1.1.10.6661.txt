kolmogorov complexity information theory interpretation terms questions answers peter gr paul vit anyi cwi box nl gb amsterdam netherlands mail cwi nl 
compare elementary theories shannon information kolmogorov complexity extent common purpose fundamentally di erent 
discuss relate basic notions theories shannon entropy kolmogorov complexity shannon mutual information kolmogorov algorithmic mutual information 
explain universal coding may viewed middle ground theories 
consider shannon rate distortion theory quanti es useful certain sense information 
communication information guiding motif explain relates sequential question answer sessions 
keywords kolmogorov complexity algorithmic information theory shannon information theory mutual information pre codes universal codes rate distortion theory data compression 
measure amount information phenomenon particular observation concerning phenomenon shannon information theory usually called just information theory introduced shannon 
kolmogorov complexity theory known algorithmic information theory 
introduced independently di erent motivations solomono born kolmogorov chaitin born respectively 
theories aim providing means measuring information 
unit bit 
cases amount information object may interpreted length description object 
shannon approach method encoding objects presupposition objects encoded outcomes known random source characteristics random source determine encoding characteristics supported part travel awarded netherlands organization scienti research nwo 
supported part eu project ist noe ist esf qit programme 
kluwer academic publishers 
printed netherlands 
gruenwald tex objects outcomes 
kolmogorov complexity approach consider individual objects isolation speak encoding object computer program turing machine generates halts 
shannon approach interested minimum expected number bits transmit message random source known characteristics error free channel 
kolmogorov complexity interested minimum number bits particular message ectively reconstructed 
little re ection reveals great di erence source emitting messages shannon information bit choose messages concerned arbitrarily high kolmogorov complexity 
shannon stresses founding article notion concerned communication kolmogorov stresses founding article notion aims supplementing gap left shannon theory concerning information individual objects 
sure notions natural shannon ignores object considers characteristics random source object possible outcomes kolmogorov considers object determine number bits ultimate compressed version irrespective manner object arose 
read introduce compare contrast shannon kolmogorov approaches 
switching back forth theories pattern rst discuss concept shannon theory discuss properties questions leaves open 
provide kolmogorov analogue concept show answers question left open shannon theory 
guiding motif communication sender receiver appropriate discuss related setting question answer session obtain understanding theories relate crucial read overview sections section discuss preliminaries notation introduce basic notions 
sections written way read separately 
text assume basic familiarity elementary notions probability theory computation 
contain new results 
theorems details context discussion standard text books cover thomas gruenwald tex standard shannon information theory li vit anyi standard kolmogorov complexity 

overview summary summary basic ideas 
notions discussed order 

coding pre codes kraft inequality 
descriptions encodings objects fundamental theories rst review elementary facts coding 
important kraft inequality 
inequality gives fundamental relationship probability mass functions pre codes type codes interested section 

shannon fundamental concept entropy de ned functional maps probability distributions equivalently random variables real numbers 
notion derived rst principles reasonable way measure average amount information conveyed outcome random variable observed 
notion related encoding communicating messages shannon famous coding theorem section 

kolmogorov fundamental concept kolmogorov complexity de ned function maps objects thought natural numbers sequences symbols natural numbers 
intuitively kolmogorov complexity sequence length bits shortest computer program prints sequence halts section 

universal coding interpolating shannon kolmogorov 
primary aim quite di erent functions de ned di erent spaces close relations entropy kolmogorov complexity 
best illustrated explaining universal coding combines elements shannon kolmogorov theory lies basis practical data compression methods section 
entropy kolmogorov complexity basic notions theories 
serve building blocks important notions respective theories 
arguably important notions mutual information gruenwald tex 
mutual information shannon kolmogorov entropy kolmogorov complexity concerned information single object random variable shannon individual sequence kolmogorov 
theories provide distinct notion mutual information measures information object gives object 
shannon theory information random variable carries kolmogorov theory algorithmic mutual information information sequence gives section 
entropy kolmogorov complexity mutual information concerned lossless description compression messages described way description original message completely reconstructed 
extending theories lossy description compression enables formalization sophisticated concepts meaningful information useful information 
meaningful information de ned kolmogorov framework kolmogorov structure function 
useful information de ned shannon framework rate distortion function 
brief treatment 
useful information rate distortion theory part shannon information theory deals situation sender allowed xed small number bits send message 
goal send useful valuable message constraint section 

coding framework kraft inequality notational preliminaries 
strings nite countable set 
notation denote set nite strings sequences example denoting empty word letters 
denotes natural numbers 
identify correspondence length number bits binary string example 
interpreted integer gruenwald tex get blog blog xc dlog xe sequel dxe smallest integer larger equal bxc largest integer smaller equal log denotes logarithm base 
shall typically concerned encoding nite length binary strings nite length binary strings 
emphasis binary strings convenience observations alphabet encoded way theory neutral 

equality constant denote inequality additive constant 
precisely functions mean exists denote situation hold 

probabilities probability distribution de ned nite countable set text denote random variable takes values fxg probability event fxg obtains 
write abbreviation 
codes repeatedly consider scenario sender say wants communicate transmit information receiver say 
information transmitted element set communicated sending binary string called message 
receives message decode hopefully reconstruct element sent 
achieve need agree code description method communicating 
intuitively binary relation source words associated code words 
relation fully characterized decoding function 
decoding function function domain set code words range set source words 
interpreted code word source word 
set code words source word set fy xg 
called encoding substitution necessarily function 
code associate length function ld source word length shortest encoding ld xg denote shortest de ned rst lexicographical order 
gruenwald tex coding theory attention restricted case source word set nite say ng 
constant code words equivalently source words call xed length code 
easy see log instance transmissions source alphabet letters consisting letters latin alphabet plus special characters 
need binary digits source letter 
electronic computers xed length ascii code 
pre code immediately clear general uniquely recover xy 
identity mapping 

introduce pre codes su er defect 
binary string proper pre binary string write xz 
set fx pre free pair distinct elements set proper pre 
function de nes pre code domain pre free 
order decode code sequence pre code simply start decode code word time 
come code word know code word pre code word pre code 
suppose encode binary string times resulting code pre determine code word ends reading left right backing 
note encoded strings pre manner price doubling length 
get ecient code applying construction length de ne interpreted binary string correspondence 
code pre code satisfying log ignore rounding error equation 
standard code encode natural numbers pre free manner call standard pre code natural numbers 
ln notation 
interpreted number correspondence see ln log log log 
pre codes kraft inequality set natural numbers consider straightforward non pre representation gruenwald tex 
elements description length description length 
pre code natural numbers binary pre code words length pre code word xz pre code word 
asymptotically pre code words length source words length quanti cation intuition countable arbitrary pre codes leads precise constraint number code words lengths 
important relation known kraft inequality due kraft 
theorem kraft inequality 
nite nite sequence natural numbers 
pre code sequence lengths binary code words uniquely decodable codes want code elements way uniquely reconstructed encoding 
codes called uniquely decodable 
pre code uniquely decodable code 
example encoded easily decoded left right unique way 
hand uniquely decodable code satis es pre condition 
pre codes distinguished uniquely decodable codes property code word recognizable 
means decoding accomplished delay observing subsequent code words pre codes called instantaneous codes 
reason emphasis pre codes 
turns theorem stays valid replace pre code uniquely decodable code 
important fact means uniquely decodable code replaced pre code changing set code word lengths 
shannon kolmogorov theories interested code word lengths uniquely decodable codes actual encodings 
previous argument may restrict set codes pre codes easier handle 
probability distributions complete pre codes uniquely decodable code complete addition new code word code word set results non uniquely decodable code 
easy see code complete equality holds associated kraft inequality 
gruenwald tex code words complete uniquely decodable code 
de ne de nition completeness 
thought probability mass functions corresponding probability distribution say distribution corresponding 
way complete uniquely decodable code mapped unique probability distribution 
course formal correspondence may choose encode outcomes code corresponding distribution outcomes distributed show theorem distributed code corresponds average sense code achieves optimal compression pre codes protocols asking questions pre codes thought protocols sequentially asking questions 
precise slightly change setting 
think receiver sequentially asks questions assume sender passes information asked question 
case answers truthfully 
questions form realized value element set subset keeps asking questions determined precise value precisely determines sequence sets satisfying conditions 

element just element yz unde ned continuation sets determine protocol follows 
asks 
answer question answer knows question answer rst questions third question answer rst question second question 
keeps asking questions way precisely determined value knows element 
relate sequential protocol pre codes consider code de ned follows set fxg 
way assigned unique code word set code words pre free 
de nes pre code source word reserves exactly gruenwald tex code word 
conversely show pre code reserves code word source word coincides sequential question protocol 
problems pre free encoding value sequentially determining asking value really equivalent 
reason pre codes natural general uniquely decodable codes 

shannon entropy versus kolmogorov complexity 
shannon entropy seldom happens detailed mathematical theory springs forth essentially nal form single publication 
case shannon information theory properly started appearance shannon mathematical theory communication shannon 
shannon proposed measure information distribution called entropy 
entropy distribution measures inherent uncertainty fact equivalently information gained outcome observed 
bit precise imagine observer knows distributed observer observes entropy stands uncertainty observer outcome observes 
think observer receiver receives message conveying value dual point view entropy stands average amount information observer gained receiving realized outcome random variable 
rst give shannon mathematical de nition entropy connect intuitive meaning 
definition 
nite countable set random variable values distribution shannon entropy random variable log entropy de ned functional mapping random variables real numbers 
texts entropy essentially equivalently de ned map distributions random variables real numbers 
de nition log gruenwald tex motivation shannon de nition motivated di erent ways 
important ones axiomatic approach coding interpretation 
concentrate rst brie sketch 
idea axiomatic approach postulate small set eminently reasonable conditions measure information relative distribution satisfy 
shows measure satisfying postulates shannon entropy 
outline approach nite sources ng 
look function maps probability distributions real numbers 
distribution measure information gained average outcome available 
write stands probability suppose require 
continuous 
equal monotonic increasing function equally events choice uncertainty possible events 

choice broken successive choices original weighted sum individual values formalizing condition give speci example 
suppose 
think generated stage process 
outcome generated distribution 
set process stops 
outcome generated probability outcome probability process stops 
nal results probabilities 
particular case require entropy equal entropy rst step generation process plus weighted sum weighted probabilities rst step entropies second step generation process 
special case fold product space space independently distributed px nh 
example total entropy independent tosses coin bias nh 
remarkably shannon proved gruenwald tex theorem 
satisfying assumptions form log constant 
requirements lead de nition entropy unimportant scaling factor 
shall give concrete interpretation factor 
de ning characteristics function properties attractive measure information 
mention 
concave function 
achieves unique maximum uniform distribution 
zero value 
zero gain information told outcome knew take place certainty note exist variations entropy violate requirements example family enyi entropies cover thomas 
alternative notions entropy useful restricted context shannon original de nition remains far important 
coding interpretation immediately stating theorem shannon continues theorem assumptions required proof way necessary theory 
provide certain plausibility de nitions 
real justi cation de nitions reside implications 
spirit shannon henceforth concentrate concrete interpretation entropy terms length number bits needed encode outcomes provides clearer intuitions lies root practical applications information theory importantly simpli es comparison kolmogorov complexity 
example 
start example 
entropy random variable equally outcomes nite sample space log jx choosing particular message remove entropy assignment produce transmit information log jx selection show log jx precise integer dlog jx je interpreted number bits needed transmitted imagined sender imagined receiver 
gruenwald tex connect entropy minimum average code lengths 
de ned follows definition 
source words produced random variable probability event characteristics xed 
consider pre codes code word source word 
denote length code word want minimize expected number bits transmit source choose pre code achieves 
order minimize average code word length ld de ne minimal average code word length minf ld pre 
pre code ld called optimal pre code respect prior probability source words 
minimal average code length optimal code depend details set code words set code word lengths 
just expected code word length respect distribution 
shannon discovered minimal average code word length equal entropy source word set 
known noiseless coding theorem 
adjective noiseless emphasizes ignore possibility errors 
theorem 

log entropy typically interested encoding binary string length entropy proportional example 
essence smallest di erence entropy minimal expected code length completely negligible 
turns optimum relatively easy achieve shannon fano code 
symbols called basic messages source words 
order symbols decreasing probability say ng probabilities binary code obtained coding binary number obtained truncating binary expansion length log log code shannon fano code 
property highly probable symbols mapped short code words symbols low probability mapped longer code words just gruenwald tex optimal setting done morse code 
note code symbol di ers codes symbols bit positions binary expansions di er rst positions 
means inverse decoding mapping better value pre value set code words pre code 
means recover source message code message scanning left right look ahead 
average number bits symbol original message 
combining previous inequality obtain log log log interpretation terms sequential questions re interpret shannon noiseless coding theorem terms protocols sequentially asking questions suppose asks questions type set subset answers truthfully question keeps asking questions determined exact value realized outcome random variable section showed protocol may thought pre code code word source word vice versa 
theorem may interpreted follows 
suppose goal determine exact value questions possible 
asks questions possible way average need ask questions plus minus nd exact value point view shannon fano code described protocol asking questions optimal optimal protocol protocol minimizes expected number questions asked 
problem shannon observes messages meaning semantic aspects communication irrelevant engineering problem 
shannon theory information fully determined probability distribution set possible messages unrelated meaning structure content individual messages 
problematic ways gruenwald tex practical cases distribution generating outcomes may unknown observer worse may exist example answer question information book viewing element set possible books probability distribution 

measure quantity hereditary information biological organisms encoded dna 
possibility seeing particular form animal set possible forms probability distribution 
contradicted fact calculation possible existence time earth give low gure shannon classical information theory assigns quantity information ensemble possible messages 
messages ensemble equally probable quantity number bits needed count possibilities 
expresses fact message ensemble communicated number bits 
say number bits needed convey individual message ensemble constitutes second shannon theory 
illustrate consider ensemble consisting binary strings length 
shannon measure require bits average encode string ensemble 
string consisting encoded bits expressing binary adding repeated pattern requirement agreed algorithm decodes encoded string 
compress string note equals consists 
discovered interesting phenomenon description strings compressed considerably provided exhibit regularity 
regularity lacking cumbersome express large numbers 
instance easier compress number number order magnitude 
interested measure information shannon rely untenable probabilistic assumptions takes account phenomenon regular strings compressible 
aim measure information content adopt bayesian subjective interpretation probability problem remains gr 
gruenwald tex individual nite object information conveyed individual nite object individual nite object 
want information content object attribute depend instance means chosen describe information content 
surprisingly turns possible large extent 
resulting theory information kolmogorov complexity notion independently proposed solomono kolmogorov chaitin li vit anyi describe history subject 

kolmogorov complexity suppose want describe object nite binary string 
care object descriptions description describe object 
descriptions object take length shortest description measure object complexity 
natural call object simple short description call complex descriptions long 
section consider description method transmit messages sender receiver 
known sender receiver message transmitted sender receiver transmitting description cost transmission measured length cost transmission determined length function recall length shortest choose length function descriptional complexity speci cation method obviously descriptional complexity depends crucially general principle involved syntactic framework description language determines succinctness description 
order objectively compare descriptional complexities objects able say complex descriptional complexity depend 
complexity viewed related universal description method priori assumed senders receivers 
complexity optimal description method assigns lower complexity object 
really interested optimality respect description methods 
speci cations useful necessary mapping executed ective manner 
principle performed humans machines 
notion formalized partial recursive functions known simply computable functions 
gruenwald tex generally accepted mathematical viewpoints coincides intuitive notion ective computation 
set partial recursive functions contains optimal function minimizes description length function 
denote function recursive function objects description shorter description 
shorter additive constant independent 
complexity respect complexities respect partial recursive functions 
identify length description respect xed speci cation function algorithmic descriptional complexity optimality sense means complexity object invariant additive constant independent transition optimal speci cation function 
complexity objective attribute described object intrinsic property object depend description formalism 
complexity viewed absolute information content amount information needs transmitted senders receivers communicate message absence priori knowledge restricts domain message 
outlined program general theory algorithmic complexity 
major innovations follows 
restricting formally ective descriptions de nition covers form description intuitively acceptable ective general viewpoints mathematics logic 

restriction ective descriptions entails universal description method description length complexity respect ective description method 
signi cantly implies item 
description length complexity object intrinsic attribute object independent particular description method formalizations thereof 

formal details kolmogorov complexity nite object de ned length shortest ective binary description broadly gruenwald tex speaking may thought length shortest computer program prints halts 
computer program may written java lisp universal language shall see universal languages resulting program lengths di er constant depending precise standard enumeration turing machines enumeration corresponding functions computed respective turing machines 
computes functions partial recursive functions computable functions 
technical reasons interested called pre complexity associated turing machines set programs inputs resulting halting computation pre free realize equipping turing machine way input tape separate tape way output tape 
turing machines called pre machines halting programs form pre free set 
rst de ne pre kolmogorov complexity relative pre machine th pre machine standard enumeration 
de ned length shortest input sequence input sequence exists remains unde ned 
course preliminary de nition highly sensitive particular pre machine 
universal pre machine comes rescue 
just exists universal ordinary turing machines exist universal pre machines 
remarkable property simulate pre machine 
speci cally exists pre machine input pair hi yi outputs halts 
pre machine property call machine 
kolmogorov complexity de ned ku 
formalize de nition 

standard invertible ective encoding pre free subset 
may thought encoding function pre code 
example set hx yi insist pre freeness recursiveness want universal turing machine able read image 
left right determine ends 
definition 
pre machine hi yi 
pre kolmogorov exists version kolmogorov complexity corresponding programs necessarily pre free go 
gruenwald tex complexity min fl min fl hi yi ng alternatively think program prints halts hi yi program input program prints halts 
de nition lexicographically rst shortest self delimiting pre program respect pre machine 
consider mapping de ned may viewed encoding function pre decoding function de nition parsimonious code 
reason working pre standard turing machines subsequent developments need pre de ned terms particular machine model kolmogorov complexity machine independent additive constant acquires asymptotically universal absolute character church thesis ability universal machines simulate execute ective process 
kolmogorov complexity object viewed absolute objective quanti cation amount information 
example 
develop intuitions useful think shortest program standard programming language lisp java 
consider lexicographical enumeration syntactically correct lisp programs lexicographical enumeration syntactically correct java programs 
assume programs encoded standard pre free manner 
proper de nitions view programs enumerations computing partial recursive functions inputs outputs 
choosing machines enumerations de ne complexities lisp java completely analogous 
measures descriptional complexities coincide xed additive constant 
show directly lisp java 
lisp universal exists lisp program implementing java lisp compiler 
translates java program equivalent lisp program 
consequently lisp java 
similarly java program lisp java compiler java lisp 
follows jk java lisp 
gruenwald tex programming language view immediately tells small simple regular objects example exists xed size program input outputs rst bits halts 
speci cation takes ln log log log bits 
consists rst binary digits log log log similarly denotes string consisting log log log hand exists program print halt 
shows 
previously noted pre code strings described bits 
particular holds pre code length function 
fraction strings length overwhelming majority sequences compressed constant 
speci cally determined independent tosses fair coin overwhelming probability 
regular strings kolmogorov complexity small sublinear length string strings random kolmogorov complexity equal length 
problem unfortunately recursive function kolmogorov complexity computable general 
means exists computer program input arbitrary string outputs kolmogorov complexity string halts 
exist feasible resource bounded forms kolmogorov complexity li vit anyi lack elegant properties original uncomputable notion 
suppose interested ecient storage transmission long sequences data 
kolmogorov compress sequences essentially optimal way storing transmitting shortest program generates 
unfortunately just seen nd program general 
shannon compress sequences optimally average sense turns high probability distributed know unfortunately practice unknown nonexistent 
shannon kolmogorov idea directly applicable actual data compression problems 
universal codes may viewed time extension shannon kolmogorov theory 
gruenwald tex 
universal coding interpolating kolmogorov shannon repeatedly coding concepts introduced section 
suppose recursive enumeration pre codes 
length functions associated codes 
xg exists 
may encode rst encoding natural number standard pre code natural numbers 
encode code leads called part code lengths construction code pre lengths satisfy min ln nite binary sequence initial bit segment sequence 
ln log log recall xed fraction sequences length compressed bits typically codes strings grows linearly implies newly constructed code list best particular di erence code lengths bounded constant depending particular nite sequence lim code satisfying called universal code relative comparison class codes fd universal sense compresses sequence essentially compresses particular sequence 
general exist types codes universal part universal code de ned just means achieving 
universal codes kolmogorov practically interesting cases may assume decoding function computable exists pre turing machine input pre free version outputs halts 
program nite length gruenwald tex encoding function de ned earlier 
comparing shows code encoding function universal code relative 
see kolmogorov complexity just length function universal code note example universal code explicitly part 
example 
create universal part code allows signi cantly compress binary strings frequency deviating signi cantly 
hn code assigns code words equal minimum length strings length zeroes code words strings 
hn pre code hn dlog universal part code relative set codes fd hi ji ng achieves lengths bit ng zeroes log log log log log log log log log stirling approximation factorial 
nd log log 
log log 
log log log log nh log note equality frequency deviates signi cantly compresses factor linear cases compresses data linear factor 
note individual code hn capable exploiting particular type regularity sequence compress sequence universal code may exploit di erent types regularities compress sequence code lengths kolmogorov complexity asymptotically exploits computable regularities maximally compress sequence 
universal codes shannon distributed distribution optimal average sense code shannon fano code 
suppose known possibly large uncountable set gruenwald tex candidate distributions 
clear code optimal 
may try shannon fano code particular code typically lead large expected code lengths turns distributed may ask exists code shannon fano code matter generates sequence 
show provided nite countable surprisingly answer 
see need notion information source 
information source may thought probability distribution arbitrarily long sequences observer gets see longer longer initial segments examples 
formally information source probability distribution set way nite sequences 
identi ed distributions 
denotes marginal distribution rst bit segments 
related follows xy 
suppose nite countable set information sources 
members may listed 
marginal distribution corresponds unique shannon fano code de ned set lengths hn ki log de ne log entropy distribution rst outcomes 
pre code assigning codeword source word noiseless coding theorem page asserts minimal average codeword length pre codes satis es entropy interpreted expected code length encoding rst bits generated source optimal shannon fano code 
look pre code length function satis es lim 
de ne part code rst encoded standard pre code gruenwald tex natural numbers 
codes hn ki minimizes hn ki encoded standard pre code nally encoded hn ki bits 
sequence hn ki ln ln holds strings length hold expectation possible distributions strings length particular gives hn ki log log follows 
historically codes satisfying called universal codes relative codes satisfying considered literature usually called universal codes individual sequences merhav feder 
part code just de ned universal individual sequence average sense achieves code lengths constant achieved hn ki individual sequence achieves expected code lengths constant shannon fano code may say interpolates shannon codes optimal speci kolmogorov code length function de nition additive constant example 
suppose sequence generated independent tosses coin bias tossing head 
identifying heads probability outcomes initial segment set corresponding information sources containing element 
uncountable set universal code exists 
fact shown code lengths example universal satis es 
reason roughly follows data generated coin bias probability frequency converges tends 
interested practical data compression assumption data generated biased coin source restricted 
richer classes distributions formulate universal codes 
example take class markov sources order probability may depend arbitrarily earlier outcomes 
ideas form basis data compression schemes practice 
gruenwald tex codes universal class markov sources order encode decode real time easily implemented 
nd shortest program generates particular sequence possible ectively nd shortest encoding quite sophisticated class codes 
expected kolmogorov complexity shannon entropy suppose source words distributed random variable probability 
xed gives shortest code word length xed constant independent probability distribution may wonder universal sense weigh individual code word length probability resulting expected code word length achieves minimal average code word length log 
sum entire support restricting summation small set example singleton set fxg give di erent result 
reasoning implies mild restrictions distributions answer 
expressed theorem quotient look di erence 
allows express really small distinctions 
call source recursive exists turing machine input hn yi outputs precision theorem li vit anyi theorem 
recursive information source 
constant depends 
shannon fano code computable distribution computable 
computable distribution universal code length function kolmogorov complexity compresses average shannon fano code intuitive reason matter computable distribution take expected kolmogorov complexity close entropy 
gruenwald tex 
mutual information 
shannon mutual information information random variable convey random variable purely combinatorial approach notion captured follows ranges sx ranges look set possible events consisting joint occurrences event event equal cartesian product sx means dependency considering set ug sx natural de ne conditional entropy jx log 
suggests immediately information jx example sx sx 
formulation obvious xjx 
approach amounts assumption uniform distribution probabilities concerned 
generalize approach account frequencies probabilities occurrences di erent values assume 
joint probability de ned probability joint occurrence event event 
leads self evident formulas joint variables log log log summation taken outcomes random variable summation taken outcomes random variable show equality case independent 
equations entropy quantity left hand side increases choose probabilities right hand side equally 
gruenwald tex conditional entropy conditional probability bja outcome outcome random variables necessarily independent de ned bja leads analysis information rst considering conditional entropy average entropy value weighted probability getting particular value jx jx bja log bja log bja quantity left hand side tells uncertain outcome know outcome log log log substituting formula bja nd jx 
rewrite expression entropy equality jx interpreted uncertainty joint event uncertainty plus uncertainty 
combining equations gives jx taken imply knowledge increase uncertainty fact uncertainty decreased independent 
information outcome de ned jx quantities jx right hand side equations equal corresponding quantities uniform distribution analyzed rst 
values gruenwald tex quantities assumption uniform distribution jx versus distribution related inequality particular direction 
equalities xjx hold distribution variables 
function outcomes function outcomes compare directly 
forming expectation de ned combining equations see resulting quantities equal 
denoting quantity calling mutual information see information symmetric example 
suppose want exchange information outcome known outcome case property require code log xjy bits communicate average joint distribution xjy bits optimal shannon noiseless coding theorem 
fact exploiting mutual information paradigm expected information outcome gives outcome expected information gives interpretation terms sequential questions just entropy re interpret mutual information terms protocols asking questions 
suppose sequentially asks questions example ask questions told sequentially asks questions nd value protocol de ned shannon fano code 

shannon noiseless coding theorem optimal protocol 
intuitively initial information expect ask fewer questions initial information 
denotes exactly fewer questions expect need ask average told value asking questions 
average average needs ask fewer questions 
may certainly exist individual negative 
example may gruenwald tex jy jy 

small quantity smaller 
problem quantity symmetrically characterizes extent random variables correlated 
inherent problem probabilistic de nitions just seen positive probability distributions turn negative de nitely contradicts naive notion information content 
possible 
concept information theory communication probabilistic notion natural information transmission communication channels 
tend identify probabilities messages frequencies messages suciently long sequence conditions stochastic source rigorously justi ed 
great kolmogorov remarks goes wrong problem lies vagueness ideas relation mathematical probability theory real random events general 
algorithmic mutual information introduce negative sense closer intuitive notion information content 

algorithmic mutual information conditional kolmogorov complexity prepare de nition shannon mutual information rst needed introduce conditional version entropy 
analogously prepare de nition algorithmic mutual information need notion conditional kolmogorov complexity 
intuitively conditional pre kolmogorov complexity xjy interpreted shortest pre program program input program prints halts 
idea providing input realized putting hp yi just input tape universal pre machine definition 
conditional pre kolmogorov complexity free xjy min fl hp yi de ne xj 
note just rede ned unconditional kolmogorov complexity exactly equal conditional kolmogorov complexity gruenwald tex empty input 
contradict earlier de nition choose pre machine hp 
automatically xj 
recall section notation 
de nition hx yi 
trivially symmetry property holds 
interesting property additivity complexity property rst standard enumeration order shortest pre program generates halts 
easy see information pair compute run programs simultaneously fashion select rst program length halts output 
fashion means phase process run programs steps kolmogorov complexity equivalent entropy equality 
equality holds true simply rewriting sides equation de nitions averages joint marginal probabilities 
fact potential individual di erences averaged 
kolmogorov complexity case truly remarkable additivity algorithmic information holds individual objects 
result due acs theorem li vit anyi dicult proof 
instructive point version just conditionals doesn hold holds additive logarithmic terms eliminated 
de ne algorithmic mutual information individual objects probabilities involved instructive rst recall probabilistic notion rewriting log log log noting log close length pre free shannon fano code led de nition 
information de ned gruenwald tex second equality consequence states information symmetrical talk mutual information 
theorem gave relationship entropy ordinary kolmogorov complexity showed entropy distribution approximately equal expected kolmogorov complexity 
theorem gives analogous result mutual information facilitate comparison theorem note may stand strings arbitrary length 
theorem 
recursive probability mass distribution constant depends length shortest pre free program computes input 
see expectation algorithmic mutual information close probabilistic mutual information 
interpretation terms sequential questions algorithmic mutual information xjy equals xjy additive logarithmic term log savings number questions needs ask get know knows clearly empty word information needs ask questions obtain consecutive bits knows needs ask xjy questions obtain shortest program compute caveat usual arbitrary amounts time storage perform computation speci individual number far average shannon mutual information 
problem entropy kolmogorov complexity mutual algorithmic information concepts distinguish di erent kinds information meaningful meaningless notation algorithmic individual notion distinguishes probabilistic average notion 
deviate slightly li vit anyi de ned 
gruenwald tex information 
re ned notions arrived constraining description methods strings allowed encoded considering lossy lossless encoding 
basic notions entropy kolmogorov complexity mutual information continue play fundamental ole 
important developments rate distortion theory shannon setting shannon cover thomas dealing useful information kolmogorov structure function kolmogorov setting dealing meaningful information kolmogorov shen cover thomas acs vereshchagin vit anyi vit anyi rissanen 
theories may relevant say notions information studied logic semantics natural language communities van 
brie illustrate rate distortion theory 

shannon rate distortion information questions consider situation sender wants communicate outcome random variable receiver distribution known allowed nite number say bits communicate send di erent messages 
encoding function map map back jx uncountable say ir code sure reconstructed 
best thing may agree code sense close possible original formalize code de ne function range may interpret estimate set values take 
assume goodness approximation measured distortion function ir 
distortion function may appropriate situation hand 
xed may consider expected distortion ir sum replaced integral stands probability density respect lebesgue measure 
gruenwald tex rate distortion setting goal determine code associated minimizes expected distortion 
example 
suppose real valued normally gaussian distributed random variable mean variance squared euclidean distance distortion measure 
allowed bits elements ir uncountably nite 
choose function minimized 
suppose rst 
optimal turns domain partitioned regions corresponding 
boundary evident symmetry gaussian distribution 
region picks representative point minimize 
similarly partitioned regions represented single point minimized 
extreme case estimate information whatsoever 
means take value expected distortion minimized picks giving distortion equal reason general distortion function symmetric fact may pertains situation hand 
considered minus utility function indicating loss incurs predict knowing precise value 
interpretation terms sequential questions previously interpreted entropy expected minimum number questions receiver ask sender order determine precise outcome random variable setting interpreted terms involved question answer game receiver allowed ask 
come guess outcome quality guess measured 
goal receiver ask possible questions reduce expected distortion possible equivalently increase expected utility possible 
relation quality quantity information exchange van studied natural language semantics 
gruenwald tex concrete case gaussian example receiver ask 
question reduces expected distortion lesser amount 
general question answer game di erent original game goal minimize total number questions 
example shows take special distortion measure goal minimizing distortion minimizing total number questions reconciled 
example 
suppose receiver wants estimate actual probability distribution bits allowed di erent distributions sent receiver 
best done partition subsets sender observes passes information receiver 
little thought reveals information tells receiver distributed conditional distribution 

natural measure quality distribution 
entropy additional number questions receiver ask knows value certainty 
take log distortion function shannon fano code length communicated distribution 
implicitly generalized de nition distortion measure require estimates take values 
set probability distributions new de nition includes special case 
log expected distortion 
minimum achievable distortion min minimum sets distributions 
particular general minimum expected number questions ask determine just answers rst questions 
pick shannon fano code length distortion measure rate distortion theory reconciled lossless compression theory 
case distortion rate function shows fast entropy decreases information gained receiver increases receiver asks possible question highest expected information gain 
rate distortion mutual information increases minimum achievable distortion smaller smaller 
shannon 
tex studying functional relationship minimum achievable distortion called distortion rate function 
technical reasons convenient study function celebrated function 
main results original shannon showed deep connection mutual information rate distortion function holds matter distortion function distortion 
mention result illustrates mutual information fundamental notion precise statement refer cover thomas 

topics reading topics shannon important developments shannon original discussed noiseless coding theorem lossless compression theorem notion related lossy compression 
discuss channel coding theorem related lossless communication noisy channel 
topics shannon information theory thoroughly discussed explained standard cover thomas 
topics kolmogorov kolmogorov complexity applications discuss 
leads formal notion randomness individual sequences refer underlying probability distribution 
lies basis powerful mathematical theory inductive inference 
third led new mathematical proof technique called incompressibility method 
topics kolmogorov complexity thoroughly discussed explained standard li vit anyi 
mentioning exciting development kolmogorov structure function 
kolmogorov structure function kolmogorov structure function kolmogorov shen cover thomas acs vereshchagin vit anyi vit anyi rissanen viewed extent analogue kolmogorov theory shannon rate distortion 
encoding objects strings parts structural random part 
encountered simple example description example rst gruenwald tex encoded frequency ones string simple structure particular sequence frequency corresponding random part description 
intuitively meaning string resides structural part size structural part quanti es meaningful information message 
exciting new results area see vereshchagin vit anyi 
kolmogorov structure function closely related rissanen minimum description length principle inductive inference 
simplest guise says best theory set data theory minimizes description length theory plus description length data theory 
data encoded rst encoding theory constituting structural part data encoding data properties data prescribed theory 
picking theory minimizing total description length leads automatic trade complexity chosen theory goodness data 
provides practical successful principle inductive inference may viewed mathematical formalization occam razor 
quite story refer gr rissanen details 
chaitin 
algorithmic information theory 
cambridge university press 
cover thomas 
elements information theory 
wiley interscience new york 
acs 
symmetry algorithmic information 
soviet math 
dokl 
correction ibid 
acs tromp vit anyi algorithmic statistics 
ieee trans 
inform 
th 

gr 
manuscript cwi 
fisher 
mathematical foundations theoretical statistics philosophical transactions royal society london ser 

kolmogorov 
approaches quantitative de nition information 
problems inform 
transmission 
kolmogorov 
talk information theory symposium estonia acs cover attended 
li vit anyi kolmogorov complexity applications 
springer verlag new york revised expanded second edition 
merhav feder universal prediction ieee trans 
inform 
theory 
rissanen 
stochastic complexity statistical inquiry 
world scienti publishing 
rissanen 
kolmogorov structure function probability models 
proc 
ieee information theory workshop 
pp 
ieee press 
gruenwald tex shannon 
mathematical theory communication 
bell system tech 

shen kh 
concept kolmogorov stochasticity properties 
soviet math 
dokl 
van 
quality quantity information exchange 
journal logic language information volume 
vereshchagin vit anyi kolmogorov structure functions application foundations model selection proc 
th ieee symp 

comput 
sci 
vit anyi li minimum description length induction bayesianism kolmogorov complexity ieee trans 
inform 
theory 
vit anyi meaningful information 
proc 
th international symposium algorithms computation isaac vol 
lecture notes computer science 
berlin pp 
springer verlag 
gruenwald tex 
