discriminant analysis gaussian mixtures trevor hastie statistics data analysis research group bell laboratories murray hill new jersey robert tibshirani department preventive medicine biostatistics department statistics university toronto toronto ontario february flat bell laboratories university toronto fisher rao linear discriminant analysis lda valuable tool classification 
lda equivalent maximum likelihood classification assuming gaussian distributions class 
fit gaussian mixtures class facilitate effective classification non normal settings especially classes clustered 
low dimensional views important product lda new techniques inherit feature 
able control class spread subclass centers relative class spread 
technique fitting models permits natural blend nonparametric versions lda 
keywords classification pattern recognition clustering nonparametric penalized 
generic classification discrimination problem outcome interest falls unordered classes convenience denote set delta delta delta jg 
wish build rule predicting class membership item measurements predictors features training sample consists class membership predictors items 
important practical problem applications fields 
traditional statistical methods problem include linear discriminant analysis lda multiple logistic regression nearest neighbor methods classification trees 
neural network classifiers powerful alternative ability incorporate large number features adaptive nonlinear model 
ripley gives informative review statisticians viewpoint 
lda derived maximum likelihood method normal populations different means common covariance matrix 
natural generalize lda assuming observed class fact mixture unobserved normally distributed subclasses 
approach mentioned statistical literature mclachlan cheng titterington example pattern recognition literature generated attention 
develop mixture approach discrimination number interesting useful directions 
brevity refer mixture discriminant analysis technique mda 
lda classification procedure data reduction tool 
represent multi class data low dimensional projections plots highlight class differences 
mda procedure feature produce hierarchy coordinates terms abilities separate classes subclasses 
interesting twist occurs class data lda produces interesting single coordinate mda produce number subclasses mixture representation 
technique known learning vector quantization lvq received lot attention pattern recognition literature kohonen mda viewed smooth version lvq 
lvq finds set cluster centers class classification performed finding closest center assigning associated class 
online learning algorithms lvq similar means algorithm biases built encourage classification 
lvq generalizes clustering classification problems mda generalizes mixture density estimation classification problems 
additional feature model subclass shrinkage regulate class spread mixture centers relative class spread 
emphasizes bias classification lvq biases clustering techniques classification 
allows subclass centers class shrink effective number centers required smaller amount 
methods proposed generalize lda allow nonlinear decision boundaries 
hastie tibshirani buja achieved adaptive nonparametric regression methods 
link nonparametric regression discriminant analysis provided optimal scoring approach suggested breiman 
hastie 
call procedure flexible discriminant analysis fda 
generic version fda smoothing splines operates expanding predictors large adaptively selected basis set performing penalized dis linear discriminant analysis mixture discriminant analysis 
learning vector quantization 
flexible discriminant analysis mixture discriminant analysis rank model 
mda fda class problem classes occurs separated circular clouds 
panels labeled technique produced decision boundary 
analysis enlarged space 
intuitively penalization works rough directions relative smooth directions enlarged space computing mahalanobis distances 
pda hastie buja tibshirani closely related technique classifying digitized analog signals 
pda starts high dimensional feature set pixels digitized image spectral values grid frequencies 
penalized discriminant analysis ensure spatial smoothness discriminant coefficients 
techniques adapt naturally mda enlarged feature space mixture gaussians single class penalized metric computing distances 
illustrates techniques simple example 
surprisingly mda technique satisfactory boundary problem setup mda boundary optimal 
lvq boundary unnecessarily biased 
fda boundary reasonable approximation optimal boundary 
lower left panel shows rank version mda fit acceptably 
lower right panel shows result mda combined nonlinear transformations fda noticeable improvement 
summarize proposal mda features ffl classes modelled mixtures gaussians single gaussian lda ffl optimal subspace identification possible lda added functionality ffl shrink subclass centers example common center ffl flexibility fda pda easily naturally accommodated 
organized follows 
section discuss normal mixture model em algorithm estimation 
section discuss reduced rank versions procedures demonstrate section known waveform example breiman friedman olshen stone 
section show mda algorithm expressed repeated regression procedure optimal scoring hastie turn allows useful nonparametric extensions 
section illustrates techniques handwritten digit recognition problem 
section centroid shrinking section describes comparative examples 
extending lda normal mixtures approach classification taken model class densities predictors gaussian mixture models 
flipped bayes theorem class priors give models class posterior probabilities basic ingredient classification 
suppose training data theta divide class artificial subclasses denoted jr define model assumes subclass multivariate normal distribution mean vector jr common covariance matrix sigma 
possible mixture model example allow subclass different covariance matrix force subclasses class covariance matrix 
particular model chosen attractive keeps total number parameters control see right structure permit generalizations mind 
pi prior class class jr mixing probability rth subclass jr 
note pi known easily estimated training data jr unknown model parameters 
gamma sigma gamma gamma mahalanobis distance mixture density class xjg sigmaj gamma jr gammad jr conditional log likelihood data mix rj sigma jr log em algorithm provides convenient method maximizing mix 
em steps jr jx prob rth subclass class jx jr gammad jr jk gammad jk jr jr jx jr jr jr jx jr jx sigma jr jx gamma jr gamma jr notation means summing observations belonging jth class 
estimation step maximization step 
straightforward generalization em algorithm estimating normal mixtures titterington smith makov page 
appearance maximumlikelihood estimate complete normal discriminant problem situation observe subclass membership 
difference subclass indicator replaced jr jx estimated probability observation falls subclass jr observed fall class jr jx function jr sigma equations iterated 
posterior class probabilities bayes theorem jjx pi prob xjj pi jr gammad jr normalized jjx 
classification rule chooses maximize jjx 
notice form linear discriminant rule subclasses particular nonlinear 
cluster sizes starting values em iteration requires choice cluster sizes starting values means jr covariance matrix sigma cluster probabilities jr jx 
currently different strategies means choose fixed number clusters say class means clustering algorithm estimate set subclass centroids jr class 
observations class jr jx set jr closest centroid 
lvq run lvq algorithm training data select jr supply 
previous case output produce jr jx 
case class weights determine new set jr sigma iterations go 
means lvq require starting centers typically randomly selected known pal bezdek tsao example procedures suffer variability due random starting values 
strategy try number different starts choose best 
likelihood criterion cheaply training sample misclassification guide choice 
experience mda outperforms starting procedure lvq kmeans respect classification training data 
fact procedure implemented software em algorithm step carried optimal scoring described section 
show computational advantages facilitates powerful generalizations model 
reduced rank discrimination linear discriminant analysis classes choose subspace rank maximally separates class centroids 
mainly useful descriptive purposes form regularization leads improved classification performance 
standard fisher rao decomposition mardia kent bibby example derived successively maximizing ratio group variance linear combinations variables bv wv class covariance class centroids pooled class covariance 
section show reduced rank lda viewed restricted gaussian maximum likelihood solution extend concept mixture model 
proposition consider maximizing gaussian log likelihood sigma gamma gamma sigma gamma gamma gamma log sigmaj subject constraints min gamma 
solution wv gamma sigma gamma gamma wv bv matrix consisting leading eigenvectors gamma solution effectively coincides reduced rank lda solution ffl projection jth sample mean discriminant subspace rank ffl classification gamma sigma gamma gamma equivalent reduced linear discriminant rule rank result proved campbell appears overlooked claim outline simple proof appendix 
consider reduced rank version mixture model 
take gaussian mixture log likelihood mix equation maximize subject rank constraint subclass means rj achieve maximization 
em algorithm 
steps remain conditional current reduced rank versions centers corresponding pooled covariance estimate 
steps viewed weighted mean pooled covariance maximum likelihood estimates weighted augmented class problem 
augment data replicating observations class times th replication having observation weights jx 
done classes resulting augmented weighted training set observations 
note sum weights impose rank restriction 
analogy early part section achieved weighted version lda 
postpone details final step carried section 
details worth noting right away ffl step em algorithm requires differences mahalanobis distances estimated centers 
convenient weighted lda supply distances dimensional subspace spanf jr ffl solution problem obtained simple reduction full rank mixture solution 
need perform reduced rank weighted lda iterations em algorithm 
reason weights computed step depend full rank solution 
collect results 
proposition consider maximization constrained mixture density loglikelihood mix jr sigma jr log jr gammad jr gamma log sigmaj subject achieved em algorithm analogous defined steps replaced equations maximizing weighted augmented log likelihood weight rj sigma gamma jr jx jr gamma log sigmaj subject maximization achieved similarly augmented weighted rank lda 
lower left panel shows effect rank reduction simple example 
practically speaking approximate reduced rank solution obtained weighted rank reduction full mixture solution attractive exact reduced rank solution described 
requires iteratively fit model dimension interest 
simulation experiment shown approaches tend agree quite 
example waveform data discriminant var subclasses penalized df discriminant var discriminant var subclasses penalized df dimensional view mda model fit sample waveform model 
points independent test data projected leading canonical coordinates 
subclass centers indicated 
illustrate ideas popular simulated example taken breiman 
pg hastie 

class problem variables considered difficult pattern recognition problem 
predictors defined uh gamma ffl class uh gamma ffl class uh gamma ffl class uniform ffl standard normal variates shifted triangular waveforms max gamma ji gamma gamma 
table results waveform data 
values averages simulations standard error average parentheses 
entries line taken hastie 

model line mda subclasses class 
line discriminant coefficients penalized roughness penalty effectively df 
third second centroids shrunk effectively centroids class 
technique error rates training test lda qda cart fda mars degree fda mars degree mda subclasses mda subclasses penalized df mda subclasses shrunk penalized df table extends simulation hastie 
include methods discussed 
training sample observations equal priors roughly observations class 
test samples size 
mda models described 
details penalization centroid shrinkage section 
shows leading canonical variates models evaluated test data 
guessed classes appear lie edges triangle 
represented points space forming vertices triangle class represented convex combination pair vertices lie edge 
bayes risk problem breiman mda comes close optimal rate surprising structure mda model similar generating model 
mda optimal scoring optimal scoring multiple linear regression followed eigen analysis fit lda mda models show section 
significant computational advantages facilitates useful generalizations techniques 
consider lda model 
yn thetaj indicator response matrix representing classes regress predictor matrix get fitted values hy followed suitably normalized eigendecomposition hy contains ingredients lda rank breiman hastie 

simple procedure carries step mda algorithm 
response indicator matrix blurred response matrix zn thetar rows consist current subclass probabilities observation 
step multiple linear regression followed eigen decomposition just lda case 
hastie 
called procedure optimal scoring describe detail mda 
mixture discriminant analysis optimal scoring mda initialize start set subclasses jr class associated subclass probabilities jr jx 
derived example lvq means preprocessing data 
iterate 
compute response define blurred response matrix zn thetar follows fill jth block entries ith row values jr jx rest zeros 
mixture analog indicator response matrix observations belong sub classes associated probabilities 

multivariate linear regression fit multi response linear regression fitted values vector fitted regression functions 

optimal scores theta largest nontrivial eigenvectors normalization theta theta diagonal theta matrix weights rth entry sum elements rth column total weight subclass 

update fitted model step optimal scores theta 

update jr jx jr formulas 
show mda algorithm corresponds em algorithm proposition need show steps fit augmented weighted rank discriminant analysis 
proof appendix 
don obtain estimates means covariance mda algorithm 
dimensional fit produced algorithm coordinate discriminant projection subspace spanned reduced rank subclass centroids known scale factors 
relative euclidean distances subspace mahalanobis distances implicitly estimated covariance 
projected means scaled rows theta 
relative distances needed ingredients needed fitting model classification 
details equivalences hastie 
advantage approach immediately clear step proposition involved weighted lda observations observations 
compelling reason optimal scoring fit lda mda models 
expressing problem multiple regression response matrix immediately generalize step regression methods exotic linear regression 
context lda ideas originally proposed breiman developed hastie 
classes generalizations emerge fda multivariate adaptive nonlinear regression replaces linear regression linear map nonlinear map 
hastie 
describe class nonlinear regression methods including adaptive additive models multivariate adaptive regression spline mars model friedman projection pursuit regression neural networks 
call resulting procedure fda flexible discriminant analysis 
procedures amount expanding original predictors adaptive way set transformed predictors performing lda mda new space 
cases nonparametric regressions consist basis expansion followed penalized regression additive splines 
case transformed space large penalized regression translates penalized version lda mda 
pda fit restrictive linear map penalized squares hastie 
idea feature vectors arise digitized analog signals spatially correlated pixels image log spectra different frequencies 
penalization amounts regularizing large near singular covariance matrix sigma adding penalty sigma omega gamma turn ensures coefficients spatially smooth borrow strength neighboring values 
don explicitly compute means covariance transformed feature space needed regression fits 
case fda feature space high dimensional space basis functions simplification important computationally 
class discriminant functions class discriminant functions class discriminant functions discriminant functions subclasses waveform example panel represents subclasses single class 
smooth functions sigma omega gamma gamma jr produced mda algorithm penalized regression method 
functions versions sigma gamma jr superimposed figures generating waveforms shifted scaled fit plots 
section demonstrate fda extension mda examples 
experience examples tried far suggests ffl fda standard mda capable producing non linear decision boundaries 
adding non linear capabilities fda mda puts components competition typically yield improved decision boundary 
supports claim 
imagine difficult situations components support centers provided nonlinearities compensate extreme nonlinearities costly terms fitted degrees freedom avoided mixtures 
ffl adaptive aspect fda plays role variable selector examples section support claim 
pda extension mda justified exactly way pda 
examples support penalization ffl waveform predictors section represent noisy functions sampled evenly spaced 
discriminant coefficients penalized nd derivative roughness penalty reducing effective number parameters gave average improvement classification performance 
shows fitted penalized discriminant functions sigma omega gamma gamma jr subclass compares versions 
see penalization plays roles variance reduction mda discriminant functions appear unnecessarily pay price generalization errors 
interpretation superimposing known generating functions see discriminant functions appropriately placed picking peaks valley mixture distribution class 
harder convincing attempt interpretations discriminant functions 
ffl handwritten digit example section natural mixture models penalization characteristic ways writing digit calling mixture model pixel grayscale values strong spatial correlation calling regularization reasons waveform example 
example handwritten digit recognition random selection digitized handwritten 
image bit grayscale version original binary image size orientation normalized theta pixels 
hastie 
illustrated pda handwritten digit recognition task 
focus difficult sub task distinguishing handwritten 
training data le cun 
normalized binary images size orientation resulting bit theta grayscale images 
training set roughly quarter number test examples 
shows examples 
inputs highly correlated spatial arrangement kind smoothing filtering helps 
dimensional orthonormal basis smooth functions theta spatial domain image 
derived roughness penalty matrix dimensional rd degree thin plate spline smoother trailing eigenvectors smoothest penalized 
theta basis matrix input pixel vector replaced px 
performance lda improved nearly filtering 
mda subclasses digit gave better performance regularization helped ffl reducing yielded improvement ffl shrinking total effective centers yielded improvement 
table digit classification results 
filtered models correspond hierarchical basis smooth dimensional functions derived thin plate smoothing spline penalty functional 
shrunken mda model shrunk centroids effective total 
technique error rates training test lda lda filtered df mda filtered df subclasses mda filtered df subclasses mda filtered df subclasses shrunk subclass centroids filtered mda fit displayed unfiltered form 
image class mixing parameter jr indicated 
centroids different suggests different numbers different classes 
subclasses filtered discriminant var discriminant var subclasses filtered discriminant var discriminant var subclasses filtered shrunk discriminant var discriminant var subclasses filtered shrunk discriminant var discriminant var top panels show discriminant coordinates mda fit centroids indicated 
lower panels show corresponding coordinates shrunken mda model 
see case leading coordinates concentrate group separation separation subclass centroids plays stronger role rd coordinate 
top left plot subclass separation weight class separation see class overlap 
displays fitted subclass centroids images spatial differences apparent class 
illustrates effect shrinkage eigenvectors discussed section 
shrinking penalization section precise notion shrinking sub class centers interpretation penalized optimal scoring context model 
mda centroid shrinking subspace reduced model proposition fit iteratively weighted lda essentially treats classes subclasses interchangeably 
proposal bias decomposition positions means way class variance subclass centroids damped relative class variance 
penalizing mixture likelihood subclass variability 
limit infinite penalty expect standard single gaussian class lda model emerge 
jk vector subclass probabilities jth class summing delta gamma identity matrix column vector ones 
vector elements delta vector deviations elements weighted mean likewise delta delta positive scalar measure spread mean 
delta delta corresponding penalty matrix diag fl fl fl appropriate composite penalty matrix penalizing sub components vector deviations class means 
relative strengths penalties diagonal controlled parameters fl consider maximizing penalized version definition theta rank matrix means rank mda problem 
shrunken mda estimate rank maximizes penalized mixture log likelihood mix sigma jr mix sigma jr gamma tr sigma gamma qu subject rank sigma gamma weight components penalty explained proposition 
em algorithm emerges just case step corresponds penalized reduced rank augmented weighted class gaussian log likelihood analogous proposition weight rj sigma gamma jr jx jr gamman log sigmaj sigma gamma qu subject rank unfortunately maximizer simple solution version 
solution characterized expression means covariance just appendix 
solution requires iteration mean covariance estimators 
apart added computational complexity destroys valuable link optimal scoring 
prefer restricted version fix estimated covariance weighted pooled subclass covariance 
restricted case proposition consider maximizing restricted step penalized log likelihood weight rj jw gamma jr jx jr gamma tr gamma qu subject rank delta delta defined terms solution corresponds shrunken lda 
theta weighted subclass centroid matrix diagonal weight matrix rth element sum weights rth subclass shrunken lda shrunken subclass matrix bq md gamma shrunken lda fit modified mda algorithm change decomposed normalization theta theta proof appendix 
form penalty requires explanation 
modelled mean matrix rows subclasses columns predictors 
penalty matrix works individual columns composite penalty add sensible way predictors correlated 
rank mean model written sigma phi thetak thetak sigmav determined 
form solutions emerge 
tr sigma gamma qu tr phi phi phi gamma projection means subspace spanned sigma orthonormal basis simply add penalties uncorrelated columns phi equivalent adding sigma gamma weighted penalties columns form clear fl get large vectors permitted constant class problem degenerate standard lda model 
choosing amount shrinking practice difficult guess suitable values fl strategy fl fl choose fl objective way example crossvalidation evaluation test data set 
describe strategy helps analyst choose fl subjective way 
proof proposition emerges shrunken means fl gamma subsequent lda 
shrinkage operator fl gamma smoother sense trace effective number parameters shrinking hastie tibshirani 
easy backwards derive fl tr df nominal value df subjective strategy select large number subclass centers class fit shrunken model ffl effective number centers df df ffl effective number centers class df df fact block diagonal compute df class 
illustrates case subclasses shrunk effective class 
similarly shrunk total centers effective total 
notice leading eigenvectors focus class separation case third starts picking subclass variation 
mda penalized optimal scoring general steps mda fda mda pda optimal scoring algorithm seen minimize jr jx rk gamma fi fi omega fi omega penalty matrix scoring functions suitable normalized 
represent original predictors penalty enforces smoothness coefficients vectors fi basis expansion original predictors penalty ensures composition smooth original domain 
reasonable goal consistent approach far derive penalized optimal scoring criterion step suitably regularized gaussian mixture likelihood 
case fi scaled versions right eigenvectors hastie natural maximize mixture likelihood subject appropriate penalty approach taken context penalized lda 
analyze simpler case penalized lda penalized gaussian likelihood 
unfortunately lead simple non iterative mle sigma unknown sigma assumed known equal differs optimal scoring procedure 
hastie 
show optimal scoring corresponds lda fixed penalized class covariance omega gamma words gaussian maximum likelihood rank reduced means sigma assumed known equal omega gamma approach taken silverman 
mda things get complicated corresponding version weighted covariance changes iteration 
unable write explicit likelihood criterion 
penalized mda optimal scoring algorithm corresponds omega place step 
examples simulated examples real data compare number classification procedures 
methods include 
lda linear discriminant analysis variables 

mda mixture discriminant analysis described section 
fda flexible discriminant analysis adaptive backfitting described hastie 

mda fda mixture discriminant analysis combined flexible discriminant analysis described section 
neural network single layer perceptron sigmoidal outputs cost function weight decay variable metric optimizer ripley 

lvq version kohonen learning vector quantization described hertz krogh palmer 
table results simulation 
values averages simulations standard error average parentheses 
technique error rates noise variables training test training test lda mda mda fda fda neural net lvq simulation separated clouds examine detail example 
classes variables 
class observations bivariate normal distribution mean mean 
second class mean 
covariance matrix identity cases 
left part table shows summary performance number procedures realizations 
mda procedure performs best adaptive procedures outperform lda 
fda works transforming predictor space subclasses class lie top 
right half table shows results noise variables added predictor set 
deterioration performance mda neural network quite striking 
fda mda fda show best performance probably role fda variable selector 
simulation unstructured data example simulated data extremely disconnected class structure 
classes spherical bivariate normal subclasses having standard deviation 
means subclasses chosen random replacement integers delta delta delta theta delta delta delta 
training sample observations subclass total observations 
testing test samples size 
table shows error rates simulations 
mda mda fda neural net clearly outperform procedures 
fda lvq offer large improvements lda 
right half table added independent noise variables 
performance mda degrades little neural net especially lvq adversely affected 
mda fda able select important input table results simulation 
values averages simulations standard error average parentheses 
technique error rates noise variables training test training test lda mda mda fda fda neural net lvq prototypes dimensions lowest error rate 
example thyroid data example data consist observations patients 
object predict thyroid status normal hyper hypo measurements 
uptake test 
total serum 
total serum 
basal thyroid stimulating hormone 
maximal absolute difference value injection micro grams releasing hormone compared basal value 
described 

took random training sample size balance observations acted test sample 
classification results shown table 
nonlinear methods perform mda fda winning small margin 
discussion classification gaussian mixtures new idea 
added functionality approach table thyroid data results technique error rates training test lda mda mda fda fda neural net hidden units ffl reduced rank versions allow valuable low dimensional views data class case provide natural means regularization 
ffl step em algorithm solved weighted optimal scoring algorithm amounts multiple linear regression blurred response matrix ffl allows natural generalizations replacing linear regression exotic forms adaptive nonparametric regressions enrich procedure expanding transforming predictor set penalized regressions regularize coefficients cases predictors sampled analog signals spatial domain 
ffl shrink subclass variability relative class variability 
leads sensible low dimensional views natural way regularize presence subclasses 
ffl procedure provides smooth enhancement lvq algorithm fact uses lvq initialization 
comparisons discrimination techniques variety different problems useful 
software written set functions fitting fda pda mda models splus language becker chambers wilks chambers hastie 
function fda fits fda pda models 
method argument allows user specify multi response regression method default linear regression fisher lda 
regression methods provided fda polynomial regression ridge regression mars 
pda generalized form ridge regression included 
users supply penalty matrix target df procedure derives appropriate penalty constant 
mda function additional arguments controlling number subclasses initialization df parameters control shrinkage 
variety functions provided making predictions varying types models class predictions posterior probability estimates canonical coordinates models specified dimensions 
functions provided producing plots misclassification confusion matrices extracting coefficients 
software publicly available statistics archive carnegie mellon university lib stat cmu edu 
send email message send fda mda statlib lib stat cmu edu login statlib ftp cd authors andreas buja richard lippmann geoff mclachlan pregibon srivastava scott helpful discussion 
second author supported natural sciences engineering research council canada 
appendix reduced rank models proposition section claimed maximizing log likelihood sigma gamma gamma sigma gamma gamma gamma log sigmaj subject equivalent reduced rank lda 
outline proof draw heavily mardia 
basic multivariate statistics results 
class covariance matrix fixed sigma denote matrix leading eigenvectors sigma gamma sigma known unknown section page solution assuming sigma known 
form usual lda solution replaced sigma 
write estimated means sigmav gamma estimated rank matrix sigmav bv sigma known sigma unknown case explicitly stated deduces easily checks equation page sigma gamma gamma obtained solving partial score equations sigma assuming known 
full maximum likelihood solution requires simultaneous solution suggests iteration 
solution easier 
plug estimated means sigma gives sigma gamma gamma gamma gamma wv bv wv bv wv spans complementary gammak dimensional subspace complete proof show optimal new metric sigma 
note 
sigmav wv 
bv sigmav dk equality definition dk diag fl fl 
established eigen matrix respect sigma show remained optimal 
note sigmav gammak bv 
gammak gammak gammak eigenvalues corresponding 
metric sigma columns orthonormal eigenvectors 
orthogonal orthogonal columns 
remain eigenvectors wrt sigma eigenvalues gammak gammak gamma gammak gammak order change 
shows constrained maximum likelihood estimated means coincide rank lda means 
fact sigma gamma 
gamma gammak gammak gammak hard show relative mahalanobis distances gamma sigma gamma gamma gamma gamma sigma gamma gamma coincide relative euclidean distances reduced lda space classification fitted constrained gaussian model lda coincide 
mda algorithm fits reduced rank discriminant model proposition reduced rank gaussian mixture model fit em algorithm step reduced rank augmented weighted lda problem 
hastie 
show optimal scoring solves lda problem 
immediate augmented weighted lda problem solved similarly augmented weighted optimal scoring procedure mda algorithm section 
example mda observations augmented problem pseudo observations 
generality justify penalized version algorithm 
augmented weighted penalized optimal scoring criterion single score jr jx jr gamma fi fi omega fi omega penalty matrix 
convenient convert matrix notation 
augmented theta indicator matrix representing observations diagonal theta weight matrix entries values jr jx 
theta margin matrix row original observation consisting augmented observations associated 
notice conditional probabilities sum observation mwm theta theta matrix score vectors augmented model matrix replicated rows 
version summed scores trace rss theta theta gamma theta gamma omega gamma minimized wrt theta subject theta wy theta solve fixed theta hastie get rss theta theta gamma ws theta wx omega gamma gamma omega gamma gamma mw mw collapsed theta smoother matrix 
written rss theta theta gamma wm mw theta theta blurred indicator matrix defined section wy diagonal theta weight matrix elements sum probabilities columns written rss theta theta gamma sz theta wish minimize theta subject theta theta mda algorithm solves 
centroid shrinking prove proposition 
weighted subclass covariance matrix 
wish maximize weight jw gamma jr jx jr gamma tr phi phi solution minimizes trd gamma gamma gamma tr phi phi theta diagonal rth element sum weights associated rth subclass 
easily shown loss generality assume mean 
mw gamma written trd gamma gamma tr phi phi fixed simple manipulations leads phi gamma gamma mv plugging back simplifying get tr mw gamma gamma tr gamma mv second term involves corresponds eigendecomposition shrunken subclass covariance matrix bq gamma version defined 
results hastie 
section tie decomposition optimal scoring 
solution corresponds generalized svd sigma left right metrics sigma sigma respectively 
corresponds optimal scoring score metric interesting duality emerges 
subtracting penalty loglikelihood shrink left eigen vectors phi leads normalization optimal scoring problem 
adding penalty optimal scoring criterion smooth right eigen vectors leads omega normalization likelihood lda problem 
simultaneously amount left right penalized canonical correlation analysis hastie 
becker chambers wilks 
new language wadsworth international group 
breiman 
nonlinear discriminant analysis scaling ace technical report univ california berkeley 
breiman friedman olshen stone 
classification regression trees wadsworth belmont california 
campbell 
canonical variate analysis general formulation australian journal statistics 
chambers hastie 
statistical models wadsworth brooks cole pacific grove california 
cheng titterington 
neural networks statistical perspectives 

comparison multivariate discriminant techniques clinical data application thyroid functional state meth 
inform 
med 

friedman 
multivariate adaptive regression splines discussion annals statistics 
hastie tibshirani 
generalized additive models chapman hall 
hastie buja tibshirani 
penalized discriminant analysis annals statistics 
press 
hastie tibshirani buja 
flexible discriminant analysis optimal scoring journal american statistical association 
hertz krogh palmer 
theory neural computation addison wesley redwood city 

canonical variate analysis high dimensional data smoothing canonical vectors technical report wa dms csiro 
kohonen 
self organization associative memory rd edition springer verlag berlin 
le cun boser denker henderson howard hubbard jackel 
handwritten digit recognition back propogation network touretzky ed advances neural information processing systems vol 
morgan kaufman denver silverman 
canonical correlation analysis data curves royal statist 
soc 
series 
mardia kent bibby 
multivariate analysis academic press 
mclachlan 
discriminant analysis statistical pattern recognition wiley new york 
pal bezdek tsao 
generalized clustering networks kohonen self organizing scheme ieee trans 
neural networks 
ripley 
classification clustering spatial image data appear proc 
von gesellschaft fur ed 
ripley 
neural networks related methods classification royal statist 
soc 
series pp 

discussion 

statistical classification linear mixture probability densities pattern recognition letters 
titterington smith makov 
statistical analysis finite mixture distributions wiley new york 

