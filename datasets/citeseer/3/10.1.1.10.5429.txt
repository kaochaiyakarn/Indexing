bogdan mean shift clustering high dimensions texture classification example ilan peter meer computer science industrial engineering management electrical computer engineering technion israel institute technology rutgers university piscataway nj usa haifa israel meer caip rutgers edu technion ac il feature space analysis main module computer vision tasks 
popular technique means clustering inherent limitations clusters constrained spherically symmetric number known priori 
nonparametric clustering methods mean shift limitations eliminated amount computation prohibitively large dimension space increases 
exploit proposed approximation technique locality sensitive hashing lsh reduce computational complexity adaptive mean shift 
implementation lsh optimal parameters data structure determined pilot learning procedure partitions data driven 
application performance mode means textons compared texture classification study 

representation visual information feature space analysis received renewed interest years motivated content image retrieval applications 
increase available computational power allows today handling feature spaces high dimensional contain millions data points 
structure high dimensional spaces dimensional geometric intuition 
spaces extremely sparse data points far away sec 
infer local structure space small number data points may available yield erroneous results 
phenomenon known statistical literature curse dimensionality effect increases exponentially dimension 
curse dimensionality avoided imposing fully parametric model data approach feasible high dimensional feature space complex structure 
goal feature space analysis reduce data significant features procedure known un der different names clustering unsupervised learning vector quantization 
different variants means clustering employed feature space represented mixture normal distributions sec 
number mixture components usually set user 
popularity means algorithm due low computational complexity number data points dimension space number iterations small relative imposes rigid delineation feature space requires reasonable guess number clusters means clustering return erroneous results embedded assumptions satisfied 
means algorithm robust points belong clusters move estimated means away densest regions 
robust clustering technique require prior knowledge number clusters constrain shape clusters mean shift clustering 
iterative technique means estimates modes multivariate distribution underlying feature space 
number clusters obtained automatically finding centers densest regions space modes 
see details :10.1.1.160.3832
original implementation mean shift clustering high dimensional spaces 
video sequence segmentation application coarse hierarchical approach introduced 
expensive operation mean shift method finding closest neighbors point space 
problem known computational geometry multidimensional range searching chap 
goal range searching algorithms represent data structure proximity relations determined time 
popular structures kd tree built operations proportionality constant increases dimension space 
query selects points rectangular region delimited interval coordinate axis query time kd trees complexity bounded number points 
high dimensions complexity query practically linear yielding computational curse dimensionality 
probabilistic algorithms proposed approximate nearest neighbor search 
algorithms yield sublinear complexity speedup depends desired accuracy 
adapted algorithm mean shift clustering high dimensions :10.1.1.121.5868
working data high dimensions required ex tend adaptive mean shift procedure introduced 
computer vision applications mean shift image segmentation object recognition tracking relatively low dimensional spaces 
implementation opens door mean shift tasks high dimensional features 
section short review adaptive mean shift technique 
locality sensitive hashing technique approximate nearest neighbor search described section introduced refinements handle data complex structure 
section performance adaptive mean shift ams high dimensions investigated section ams texture classification textons 

adaptive mean shift review results described consulted details 
assume data associated bandwidth value 
sample point point estimator spherically symmetric bounded support satisfying kernel adaptive nonparametric estimator density lo cation feature space 
function called profile kernel normalization con stant assures integrates 
function defined derivative kernel profile exists 
profile kernel defined gradient property proven positive constant called mean shift vector 
expression shows location weighted mean data points selected kernel proportional normalized density gradient estimate obtained kernel 
mean shift vector points direction maximum increase density 
implication mean shift property iterative procedure hill climbing technique nearest stationary point density point density gradient vanishes 
initial position kernel starting point procedure chosen data points 
points convergence iterative procedure modes local maxima density 
numerous methods described statistical literature define bandwidth values associated data points pilot density estimate sec 
simplest way obtain pilot density estimate nearest neighbors sec 
nearest neighbor point 
take norm suitable data structure introduced section 
choice norm major effect performance 
number neighbors chosen large assure increase density support kernels having bandwidths 
value increase dimension feature space dependence critical performance mean shift procedure seen section 
single global bandwidth value adaptive mean shift ams procedure fixed bandwidth mean shift ms discussed 
robust nonparametric clustering data achieved applying mean shift procedure representative subset data points 
convergence detected modes cluster centers shape clusters determined basins attraction 
see details :10.1.1.160.3832

locality sensitive hashing bottleneck mean shift high dimensions need fast algorithm perform neighborhood queries computing 
problem addressed vision community sorting data coordinates significant speedup achieved data close low dimensional manifold 
new algorithms tools probabilistic approximation theory suggested performing approximate nearest neighbor search high dimensions general datasets clustering data :10.1.1.38.249
approximate nearest neighbor algorithm locality sensitive hashing lsh adapted handle complex data met computer vision applications :10.1.1.121.5868
task estimating pose articulated objects described proceedings lsh technique extended accommodate distances parameter space 

high dimensional neighborhood queries points mean shift iterations require neighborhood query current location naive method scan dataset test kernel point covers 
mean computation complexity assuming point dataset operation performed times value depends distribution data complexity mean shift algorithm improve efficiency neighborhood queries data structure constructed 
data tessellated times random partitions defined inequalities 
partition pairs random numbers 
integer chosen followed value range data th coordinate 
pair partitions data inequality selected coordinate data point 
point partition yields dimensional boolean vector inequality true false 
points vector lie cell parti tion 
hash function points belonging cell placed bucket hash table 
partitions point belongs simultaneously cells hash table buckets 
find neighborhood radius query point boolean vectors computed 
index cells vectors hash table 
points union ones returned query 
note intersection mines resolution data structure determines set points returned query 
described technique called locality sensitive hashing lsh introduced 
points close higher probability collision hash table 
lies close center query return nearest neighbors example illustrates approximate nature query 
parts neighborhood centered covered different shape 
approximation errors reduced building data structures increase running time larger query 
return result 
deter locality sensitive hashing data structure 
query point overlap cells yields region approximates desired neighborhood 

optimal selection values determine expected volumes average number inequalities coordinate partitioning data regions 
average number points cell union note estimating disregard points belong qualitatively larger value number cuts partition smaller average volume similarly number partitions increases cells volume decreases increases 
values certain bound interest 
exceeds bound neighborhood radius covered larger values increase query time improvement quality results 
optimal values derived data 
subset points data selected random sampling 
data points distance nearest neighbor determined accurately traditional linear algorithm 
approximate nearest neighbor algorithm lsh pair define distance nearest neighbor returned query 
query return neighbors correct nearest 
total running time queries optimal chosen points subject lsh approximation threshold set user 
optimization performed numerical search procedure 
compute function approximation error queries 
shown fig ure thirteen dimensional real data set 
thresholding family graphs function obtained 
running time expressed dimensional function number employed cuts 
minimum optimal pa rameters lsh data structure 
error optimal determining dependence approximation error 
curves thresholded dashed line 
dependence running time minimum marked 
family error curves efficiently generated 
number partitions bounded available computer memory 
bound 
similarly set maximum number cuts lsh data structure built approximation error computed incrementally adding partition time 
yields subsequently 
data driven partitions strategy generating random tessellations important influence performance locality sensitive hashing 
coordinates equal chance selected values uniformly distributed range corresponding coordinate :10.1.1.121.5868
partitioning strategy works density data approximately uniform entire space 
feature spaces associated vision applications multimodal 
problem nonuniformly distributed data dealt building data structures different values accommodate different local densities :10.1.1.38.249
query performed assumption high density fails repeated lower densities 
process terminates nearest neighbors 
approach sample marginal distributions coordinate 
points chosen random data set 
point coordinates selected random define cut 
coordinate point imply sampling partial joint densities advantageous 
adaptive data driven strategy assures denser regions cuts yielding smaller cells sparser regions cuts 
average cells contain similar number points 
dimensional data comprised clusters uniformly distributed background demonstrate sampling strategies 
cases number cuts data driven method places cuts clusters 
quantitative performance assessment data set normal distributions arbitrary shapes points defined dimensions 
data driven strategy distribution number points cell compact average value lower 
consequence data driven strategy yields efficient nearest neighbor queries complex data sets 

mean shift high dimensions current location iterations lsh query retrieves approximate set neighbors needed compute location 
resolution data analysis controlled user 
fixed bandwidth ms method user provides bandwidth parameter ams method user sets number number cells uniform distribution data driven distribution number neighbors uniform vs data driven partitions 
typical result dimensional data obtained uniform data driven strategy 
distribution points cell dimensional data set 
neighbors pilot density procedure 
parameters lsh data structure selected employing technique discussed section 
bandwidths associated data points obtained performing neighborhood queries 
bandwidths set adaptive mean shift procedure runs approximately cost fixed bandwidth mean shift 
difference ms ams additional query point 
ad hoc procedure provides speedup 
resolution data structure high probability assume points converge mode 
point associated mode subsequent queries automatically return mode mean shift iterations 
modes stored separate hash table keys boolean vectors associated 
adaptive vs fixed bandwidth mean shift illustrate advantage adaptive mean shift data set containing points dimensional cube generated 
points belonged spherical normal distributions clusters means positioned line origin 
standard deviation increases mean distant origin 
adjacent pair clusters ratio sum standard deviations distance means kept constant 
remaining points uniformly distributed dimensional cube 
plotting distances data points origin yields graph similar 
note points farther origin larger spread 
performance fixed bandwidth ms adaptive mean shift ams procedures compared various parameter values 
experiments performed points chosen random cluster total points 
location associated selected point mean shift procedure employed performance measure 
ideally location near center cluster point belongs 
ms strategy bandwidth small due sparseness high dimensional space points neighbors distance 
mean shift procedure start allocation points 
hand increases windows large local structures points may converge incorrectly center mode adjacent cluster figures 
pilot density estimation ams strategy automatically adapts bandwidth local structure 
parameter number neighbors pilot estimation strong influence 
data processed correctly points figures points cluster largest spread converge adjacent mode 
superiority adaptive mean shift high dimensions clearly visible 
due sparseness dimensional space points background interfere mean shift processes strategy proving robustness 
lsh data structure mean shift procedure assures significant speedup 
derived different features spaces texture image filter banks discussed section 
spaces dimen sion contained points 
ams procedure run linear approximate queries points 
number neighbors pilot density estimation 
approximation error lsh running times seconds table show achieved speedups 
table running times ams implementations traditional lsh speedup speedup increase number data points decrease number neighbors mean shift procedure speedup high applications small number neighbors required 
distance distance points distance points distance points distance points points distance points distance points distance points distance origin points dimensional clusters fixed bandwidth mean shift ms adaptive mean shift ams 
parameters ms bandwidth ams number neighbors 
texture classification efficient methods exist texture classification varying illumination viewing direction :10.1.1.143.7909
state art approaches texture characterized textons cluster centers feature space derived input 
feature space built output filter bank applied pixel 
shown neighborhood information spatial domain may suffice 
approaches differ employed filter bank 
lm combination anisotropic isotropic filters leung malik dana 
filters gaussian masks derivative laplacian defined scales 
oriented filters representation sensitive texture rotations 
feature space dimensional 
set circular symmetric filters schmid obtain rotationally invariant feature set 
feature space dimensional 
representations proposed varma 
rotationally symmetric oriented filters 
second set extension different scales 
feature vector computed retaining maximum response oriented filters reducing dependence global texture orientation 
feature space respectively dimensional 
find textons usually standard means clustering algorithm discussed sec tion limitations 
shape clusters restricted spherical number set prior processing 
significant textons aggregated texton library 
serves dictionary representative local structural features general characterize large variety texture classes 
texture modeled texton histogram 
histogram computed defining pixel feature vector replacing closest texton library vector quantization accumulating results entire image 
textures characterized histograms built textons 
distance texton distributions measure similarity note absence factor take account comparison histograms derived data 
texture classification task training image smallest distance test image determines class 
experiments substituted means clustering module adaptive mean shift ams robust nonparametric clustering 
textons mean mode number significant ones determined automatically 
complete brodatz database containing textures varying degrees complexity experiments 
classification brodatz database challenging contains nonhomogeneous textures 
images divided subimages half subimages training models half testing queries 
normalizations recommended image filter domains performed 
number significant textons detected ams procedure depends texture 
limited number mode textons extracted texture class 
number mean textons 
adding textons library texton histogram bins 
table classification results brodatz database filter lm rnd means ams classification results different filter banks table 
best result obtained lm mode textons additional correct classifications errors mean textons 
clear advantage mode textons filter banks 
classification performance close upper bound defined texture inhomogeneity due test training images class different 
observation supported performance degradation obtained database images divided subimages half half partition sixteen yielded models queries 
recognition rate decreased filter banks 
best result obtained lm filters mean mode textons 
setup employing different texture representation textures brodatz database recognition rate texture class characterized histogram textons approximation feature space distribution 
histogram constructed voronoi diagram cells 
vertices diagram textons histogram bin contains number feature points cell 
variations textons translate approximating distribution different diagram appears weak influence classification performance 
uniform sampling random vectors chosen textons classification performance rnd decreased 
means clustering imposes rigidly number identical spherical clusters feature space 
expected structure adequate mode textons provide meaningful decomposition texture image 
proven follow percentage texton 
texture classified pixels mode mean mode vs mean textons 
local structure better captured mode textons 
texture lm filter bank 
ing examples 
lm filter bank applied regular texture 
ams procedure extracted textons number means clustering 
ordered size mode textons associated pixels image mean textons account similar number pixels 
difference mode mean textons seen marking pixels associated textons local structure bottom 
advantage mode representation evident nonregular texture cumulative distribution mode textons classified pixels sharper increase 
textons capture local spatial configurations believe combining mode textons representation proposed offer insight texton approach superior previous techniques 
percentage texton 
texture classified pixels mode mean mode vs mean textons 
local structure better captured mode textons 
texture filter bank 

introduced computationally efficient method possible high dimensional spaces detection modes distributions 
employing data structure locality sensitive hashing significant decrease running time obtained maintaining quality results 
new implementation mean shift procedure opens door development vision algorithms exploiting feature space analysis including learning techniques high dimensions 
source code mean shift downloaded www caip rutgers edu acknowledgments bogdan sarnoff princeton nj calling attention lsh data structure 
done sabbatical rutgers university 
support national science foundation iri gratefully acknowledged 
comaniciu meer 
mean shift robust approach feature space analysis 
ieee trans 
pattern anal 
machine intell 
comaniciu ramesh meer 
variable bandwidth mean shift data driven scale selection 
proc 
th intl 
conf 
computer vision vancouver canada volume pages july 
dana 
compact representation bidirectional texture functions 
proc 
ieee conf 
computer vision pattern recognition kauai hawaii volume pages 
de berg van kreveld overmars 
computational geometry 
algorithms applications 
springer second edition 
dementhon 
spatio temporal segmentation video hierarchical mean shift analysis 
proc 
statistical methods video processing workshop copenhagen denmark 
car tr center automat 
res md college park 
duda hart stork 
pattern classification 
wiley second edition 
gionis indyk motwani :10.1.1.121.5868
similarity search high dimensions hashing 
proc 
int 
conf 
large data bases pages 
manjunath 
rotation invariant texture classification complete space frequency model 
ieee trans 
image process 
indyk 
sublinear time approximation scheme clustering metric spaces 
proc 
ieee symp 
foundations computer science pages 
indyk motwani 
approximate nearest neighbors removing curse dimensionality 
proc 
symp 
theory computing pages 
kushilevitz ostrovsky rabani 
efficient search approximate nearest neighbor high dimensional spaces 
proc 
symp 
theory computing pages 
leung malik 
representing recognizing visual appearance materials dimensional textons 
intl 
computer vision 
nene nayar 
simple algorithm nearestneighbor search high dimensions 
ieee trans 
pattern anal 
machine intell 
ostrovsky rabani 
polynomial time approximation schemes geometric clustering 
proc 
ieee symp 
foundations computer science pages 
schmid 
constructing models content image retrieval 
proc 
ieee conf 
computer vision pattern recognition kauai hawaii volume pages 
shakhnarovich viola darrell 
fast pose estimation parameter sensitive hashing 
proc 
th intl 
conf 
computer vision nice france 
silverman 
density estimation statistics data analysis 
chapman hall 
varma zisserman 
classifying images materials 
proc 
european conf 
computer vision copenhagen denmark volume iii pages 
varma zisserman 
texture classification filter banks necessary 
proc 
ieee conf 
computer vision pattern recognition madison wisconsin volume ii pages 
