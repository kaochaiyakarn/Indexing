global versus local methods nonlinear dimensionality reduction vin de silva department mathematics stanford university stanford 
ca silva math stanford edu joshua tenenbaum department brain cognitive sciences massachusetts institute technology cambridge 
ma ai mit edu proposed algorithms nonlinear dimensionality reduction fall broadly categories different advantages disadvantages global isomap local locally linear embedding laplacian eigenmaps :10.1.1.111.3313
variants isomap combine advantages global approach previously exclusive advantages local methods computational sparsity ability invert conformal maps 
discuss problem nonlinear dimensionality reduction nldr task recovering meaningful low dimensional structures hidden high dimensional data 
example set pixel images individual face observed different pose lighting conditions task identify underlying variables pose angles direction light high dimensional pixel image data 
cases interest observed data lie embedded submanifold high dimensional space 
degrees freedom submanifold correspond underlying variables 
form nldr problem known manifold learning 
classical techniques manifold learning principal components analysis pca multidimensional scaling mds designed operate submanifold embedded linearly linearly observation space 
generally wider class techniques involving iterative optimization procedures unsatisfactory linear representations obtained pca mds may improved successful nonlinear representations data 
techniques include gtm self organising maps :10.1.1.137.495
algorithms fail nonlinear structure simply regarded perturbation linear approximation swiss roll 
cases iterative approaches tend get stuck locally optimal solutions may grossly true geometry situation 
entirely new approaches devised address problem 
methods combine advantages pca mds computational efficiency free parameters non iterative global optimisation natural cost function ability recover intrinsic geometric structure broad class nonlinear data manifolds 
algorithms come flavors local global 
local approaches lle laplacian eigenmaps attempt preserve local geometry data essentially seek map nearby points manifold nearby points low dimensional representation :10.1.1.111.3313
global approaches isomap attempt preserve geometry scales mapping nearby points manifold nearby points low dimensional space faraway points faraway points 
principal advantages global approach tends give faithful representation data global structure metric preserving properties better understood theoretically 
local approaches principal advantages computational efficiency involve sparse matrix computations may yield polynomial speedup representational capacity may give useful results broader range manifolds local geometry close euclidean global geometry may 
show global geometric approach implemented isomap extended directions 
results computational efficiency representational capacity equal excess existing local approaches lle laplacian eigenmaps greater stability theoretical tractability global approach 
conformal isomap isomap extension isomap capable learning structure certain curved manifolds 
extension comes cost making uniform sampling assumption data 
landmark isomap isomap technique approximating large global computation isomap smaller set calculations 
focuses small subset data called landmark points 
remainder sections 
section describe perspective manifold learning isomap appears natural generalisation isomap 
section derive isomap landmark version classical mds 
isomap conformal embeddings manifold learning geometric invariants view problem manifold learning attempt invert generative model set observations 
dimensional domain contained euclidean space smooth embedding object manifold learning recover set observed data observed data arise follows 
hidden data generated randomly mapped observed data problem stated ill posed restriction needed relate 
observed geometry data structure hidden variables discuss possibilities 
isometric embedding sense riemannian geometry preserves lengths angles 
second possi bility conformal embedding preserves angles lengths 
equivalently point scalar infinitesimal vectors get magnified length factor class conformal embeddings includes isometric embeddings families maps including projections mercator projection 
approach solving manifold learning problem identify aspects geometry invariant mapping example isometric embedding definition infinitesimal distances preserved 
true 
length path defined integrating infinitesimal distance metric path 
true preserves path lengths 
points shortest path lying inside length shortest path geodesic distances preserved 
isometric regarded metric spaces geodesic distance 
isomap exploits idea constructing geodesic metric approximately matrix observed data 
solve conformal embedding problem need identify observable geometric invariant conformal maps 
conformal maps locally isometric scale factor natural try estimate point observed data 
rescaling restore original metric structure data proceed isomap 
noting conformal map local volumes factor hidden data sampled uniformly local density observed data follows conformal factor estimated terms observed local data density provided original sampling uniform 
isomap implements version idea independent dimension uniform sampling assumption may appear severe restriction believe reflects necessary tradeoff dealing larger class maps 
illustrate algorithm appears practice robust moderate violations assumption 
isomap isomap algorithms stages isomap 
determine neighbourhood graph observed data suitable way 
example contain iff nearest neighbours vice versa 
alternatively contain edge iff 
compute shortest paths graph pairs data points 
edge graph weighted euclidean length useful metric 

apply mds resulting shortest path distance matrix find new embedding data euclidean space approximating premise local metric information case lengths edges neighbourhood graph regarded trustworthy guide local metric structure original latent space 
shortest paths computation gives estimate global metric structure fed mds produce required embedding 
known step converges true geodesic structure manifold sufficient data isomap yields faithful low dimensional euclidean embedding function isometry 
precisely see theorem 
sampled bounded convex region respect density function smooth isometric embedding region suitable choice neighbourhood size parameter recovered distance original distance probability provided sample size sufficiently large 
formula taken hold pairs points simultaneously 
isomap simple variation isomap 
specifically neighbours method step replace step 
compute shortest paths graph pairs data points 
edge graph weighted mean distance nearest neighbours 
similar arguments prove convergence theorem isomap 
exact formula weights critical asymptotic analysis 
point rescaling factor asymptotically accurate approximation conformal scaling factor neighbourhood theorem 
sampled uniformly bounded convex region smooth conformal embedding region suitable choice neighbourhood size parameter original distance recovered distance probability provided sample size sufficiently large 
possible unpleasant find explicit lower bounds sample size 
qualitatively expect require larger sample size isomap depends approximations local data density geodesic distance 
special case conformal embedding isometry preferable isomap isomap 
borne practice 
examples ran isomap isomap mds lle examples different data distributions realistic simulated data set 
refer 
datasets differ probability density generate points 
conformal column points generated randomly uniformly circular disk projected conformally mapped sphere 
note high concentration points near rim 
metrically faithful way embedding curved inside euclidean plane classical mds isomap succeed 
predicted isomap recover original disk structure lle 
contrast uniform column data points sampled uniform measure 
situation isomap behaves isomap rescaling factor approximately constant unable find topologically faithful dimensional representation 
offset column perturbed version conformal points sampled shallow gaussian offset center projected sphere 
theoretical conditions perfect recovery met isomap robust find topologically correct embedding 
lle contrast produces topological errors metric distortion cases data uniformly sampled columns 
face images artificial images face rendered pixel images rasterized dimensional vectors 
images varied randomly independently parameters left right pose angle distance camera natural family conformal transformations data manifold ignore perspective distortions closest images effect shrinking magnifying apparent size images constant factor 
sampling uniformly gives data set approximately satisfying required conditions isomap 
generated face images way spanning range indicated 
algorithms returned dimensional embedding data 
expected isomap returns cleanest embedding separating degrees freedom reliably horizontal vertical axes 
isomap returns embedding narrows predictably face gets away 
lle embedding highly distorted 
conformal mds isomap isomap lle uniform mds isomap isomap lle offset mds isomap isomap lle face images mds isomap isomap lle dimensionality reduction algorithms mds isomap isomap lle applied versions toy dataset complex data manifold face images 
set face images randomly generated varying independently parameters distance left right pose 
extreme cases shown 
isomap landmark points isomap algorithm computational bottlenecks 
calculating shortest paths distance matrix floyd algorithm improved implementing dijkstra algorithm fibonacci heaps neighbourhood size 
second bottleneck mds eigenvalue calculation involves full matrix complexity contrast eigenvalue computations lle laplacian eigenmaps sparse considerably cheaper 
isomap addresses inefficiencies 
designate data points landmark points computing compute matrix distances data point landmark points 
new procedure lmds landmark mds find euclidean embedding data leads enormous savings computed dijkstra time lmds runs lmds feasible precisely expect data low dimensional embedding 
step apply classical mds landmark points embedding faithfully remaining point located known dis landmark points constraints 
analogous global positioning system technique finite number distance readings identify geographic location 
landmarks general position constraints locate uniquely 
landmark points may chosen randomly taken sufficiently larger minimum ensure stability 
landmark mds algorithm lmds begins applying classical mds landmarks distance matrix recall procedure 
step construct inner product matrix defined formula find eigenvalues eigenvectors write positive eigenvalues labelled corresponding eigenvectors written column vectors non positive eigenvalues ignored 
required optimal dimensional embedding vectors columns matrix matrix squared distances centering matrix embedded data automatically mean centered principal components aligned axes significant 
negative eigenvalues dimensional embedding perfect exact euclidean embedding 
second stage lmds embed remaining points denote column vector squared distances data point landmark points 
embedding vector related linearly formula column mean pseudoinverse transpose original points isomap landmarks isomap landmarks isomap landmarks isomap landmarks swiss roll embedding lle lle lle lle isomap stable wide range values sparseness parameter number landmarks 
results lle shown comparision 
final optional stage pca data coordinate axes 
full discussion lmds appear 
note results 
landmark point embedding lmds consistent original mds embedding 

distance matrix represented exactly euclidean configuration landmarks chosen affine span configuration dimensional general position lmds recover configuration exactly rotation translation 
way satisfy affine span condition pick landmarks randomly plus extra stability 
important isomap distances inherently slightly noisy 
robustness lmds noise depends matrix norm small landmarks lie close hyperplane lmds performs poorly noisy data 
practice choosing extra landmark points gives satisfactory results 
example shows results testing isomap swiss roll data set 
points generated uniformly rectangle top left mapped swiss roll configuration ordinary isomap recovers rectangular structure correctly provided neighbourhood parameter large case works 
tests show significantly degraded isomap 
chose landmark points random landmarks embedding closely approximates non landmark isomap embedding 
configuration landmarks chosen especially illustrate affine distortion may arise landmarks lie close subspace case line 
landmarks chosen random results generally better 
contrast lle unstable changes sparseness parameter neighbourhood size 
fair principally topological parameter incidentally sparseness parameter lle 
isomap roles separately fulfilled local approaches nonlinear dimensionality reduction lle laplacian eigenmaps principal advantages global approach isomap tolerate certain amount curvature lead naturally sparse eigenvalue problem 
curvature tolerance computational sparsity explicitly part formulation local approaches features emerge goal trying preserve data local geometric structure 
explicit goals convenient fact reliable features local approach 
conformal invariance lle fail surprising ways computational sparsity tunable independently topological sparsity manifold 
contrast extensions isomap explicitly designed remove characterized form curvature exploit computational sparsity intrinsic low dimensional manifolds 
extensions amenable algorithmic analysis provable conditions return accurate results tested successfully challenging data sets 
acknowledgments supported part nsf dms schlumberger merl darpa human id program 
authors wish thomas vetter providing range texture maps synthetic face schmidt help rendering actual images curious labs poser software 
tenenbaum de silva langford global geometric framework nonlinear dimensionality reduction 
science 
roweis saul :10.1.1.111.3313
nonlinear dimensionality reduction locally linear embedding 
science 
belkin niyogi 
laplacian eigenmaps spectral techniques embedding clustering 
dietterich becker ghahramani eds advances neural information processing systems 
mit press 
bishop williams 
gtm generative topographic mapping 
neural computation 
kohonen 
self organisation associative memory 
springer verlag berlin 
bregler omohundro 
nonlinear image interpolation manifold learning 
tesauro touretzky leen eds advances neural information processing systems 
mit press 
demers cottrell 
non linear dimensionality reduction hanson cowan giles eds advances neural information processing systems 
morgan kaufmann 
bernstein de silva langford tenenbaum 
december graph approximations geodesics embedded manifolds 
preprint may downloaded url isomap stanford edu pdf torgerson 
theory methods scaling 
wiley new york 
cox cox 
multidimensional scaling 
chapman hall london 
de silva tenenbaum 
preparation sparse multidimensional scaling landmark points 
