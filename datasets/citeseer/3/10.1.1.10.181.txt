incremental induction decision trees paul utgoff utgoff cs umass edu department computer information science university massachusetts amherst ma article presents incremental algorithm inducing decision trees equivalent formed quinlan nonincremental id algorithm training instances 
new algorithm named id lets apply id induction process learning tasks training instances serially 
basic tree building algorithms differ decision trees constructed experiments show incremental training possible select training instances carefully result smaller decision trees 
id algorithm variants compared terms theoretical complexity empirical behavior 
keywords decision tree concept learning incremental learning learning examples 
ability learn classifications fundamental intelligent behavior 
concepts dangerous edible ally profitable learned routinely essential getting world 
decades researchers developed number algorithms automated concept acquisition 
include meta dendral buchanan mitchell learned concept chemical bond break mass spectrometer aq michalski learned diagnose soybean disease observable symptoms 
dimension concept learning algorithms distinguished operate incremental nonincremental mode 
case algorithm infers concept entire set available training instances 
nonincremental algorithms include vere michalski induce quinlan id 
incremental case algorithm revises current concept definition necessary response newly observed training instance 
incremental algorithms include mitchell candidate elimination algorithm schlimmer granger stagger fisher cobweb gallant pocket algorithm 
nonincremental algorithm appropriate learning tasks single fixed set training instances provided incremental algorithm appropriate learning tasks stream training instances 
serial learning tasks prefer incremental algorithm assumption efficient revise existing hypothesis generate hypothesis time new instance observed 
cost generating hypotheses scratch may expensive apply nonincremental method serial learning task 
article presents algorithm incremental induction decision trees 
method infers decision tree quinlan nonincremental id algorithm correct citation article copyright kluwer academic publishers utgoff 

incremental induction decision trees 
machine learning 
incremental induction decision trees table 
basic id tree construction algorithm 

instances exactly class decision tree answer node containing class name 

define best attribute lowest score 
value best best grow branch best decision tree constructed recursively instances value best attribute best set training instances 
practical build decision trees tasks require incremental learning 
algorithm practical choose training instances selectively lead smaller decision tree 
section reviews decision trees id algorithm 
section explains need incremental algorithm reviews schlimmer fisher id algorithm presents new incremental algorithm called id 
section provides theoretical analysis worst case complexity decision tree algorithms section compares algorithms terms empirical behavior 
sections summarize main discuss directions 
decision trees decision tree representation decision procedure determining class instance 
node tree specifies class name specific test partitions space instances node possible outcomes test 
subset partition corresponds classification subproblem subspace instances solved subtree 
decision tree seen divide conquer strategy object classification 
formally define decision tree 
leaf node answer node contains class name 
non leaf node decision node contains attribute test branch decision tree possible value attribute 
comprehensive discussion decision trees see moret breiman friedman olshen stone 
quinlan id program induces decision trees form 
training instance described list attribute value pairs constitutes conjunctive description instance 
instance labelled name class belongs 
simplify discussion assumed instance belongs classes positive instances examples concept learned target concept negative instances counterexamples target concept 
algorithm applied classes straightforward manner 
addition assumed attributes discrete values 
incremental induction decision trees complete id algorithm discussed section includes method selecting set training instances decision tree built 
discussion focuses basic problem constructing decision tree set selected training instances 
table shows just tree construction algorithm called basic id tree construction algorithm embedded complete id algorithm 
set attributes describe instance denoted individual attributes indicated number attributes denoted jaj 
attribute set possible values attribute denoted individual values indicated number values attribute denoted jv score result computing quinlan expected information function attribute node 
node number positive instances number negative instances number positive instances value attribute number negative instances value attribute jv log log quinlan pointed selecting attribute lowest score equivalent selecting attribute highest information gain defined 
function information theoretic metric entropy shannon lewis 
function estimates amount ambiguity classifying instances result placing attribute test decision node 
attribute lowest score assumed give partition instances subproblems classification 
algorithm guaranteed find smallest tree experience shown usually builds small decision tree generalizes unobserved instances 
quinlan reported improved attribute selection metric called gain ratio criterion compensates fact attributes values tend preferred attributes fewer attributes 
improved metric discussed section 
incremental induction decision trees id useful concept learning algorithm efficiently construct decision tree generalizes 
nonincremental learning tasks algorithm choice building classification rule 
incremental learning tasks far preferable accept instances incrementally needing build new decision tree time 
section reviews schlimmer fisher id algorithm designed learn decision trees incrementally presents new incremental tree construction algorithm named id see watanabe historical perspective applying notion physical entropy information processes 
incremental induction decision trees table 
basic id tree update procedure 

possible test attribute current node update count positive negative instances value attribute training instance 

instances observed current node positive negative decision tree current node answer node containing indicate positive negative instance 

current node answer node change decision node containing attribute test lowest 
current decision node contains attribute test lowest score change attribute test lowest score 
ii 
discard existing subtrees decision node 
recursively update decision tree current decision node branch value current test attribute occurs instance description 
grow branch necessary 
builds decision tree id basic tree building algorithm set training instances 
id algorithm previously schlimmer fisher designed algorithm named id address incremental construction decision trees 
shown concepts learnable id learnable id 
need incremental tree induction algorithm inadequacy id algorithm regard motivated research reported 
id algorithm detail reasons 
algorithm needs understood inability learn certain concepts explained 
second mechanism id algorithm determining change attribute test node included new algorithm id described section 
shown table id algorithm accepts training instance updates decision tree kept global data structure 
step indicates information needed compute score possible test attribute node kept node 
information consists positive negative counts possible value possible test attribute node current decision tree 
ease discussion non test attribute attribute test attribute node 
information kept node possible determine attribute lowest score 
current test attribute incremental induction decision trees lowest score test attribute replaced non test attribute lowest score 
counts maintained need reexamine training instances 
complete id algorithm includes test independence described quinlan 
test helps prevent overfitting training data noise preventing replacement answer node decision node 
include test step algorithm executed attribute independent 
article discussion focuses basic algorithms omitting test 
id algorithm builds tree basic id tree construction algorithm attribute decision node clearly best choice terms score 
relative ordering possible test attributes node changes training step ii discards subtrees node 
relative ordering stabilize training subtrees discarded repeatedly rendering certain concepts algorithm 
thrashing observed concepts decision tree extend levels current node stable best test attribute node 
example consider concept parity boolean variables level tree needed attributes equally root small fluctuations scores training cause test attribute root change repeatedly 
instances repeatedly fixed order simple fair sampling strategy tree stabilize root rendering concept 
instances sampled random order hope sequence permits consistent tree lost 
note includes test concept attributes root appear noisy 
new algorithm id section presents new incremental algorithm called id guaranteed build decision tree id set training instances 
effectively lets apply quinlan nonincremental method incremental learning tasks expense building new decision tree new training instance 
id successor id algorithm utgoff described section 
id id algorithm maintains sufficient information compute score attribute node making possible change test attribute node lowest score 
id differs method changing test attribute node 
discarding subtrees old test attribute id restructures tree desired test attribute root 
restructuring process called pull tree manipulation preserves consistency observed training instances brings indicated attribute root node tree subtree 
advantage restructuring tree lets recalculate various positive negative counts tree manipulations reexamining training instances 
formally define id decision tree 
leaf node answer node contains class name set instance descriptions node belonging class 
non leaf node decision node contains incremental induction decision trees table 
id tree update algorithm 

tree empty define unexpanded form setting class name class instance set instances singleton set containing instance 

tree unexpanded form instance class add instance set instances kept node 

tree unexpanded form expand level choosing test attribute root arbitrarily 
test attribute attributes current node update count positive negative instances value attribute training instance 
current node contains attribute test lowest score restructure tree attribute lowest score root 
ii 
recursively reestablish best test attribute subtree updated step 
recursively update decision tree current decision node branch value test attribute occurs instance description 
grow branch necessary 
attribute test branch decision tree possible value attribute positive negative counts possible value attribute set non test attributes node positive negative counts possible value attribute 
form decision tree differs id id retains training instances tree 
accordingly tree interpreted differently 
tree classify instance tree traversed node reached training instances class point classification determined answer returned 
node leaf non leaf node 
case class non zero instance count 
method interpreting tree structure allows intended generalization discarding information training instances retained restructuring remains possible 
call leaf node form tree unexpanded tree nonleaf node form expanded tree 
note method interpreting tree means tree structure incremental induction decision trees table 
id pull algorithm 

attribute new pulled root 

recursively pull attribute new root immediate subtree 
convert unexpanded tree expanded form necessary choosing attribute new test attribute 
transpose tree resulting new tree new root old root attribute old root immediate subtree 
underlying tree interpretation may unique 
actual form tree unexpanded form expanded form superfluous attribute tests 
instances node described attribute value pairs determined tests node 
means instance descriptions reduced attribute value pair attribute tested tree 
imagine simplest approach maintain tree structure fully expanded form 
inefficient time space non leaf node counts non test attributes maintained 
table shows id algorithm updating decision tree 
tree unexpanded form instance class instance added set kept node 
tree expanded level necessary choosing test attribute arbitrarily counts positives negatives exist node attribute value counts updated test non test attributes training instance 
test attribute longer lowest score tree restructured test attribute lowest score 
tree restructured recursively checks subtrees touched tree update restructuring necessary test attribute node lowest score node 
tree update continues recursively lower level branch value appears instance description 
id algorithm builds tree basic id tree construction algorithm instances method breaking ties equally attributes 
table shows algorithm restructuring decision tree desired test attribute root 
immediate subtree node subtree rooted child node 
transpose tree root level zero order attribute tests level zero level reversed resulting regrouping level subtrees 
level zero attribute value counts known tree update led pull level trees touched counts level test non test attributes needs computed 
obtained directly adding counts level subtrees 
tree update procedure continues recursively level 
tree transposition step equivalent operation described cockett 
incremental induction decision trees table 
training instances quinlan example 
class height hair eyes short blond brown tall dark brown tall blond blue tall dark blue short dark blue tall red blue tall blond brown short blond blue illustration id algorithm section illustrates id algorithm showing behavior serially training instances quinlan 
instances shown table described conjunction attribute value pairs attributes height hair color color eyes 
trees drawn attribute value names clipped characters 
positive negative counts shown bracketed ordered pair positives negatives 
instance height short hair blond eyes brown tree updated simply leaf node eyes brown hair blond height short second instance height tall hair dark eyes brown negative added list instances leaf node giving eyes brown hair dark height tall eyes brown hair blond height short third instance height tall hair blond eyes blue positive tree expanded arbitrarily choosing attribute height test attribute 
positive negative counts updated level zero non test attribute eyes lowest score 
moment tree short eyes brown hair blond tall eyes brown hair dark height attribute eyes pulled root 
note tree process updated level zero training instances level training instances 
update process finished entire tree training instances 
step restructure immediate subtrees eyes incremental induction decision trees test attribute 
current tree subtrees unexpanded form expanded choosing attribute eyes test attribute 
point tree short brown hair blond eyes tall brown hair dark eyes height transposed blue brown short hair blond tall hair dark height eyes branch value blue attribute eyes comes possible level zero instance counts updated 
eyes test attribute possible branches known 
algorithm needs ensure best test attribute root immediate subtree tree blue updated rest training instance processed 
current test attribute height root tree brown due transposition due having lowest score 
note tree updated level zero updated level subtree eyes blue currently empty 
subtree created level updated 
happens tree restructuring needed 
rest instance processed recursively level resulting blue hair blond height tall brown short hair blond tall hair dark height eyes note right subtree height root contracted unexpanded form instances node class 
id algorithm include contraction step known contractions worthwhile 
unexpanded tree requires space cheaper update 
expense expanding tree instances examined positive negative counts computed 
experience algorithm shown contraction unexpanded form generally worthwhile 
fourth instance height tall hair dark eyes blue leads expansion left incremental induction decision trees level subtree selection hair test attribute giving blue blond tall height dark height tall hair brown short hair blond tall hair dark height eyes fifth instance height short hair dark eyes blue causes test attribute root changed hair 
immediately transposition step tree blond blue tall height brown short height eyes dark blue height tall brown tall height eyes hair happens tree restructuring needed 
rest instance processed resulting tree blond blue tall height brown short height eyes dark blue height short height tall brown tall height eyes hair sixth instance height tall hair red eyes blue require changing test attribute root 
red new value hair new value branch grown giving incremental induction decision trees blond blue tall height brown short height eyes dark blue height short height tall brown tall height eyes red eyes blue height tall hair seventh eighth instances update various counts cause tree revision 
eighth instance tree blond blue short tall height brown short tall height eyes dark blue height short height tall brown tall height eyes red eyes blue height tall hair tree contracted converting unexpanded form possible result blond blue height short height tall brown height short height tall eyes dark eyes blue height short eyes blue height tall eyes brown height tall red eyes blue height tall hair algorithm perform step 
tree shown compact form facilitate interpretation 
final tree equivalent basic id tree construction algorithm build training instances 
incremental induction decision trees theoretical analysis decision tree algorithms alternative methods constructing decision tree natural ask best 
section presents theoretical worst case analysis algorithms build decision tree set training instances 
algorithm basic tree construction algorithm nonincremental id 
second algorithm variant suggested schlimmer fisher called id 
algorithm accepts training instances time time building new tree accumulated set training instances basic id tree construction algorithm 
obvious brute force method inferring decision tree instances serially 
third algorithm incremental id 
fourth algorithm incremental id predecessor id algorithm 
analysis terms worst case cost build tree set instances similar schlimmer fisher 
analysis measured terms total number attribute comparisons building tree 
analysis metrics 
measures cost maintaining positive negative instance counts terms number additions performed 
addition needed compute count called instance count addition 
metric similar schlimmer fisher number attribute comparisons includes cost adding counts tree transposition process 
second measures cost attribute selection terms number score computations 
activity costly basic id tree construction algorithm considered id id algorithms compute score attribute decision node tree 
worst case analysis lets ignore domain characteristics ability attributes discriminate class 
ideal representation single attribute determines class membership leading shallow decision tree ideal representation require deeper tree 
general larger trees expensive construct smaller trees suitability representation affects size decision tree expense building tree 
analysis assumes possible training instances observed 
pessimistic worst case assumptions possible draw useful analyses 
simplify notation analyses number attributes jaj maximum number possible values attribute number training instances 
analysis basic id algorithm basic tree construction algorithm id checks instances node class 
tree just answer node containing class name 
algorithm counts number positive negative instances value attribute 
computes score attribute selects attribute lowest score test attribute node builds tree recursively subset instances value selected test attribute 
possible attribute node need compute score 
choosing test attribute root algorithm count attribute values instances 
root level zero 
instance count additions score calculations 
id algorithm omitted concept may id making worst case cost building correct tree undefined 
incremental induction decision trees level subtrees worst case 
level zero test partitions instances set subproblems subtrees built instances examined level worst case 
level instance described attributes 
instance count additions 
calculations 
general worst case cost building tree terms instance count additions 



terms score calculations worst case cost building tree 
analysis shows complexity worst case cost building tree basic id tree construction algorithm linear number instances instance count additions constant score calculations 
quinlan personal communication pointed tree instances contain decision nodes score calculations node 
score calculations 
usually case 
expect better worst case 
example problem worst case performance observed training instances bit parity concept 
analysis id variant id incremental tree construction algorithm suggested schlimmer fisher comparison purposes builds new tree training instance observed 
algorithm adds new training instance set observed training instances uses basic id tree construction algorithm build new decision tree 
obvious incremental algorithm provides basic level complexity incremental algorithms improve worth 
analysis basic id tree construction algorithm worst case number instance count additions 




number score calculations 

incremental induction decision trees analysis shows instance count additions worst case cost building tree id algorithm polynomial number training instances number score calculations worst case cost linear analysis id algorithm id algorithm builds decision tree incrementally observed training instances 
tree revised needed giving tree equivalent basic id algorithm construct set training instances 
information maintained possible test attributes node remains possible change test attribute node 
information consists positive negative instance counts possible value attribute 
accounting necessary revising tree analysis lengthy cases 
analysis parts worst case number instance count additions worst case number score calculations 
worst case number instance count additions components expense updating decision tree 
cost updating instance counts incorporating training instance tree structure 
second cost restructuring tree desired test attribute root 
cost recursively restructuring subtrees necessary ensure best test attribute root 
components analyzed individually combined 
consider worst case cost updating tree training instance tree revision 
assuming expanded tree cost updating tree root level zero instance count additions possible attributes 
update procedure traverses corresponding value branch level tree 
level instance count additions 
general worst case number instance count additions update tree instance 
second consider cost changing test attribute root tree 
convenience pa worst case cost changing test attribute node tree depth tree attribute pull needed giving pa 
larger values cost derived definition pull process involves transposing top levels tree 
level zero instance counts correct due regular update procedure 
level instance counts touched 
level instance counts non test attributes need recomputed due regrouping level subtrees 
tree possible attribute level non test attributes level giving pa 
tree non test attribute level 
non test attribute possible values positive negative count 
counts computed summing counts corresponding level trees 
level subtrees instance count additions non test attributes 
non test attributes total additions 
worst case attribute pulled needs pulled recursively incremental induction decision trees deepest level tree 
general pa 
pa gives pa third consider cost recursively restructuring subtrees ensure best attribute test root 
process pulling attribute root tree leaves certain amount 
old test attribute root immediate subtree root 
byproduct tree revision result selection score 
immediate subtree visited training instance processed level necessary compute new scores possible test attributes pull best attribute root necessary repeat process subtrees 
simplify analysis assume best test attribute reestablished subtrees 
convenience ra worst case cost restructuring subtrees level 
possible attributes level giving ra 
possible attribute level subtrees meaning restructuring possible giving ra 
level subtrees best attribute pulled root 
general ra 
pa ra 
pa simplify analysis cost computing pa assume pa ra 

worst case cost updating decision tree sum components 
instance worst case number instance count additions pa ra pa ra simplify analysis pa ra db pa ra ib db 
incremental induction decision trees instances worst case total number instance count additions 

analysis shows instance count additions worst case cost building tree id algorithm linear number training instances worst case number score calculations terms worst case number score calculations components expense updating decision tree 
cost incurred incorporating training instance tree structure 
second cost recursively restructuring subtrees necessary ensure best test attribute root 
components analyzed individually combined 
consider worst case cost updating tree instance tree revision 
assuming expanded tree cost updating tree root level zero score calculations possible attributes 
update procedure traverses corresponding value branch level tree 
level instance count additions 
case instance count additions worst case number score calculations update tree instance 
consider worst case number score calculations needed result tree restructuring 
portion pull process test attribute moved root scores calculated tree may revised best test attribute root subtrees 
need compute worst case number score calculations portion pull process 
convenience re number score calculations required best test attribute tree depth tree possible attributes level giving re 
tree attribute level need compute score giving re 
tree depth level subtrees possible attributes giving re score calculations 
simplify analysis assume best test attribute reestablished subtrees 
general score calculations level re score calculations required subtrees giving re re 
reduces re db putting worst case score cost update worst case score cost gives instance update cost re re incremental induction decision trees simplify analysis re re instances worst case total number score calculations 
analysis shows score calculations worst case cost building tree id algorithm linear number training instances analysis id algorithm id algorithm utgoff equivalent id algorithm restructuring tree bring desired attribute root subtrees restructured recursively 
specifically step ii id algorithm shown table omitted 
eliminating step hopes reduce expense tree manipulation 
important consequence omitting step resulting tree guaranteed basic id tree construction algorithm built training instances 
analysis id algorithm follows directly id 
instance worst case number instance count additions pa pa simplify analysis pa pa instances worst case total number instance count additions 
factor expensive id 
terms score calculations worst case number instance simply incremental induction decision trees table 
summary worst case complexities 
instance count additions score calculations id 
id 

id 


id 

instances complexity terms score calculations 
simpler id independent number possible attribute values 
summary analyses algorithms compare 
table summarizes complexities listing algorithms order complex number instance count additions 
basic id tree construction algorithm expensive incremental 
id algorithm incremental expensive terms instance count additions 
remaining algorithms worst case cost building tree set instances id id 
unfortunately id guaranteed build tree id 
wants build decision tree incrementally wants tree id build analysis indicates id algorithm expensive worst case 
empirical behavior id variants section reports empirical behavior variants id discussed 
analyses section indicate worst case complexity building decision tree algorithms worst case assumptions quite strong get sense expected behavior 
particular tree restructuring potentially expensive id worthwhile expect significantly better worst case 
general empirical observation expose aspects behavior captured asymptotic complexity analysis 
rest section describes experiments 
experiments similar structure comparing basic id tree construction routine incremental algorithms id id id id 
addition incremental algorithm run different training strategies giving total algorithms 
third experiment applies id quinlan classic chess task comparing different attribute selection metrics quinlan 
experiments id id id algorithms set break ties equally attributes identical manner 
addition score values rounded nearest remove differences score values due solely different orderings floating point operations 
algorithms bit multiplexor experiment variants id algorithm applied learning rule corresponds simple multiplexor barto wilson quinlan 
version problem address bits data bits 
classification incremental induction decision trees table 
behavior bit multiplexor task 
alg 
insts 
trained scores cpu nodes acc 
id id id id id id id id id instance equal value addressed data bit 
problem difficult relevance data bit attributes function values address bit attributes 
algorithms incremental id id id id 
algorithms come modifying training strategy 
suggested schlimmer fisher alternative training strategy update tree existing tree misclassify training instance just 
algorithms strategy denoted id id id id respectively called hat versions 
ninth algorithm basic id tree construction routine applied entire training set instances 
incremental variants instances random order replacement tree gave correct classification instances instance space instances 
variant run times results averaged 
identical sequence initial random number seeds runs incremental algorithms 
table shows measurements algorithms 
incremental algorithms values averaged runs 
measurement insts 
number training instances algorithm 
third column trained number instances update tree 
hat variants number usually number instances misclassified current tree update tree 
fourth column shows average number instance count additions fifth column shows average number score computations 
total cumulative cpu seconds spent processing training instances shown column 
size final decision tree shown column labeled nodes measured total number decision answer nodes 
column shows percentage possible instances classified correctly final decision tree 
results show id expensive id problem id id difference pronounced 
smaller number instances update trees id comparable 
greater expense id algorithm evident larger number instances id id note hat variants find algorithms implemented common lisp run sun 
incremental induction decision trees smaller trees plain versions 
remaining algorithms included comparison purposes flawed incremental version id 
id variant build consistent decision tree set training instances tree necessarily id build 
experiment shows id needs examine training instances id id order induce tree correct entire training set 
id algorithm expensive id id finds larger trees 
id algorithm guaranteed find tree id guaranteed find consistent decision tree 
experiment shows algorithm requires large number training instances making expense high comparison 
final tree large comparison 
id algorithm consistent decision tree cutoff training instances 
average final tree size nodes gives little hope algorithm find consistent tree training 
expense excessive comparison hat algorithms 
id succeed id fails 
hat version trains instances misclassified current tree 
tree changes set instances misclassify changes 
new set instances tree misclassify may lead change test attribute node 
changing training set id algorithm resulting thrashing 
basic id tree construction algorithm incremental 
algorithm efficient incremental variants built largest tree 
course id train entire set build identical tree 
issue training larger set instances tends produce larger decision tree 
phenomenon expected remains unexplained 
apparently probability distributions inferred incorporating misclassified training instances leads better attribute selection distributions inferred larger necessary training sets entire training set 
algorithms simplified chess problem second experiment similar multiplexor described differences 
domain chess task test problem schlimmer fisher simplified version quinlan chess task 
simplified version problem learn rule decides chess position white having king rook versus black having king knight lost ply black move 
training instance chess position described terms sixteen valued attributes 
second difference set test instances drawn independently set training instances 
training purposes single fixed set training instances drawn random 
incremental variants instances random order replacement tree gave correct classification training instances instances 
training strategy produces learning task terminating condition depend fixed number training instances 
important variants require training instances variants differing processing cost 
testing purposes second fixed set test instances drawn random 
classification accuracy measured proportion test instances correctly classified 
variant run times results averaged 
nonincremental id single tree built training instances 
sequence initial incremental induction decision trees table 
behavior simplified chess task 
alg insts trained scores cpu nodes acc id id id id id id id id id random number seeds runs incremental algorithms 
table shows measurements algorithms 
table seventh reports accuracy final tree independent test set instances 
results experiment similar 
id algorithm expensive id algorithm problem 
small number training instances needed id id id expensive 
simple learning problem id beat id 
id algorithm trouble finding consistent concept description expense id id id 
hat variants generally expensive id trouble finding consistent tree 
cut instances eighteen runs 
attributes provided learning task profound effect difficulty task 
attributes multiplexor task describe bit values 
contrast attributes simplified chess task describe board position measure relationships pieces predictive winning losing 
result chess rule easier learn multiplexor rule space instance descriptions chess task sixteen valued attributes larger multiplexor task valued attributes 
quinlan chess task order observe id algorithm behavior larger problem applied quinlan classic chess task available training instances described binary attributes 
algorithm run times attribute selection strategies 
approach selects attribute lowest score 
second chooses attribute highest gain ratio quinlan compensates bias metric attributes possible values 
definitions section gain ratio criterion defined gain ratio iv incremental induction decision trees table 
behavior id quinlan chess task 
selection insts trained scores cpu nodes gain ratio score number training instances accuracy 
change classification accuracy chess task id algorithm 
iv jv log jaj jaj table shows average values measures attribute selection metrics 
expected gain ratio criterion better terms finding smaller trees leads fewer instance count additions score calculations 
gain ratio criterion requires greater total cpu time additional expense computing function iv functions picking best attribute 
surprise average case tree size comparable previously known smallest tree size gain ratio criterion quinlan personal communication lowest score criterion quinlan 
gain ratio criterion trees nodes smallest known decision tree concept 
id builds tree id set instances difference lies selection instances build tree utgoff 
surmise selecting training instances time inconsistency current tree leads smaller tree selecting instances time done complete id learning algorithm 
plots classification accuracy node tree instances original figures lost 
figures appear generated node tree incremental induction decision trees number training instances tree size 
change tree size chess task id algorithm 
training instance added tree plots size decision tree training instance observed 
tree size generally increasing significant differences training instance 
consider change tree size th th th training instance 
tree size changes nodes back 
clear kind variation characteristic instance selection strategy specific chess domain 
case large variation indicate quality learning measured resulting tree size sensitive selected training instances 
improvements id algorithm possible 
example significant expense id algorithm computation scores 
dominating cost computing scores evaluation log function computation 
efficiency algorithm improved storing result tree structure 
positive negative count attribute value pair changed incorporating instance computation needed order compute score 
issue addressed learning noisy instances 
extensions id algorithm deal noise come mind 
new training instance encountered previously different classification old instance saved tree remove older instance add newer instance revise tree necessary 
modification system handle concept drift 
alternatively retain instance results smaller tree result test attribute lower lowest score 
second extension include test dependence 
splitting attribute justified allow test attribute node 
split justified decision tree rooted node need unexpanded tree 
definition unexpanded tree allows set training instances 
modification trivial differs definition unexpanded tree 
original node tree 
paragraph left discussion tree size quite match figures 
incremental induction decision trees problem classifying instances certain attribute values missing received attention quinlan id modified deal issue restructuring tree classification 
select best attribute value available instance restructure tree necessary attribute test attribute descend level 
branches corresponding available attribute values instance traversed determining class instance counts node majority vote decide classification 
important open problem finding decision trees problem defining set attributes 
example chess task easier multiplexor provided chess attributes better capturing regularity domain 
additional attribute included multiplexor specified address indicated address bits multiplexor task significantly easier 
problem discovering set attributes needs attention principal issue explored research continues 
id algorithm builds decision tree incrementally restructuring tree necessary best test attribute decision node 
algorithm retains training instances tree structure instances restructuring purposes 
algorithm builds tree basic id tree construction algorithm training instances assuming ties equally attributes broken identically 
empirical tests id algorithm expensive brute force id algorithm 
earlier incremental algorithm id incapable learning certain classes concepts id learn 
furthermore concepts id acquire empirical testing shows algorithm expensive comparison id 
strategy training instances tree misclassify detrimental id reducing class concepts learn 
id training strategy generally leads smaller final decision tree strategy id accumulates misclassified instances time 
phenomenon finding smaller trees observed multiplexor task quinlan chess task 
training strategy id smallest known tree date chess domain 
id algorithm viewed incremental variant id tree building routine 
find identical tree initial window contains just instance window grown instance time 
creation incremental id algorithm approach prohibitively expensive 
material supported national science foundation 
iri general electric faculty fellowship office naval research university research initiative program contract number 
discussions jeff schlimmer id id helpful 
addition provided instance generator simplified chess task useful comments review process 
ross quinlan helpful observations suggestions provided chess data 
pat langley provided extensive comments particularly regard clarifying presentation 
am indebted maria pointing robin cockett robin cockett discussions revising decision trees 
presentation bene incremental induction decision trees fitted comments sharad saxena connell jamie callan peter kishore swaminathan dan suthers 
barto 

learning statistical cooperation self interested neuron computing elements 
human neurobiology 
breiman friedman olshen stone 

classification regression trees 
belmont ca wadsworth international group 
buchanan mitchell 

model directed learning production rules 
waterman hayes roth eds pattern directed inference systems 
new york academic press 
cockett 

discrete decision theory manipulations 
theoretical computer science 
fisher 

knowledge acquisition incremental conceptual clustering 
machine learning 
gallant 

connectionist expert systems 
communications acm 
lewis 

characteristic selection problem recognition systems 
ire transactions information theory 
michalski 

learning told learning examples experimental comparison methods knowledge acquisition context developing expert system soybean disease diagnosis 
policy analysis information systems 
michalski 

theory methodology inductive learning pp 

michalski carbonell mitchell eds machine learning artificial intelligence approach 
san mateo ca morgan kaufmann 
mitchell 

version spaces approach concept learning 
doctoral dissertation department electrical engineering stanford university palo alto ca 
moret 

decision trees diagrams 
computing surveys 
quinlan 

learning efficient classification procedures application chess games pp 

michalski carbonell mitchell eds machine learning artificial intelligence approach 
san mateo ca morgan kaufmann 
quinlan 

induction decision trees 
machine learning 
quinlan 

empirical comparison genetic decision tree classifiers 
proceedings fifth international conference machine learning pp 

ann arbor mi morgan kaufman 
schlimmer fisher 

case study incremental concept induction 
proceedings fifth national conference artificial intelligence pp 

pa morgan kaufmann 
incremental induction decision trees schlimmer granger jr 

incremental learning noisy data 
machine learning 
shannon 

mathematical theory communication 
bell system technical journal 
utgoff 

id incremental id 
proceedings fifth international conference machine learning pp 

ann arbor mi morgan kaufman 
utgoff 

improved training incremental learning 
proceedings sixth international workshop machine learning 
ithaca ny morgan kaufmann 
vere 

multilevel counterfactuals generalizations relational concepts productions 
artificial intelligence 
watanabe 

pattern recognition human mechanical 
new york wiley sons 
wilson 

classifier systems animat problem 
machine learning 
