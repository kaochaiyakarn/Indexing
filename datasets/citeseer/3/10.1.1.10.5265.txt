journal machine learning research submitted published reducing multiclass binary unifying approach margin classifiers erin allwein org southwest research institute road san antonio tx robert schapire schapire research att com labs gamma research shannon laboratory park avenue room florham park nj yoram singer singer cs huji ac il school computer science engineering hebrew university jerusalem israel editor leslie pack kaelbling unifying framework studying solution multiclass categorization problems reducing multiple binary problems solved margin binary learning algorithm 
proposed framework unifies popular approaches class compared pairs classes compared output codes error correcting properties 
propose general method combining classifiers generated binary problems prove general empirical multiclass loss bound empirical loss individual binary learning algorithms 
scheme corresponding bounds apply popular classification learning algorithms including support vector machines adaboost regression logistic regression decision tree algorithms 
give multiclass generalization error analysis general output codes adaboost binary learner 
experimental results svm adaboost show scheme provides viable alternative commonly multiclass algorithms 

supervised machine learning tasks cast problem assigning elements finite set classes categories 
example goal optical character recognition ocr systems determine digit value image 
number applications require multiclass categorization immense 
examples applications text speech categorization natural language processing tasks part speech tagging gesture object recognition machine vision 
designing machine learning algorithms easier devise algorithms distinguishing classes 
machine learning algorithms quinlan cart breiman friedman olshen stone naturally extended handle multiclass case 
algorithms adaboost freund schapire fl allwein schapire singer schapire singer support vector machines svm algorithm vapnik cortes vapnik direct extension multiclass case may problematic :10.1.1.32.8918
typically cases multiclass problem reduced multiple binary classification problems solved separately 
connectionist models rumelhart hinton williams class represented output neuron notable example output neuron serves discriminator class represents classes 
training algorithm reduction multiclass problem binary problems number classes 
ways reduce multiclass problem multiple binary classification problems 
simple approach mentioned class compared 
hastie tibshirani suggest different approach pairs classes compared 
dietterich bakiri general framework classes partitioned opposing subsets error correcting codes 
methods binary classification problems solved resulting set binary classifiers combined way 
study general framework simple extension dietterich bakiri framework unifies methods reducing multiclass problem binary problem 
pay particular attention case binary learning algorithm margin training example 
roughly speaking margin training example number positive example correctly classified classifier magnitude measure confidence prediction 
known algorithms directly margins 
instance svm algorithm vapnik cortes vapnik attempts maximize minimum margin training example 
algorithms attempt minimize loss function margin 
adaboost freund schapire schapire singer example shown adaboost greedy procedure minimizing exponential loss function margins :10.1.1.32.8918
section catalog algorithms viewed margin learning algorithms including regression logistic regression decision tree algorithms 
simplest method combining binary classifiers call hamming decoding ignores loss function training confidences attached predictions classifier 
section give new general technique combining classifiers suffer defects 
call method loss decoding 
prove theoretical properties methods section 
particular decoding methods prove general bounds training error multiclass problem terms empirical performance individual binary problems 
bounds indicate loss decoding superior hamming decoding 
bounds depend manner multiclass problem reduced binary problems 
approach bounds linear number classes reduction random partitions classes bounds independent number classes 
results generalize specialized bounds proved schapire singer guruswami sahai 
section prove bound generalization error method binary learner adaboost 
particular generalize analysis schapire 
expressing bound generalization error terms training set margins combined multiclass classifier showing boosting way tends aggressively increase margins training examples 
reducing multiclass binary section experiments svm adaboost variety multiclass binary reductions 
results show predicted theory loss decoding better hamming decoding 
results show commonly reduction easy beat best method problem dependent 

margin learning algorithms study methods handling multiclass problems general class binary algorithms attempt minimize margin loss function 
section describe class learning algorithms examples 
binary margin learning algorithm takes input binary labeled training examples instances belong domain labels gamma 
learning algorithm uses data generate real valued function hypothesis belongs hypothesis space margin example respect yf 
note margin positive sign agrees interpret sign prediction exactly training error case count zero output mistake 
predicate holds 
minimization training error may worthwhile goal general form problem intractable see instance simon 
advantageous minimize nonnegative loss function margin minimize loss function 
different choices loss function different algorithms approximately minimizing eq 
hypothesis space lead various learning algorithms 
list examples 
particularly concerned method achieve small empirical loss algorithms black boxes 
focus loss function properties allow prove main theorem effectiveness output coding methods multiclass problems 
support vector machines 
training data may linearly separable machines svm algorithm vapnik cortes vapnik seeks linear classifier form delta minimizes objective function jjwjj parameter subject linear constraints delta gamma allwein schapire singer put way svm solution minimizer regularized empirical loss function jjwjj gamma delta 
formal treatment see instance scholkopf 

role norm objective function fundamental order svm analysis section corresponding multiclass algorithm depends loss function function margins 
svm viewed binary margin learning algorithm seeks achieve small empirical risk loss function gamma adaboost 
algorithm adaboost freund schapire schapire singer builds hypothesis linear combination weak base hypotheses ff hypothesis built series rounds selected weak base learning algorithm ff chosen :10.1.1.32.8918
observed breiman authors collins schapire singer friedman hastie tibshirani mason baxter bartlett frean ratsch onoda muller appear schapire singer ff effectively greedily chosen minimize gammay adaboost binary margin learning algorithm loss function gammaz adaboost randomized predictions :10.1.1.30.3515
little studied variant adaboost freund schapire allow adaboost output randomized predictions predicted label new example chosen randomly probability gamma :10.1.1.32.8918
loss suffered probability randomly chosen predicted label disagrees correct label def gamma 
loss gamma gamma 
simple algebraic manipulation loss shown yf 
variant adaboost set 
case note learning algorithm directly attempting minimize loss minimizing exponential loss described 
regression 
various algorithms neural networks squares regression attempt minimize squared error loss function gamma gamma function rewritten gamma gamma yy gamma yf gamma yf binary problems minimizing squared error fits framework gamma reducing multiclass binary logistic regression 
logistic regression related methods iterative scaling csiszar della pietra della pietra lafferty lafferty logitboost friedman posits logistic model estimating conditional probability positive label pr jx gamma attempts maximize likelihood labels sample equivalently minimize log loss gamma log pr yjx log gamma yf logistic regression related methods take log gamma 
decision trees 
popular decision tree algorithms naturally linked loss functions 
instance quinlan simplest form binary classification problems splits decision nodes manner greedily minimize ln gamma gamma ln gamma gamma gamma fraction positive negative examples reaching leaf respectively 
prediction leaf sign gamma gamma 
viewed differently imagine decision tree outputs real number leaf intention performing logistic regression 
empirical loss associated logistic regression ln gamma gamma ln minimized choices ln gamma 
plugging choice gives exactly eq 
thresholding gives hard prediction rule earlier 
simple form viewed margin learning algorithm naturally linked loss function logistic regression 
similar reasoning cart breiman splits gini index linked square loss function kearns mansour splitting rule linked exponential loss adaboost 
analysis section hold algorithms tacitly employ function margin 
instance freund brownboost algorithm implicitly uses instance potential function satisfies condition impose combined output coding solve multiclass problems 
conclude section plot loss functions discussed 

output coding multiclass problems section discussed margin algorithms learning binary problems 
suppose faced multiclass learning problem label chosen set cardinality 
binary margin learning algorithm modified handle class problem 
allwein schapire singer exp log exp margin loss functions discussed exponential loss adaboost top left square loss squares regression top right hinge loss support vector machines bottom left logistic loss logistic regression bottom right 
solutions proposed question 
involve reducing multiclass problem way set binary problems 
instance simplest approach create binary problem classes 
apply margin learning algorithm binary problem examples labeled considered positive examples examples considered negative examples 
hypotheses combined 
call approach 
approach suggested hastie tibshirani binary learning algorithm distinguish pair classes 
distinct pair run learning algorithm binary problem examples labeled considered positive labeled negative 
examples simply ignored 
gamma delta hypotheses generated process combined 
call pairs approach 
general suggestion handling multiclass problems dietterich bakiri 
idea associate class row coding matrix gamma theta 
binary learning algorithm run column matrix induced binary problem label example labeled mapped 
yields reducing multiclass binary hypotheses example predict label row matrix closest 
method error correcting output codes ecoc 
section propose unifying generalization methods applicable margin learning algorithm 
generalization closest ecoc approach dietterich bakiri differs coding matrix taken larger set gamma theta entries may zero indicating don care hypothesis categorizes examples label scheme learning multiclass problems binary margin learning algorithm works follows 
coding matrix gamma theta learning algorithm provided labeled data form examples training set omitting examples 
algorithm uses data generate hypothesis example approach thetak matrix diagonal elements elements gamma 
pairs approach theta gamma delta matrix column corresponds distinct pair 
column row gamma row zeros rows 
alternative calling repeatedly cases may wish add column index distinguished attribute instances received learn single hypothesis larger learning problem hypotheses smaller problems 
provide instances form training examples columns 
algorithm produces single hypothesis theta consistency preceding approach define 
call approaches called repeatedly multi call single call variants respectively 
note passing fundamental differences single multi call variants 
previous output coding employed multi call variant due simplicity 
single call variant handy implementation classification learning algorithm outputs single hypothesis form theta available 
describe experiments variants section 
variant algorithm attempts minimize loss induced binary problem 
recall function margin example loss example induced label gamma 
want entirely ignore hypothesis computing loss 
define loss constant case convenience choose loss loss associated example cases 
average loss choices examples call average binary loss hypotheses training set respect coding matrix loss quantity calls implicit intention allwein schapire singer minimizing 
see section quantity relates misclassification error final classifier build original multiclass training set 
denote row vector predictions instance predictions test point labels predicted 
methods combining devised focus simple implement analyze empirical risk original multiclass problem 
basic idea methods predict label row closest predictions 
words predict label minimizes distance formulation begs question measure distance vectors 
way doing count number positions sign prediction differs matrix entry 
formally means distance measure dh gamma sign sign gamma 
essentially computing hamming distance row signs 
note zero component contributes sum 
instance matrix predicted label kg arg min dh call method combining hamming decoding 
disadvantage method ignores entirely magnitude predictions indication level confidence 
second method combining predictions takes potentially useful information account relevant loss function ignored hamming decoding 
idea choose label consistent predictions sense example labeled total loss example minimized choices formally means distance measure total loss proposed example dl analogous hamming decoding predicted label kg arg min dl call approach loss decoding 
illustration decoding methods 
shows decoding process problem classes output code length 
clarity denote entries output code matrix gamma gamma 
note example predicted class loss decoding case uses exponential loss different hamming decoding 
note passing loss decoding method log loss known widely maximum likelihood decoding studied briefly context ecoc guruswami sahai 
reducing multiclass binary class prediction sign binary classifiers output binary classifiers prediction class illustration multiclass prediction procedure hamming decoding top loss decoding bottom class problem code length 
exponential function loss decoding 

analysis training error section analyze training error output coding methods described section 
specifically upper bound training error decoding methods terms average binary loss defined eq 
measure minimum distance pair rows coding matrix 
simple generalization hamming distance vectors set gamma 
specifically define distance rows gamma delta gamma gamma delta analysis depends minimum distance ae pairs distinct rows ae minf delta example code ae 
pairs code ae gamma delta gamma rows exactly component opposite signs gammam allwein schapire singer rest component 
random matrix components chosen uniformly gamma gamma expected value delta distinct pair rows exactly 
intuitively larger ae decoding correct errors individual hypotheses 
dietterich bakiri insight suggesting output codes error correcting properties 
intuition reflected analysis larger value ae gives better upper bound training error 
particular theorem states training error ae times worse average binary loss combined hypotheses scaling loss 
matrix ae large number classes large 
hand pairs matrix random matrix ae close constant independent analysis loss decoding 
analysis hamming decoding follow corollary 
concerning loss analysis assumes gammaz note property holds convex convexity means necessary condition 
note loss functions section satisfy property 
property illustrated loss functions discussed section 
theorem average binary loss defined eq 
hypotheses training set respect coding matrix gamma theta loss cardinality label set ae eq 

assume satisfies eq 
training error loss decoding ael proof suppose loss decoding incorrectly classifies example 
label dl dl delta fs set columns rows differ non zero 
fs set columns row row zero 

eq 
implies delta delta reducing multiclass binary delta turn implies delta delta delta delta gammaz assumption gammaz 
term eq 
js delta 
case implies 
second term eq 
js 
eq 
js delta js delta ael words mistake training example implies ael number training mistakes ael ael training error ael claimed 
corollary give similar weaker theorem hamming decoding 
note different assumption loss function holds loss functions described section 
corollary set hypotheses training set gamma theta coding matrix cardinality label set ae eq 

training error hamming decoding aem gamma sign loss function satisfying average binary loss respect loss function training error hamming decoding ael allwein schapire singer proof consider loss function gamma sign 
eqs 
clear hamming decoding equivalent loss decoding loss function 
satisfies eq 
apply theorem get upper bound training error aem equals eq 

second part note 
implies eq 
bounded eq 

theorem corollary broad generalizations similar results proved schapire singer specialized setting involving adaboost 
corollary generalizes results guruswami sahai bound multiclass training error terms training misclassification error rates binary classifiers 
bounds theorem corollary depend implicitly fraction zero entries matrix 
intuitively zeros examples ignored harder drive training error 
extreme zeros ae fairly large learning certainly possible 
dependence explicit set pairs inducing examples ignored learning 
jt fraction ignored pairs 
average binary loss restricted pairs ignored training jt 
bound theorem rewritten ael ae gamma similarly ffl fraction misclassification errors ffl jt sign part corollary implies training error hamming decoding bounded ae gamma ffl see bounds trade offs design coding matrix hand want rows far apart ae large want non zero entries small 
hand attempting ae large small may produce binary problems difficult learn yielding large restricted average binary loss 
reducing multiclass binary 
analysis generalization error boosting loss decoding previous section considered training error output codes 
section take difficult task analyzing generalization error 
difficulty obtaining results kind general results obtained training error apply broad class loss functions 
focus generalization error adaboost output coding loss decoding 
specifically show analysis schapire 
extended complicated algorithm 
briefly schapire analysis proposed means explaining empirically observed tendency adaboost resist overfitting 
theory notion example margin informally measures confidence prediction classifier example 
gave part analysis adaboost proved bound generalization error terms margins training examples bound independent number base hypotheses combined bound suggesting larger margins imply lower generalization error 
second part analysis proved adaboost tends aggressively increase margins training examples 
section give counterparts parts analysis combination adaboost loss decoding 
assume single call variant described section 
result essentially adaboost mo algorithm schapire singer specifically called variant 
algorithm works follows 
assume coding matrix 
algorithm works rounds repeatedly calling base learning algorithm obtain base hypothesis 
round algorithm computes distribution pairs training examples columns matrix set mg theta base learning algorithm uses training data binary labels encoded distribution obtain base hypothesis theta gamma 
general range may simplicity assume binary valued 
error ffl probability respect misclassifying examples 
ffl pr distribution updated rule exp gammaff ff ln gamma ffl ffl nonnegative assuming ffl normalization constant ensuring distribution 
straightforward show ffl gamma ffl initial distribution chosen uniform 
rounds procedure outputs final classifier decoding arg min exp gammam ff allwein schapire singer margin theoretic analysis definition margin combined multiclass classifier 
gamma ln ff ff rewrite eq 
arg max transformed argument minimum eq 
strictly decreasing function 
gamma ln arrive eq 
clear changed definition rewriting effect normalizing argument maximum eq 
range gamma 
define margin labeled example difference vote correct label largest vote label 
denote margin 
formally gamma max factor simply ensures margin range gamma 
note margin positive correctly classifies example 
definition margin seemingly different earlier binary problems schapire comparatively simple context show maximizing training example margins translates better bound generalization error independent number rounds boosting 
base hypothesis space gamma valued functions theta denote convex hull 
ff ff ff understood sums finite subset hypotheses ff 
defined eq 
belongs 
assume training examples chosen distribution theta write probability expectation respect random choice example prd delta ed delta 
similarly probability expectation respect example chosen uniformly random training set denoted pr delta delta 
prove main theorem section shows generalization error usefully bounded training examples large margin 
similar results schapire 
fact applies loss decoding general coding matrix reducing multiclass binary theorem distribution theta sample examples chosen independently random suppose base classifier space vc dimension ffi 
assume number columns coding matrix probability gamma ffi random choice training set weighted average function satisfies bound prd pr log log ffi proof prove theorem need define notion sloppy cover slightly specialized purposes 
class real valued functions theta training set ae theta size real numbers ffl say function class ffl sloppy cover respect exists pr gamma ffl 
ffl denote maximum training sets size size smallest ffl sloppy cover respect techniques bartlett schapire 
theorem give theorem states ffl probability random choice training set exists function prd pr ffl ffl gammaffl prove theorem applying result family functions fm need construct relatively small set functions approximate functions start lemma implies function approximated small finite set ae ln ln oe lemma exists gamma allwein schapire singer proof ln claim gamma gamma ln inequality suffices show nondecreasing 
differentiating find df dj ln ln entropy symbols exceed ln quantity nonnegative 
second inequality eq 
suffices show ln nonincreasing 
differentiating reusing eq 
find dg dj ln nonpositive entropy negative 
mine ln largest element bigger ln ln gamma gamma ln gamma ln ln gamma gamma ln ln gamma ln gamma ln remains handle case small 
assume gamma ln ln exp reducing multiclass binary proved hoeffding random variable jx exp gamma je hand eq 
lim lim equality uses rule 
mine take mine implies gamma assuming gamma gamma gammam gamma completes lemma 
fixed subset theta size vc dimension exists subset cardinality includes behaviors exists cn 
set unweighted averages elements fn fm cn show fn sloppy cover 
write ff ff ff 
interested behavior points assume loss generality lemma suppose cn jf gamma lemma 
jm gamma allwein schapire singer proof gamma ln exp exp ln max exp gamma max gamma max jm jjf gamma inequality uses simple fact max positive 
symmetry argument gamma lemma gamma gamma definition margin implies lemma 
recall coefficients ff nonnegative sum words define probability distribution useful imagine sampling distribution 
specifically suppose chosen manner 
fixed gamma valued random variable expected value exactly 
consider choosing functions hn independently random distribution cn denote resulting distribution functions cn note fixed average gamma trials expected value 
pr gq jm gamma pr gq jf gamma pr gq jf gamma gamman inequalities follow respectively lemma union bound hoeffding inequality 
gq pr jm gamma pr gq jm gamma gamman exists cn pr jm gamma gamman reducing multiclass binary shown fn gamman sloppy cover words gamman fn dn ln making appropriate substitutions gives eq 
ln ln ffl ffl ln ln ffi ln ln note quantity inside square root 
ffl 
approximation occurrence ffl follows eq 
ffi 
proved bound theorem single choice high probability 
prove bound holds simultaneously exactly argument schapire singer part theorem 
shown large training set margins imply better bound generalization error independent number rounds boosting 
turn second part analysis prove adaboost mo tends increase margins training examples assuming binary errors ffl base hypotheses bounded away trivial error rate see discussion follows proof 
theorem prove direct analog theorem schapire 
binary adaboost 
note focus coding matrices contain zeros 
slightly weaker result proved general case 
theorem suppose base learning algorithm called adaboost mo coding matrix gamma theta generates hypotheses weighted binary training errors ffl ffl ae eq 

pr ae ffl gamma gamma ffl eqs 

proof suppose labeled example 
definition margin exists label gamma fyg gamma allwein schapire singer jm gamma jm 
delta fs eq 
implies gammaz gammaz gammaz gammaz gammaz delta gammaz gammaz delta gammaz zs js delta ae delta gammaz 
gammaj fraction training examples aem ae exp gamma ff ae ae eq 
uses definition eqs 

eq 
uses definition defined recursively eq 

eq 
uses fact distribution 
theorem follows plugging eq 
applying straightforward algebra 
noted schapire 
bound usefully understood assume ffl gamma fl upper bound simplifies ae gamma fl gamma fl fl expression inside parentheses smaller fraction training examples margin drops zero exponentially fast reducing multiclass binary number classes avg 
binary error loss decoding hamming decoding number classes avg 
binary error loss decoding hamming decoding comparison average binary error multiclass error hamming decoding multiclass error loss decoding adaboost synthetic data complete code left code right 

experiments section describe discuss experiments performed synthetic data natural data uci repository 
run sets experiments base learners adaboost svm 
primary goals experiments compare hamming decoding loss decoding compare performance different output codes 
start description experiment real valued synthetic data underscores tradeoff complexity binary problems induced output code error correcting properties 
experiments synthetic data generated instances normal distribution zero mean unit variance 
create multiclass problem classes set thresholds denoted 
instance associated class gamma jxj generated sets examples set size 
thresholds set exactly examples associated class 
similarly generated number test sets size thresholds obtained training data label test data 
adaboost base learner set number rounds boosting binary problem called adaboost repeatedly column multi call variant 
weak hypotheses set possible threshold functions 
weak hypothesis threshold label instance jxj gamma 
plot test error rate function output coding matrices 
complete code columns correspond non trivial partitions set kg subsets 
code matrix size theta gamma gamma 
second code code 
code plot average binary test error multiclass errors hamming decoding multiclass errors loss decoding 
graphs clearly show hamming decoding inferior loss decoding yields higher error rates 
multiclass errors codes loss decoding comparable 
multiclass error rate complete code slightly lower error rate code situation reversed average binary errors 
phenomenon underscores allwein schapire singer examples problem train test attributes classes satimage glass segmentation ecoli pendigits yeast vowel soybean thyroid audiology isolet letter table description datasets experiments 
tradeoff redundancy correcting properties output codes difficulty binary learning problems induces 
complete code error correcting properties distance pair rows ae gamma binary problems complete code induces difficult learn 
distance pair rows code small ae 
empirical error bound theorem inferior 
binary problems induces simpler average binary loss lower average binary loss complete code result comparable performance 
describe experiments performed multiclass data uci repository merz murphy 
different popular binary learners adaboost svm 
chose datasets ordered number classes satimage glass segmentation coli pendigits yeast vowel soybean large thyroid audiology isolet 
properties datasets summarized table 
svm experiments skipped audiology isolet letter recognition segmentation thyroid datasets big handled current implementation svm contained nominal features missing values problematic svm 
datasets classes 
available original partition datasets training test sets 
datasets fold cross validation 
svm polynomial kernels degree multi call variant 
adaboost decision stumps base hypotheses 
modifying existing package adaboost mh schapire singer able devise simple implementation single call variant described section 
summaries results different output codes described tables 
tested different types output codes complete column possible non trivial split classes pairs types random codes 
type random code log columns problem classes 
reducing multiclass binary hamming decoding problem vs complete pairs dense sparse satimage glass segmentation ecoli pendigits yeast vowel soybean thyroid audiology isolet letter loss decoding satimage glass segmentation ecoli pendigits yeast vowel soybean thyroid audiology isolet letter loss decoding exp satimage glass segmentation ecoli pendigits yeast vowel soybean thyroid audiology isolet letter table results experiments output codes datasets uci repository adaboost base binary learner 
problem output codes evaluated see text decoding methods hamming decoding loss decoding adaboost randomized predictions denoted loss decoding exponential loss function denoted exp 
allwein schapire singer hamming decoding problem vs complete pairs dense sparse satimage glass ecoli pendigits yeast vowel soybean loss decoding satimage glass ecoli pendigits yeast vowel soybean table results experiments output codes datasets uci repository support vector machine svm algorithm base binary learner 
problem different classes output codes tested evaluated hamming decoding appropriate loss decoding svm 
element code chosen uniformly random gamma 
brevity call dense random codes 
generated dense random code multiclass problem examining random codes choosing code largest ae identical columns 
second type code called sparse code chosen random gamma 
element sparse code probability gamma probability 
sparse codes log columns 
problem picked code high value ae examining random codes 
check code column row containing zeros 
note problems classes evaluate complete pairs codes large 
compared hamming decoding loss decoding families codes 
results plotted figures 
tested uci datasets plotted bar figures height bar possibly negative proportional test error rate loss decoding minus test error rate hamming decoding 
datasets indexed plotted order listed 
tested adaboost loss functions decoding exponential loss denoted exp drawn black loss function yf denoted drawn gray result adaboost reducing multiclass binary vs complete pairs dense sparse number classes test error difference exp complete number classes test error difference exp pairs number classes test error difference exp random dense number classes test error difference exp random sparse number classes test error difference exp comparison test error hamming decoding loss decoding binary learners trained adaboost 
loss functions decoding plotted exponential loss exp black yf adaboost randomized predictions gray 
vs complete pairs dense sparse number classes test error difference complete number classes test error difference pairs number classes test error difference random dense number classes test error difference random sparse number classes test error difference comparison test error hamming decoding loss decoding binary learner support vector machines 
randomized predictions 
clear plots loss decoding gives better results hamming decoding svm adaboost 
difference significant 
instance dataset satimage pairs code svm achieves error loss decoding hamming decoding results error rate 
similar results obtained adaboost 
difference especially significant random dense codes 
note loss decoding adaboost randomized predictions yield results straightforward loss decoding adaboost exponential loss 
partially explained fact adaboost randomized predictions directly attempting minimize loss uses decoding 
conclude section discuss set experiments compared performance different output codes 
figures plot test error difference pairs codes loss decoding svm adaboost binary learners 
plot consist theta matrix bar graphs 
rows columns correspond order coding methods pairs complete random dense random sparse 
bar graph row column shows difference test error coding method minus test error coding method datasets tested 
svm clear widely code inferior codes tested 
note bars top row correspond large positive values 
results error rates higher error rates codes 
instance dataset yeast code error rate error rate codes random sparse low allwein schapire singer vs complete number classes test error difference pairs number classes test error difference random dense number classes test error difference random sparse number classes complete number classes test error difference complete complete pairs number classes test error difference complete random dense number classes complete random sparse number classes pairs number classes test error difference pairs complete number classes test error difference pairs pairs random dense number classes test error difference pairs random sparse number classes random dense number classes test error difference random dense complete number classes test error difference random dense pairs number classes test error difference dense random dense random sparse number classes random sparse number classes test error difference random sparse complete number classes test error difference random sparse pairs number classes test error difference random sparse random dense number classes test error difference sparse difference test errors pairs error correcting matrices support vector machines binary learners 
random dense 
cases performs better codes difference statistically significant 
clear winner output codes 
adaboost codes persistently better best code problem dependent 
results suggest interesting direction research methods designing problem specific output codes 
progress direction crammer singer 
acknowledgment research done authors labs 
anonymous reviewers careful reading helpful comments 
part research supported binational science foundation number 
reducing multiclass binary vs vs complete number classes test error difference vs pairs number classes test error difference vs random dense number classes test error difference vs random sparse number classes complete vs number classes test error difference complete complete vs pairs number classes test error difference complete vs random dense number classes test error difference complete vs random sparse number classes pairs vs number classes test error difference pairs vs complete number classes test error difference pairs pairs vs random dense number classes test error difference pairs vs random sparse number classes random dense vs number classes test error difference random dense vs complete number classes test error difference random dense vs pairs number classes test error difference dense random dense vs random sparse number classes random sparse vs number classes test error difference random sparse vs complete number classes test error difference random sparse vs pairs number classes test error difference random sparse vs random dense number classes test error difference sparse difference test errors pairs error correcting matrices adaboost binary learner 
bartlett 

sample complexity pattern classification neural networks size weights important size network 
ieee transactions information theory 
breiman 

arcing edge 
tech 
rep statistics department university california berkeley 
breiman 

prediction games arcing classifiers 
tech 
rep statistics department university california berkeley 
breiman friedman olshen stone 

classification regression trees 
wadsworth brooks 
collins schapire singer 

logistic regression adaboost bregman distances 
proceedings thirteenth annual conference computational learning theory 
allwein schapire singer cortes vapnik 

support vector networks 
machine learning 
crammer singer 

learnability design output codes multiclass problems 
proceedings thirteenth annual conference computational learning theory 
csiszar 

information geometry alternating minimization procedures 
statistics decisions supplement issue 
della pietra della pietra lafferty 

inducing features random fields 
ieee transactions pattern analysis machine intelligence 
dietterich bakiri 

solving multiclass learning problems error correcting output codes 
journal artificial intelligence research 
freund 

adaptive version boost majority algorithm 
proceedings twelfth annual conference computational learning theory pp 

freund schapire 

decision theoretic generalization line learning application boosting 
journal computer system sciences 
friedman hastie tibshirani 

additive logistic regression statistical view boosting 
annals statistics 
guruswami sahai 

multiclass learning boosting error correcting codes 
proceedings twelfth annual conference computational learning theory pp 

hastie tibshirani 

classification pairwise coupling 
annals statistics 
hoeffding 

probability inequalities sums bounded random variables 
journal american statistical association 
simon 

robust single neurons 
proceedings fifth annual acm workshop computational learning theory pp 

kearns mansour 

boosting ability top decision tree learning algorithms 
proceedings eighth annual acm symposium theory computing 
lafferty 

additive models boosting inference generalized divergences 
proceedings twelfth annual conference computational learning theory pp 

mason baxter bartlett frean 

functional gradient techniques combining hypotheses 
smola bartlett scholkopf schuurmans 
eds advances large margin classifiers 
mit press 
merz murphy 

uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
reducing multiclass binary quinlan 

programs machine learning 
morgan kaufmann 
ratsch onoda muller 
appear 
soft margins adaboost 
machine learning 
rumelhart hinton williams 

learning internal representations error propagation 
rumelhart mcclelland 
eds parallel distributed processing explorations microstructure cognition chap 
pp 

mit press 
schapire freund bartlett lee 

boosting margin new explanation effectiveness voting methods 
annals statistics 
schapire singer 

improved boosting algorithms confidence rated predictions 
machine learning 
scholkopf smola williamson bartlett 

new support vector algorithms 
tech 
rep nc tr neurocolt 
vapnik 

nature statistical learning theory 
springer 

