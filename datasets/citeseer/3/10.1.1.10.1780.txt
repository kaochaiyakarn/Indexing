incremental reduced error pruning johannes furnkranz gerhard widmer austrian research institute artificial intelligence vienna austria department medical cybernetics artificial intelligence university vienna mail gerhard ai univie ac keywords inductive logic programming pruning noise outlines problems may occur reduced error pruning inductive logic programming notably efficiency 
new method incremental reduced error pruning proposed attempts address problems 
experiments show noisy domains method efficient alternative algorithms slight gain accuracy 
experiments show algorithm recommended domains specific concept description 
tr able deal noisy data algorithms meant learn concepts real world domains 
significant effort gone investigating effect noisy data decision tree learning algorithms see quinlan breiman 
surprisingly noise handling methods entered emerging field inductive logic programming ilp muggleton 
linus lavrac dzeroski relies directly noise handling abilities decision tree learning algorithms cn clark niblett clark boswell assistant bratko kononenko 
dzeroski bratko adapted known methods attribute value learning ilp framework 
pruning standard way dealing noise decision tree learning see mingers esposito 
fundamentally different approaches cestnik pre pruning means concept generation training examples deliberately ignored final concept description classify training instances correctly 
post pruning means concept description generated perfectly explains training instances 
theory subsequently generalized cutting branches decision tree quinlan breiman 
ilp pre pruning common form stopping criteria foil quinlan dzeroski bratko fossil furnkranz 
post pruning introduced ilp reduced error pruning rep brunk pazzani ideas quinlan pagallo haussler 
training set split subsets growing set pruning set concept description explaining examples growing set generated relational learning algorithm 
resulting concept generalized deleting literals clauses theory deletion result decrease predictive accuracy measured pruning set 
approach disadvantages highlight section 
section briefly presents approach cohen designed solve problems 
section propose incremental reduced error pruning method integrates pre post pruning alternative solution 
section reports experiments versions algorithm 
problems reduced error pruning reduced error pruning rep brunk pazzani proven quite effective raising predictive accuracy noisy domains 
method shortcomings discuss section 
efficiency cohen shown worst case time complexity rep bad omega gamma random data number examples 
growing initial concept hand omega gamma log 
derivation numbers cohen rests assumption random incompressible data concept description growing phase contain rule example rules altogether having log conditions literal cover half random instances 
assumed final pruned theory constant size size independent costs adding literal rule proportional constant number literals tested instances growing set 
log literals concept description total cost order log growing phase 
step pruning phase clauses simplified deleting literal deleting clause 
simplifications test rules examples 
assuming rule modified deleted final concept description constant size simplification loop performed times 
get total cost omega gamma 
detailed proof cohen 
pointed result random data generalizes data containing noise constant fraction random incompressible data 
concluded long run costs pruning far outweigh costs generating initial concept description shown experimentally variety data sets 
split training data disadvantage rep training data split sets growing set usually pruning set usually 
fold problem results learning occurs examples growing set 
algorithm learn important rules examples rule pruning set growing set 
number literals constant number literal increasing new variables 
assume number variables bounded constant 
hand learned rule support pruning set examples rule growing set rule learned correctly pruned deleted altogether 
bad split examples negative influence behavior learning pruning algorithm 
separate conquer strategy decision tree learning usually divide conquer strategy 
means training set split disjoint sets outcome test chosen top level decision 
algorithm recursively applied sets independently 
greedy covering algorithms foil follow separate conquer strategy pagallo haussler 
method learns rule training set subsequently removes examples covered rule 
algorithm recursively tries find rules explain remaining examples 
separate conquer approach shares similarities divide conquer strategy important difference pruning branches decision tree affect neighboring branches pruning literals rule affect subsequent rules 
pruning literal clause means clause generalized cover positive instances negative instances 
negative instances deliberately ignored practically identified wrong noisy 
consequently additional positive negative instances removed influence learning subsequent clauses 
initial growing phase rep know instances noisy henceforth carry instances covered previous clauses 
best case superfluous examples growing phase lead generation additional clauses pruned pruning phase 
worst case instances may lead learner garden path may change evaluation literals subsequent learning correct literals selected 
wrong choice literal undone pruning 
bottom hill climbing rep employs greedy hill climbing strategy literals clauses deleted concept definition predictive accuracy pruning set maximized 
possible operator leads decrease predictive accuracy search process stops 
noisy domains expected theory generated growing phase specific rep lot pruning ample opportunity get caught local maximum 
rep specific general search expected slow imprecise noisy data prune significant portion theory previously generated growing phase local maximum process 
furnkranz reports experiments algorithm able find starting theory closer final theory specific theory 
method succeeded improving run time accuracy rep cohen grow algorithm cohen problems section particular efficiency recognized 
cohen proposed pruning algorithm technique grove learning system pagallo haussler 
rep grow finds theory overfits data 
pruning intermediate theory deletion results decrease accuracy pruning set step intermediate theory augmented generalizations clauses 
second step clauses expanded theory subsequently selected form final concept description clause improves predictive accuracy pruning set 
generalizations clauses formed repeatedly deleting final sequence conditions clause error growing set goes 
detailed description grow algorithm see cohen 
algorithm solves problems section ffl assumptions section costs pruning random data reduced log clause contains log literals literal generalized clause 
worst case get total log clauses augmented intermediate theory 
tentatively added initially empty final theory resulting set clauses tested training examples 
repeated clauses final concept description constant size 
costs algorithm log 
consult cohen detailed proof 
ffl grow replaces bottom hill climbing search rep top approach see section 
remove useless clause literal specific theory adds promising generalization rule initially empty theory 
results significant gain efficiency slight gain accuracy 
explanation top hill climbing starts empty theory domains closer correct theory specific 
cohen grow algorithm attempt solve remaining problems bad splits separate conquer strategy 
costs grow algorithm include cost overfitting data unnecessarily high 
consequently cohen tried improve grow adding stopping heuristics initial stage overfitting achieved speed algorithm 
way combining pre pruning post pruning methods get better results furnkranz 
section alternative approach integrates pre pruning post pruning way avoids expensive initial phase overfitting altogether 
incremental rep attempt solve problems mentioned section developed new algorithm integrates pre pruning post pruning learning 
basic idea growing complete concept description pruning clause pruned right generated see learning clause growing set literals deleted clause greedy fashion deletion decrease accuracy clause pruning set 
literal considered pruning final sequence literals 
resulting rule added concept description covered positive negative examples removed training growing pruning set 
remaining instances training set redistributed growing pruning set new clause learned 
predictive accuracy pruned clause predictive accuracy empty clause clause body fail clause added concept description rep returns learned clauses 
accuracy pruned clauses pruning set serves stopping criterion 
algorithm prune entire set clauses prunes successively named incremental reduced error pruning rep 
addresses problems section ways efficiency rep generate intermediate concept description 
costs rep roughly costs generating final theory rep grow generate specific theory 
rep growing clause purely random data costs log see section 
rep considers literal clause pruning log literals tested examples final clause log times 
costs pruning clause log final theory constant number clauses costs order log procedure rep pos neg clauses os pos neg clause clause clause clause cover clause cover clause clause clause rune rune accuracy clause accuracy fail return clauses os os gamma cover clause os neg neg gamma cover clause neg clauses clauses clause return clauses incremental reduced error pruning split training data rep redistributes pruning growing sets clause 
scope problems discussed section reduced learning single clause 
may happen clause learned bad growing set evaluated bad pruning set appears worse empty clause causes rep learning rep continue learning prune bad clause 
separate conquer strategy rep learns clauses order prolog interpreter 
subsequent rules learned clause completed learned pruned covered examples removed 
reason problems discussed section appear rep bottom hill climbing similarly grow rep uses top approach rep bottom search final programs removing unnecessary clauses literals overly specific theory repeatedly adding clauses initially empty theory 
grow generate intermediate specific concept description rep directly constructs final theory 
cases correct concept definition fairly simple top approach expected sensitive local optima discussed section 
efficiency rep algorithm comes integration pre pruning post pruning defining stopping criterion accuracy pruned clause pruning set 
may cause problems pruned clause accuracy accuracy empty clause clauses learned 
inversion rep delete clause operator 
accuracy measured appropriately examples left bad split examples rep prone overgeneralization 
terminology schaffer rep strong fitting avoidance bias detrimental domains 
experiments implementations algorithms tested different implementations rep differ way prune clauses number positive negative examples covered current clause total positive negative examples current pruning set rep prunes clauses number covered positive examples plus number covered negative examples maximized gamman 
accuracy empty clause clause body fail accuracy best pruned clause value learning stops 
rep prunes clauses purity clause maximized 
purity empty clause meaningless adopted stopping criterion clauses cover positive negative examples permitted may increase accuracy concept 
algorithms rep grow implemented described cohen exception delete clause pruning operator brunk pazzani cohen operator deletes final sequence literals clause 
systems implemented sicstus prolog run times measured sun sparcstation ipx 
cohen delete final sequence operator rep grow delete literal algorithms 
cohen personal communication pointed implementation grow cases produces generalizations produced delete literal 
domains run experiments variety domains 
current implementation algorithms capable handling continuous attributes multi valued classes choice test domains somewhat limited 
tests performed datasets uci machine learning repository classes symbolic attributes krk muggleton mesh muggleton domains 
table gives overview databases comparison run times different algorithms 
domains artificial noise generated inverting classification examples 
column overfitting refers initial growing phase rep grow common rep grow give results pruning phases 
total run time rep grow run time overfitting plus run time rep grow 
best result line emphasized 
domain overfitting rep grow rep rep krk krk krk krk krk monks monks monks mesh promoters votes votes vi mushroom tic tac toe mushroom tic tac toe table average run time votes promoters data fold cross validation performed 
mesh data tested runs described dzeroski bratko classification accuracy negative examples measured 
krk data tested different example set sizes evaluated noise free examples 
training set size different example sets sets examples tested sets high run times task 
done data training set size 
monks data dedicated testing set runs performed different pruning growing set distributions evaluated testing set 
domains sample sets size examples generated rules tested remaining examples 
votes vi set votes data set significant attribute removed 
propositional domains equality relation added background knowledge 
promoters data included background relations specifying dna bases split groups ali pazzani data relation added integer valued attributes 
domains system task learning definitions minority class 
results looking table easily seen grow faster rep rep grow intermediate theory faster 
fact rep usually significantly faster initial growing phase rep grow common 
illustrates effect training set size performance algorithms krk domain 
looking classification accuracies table picture diverse 
drawn experiments krk domain illustrate rep sensitive small training set sizes improves larger sizes see 
reason bad distribution growing pruning examples may cause rep stopping criterion prematurely learning 
redistributing examples learning clause help little redundancy data small sample size 
example significant influence outcome learning 
larger example set sizes rep better algorithms rep gets caught local maxima able generalize right level 
interestingly despite top search strategy grow occasionally overfits noise data highly specialized clauses intermediate theory fit noisy examples pruning set added concept description 
rep happen generates fewer clauses stops clause fits noisy examples growing pruning set 
domains specific theory pruning done pruning detrimental bottom approach pruning appropriate 
see clearly noise free tic tac toe monks domains rep significantly accurate little faster grow clearly amount pruning performed 
run time training set sizes krk run time vs training set size pruning rep grow rep accuracy training set sizes krk accuracy vs training set size pruning rep grow rep different training set sizes krk domain 
domain overfitting rep grow rep rep krk krk krk krk krk monks monks monks mesh promoters votes votes vi mushroom tic tac toe mushroom tic tac toe table average accuracy rep worst classification accuracy domains 
reason rep top approach pruning stronger overfitting avoidance bias grow inappropriate domains schaffer 
rep general worse rep purity criterion evaluating clause tries maximize percentage covered positive examples preference specific clauses rep seen higher run times 
reason rep worse rep domains fairly general theory little better domains specific theories 
natural domains mesh promoters votes rep equal algorithms terms predictive accuracy significantly faster 
confirm observation holte commonly data sets simple rules perform reasonably 
general may say grow outperforms rep rep better rep grow fairly general theory rep appropriate underlying theory specific 
introduced new method integrating pruning learning incremental reduced error pruning rep 
system builds ideas introduced brunk pazzani cohen improves ways efficiency cohen shown complexity rep omega gamma random data proposed alternative algorithm grow time complexity omega gamma log 
complexity algorithm due new method integrating pruning learning order log experiments confirm significant run time improvement rep grow rep uses expensive pruning operator 
separate conquer strategy section argued separate andconquer strategy relational learning algorithms may lead problems overfitting noisy data 
algorithm avoids problem rules pruned right generated 
immediately adjusted right level generality learning subsequent clauses disturbed influence overly specific clause 
split training data performance algorithms depends reasonable split training set growing pruning set 
irep prone problem method redistributing examples clause learned may help stabilize behavior algorithm 
rep efficiency stems tight integration post pruning 
algorithm learns clause worse empty clause learning stops 
may cause algorithm generalize domains specific concept description schaffer 
similar problems may occur small training sets 
near rep adapted capable dealing numeric data multi valued classes allow test broader variety realworld domains cohen 
direct comparison results reported possible cohen experiments performed propositional learning algorithm experiments relations equality available background knowledge learners 
experiments cohen grow delete literal operator rep indicate usage may result increase runtime surprisingly decrease accuracy 
delete literal operator rep improve predictive accuracy remains investigated speed algorithm case 
research sponsored austrian fonds zur der wissenschaftlichen forschung fwf number tec 
financial support austrian research institute artificial intelligence provided austrian federal ministry science research 
colleagues pfahringer ch 
help improvements prolog implementation foil 
ali pazzani kamal ali michael pazzani 
hydra relational concept learning algorithm 
proceedings thirteenth joint international conference artificial intelligence pages chamb ery france 
bratko kononenko ivan bratko igor kononenko 
learning diagnostic rules incomplete noisy data 
phelps editor interactions ai statistical methods pages london 
breiman breiman friedman olshen stone 
classification regression trees 
wadsworth brooks pacific grove ca 
brunk pazzani clifford brunk michael pazzani 
investigation noise tolerant relational concept learning algorithms 
proceedings th international workshop machine learning pages evanston illinois 
cestnik cestnik igor kononenko ivan bratko 
assistant knowledge elicitation tool sophisticated users 
ivan bratko nada lavrac editors progress machine learning pages england 
sigma press 
clark boswell peter clark robin boswell 
rule induction cn improvements 
proceedings th european working session learning pages porto portugal 
clark niblett peter clark tim niblett 
cn induction algorithm 
machine learning 
cohen william cohen 
efficient pruning methods separate conquer rule learning systems 
proceedings th international joint conference artificial intelligence pages chambery france 
muggleton stephen muggleton 
application inductive logic programming finite element mesh design 
stephen muggleton editor inductive logic programming pages 
academic press london 
dzeroski bratko saso dzeroski ivan bratko 
handling noise inductive logic programming 
proceedings international workshop inductive logic programming tokyo japan 
esposito esposito donato malerba giovanni semeraro 
decision tree pruning search state space 
proceedings european conference machine learning pages vienna austria 
springerverlag 
furnkranz johannes furnkranz 
fossil robust relational learner 
proceedings european conference machine learning catania italy 
springer verlag 
furnkranz johannes furnkranz 
top pruning relational learning 
technical report tr austrian research institute artificial intelligence 
holte robert holte 
simple classification rules perform commonly datasets 
machine learning 
lavrac dzeroski nada lavrac saso dzeroski 
inductive learning relations noisy examples 
stephen muggleton editor inductive logic programming pages 
academic press london 
mingers john mingers 
empirical comparison pruning methods decision tree induction 
machine learning 
muggleton stephen muggleton michael bain jean hayes michie donald michie 
experimental comparison human machine learning formalisms 
proceedings th international workshop machine learning pages 
muggleton stephen muggleton editor 
inductive logic programming 
academic press london 
pagallo haussler pagallo david haussler 
boolean feature discovery empirical learning 
machine learning 
quinlan john ross quinlan 
simplifying decision trees 
international journal man machine studies 
quinlan john ross quinlan 
learning logical definitions relations 
machine learning 
quinlan john ross quinlan 
programs machine learning 
morgan kaufmann san mateo ca 
schaffer cullen schaffer 
overfitting avoidance bias 
machine learning 

