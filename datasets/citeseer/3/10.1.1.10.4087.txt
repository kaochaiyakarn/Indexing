
personalized focused web spiders michael chau chen department management information systems university arizona tucson arizona usa size web continues grow searching useful information increasingly difficult 
researchers studied different ways search web automatically programs known spiders crawlers web robots web agents chapter review research area case studies suggest research directions 
number indexable pages world wide web exceeded growing substantial rate 
increasingly difficult retrieve information web 
address problem programs built automatically retrieve web pages 
programs known variety names spiders crawlers web robots web agents worms 
term spiders chapter 
web spiders defined software programs traverse world wide web information space hypertext links retrieving web documents standard protocol 
broader definition include software automatically retrieves web documents standard protocol hypertext links methods 
include programs metasearch spiders spiders connect different search engines combine results 
remainder chapter discussion accepts broader definition 
web robots chatterbots generally considered spiders 
research spiders began early shortly world wide web attract increasing traffic attention 
written claimed spider web 
different versions spiders developed studied 
overview web spider research subsection 
web spider research general web spider research directions classified categories 
speed efficiency 
category researchers study different ways increase harvest speed spider 
projects focus building fast spiders scaled large collections applying program optimization chau chen techniques operations procedures ip address lookup 
mercator internet archive crawler google crawler examples 
currently sophisticated spiders download documents day single workstation 

spidering policy 
research category studies behaviors spiders impacts individuals web 
designed polite spider avoid overloading web servers 
webmasters web page authors able specify want exclude particular spiders access 
standard ways 
called robot exclusion protocol allows web site administrators indicate specifying file named robots txt web site root directory parts site visited robot 
second method usually known robots meta tag web page authors indicate visiting robots document may indexed extract links 
standards strictly enforced commercial spiders reported follow 
studies survey standards web sites investigate potential impacts 

information retrieval 
web spider research fall category 
studies investigate different spidering algorithms heuristics spiders retrieve relevant information web effectively 
studies apply web spiders techniques shown effective traditional information retrieval applications vector space model 
chapter focus mainly research category 
applications web spiders spiders shown useful various web applications 
main areas spiders widely 
personal search 
personal spiders try search web pages interest particular user 
spiders usually run client machine computational power available search process functionalities possible 
discussed detail sect 

building collections 
web spiders extensively collect web pages needed create underlying index search engine 
addition building search engines spiders collect web pages processed serve purposes 
example bharat broder spider crawl yahoo hierarchy create lexicon words henzinger mercator random url sampling web spiders collect email addresses resumes web 
details type spider sect 

archiving 
projects tried archive particular web sites web 
meet challenge enormous size web 
personalized focused web spiders fast powerful spiders developed download targeted web sites storage tapes 

web statistics 
large number pages collected spiders provide useful interesting statistics web 
statistics include total number servers web average size html document number urls return page response 
statistics useful different web related research projects spiders designed primarily purpose 
analysis web content structure research different ways representing analyzing content structure web important web spiders may need rely information guide searches 
section different analysis techniques relevant web spider research reviewed 
general web analysis techniques classified main categories content approaches link approaches :10.1.1.40.3107
content approaches actual html content web page analyzed induce information page 
example body text web page analyzed determine page relevant target domain 
indexing techniques extract key concepts represent page 
addition relevance page determined looking title 
words phrases appear title headings html structure usually assigned higher weight 
domain knowledge incorporated analysis improve results 
example words checked list domain specific terminology 
web page containing words list considered relevant 
url address web page contains useful information page 
example url com homepages tell url comes domain com related topic lung cancer 
know page comes com site may considered authoritative pages gov site 
metrics consider urls fewer slashes useful slashes 
link approaches attracted increasing attention years web link structure come infer important information pages 
basic assumption author web page places link web page believes relevant similar quality 
term links indicate hyper links pointing page 
usually larger number links better page considered 
rationale page referenced people important page chau chen seldom referenced 
similar citation analysis cited article considered better cited 
link analysis applications years 
link analysis techniques guide searching spider applications 
focused crawler hyperlinks information enhance web page classification clustering respectively 
link analysis applied identify cyber communities web groups individuals share common interest web pages popular 
analyzing pages containing link interest possible obtain anchor text describes link 
anchor text underlined clickable text outgoing link web page 
anchor text may provide description target page represents people actual description page 
studies tried anchor text text nearby predict content target page 
studies analyze text appears near hyper link 
addition reasonable give link authoritative source yahoo higher weight link unimportant personal homepage 
researchers developed algorithms address issue 
pagerank hits widely 
pagerank algorithm computes page score weighting link page proportionally quality page containing link 
quality referring pages determined pagerank 
pagerank page calculated recursively follows damping factor number outgoing links intuitively web page high pagerank page linked pages scores higher referring pages pages pages high pagerank scores 
illustrated fig 

interesting note pagerank algorithm follows random walk model pagerank page proportional probability random surfer clicking random links arrive page 
pagerank algorithm applied commercial search engine google shown effective ranking search results 
computation time major problem pagerank 
pagerank score web page calculated iteratively making computationally expensive 
kleinberg proposed hits hyper link induced topic search algorithm similar pagerank 
hits algorithm authority pages de 
personalized focused web spiders fig 

pagerank hits pagerank score page depends pagerank scores pages pointing 
hits algorithm authority score page depends hub scores pages pointing hub score page depends authority scores pages pointing fined high quality pages related particular topic search query 
hub pages necessarily authority provide pointers authority pages 
page point authority page points hub 
intuition scores calculated hits algorithm web page authority score hub score illustrated fig 

calculated follows page high authority score pointed hubs page high hub score points authorities 
example applies hits algorithm clever search engine achieves higher user evaluation manually compiled directory yahoo 
search engine uses similar algorithm ranking process 
bharat henzinger added extensions basic hits algorithm regulating node relevance influences neighbors 
similarly pagerank algorithm drawback hits algorithm high com chau chen putational requirement hub authority scores calculated iteratively 
graph traversal algorithms traditional graph search algorithms extensively studied field computer science 
researchers view web directed graph set nodes pages connected directed edges hyper links algorithms applied web applications 
section review categories graph search algorithms relevant study uninformed search informed search parallel search :10.1.1.40.3107
category graph search algorithms consists simple algorithms breadth search depth search 
known uninformed search information guide search process 
breadth search popular methods web search spiders collect pages current level proceeding level 
algorithms easy implement different applications usually efficient simplicity 
second category informed search information search node available search process 
information heuristics guide search 
best search example widely 
best search explores promising node step 
class algorithms studied different search engine spiders search agent systems different variations 
different metrics number links pagerank score keyword frequency similarity search query guiding heuristics 
category parallel search 
algorithms category try explore different parts search space parallel 
example spreading activation algorithm artificial neural network models tries achieve human performance modeling human nervous system 
neural network graph active nodes neurons connected weighted links synapses 
neural network uses spreading activation nodes represent retrieve concepts knowledge 
example genetic algorithms increasingly optimization problems financial portfolio optimization resource allocation 
genetic algorithm evolutionary process designed search optimal solution crossover mutation operations 
gordon provides model genetic algorithms textual analysis 
chen extended model web spider 
algorithms powerful traditional information retrieval applications widely applied web applications 
web spiders personal search 
personalized focused web spiders web spiders developed help individual users search useful information web 
spiders usually run client machine cpu time memory allocated search process functionalities possible 
tools allow users control personalization options search process 
personal web spiders prominent early example personal web spiders 
users enter keywords specify depth width search links contained current homepages displayed request spider agent fetch homepages connected current homepage 
uses fish search algorithm modified best search method 
powerful personal spiders developed 
spiders designed provide additional functionalities 
tk www robot program integrated browser 
dispatched browser search web neighborhoods find relevant pages returns list links look promising 
sphinx spider written java allows users perform breadth search view search results dimensional graph 
ci spider performs linguistic analysis clustering search results 
collaborative spider extended version ci spider multiagent system designed improve search effectiveness sharing relevant search sessions users 
studies spiders advanced algorithms search process 
itsy spider searches web best search genetic algorithm approach 
url modeled individual initial population 
crossover defined extracting urls pointed multiple starting urls 
mutation modeled retrieving random urls yahoo 
genetic algorithm optimization process suited finding best web pages particular criteria 
spider applies genetic algorithms 
advanced search algorithms personal spiders 
yang apply hybrid simulated annealing personal spider application 
focused crawler locates web pages relevant pre defined set topics example pages provided user 
addition analyzes link structures web pages collected 
context focused crawler uses naive bayesian classifier guide search process 
relevance feedback applied spiders 
commercial web spiders available 
webminer teleport examples software help users download specified files web sites 
excalibur internet spider collect monitor index information text documents web graphic files 
autonomy products support wide range information collection analysis tasks chau chen include automatic searching monitoring information sources internet corporate intranets classifying documents categories predefined users domain experts 
verity knowledge management products agent server information server intelligent classifier perform similar tasks integrated manner 
important category personal spiders composed meta spiders programs connect different search engines retrieve search results 
lawrence giles showed best search engine covered web sites 
combining results different search engines achieves comprehensive coverage 
metacrawler meta spider reported 
provides single interface allowing users search simultaneously different search engines lycos webcrawler infoseek galaxy open text yahoo 
metacrawler search options available service 
success metacrawler metasearch services developed 
profusion allows users choose list search engines queried 
com connects different search engines 
provides metasearch service news usenet white pages yellow pages images addition web pages 
savvysearch connects large number general topic specific search engines selects return useful results past performance 
similarly yu link analysis decide appropriate search engines chen soo domain ontology meta search agents assist users query formulation 
blue squirrel software connect search engines monitor web pages changes schedule automatic search 
grouper extension metacrawler clusters search results various search engines suffix tree clustering algorithm 
addition getting list urls summaries returned search engines meta spiders download analyze documents result set 
inquirus known neci metasearch engine downloads actual result pages generates new summary page search query 
pages longer available dead links longer contain search terms filtered search results 
meta spider provides functionalities inquirus performs linguistic analysis clustering search results 
similar system performs hierarchical graph classification result set 
focused crawler search results popular search engines expand result set urls 
dwork proposed markov chain combine search results different search engines achieved promising experimental results 
search spiders developed basis peer peer technology success applications napster 
jxta search known uses gnutella backbone links computers request received user 
request 
personalized focused web spiders passed neighboring computers see fulfil request 
computer strategy respond request 
case study section architecture search agents enhanced analysis capabilities 
competitive intelligence spider ci spider search agent collects web pages real time basis web sites specified user performs indexing categorization analysis provide user comprehensive view web sites interest 
second tool meta spider functionalities similar ci spider performing breadth search particular web site connects different search engines internet integrates results 
architecture ci spider meta spider shown fig 

main components user interface internet spiders arizona noun phraser self organizing map som :10.1.1.40.3107
components unit perform web search analysis 
arizona noun phraser developed university arizona indexing tool index key phrases appear document collected web internet spiders 
extracts noun phrases document part speech tagging linguistic rules 
som employs artificial neural network algorithm automatically cluster web pages collected different regions dimensional map 
document represented input vector keywords dimensional grid output nodes created 
network trained documents submitted network clustered different regions 
region labeled phrase key concept accurately representing cluster documents region 
important concepts occupy larger regions similar concepts grouped neighborhood 
map displayed user interface user view documents region clicking 
separate experiments conducted evaluate ci spider meta spider 
experimental tasks designed permit evaluation combination functionalities performed identifying major themes related certain topic searched 
undergraduate subjects recruited participate experiment 
experiment ci spider compared lycos manual browsing searching 
experimental results showed precision recall rates ci spider significantly higher lycos significance level 
ci spider usability achieved statistically higher value lycos site browsing searching 
second experiment meta spider compared metacrawler northernlight 
terms precision meta spider performed better difference northernlight statistically significant 
meta spider recall rate comparable metacrawler better northernlight 
chau chen fig 

architecture ci spider meta spider suggest main reason high precision rate ci spider meta spider ability fetch verify content web page real time 
means spiders ensure page shown user contains keyword searched 
hand indexes lycos northernlight search engines outdated 
high recall rate ci spider mainly attributable exhaustive searching characteristic 
lycos showed lowest recall rate commercial search engines samples number web pages web site missing pages contain search keyword 
user performing manual site browsing searching important pages process mentally exhausting 
subjects commented liked capabilities arizona noun phraser som 
web spiders create specialized search engines web search engines google altavista northernlight excite lycos infoseek popular way look information web 
users web activities submitting query search engine 
search engines rely spiders collect web pages processed create underlying search indexes 
examples include altavista google lycos rex excite 
search engine spider research began early world wide web worm widely search engine spider indexed document title header 
repository software engineering spider spider full text indexing capabilities 
soon fulltext indexing spiders developed including webcrawler lycos 
personalized focused web spiders harvest 
spiders follow simple breadth search algorithm widely spiders major commercial generalpurpose search engines crawl web 
research studied incremental spider tries collect web pages changed 
specialized search engines enormous size web general purpose search engines longer satisfy needs searching specific information 
specialized search engines built address various problems 
search engines specialize particular web site topics computers medicine languages chinese japanese file types images research papers 
size manageable smaller entire web search engines usually provide precise results customizable functions 
instance specializes searching building industry domain searches educational resources specializes searching legal information web 
content type specific search engines 
example searches news articles webseek searches image files 
general purpose search engines search engines rely spiders collect web pages 
task relatively easy site specific search engines spiders restricted downloading web pages domain name url 
example spiders instructed discard url starting string www arizona edu 
task difficult specialized search engines spiders need address main problems 
spiders need identify list unvisited urls ones contain relevant information 
improve efficiency spider visit web pages 

downloaded document spiders need determine relevance specific purpose 
spiders avoid irrelevant documents determining quality reputation document 
kind search engine spider known focused spider focused crawler 
years different focused search engine spiders developed evaluated 
focus research investigating efficient spidering algorithms aim address problem 
focused spidering algorithms specialized search engines despite simplicity breadth search widely specialized search engines primarily easy implement fast execute 
intuitively url relevant target domain web pages neighborhood relevant 
shown breadth search discover chau chen high quality pages early spidering process 
important pages links pointing numerous hosts links usually early search process 
people choose free tools webglimpse alkaline ht dig greenstone build specialized search engines 
building site specific search engines difficult building topic specific search engines heuristic locating identifying relevant web pages 
alleviate problem users usually allowed specify maximum depth spider explore 
tactic usually inadequate results collections diverse specific topics 
best search widely algorithm focused spiders 
depending application different heuristics keyword frequency similarity starting examples number links pagerank score 
cho shown experiment stanford web site best search spider pagerank score performed better breadth search spider search spider number links 
chakrabarti combined similarity link analysis focused crawler visits urls page probability having relevant content 
machine learning techniques applied search engine spiders 
mccallum reinforcement learning design spider traverses web immediate reward measured terms web page relevance 
spider cora computer science research search engine 
case study seeking combine different web content structure analysis techniques traditional graph search techniques build spider programs vertical search engines developed compared versions web spiders breadth search spider pagerank spider hopfield net spider :10.1.1.40.3107
section describe designs approaches adopted study 
breadth search spider bfs spider collects web pages current level proceeding level 
words visits urls order discovered 
implemented queue generally breadth search applications 
runs required number pages collected 
pagerank spider adapted algorithm reported 
aiming combine link analysis heuristics traversal algorithm designed perform best search pagerank described earlier heuristics 
urls higher pagerank scores visited earlier 
step spider gets url highest pagerank score fetches content extracts enqueues outgoing links page 
process runs required number pages collected 
pagerank score calculated iteratively algorithm described earlier convergence reached 
damping factor set implementation 
hot queue 
personalized focused web spiders approach original study adopted pagerank spider anchor text analysis 
priority queues established hot queue normal queue 
urls queue ordered pagerank score descending order 
spider dequeues hot queue 
hot queue empty spider dequeues normal queue 
design url placed hot queue anchor text pointing url contains relevant term 
hopfield net spider web viewed large network structure massive distributed knowledge composed pages hyperlinks contributed web page authors 
viewed neural network nodes represented pages links simply represented hyperlinks 
approach model web hopfield net single layered weighted neural network 
nodes activated parallel activation values different sources combined individual node activation scores nodes network reach stable state convergence 
spreading activation algorithm shown effective knowledge retrieval discovery hopfield net developed hopfield net spider perform searching web 
approach aimed combine parallel search algorithm content link analysis 
implementation incorporated basic hopfield net spreading activation idea significant modification take consideration unique characteristics web 
hopfield net spider starts set seed urls represented nodes activates neighboring urls combines weighted links determines weights newly discovered nodes 
process repeats required number urls visited 
algorithm adopted follows 
initialization seed urls 
initial set seed urls system represented node weight defined weight node iteration spider fetches analyzes seed web pages iteration 
new urls pages added network 

activation weight computation iteration 
proceeding iteration weight node calculated follows weight link nodes sigmoid transformation function normalized weight value 
weight linkage urls tries estimate url pointed web page relevant target chau chen domain anchor text score calculated function number anchor text words relevant target domain 
slightly modified sigmoid function sure resulting value positive number 
weights nodes current iteration calculated spider needs decide node url activated visited 
weights decide order urls visited critical effectiveness algorithm 
set nodes current iteration visited fetched web descending order weight 
order filter low quality urls nodes weight smaller threshold visited 
activation process illustrated fig 

fig 

spreading activation starting set seed urls hopfield net spider activates neighboring urls combines weighted links determines weights newly discovered nodes 
nodes low weight node node discarded pages weight greater visited downloaded weight node new iteration updated reflect quality relevance downloaded page content follows 
personalized focused web spiders weight represents relevance textual content page score function number phrases page content relevant target domain 
page relevant phrases receive higher score 
phrases extracted page arizona noun phraser 

stopping condition 
process repeated required number web pages collected average weight nodes iteration maximum allowable error small number 
different approaches implemented backend spiders medical search engine called 
simulation experiment user study carried evaluate precision execution time spiders 
experiment results show hopfield net spider significantly better precision bfs spider pagerank spider level 
bfs spider significantly better pagerank spider level 
execution time hopfield net spider slightly time bfs spider pagerank spider spent times execution time spiders 
past decade web spiders evolved simple breadth search spiders intelligent adaptive spiders 
time size web grown times web hosts june hosts february 
content web diverse terms topic language file type encoding method dynamically generated web pages 
locating desired information web easy despite availability various search spiders search engines 
spiders improved extended ways currently spiders index static web pages 
amount dynamic content web increases spiders need able retrieve manipulate dynamic content autonomously 
spiders perform better indexing applying computational linguistic analysis extract meaningful entities mere keywords web pages 
interesting issue semantic web mature 
quality credibility web pages vary considerably spiders need advanced intelligent techniques distinguish bad pages 
chau chen ideal personal spider behave human librarian tries understand answer user queries autonomous interactive process natural language 
witnessed state art search services webcrawler lycos introduced decade ago surpassed services google utilize newer algorithms 
web continues evolve spiders search engines evolve order accommodate size dynamics web 
ci spider meta spider projects funded part nsf digital library initiative pi chen high performance digital library systems information retrieval knowledge management iis april march nih nlm pi chen umls enhanced dynamic agents manage medical knowledge lm february january nsf cise css pi chen intelligent cscw workbench analysis visualization agents iis june june 
current previous members artificial intelligence lab university arizona contributed projects 
appendix urls spiders search engines com www com altavista www altavista com autonomy www autonomy com blue squirrel www com www com www com www com www com www com excalibur www com excite www excite com google www google com ai bpa arizona edu infoseek infoseek go com jxta search search jxta org www com lycos www lycos com metacrawler www metacrawler com northernlight www northernlight com savvysearch www savvysearch com www com verity www verity com webseek www ctr columbia edu webseek yahoo www yahoo com 
personalized focused web spiders amitay common hypertext links identify best phrasal description target web documents 
proc 
st acm sigir post conference workshop hypertext information retrieval web melbourne australia arasu cho garcia molina paepcke raghavan searching web 
acm transactions internet technology armstrong freitag joachims mitchell webwatcher learning apprentice world wide web :10.1.1.40.3107
proc 
aaai spring symposium information gathering heterogenous distributed environments stanford california usa balabanovic shoham learning information retrieval agents experiment web browsing 
proc 
aaai spring symposium information gathering heterogenous distributed environments stanford california usa belew adaptive information retrieval connectionist representation retrieve learn documents 
proc 
th acm sigir conference research development information retrieval cambridge massachusetts usa ben shaul maarek pelleg ur adding support dynamic focused search 
proc 
th world wide web conference toronto may berners lee weaving web 
harper san francisco bharat broder technique measuring relative size overlap public web search engines 
proc 
th international world wide web conference brisbane australia bharat henzinger improved algorithms topic distillation hyperlinked environment 
proc 
st conference research development information retrieval melbourne australia bowman danzig manber schwartz scalable internet resource discovery research problems approaches 
communications acm brin page anatomy large scale hypertextual web search engine 
proc 
th international world wide web conference brisbane australia broder kumar maghoul raghavan rajagopalan stata tomkins wiener graph structure web 
proc 
th international world wide web conference amsterdam netherlands may burner crawling eternity building archive world wide web 
web techniques chau chen chakrabarti dom kumar raghavan rajagopalan tomkins gibson kleinberg mining web link structure 
ieee computer chakrabarti joshi enhanced topic distillation text markup tags hyperlinks 
proc 
th acm sigir conference research development information retrieval new orleans louisiana usa sep chakrabarti van den berg dom focused crawling new approach topic specific web resource discovery 
proceedings th international world wide web conference toronto canada may chau zeng chen personalized spiders web search analysis 
proc 
st acm ieee joint conference digital libraries virginia usa jun pp 

chau zeng chen huang design evaluation multi agent collaborative web mining system 
decision support systems press 
chen machine learning information retrieval neural networks symbolic learning genetic algorithms 
journal american society information science chen chau zeng ci spider tool competitive intelligence web 
decision support systems press 
chen chung ramsey yang smart itsy spider web 
journal american society information science special issue ai techniques emerging information systems applications chen chung ramsey yang intelligent personal spider agent dynamic internet intranet searching 
decision support systems chen fan chau zeng meta searching categorization web 
journal american society information science technology chen houston sewell schatz internet browsing searching user evaluations category map concept space techniques 
journal american society information science special issue ai techniques emerging information systems applications chen ng algorithmic approach concept exploration large knowledge network automatic thesaurus consultation symbolic branch bound search vs connectionist hopfield net activation 
journal american society information science 
chen internet categorization search self organizing approach journal visual communication image representation chen soo ontology information gathering agents :10.1.1.40.3107
proc 
st asia pacific conference web intelligence city japan oct pp 

internet agents spiders brokers bots new riders publishing indianapolis indiana usa cho garcia molina evolution web implications incremental crawler 
proc 
th international conference large databases vldb cairo egypt cho garcia molina page efficient crawling url ordering 
proc 
th international world wide web conference brisbane australia apr smeaton information discovery internet 
ieee intelligent system jul aug 
personalized focused web spiders debra post information retrieval world wide web making client searching feasible 
proc 
international world wide web conference geneva switzerland diligenti coetzee lawrence giles gori focused crawling context graphs 
proc 
th international conference large databases vldb cairo egypt pp 
doorenbos etzioni weld scalable comparison shopping agent world wide web 
proc 
international conference autonomous agents agents marina del rey california usa feb pp 
indexing aids corporate websites robots txt meta tags 
information processing management dwork kumar sivakumar rank aggregation methods web 
proc 
th international world wide web conference hong kong may eichmann spider balancing effective search web load 
proc 
st international world wide web conference geneva switzerland eichmann ethical web agents 
proc 
nd international world wide web conference chicago illinois usa flake lawrence giles coetzee self organization web identification communities 
ieee computer gauch wang gomez profusion intelligent fusion multiple different search engines 
journal universal computer science gordon probabilistic genetic algorithms document retrieval 
communications acm gray internet growth statistics credits background 
online 
available www mit edu people net background html haveliwala efficient computation pagerank 
stanford university technical report 
available stanford edu pub henzinger hyperlink analysis web 
ieee internet computing :10.1.1.40.3107
henzinger heydon mitzenmacher najork near uniform url sampling 
proc 
th international world wide web conference amsterdam netherlands may heydon najork performance limitations java core libraries 
proc 
acm java grande conference jun pp 

heydon najork mercator scalable extensible web crawler 
world wide web dec hopfield neural network physical systems collective computational abilities 
proc 
national academy science usa 
howe dreilinger savvysearch meta search engine learns search engines query 
ai magazine kahle preserving internet 
scientific america mar 
kleinberg authoritative sources hyperlinked environment 
proc 
th acm siam symposium discrete algorithms san francisco california usa jan pp 

kohonen self organizing maps springer berlin koster standard robot exclusion 
online 
available www org wc html kumar raghavan rajagopalan tomkins trawling web emerging cyber communities 
proc 
th world wide web conference toronto may kwok neural network probabilistic information retrieval 
proc 
th acm sigir conference research development information retrieval cambridge massachusetts usa jun pp 
chau chen lawrence giles inquirus neci meta search engine 
proc 
th international world wide web conference brisbane australia apr lawrence giles context page analysis improved web search 
ieee internet computing jul aug 
lawrence giles accessibility information web 
nature lin chen nunamaker verifying proximity size hypothesis selforganizing maps 
journal management information systems lyman varian information 
online 
available www 
sims berkeley edu info manber smith gopal webglimpse combining browsing searching 
proc 
usenix annual technical conference anaheim california jan mauldin lycos design choices internet search service 
ieee expert mauldin spidering report :10.1.1.40.3107
report distributed indexing searching workshop cambridge massachusetts usa may mcbryan wwww tools taming web 
proc 
st international world wide web conference geneva switzerland mccallum nigam rennie seymore machine learning approach building domain specific search engines 
proc 
international joint conference artificial intelligence ijcai pp 
michalewicz genetic algorithms data structures evolution programs 
springer berlin miller bharat sphinx framework creating personal site specific web crawlers 
proceedings th international world wide web conference brisbane australia apr najork wiener breadth search crawling yields high quality pages 
proceedings th international world wide web conference hong kong may web server survey 
online 
available www com survey reports nick web search genetic algorithm 
ieee internet computing pearl heuristics intelligent search strategies computer problem solving 
addison wesley publishing reading massachusetts usa pinkerton finding people want experiences webcrawler 
proc 
nd international world wide web conference chicago illinois usa pirolli pitkow rao silk sow ear extracting usable structures web 
proc 
acm conference human factors computing systems vancouver canada apr rennie mccallum reinforcement learning spider web efficiently 
proc 
th international conference machine learning icml bled slovenia pp 
salton look automatic text retrieval systems 
communications acm selberg etzioni multi service search comparison metacrawler 
proc 
th world wide web conference boston ma usa december selberg etzioni metacrawler architecture resource aggregation web 
ieee expert jan feb smith chang visually searching web content 
ieee multimedia spertus parasite mining structural information web 
proc 
th international world wide web conference santa clara california usa apr 
personalized focused web spiders robot browsing 
proc 
nd international world wide web conference chicago illinois usa sumner yang dempsey interactive www search engine userdefined collections 
proc 
rd acm conference digital libraries pittsburgh pennsylvania usa jun pp 
ht dig group 
online 
available www org html tolle chen comparing noun phrasing techniques medical digital library tools 
journal american society information science special issue digital libraries fuzzy rule agent web retrieval filtering 
proc 
st asia pacific conference web intelligence city japan oct pp 
waterhouse kan distributed search networks 
ieee internet computing weiss velez sheldon hierarchical network search engine exploits content link hypertext clustering :10.1.1.40.3107
proceedings acm conference hypertext washington dc usa weizenbaum eliza computer program study natural language communication man machine 
communication acm witten bainbridge greenstone open source dl software :10.1.1.40.3107
communications acm witten mcnab bainbridge greenstone comprehensive open source digital library software system 
proc 
th acm conference digital libraries san antonio texas usa pp 
whinston artificial intelligence addison wesley publishing reading massachusetts second edition yang yen chen intelligent internet searching agent hybrid simulated annealing 
decision support systems yu meng liu efficient effective metasearch text databases incorporating linkages documents 
proc 
acm sigmod international conference management data dallas texas may pp 
zamir etzioni grouper dynamic clustering interface web search results 
proc 
th world wide web conference toronto may 
