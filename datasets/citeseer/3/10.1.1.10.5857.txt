file system workload analysis large scale scientific computing applications feng wang qin xin bo hong scott brandt ethan miller darrell long storage systems research center university california santa cruz santa cruz ca elm darrell cs ucsc edu tel fax development environment group integrated computing communications lawrence livermore national laboratory livermore ca llnl gov tel fax parallel scientific applications require high performance support underlying file systems 
comprehensive understanding expected workload essential design high performance parallel file systems 
re examine workload characteristics parallel computing environments light technology advances new applications 
analyze application traces cluster hundreds nodes 
average application typical request sizes 
large requests kilobytes megabytes common 
applications small requests account requests data transferred large requests 
applications show bursty access patterns 
write requests inter arrival times millisecond applications 
running benchmark different file models find write throughput individual output file node exceeds shared file nodes factor 
indicates current file systems optimized file sharing 

parallel scientific applications impose great challenges computational speeds data transfer bandwidths capacities subsystems 
department energy ac published st ieee th nasa goddard conference mass storage systems technologies april college park maryland usa 
strategic computing initiative asci projects computers rates gigabytes second storage system capacities pb 
projected computing storage requirements estimated gigabytes second pb 
observed widening disparity performance devices processors communication links results growing imbalance computational performance subsystem performance 
reduce eliminate growing performance bottleneck design high performance parallel file systems needs improved meet requirements parallel scientific applications 
success file system designs comes comprehensive understanding workloads generated targeted applications 
early middle significant research efforts focused characterizing parallel workload patterns providing insights parallel system designs 
decade witnessed significant improvements computer hardware including processors memory communication links devices 
time systems scaling match increasing demands computing capability storage capacity 
advance technologies enables new scientific applications 
changes motivate re examine characteristics parallel workloads decade 
research traces system activities typical parallel scientific applications benchmark ior physics simulation running nodes physics simulation running nodes 
study static file system dynamic workload characteristics 
results address questions file sizes 
old 
files opened read written 
sizes 
frequent typical file system operations 
nodes send requests 
request sizes 
forms locality 
caching useful 
nodes share data 
file sharing patterns 
nodes utilize bandwidth 
remainder organized follows brief overview related section 
describe tracing methodology section results section 
conclude section 
related subsystem system performance bottleneck long time 
parallel scientific computing environments high demands bottleneck problem severe 
kotz jain surveyed impacts bottlenecks major areas parallel distributed systems pointed subsystem performance considered levels system design 
previous research showed behavior scientific applications regular predictable 
users attempts adjust access patterns improve performance parallel file systems 
studies file system workload characterizations scientific environments 
shown file access patterns share common properties large file sizes sequential accesses bursty program accesses strong file sharing processes job 
study showed applications combination sequential interleaved access patterns requests single node applications require concurrent accesses observe similar phenomena applications examinations 
pasquale data transfer rates ranges megabytes sec longrunning large scale scientific applications 
demonstrated request burstiness periodic regular 
showed request rate order hundreds requests second similar results 
large majority requests order kilobytes requests order megabytes results differ regard 
previous research mainly investigated scientific workload technology evolved quickly 
observed changes large scale scientific workloads study provided guidelines file system designs thorough understanding current requirements large scale scientific computing 

tracing methodology trace data study collected large linux cluster dual processor nodes lawrence livermore national laboratory llnl 
development version lustre lite employed parallel file system linux kernel variant 

data collection tracing activities large scale distributed file systems challenging 
critical issues minimizing disturbance tracing system behaviors 
commonly method develop trace module intercepts specific system calls dedicated node cluster collects trace data stores local disks 
due time limits chose simpler approach employed utility parameters tuned tracing file related system calls 
trace data written local files 
rely local host file systems buffer trace data 
approach shortcomings intercepts related activities including parallel file system local file system standard input output activities 
results relatively large data footprint 
second utility relies local file system buffer traced data 
buffer scheme works poorly host file system heavy workloads 
scenario host system performance affected frequent os traced data 
utility greatly simplifies tedious data collection process simple shell script 
importantly shortcomings mentioned significant trace col table 
asci linux cluster parameters total nodes ibm compute nodes login nodes gateway nodes metadata server nodes processor nodes pentium total number processors processor speed ghz theoretical peak system performance memory node gb total memory tb total local disk space tb nodes interconnection quadrics switch lection large requests relatively short tracing periods 
discuss section requests large system usually kilobytes megabytes 
bursty period total number os node tens requests second 
trace records generated node second average 
buffering storing data slight impact system performance 
tracing cluster study typical scientific applications 
applications usually composed stages computation phase phase 
typical stage ranges minutes hours 
period node usually generates kilobytes trace data easily buffered memory 

applications traces trace data collected asci linux cluster lawrence livermore national laboratory 
machine currently limited access mode science runs file system testing 
dual processor nodes connected quadrics switch 
dedicated metadata servers nodes gateways accessing global parallel file system 
detailed configuration machine provided table 
traced typical parallel scientific applications july 
total size traces megabytes 
application parallel file system benchmark ior developed llnl 
benchmarking parallel file systems posix hdf interfaces 
basically writes large amount data files reads back verify correctness data 
data set large minimize operating system caching effect 
different file usages collected different benchmark traces named ior ior shared ior stride respectively 
running node cluster 
ior configured assign individual output file node ior shared ior stride share file nodes 
difference traces ior shared allocates contiguous region shared file node ior stride strides blocks different nodes shared file 
second application physics simulation run processes 
application single node gathers large amount data small pieces nodes 
small set nodes write data shared file 
reads executed single file independently node 
application intensive phases restart phase read dominant result dump phase write dominant 
corresponding traces named restart write respectively 
application physics simulation runs nodes 
application individual output file node 
previous application restart phase result dump phase 
corresponding traces referred restart write respectively 

analysis raw trace files required processing easily analyzed 
unrelated system calls signals filtered 
node maintained trace records raw trace application composed hundreds individual files 
merged individual files chronological order 
quadrics switch common clock traced time individual trace files globally synchronized 
analysis request inter arrival time greatly simplified sorting requests chronologically sorted trace file 
understanding file metadata operation characteristics important traces large capture general metadata access patterns 
focus file data characterization section 

workload characteristics characteristics workloads including file distributions request properties 
study distributions file size lifetimes show uniqueness large scale scientific workloads 
focus typical applications described section examine characteristics requests size number read write requests burst distribution requests various nodes 

file distributions collected file distributions file servers asci linux cluster science runs phase 
file server storage capacity terabytes 
file servers dedicated small number large scale scientific applications provides model data storage patterns 
average number files file server server stores terabytes data capacity 
file servers number capacity files similar file servers 
table displays statistic values number capacity files servers including mean standard deviation std 
dev median minimum min maximum max 
presents file size distributions number file capacity 
ranges file sizes sampled byte gigabytes 
partitions merged due space limitations 
observed files kilobytes megabytes percentage files percentage files table 
file numbers capacity file servers number capacity mean gb standard deviation gb median gb minimum gb maximum gb number files capacity files kb kb mb mb kb mb mb mb mb mb gb range file sizes number files capacity files file sizes day wk wk wk wk wk wk wk range file ages file ages 
distribution files size files accounted total capacity 
various file size ranges noticeable megabytes megabytes files bytes range 
divided file lifetimes categories day weeks older 
illustrated files bytes lived weeks weeks files bytes lived day 
lifetime traced system year files lived longer weeks 

request sizes shows cumulative distribution function request sizes request numbers 
ior benchmarks identical request size distributions show 
shown ior unique request size kilobytes 
shows write request size distribution result dump stage physics simulation 
write requests smaller bytes data fraction requests fraction requests fraction requests read num read size write num write size request size bytes write read num read size write num write size request size bytes write num write size request size bytes write ior fraction requests fraction requests read num read size request size bytes restart read num read size write num write size request size bytes restart 
cumulative distribution functions cdf size number requests axis logscale 
read num write num curves indicate fraction requests smaller size axis 
read size write size curves indicate fraction transferred data live requests size smaller value axis 
transferred requests sizes larger megabyte 
turns common pattern scientific applications master node collects small pieces data computing nodes writes data files results huge number small writes 
nodes read write data files large chunks afterward 
read requests result dump stage write requests restart stage ignore write request curves 
show write request distribution restart result dump stages physics simulation 
spikes write num curves indicate major write sizes kilobytes megabytes respectively 
accounts write requests 
data transfered large requests shown figures 
reads dominated small requests kilobytes 
small faction kilobyte requests accounts read data transfer 
similar read distribution read requests contribute data read 

accesses characteristics show accesses characteristics time 
resolution figures second uses resolution seconds 
shows request number distribution request size distribution identical ior due fixed size requests benchmarks 
ior benchmark file node model presents best write performance 
write requests second totaling gigabytes second generated nodes 
ior shared ior stride benchmarks achieve write requests second totaling gigabytes second 
benchmarks shared region shared stride file model respectively 
believe performance degradation caused underlying file consistency protocol 
result somewhat counterintuitive 
shared region file model appears similar file model contiguous regions analogous separate files 
performance comparable 
severe performance degradation implies shared file model optimized scenario 
write node reads back node data soon available 
gaps write read curves sub reflect actual times 
obviously ior benchmark demonstrates better performance seconds model seconds needed dump amount data shared file model 
reads synchronous easily file system read bandwidth read size curve 
ior ior shared benchmarks comparable read performance 
ior stride worst read performance megabytes second nodes 
result surprising stride data layout shared files limits chances large sequential reads 
shows access pattern application 
mentioned write reads restart writes 
ignore requests corresponding figures 
chose resolution seconds unreadable finer time resolutions 
spike write num curve caused activities master node collect small pieces data computing nodes 
peak time nearly requests issued file systems second 
due small request size bytes intensive write phase contributes amounts data data size 
rest application large write requests nodes dominate activities 
requests issued bursty manner 
zooms small region second resolution 
shows sharp activity spikes separated long idleness 
peak time megabytes second data generated nodes 
restart phase read requests dominant 
number data size read requests small compared write phase 
presents access pattern physics application 
demonstrates read performance nearly gigabytes second bandwidth achieved nodes large read size megabytes megabytes 
write activities bursty 
observed write curves similar shapes 
sharp spike followed intensive spikes 
possible file system buffer cache absorbs coming write requests writes 
soon buffer filled rate drops greatly served persistent storage 
number op number op 
number op 
number op 
data size op 
gb read num write num snapshot time sec 
ior number read num write num snapshot time sec 
ior shared number read num write num snapshot time sec 
ior number 
burstiness data size op 
gb data size op 
gb data size op 
gb read size write size snapshot time sec 
ior size read size write size snapshot time sec 
ior shared size read size write size snapshot time sec 
ior size 
requests time ior benchmarks write num write size snapshot time sec 
time write number op 
data size op 
kb data size op 
gb write size snapshot time sec 
snapshot time sec 
time restart time write short read num read size 
requests time application study burstiness measure request inter arrival times shows cumulative distribution functions cdf request inter arrival times 
note axis logarithmic scale 
write activities bursty ior benchmarks application number op 
number op 
read num write num snapshot time sec 
restart num read num write num snapshot time sec 
write num data size op 
gb data size op 
gb read size write size snapshot time sec 
restart size read size write size snapshot time sec 
write size 
requests time application write requests inter arrival times millisecond 
ior write activities due memory dump nodes issue write requests quickly 
write activities intensive ior hand read requests generally intensive write requests reads synchronous 
particular indicates ior shared strided files suffers low read performance described section 
scenario data interleaved shared file read accesses sequential 

nodes section study distributions request sizes numbers nodes shown 
ior benchmarks read writes distributed evenly nodes shown figures node executes sequence operations benchmarks 
physics application small set nodes write gathered simulated data shared file 
nodes significant activity write phase transfered data large write requests write requests shown figures 
little read activity write phase 
read requests evenly distributed nodes restart phase sizes kilobyte shown figures 
little write activity restart phase 
restart write phases physics application activity balanced nodes shown figures 
observe significant write activity restart phase 
fraction requests fraction requests fraction requests read write time ms inter ior fraction requests read write time ms inter write read write time ms 
inter write fraction requests read write time ms 
inter ior fraction requests fraction requests read write time ms inter ior shared read write time ms inter restart read write time ms 
inter restart 
cumulative distribution functions cdf inter arrival time requests axis logscale table 
file open statistics number file opens number data file opens read write read write read write read write ior write restart restart write fraction nodes fraction nodes fraction nodes fraction nodes fraction nodes read write number requests node ior read write number requests node write num number requests read write node restart num number requests read write node write num read write number requests node restart num fraction nodes fraction nodes fraction nodes fraction nodes fraction nodes read write size requests gb node ior read write size requests gb node write size read write size requests kb node restart size read write size requests gb node write size read write size requests gb node restart size 
cumulative distribution functions cdf size requests nodes table 
operations file open avg 
open time avg 
ios open avg 
io size open applications data file data file data file ior sec sec mb mb ior shared sec sec mb mb ior stride sec sec mb mb write sec sec mb mb restart sec sec mb mb restart sec sec mb mb write sec sec mb mb 
file opens section study file open patterns applications 
term data files refer files store results dumped applications 
applications files tend opened read write read 
observe significant write files physics application shown table 
data files opened read write benchmark ior 
open operations data files account small portion file opened 
fact data file operations dominate os small number data file opens implies longer open time operations open 
listed table open duration data files ranges seconds seconds typically times longer file open durations 
average number operations size data files open operation larger files 
example mb data transferred data file open physical application write 

study analyze application traces cluster hundreds nodes 
average application typical request sizes 
large requests kilobytes megabytes common 
applications small requests account requests data transferred large requests 
applications show bursty access patterns 
write requests inter arrival times millisecond applications 
running benchmark different file models find write throughput individual output file node exceeds shared file nodes factor 
indicates current file systems optimized file sharing 
applications os small set files containing inter mediate final computation results 
files tend opened relatively long time seconds seconds 
large amount data transferred open 
acknowledgments feng wang qin xin scott brandt ethan miller darrell long supported part lawrence livermore national laboratory los alamos national laboratory sandia national laboratory contract 
bo hong supported part national science foundation number ccr 
effort auspices department energy university california lawrence livermore national laboratory contract 
eng 
document reviewed released unclassified unlimited distribution llnl conf 
grateful sponsors national science foundation usenix association hewlett packard laboratories ibm research intel microsoft research storage veritas 
wu 
parallel workload characteristics vesta 
proceedings ipps workshop input output parallel distributed systems pages apr 
doe national nuclear security administration doe national security agency 
proposed statement sgs file system apr 
kotz jain 
parallel distributed systems 
kent williams editors encyclopedia computer science technology volume pages 
marcel dekker 
supplement 
kotz 
file system workload scientific multiprocessor 
ieee parallel distributed technology 
lawrence livermore national laboratory 
asci linux cluster 
www llnl gov linux alc 
lawrence livermore national laboratory 
ior software 
www llnl gov icc lc downloads download html 
miller katz 
input output behavior supercomputing applications 
proceedings supercomputing pages nov 
reddy banerjee 
study behavior perfect benchmarks multiprocessor 
proceedings th international symposium computer architecture pages 
ieee 
pasquale polyzos 
static analysis characteristics scientific applications production workload 
proceedings supercomputing pages portland 
ieee 
pasquale polyzos 
dynamic characterization intensive scientific applications 
proceedings supercomputing pages 
ieee 
ellis kotz best 
characterizing parallel file access patterns large scale multiprocessor 
proceedings th international parallel processing symposium ipps pages 
ieee computer society press 
schwan 
lustre building file system node clusters 
proceedings linux symposium july 
smirni chien reed 
requirements scientific applications evolutionary view 
proceedings th ieee international symposium high performance distributed computing hpdc pages 
ieee 
smirni reed 
lessons characterizing input output behavior parallel scientific applications 
performance evaluation international journal june 

