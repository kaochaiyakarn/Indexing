proceedings ijcai pp acapulco mexico august constructing diverse classifier ensembles artificial training examples prem melville raymond mooney department computer sciences university texas university station austin tx melville cs utexas edu mooney cs utexas edu ensemble methods bagging boosting combine decisions multiple hypotheses strongest existing machine learning methods 
diversity members ensemble known important factor determining generalization error 
presents new method generating ensembles directly constructs diverse hypotheses additional artificially constructed training examples 
technique simple general strong learner base classifier build diverse committees 
experimental results decision tree induction base learner demonstrate approach consistently achieves higher predictive accuracy base classifier bagging boosting occasionally decrease accuracy obtains higher accuracy boosting early learning curve training data limited 
major advances inductive learning past decade development ensemble committee approaches learn retain multiple hypotheses combine decisions classification dietterich example boosting freund schapire ensemble method learns series weak classifiers focusing correcting errors previous currently best generic inductive classification methods hastie constructing diverse committee hypothesis different possible decorrelated members ensemble maintaining consistency training data known theoretically important property committee krogh vedelsby successful ensemble methods encourage diversity extent focused directly goal maximizing diversity 
existing methods focus achieving diversity opitz shavlik rosen fairly complex general meta learners bagging breiman boosting applied base learner produce effective committee witten frank new meta learner decorate diverse ensemble creation relabeling artificial training examples uses existing strong learner provides high accuracy training data build effective diverse committee fairly simple straightforward manner 
accomplished adding different randomly constructed examples training set building new committee members 
artificially constructed examples category labels disagree current decision committee easily directly increasing diversity new classifier trained augmented data added committee 
boosting bagging provide diversity sub sampling re weighting existing training examples 
training set small limits amount ensemble diversity methods obtain 
decorate ensures diversity arbitrarily large set additional artificial examples 
hypothesis result higher generalization accuracy training set small 
presents experimental results wide range uci data sets comparing boosting bagging decorate decision tree induction java implementation quinlan introduced witten frank base learner 
cross validated learning curves support hypothesis decorated trees generally result greater classification accuracy small training sets 
ensembles diversity ensemble combination output classifiers useful disagree inputs krogh vedelsby refer measure disagreement diversity ensemble 
methods proposed measure ensemble diversity kuncheva whitaker usually dependent measure accuracy 
regression mean squared error commonly measure accuracy variance measure diversity 
diversity th classifier example defined predictions th classifier ensemble respectively 
setting krogh show generalization error ensemble expressed mean error diversity ensemble respectively 
classification problems loss function commonly measure accuracy diversity th classifier defined case simple linear relationship hold strong reason believe increasing diversity decrease ensemble error cunningham underlying principle approach build ensembles classifiers consistent training data maximize diversity defined 
decorate algorithm definition decorate see algorithm ensemble generated iteratively learning classifier iteration adding current ensemble 
initialize ensemble contain classifier trained training data 
classifiers successive iteration trained original training data artificial data 
iteration artificial training examples generated data distribution number examples generated specified fraction size training set size 
labels artificially generated training examples chosen differ maximally current ensemble predictions 
construction artificial data explained greater detail section 
refer labeled artificially generated training set diversity data 
train new classifier union original training data diversity data 
adding new classifier current ensemble increases ensemble training error reject classifier add current ensemble 
process repeated reach desired committee size exceed maximum number iterations 
classify unlabeled example employ method 
base classifier ensemble provides probabilities class membership pc probability example belonging class classifier compute class membership probabilities entire ensemble pc jc probability belonging class select probable class label argmax construction artificial data generate artificial training data randomly picking data points approximation training data distribution 
numeric attribute compute mean standard deviation training set generate values gaussian distribution defined 
nominal attribute compute probability occurrence distinct value domain generate values distribution 
laplace smoothing nominal attribute values represented training set non zero probability occurrence 
constructing artificial data points simplifying assumption attributes independent 
possible accurately estimate joint probability distribution attributes time consuming require lot data 
iteration artificially generated examples labeled current ensemble 
example find class membership probabilities predicted ensemble replacing zero probabilities small non zero value 
labels selected probability selection inversely proportional current ensemble predictions 
algorithm decorate algorithm input base learning algorithm set training examples ym labels size desired ensemble size max maximum number iterations build ensemble size factor determines number artificial examples generate 

trials 

initialize ensemble fc 
compute ensemble error 
size trials max 
generate size jt training examples distribution training data 
label examples probability class labels inversely proportional predictions 


fc 
remove artificial data 
compute training error step 




fc 
trials trials experimental evaluation methodology evaluate performance decorate ran experiments representative data sets uci repository blake merz similar studies webb quinlan compared performance deco rate adaboost bagging base learner ensemble methods weka implementations methods witten frank ensemble methods set ensemble size 
note case decorate specify maximum ensemble size algorithm terminates number iterations exceeds maximum limit desired ensemble size reached 
experiments set maximum number iterations decorate 
ran experiments varying amount artificially generated data size results vary range 
size values lower adversely affect decorate insufficient artificial data give rise high diversity 
results report size set number artificially generated examples equal training set size 
performance learning algorithm evaluated complete fold cross validations 
fold cross validation data set randomly split segments results averaged trials 
trial segment set aside testing remaining data available training 
test performance varying amounts training data learning curves generated testing system training increasing subsets training data 
summarize results data sets different sizes select different percentages total training set size points learning curve 
compare learning algorithms domains employ statistics webb win draw loss record geometric mean error ratio 
win draw loss record presents values number data sets algorithm obtained better equal worse performance algorithm respect classification accuracy 
report statistically significant win draw loss record win loss counted difference values determined significant level paired test 
geometric mean error ratio defined ea eb ea eb mean errors algorithm domain 
geometric mean error ratio implies algorithm performs better vice versa 
compute error ratios capture degree algorithms perform win loss outcomes 
results results summarized tables 
cell tables presents accuracy decorate versus algorithm 
difference statistically significant larger shown bold 
varied training set sizes total available data points lower learning curve expect see difference algorithms 
bottom tables provide summary statistics discussed points learning curve 
decorate significant wins losses bagging points learning curve see table 
decorate outperforms bagging geometric mean ratio 
suggests cases bagging beats decorate improvement decorate improvement bagging rest cases 
decorate outperforms adaboost early learning curve significant wins draw loss record geometric mean ratio trend reversed data 
note large amounts training data decorate performance quite competitive adaboost data decorate produces higher accuracies data sets 
observed previous studies webb bauer kohavi adaboost usually significantly reduces error base learner occasionally increases large extent 
decorate problem clear table 
data sets decorate achieves higher accuracy bagging adaboost fewer training examples 
show learning curves clearly demonstrate point 
domains little data available acquiring labels expensive decorate advantage ensemble methods 
performed additional experiments analyze role diversity plays error reduction 
ran decorate different settings size ranging varying diversity ensembles produced 
compared diversity ensembles reduction generalization error 
diversity ensemble computed mean diversity ensemble members eq 

compared ensemble diversity ensemble error reduction difference average error ensemble members error entire ensemble cunningham carney 
correlation coefficient diversity ensemble error reduction fairly strong 
furthermore compared diversity base error reduction difference error base classifier ensemble error 
base error reduction gives better indication improvement performance ensemble base classifier 
correlation diversity versus base error reduction 
note correlation weak statistically significant positive correlation 
results reinforce belief increasing ensemble diversity approach reducing generalization error 
determine performance decorate changes ensemble size ran experiments increasing sizes 
compared results training available data advantage decorate noticeable low learning curve 
due lack space include results datasets representative datasets see 
performance datasets similar 
note general accuracy decorate increases ensemble size datasets performance levels ensemble size 
value probability getting correlation large observed value random chance true correlation zero 
number training examples adaboost decorate bagging number training examples adaboost decorate bagging breast iris number training examples adaboost decorate bagging number training examples adaboost decorate bagging labor heart decorate compared adaboost bagging ensemble size breast labor colic iris credit decorate different ensemble sizes related attempts building ensembles focus issue diversity 
liu rosen simultaneously train neural networks ensemble correlation penalty term error functions 
opitz shavlik genetic algorithm search ensemble networks 
guide search objective function incorporates accuracy diversity term 
build ensembles different feature subsets feature selection done hill climbing strategy classifier error diversity 
classifier rejected improvement metrics lead substantial deterioration substantial defined pre set threshold 
approaches ensembles built attempting simultaneously optimize accuracy diversity individual ensemble members 
decorate goal minimize ensemble error increasing diversity 
point training accuracy ensemble go table decorate vs dataset anneal audio autos breast credit glass heart hepatitis colic iris labor lymph segment soybean splice win draw loss sig 
gm error ratio table decorate vs bagging dataset anneal audio autos breast credit glass heart hepatitis colic iris labor lymph segment soybean splice win draw loss sig 
gm error ratio base classifier possibility previous methods 
furthermore previous studies compared methods standard ensemble approaches boosting bagging opitz shavlik compares bagging boosting 
compared boosting requires weak base learner completely fit training data boosting terminates constructs hypothesis zero training error decorate requires strong learner artificial diversity training data may prevent adequately fitting real data 
applying boosting strong base learners appropriately weakened order benefit boosting 
decorate may preferable ensemble meta learner strong learners 
knowledge ensemble approach utilize artificial training data active learning method introduced cohn goal committee select new training examples improve accuracy existing training data 
labels artificial examples selected produce hypotheses faithfully represent entire version space produce diversity 
cohn approach labels artificial data positive negative encourage respectively learning general specific hypotheses 
current approach encouraging diversity artificial training examples 
domains large amount unlabeled data available 
exploit unlabeled examples label diversity data 
allow decorate act form semisupervised learning exploits labeled unlabeled data nigam current study base learner expect similarly results base learners 
decision tree induction commonly base learner ensemble studies neural networks naive bayes bauer kohavi opitz maclin experiments decorating learners area 
manipulating artificial training examples decorate able strong base learner produce effective diverse ensemble 
experimental results demonstrate approach particularly effective producing highly accurate ensembles training data limited outperforming bagging boosting low learning curve 
empirical success decorate raises issue developing sound theoretical understanding effectiveness 
gen table decorate vs adaboost dataset anneal audio autos breast credit glass heart hepatitis colic iris labor lymph segment soybean splice win draw loss sig 
gm error ratio eral idea artificial unlabeled examples aid construction effective ensembles promising approach worthy study 
acknowledgments supported darpa 
bauer kohavi bauer kohavi 
empirical comparison voting classification algorithms bagging boosting variants 
machine learning 
blake merz blake merz 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
breiman leo breiman 
bagging predictors 
machine learning 
cohn cohn atlas ladner 
improving generalization active learning 
machine learning 
cunningham carney cunningham carney 
diversity versus quality classification ensembles feature selection 
th european conference machine learning pages 
dietterich dietterich 
ensemble methods machine learning 
kittler roli editors international workshop multiple classifier systems lecture notes computer science pages 
springer verlag 
freund schapire yoav freund robert schapire 
experiments new boosting algorithm 
proceedings th international conference machine learning july 
hastie trevor hastie robert tibshirani jerome friedman 
elements statistical learning 
springer verlag new york august 
krogh vedelsby krogh vedelsby 
neural network ensembles cross validation active learning 
advances neural information processing systems 
kuncheva whitaker kuncheva whitaker 
measures diversity classifier ensembles relationship ensemble accuracy 
submitted 
liu yao liu yao 
ensemble learning negative correlation 
neural networks 
nigam nigam mccallum thrun mitchell 
text classification labeled unlabeled documents em 
machine learning 
opitz maclin david opitz richard maclin 
popular ensemble methods empirical study 
journal artificial intelligence research 
opitz shavlik opitz shavlik 
actively searching effective neural network ensemble 
connection science 
quinlan ross quinlan 
programs machine learning 
morgan kaufmann san mateo ca 
quinlan ross quinlan 
bagging boosting 
proceedings th national conference artificial intelligence august 
rosen rosen 
ensemble learning decorrelated neural networks 
connection science 
webb webb 
technique combining boosting 
machine learning 
witten frank ian witten eibe frank 
data mining practical machine learning tools techniques java implementations 
morgan kaufmann san francisco 
cunningham cunningham 
diversity preparing ensembles classifiers different feature subsets minimize generalization error 
proceedings european conference machine learning 
