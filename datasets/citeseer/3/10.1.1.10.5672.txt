support vector machines hype 
kristin bennett math sciences department rensselaer polytechnic institute troy ny rpi edu support vector machines svms related kernel methods increasingly popular tools data mining tasks classification regression novelty detection 
goal tutorial provide intuitive explanation svms geometric perspective 
classification problem investigate basic concepts svms examine strengths weaknesses data mining perspective 
overview comprehensive provide resources interested exploring svms 
keywords support vector machines kernel methods statistical learning theory 

explosion number research papers topic support vector machines svms 
svms successfully applied number applications ranging particle identification face identification text categorization engine knock detection bioinformatics database marketing 
approach systematic reproducible properly motivated statistical learning theory 
training involves optimization convex cost function false local minima complicate learning process 
svms known class algorithms idea kernel substitution broadly refer kernel methods 
general svm kernel methodology appears suited data mining tasks 
tutorial motivate primary concepts svm approach examining geometrically problem classification 
approach produces elegant mathematical models geometrically intuitive theoretically founded 
existing new special purpose optimization algorithms efficiently construct optimal model solutions 
illustrate flexibility generality approach examining extensions technique classification linear programming regression novelty detection 
tutorial exhaustive approaches kernel pca density estimation considered 
users interested svms consult thorough treatments books cristianini shawe taylor vapnik books statistical learning theory edited volumes 
readers consult web resources comprehensive current treatment methodology 
colin campbell department engineering mathematics bristol university bristol bs tr united kingdom campbell bristol ac uk conclude tutorial general discussion benefits shortcomings svms data mining problems 
understand power elegance svm approach grasp key ideas margins duality kernels 
examine concepts case simple linear classification show extended complex tasks 
mathematically rigorous treatment geometric arguments 

linear discriminants consider binary classification task datapoints having corresponding labels 
datapoint represented dimensional input attribute space 
classification function sign 
vector determines orientation discriminant plane 
scalar determines offset plane origin 
assuming sets linearly separable exists plane correctly classifies points sets 
infinitely possible separating planes correctly classify training data 
illustrates different separating planes 
preferable 
intuitively prefers solid plane small perturbations point introduce misclassification errors 
additional information solid plane generalize better data 
geometrically characterize solid plane furthest classes 
possible linear discriminant planes construct plane furthest classes 
illustrates approach 
examine convex hull class training data indicated dotted lines find closest points convex hulls circles labeled 
convex hull set points smallest convex set containing points 
construct plane bisects points resulting classifier robust sense 
sigkdd explorations 
volume issue page best plane bisects closest points convex hulls closest points convex hulls solving quadratic problem 
min yi class yi class yi class yi class existing algorithms solving general purpose quadratic problems new approaches exploiting special structure svm problems see section 
notice solution depends boldly circled points 
best plane maximizes margin alternative approach maximize margin parallel supporting planes 
plane supports class points class side plane 
points class label exist depending class label 
suppose smallest value 
argument inside decision function invariant positive rescaling implicitly fix scale requiring 
points class label similarly require find plane furthest sets simply maximize distance margin support planes class illustrated 
support planes pushed apart bump small number data points support vectors class 
support vectors outlined bold circles 
distance margin supporting planes 
maximizing margin equivalent minimizing quadratic program min sigkdd explorations 
copyright acm sigkdd december 
volume issue page class class constraints simplified note solution maximizing margin parallel supporting planes method identical bisecting closest points convex hull method 
maximum margin method supporting planes pushed apart bump support vectors boldly circled points solution depends support vectors 
support vectors determine closest points convex hull 
coincidence solutions identical 
wonderful example mathematical programming concept duality 
lagrangian dual supporting plane qp yields dual qp see derivation min equivalent modulo scaling closest points convex hull qp 
choose solve primal qp dual qp 
yield normal plane support vectors threshold determined 
choose solve primal supporting plane qp problem dual convex hull qp problem give solution 
mathematical programming perspective relatively straightforward problems studied class convex quadratic programs 
effective robust algorithms solving qp tasks 
qp problems convex local minimum identified global minimum 
practice dual formulations preferable simple constraints readily admit extensions nonlinear discriminants kernels discussed sections 

theoretical foundations statistical learning theory perspective qp formulations founded 
roughly statistical learning proves bounds generalization error points training set obtained 
bounds function misclassification error training data terms measure complexity capacity classification function 
linear functions maximizing margin separation discussed reduces function capacity complexity 
explicitly maximizing margin minimizing bounds generalization error expect better generalization high probability 
size margin directly dependent dimensionality data 
expect performance high dimensional data large number attributes 
sense problems caused overfitting high dimensional data greatly reduced 
reader referred large volume literature topic technical discussions statistical learning theory 
gain insight results geometric arguments 
classification functions capacity fit training data overfit resulting poor generalization 
figures illustrate linear discriminant separates classes small margin capacity fit data large margin 
skinny plane take possible orientations strictly separate data 
fat plane limited flexibility separate data 
sense fat margin complex skinny 
complexity capacity linear discriminant function margin separation 
usually think complexity linear function determined number variables 
margin fat complexity function low number variables high 
maximizing margin regulates complexity model 
possible skinny margin planes possible fat margin planes 
linearly inseparable case inseparable data convex hulls intersect far assumed datasets linearly separable 
true strategy constructing plane bisects closest points convex hulls fail 
illustrated points linearly separable convex hulls intersect 
note single bad square removed strategy 
need restrict influence single point 
accomplished reduced convex hulls usual definition convex hulls 
influence point restricted introducing upper bound multiplier point 
formally reduced convex hull defined yi class yi class sigkdd explorations 
copyright acm sigkdd december 
volume issue page sufficiently small reduced convex hulls intersect 
shows reduced convex hulls separating plane constructed bisecting closest points reduced convex hulls 
reduced convex hulls set indicated dotted lines 
best plane bisects reduced convex hulls find closest points convex hulls modify quadratic program separable case adding upper bound multiplier constraint yield min xi xi yi yi yi yi select plane maximize margin minimize error linearly inseparable case primal supporting plane method fail 
qp task feasible linearly inseparable case constraints relaxed 
consider linearly inseparable problem shown 
ideally points misclassified points fall margin 
relax constraints insure point appropriate side supporting plane 
point falling wrong side supporting plane considered error 
want simultaneously maximize margin minimize error 
accomplished minor changes supporting plane qp problem 
nonnegative slack error added constraint added variable weighted penalty term objective follows min show primal relaxed supporting plane method equivalent dual problem finding closest points reduced convex hulls 
lagrangian dual qp task min see formal derivation dual 
commonly svm formulation classification 
note difference qp separable case qp addition upper bounds upper bounds reduced convex hull qp bounds limit influence particular data point 
analogous linearly separable case geometric problem finding closest points reduced convex hulls qp shown equivalent qp task modulo scaling size optimal margin 
point examined linear discrimination linearly separable inseparable cases 
basic principle svm construct maximum margin separating plane 
equivalent dual problem finding closest points reduced convex hulls class 
approach control complexity svms construct linear classification functions theoretical practical generalization properties high dimensional attribute spaces 
robust efficient quadratic programming methods exist solving dual formulations 
linear discriminants appropriate data set resulting high training set errors svm methods perform 
section examine svm approach generalized construct highly nonlinear classification functions 

nonlinear functions kernels example requiring quadratic discriminant consider classification problem 
simple linear discriminant function 
quadratic function circle pictured needed 
classic method converting linear classification algorithm nonlinear classification algorithm simply add additional attributes data nonlinear functions original data 
existing linear classification algorithms applied expanded dataset feature space producing nonlinear functions original input space 
construct quadratic discriminant dimensional vector space attributes simply map original dimensional input space dimensional feature space rs construct linear discriminant space 
specifically define sigkdd explorations 
copyright acm sigkdd december 
volume issue page rs rs resulting classification function sign sign rs linear mapped dimensional feature space quadratic dimensional input space 
high dimensional datasets nonlinear mapping method potential problems stemming fact dimensionality feature space explodes exponentially 
problem overfitting problem 
svms largely immune problem rely margin maximization provided appropriate value parameter chosen 
second concern practical compute svms get issue kernels 
examine happens nonlinear mapping introduced qp 
define need optimize min notice mapped data occurs inner product objective 
apply little mathematically rigorous magic known hilbert schmidt kernels applied svms 
mercer theorem know certain mappings points inner product mapped points evaluated kernel function explicitly knowing mapping popular known kernels 
new kernels developed fit domain specific requirements 
degree polynomial radial basis function machine exp layer neural network sigmoid table examples kernel functions substituting kernel dual svm yields min change linear nonlinear classifier substitute kernel evaluation objective original dot product 
changing kernels get different highly nonlinear classifiers 
algorithmic changes required linear case substitution kernel evaluation simple dot product 
benefits original linear svm method maintained 
train highly nonlinear classification function polynomial radial basis function machine sigmoidal neural network robust efficient algorithms problems local minima 
kernel substitution linear algorithm capable handling separable data turned general nonlinear algorithm 

summary svm method resulting svm method popular form summarized follows 
select parameter representing tradeoff minimizing training set error maximizing margin 
select kernel function kernel parameters 
example radial basis function kernel select width gaussian 

solve dual qp alternative svm formulation appropriate quadratic programming linear programming algorithm 

recover primal threshold variable support vectors 
classify new point follows sign typically parameters step selected crossvalidation sufficient data available 
model selection strategies give reasonable estimate kernel parameter additional validation data 
example consider scheme proposed joachims 
approach number leave errors svm bounded ib solutions optimization task upper bound determine jk 
value kernel parameter leave error estimated quantity system retrained datapoints left bound determined solution 
kernel parameter incremented decremented direction needed lower bound 
model selection approaches scheme increasingly accurate predicting best choice kernel parameter need validation data 
basic svm approach extended variations applied different types inference problems 
different mathematical programming models produced typically require solution linear quadratic programming problem 
choice algorithm solve linear quadratic program critical quality solution 
modulo numeric differences appropriate optimization algorithm produce optimal solution computational cost obtaining solution dependent specific optimization utilized course 
briefly discuss available qp lp solvers section 

algorithmic approaches typically svm approach requires solution qp lp problem 
lp qp type problems extensively studied field mathematical programming 
advantage svm methods prior optimization research immediately exploited 
existing general purpose qp algorithms quasi newton methods primal dual interior point methods successfully solve problems small size thousands points 
existing lp solvers simplex interior points handle problems moderate size hundreds thousands data points 
algorithms suitable original data matrix linear methods kernel matrix needed nonlinear methods longer fits main memory 
larger datasets alternative techniques 
divided categories techniques kernel components evaluated discarded learning decomposition methods evolving subset data new optimization approaches specifically exploit structure svm problem 
category obvious approach sequentially update approach kernel ka algorithm 
variants svm models method easy implement give quick impression performance svms classification tasks 
equivalent hildreth method optimization theory 
fast qp routines especially small datasets 
general methods linear convergence rates may require scans data 
chunking decomposition methods optimize svm respect subsets 
sequentially updating alternative update parallel subset working set data stage 
chunking qp sigkdd explorations 
copyright acm sigkdd december 
volume issue page optimization algorithm optimize dual qp initial arbitrary subset data 
support vectors retained datapoints discarded 
new working set data derived support vectors additional datapoints maximally violate storage constraints 
chunking process iterated margin maximized 
course procedure may fail dataset large hypothesis modeling data sparse non zero say 
case decomposition methods provide better approach algorithms fixed size subset data called working set remainder kept fixed 
smaller qp lp solved working set 
small subproblems solved massive 
successful codes decomposition strategies 
svm codes available online svmtorch svmlight working set strategies 
lp variants particularly interesting 
fastest lp methods decompose problem rows columns solve largest reported nonlinear svm regression problems sixteen points kernel matrix elements 
limiting case decomposition sequential minimal optimization smo algorithm platt optimized iteration 
smallest set parameters optimized iteration plainly constraint iy hold 
remarkably parameters optimized rest kept fixed possible derive analytical solution executed numerical operations 
eliminates need qp solver subproblem 
method consists heuristic step finding best pair parameters optimize analytic expression ensure dual objective function increases monotonically 
smo improved versions proven effective approach large problems 
third approach directly attack svm problem optimization perspective create algorithms explicitly exploit structure problem 
frequently involve reformulations base svm problem proven just effective original svm practice 
keerthi proposed effective algorithm dual geometry finding closest points convex hulls discussed section 
approaches particularly effective linear svm problems 
give examples developments massive linear svm problems 
lagrangian svm method reformulates classification problem optimization problem solves problem algorithm requiring solution systems linear equalities 
eleven line matlab code solves linear classification problems millions points minutes pentium iii 
uses method sherman morrison woodbury formula requires solution systems linear equalities 
technique solve linear svms points 
interior point semi smooth support vector methods ferris munson core algorithms solve linear classification problems data points dimensions 
rapid progress scalability svm approaches 
best algorithms optimization svm objective functions remains active research subject 

svm extensions major advantages svm approach flexibility 
basic concepts maximizing margins duality kernels paradigm adapted types inference problems 
illustrate flexibility examples 
illustrates simply changing norm regularization margin measured produce linear program lp model classification 
second example shows technique adapted unsupervised learning task novelty detection 
third example shows svms adapted regression 
just variations extensions svm approach inference problems data mining machine learning 
lp approaches classification 
common strategy developing new svm methods desirable properties adjust error margin metrics mathematical programming formulation 
quadratic programming possible derive kernel classifier learning task involves linear programming lp 
recall primal svm formulation maximizes margin supporting planes class distance measured norm 
resulting qp minimizing error minimizing norm model changed maximize margin measured infinity norm minimizes error minimizes norm sum absolute values components min sigkdd explorations 
copyright acm sigkdd december 
volume issue page problem easily converted lp problem solvable simplex interior point algorithms 
norm minimized optimal sparse 
attributes dropped receive weight optimal solution 
formulation automatically performs feature selection capacity 
create nonlinear discriminants problem formulated directly kernel feature space 
recall original svm formulation final classification done follows sign directly substitute function lp yield min minimizing obtain solution sparse relatively datapoints support vectors 
furthermore efficient simplex interior point methods exist solving linear programming problems practical alternative conventional qp 
linear programming approach evolved independently qp approach svms see linear programming approaches regression novelty detection possible 
novelty detection real world problems task classify detect novel abnormal instances 
novelty abnormality detection potential applications problem domains condition monitoring medical diagnosis 
approach model support data distribution having find real valued function estimating density data 
simplest level objective create binary valued function positive regions input space data predominantly lies negative 
approach find hypersphere minimal radius center contains data novel test points lie outside boundary hypersphere 
technique outline originally suggested tax duin authors real life applications 
effect outliers reduced slack variables allow datapoints outside sphere 
task minimize volume sphere distance datapoints outside min zi methodology explained svm classification dual lagrangian formed kernel functions substituted produce dual qp task novelty detection min bound examples occur correspond outliers training process 
having completed training process test point declared novel computed finding example setting inequality equality 
alternative approach developed sch lkopf 
suppose restricted attention rbf kernels case data lie region surface hypersphere feature space 
objective separate region surface region containing data 
achieved constructing hyperplane maximally distant origin datapoints lying opposite side origin 
kernel substitution dual formulation learning task involves minimization min sigkdd explorations 
copyright acm sigkdd december 
volume issue page determine find example say non bound nonzero determine support distribution modeled decision function sign models parameter neat interpretation upper bound fraction outliers lower bound fraction patterns support vectors 
sch lkopf provide experimental evidence favor approach including highlighting abnormal digits usps handwritten character dataset 
novelty detection points outside boundary viewed novel 
model sch lkopf origin feature space plays special role 
effectively acts prior class abnormal instances assumed lie 
repelling away origin consider attracting hyperplane datapoints feature space 
input space corresponds surface wraps data clusters achieved linear programming task min parameter just treated additional parameter minimization process unrestricted sign :10.1.1.42.8795
noise outliers handled introducing soft boundary error method successfully detection abnormalities blood samples detection faults condition monitoring ball bearing cages :10.1.1.42.8795
regression svm approaches real valued outputs formulated theoretically motivated statistical learning theory 
svm regression uses insensitive loss function shown 
deviation actual predicted value regression function considered error 
mathematically geometrically visualize band tube size hypothesis function points outside tube viewed training errors see 
piecewise linear insensitive loss function minimize penalize 
account training errors introduce slack variables types training error 
computes error underestimating function 
second computes error overestimating function 
slack variables zero points inside tube progressively increase points outside tube loss function 
general approach called sv regression common approach sv regression 
linear insensitive loss function task optimize plot wx versus insensitive tube 
points outside tube errors 
min zi sigkdd explorations 
copyright acm sigkdd december 
volume issue page strategy computing lagrangian dual adding kernels functions construct nonlinear regression functions 
apart formulations possible define loss functions giving rise different dual objective functions 
addition specifying priori possible specify upper bound fraction points lying outside band find optimizing 
classification novelty detection possible formulate linear programming approach regression min minimizing sum approximately minimizes number support vectors 
method favors sparse functions smoothly approximate data 

svm applications svms successfully applied number applications ranging particle identification face detection text categorization engine knock detection bioinformatics database marketing :10.1.1.11.6124
section discuss successful application areas illustrations machine vision handwritten character recognition bioinformatics 
rapidly changing research areas contemporary accounts best obtained relevant websites 
applications machine vision svms suited tasks commonly arise machine vision 
example consider application involving face identification 
experiment standard orl dataset consisting images person different persons 
methods tried direct svm classifier learned original images directly apart local rescaling classifier extensive pre processing involving rescaling local sampling local principal component analysis invariant svm classifier learned original images plus set images translated zoomed 
invariant svm classifier training set images person increased translated zoomed examples rbf kernel 
test set methods gave generalization errors respectively 
compared number alternative techniques best result 
face gender detection successfully achieved 
object recognition successful area application including face recognition pedestrian recognition handwritten digit recognition united states postal service usps dataset consists handwritten digits consisting vector entries 
rbf network svm compared dataset 
rbf network spherical gaussian rbf nodes number gaussian basis functions support vectors svm 
centers variances gaussians classical means clustering 
gaussian kernels system trained soft margin 
set classifiers multi class problem 
training set test set svm outperformed rbf network digits 
svms applied larger nist dataset handwritten characters consisting training test images pixels 
decoste scholkopf shown svms outperform techniques dataset 
applications bioinformatics functional interpretation gene expression data 
development dna microarray technology creating wealth gene expression data 
technology rna extracted cells sample tissues reverse transcribed labeled cdna 
fluorescent labels cdna binding dna probes highlighted laser excitation 
level expression gene proportional amount cdna dna probe proportional intensity fluorescent excitation site 
example gene expression data consider ovarian cancer dataset investigated 
microarray dna probes tissue samples 
task considered binary classification ovarian cancer cancer 
example fairly typical current high dimensionality comparatively examples 
viewed machine learning task high dimensionality sparsity datapoints suggest svms generalization ability svms doesn depend dimensionality space maximizing margin 
high dimensional feature vector absorbed kernel matrix purposes computation learning task follows reduced dimensionality example set size number features 
constrast neural network need input nodes correspondingly large number weights adjust 
motivation considering svms comes existence model selection bounds mentioned section may exploited achieve effective feature selection highlighting genes significantly different expression levels cancer 
study cancer datasets considered ovarian cancer dataset mentioned colon tumor dataset datasets acute leukemia aml acute leukemia 
ovarian cancer possible get perfect classification leave testing choice model parameters 
colon cancer expression levels tumor normal colon tissues determined dna microarray leave testing gave incorrectly labelled tissues 
leukemia datasets training set consisted examples aml test set consisted examples aml 
weighted voting scheme correctly learned instances self organizing map gave clusters aml aml 
svm correctly learned training data 
test data weighted voting scheme gave correct declining predict 
svm results varied different configurations achieved zero training error 
test instances correctly labeled choice correct declined weighted voting scheme classified incorrectly 
svms successfully applied bioinformatics tasks 
second successful application protein homology detection determine structural functional properties new protein sequences 
determination properties achieved relating new sequences proteins known structural features 
application svm outperformed number established systems homology detection relating test sequence correct families 
third application mention detection translation initiation sites points nucleotide sequences regions encoding proteins start 
svms performed task kernel function specifically designed include prior biological information 

discussion support vector machines appealing features 

svms rare example methodology geometric elegant mathematics theoretical guarantees practical algorithms meet 

svms represent general methodology types problems 
seen svms applied wide range classification regression novelty detection tasks applied areas sigkdd explorations 
copyright acm sigkdd december 
volume issue page covered operator inversion unsupervised learning 
generate possible learning machine architectures rbf networks feedforward neural networks appropriate choice kernel 
general methodology flexible 
customized meet particular application needs 
ideas margin regularization duality kernels extend method meet needs wide variety data mining tasks 

method eliminates problems experienced inference methodologies neural networks decision trees 
problems local minima 
construct highly nonlinear classification regression functions worrying getting stuck local minima 
model parameters pick 
example chooses construct radial basis function rbf machine classification need pick parameters penalty parameter misclassification width gaussian kernel 
number basis functions automatically selected svm algorithm 
final results stable reproducible largely independent specific algorithm optimize svm model 
users apply svm model parameters data get solution modulo numeric issues 
compare neural networks results dependent particular algorithm starting point 

robust optimization algorithms exist solving svm models 
problems formulated mathematical programming models state art research area readily applied 
results reported literature classification problems millions data points 

method relatively simple 
need svm expert successfully apply existing svm software new problems 

successful applications svm 
proven robust noise perform tasks 
svms powerful paradigm issues remain solved indispensable tools data miner toolbox 
consider challenging questions svms progress date 

svms perform best 
beat best hand tuned method particular dataset 
anticipate existence datasets svms perform worse alternative techniques exclude possibility perform best average outperform techniques range important applications 
seen section svms perform best important application domains 
svms panacea 
require skill apply methods may better suited particular applications 

svms scale massive datasets 
computational costs svm approach depends optimization algorithm 
best algorithms date typically quadratic involved multiple scans data 
algorithms constantly improved 
latest linear classification algorithms report results data points 
progress 

svms eliminate model selection problem 
svm method select attributes included problems type kernel including parameters model parameters trade error capacity control 
currently commonly method picking parameters cross validation 
cross validation quite expensive 
discussed section researchers exploiting underlying svm mathematical formulations associated statistical learning theory develop efficient model selection criteria 
eventually model selection probably strengths approach 

incorporate domain knowledge svm 
right way incorporate domain knowledge preparation data choice design kernels 
implicit mapping higher dimensional feature space prior knowledge difficult 
interesting question svm perform alternative algorithmic approaches exploit prior knowledge problem domain 

interpretable results produced svm 
interpretability priority date svm research 
support vectors algorithms provide limited information 
research producing interpretable results confidence measures needed 
format data svms 
effect attribute scaling 
handle categorical variables missing data 
neural networks svms primarily developed apply real valued vectors 
typically data converted real vectors scaled 
different methods doing conversion affect outcome algorithm 
usually categorical variables mapped numeric values 
problem missing data explicitly addressed methodology depend existing preprocessing techniques 
potential svms handle issues better 
example new types kernels developed explicitly handle data graphical structure missing values 
questions remain open current time progress years resulted new insights expect svms grow importance data mining tool 

acknowledgments sigkdd explorations 
copyright acm sigkdd december 
volume issue page performed support national science foundation iis 

pontil verri support vector machines vs multi layer perceptrons particle identification 
proceedings european symposium artifical neural networks facto press belgium 
bennett bredensteiner geometry learning geometry editor mathematical association america washington 
bennett bredensteiner duality geometry svms 
langley editor proc 
th international conference machine learning morgan kaufmann san francisco bennett demiriz shawe taylor column generation algorithm boosting 
langley editor proc 
th international conference machine learning morgan kaufmann san francisco 
bennett wu support vector decision trees database marketing 
research report rensselaer polytechnic institute troy ny 
bradley mangasarian optimization massive datasets 
appear abello pardalos resende eds handbook massive datasets kluwer 
brown grundy lin cristianini ares jr haussler 
knowledge analysis microarray gene expression data support vector machines 
proceedings national academy sciences 
burges tutorial support vector machines pattern recognition 
data mining knowledge discovery 
campbell bennett linear programming approach novelty detection :10.1.1.42.8795
appear advances neural information processing systems morgan kaufmann 
chapelle vapnik model selection support vector machines 
appear advances neural information processing systems ed 
solla leen 
muller mit press 
cortes vapnik support vector networks 
machine learning 
crisp burges geometric interpretation svm classifiers 
advances neural information processing systems ed 
solla leen 
muller mit press 
cristianini campbell shawe taylor dynamically adapting kernels support vector machines 
advances neural information processing systems ed 
kearns solla cohn mit press 
cristianini shawe taylor support vector machines kernel learning methods 
cambridge university press 
www net 
bengio svmtorch web page www idiap ch learning svmtorch html decoste scholkopf training invariant support vector machines 
appear machine learning kluwer 
drucker wu vapnik support vector machines spam categorization 
ieee trans 
neural networks 

drucker burges kaufman smola vapnik support vector regression machines 
mozer jordan petsche eds 
advances neural information processing systems mit press cambridge ma 
dumais platt heckerman sahami inductive learning algorithms representations text categorization 
th international conference information knowledge management 
fernandez face identification support vector machines 
proceedings european symposium artificial neural networks esann facto press brussels ferris munson semi smooth support vector machines 
data mining institute technical report computer sciences department university wisconsin madison wisconsin 
ferris munson interior point methods massive support vector machines 
data mining institute technical report computer sciences department university wisconsin madison wisconsin 
cristianini campbell kernel algorithm fast simple learning procedure support vector machines 
th intl 
conf 
machine learning morgan kaufman publishers 
cristianini duffy schummer haussler support vector machine classification validation cancer tissue samples microarray expression data 
bioinformatics 
golub slonim tamayo mesirov loh downing bloomfield lander classification cancer class discovery class prediction gene expression monitoring 
science 
guyon matic vapnik discovering informative patterns data cleaning 
fayyad piatetsky shapiro smyth uthurusamy editors advances knowledge discovery data mining mit press 
guyon web page svm applications www com isabelle projects svm html sigkdd explorations 
copyright acm sigkdd december 
volume issue page jaakkola haussler discriminative framework detecting remote protein homologies 
mit preprint 
joachims text categorization support vector machines learning relevant features :10.1.1.11.6124
proc 
european conference machine learning ecml 
joachims estimating generalization performance svm efficiently 
proceedings th international conference machine learning morgan kaufmann 

joachims text categorization support vector machines learning relevant features 
proc 
european conference machine learning ecml 
joachims web page svmlight www ai cs uni dortmund de software svm light svm light eng html keerthi bhattacharyya murthy improvements platt smo algorithm svm classifier design 
tech report dept csa india 
keerthi bhattacharyya murthy fast iterative nearest point algorithm support vector machine classifier design report tr isl intelligent systems lab dept computer science automation indian institute science bangalore india accepted publication ieee transaction neural networks 
luenberger linear nonlinear programming 
addison wesley 
mangasarian massive support vector regression data mining institute technical report dept computer science university wisconsin madison august 
mangasarian lagrangian support vector regression data mining institute technical report june 
mukherjee tamayo slonim verri golub mesirov poggio support vector machine classification microarray data mit ai memo mit cbcl 
orl dataset olivetti research laboratory 
www uk research att com html osuna freund girosi training support vector machines application face detection 
proceedings cvpr puerto rico osuna freund girosi proc 
ieee amelia island fl 
osuna girosi reducing run time complexity support vector machines 
scholkopf burges smola ed advances kernel methods support vector learning mit press cambridge ma 
platt fast training svms sequential minimal optimization 
scholkopf burges smola ed advances kernel methods support vector learning mit press cambridge ma 
papageorgiou oren poggio general framework object detection 
proceedings international conference computer vision 
demiriz bennett sparse regression ensembles infinite finite hypothesis space 
neurocolt technical report royal holloway college london september 
support vector approaches engine knock detection 
proc 
international joint conference neural networks ijcnn july washington usa improving generalization linear support vector machines application object recognition cluttered background 
proc 
workshop support vector machines th international joint conference artificial intelligence july august stockholm sweden 
scholkopf bartlett smola williamson support vector regression automatic accuracy control 
niklasson boden ziemke editors proceedings th international conference artificial neural networks perspectives neural computing berlin springer verlag 
scholkopf bartlett smola williamson shrinking tube new support vector regression algorithm 
appear kearns solla cohn eds advances neural information processing systems mit press cambridge ma 
scholkopf burges smola advances kernel methods support vector machines 
mit press cambridge ma 

scholkopf platt shawe taylor smola williamson estimating support highdimensional distribution 
microsoft research technical report msr tr 
scholkopf shawe taylor smola williamson kernel dependent support vector error bounds 
ninth international conference artificial neural networks iee conference publications 
scholkopf smola muller 
kernel principal component analysis 
scholkopf burges smola editors advances kernel methods support vector learning 
mit press cambridge ma 

scholkopf smola williamson bartlett new support vector algorithms 
appear neural computation 
scholkopf sung burges girosi niyogi poggio vapnik comparing support vector machines gaussian kernels radial basis function classifiers 
ieee transactions signal processing 
smola bartlett scholkopf schuurmans 
eds advances large margin classifiers chapter mit press 
sigkdd explorations 
copyright acm sigkdd december 
volume issue page shawe taylor cristianini margin distribution soft margin 
smola scholkopf schuurmans eds advances large margin classifiers chapter mit press 
smola scholkopf tutorial support vector regression 
neurocolt tr 
smola scholkopf regularization operators support vector kernels 
mozer jordan petsche eds 
advances neural information processing systems mit press cambridge ma 
smola scholkopf muller 
connection regularisation operators support vector kernels 
neural networks 
smola williamson mika scholkopf regularized principal manifolds 
computational learning theory th european conference volume lecture notes artificial intelligence springer 
tax duin data domain description support vectors 
proceedings esann ed 
verleysen facto press brussels 
tax duin 
support vector data description applied machine vibration analysis 
boasson eds proc 
th annual conference advanced school computing imaging nl june 
www ics uci edu mlearn mlrepository html vapnik nature statistical learning theory 
springer new york 
vapnik statistical learning theory 
wiley 
weston gammerman stitson vapnik vovk watkins support vector density estimation 
scholkopf burges smola 
advances kernel methods support vector machines 
mit press cambridge 
vapnik chapelle bounds error expectation support vector machines 
submitted neural computation weston mukherjee chapelle pontil poggio vapnik feature selection svms 
appear advances neural information processing systems morgan kaufmann 
kernel machines org zien ratsch mika scholkopf smola lengauer muller 
engineering support vector machine kernels recognize translation initiation sites 
german conference bioinformatics 
authors kristin bennett associate professor mathematical sciences rensselaer polytechnic institute 
research focus support vector machines mathematical programming methods data mining machine learning application practical problems drug discovery properties materials database marketing 
returned visiting researcher microsoft research consulted chase manhattan bank kodak 
earned ph computer sciences department university wisconsin madison 
www rpi edu 
colin campbell gained bsc degree physics imperial college london phd applied mathematics department mathematics king college university london 
appointed faculty engineering bristol university 
interests include neural computing machine learning support vector machines application techniques medical decision support bioinformatics machine vision 
lara enm bris ac uk cig 
sigkdd explorations 
copyright acm sigkdd december 
volume issue page 
