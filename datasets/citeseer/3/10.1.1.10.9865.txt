testing significance attribute interactions acm org ivan bratko ivan bratko fri uni lj si faculty computer information science tr za ska cesta si ljubljana slovenia attribute interactions irreducible dependencies attributes 
interactions underlie feature relevance selection structure joint probability classification models attributes interact connected 
issue way interactions especially attribute label addressed introduce operational definition generalized way interaction highlighting models part approximation model reconstructed models parts holistic model modelled directly 
interaction deemed significant models significantly different 
propose kirkwood superposition approximation constructing part approximations 
model data assume particular structure interactions construct model testing presence interactions 
resulting map significant interactions graphical model learned data 
confirm values computed assumption asymptotic distribution closely match obtained bootstrap 


information shared attributes address problem attribute tells information shared attributes 
general problem comprises appearing proceedings st international conference machine learning banff canada 
copyright author 
attribute relevance attribute interactions 
need define terms 
formally attribute considered collection independent mutually exclusive attribute values 

write example value instance corresponds event conjunction attributes values 
example instance playing tennis hot weather instances described attributes range play play attribute cold warm hot 
task deciding play play attribute role label 
attribute relevant predicting label common 
able estimate need general model connects attribute label functions uncertain noisy data 
general models uncertainty stated terms joint probability density functions 
joint probability density function joint pdf maps possible combination attribute values probability occurrence 
joint pdf example map 
joint pdf obtain marginal pdf removing marginalizing attributes 
removal performed summing probabilities combinations values removed attributes 
example pdf attribute 
way measuring uncertainty joint pdf shannon entropy defined joint pdf set attributes log range cartesian product ranges individual attributes 
uncertainty joint pdf ab uncertainties marginal pdfs shared uncertainty mutual information information gain attributes defined ab 
ab understood joint attribute derived attribute domain cartesian product domains mutual information reduction uncertainty achieved looking attributes time 
higher mutual information better predict vice versa 
mutual information non zero say involved way interaction 
mutual information limited attributes bratko mcgill han quantify interaction attributes way interaction information 
interaction information seen generalization mutual information 
mutual information attributes way interaction information equal 
way interaction information attributes denoted 
link interaction information conditional mutual information stands conditional mutual information context conditionally independent iff 
case redundancy wholly contained eliminate controlling way interaction information correction term determining mutual information label attributes ab 
positive way interaction information indicates synergy attributes meaning yield information expected individual interactions label 
negative interaction information suggests redundancy meaning provide part information label 
quantification possible analyze relationships attributes perform attribute clustering guide feature selection construction bratko 

complexity joint probability density functions problems attributes joint pdf may sparse 
objective learning construct model joint pdf avoid sparseness 
basic operators compacting latent attributes factorization 
may reduce dimensionality attribute space creating latent attribute useful lower cardinality dimensionality original attribute space 
factorization take advantage independencies attributes 
example factorize reducing original dimensional space independent dimensional spaces 
course factors need independent factorize conditional expressions 
popular learning algorithms instances approaches 
best illustrated example na bayesian classifier applied supervised learning problem attributes label attributes 
na bayesian classifier factorizing 
conditional independence assumption conflicting data resulting inferior predictions outcome probabilities just outcome important applications medical risk assessment 
kononenko pazzani friedman 
proposed approaches aggressive factorization making certain dependencies attributes part model 
rish offered approach discovery latent attribute 

contributions bratko suggested considerably high low interaction information attributes heuristic indication attributes interact factorized 
provide justification relevance heuristic replace vague notion high low statistical significance 
interaction defined group attributes interact factorize joint pdf decide factorization warranted investigate loss incurred best approximation construct directly observing true joint pdf 
methods referred part approximations 
discuss method kirkwood superposition approximation ksa kirkwood boggs define sect 

turns interaction information equal kullback leibler divergence ksa joint pdf 
second part investigate techniques determining significance importance particular part approximation basis evidence provided data 
example require complex feature way interaction supported plentiful evidence run risk overfitting 
determine significance interactions note kullback leibler divergence distribution asymptotically 
alternative consider nonparametric bootstrap procedure cross validation 
turns nonparametric bootstrap procedure yields similar results cross validation deviates somewhat 
apply instruments identify significant interactions data illustrate form interaction graph revealing interesting dependencies attributes describing information label provided individual attributes correction factors arising interactions label 
graphs useful exploratory data analysis tool serve initial approximation constructing predictive models 

modelling interactions interaction understood irreducible 
interaction differs mere dependency 
dependency may interactions interaction dependency broken 
need practical definition difference reduction 
view reducible predict observing involved variables time 
observe directly measurement system limited part system 
language probability view part system results marginalization removal attributes achieved summing integrating joint pdf 
favor attribute particular observe system sides attributes missing 
formally verify factorized attempt approximate set attainable marginals 
approximations referred part approximations 
approximation obtained marginal densities fits 
accept interaction possibly seek latent attributes simplify 

kirkwood superposition approximation kirkwood superposition approximation kirkwood boggs uses available pairwise dependencies order construct complete model 
phrased ksa terms approximation pk joint probability density function follows pk 
kirkwood superposition approximation ways result normalized pdf pk may violating normalization condition 
define normalized ksa pk 
approximations closed form violate normalization condition models built assumption conditional independence 
attributes models 
example model assumes independent context find approximations proper part approximations employ available parts example model disregards dependence choice conditioning attribute arbitrary keep considering models single part know advance best 
normalization kirkwood superposition approximation superior models conditional independence shown experimentally sect 

interaction testing methodology applied loglinear models agresti 
loglinear part model employs set constraints association terms 
advantage loglinear models fitted iterative scaling addition additional consistent constraints improve fit model 
hand kirkwood superposition approximation better conditional independence model disregards part information available part approximation method 
kirkwood superposition approximation closed form making simple efficient fitting loglinear models type requires iterative methods 

kullback leibler divergence statistic objective determine interaction exists attributes domain 
described earlier approximation fits data evidence interaction 
employ loss function assess similarity joint pdf part approximation 
kullback leibler divergence frequently measure difference joint probability density functions log unit measure bit 
kl divergence referred expected log factor logarithm bayes factor expected weight evidence favor cross entropy 
equations clear way interaction information defined equal kullback leibler divergence true pdf kirkwood superposition approximation pk pk 
analogously divergences conditional independence models true joint pdf metrics assessing attributes assumed dependent model assume conditional independence 
generalized kirkwood superposition approximation attributes derived equality 
interpret interaction information approximate weight evidence favor approximating joint pdf generalized kirkwood superposition approximation 
approximation inconsistent normalization condition interaction information may negative may underestimate true loss approximation 
kirkwood superposition approximation normalized computing divergence 
underlying pdf categorical attributes relative frequencies estimated instances kl divergence independent joint pdf multiplied log equal wilks likelihood ratio statistic context goodness fit test large df distribution df degrees freedom log df cardinality set possible combinations attribute values 
subset cartesian product ranges individual attributes 
certain value combinations impossible joint domain binary attributes reduced possible combinations 
impossibility particular value conjunction inferred zero count set instances followed approach 
guideline agresti asymptotic approximation poor df 
example evaluate way interaction valued attributes df instances 
null hypothesis part approximation matches observed data alternative approximation fit interaction 
value weight evidence accepting null hypothesis part approximation defined df nd log value interpreted probability average loss incurred independent sample null gaussian model approximating multinomial distribution parameterized greater equal average loss incurred approximation original sample 
case loss measured kl divergence 
followed pearson approach selecting number degrees freedom disregards complexity approximating model assuming null hypothesis alternative distribution hypothesized estimated 
value interpreted lower bound values approximations 
part approximations discussed previous section df residual degrees freedom fisher scheme may reject favor simpler approximations 
df assures simplification able reduce value regardless complexity 
way simplifying part approximation means reducing set performed value low 

obtaining values resampling assuming distribution simply randomly generate independent bootstrap samples size original training set 
bootstrap sample created randomly independently picking instances original training set replacement 
nonparametric bootstrap corresponds assumption training instances samples multinomial distribution parameterized bootstrap sample measure relative frequency compute loss incurred prediction actual sample 
observe lies distribution losses 
value set bootstrap estimates bootstrap sample size nuisance parameter affects result larger value lower deviation value conditional usually size bootstrap sample assumed equal original sample larger rejection approximation kl divergence 
frequently method model selection machine learning cross validation 
define similar notion cv values fold cross validation 
replication fold set instances partitioned test training subsets 
subsets estimate joint pdfs training testing basis partially observed construct approximation cv value defined large set cross validated estimates 
bootstrap number folds nuisance parameter 

making decisions values basis obtained values decide interaction exists 
value identifies probability loss obtained null model predicting sample null model 
example value means loss incurred null model greater equal loss obtained approximation training sample average independent samples 
basis value may classify situation types interaction discovered interaction rejected 
holistically biased interaction risk overfitting high value threshold 
choose bias preferring simpler interaction model risk underfitting low value 
value provides measure robustness interaction importance 
continue employ interaction information measure importance 

experiments 
way attribute interactions taken data sets uci repository pair attributes domain investigated way interaction pair label 
compared kullback leibler divergence maximum likelihood joint probability density function part approximation obtained normalized kirkwood superposition approximation basis maximum likelihood marginals 
kirkwood superposition approximation conditional independence models compared kirkwood superposition approximation best conditional independence models 
turns conditional independence model somewhat frequently worse vs average error times lower kirkwood superposition approximation fig 

shows interaction may significant kirkwood superposition approximation significant tried conditionally independent approximations 
hand shows models include ksa may achieve better results limited models conditional independence 
kirkwood superposition approximation best conditional independence approximation 
comparison approximation divergence achieved kirkwood superposition approximation best possible conditional independence models 
constructing graphical models employed interaction testing approach construct model significant way way interactions supervised learning domain 
resulting interaction graph map dependencies wife religion media exposure standard living husband occupation wife education husband education wife age number children 
interaction graph bratko illustrating interactions attributes label cmc domain 
label domain method couple 
chosen value cutoff eliminated attributes wife working 
dashed lines indicate redundancies full lines indicate synergies 
percentages indicate reduction conditional entropy label attribute interaction 
rate duration credit history telephone residence duration credit amount employment plans job 
significant interactions german credit domain shown graph 
label domain credit risk 
notably attributes telephone residence duration job useful part way interaction 
consider moderators 
label attributes illustrated figs 

kirkwood superposition approximation observed negative positive interactions 
interactions may explained model assuming conditional independence loss removing negatively interacting attribute lower imperfectly modelling way dependence 
attributes conditionally independent label appear redundant 
interaction graph attempt minimize global fitness criterion seen approximate guideline model look 
may turn attributes may dropped 
example results fig 
indicate kirkwood superposition approximation uniformly better conditional independence models 
conditional independence models triplet attributes fit data better kirkwood superposition approximation interaction longer considered significant 
procedure constructing interaction graphs complete model building procedure 
values may meaningful performing single hypothesis test analysis domain involves large number tests account consequently increased risk making error 
best case approach assume values perfectly correlated adjustment 
worst case approach assume values perfectly independent adjust bonferroni correction 
proposed values making decisions interaction take account ignore ranking values interactions really matters 

inferential procedures compared way interactions attribute label standard benchmark domains number instances order magnitude soybean small lung post op lymphography breast cancer 
domains instances way interactions practically significant means data worth model 
small domains better disregard weakly relevant attributes may cause overfitting 
comparing bootstrap values examined similarity values obtained assumption distribution values obtained bootstrap procedure 
match shown fig 
recommend values reasonable heuristic tends slightly underestimate 
number bootstrap samples 
difference values cv values compared values obtained bootstrap similarly obtained cv values replications fold cross validation 
result illustrated fig 
shows estimates significance correlated behave somewhat differently 
values conservative low high values guarantee improvement deterioration cv performance 
cv values intuitively appealing number folds nuisance parameter aware suitable asymptotic approximations allow quick estimation 
values cross validated performance employed cross validation verify classifier benefits attribute compared classifier just prior label probability distribution 
words comparing classifiers label attribute 
loss function expected change negative log likelihood label value test set instance instance attribute value 
way probabilistic model testing set merely consider instances samples 
probabilities estimated laplacean prior avoid zero probabilities infinitely negative 
employed fold cross validation replications 
final loss average loss instance instances folds replications 
results fig 
show value predictor increase loss 
useful attributes appear left hand side graph 
pick total attributes single increase loss 
hand picked attributes basis mutual information information gain deterioration cases fold improvement base rate attributes yield deterioration experiment 
hand noted insignificant attributes result decrease prediction loss 
cut detecting overfitting increase loss cross validation obviously somewhat ad hoc especially cv values values turned largely equivalent experiment 
reason skeptical results cross validation 
significance seen necessary condition model carrying aversion chance complexity sufficient neglecting expected performance difference 

discussion shown interaction information interpreted kullback leibler divergence true joint pdf generalized kirkwood superposition approximation 
approximation normalized employ methods statistical hypothesis testing question group attributes interact 
shown distribution asymptotically validated distribution bootstrap sampling observing match noting computations orders magnitude cheaper 
introduce notion approximation captures intuition associated irreducibility interaction kirkwood superposition approximation example approximation 
values penalize attributes values making interaction insignificant total subset values interaction significant 
latent attributes involved modelling joint pdfs marginals constructing part approximation 
alternatively employ techniques subgroup discovery kind latent attribute identify subset situations interaction apply 
experiments difference bootstrap formulation formulation hypothesis testing considerably different comes judging risk average deterioration 
disputed tenable explanation results evaluations kullback leibler divergence earlier experiments tried employ statistical testing probabilistic statistics improving classification performance assessed conceptually different notions classification accuracy error rate instance ranking area roc 
pearson goodness fit testing primarily just possible testing protocols debate topic berger 
example values accept model rare grave errors reject model frequent negligible ones 
similarly accept model frequent negligible yields reject model rare large benefits 
significance testing average performance insufficient account complexity risk 
pearson approach close fisher significance testing differing just choice degrees freedom cv values resemble neyman pearson hypothesis testing interaction non interaction models dispersion taken consideration 
expected loss approach closest jeffreys approach models included difference loss logarithm associated bayes factor models 
offer cv values expected decrease loss viable alternatives choice values interaction sig testing influenced simplicity efficiency closed form computations distribution 
agresti 

categorical data analysis 
wiley series probability statistics 
john wiley sons 
nd edition 
berger 

fisher jeffreys neyman agreed testing 
statistical science 
friedman geiger goldszmidt 

bayesian network classifiers 
machine learning 


maximum entropy hypothesis formulation 
annals mathematical statistics 
han 

multiple mutual informations multiple interactions frequency data 
information control 
bratko 

analyzing attribute dependencies 
pkdd pp 

springer verlag 
bratko 

quantifying visualizing attribute interactions approach entropy 
arxiv org abs cs ai 
kirkwood boggs 

radial distribution function liquids 
journal chemical physics 
kononenko 

semi naive bayesian classifier 
ewsl 
springer verlag 


physical nature higher order mutual information intrinsic correlations frustration 
physical review 
mcgill 

multivariate information transmission 
psychometrika 
pazzani 

searching dependencies bayesian classifiers 
learning data ai statistics springer verlag 
rish 

decomposition classes clustering explain improve naive bayes 
ecml pp 

springer verlag 
chi squared value small lung horse postop breast german lymph bootstrap value 
comparison values estimated bootstrap assuming distribution 
bootstrap value small lung horse postop breast german lymph cross validation cv value 
comparison values estimated bootstrap probability test set loss interaction assuming model lower independence assuming fold cross validation cv value 
chi squared value small lung horse postop breast german lymph expected change cv loss including attribute 
comparison values assuming distribution average change log likelihood data information attribute value 
