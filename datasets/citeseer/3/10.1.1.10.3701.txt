exploiting replication data reuse eciently schedule data intensive applications grids santos neto cirne francisco lima universidade federal de grande www lsd edu br ourgrid dsc edu br th march data intensive applications executing computational grid demand large data transfers 
costly operations 
account mandatory achieve ecient scheduling data intensive applications grids 
heterogeneous changing environment grid better schedules typically attained heuristics dynamic information grid applications 
information dicult accurately obtained 
hand schedulers attain performance requiring dynamic information designed take data transfer account 
presents storage anity novel scheduling heuristic bag tasks data intensive applications running grid environments 
storage anity exploits data reuse pattern common data intensive applications allows take data transfer delays account reduce makespan application 
uses replication strategy yields ecient schedules relying dynamic information dicult obtain 
results show storage anity may attain better performance state art knowledge dependent schedulers 
achieved expense consuming cpu cycles network bandwidth 
year data generated need processed 
currently scienti enterprise applications deal huge amount data 
applications called 
order process large datasets applications typically need high performance computing infrastructure 
fortunately data splitting procedure easy data element processed independently solution data parallelism employed 
task independence main characteristic parallel bag tasks bot applications 
bot application composed tasks need communicate proceed computation 
interested class applications bot data intensive characteristics 
named processors huge data phd 
shortly phd bot data intensive 
important applications fall category 
case instance data mining image processing genomics 
due independence tasks bot applications normally suitable executed grids 
resources grid connected wide area network links wan bandwidth limitation issue considered running phd applications environments 
particularly relevant phd applications data pattern 
applications data reuse pattern exploited achieve better performance 
data tasks particular application succession applications executions 
system model instance visualization process quantum optics simulations results common perform sequence executions parallel visualization application simply sweeping arguments zoom view angle preserving huge portion data input previous executions 
exists algorithms able take data transfer account scheduling phd applications grid environments 
require knowledge trivial accurately obtained practice especially widely dispersed environment computational grid 
example xsu erage uses information cpu network loads execution time task machine known priori perform scheduling 
hand cpu intensive bot applications schedulers dynamic information achieve performance workqueue replication wqr 
replication tolerate inecient scheduling decisions taken due lack accurate information environment application 
schedulers conceived target cpu intensive applications data transfers taken account 
introduce storage anity new heuristic scheduling phd applications grids 
storage anity takes account fact input data frequently reused multiple tasks phd application successive executions application 
tracks location data produce schedules avoid possible large data transfers 
reduces ect inecient task processor assignments judicious task replication 
rest organized way 
section system model considered 
section storage anity heuristic heuristics comparative purposes 
section evaluate performance discussed schedulers 
section concludes nal remarks brief discussion perspectives 
system model section formally describes problem investigated provides terminology rest 
wan link local file system 
site network file system lan link home home site network file system lan link system environment model system environment consider scheduling sequence jobs grid infrastructure 
grid formed collection sites 
site comprised number processors able run tasks single terms job application interchangeably 
system model data server able store input data required execution task output data generated execution task 
formally site site fs non empty set processors site data server site assume resources owned various sites disjoint site site 
processors belonging site connected high bandwidth local area network latency small throughput large compared experienced wide networks interconnect processors belonging di erent sites 
assumption consider data server site collection data servers may site collapsed single data server 
de ne sets encompass processors pg data servers sg grid say pg jgj sg jgj fs assume user spawns execution applications home machine belong grid home 
assume rst execution application input data stored local le system home machine 
illustrates assumed environment 
application job th execution application job composed non empty collection tasks task de ned datasets input output datasets 
formally ft input output datasets task respectively 
data server sg ds set data elements stored execution th job started ds set data elements stored home machine 
de ne set data elements available taken input th job executed 
ds sg ds execution th job set available data elements union data elements output jj de ne input dataset entire application union input dataset task job 
expressed jj scheduling heuristics job scheduling performance metrics schedule job comprises schedule tasks form schedule particular task speci es processor assigned execute note possible processor assigned task 
formally fp jj pg assume task access data server site processor task running 
consequently data elements dataset stored absent data elements rst transferred execution started executed site stored data elements dataset measure application execution time evaluate eciency scheduling 
heuristic propose discuss common goal minimize metric 
application execution time normally referred makespan time elapsed moment rst task started earliest moment tasks nished execution 
scheduling heuristics despite fact phd applications suitable run computational grids ecient scheduling applications grid environments trivial 
diculty scheduling phd application twofold 
rst problem relates nature phd applications deal huge amount data 
issue application performance greatly ected large data transfers occur execution tasks 
second problem related obtaining accurate information performance resources deliver application 
despite fact information typically available priori input available schedulers 
fact great deal research predicting cpu network performance application execution time 
results orts show means easy task 
complicate matters lack central grid control poses obstacle deploying resource monitoring middleware 
observe diculty obtaining dynamic information impact large data transfers individually attacked 
comment scheduling heuristics deal problems separately workqueue replication wqr xsu erage 
introduce approach address phd scheduling problems 
workqueue replication wqr scheduling heuristic conceived solve problem obtaining precise information performance tasks experience grid resources 
initially wqr similar traditional workqueue scheduling heuristic 
tasks sent random idle processors processor nishes task receives new task execute 
wqr di ers workqueue processor available waiting task start 
point workqueue just wait tasks nish 
wqr starts replicating tasks running 
result task comes rst replica nish 
rst replica completes replicas killed 
idea task replication improve application performance increasing chances running task fast unloaded processors 
wqr achieves performance cpu intensive application kind dynamic information processors network links tasks 
drawback cpu cycles wasted replicas complete 
wqr take data transfers account results poor performance phd applications shall see section 
scheduling heuristics xsu erage xsu erage knowledge scheduling heuristic deals impact large data transfers phd applications running grid environments 
xsu erage extension su erage scheduling heuristic 
su erage prioritizes task su er assigned processor fastest runs 
task su er gauged su erage value de ned di erence best second best completion time task 
main di erence xsu erage su erage algorithms su erage value determination method 
xsu erage su erage value calculated site level task completion times 
completion time task minimum completion time achieved processors site 
site level su erage di erence best second best site level completion times task 
di erence xsu erage considers input data transfers calculation completion time task di erently su erage requires information network available bandwidth input 
algorithm input job grid algorithm traverses set nds task highest su erage value 
task assigned processor earliest completion time 
action repeated tasks scheduled 
rationale xsu erage consider data location performing task host assignments 
expected ect minimization impact unnecessary data transfers application makespan 
evaluation xsu erage shows avoiding unnecessary data transfers improves application performance 
xsu erage calculates su erage values knowledge cpu loads network bandwidth utilization task execution times 
general information easy obtain 
input output site sa site sa site ft pg busy jr jr jr jr ft jr site sa site jr jr ft jr site sa site sa site jr pg busy jr algorithm storage anity scheduling heuristic storage anity storage anity conceived exploit data improve performance application 
data appears basic avors inter job inter task 
arises job uses data produced job executed previously appears performance evaluation applications tasks share input data 
formally inter job data pattern occurs relation holds hand inter task data pattern occurs relation holds jj order take advantage data pattern improve performance phd applications introduce storage anity metric 
metric determines close site task close mean bytes task input dataset stored speci site 
storage anity task site number bytes task input dataset stored site 
formally storage anity value site sa site ds jdj jdj represents number bytes data element claim information data size data location obtained priori diculty loss accuracy example cpu network loads completion time tasks 
instance information obtained data server able answer requests data elements stores large data element 
alternatively implementation storage anity scheduler easily store history previous data transfer operations containing required information 
naturally storage anity dynamic information grid application dicult obtain inecient task processor assignments may occur 
order circumvent problem storage anity applies task replication 
replicas chance submitted faster processors processors assigned original task increasing chance task completion time decreased 
algorithm presents storage anity note heuristic divided phases 
rst phase storage anity assigns task processor phase algorithm calculates highest storage anity value task 
calculation task largest storage anity value chosen scheduled 
continues tasks scheduled 
second phase consists task replication 
starts waiting tasks available processor 
replica created running task 
considering replication degree particular task number replicas created task processor available criteria considered choose task replicated task positive storage anity site available processor ii current replication degree task smallest running tasks iii task largest storage anity value remaining candidates 
task completes execution scheduler kills remaining replicas task 
algorithm nishes running tasks complete 
occurs algorithm proceeds replications 
performance evaluation section analyze performance storage anity comparing wqr xsu erage 
decided compare approach heuristics wqr represents state art solution circumvent dynamic information dependence xsu erage state art dealing impact large data transfers 
simulations evaluate performance scheduling algorithms 
simulations validated performing set real life experiments see section 
performance attained scheduler strongly uenced workload designed experiments cover wide variety scenarios 
scenarios vary heterogeneity performance evaluation grid application application granularity see section 
hope identify scheduler performs better understand di erent factors impact performance 
grid environment task computational cost expresses long task take execute dedicated processor 
processors may run di erent speeds 
de nition processor speed 
processor speed runs second task seconds dedicated 
computational grid may comprise processors acquired di erent points time grids tend heterogeneous processors speed may vary widely 
order investigate impact grid heterogeneity scheduling consider levels grid heterogeneity shown table heterogeneity speed grid homogeneous 
hand heterogeneity maximal heterogeneity fastest machines times faster slowest ones 
note cases average speed machines forming grid 
grid heterogeneity processor speed distributions table grid heterogeneity levels distributions relative speed processors grid power sum speed processors comprise grid 
experiments xed grid power 
speed processors obtained processor speed distributions grid constructed adding processor time grid power reaches 
average number processors grid 
processors distributed sites form grid equal proportions 
similarly casanova assume grid sites 
simplicity assume data servers run disk space address data replacement policies 
previously indicated neglect data transfers site 
inter site communication modeled single shared mbps link connects home machine sites 
important highlight model mbps maximum bandwidth application wide area network 
connections frequently shared applications limit achieved particular application 
nws real traces simulate contention cpu cycles network bandwidth 
example processor speed availability runs second task seconds 
phd applications phd applications application execution time typically related size input data 
explanation fact quite simple 
data process longer tasks take complete 
fact phd applications cost completely determined size input data 
case example scienti data visualization application processes input data produce output image 
applications cost uenced completely determined size input data 
case pattern search application size input data task determines upper bound cost task cost task 
simulated kinds applications 
total size input data simulated application xed gbytes 
experimental data available able convert amount input data processed task visualization application time seconds required process data computational cost 
proportionality factor ms kbyte calculate computational cost pattern search application function amount data processed tasks 
note denotes uniform distribution range 
performance evaluation determine computational cost task pattern search application uniform distribution upperbound upperbound computational cost process entire input particular task 
wanted analyze relation average number tasks number processors grid impact performance schedule 
note application grid sizes xed relation inversely proportional average size input data tasks comprise application application granularity 
considered application groups de ned application granularity values mbytes mbytes mbytes 
tasks comprise application vary size 
simulate variation introduced application heterogeneity factor 
heterogeneity factor determines di erent sizes input data elements tasks form job consequently costs 
size input data taken uniform distribution ha ha mbytes mbytes simulation setting environment total simulations performed half type application visualization pattern search 
shall see section simulations results simulation tool developed adapted version toolkit 
toolkit provides basic functionalities simulation distributed applications grid environments 
set simulations bot application executed grid composed machines distributed di erent administrative domains lsd instituto ucsd 
mygrid middleware execute simulations 
simulation results section show results obtained simulations scheduling heuristics discuss statistical validity 
analyze uence application granularity heterogeneity grid application performance application scheduling 
summary results table presents summary simulation results 
possible note average storage anity xsu erage achieve comparable performances 
standard deviation values indicate makespan presents smaller variation application scheduled storage anity compared heuristics 
makespan seconds storage anity wqr xsu erage mean standard deviation cpu wasting storage anity wqr xsu erage mean standard deviation bandwidth wasting storage anity wqr xsu erage mean standard deviation table summary simulation results order evaluate precision con dence summarized means table determined con dence interval population mean values table 
sample mean standard deviation sample size number makespan values estimate con dence intervals shown table 
performance evaluation average application makespan seconds number executions storage affinity wqr summary performance scheduling heuristics heuristic con dence interval storage anity workqueue replication xsu erage table con dence intervals mean makespan heuristic 
width con dence interval relatively small compared results see table feel performed simulation obtain precision results 
heuristic respect makespan storage anity workqueue replication xsu erage table width con dence intervals proportion respect mean show average application makespan resource waste performed simulations respect heuristics analyzed 
results show heuristics attain better performance wqr 
data transfer delays dominate makespan application account severely hurts performance application 
case wqr execution task preceded costly data transfer operation inferred large bandwidth small cpu waste shown 
impairs improvement replication strategy wqr bring 
hand replication strategy storage anity able cope lack dynamic information yield performance similar xsu erage 
main inconvenience xsu erage need knowledge dynamic information drawback storage anity consumption extra resources due replication strategy average extra cpu cycles negligible amount extra bandwidth 
result state storage anity task replication strategy feasible technique obviate need dynamic information scheduling phd applications expenses consuming cpu 
performance evaluation waste number executions storage affinity cpu waste wqr cpu waste storage affinity bandwidth waste wqr bandwidth waste summary resource waste average application makespan seconds number executions storage affinity mbytes mbytes storage affinity mbytes mbytes storage affinity mbytes mbytes impact application granularity application granularity investigate impact application granularity application scheduling performance 
see uence di erent granularities data aware schedulers 
results conclude matter heuristic smaller granularities yield better performance 
smaller tasks allow greater parallelism 
observe xsu erage achieves better performance storage anity granularity application mbytes 
larger particular task bigger uence makespan application 
impact possible inecient task host assignment larger task greater smaller 
words replication strategy storage anity ecient circumventing ects inecient task host assignments application granularity small 
phd applications normally possible quite easy reduce application granularity converting task large input tasks smaller input datasets 
conversion performed simply slicing large input datasets smaller ones 
performance evaluation average application makespan seconds number executions storage affinity wqr performance heuristics respect granularities mbytes mbytes discussion show values makespan applications considering granularities mbytes mbytes 
simulations storage anity outperforms xsu erage average 
seen percentage cpu cycles wasted reduced average 
emphasize reducing application granularity policy smaller tasks yields parallelism see 
waste number executions storage affinity cpu waste wqr cpu waste storage affinity bandwidth waste wqr bandwidth waste resource waste respect granularities mbytes mbytes application type order analyze uence di erent characteristics phd applications application makespan resource waste considered types applications see section 
results show behavior heuristics ected di erent characteristics application 
hand waste resources ected type application considered 
show results attained 
recall data visualization application computational cost task completely determined size input dataset 
storage anity prioritizes task largest storage anity value means largest tasks scheduled rst 
task replication starts application performance evaluation average application makespan seconds grid heterogeneity storage affinity workqueue replication grid heterogeneity impact pattern search application executed 
case pattern search application computational cost tasks completely determined size input dataset task proportionally large tasks scheduled stages execution application 
replication may start large portion application accomplished consequently resources wasted improve application makespan 
average application makespan seconds grid heterogeneity storage affinity workqueue replication grid heterogeneity impact scienti visualization application grid application heterogeneity analyzed impact heterogeneity grid application scienti visualization pattern search applications 
see heterogeneity grid uences makespan types applications considering heuristics discussed 
possible see heuristics greatly ected variation grid heterogeneity 
surprising xsu erage presents behavior uses information environment 
performance evaluation storage anity shows replication strategy circumvents ects variations speed processors grid information environment 
wqr uenced lot grid heterogeneity variation see increasing grid heterogeneity application makespan get worse 
waste number executions storage affinity cpu waste wqr cpu waste storage affinity bandwidth waste wqr bandwidth waste resource waste considering scienti visualization application waste number executions storage affinity cpu waste wqr cpu waste storage affinity bandwidth waste wqr bandwidth waste resources wasted considering pattern search application storage anity xsu erage similar behavior respect application heterogeneity 
heuristics show tolerance variation application heterogeneity 
observe application makespan presents tiny uctuation types application visualization search 
performance evaluation average application makespan seconds application heterogeneity storage affinity workqueue replication application heterogeneity impact scienti visualization application average application makespan seconds application heterogeneity storage affinity workqueue replication application heterogeneity impact pattern search application validation order validate simulations conducted experiments prototype version storage anity 
storage anity prototype developed new scheduling heuristic mygrid 
grid environment experiments comprised processors located sites cluster brazil ucsd san diego usa 
home machine home located de sistemas distribu dos grande brazil 
important highlight experiments resources shared applications 
respect application blast 
blast application searches sequence characters database 
characters represent protein sequence database contains identi ed sequences proteins 
application receives parameters database sequence characters searched 
database size order gbytes sliced slices mbytes 
hand size sequence characters searched surpass kbytes 
performance evaluation application composed tasks 
task application receives slice mbytes large database downloaded blast site sequence characters smaller kbytes 
simulations focused applications inter job data reuse pattern section set application data reuse pattern 
input task reused database minor part input search target kbytes changed executions 
methodology scheduling heuristics analyzed experiments storage anity workqueue replication 
xsu erage due lack deployed monitoring infrastructure provide resource load information 
hand mygrid version workqueue replication heuristic available 
order minimize ect grid dynamism results experiment consisted back executions scheduling heuristics intermixed experiments scheduling heuristics 
approach experiments executed 
experiment consisted successive executions application scheduling heuristic adding total application executions 
results average application makespan scheduling heuristic 
contains simulation scenario experiment 
results show storage anity workqueue replication behavior noticed simulations 
experiment di er simulation aspects 
greater uctuation makespan values experiment 
due high level heterogeneity grid environment fact ran fewer cases simulated 
storage affinity workqueue replication experiment results average application makespan heuristic observe di erence makespan experiment results simulated scenario 
believe reasons discrepancy 
collect cpu network loads experienced real life experiments 
standard nws logs 
grid scenario quite simulation experiments 
second storage anity prototype queries sites obtain information existence size les 
costly remote operation modeled simulator 
scheduler responsible transfering les sites information cached locally greatly reducing need remote invocations execution storage anity 
currently implementing caching strategy expect modi cation greatly reduce discrepancy simulation experimentation 
simulation scenario considered experiments storage anity novel heuristic scheduling phd grid environments 
compared performance established heuristics xsu erage wqr 
knowledge centric heuristic takes data transfer delays account knowledge free approach uses replication cope inecient task processor assignments consider data transfer delays 
storage anity uses replication avoids unnecessary data transfers exploiting data pattern commonly phd applications 
contrast information needed xsu erage data location information required storage anity trivially obtained grid environments 
results show data transfer account mandatory achieve ecient scheduling phd applications 
shown grid application heterogeneity little impact performance studied schedulers 
hand granularity application important impact performance data aware schedulers analyzed 
storage anity outperformed xsu erage application granularity large 
granularity phd applications easily reduced levels storage anity outperform xsu erage 
fact independently heuristic smaller application granularity better performance scheduler granularity size corresponds overhead starts dominate execution time 
favorable scenarios storage anity achieves makespan average smaller xsu erage 
drawback storage anity waste grid resources due replication strategy 
results show wasted bandwidth negligible wasted cpu reduced 
intend investigate issues impact inter task data pattern application scheduling ii disk space management data servers iii emergent behavior community storage anity schedulers competing shared resources iv introspection techniques data staging provide scheduler information data location disk space utilization 
release stable version storage anity mygrid middleware 
hope practical experience scheduler help identify aspects model need re ned 
lyman varian dunn swearingen information www sims berkeley edu research projects info october 
altschul gish miller myers lipman basic local alignment search tool journal molecular biology vol 
pp 

group www griphyn org 
santos neto fonseca parallel visualization optical pulse optical ber proceedings annual meeting division computational physics june 
cirne costa santos neto da silva barros running bag tasks applications computational grids mygrid approach proceedings international conference parallel processing october 
smith shrivastava system fault tolerant execution data compute intensive programs network workstations lecture notes computer science vol 
ieee press 
foster kesselman eds grid blueprint computing infrastructure 

carter ferrante robert bandwidth centric allocation independent task heterogeneous proceedings parallel distributed processing symposium fort lauderdale florida april 
casanova berman heuristics scheduling parameter sweep applications grid environments proceedings th heterogeneous computing workshop cancun mexico pp 
ieee computer society press may 
su berman adaptive performance prediction distributed applications proceedings acm ieee sc conference high performance networking computing portland oh usa acm press 
marzullo amd nile wide area computing high energy physics proceedings th acm european operating systems principles conference 
system support worldwide applications ireland pp 
acm press sept 
cirne trading cycles information replication schedule bag tasks applications computational grids proceedings euro par international conference parallel distributed computing klagenfurt austria august 
kedem spirakis ecient robust parallel computations extended acm symposium theory computing pp 

scheduling theory algorithms systems 
new jersey usa prentice hall nd edition august 
downey predicting queue times space sharing parallel computers proceedings th international parallel processing symposium ipps april 
gibbons historical application pro ler parallel schedulers lecture notes computer science vol 
pp 

smith foster taylor predicting application run times information lecture notes computer science vol 
pp 

wolski spring hayes predicting cpu availability time shared unix systems computational grid proceedings th international symposium high performance distributed computing hpdc august 
francis jamin paxson zhang jim architecture global internet host distance estimation service proceedings ieee infocom 
ibarra kim heuristic algorithms scheduling independent tasks processors journal acm jacm vol 
pp 

feitelson rudolph metrics benchmarking parallel job scheduling job scheduling strategies parallel processing feitelson rudolph eds vol 
pp 
lecture notes computer science springer verlag 
feitelson metric workload ects computer systems evaluation computer vol 
pp 
september 
lo comparative study real workload traces synthetic workload models parallel job scheduling 
wolski spring hayes network weather service distributed resource performance forecasting service metacomputing generation computer systems vol 
pp 

casanova toolkit simulation application scheduling proceedings ieee acm international symposium cluster computing grid may devore probability statistics engineering sciences vol 

john wiley sons 
mygrid site www ourgrid org mygrid 
blast webpage www ncbi nlm nih giv blast 
kubiatowicz bindel chen czerwinski eaton geels gummadi rhea weatherspoon weimer wells zhao oceanstore architecture global scale persistent storage proceedings ninth international conference architectural support programming languages operating systems ieee computer society press nov 
