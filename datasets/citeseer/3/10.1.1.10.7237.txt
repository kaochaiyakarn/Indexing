close official version published proceedings ieee vol 
pp 
oct blind signal separation statistical principles jean francois cardoso cardoso tsi enst fr tsi enst fr cardoso html blind signal separation bss independent component analysis ica emerging techniques array processing data analysis aiming recovering unobserved signals sources observed mixtures typically output array sensors exploiting assumption mutual independence signals 
weakness assumptions powerful approach requires venture familiar second order statistics 
objective review approaches developed address exciting problem show stem basic principles relate 
keywords signal separation blind source separation independent component analysis 
blind signal separation bss consists recovering unobserved signals sources observed mixtures 
typically observations obtained output set sensors sensor receiving di erent combination source signals 
adjective blind stresses fact source signals observed ii information available mixture 
sound approach modeling transfer sources sensors di cult unavoidable priori information available transfer 
lack ofa priori knowledge mixture compensated statistically strong physically plausible assumption independence source signals 
socalled blindness understood negatively weakness prior information precisely strength bss model making tool exploiting spatial diversity provided array sensors 
promising applications processing communications signals biomedical signals ecg eeg monitoring alternative principal component analysis see :10.1.1.164.7477
simplest bss model assumes existence independent signals sn observation mixtures xn mixtures linear instantaneous xi see ica page cnl group www cnl salk edu ica cnl html biomedical applications 
sn fig 

mixing separating 
unobserved signals observations estimated source signals fig 

outputs top row bottom row separating matrix obtained adaptation samples mixture constant modulus signals 
subplot complex plane clustering circles shows restoration constant modulus property 
compactly represented mixing equation yn sn column vector collecting source signals vector similarly collects observed signals square mixing matrix contains mixture 
denotes transposition 
bss problem consists recovering source vector observed data assumption independence entries input vector possibly priori information probability distribution inputs 
formulated computation separating matrix output bx estimate vector source signals 
shows example adaptive separation real digital communications signals sensor array collects complex valued noisy mixtures sources signals constant modulus envelope 
successful separation adaptation evidenced restoration constant modulus output 
gure underlying bss algorithm optimizes cost function composed penalty terms correlation outputs deviation bs close official version published proceedings ieee vol 
pp 
oct modulus constant value 
example introduces points developed penalty term involving pairwise decorrelation second order statistics lead separation source separation go secondorder statistics see section ii source separation obtained optimizing contrast function scalar measure distributional property output constant modulus property isvery speci general contrast functions measures entropy mutual independence high order divergence joint distribution model 
contrast functions discussed sec 
iii show relate derived maximum likelihood principle 
fast adaptation possible simple algorithms see secs 
iv blind identi cation accurate small number samples see sec 
vi performance analysis 
basic bss model extended directions 
considering instance sensors sources noisy observations complex signals mixtures obtains standard narrow band array processing beam forming model 
extension consider convolutive mixtures results multichannel blind deconvolution problem 
extensions practical importance restricted simplest model real signals sensors sources non convolutive mixtures noise free observations captures essence bss problem objective basic statistical ideas focusing principles 
pointers provided section papers addressing general models 
organized follows section ii discusses blind identi ability section iii iv contrast functions estimating functions starting information theoretic ideas moving suboptimal high order approximations adaptive algorithms described section section vi addresses performance issues 
ii 
modeling identifiability 
source separation possible 
extent source signals recovered 
properties source signals allowing partial complete blind recovery 
issues addressed section 
bss model source separation exploits primarily spatial diversity fact di erent sensors receive di erent mixtures sources 
spectral diversity exists exploited approach source separation essentially spatial looking structure sensors time 
consequence ignoring time structure information contained data exhaustively represented sample distribution observed vector graphically depicted instance 
bss problem identifying probability distribution vector sample distribution 
perspective statistical model components mixing matrix probability distribution source vector mixing matrix 
mixing matrix parameter interest 
columns assumed linearly independent see discussion general case invertible 
special having invertible matrix unknown parameter matrices represent linear transformations 
model particular instance transformation model 
furthermore set invertible matrices forms multiplicative group 
simple fact profound impact source separation allows design algorithms uniform performance behavior completely independent particular mixture sec 
sec 
vi 
source distribution 
probability distribution source nuisance parameter means primarily interested knowing estimating distributions necessary estimate ciently parameter interest 
distribution source say lot joint distribution key assumption mutual source independence 
source assumed probability density function pdf denoted qi independence assumption simple mathematical expression joint pdf source vector qn sn qi si product densities sources marginal densities 
source separation techniques di er widely explicit implicit assumptions individual distributions sources 
range options 
source distributions known advance 

features known moments heavy tails bounded support 

belong parametric family 

distribution model available 
priori stronger assumption narrower applicability 
designed approaches fact surprisingly robust gross errors modeling source distributions shown 
ease exposition zero mean 
cardoso blind signal separation statistical principles fig 

sample distributions di erent transformation matrices pairs distributions 
left right identity transform permutation sources sign change rotation generic linear transform 
sources assumed es esi blind identi ability issue blind identi ability understand extent matrix determined sole distribution observed vector 
answer depends distribution known 
square matrix said non mixing non zero entry row column 
non mixing cs copy entries identical permutations changes scales signs 
source separation achieved copy obtained 
distribution unknown expect better signal copy situation bit di erent prior information distribution available sources distinct distributions possible permutation detected scale source known amplitude corresponding column estimated intuition identi ability gained considering simple examples mixing 
row gure shows sample distributions pair independent variables various linear transforms 
columns successively show ect rotation linear transform 
visual inspection transformed distribution compared original gives feeling transform matrix observation mixture 
rst row shows case second column identi ed sign symmetrically distributed origin distribution 
second row shows severe symmetric distribution transform determined arbitrary changes sign permutation 
row shows severe case normally distributed equal variance joint distribution invariant rotation 
simple examples suggest blindly identi ed possibly induced symmetries distribution source vector case known source distributions 
knowledge necessary eye certainly capture distortion columns gure undistorted shapes rst column 
graphical signature independence pdf shape rst column clearly appears distorted 
intuition supported statement adapted comon theorem 
see 
vector independent entries gaussian entry invertible matrix entries cs independent copy non mixing 
linear transform non mixing independent entries gaussian vector entries independent 
key result entails blind signal separation achieved restoring statistical independence 
theoretical result blind identi ability suggests bss algorithms devised maximizing independence outputs separating matrix 
section iii shows maximum likelihood principle support idea leads speci measure independence 
independence decorrelation 
blind separation independence independence reduced simple decorrelation conditions pairs readily seen fact symmetry conditions pair sources unknown parameters 
second order information decorrelation reduce bss problem simpler form 
assume simplicity source signals covariance matrix identity matrix ess vector said spatially white 
whitening matrix spatially white 
composite transform wa necessarily rotation matrix relates spatially white vectors 
whitening sphering data reduces mixture rotation matrix 
means separating matrix product uw close official version published proceedings ieee vol 
pp 
oct fig 

decorrelation leaves unknown rotation 
whitening matrix rotation matrix 
note rotation uz preserves spatial whiteness equivalent approaches exploiting source decorrelation nd uw spatial rotation ii nd whiteness constraint write whiteness constraint ehw hw yy spatial whiteness imposes constraints leaving unknown rotation parameters determined second order information second order information able half bss job 
approach sensible algorithmic point view necessarily statistically cient see sec 
vi 
enforcing whiteness constraint amounts believe second order statistics nitely reliable kind statistics 
course untrue 
likelihood section examines simple graphical way likelihood source separation models 
likelihood model probability data set function parameters model 
simple model vector discussed sec 
ii parameterized pair mixing matrix density source vector pair classically det aj samples modeled independent 
normalized divided log likelihood parameter pair tx log log log det aj figures show likelihood landscape varied kept xed 
gure independent realizations drawn pdf mixed matrix produce samples data set log likelihood true distribution model distribution pi skew symmetric pi symmetric fig 

log likelihood slightly ed model source distribution maximum reached close true value 
follows exactly model true mixing matrix true source distribution 
gures show log likelihood varied true value model density kept xed 
gures illustrate impact choice particular model density 
gures matrix parameter varied directions matrix space am matrix cosh sinh sinh cosh cos sin sin cos just convenient way generate neighborhood identity matrix 
small called symmetric parameters respectively 
controls particular deviation away identity 
true source distributions uniform model takes mixture normal distributions variance di erent means second column 
true hypothesized sample distributions displayed upper left right corners plot 
incorrect model source distribution gure shows likelihood maximal mixing matrix data model close true sources binary see upper left corner gaussian model exp 
gure shows likelihood am depend 
cardoso blind signal separation statistical principles log likelihood true distribution model distribution pi skew symmetric pi symmetric fig 

log likelihood gaussian model source distribution contrast skew symmetric direction 
log likelihood true distribution model distribution pi skew symmetric pi symmetric fig 

log likelihood widely ed model source distribution maximum reached mixing system 
skew symmetric parameter insu ciency gaussian modelling 
source modeled true identical source distributions mixtures normal distributions mean di erent variances second column 
disaster happens likelihood longer maximum vicinity value maximizing likelihood estimate source signals bs ax obtains maximally mixed sources 
explained section iii 
bottom line informal study necessity non gaussian modeling possibility approximate model sources existence limit cation source model 
wrong source distribution model 
quanti ed section vi 
iii 
contrast functions section introduces contrast functions objective functions source separation 
maximum likelihood principle starting point suggesting information theoretic objective functions sec 
iii shown related class objective functions high order correlations sec 
iii 
minimum contrast estimation general technique statistical inference encompasses techniques maximum likelihood squares 
relevant blind deconvolution see inspiring introduced related bss problem comon 
instances contrast function real function probability distribution 
deal functions special notation useful agiven random variable generically denotes function denotes function distribution instance mean value denoted ex 
contrast functions source separation contrasts short generically denoted 
real valued functions distribution output bx serve objectives designed way source separation achieved reach minimum value 
words valid contrast function matrix satisfy cs equality cs copy source signals 
mixture reduced rotation matrix enforcing whiteness constraint sect 
ii consider orthogonal contrast functions denoted minimized whiteness constraint information theoretic contrasts maximum likelihood ml principle leads contrasts expressed kullback divergence 
kullback divergence probability density functions de ned fjg log ds integral exists 
divergence distributions random vectors concisely denoted wjz 
important property ofk wjz equality distribution 
distance symmetric understood statistical way quantifying closeness distributions 
close official version published proceedings ieee vol 
pp 
oct true distribution hypothesized distribution estimated distribution fig 

maximum likelihood estimator misled 
matching distributions likelihood infomax likelihood landscapes displayed gures assumes particular pdf source vector 
denoting random vector distribution simple calculus shows log 
xjs cst gures approximately display upto constant term minus kullback divergence distribution hypothesized distribution sources 
shows maximum likelihood principle associated contrast function ml normalized log likelihood seen estimate constant 
ml principle says simple applied bss problem nd matrix distribution close possible kullback divergence hypothesized distribution sources 
instability problem illustrated may understood follows gure likelihood maximum isa rotation true source distribution closer hypothesized source distribution rotated 
gure shows rotation areas highest density correspond points highest probability hypothesized source model 
di erent approach contrast function popular neural network community 
denote gi distribution function gi qi dt qi denote gn sn interpretation infomax principle see suggests contrast function im denotes shannon entropy random vector density logp du convention log :10.1.1.31.5414
idea understood follows hand uniformly distributed pdf hand uniform distribution highest entropy distributions 
cs highest entropy infomax idea yields contrast likelihood fact im ml 
connection maximum likelihood infomax noted authors see :10.1.1.48.120
matching structure mutual information simple likelihood approach described xed hypothesis distribution sources 
problem hypothesized source distributions di er true ones illustrated 
suggests observed data modeled adjusting unknown system distributions sources 
words minimize divergence respect distribution respect model distribution minimization problem simple intuitive theoretical solution 
denote random vector independent entries ii entry distributed corresponding entry classic property yj vector independent entries 
yj depend eq 
shows minimizing second term simply achieved mins yj 
having minimized likelihood contrast respect source distribution leading yj program completed minimize respect minimize contrast function mi yj kullback divergence yj distribution closest distribution independent entries traditionally called mutual information entries 
satis es mi equality dis tributed de nition happens entries independent 
words mi measures independence entries mutual information quantitative measure independence associated maximum likelihood principle 
note independent entries 

cardoso blind signal separation statistical principles ml mi nx decomposition global distribution matching criterion ml understood total mismatch deviation independence marginal mismatch maximizing likelihood xed assumptions distributions sources amounts minimize sum terms rst term true objective mutual information measure independence second term measures far marginal distributions outputs yn assumed distributions 
orthogonal contrasts mixing matrix reduced rotation matrix whitening explained sect 
ii contrast functions ml mi 
takes interesting alternative form whiteness constraint show mi constant term equal sum shannon entropies output 
whiteness constraint minimizing mutual information entries equivalent minimizing sum entropies entries de ne yi mi simple interpretation mixing entries tends increase entropies natural nd separated source signals minimum marginal entropies 
interesting notice yi constant kullback divergence distribution yi zero mean unit variance normal distribution 
minimizing sum marginal entropies equivalent driving marginal distributions far away possible normality 
interpretation mixing tends marginal distributions separating technique go opposite direction 
visual illustration tendency normality mixing 
rst column shows histograms independent variables bimodal distribution superimposed solid line best gaussian approximation 
columns shows histograms rotations steps going mixing maximal 
tendency normality isvery apparent 
fig 

mixing 
histograms top row bottom row rotated 
subplot shows estimated kurtosis de ned eq 
decreasing absolute value mixing 
entropic form mutual information starting point comon remains valid contrast weaker constraint volume preserving transformation 
discussion canonical contrast source separation mutual information mi expresses key property source independence include explicit implicit assumption distributions sources 
hand source distributions known ml appropriate expresses directly data model 
ml easier minimize gradient easily estimated see eq 
estimating gradient mi computationally demanding 
source distributions unknown may ml hypothesized source distributions need close true distributions recall sec 
ii qualitative explanation see sec 
vi statement sec 
adapting model distributions 
approach approximate kullback contrasts high order statistics examined 
high order approximations high order statistics de ne contrast functions simple approximations derived ml approach 
high order information simply expressed cumulants 
discussion limited cumulants order de nitions needed 
zero mean random variables nd order cumulants identical nd order moments cum th order cumulants cum random variables split groups mutually independent cumulant zero 
indepen close official version published proceedings ieee vol 
pp 
oct dence second order decorrelation easily tested high order cumulants 
simplicity notation cumulants elements vector cij cum yi yj cum yi yj yk yl contrast functions introduced section 
clearly contrast expresses decorrelation entries contrary show contrast sources known non zero kurtosis 
fourth order information su cient solve bss problem interesting conjunction jointly provide approximation likelihood contrast symmetrically distributed distributions close normal room lacking discuss validity approximation stems edgeworth expansion see sec 

point determine closely approximates follow suggestion second fourth order information jointly 
orthogonal contrasts 
consider cumulant orthogonal contrasts 
orthogonal approach enforces whiteness corresponds replacing factor eq 
nite weight optimal weighting considered see sec 
equivalently minimizing whiteness constraint 
simple algebra shows term nx ef source vector independent entries cross cumulants vanish cij ij ki kronecker symbol de ned variance kurtosis ki th source second fourth order si cii es ki es likelihood contrast ml measure mismatch output distribution model source distribution 
measure de ned quadratic mismatch cumulants cij cij ij cij ij ij ki pleasant ki yi 
nding contrast function expectation function simple estimate sample average 
recall contrast function ml de ned eq 
depends source model de ned hypothetical density source distribution 
similarly fourth order approximation requires hypothesis sources fourth order hypothesis sense kurtosis ki source speci ed de nition 
manner minimizing ml source distribution yields mutual information contrast ml minimizing approximates ml kurtosis ki source yields approximation mutual information 
nds mi ica iiii iiii cst orthogonal fourth order approxima de ned tion 
obtained rst comon slightly di erent route approximating likelihood gram charlier expansion 
contrast similar mi rst form involves terms measuring th order independence entries second form stems fact holds see 
similar ki case close separation 
bene considering th order orthogonal contrasts ica optimized jacobi technique unknown rotation sec 
ii sequence rotations applied sequence pairs yi yj optimal angle step available close form 
comon formula ica case real signals 
independence tested smaller subset cross cumulants jade motivation speci subset jade joint diagonalization criterion entailing optimized rotation angles close form complex case 
similar technique described 

cardoso blind signal separation statistical principles 


fig 

variation orthogonal contrast functions solid ica dash dots jade dashes dots sources kurtosis rotated 
left center right 
simpler contrasts kurtosis sources known 
instance eq 
suggests negative kurtosis ki avery simple contrast nx ey see condition ki kj pairs sources su cient stationary points orthogonal contrast function locally stable see sec vi :10.1.1.131.165
properties fourth order contrasts discussed illustrated displaying variation orthogonal contrast functions source case source vector kurtosis rotated angle 
left panel sources identical kurtosis contrasts minimized integer multiples 
center panel source gaussian contrasts show smaller variations note knows source zero kurtosis distinguishing odd multiplies 
right panel violates condition contrast minima maxima vice versa 
phenomenon illustrated gure 
iv 
estimating functions design valid contrast functions reach minima separating point model holds sense better 
practice contrasts estimated nite data set sample contrasts depend distribution distribution 
estimation nite data set introduces stochastic errors depending available samples contrast function 
statistical characterization minima contrast functions needed provide basis comparing contrast functions 
purpose notion estimating function introduced closely related gradient algorithms bss sec 

relative gradient variation contrast function linear transform may de ning relative gradient 
speci notion builds fact parameter interest square matrix 
de nition 
nitesimal transform 
ey small matrix 
ey smooth ey expanded ey nx kek gij partial derivative ey respect eij 
coe cients form matrix denoted called relative gradient 
matrix form expansion reads ey hr kek euclidean scalar product matrices trace mn 
note relative gradient de ned explicit possible dependence bx characterizes rst order variation contrast function 
course possible relate regular gradient respect bx 
elementary calculus yields bx bx notion natural gradient independently introduced amari 
distinct general relative gradient de ned continuous group transformation de ned smooth statistical model 
bss model statistical transformation model combines features ideas yield class algorithms sec 

score functions 
source densities qn de ne likelihood bss model enter estimating function log derivatives called score functions de ned log qi qi qi close official version published proceedings ieee vol 
pp 
oct fig 

densities associated scores 
displays densities associated score functions 
note score basic distribution basic function zero mean unit variance gaussian variable exp associated score gaussian densities precisely densities associated linear score functions 
necessity non gaussian modeling recall section ii translates necessity considering non linear score functions 
relative gradient likelihood contrast 
core bss contrast functions ml associated likelihood source densities qn 
relative gradient :10.1.1.29.2911
ml eh 
entry wise non linear func tion yn collecting score functions related source 
remarkably simple result merely expected value xed function interpretation 
ml contrast function ml minimum points relative gradient cancels points solutions matrix equation eh 
interpreted examining th entry matrix equation 
nd yi yi depends yi determines scale th source estimate 
th entry eh reads yi yj meaning jth output yj uncorrelated non linear version yi ith output 
non linear functions conditions pairs pair general equivalent 
note source signals modeled zero mean unit variance normal variables yi yi yy hw recalling def 

ml minimum points obtain whiteness condition 
su cient determine separating solution score functions non linear source model non gaussian 
idea non linear functions obtain su cient set independence conditions traced back seminal herault jutten see english choice non linear functions somewhat ad hoc gave interpretation non linear functions ampli ers signals interest bar ness produced early non linear functions 
ml principle clear non linear functions related non gaussian model source distributions 
estimating functions estimating function bss problem function 
associated estimating equation tx called matrix valued equation speci es priori constraints unknown parameters bss problem 
bss estimates characterized estimating function 
simple instance estimating function hw eq 
express decorrelation entries equation hw equivalent expresses empirical whiteness batch samples opposed actual whiteness ehw 
estimating function hw appropriate bss whitening decorrelation su cient separating matrix 
simplest example estimating function bss obtained ml approach 
gradient likelihood may shown cancel points aml exactly characterized eq :10.1.1.29.2911
mlx de ned 
words maximum likelihood estimates correspond exactly solution estimating equation 
equation sample counterpart eh characterizes stationary points ml 
recall obtained eqs 
limit log likelihood 
value estimating function square matrix decomposed symmetric part equal transpose skew symmetric part opposite transpose 

cardoso blind signal separation statistical principles decomposition simply optimization regular contrast function corresponds estimating function optimization contrast whiteness constraint corresponds estimating function hw symmetric part replaced hw yy introduced eq 
ect enforce whiteness constraint 
particular maximum likelihood estimates whiteness constraint solutions eq 
estimating function yy orthogonal contrast functions associated similar estimating functions 
instance simple th order contrasts eqs 
yield estimating equations form non linear functions respectively yi yi recall contrast function supposes sources negative kurtosis ki 
functions agree sign cubic distortion expected 
contrast functions ica jade estimated samples minimized points represented exactly solution xed estimating function 
nd asymptotic estimating function sense solution associated estimating equation close minimizer estimated contrast 
instance contrast ica jade asymptotically associated estimating function 
implies minimizing ica jade cumulants estimated samples yields estimates equivalent di er term smaller estimation error large functions appropriate estimating functions 
think eh estimating function estimating equation just sample counterpart eh priori provide scalar equations unknown parameters 
ml principle suggests speci forms non linear functions approximations score functions probability densities signals separated 
adaptive algorithms simple generic technique optimizing objective function gradient descent 
optimization problems simplicity expense performance sophisticated techniques newton algorithms second derivatives addition gradient signi cantly speed convergence 
bss problem turns simple gradient descent ers newton performance see 
surprising fortunate result obtained descending relative gradient de ned sec 
iv 
relative gradient techniques relative gradient descent 
rst describe generic relative gradient descent 
generally steepest descent technique minimization consists moving small step direction opposite gradient objective function 
relative gradient contrast de ned sec 
iv respect relative variation changed resulting variation rst order scalar product hr ei relative variation relative gradient eq 

aligning direction change direction opposite gradient fora small positive step step relative gradient descent formally described resulting variation hr hr kr negative positive formal description turned line line algorithms described 
line relative gradient descent 
consider separation batch samples minimization contrast function relative gradient eh 
looks linear transform data satisfying corresponding estimating equation 
relative gradient descent solve goes follows set iterate steps bh tx hy rst step computes estimate relative gradient current values data second step updates data relative direction opposite relative gradient 
close official version published proceedings ieee vol 
pp 
oct algorithm stops pt estimating equation solved 
amusing note implementation need maintain separating matrix directly operates data set source signals emerging iterations 
line relative gradient descent 
line algorithms update separating matrix bt reception new sample 
relative linear transform corresponds changing eb 
line mode uses stochastic gradient technique gradient eh replaced instantaneous value 
stochastic relative gradient rule bt bt th bt sequence positive learning steps 
uniform performance relative gradient descent 
striking feature bss model hardness statistical sense discussed section vi separating mixed sources depend particular value mixing matrix problem uniformly hard mixture 
signi cantly device relative updating produces algorithms behave uniformly mixture 
right multiplying updating rule matrix bx bas readily nds trajectory global system ct bta combines mixing unmixing matrices governed ct ct th cs ct trajectory expressed sole function global system ct ect mixing matrix determine initial value global system 
desirable property means algorithms studied optimized actual mixture inverted 
true estimating function uniformly performance expected correctly adjusted distribution source signals instance deriving contrast function 
algorithms estimating function form described line version line version form studied detail :10.1.1.29.2911
uniform performance obtained 
regular gradient algorithms 
interesting compare relative gradient algorithm algorithm obtained regular gradient applying gradient rule entries minimization bx 
bt bt th form costly requires general inversion bt step lacks uniform performance property trajectory global system depends particular mixture inverted 
adapting sources iterative adaptive algorithms described require speci cation estimating function forms eqs 
suggested theory 
forms turn depend non linear functions ideally score functions associated distributions sources sec 
iv 
source distributions unknown may estimate data instance parametric model directly estimate non linear functions 
rst idea edgeworth expansions see approximations probability densities vicinity gaussian density 
simplest non trivial edgeworth approximation symmetric pdf vicinity standard normal distribution exp kurtosis corresponding approximate score function edgeworth expansion suggests linear cubic approximation score function coe cient cubic part ki ith source 
asymptotic analysis shows guarantees local stability sec 
vi 
possibilities deriving score functions density expansion see instance di erent proposal involving odd terms 
direct approach pdf expansion proposed pham considers approximating linear combination lx lfl xed set ff flg arbitrary basis functions :10.1.1.29.2911:10.1.1.29.2911
surprisingly set lg coe cients minimizing mean square error true score 
cardoso blind signal separation statistical principles gamma gamma gamma fig 

top row distributions values measure non gaussianity see sec 
vi 
bottom row score function solid linear cubic approximations edgeworth expansion optimal dashes 
approximation knowing best mean square approximation involves expectation operator 
ef ef fl column vector basis functions column vector derivatives 
nice result expression simply estimated replacing expectations sample averages values estimated source signals 
approaches edgeworth expansion mean square respectively leading approximations compared gure 
pdf displayed top row bottom row shows corresponding score function solid line linear cubic approximation dash dotted line pham approximation dashed line obtained 
approximations similar rst example pdf close gaussian second case optimal approximation ts better true score area highest probability 
approximations really third example simple reason true score approximated linear cubic function 
approximations score guarantee stability gradient algorithms see sect 
vi 
vi 
performance issues section concerned performance bss algorithms presents asymptotic analysis results 
repeatedly stressed necessary know source distributions equivalently associated score functions great accuracy obtain consistent bss algorithms 
limit cation source distributions illustrated elucidated sec 
vi gives explicit stability limits 
hypothesized distribution preserve stability may expect loss estimation accuracy due cation nite number samples available quanti ed sec 
vi describes ultimate achievable separation performance 
concluding section vi discusses general property equivariance governs performance bss algorithms 
local stability stationary point equilibrium point learning rule characterized eh eh bx mean value update zero 
seen separating matrices proper scale equilibrium points interested nding locally stable small deviation equilibrium pulled back separating point 
words want separating matrix local attractor learning rule 
limit small learning steps exists simple criterion testing local stability depends derivative eh bx respect symmetric form asymmetric form stability condition worked exactly 
depend nonlinear moments si es si si si rescaled eh si si es leaving aside issue stability respect scale stability conditions symmetric form asymmetric form conditions stability appears depend pairwise conditions 
stability domains pair sources displayed plane 
note stability domain larger symmetric form consequence letting second order information whiteness constraint half job see sec 
ii 
comments order 
appears cases su cient stability condition sources 
regarding stability tuning non linear functions close official version published proceedings ieee vol 
pp 
oct stability regions boundary symmetric form boundary asymmetric form fig 

stability domains plane 
source distributions understood making positive 
second show si gaussian function 
stability conditions met gaussian source agreement identi ability statements sec 
ii 
third shown taken score function true density si equality si gaussian 
section ii illustrated fact hypothesized source distributions close true distributions likelihood show maximum separating point 
de nition provides quantitative measure wrong hypothesis allow negative 
note necessary positive source compensated moments large seen stability domains source arbitrarily negative symmetric form stability asymmetric form requests 
considered linear cubic score functions secs 
iv si isi constants es iki ki denotes kurtosis 
note linear part ect stability stability guaranteed coe cient cubic part sign opposite sign kurtosis 
quite naturally functions eq 
come naturally right sign 
wishes cubic non linearities su cient know sign kurtosis source separating matrices stable 
cubic scores stability depends sign sign kurtosis 
accuracy estimating equations section characterizes accuracy signal separation obtained solving estimating equation independent realizations matrix separation pth entry bx bas contains signal interest sp power ba pp qth interfering signal sq power ba pq matrix quantity pq ba ba ppes measures interference signal ratio isr provided rejecting qth source estimate pth source 
bt separating matrix obtained particular algorithm samples 
general estimation error regular statistical models decreases limit lim pq bt usually exists provides asymptotic measure performance separation line bss technique 
estimating equation asymptotic isr depends moments si es si si simplicity consider identically distributed signals identical non linear functions symmetric estimating function nds isr pq isr note isr lower bounded regardless value general property orthogonal bss techniques price pay blindly trusting second order statistics whitening 
rejection rates obtained whiteness constraint better asymmetric estimating function isr take simple form common score obtained pham method sec 

nds isr isr pq isr isr isr equation stems pham method guarantees expressions show isr isr minimized maximizing surprisingly shown reach maximum value precisely 
cardoso blind signal separation statistical principles score function corresponding true density sources es note solution ml estimator true model 
follows expression isr asymptotic cramer rao bound source separation best achievable isr rate independent samples see 
achievable performance depends magnitude moment characterizes hardness bss problem respect source distribution 
surprisingly relate non gaussianity sources follows 
denote score function true distribution denote score function gaussian distribution variance just es 
large non gaussianity translates large di erence just saw measure non gaussianity asymptotic point measured 
nds see values examples 
close gaussian sources ily small case best achievable rejection rates symmetric asymmetric forms 
gives idea minimum number samples required achieve agiven separation 
extreme sources far away moment bounded 
particular tends source distributions tend discrete bounded support 
case discrete sources deterministic error free blind identi cation possible nite number samples 
case sources bounded support mse blind identi cation decreases faster rate rate obtained nite values see particular 
equivariance uniform performance rst thought hardness bss problem depend distributions source signals mixing matrix harder problems sources nearly gaussian mixing matrix poorly conditioned 
correct bss problem uniformly hard mixing matrix 
summarize instances property appeared ultimate separation performance depends 
eq 
asymptotic performance index eqs 
depend statistical moments stability adaptive algorithms depends values better trajectory global system ct bta depend sole ect determine initial point 
problem appears uniformly hard mixing matrix exists estimation techniques statistical behavior regarding signal separation independent particular value system inverted 
desirable property algorithms studied tuned independently particular mixture inverted performance predicted independently mixture 
instance equivariance property holding generally transformation models 
simple prescription design algorithms uniform performance adjust freely constraint separating matrix rule expressed terms output understand output prescription ensures uniform performance consider instance particular estimating function separate mixture samples 
source signals mixed matrix solution matrix ba matrix solution cs 
matrix depend global system ba independent estimated signals cs regardless particular recovered signals exactly identical obtained mixing 
argument estimating equations extends minimizers contrast functions de ned functions distribution output argument apply orthogonal contrast functions whiteness constraint terms 
argument justi es speci de nition relative gradient device needed express rst order variations contrast function terms variation stressed argument involve asymptotics equivariance exactly observed nite value bss algorithms equivariant 
instance original algorithm jutten herault imposes constraints separating matrix resulting greatly complicated analysis behavior see 
instances non equivariant techniques algebraic approaches see sec 
vii structure cumulants observed vector precisely identi cation close official version published proceedings ieee vol 
pp 
oct approaches equivariant general shown equivalent optimization contrast function caution necessary concluding equivariance holds exactly noise free model wehave considered far 
practice kind noise taken account 
assume better model represents additive noise 
rewritten 
long neglected respect noise free situation 
shows limit equivariance poorly conditioned matrix large inverse ampli es ect noise 
precisely expect equivariance high snr domain covariance matrix remains larger covariance matrix vii 
due limited space focus principles interesting issues left discussion connections bss blind deconvolution convergence rates adaptive algorithms design consistent estimators noisy observations detection number sources concluding brie mention points 
algebraic approaches 
th order cumulants avery regular structure bss model nx sample estimates cumulants equation set subset solved square sense 
cumulant matching approach yield equivariant estimates 
optimal matching shown correspond contrast function 
speci form calls algebraic approaches 
simple algorithms eigen structure cumulant matrices built cumulants 
exciting direction research investigate decompositions generalize matrix factorizations svd evd th order cumulants 
temporal correlation 
approaches bss described exploit properties distribution 
source signals temporally correlated time structures exploited 
possible achieve separation source signals distinct spectra source signal gaussian process 
simple algebraic techniques devised see whittle approximation likelihood investigated 
properties exist exploited 
deterministic identi cation 
indicated sec 
vi sources discrete support allow deterministic identi cation nite fisher information 
speci contrast functions devised take advantage discreteness 
rich domain application digital communication signals coding information discrete symbols deterministic identi cation possible 
see review van der veen papers cma issue 
open problems perspectives 
learning source distributions 
bss problem source distributions nuisance parameter 
large sample size possible estimate distributions obtain asymptotic performance distributions known advance design practical algorithms achieving source adaptivity question 

dealing noise 
bss techniques remaining consistent presence additive noise described 
additive gaussian noise techniques may resort high order cumulants noise modeling 
clear worth combating noise 
matter fact may argue noise ects account unnecessary high snr futile low snr bss problem di cult anyway 
believe open question determine application domains really bene noise modeling 

global convergence 
cumulant contrast functions proved free spurious local minima source case see de ation approach successive extractions source signals 
lack general understanding global shape contrast functions general case 

multidimensional independent components 
interesting original variation basic ica model decompose random vector sum independent components requirement components linearly independent necessarily dimensional 
bss model equivalent grouping source signals subsets independence subsets subsets 
general decomposition called multidimensional independent component analysis mica 

convolutive mixtures 
challenging open problem bss probably extension convolutive mixtures 
active area research mainly motivated applications audio frequency domain bss 
cardoso blind signal separation statistical principles termed cocktail party problem 
convolutive problem signi cantly harder instantaneous problem input output non blind identi cation challenging large number parameters usually necessary describe audio channels 

model hold 
mentioned successful applications bss biomedical signals 
examining data striking realize extracted source signals far obeying simple bss model 
fact bss yields apparently meaningful experts results worth consideration 
partial explanation stems basing separation contrast functions model hold independent source signals system inverted algorithms try produce output independent possible 
tell story data sets stochastic description appropriate 
believe interesting challenge understand behavior bss algorithms applied outside model 
indebted anonymous reviewers constructive comments helped improving rst version 
ica cnl group salk institute 
www cnl salk edu ica cnl html 

amari 
natural gradient works ciently learning 
neural computation 

amari 
cardoso 
blind source separation semiparametric statistical approach 
ieee trans 
sig 
proc nov 
special issue neural networks 

amari 
chen cichocki 
stability analysis adaptive blind source separation 
neural networks 

amari cichocki yang 
new learning algorithm blind signal separation 
advances neural information processing systems pages denver 
mit press 
anand reddy 
blind separation multiple channel bpsk signals arriving antenna array 
ieee signal proc 
letters sept 
back weigend 
rst application independent component analysis extracting structure stock returns 
int 
journal neural systems vol 
pp 
aug 
bar ness 
adaptive interference practical limitations 
proc 
globecom pages nov 
bell sejnowski 
approach blind separation blind deconvolution 
neural computation 
bell sejnowski 
edges independent components natural scenes 
advances neural information processing systems 
mit press 

cardoso eric moulines 
blind source separation technique second order statistics 
ieee trans 
sig 
proc feb 
benveniste 
robust identi cation non minimum phase system 
blind adjustment linear equalizer data communication 
ieee tr 
ac 
schell gardner 
spectral self coherence new approach blind adaptive signal extraction antenna arrays 
proceedings ieee pages apr 

cao 
liu 
general approach source separation 
ieee trans 
signal processing mar 

cardoso 
super symmetric decomposition fourth order cumulant tensor 
blind identi cation sources sensors 
proc 
icassp pages 

cardoso 
performance orthogonal source separation algorithms 
proc 
eusipco pages edinburgh sept 

cardoso 
equivariant approach source separation 
proc 
pages 

cardoso 
estimating equations source separation 
proc 
icassp pages 

cardoso 
infomax maximum likelihood source separation 
ieee letters signal processing apr 

cardoso bose 
optimal source separation second fourth order cumulants 
proc 
ieee workshop greece 

cardoso comon 
tensor independent component analysis 
proc 
eusipco pp 


cardoso laheld 
equivariant adaptive source separation 
ieee trans 
sig 
proc dec 

cardoso 
blind beamforming non gaussian signals 
iee proceedings dec 
chaumette comon muller 
ica technique radiating sources estimation application airport surveillance 
iee proceedings dec 
cichocki unbehauen 
robust learning algorithm blind separation signals 
electronic letters 
comon 
independent component analysis new concept signal processing elsevier apr 
special issue higher order statistics 
comon 
decomposition sums powers linear forms 
signal processing sept 
cover thomas 
elements information theory 
wiley series telecommunications 
john wiley 

unconstrained single stage criterion blind source separation 
proc 
icassp volume pages 
de lathauwer de moor vandewalle 
blind source separation higher order singular value decomposition 
proc 
eusipco volume pages 
de lathauwer andj vandewalle 
fetal extraction source subspace separation 
proc 
hos pages spain june 
de lathauwer de moor vandewalle 
blind source separation simultaneous third order tensor diagonalization 
proc 
eusipco trieste pp 

de lathauwer de moor vandewalle 
independent component analysis higher order statistics 
proc 
ieee workshop corfu pages 

adaptative blind separation independent sources de ation approach 
signal processing 
deville 
uni ed stability analysis herault close official version published proceedings ieee vol 
pp 
oct jutten source separation neural network 
signal processing june 
deville 
application blind source separation techniques multi tag identi cation system 
ieice transactions fundamentals electronics communications computer sciences 
donoho 
minimum entropy deconvolution 
applied time series analysis ii pages 
academic press 
vincent 
blind identi cation methods applied edf civil works power plants monitoring 
proc 
hos pages ban canada june 

de traitement aux radio communications 
de doctorat 
telecom paris june 

fort 
stability source separation algorithm jutten herault 
kohonen simula kangas editors arti cial neural networks pages 
elsevier 


source separation priori knowledge maximum likelihood solution 
proc 
eusipco pages 
elizabeth 
source separation input sources discrete constant modulus 
ieee trans 
signal processing 
giannakis 
modelling non gaussian array data cumulants doa estimation sources sensors 
signal processing july 
herault jutten ans 
detection de primitives dans un message composite par une architecture de calcul en apprentissage non supervise 
proc 
pages nice france 
hyvarinen oja 
fast xed point algorithm independent component analysis 
neural computation 
jutten herault 
blind separation sources adaptive algorithm neuromimetic architecture 
signal processing july 
karhunen hyvarinen oja 
applications neural blind separation signal image processing 
proc 
icassp volume pages 
karhunen oja wang 
class neural networks independent component analysis 
ieee transactions neural networks may 
eric moreau 
self adaptive part convergence analysis direct linear network jutten algorithm 
ieee trans 
signal processing apr 
mackay :10.1.1.48.120
maximum likelihood covariant algorithms independent component analysis 
preparation 
makeig bell 
jung andt sejnowski 
independent component analysis electroencephalographic data 
advances neural information processing systems 
mit press 
mccullagh 
tensor methods statistics 
monographs statistics applied probability 
chapman hall 
moody wu 
true price state space models high frequency nancial data 
progress neural information processing 
proceedings international conference neural information processing volume pages 
springer verlag 
moreau 
high order contrasts self adaptive source separation 
international journal adaptive control signal processing jan 

parga 
nonlinear neurons low noise limit factorial code maximizes information transfer 
network 
deco 
information theory learning paradigm linear feature extraction 
neurocomputing 
pearlmutter parra 
context sensitive generalization ica 
international conference neural information processing hong kong 

asymptotic expansions related minimum contrast estimators 
annals statistics 

pham 
blind separation instantaneous mixture sources independent component analysis 
research lmc imag grenoble france 

pham 
blind separation instantaneous mixture sources independent component analysis 
ieee trans 
sig 
proc nov 

pham 
separation de sources 
proc 
pages 

pham 
blind separation mixture independent sources quasi maximum likelihood approach 
ieee tr 
sp july 
soon tong huang liu 
extended fourth order blind identi cation algorithm spatially correlated noise 
proc 
icassp pages 

experiments array data collected actual urban environments 
ieee workshop signal processing advances wireless communications pages apr 
tong inouye liu 
waveform preserving blind estimation multiple independent sources 
ieee tr 
sp july 
tong soon huang liu 
new blind identi cation algorithm 
proc 
iscas 
tong soon huang liu 
necessary su cient condition blind identi cation memoryless systems 
proc 
iscas volume pages singapore 

van der veen 
blind beamforming 
issue 
yang amari 
adaptive line learning algorithms blind separation maximum entropy minimum mutual information 
neural computation oct 
yellin 
multi channel system identi cation deconvolution performance bounds 
proc 
ieee workshop corfu pages 
yuen 
asymptotic performance analysis blind signal copy fourth order cumulants 
international journal adaptive control signal processing pages mar 
