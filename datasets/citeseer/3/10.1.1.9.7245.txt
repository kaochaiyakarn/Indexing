feature saliency unsupervised learning martin law figueiredo anil jain clustering common unsupervised learning technique discover structure set multidimensional data 
exist algorithms clustering important issue feature selection attributes data clustering algorithms rarely touched 
feature selection clustering dicult supervised learning class labels data obvious criteria guide search 
important satisfactory solved problem partitional clustering determine correct number clusters 
propose algorithm solve problems simultaneously 
making hard decision select di erent features estimate salient individual features 
requiring user specify number clusters estimate number directly data 
expectation maximization em algorithm minimum message length mml criteria derived 
synthetic real data demonstrate potential proposed algorithm 
clustering feature selection unsupervised learning deals nding structure collection patterns class labels 
clustering arguably important unsupervised learning problem 
goal clustering determine intrinsic grouping set unlabeled data 
important applications clustering including analysis genomic data mining customers behavior automatic text categorization web documents image segmentation mention examples 
large variety clustering algorithms proposed martin law anil jain department computer science engineering michigan state university east lansing mi 
mail cse msu edu jain cse msu edu 
research supported onr 

figueiredo institute telecommunications department electrical computer engineering instituto superior lisboa portugal mail mtf lx pt 
research supported foundation science technology portuguese ministry science technology project posi sri 
di erent application scenarios 
consider known model approach clustering learning mixture gaussians 
component mixture interpreted cluster 
central problem clustering decide data represented 
applications datum represented list vector attributes usually called features 
naturally features equally useful features may just noisy hampering degrading contributing clustering process 
illustrated 
feature considered nd separated gaussian clusters evident marginal distribution shown bottom gure 
noisy feature added clustering structure changes gaussian clusters best description mixture gaussians data 
note marginal distribution uniform 
addition noisy feature obscure change true structure data 
task selecting best feature subset known feature selection variable selection 
feature selection appears disguise metric selection metric depend irrelevant features 
example illustrating feature selection important clustering 
uniformly distributed noisy feature added spurious clusters appear 
feature selection important reasons 
important noisy features degrade performance learning algorithm example shows 
feature selection regarded method circumvent curse dimensionality learning exponentially dicult features 
smaller subset useful features may enable learning algorithm produce better results time 
feature selection important emergence data sets large numbers features 
example classi cation problems molecular biology may involve thousands features 
feature selection expert knowledge possible principle time consuming sub optimal 
similar phenomena observed text categorization 
web page example represented di erent key terms setting described number unique terms exceeds 
supervised learning scenarios classes approaches feature selection lters wrappers 
lter approaches features selected inherent properties independent learning algorithm subsequently selected features 
wrapper approaches learning algorithm black box help determine quality possible feature subset 
lter approaches usually faster performance may optimal learning task hand 
algorithm described loosely classi ed wrapper designed unsupervised learning scenario 
feature selection extensively studied decades see reported limited supervised learning 
feature selection unsupervised learning dicult class labels hard assess relevance feature subset features 
problem worse fact number clusters data set seldom known optimal number clusters optimal feature subset inter related 
fact illustrated example adopted 
features clusters data set 
feature considered see clusters cluster seen feature 
example illustrating choice feature subset related optimal number clusters feature selection unsupervised learning ways address problem feature selection unsupervised learning 
feature selection method assume class labels unsupervised learning 
example algorithm evaluates closely features related eigenvalues covariance matrix 
authors suggest heuristic compare quality di erent feature subsets normalized log likelihood cluster separability 
bayesian approach concept stochastic complexity evaluate di erent feature subsets numbers clusters 
clustering tendency feature assessed entropy index data similarity feature distributed uniformly entropy high feature irrelevant 
genetic algorithm perform feature selection means clustering encoding scheme ad hoc computational burden heavy 
feature selection unsupervised learning addressed conditional gaussian networks 
clique algorithm described tries nd dense regions feature subspace :10.1.1.131.5152
authors tackle unsupervised feature selection problem web documents hyper graph algorithms 
feature selection symbolic data addressed assuming irrelevant features related rest features 
devaney ram notion category utility feature selection conceptual clustering task 
proposed formulation feature saliency methods referred previous paragraph hard decisions utility feature feature selected discarded 
usefulness feature usually clear cut 
phenomenon eminent unsupervised learning features carry information sure feature completely useless say information provided feature useful 
way formalize consider saliency feature 
considering feature saliency decision maker estimate information may lost individual feature subset features discarded 
threshold feature saliency lead better feature subset threshold number useful features 
notion feature saliency considered 
authors assign weights di erent groups features means clustering score related fisher discriminant 
feature weighting means clustering considered goal concept learning nd best way describe cluster clusters learnt 
contribution algorithm estimates saliency feature mixture clustering framework require knowledge true number clusters 
dicult task di erent feature saliencies lead di erent optimal numbers clusters vice versa 
need simplifying assumption features conditionally independent mixture component 
case gaussian mixtures means covariance matrices diagonal 
assumption frequently particularly high dimensional data 
example speech recognition gaussian mixtures state hidden markov model hmm usually diagonal covariance matrices 
inspired derive expectation maximization em algorithm estimate feature saliencies 
feature selection algorithms method require forward backward search 
argument minimum message length mml criterion see incorporated select number components resulting simple algorithm performs automatic component pruning 
organization remainder follows 
section describe proposed formulation basic algorithm strengthened mml criterion section 
algorithm tested synthetic real data section 
conclude discuss directions section 
appendix containing mathematical details 
feature saliency section describe em algorithm estimating feature saliency unsupervised learning 
derive discuss mixture density feature saliency section introduce em algorithm estimating parameters model section 
mixture density nite mixture density components de ned having form yj non negative weights sum yj pdf th mixture component 
yd denotes features object 
set objects considered write yn id 
denote hidden component label indicates component generates sequel indices run data points mixture components features respectively 
consider class label features independent 
case mixture density rewritten jl jl set parameters component corresponding feature general jl distribution shall consider case gaussians 
addition assume features irrelevant mixture structure density sense feature irrelevant jl common density followed feature set binary indicator variables feature relevant irrelevant write mixture density jl considered similar mixture model feature selection supervised learning 
density slightly di erent shared mixture components classes 
em algorithm nds locally optimal estimate jl derived usual manner 
novel characteristics mixture equation indicator variables means th feature signi cant modeled individual mixture components 
th feature useless represented common component 
shall refer jl th mixture component common component 
referred common component applies mixture components evident rewriting equation jl problems formulation 
matches distribution non salient features optimal values lead feature selection 
may specify number features selected constraining number ones choosing desired number features dicult practice 
small useful features may discarded large irrelevant features may kept 
exibility achieved forcing zero estimate takes value 
motivates proposed mixture model section 
proposed formulation key insight treating binary parameters consider missing variables 
de ne probability th feature useful salient call feature saliency 
modi cation mixture density written see proof appendix jl recall jl distribution th feature th mixture component distribution th feature common component 
form re ects prior knowledge distribution non salient features 
principle distribution gaussian student distribution mixture gaussians 
shall limit gaussian leads reasonable results practice 
properties proposed formulation addition motivation equation feature saliency consideration interesting properties worth mentioning 
consider conditional distribution class label jl see features conditionally independent just case 
expected distributed independently 
observation distribution th feature components th mixture component weight common component weight 
represents important mixture components general th feature 
agrees feature saliency interpretation equation generative interpretation 
recall generate data point standard nite mixture equation rst generate component label sampling multinomial distribution parameters generating data density mixture component selected 
equation generating component label ip biased coin probability getting head th feature get head mixture component jl generate th feature common component 
interpret equation model communication insecure channel 
sender generates message rst selecting component label randomly sampling jl message sent receiver hacker eavesdropping channel attempts replace message message distributed 
probability hacker replaces th part message successfully 
distribution messages receiver follows equation 
di ers usual model noisy communication channel noise added message replacing parts 
parameter estimation em algorithm set observations yn id parameters jl estimated maximum likelihood criterion arg max log arg max log il jl il estimate closed form solution alternatively expectation maximization em algorithm see 
treating missing data derive see appendix equations step step 
note jl gaussians jl just consist unknown means variances 
step compute quantities ijl il jz il jl ijl il jz il ijl il jz ijl ijl ij jjy ijl ijl ijl jjy ijl ijl ij ijl jjy ij ijl ijl jy ijl step re estimate parameters expressions ij ij ij ij mean jl ijl il ijl var jl ijl il mean jl ijl mean ijl il ij ijl var ijl il mean jl ij ijl ijl ijl ijl ijl equations em algorithm clear intuitive interpretations 
variable ijl measures important th pattern th component th feature 
natural estimates mean variance jl weighted sums weights ijl similar relationship exists ijl term ijl interpreted equals explaining estimate proportional ij ijl interesting interpretation measures cluster consistency th feature 
consider compare values il jl il 
il jl larger il ijl ijl near 
contrary il jl smaller il ijl ijl near zero 
il interpreted kind density order account di erent scales data 
suppose ij large ijl ijl small 
means estimated th component features appear th component th feature considered 
words cluster assignment th feature inconsistent assignment features large ij small ijl ijl lead small ijl inconsistency occurs ij ijl small justifying cluster consistency interpretation related point view common component data mixture components explain 
th feature data small common component important th feature 
applying mml criterion weakness em mixtures standard em algorithm mixtures exhibits weak points course apply em algorithm described previous section 
em algorithm mixtures standard form requires priori speci cation number components rarely known practice 
appropriate initial parameter estimate fundamental order obtain local optimum 
problem particularly serious current setting combination feature saliencies corresponding optimal parameters mixture components may quite di erent corresponding di erent local optimum 
potential problem may estimated general easier explain distribution feature mixture single common component 
mml criterion overcome diculties described previous paragraph adopt minimum message length mml principle argument similar 
mml estimation criterion derived appendix proposed model arg min log ds log log log denote number parameters jl respectively 
assumed univariate gaussians arbitrary mean variance 
purely parameter estimation viewpoint equation equivalent arg max log rd log log rk log seen maximum posteriori map estimate improper priors rd rk priors conjugate respect complete data likelihood impact em algorithm just minor modi cation step equations modi ed max ij rd max ij rd max ijl kr max ijl kr max ijl interpretations rewrite mml criterion equation arg min log log log log log simple interpretation 
term log typical minimum encoding length criteria minimum description length mdl see mml see best known instances term related shannon optimal code length data model speci ed see 
terms log log standard mdl type parameter code lengths corresponding values values 
th feature th component ective number data points estimating jl parameters jl corresponding code length log 
similarly th feature common component number ective data points estimation 
term log equation feature 
penalties look appropriate 
mixture component rd parameters equation re estimation formula disguise 
large likelihood due mixture components signi cant general leading larger ijl penalty estimating large large case penalty kr 
similar penalty imposed common component regarded special case 
pruning behavior key feature equations pruning behavior occurs goes zero goes zero 
worries message length equation may invalid boundary values circumvented arguments 
speci cally goes zero jd removed model ectively reducing number mixture components 
goes zero th feature longer salient kl removed 
goes dropped 
mind criterion arg min log log log log denotes current number mixture components denote number features active mixture components active common component active mixture common component respectively 
equations step modi ed max ij rd max ij rd max ijl max ijl max ijl intuitive insight pruning behavior equations notice ective number data points involved estimating parameters component saliency individual feature small longer reliable perform estimation component feature pruned 
notice determine number components automatically initialize model large number mixture components alleviating need initial estimate 
component wise version em algorithm similar adopted details appendix 
experiments results section describe experiments carried demonstrate performance proposed approach 
synthetic real data sets 
synthetic data rst synthetic data set consists data points mixture equiprobable gaussian densities 
noisy features sampled density added data yielding set dimensional patterns 
ran proposed algorithm times initialized common component represented dashed ellipse initialized cover entire data feature saliencies initialized 
stopping threshold typical run algorithm shown 
algorithm proceeds mixture components pruned weights zero common component covers fewer data points snapshot shown 
note feature saliencies increased 
feature saliency feature pruned 
means common component feature longer variations axis explained mixture components shown common component shrinks ellipse straight line lower left data cluster 
learning algorithm may get stuck local minimum 
happens try escape local minimum forcing smallest mixture weight zero 
algorithm compare local minimums encountered select smallest mml shown 
observe common component shrunk dot origin gure saliencies features exactly 
runs mixture components correctly identi ed 
saliencies features standard deviations error bars shown 
expected rst features relevant larger saliency remaining irrelevant clustering data 
phenomenon consistent shown small error bars 
average runs distance estimated cluster centers true centers standard deviation 
conclude data algorithm successfully locates clusters correctly assigns feature saliencies 
feature feature dimensional data set iteration feature feature initialization iteration feature feature intermediate state iteration feature feature saliency feature pruned iteration feature local optimum clusters iteration feature final result typical run synthetic data 
crosses represent data points 
solid lines represent mixture components 
dotted lines represent common component 
number parenthesis axis label feature saliency 
saliency feature reaches common component longer applicable feature 
common component line 
feature saliency feature 
common component longer seen dot 
feature number feature saliency feature saliency synthetic data section 
error bar mean value plus minus standard deviation 
note features noisy features 
trunk data second experiment synthetic data consider scenario proposed trunk 
scenario examined previous feature selection study 
dimensional gaussians dimensional data set obtained sampling points gaussians 
note case features arranged descending order relevance 
stopping threshold set initial 
runs performed components detected 
ordering feature saliencies shown gure 
lower rank number important feature 
see general trend feature number increases saliency decreases 
agrees true characteristics data 
real data rst experiment real data done wine data set data set includes points classes continuous features 
data set result chemical analysis wines di erent regions 
features standardized mean zero unit standard deviation 
available uci machine learning repository www ics uci edu mlearn mlrepository html feature number feature rank rank feature saliency trunk data set 
smaller rank salient feature 
table feature saliency wine data rst applied supervised feature selection algorithm software particular algorithm nearest neighbor leave cv backward forward 
algorithm selected features 
feature saliency algorithm gave feature saliencies shown table 
rank features descending order saliency get ordering 
features highest saliency feature subset selected 
skip sixth feature features selected 
see data set algorithm totally unsupervised performs comparably supervised feature selection algorithm 
texture data experiment texture data set supervised learning problem 
collage brodatz textures randomly located gabor lter features obtained 
random sub sample data points data set 
available www ll mit edu ist course algorithm unsupervised class category information 
create new set features data rst normalizing zero mean rotating diagonal covariance matrix 
new features uncorrelated 
name features ascending order variance feature smallest variance feature largest variance 
ran proposed algorithm obtained feature saliency shown 
may expect larger numbered features salient common way assess importance feature consider variance principal component analysis 
case higher numbered features larger feature saliencies 
salient features small variance features 
feature number feature saliency feature saliency texture data evaluating performance unsupervised learning algorithm rarely easy 
consider cluster stability evaluation 
feature subset lead stable clusters 
cluster stability estimated bootstrapping 
speci cally rst generate bootstrap sample perform clustering subset features 
bootstrap sample generated clustering performed obtain partition data 
compare partitions di erent bootstrap samples hubert index 
details hubert appendix 
clusters stable average hubert index large 
set threshold feature saliency obtain feature subset compare feature subset selected variance consideration simply 
generate bootstrap samples calculate pair wise hubert threshold set higher feature subset selected variance criterion 
feature subsets 
rst subset average value standard deviation 
second subset average value standard deviation 
see thresholding feature saliency yield subset gives slightly stable clusters di erence signi cant 
texture data illustrate concept cluster inconsistency described section 
show plots texture data salient features 
see mixture components cover data 
non salient features shown 
see number data points really covered mixture components common component responsible explaining data points inconsistently labeled features 
feature feature iteration salient features iteration feature feature non salient features cluster inconsistency proposed de nition feature saliency context mixture unsupervised learning described em algorithm estimate saliency unlabeled data 
proposed algorithm estimates number mixture components 
feature selection methods explicit search required 
experiments synthetic real data demonstrate potential proposed method 
avenues enhancements 
features weakly relevant sense de ned removed current algorithm 
investigate scalability proposed em algorithm methods described incorporated 
derivation em algorithm recall equation conditional density yj jl propose treat set missing variables de ne set parameters estimated feature saliencies 
assume independent independent hidden component label pattern yj jl jl marginal density jl jl jl equation section 
notice features independent component label jl derive em algorithm estimate parameter kg dg jl dg dg mixture density equation equation section 
consider data yn missing component label denoted 
complete data log likelihood il jl il facilitate derivation de ne quantities ij jjy ijl jy ijl jy calculated current parameter estimate note ijl ijl ij ij expected complete data log likelihood log jy log log il jl log log il log jjy log jy log il jl log log il log ij log ijl log il jl log ijl log il log ij log part ijl log il jl part ijl log il part log ijl log ijl part ranges summation omitted brevity 
parts equation maximized separately 
recall densities univariate gaussians characterized means variances 
result maximizing expected complete data log likelihood leads update equations step ij ij ij ij mean jl ijl il ijl variance jl ijl il mean jl ijl mean ijl il ij ijl variance ijl il mean jl ij ijl ijl ijl ijl ijl update equations involve quantities computation constitutes step 
rst compute ijl il jz il jl ijl il jz il ijl il jz ijl ijl observe jz jz jz jl jl jl jl jl ijl ijl see ij ijl ijl ijl jz jjy ijl ijl ij ijl ij ijl ijl update equations done 
applying minimum message length minimum message length mml criterion see details arg min log log log ji log set parameter model dimension log expected fisher information matrix negative expected value hessian log likelihood ji determinant 
model equation parameter consists jl 
information matrix model hard obtain analytically 
consider information matrix complete data log likelihood 
logarithm equation write log zj log jl log zj jl log jl log jl log zj zero 
second derivatives stated zero 
zj zj independent jl log jl jl log expectation approximated block diag size ds number parameters jl respectively 
note information bernoulli distribution parameter diag information matrix multinomial distribution 
write ji log log log log jl log prior densities parameters assume di erent groups parameters independent 
speci cally di erent ls jl di erent js ls di erent ls independent 
furthermore knowledge parameters adopt non informative je rey priors see details proportional square root determinant corresponding information matrices 
substitute ji equation drop order term obtain nal criterion just equation arg min log yj ds log log log component wise em step equation prunes component weight small 
start algorithm large number components usually large number ective mixture features 
penalty fairly large direct em implementation may simply prune away components rst update 
order circumvent diculty retaining ability initialize large number components follow implement component wise version em 
key aspect parameters mixture components updated turns parallel 
component pruned probability mass immediately distributed remaining components helping survive 
precisely speaking component wise em maintain un normalized version ij numerator equation 
th mixture component rst update jl standard em 
update equation 
denominator equation rd assume ij rd reasonable way update rd early stages training large denominator negative 
put lower bound say denominator 
updating equation value zero th component killed 
simply move component update parameters move component forth components updated 
update parameters common component nish current em iteration 
implementation notes implementation ijl small machine representation assume ijl smaller ijl ijl set zero 
due round errors ijl may slightly larger 
sum rounded ijl zero accordingly 
ijl 
component wise em components die away quickly leading poor local optimum 
heuristic alleviate reduce penalty rst iterations 
gradually increases amount penalty zero rd iterations 
possible enforce strictly give chance relevant features irrelevant number components smaller 
message length simpler model may longer complicated model quantization error 
case correct estimated mml calculated simpler model 
protect case arbitrary small arbitrary close zero 
estimating precision reciprocal variance variance directly save oating point divisions 
hubert index section hubert index estimate similarity clustering results 
calculation hubert index listed 
data yn clustering results number clusters respectively de ne cluster represents number pairs data cluster denotes number pairs data cluster denotes number pairs data cluster means number pairs data di erent clusters ecient way compute de ne ij number data th cluster th cluster total number pairs data cluster denote total number pairs data ij ij ij ij ij ij ij ij ij similarity measures de ned hubert index 
rand index jaccard index hubert index ma note ranges rand index jaccard index 
larger indices similar hubert index range 
regarded similar absolute value index closer 
note hubert index unde ned 
practice partitions want compare section di erent bootstrap samples calculate hubert index data common partitions 
agrawal gehrke gunopulos raghavan :10.1.1.131.5152
automatic subspace clustering high dimensional data data mining applications 
proc 
acm sigmod international conference management data seattle washington june pages 
bradley fayyad reina 
clustering large database em mixture models 
proc 
th international conf 
pattern recognition icpr pages september 
celeux chr forbes 
component wise em algorithm mixtures 
journal computational graphical statistics 
dash liu 
feature selection clustering 
proc 
paci asia conference knowledge discovery data mining pakdd 
dempster laird rubin 
maximum likelihood estimation incomplete data em algorithm 
journal royal statistical society 
devaney ram 
ecient feature selection conceptual clustering 
proc 
th international conference machine learning pages 
dy brodley 
feature subset selection order identi cation unsupervised learning 
proc 
th international conference machine learning pages 
morgan kaufmann san francisco ca 
figueiredo jain 
unsupervised learning nite mixture models 
ieee transactions pattern analysis machine intelligence 
jain zongker 
feature selection evaluation application small sample performance 
ieee transactions pattern analysis machine intelligence february 
jain dubes 
algorithms clustering data 
prentice hall 
jain duin mao 
statistical pattern recognition review 
ieee transactions pattern analysis machine intelligence 
jain murty flynn 
data clustering review 
acm computing surveys september 
kim street menczer 
feature selection unsupervised learning evolutionary search 
proc 
th acm sigkdd international conference knowledge discovery data mining pages 
kohavi john 
wrappers feature subset selection 
arti cial intelligence 
mclachlan basford 
mixture models inference application clustering 
marcel dekker new york 
mclachlan krishnan 
em algorithm extensions 
john wiley sons new york 

concept learning feature selection square error clustering 
machine learning 
mitra murthy 
unsupervised feature selection feature similarity 
ieee transactions pattern analysis machine intelligence 
modha scott 
feature weighting means clustering 
appear machine learning 
moore han boley gini gross hastings karypis kumar mobasher 
web page categorization feature selection association rule principal component clustering 
proc 
th workshop information technologies systems wits december 
narendra fukunaga 
branch bound algorithm feature selection 
ieee transactions computers september 
pena lozano 
dimensionality reduction unsupervised learning conditional gaussian networks 
ieee transactions pattern analysis machine intelligence 
kittler 
feature selection approximation class densities nite mixtures special type 
pattern recognition 
rissanen 
stochastic complexity inquiry 
world scienti singapore 

dependency feature selection clustering symbolic data 
intelligent data analysis 
trunk 
problem dimensionality simple example 
ieee transactions pattern analysis machine intelligence 
vaithyanathan dom 
generalized model selection unsupervised learning high dimensions 
solla leen muller editors proceedings neural information processing systems 
mit press november 
wallace freeman 
estimation inference compact coding 
journal royal statistical society 
xing jordan karp 
feature selection high dimensional genomic microarray data 
proc 
th international conference machine learning pages 
morgan kaufmann san francisco ca 

