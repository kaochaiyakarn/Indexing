learning probabilistic networks paul krause philips research laboratories lane surrey rh ha united kingdom probabilistic network graphical model encodes probabilistic relationships variables interest 
model records qualitative influences variables addition numerical parameters probability distribution 
provides ideal form combining prior knowledge limited solely experience influences variables interest data 
show data revise initial estimates parameters model 
progress showing structure model revised data obtained 
techniques learning incomplete data covered 
order self contained possible start probability theory probabilistic graphical models 
concludes short discussion techniques applied problem learning causal relationships variables domain interest 

motivation cited tutorials learning probabilistic networks extant 
buntine primarily annotated bibliography 
second heckerman available extended form quite technical 
need accessible introductory 
aims provide broad survey field limited space try strike balance technical content accessibility 
primary goal produce provide reader approaches taken main issues arising cited need consulted order obtain full technical details 
personal needed 
brought order clear lineage tutorial outset 
initially started joint effort david heckerman 
drawn david tutorials points 
addition david provided extensive comments early versions 
points put forward perspective subjective probabilities expert judgements true physical probability degree belief 
whilst david encouraged air viewpoint personally subscribes 
addition things turned essentially ended writing entire topics sections david tutorials content wording presentation style different 
reasons agreed sole author 
mention length merely david heckerman helpful comments earlier drafts poor help support simply produced worthy submission help 
discussions subjective probabilities degrees belief example emphasise necessarily step road understanding subject 
look exercise scientific journalism 

aims decades steadily expanding interest rigorous probabilistic inference technique development expert systems 
track research addressed development efficient inference techniques complex large scale knowledge models 
course inference technique presupposes availability valid knowledge model 
second track research targeted development techniques enable combination expert knowledge data develop refine desired knowledge models 
second track rapidly matured half decade 
result feel time ripe provide tutorial overview current state art second track 
aims provide overview form accessible detailed technical knowledge area 
order self contained possible start succinct revision basic probability theory 
primary agenda revision emphasise role probability measure 
shall see probability event measured directly 
times data measurement may available need rely initially probabilities elicited expert experts 
numerical parameters sole components knowledge model 
structure influences variables knowledge model 
structure key importance subject sections 
main question wish answer information derive probabilities structure knowledge model 
expert judgement elicit structure associated probabilities update probabilities light experience 
update structure 
learn structure probabilities directly statistical data 
sections provide positive answers main ancillary questions 
see influences variables knowledge model usefully considered terms causal influence eliciting structure model 
able learn structure question naturally arises learn causality 
final question discussed section 
probability probability measure basic familiarity probability theory assumed purposes 
completeness give definitions 
probability may talk terms probability cancer patient respond certain form chemotherapy probability projectile hit region space probability observing string identical outcomes dice throws 
shall general term sample point refer things talking abstraction cancer patient geometric point chance outcome 
sample space universe set possible sample points situation interest 
usual designate specific sample space 
sample points sample space mutually exclusive collectively exhaustive 
probability measure function subsets sample space 
subsets called events 
refer values probabilities respective events 
function measure properties definition probability measure sample space function mapping subsets interval 


countably infinite collection disjoint subsets general need check sets events satisfy certain properties ensure measurable 
details obtained example 
estimate probabilities want counting ratio number cancer patients cured total number treated example form direct measurement 
sense saying probability attribute property real world 
term physical probability denote interpretation probability 
reliability measurements course act measurement element imprecision associated 
expect probabilities obtained measurement imprecise strictly physical probability represented distribution possible values 
general information tighter distribution 
direct physical measurements estimate probability 
example case asked toss coin seen judge probability land heads 
believes coin fair estimate physical probability reasonable 
complex situations value elicited probability may vary subject subject dependent level relevance expertise subjects 
probability elicited way taken measure expert belief certain situation arise 
lead extensive discussions experts laws probability update beliefs new evidence available 
avoid discussions just take elicitation probabilities act expert judgement 
whichever view takes probability theory offers gold standard probability estimates may revised light experience 
discussed clearly fundamental level bernardo smith chapter 
theme expanded 
clear stage interest sources information available produce model real world 
interested modelling expert 
expert judgement useful starting point ultimate derivation physical probabilities 
bayes theorem far concentrated largely static aspects probability theory 
probability dynamic theory provides mechanism coherently revising probabilities events evidence available 
conditional probability bayes theorem play central role 
completeness include brief discussion 
write represent probability event hypothesis conditional occurrence event evidence 
counting sample points interested fraction events true switching attention universe universe clear comma denoting conjunction events written form referred product rule fact simplest form bayes theorem 
important realise form rule stated definition 
theorem derivable simpler assumptions 
bayes theorem easily rewritten form tells obtain posterior probability hypothesis observation evidence prior probability likelihood observing case simple formula immense practical importance domain diagnosis 
easier elicit probability example observing symptom disease disease symptom 
operationally usually required 
general form bayes theorem stated follows 
proposition suppose partition sample space disjoint sets 
important appreciate bayes theorem applicable meta level domain level 
handle case hypothesis proposition knowledge domain specific disease evidence observation condition symptom 
handle case hypothesis parameter knowledge model certain value distribution values model certain structure evidence incoming case data 
probability distributions sets variables points sample space may concrete 
considering epidemiological study may people case quality control assembly line may specific electronic components 
may possess certain qualities interested may observed measured way 
example electronic logic gate may functional non functional may certain weight distinction may interest purely functional point view 
refer distinction may uncertain variable 
variable set states corresponding mutually exclusive exhaustive set events 
may discrete case finite countable number states may continuous 
example may discrete binary variable represent possible functioning logic gate selected production line continuous variable represent weight 
strictly variable function say sample points domain representing qualities distinctions interest 
element randomness induced selection random sample point specific logic gate specific throw die 
sample point chosen outcome fixed measured determined 
case elements underlying sample space implicitly understood case capital letter represent variable 
custom remainder uppercase letters represent single variables 
lower case letters states example denoting variable state course qualities sample point may easier determine 
example readily determine logic gate functional determining underlying cause non functional gate normally possible invasive inspection device 
experience help 
suppose carefully analysed non functional devices faulty bond encapsulated silicon chip connecting pins malfunctioning chip 
new observation non functioning gate statistics predict chances gate having specific states easily observable qualities 
real world problems typically complex 
move little closer real example lists set variables specific states person interest 
states variables visit asia smoking may easy determine 
goal predict states variables harder observe directly lung cancer bronchitis observ ed 
term random variable 
reserve term random variable situation repeated observations strictly case variables probabilistic networks discussed 
states 
able elicit probability distribution variables interest 
variable states need elicit distinct values order define probability distribution completely 
require massive data collection exercise hope physical probabilities alternatively unreasonable demands domain experts think terms eliciting probabilities 
toy problem relation real applications built 
problem defining joint probability distribution need assign probabilities possible events 
knowledge elicitation problem tractable exploit structure implicit domain knowledge 
sections expand 

graph theory visit asia bronchitis lung cancer smoking tuberculosis positive ray hypothetical set variables associated sample space humans 
problem domains structured graphical representation 
essentially identifies concepts items information relevant problem hand nodes graph explicit influences concepts 
section introduces terminology associated graphs 
graph simply collection vertices edges vertices 
associate graph set variables establishing relationship nodes graph variables example label nodes nodes associated appropriately subscripted variable edge directed node node case edge simultaneously belong say node parent child node belong say edge undirected example graph shown associated set variables introduced 
graphs directed arcs consider 

additional node introduced order simplify graph 
note graph edges directed nodes possible follow sequence directed edges return starting point directed cycles 
graph contains directed edges known directed graph 
graphs contain directed cycles particularly studied context probabilistic expert systems 
referred directed acyclic graphs dags 
graph contains undirected edges known undirected graph 
received attention expert systems literature importance statistical modelling 
graphs mixture directed undirected edges referred mixed graphs 
mentioned opening section important point graphical representation set variables edges indicate relevance influences variables 
absence edge variables hand provides form independence statement state variable inferred state direct relationship independence relationships expressed graphically independence relationships defined terms probability distributions 

independence visit asia tuberculosis positive ray lung cancer forms independence notions independence conditional independence fundamental component smoking bronchitis graph associated variable set 
extra node introduced order simplify graph 
example due lauritzen spiegelhalter 
conditionally independent rain probability theory 
combination qualitative information quantitative information numerical parameters probability theory expressive 
detailed studies conditional independence properties dawid pearl 
completeness include definitions basic notions 
shall notation introduced dawid 
variables 
denotes independent 
corresponding probabilistic expression 
introduce variable denotes conditionally independent expression terms probability distributions 
draw directed acyclic graph directly encodes assertion conditional independence 
shown 
significant feature structure decompose joint probability distribution variables product terms involving variables 
concrete example think variable representing disease measles 
variables represent distinct symptoms red spots spots respectively 
observe disease measles probability symptom determined 
actual confirmation symptom alter probability occurrence 
different scenario illustrated 
marginally independent 
small white spots inside mouth 
conditionally dependent wet lawn sprinkler conditionally dependent best illustrated simple example 
rain sprinkler may cause lawn wet 
observation lawn probability rain probability sprinkler independent 
lawn observed wet confirmation raining may influence probability sprinkler large alternative causes 
example explaining away presence cause making alternative 
see discussion 
provides graphical representation situation 
probability distribution factorised 
time note making marginal independence 
third final example completes cases interest 
disease kawasaki disease known cause pathological process myocardial 
turn associated symptom chest pain 
additional test myocardial diagnosed observation chest pain influence probability kawasaki disease underlying cause expression pathological process 
conditionally independent case probability distribution pattern emerging 
provide generalisation happening 
factorised distributions just seen series examples cases directed acyclic graph admitted simple factorisation joint probability distribution 
examples factor conditional probability involving just parent child nodes 
general property directed acyclic graphs 
proposition conditionally independent associated graph dag 
joint probability distribution admits direct factorisation xi pa xi eqn 

pa denotes value assignment parents slightly larger example help bring importance property 
took variables listed generated dag represents influences variables additional variable introduced simplify graph 
factorisation probability distribution written directly equation net result probability distribution large set variables may represented product conditional probability relationships small clusters semantically related propositions 
needing elicit joint probability distribution set complex events problem broken assessment conditional probabilities parameters graphical representation 
population relationships numerical parameters tractable 
clear elicitation qualitative graphical structure fundamental importance easing problem eliciting probability distribution knowledge model 
separation markov properties important establish precise relationships topology graph independence properties associated probability distribution 
pearl separation provides straightforward technique reading independence relationships graph 
separation illustrated section pointers literature related studies independence properties 
notion markov equivalence introduced 
major importance connection learning structure probabilistic networks 
separation directional stated follows 
definition path contiguous sequence edges ignoring directions 
definition disjoint subsets nodes dag said separate path node node conditions hold node converging arrows descendent node outside best illustrated means example 
illustrates simple dag composition earlier structures 

consider case 
separation little hard apply encountered defined terms negative condition path 
simply means establishing separation holds consider possible paths sets variables 
case 
case node converging arrows second node converging arrows descendent outside path satisfies conditions said blocked separated 
consider 
descendant node node 
potentially opens pathway learn value causes rendered dependent 
case path said active separated 
worked examples separation recommend reader examine additional examples gain understanding separation 
general principal provides mechanism reading dependencies independencies implicit dag 
furthermore result due geiger pearl theorem simple dag composing structures figures 
dag exists probability distribution triple disjoint subsets variables holds separates iff shows rule detecting independence dag improve separation terms completeness 
note equivalence taken direction probability distributions independence properties represented dag 
see examples general discussions modelling probability distributions graphical representations 
mention literature contains number alternative criteria assessing independence 
overview lauritzen contains detailed study independence properties distributions 
show directed acyclic graph probability measure subject certain constraints properties equivalent admits recursive factorisation obeys global directed markov property relative obeys local directed markov property relative directed local markov property easily expressed words variable conditionally independent parents 
case global directed markov property sets variables conditionally independent third separates graph separates denotes paths pass wish dwell criteria 
important point emphasise lauritzen demonstrate equivalence global directed markov property pearl separation 
extend generality results dropping features probability conditional independence 
equivalent criteria thought generalising algebra relevance just probability 
discussion specific context separation 
implications building implementable probabilistic models lessons section summarised quite succinctly 
firstly graphs may represent qualitative influences domain 
secondly conditional independence statements implied graph associated probability distribution 
factorisation exploited ease problem eliciting global probability distribution allow development computationally efficient algorithms updating probabilities receipt evidence 
concerned development computationally efficient algorithms 
interest method construction probabilistic models implied 
stages qualitative stage consider general relationships variables interest terms relevance variable specified circumstances 
quantitative stage numerical specification parameters model 
definition order fully specify model needs graph set parameters factorised probability distribution 
shall refer graphical model 
certainly helps complete solution 
general large number parameters elicit 
assignment values parameters varying amounts information experience varying precision reliability 
emphasised underlying graphical representation element subjectivity specification 
number questions immediately arise 
imprecision values parameters update light experience 
equally update structure light experience 
remove element subjectivity learn structure parameters directly statistical data 
remainder look aids knowledge elicitation problem progress reviewing carried date address questions 
pointers reading decomposable models distributions primary focus exploiting relationships graphical representations distributions knowledge representation elicitation 
relationships exploited develop computationally efficient techniques revising probability distributions light evidence 
basically revision global probability distribution decomposed sequence local computations exploiting relevance properties implied model 
general approaches arc reversal node reduction technique shachter message passing algorithm due pearl clique tree approach lauritzen spiegelhalter 
shachter demonstrated arc reversal node reduction technique pearl algorithm lauritzen spiegelhalter algorithm essentially identical dag 
lauritzen spiegelhalter algorithm developed jensen form basis hugin expert system shell 
jensen extends earlier restricted singly connected trees cover multiply connected trees may path nodes introducing secondary structure called junction tree 
jensen provides accessible account algorithm basis hugin shell 
slightly different vein smyth reviews applicability utility techniques graphical modelling hidden markov models hmms 
particular merit context contains date self contained review basic principles probabilistic independence networks 
short discussions parameter estimation model selection techniques included 
lauritzen provides date text graphical models 

problem knowledge elicitation central topic learning parameters ultimately structure knowledge model data 
number techniques developed enable initial model constructed experience domain experts 
review briefly section 
similarity graph technique heckerman exploits clinical technique differential diagnosis elicit directed graphical structures relating diseases similar 
iterative technique graph built differential diagnosis resulting graphs superimposed provide complete model specified domain 
nilsson suggested number parameter assessments insufficient specify joint distribution uniquely maximum entropy arguments may complete distribution 
number techniques assist elicitation probabilities winkler cited context 
extremely time consuming large network expert parameters elicited 
druzdzel van der gaag offer non invasive technique eliciting network parameters accommodate probabilistic information expert willing comfortable provide 
information may range precise value range values parameter qualitative influences pairs variables specific combination values vari ables may seen making state example 
information applied constraints distribution hyperspace possible joint distributions set variables 
second order probability distributions obtained parameters probabilities assessed provided constraints consistent 
general elicitation second order probability distribution parameters may iterative process 
expert may need confronted inconsistencies constraints order refine elicitation 
interesting approach time writing tried substantial problem druzdzel pers 
comm 
emphasise eliciting probabilities directly experts active area research 
elicited necessarily sure reliability 
learning techniques critique probabilities parameters network revise necessary 

learning probabilities data known structure variables observable straightforward learning situation consider structure believed known data obtainable variables network 
section demonstrate basic techniques context learning probability distribution single variable moving network variables 
consider flipping common drawing pin thumb tack 
pin thrown air allowed land hard flat surface 
event possible outcomes heads drawing pin lands point touching surface tails pin lands back see 
suppose carries long series events measures fraction flips outcome heads 
perspective long run fraction head outcomes probability 
measurements carried observer provide estimate probability 
define variable true value corresponds long run fraction head events greek characters variables refer parameters network 
express uncertainty current state evidence concerning defining probability density function possible values updated flips drawing pin observed 
shows possible probability density function 
suppose flip drawing pin observe heads 
bayes theorem posterior probability heads heads normalisation coefficient 
note know value probability heads flip equal matter outcomes observe 
heads heads 
probability density function represents probability distribution continuous set outcomes 
heads tails possible outcomes flips drawing pin thumb tack 
probability density function parameter value long run fraction heads associated drawing pin 
redrawn 
heads obtain posterior probability density multiplying prior density function 
procedure depicted graphically 
expect resulting probability density function slightly tighter shifted slightly right 
quite straightforward see general observe heads tails heads tails assessed prior density function relevant random 
provided observations independent probability heads constant 
heads revision probability density function observation heads redrawn 
sample events number heads number tails 
said sufficient statistics model provide summarisation data sufficient compute posterior prior 
far considered case outcome variable states 
results generalise quite straightforwardly cover situations outcome variable states 
case denote probabilities outcomes state assumed possible addition know probabilities outcome xi event inde pendent outcomes events 
database outcomes satisfies conditions called random sample dimensional multinomial distribution parameters 
specific case sequence referred binomial sample 
referring back earlier example seen probability density function may updated binomial sample prior density function 
formal constraints choice prior density function 
practice certain advantages known beta distribution state case 
variable said beta distribution hyperparameters probability density function eqn 

density function 
property notice observing heads drawing pin example prior distribution revised heads general heads tails heads tails eqn 

clearly result sampling remains beta distribution 
second property notice expectation respect distribution simple form 
term hyperparameter distinguish parameter 
xn data random sample eqn 

note clarity subscripted distribution expectation 
may take probability observing heads flip drawing pin example 
particular prior distribution ep 
observation heads revised density function right hand side expectation 
note may think denominator equation equivalent sample size 
prior assessed observations 
generalisation beta distribution states known dirichlet distribution hyperparameters 
properties generalise fully 
particular say set dirichlet distributions obtained dirichlet prior updated conjugate family distributions sampling multinomial distribution 
details dirichlet distribution 
simplicity refer dirichlet distribution hyperparameters basic machinery needed learn probabilities network situation network structure assumed known variables observable provided additional assumption 
denote hypothesis joint probability factored set case variables network spiegelhalter lauritzen introduced idea considering conditional probabilities network generated set parameters 
parameters uncertain prior distribution 
random data sample wish update parameterisation components additional simplifying assumption situation parameter independence 
parameters assumed priori independent variables 
case joint distribution variables parame ters written pa eqn 

equation clear node considered just parent extended network 
gives example parameterised network visit asia example 
addition parameter independence assume missing data conjugate priors 
obtain data variable assuming parameters independent update parameter independently just techniques single variable case 
particular parameter set dirichlet prior distribution posteriori distribution remains dirichlet 
consider node states 
specific parent configuration pa variable parameterised visit asia tuberculosis positive ray lung cancer pa pa pa eqn 

assume dirichlet prior distribution pa order predict outcome case expectations parameters conditional probability variable 
particular node state vk pa sh ed pa sh eqn 

expectation taken distribution 
occurrence case observe jth state pa taken configuration pa update parameter pa pa eqn 

note lesson section quite simply summarised 
case learning parameters complete data known structure reduced maintaining sufficient statistics certain distributions dirichlet example 
outlines updating scheme single observation 
details generalisation database observations 
quite straightforward case certain variables network may physically impossible expensive observe certainty 
techniques handle cases considered section 
known structure incomplete data case medical diagnosis may elicit model variables smoking bronchitis parameterisation visit asia example account uncertainty values conditional probabilities variables network 
observed 
routine values variables expensive terms cost risk patient observe regular basis 
analogous situations arise domains need extend previous section cover situation subset variables model unobservable 
simplicity restrict discussion section case availability observation independent actual states variables 
methods addressing dependencies omissions studied example 
come back section 
variable case illustrates general situation quite simply 
variables states respectively 
observed state whilst state unknown 
possible completions database possible variable instantiation unknown 
posterior distribution obtained summing possible states follows case terms beta distrib resulting posterior distribution mixed distribution mixing coefficients 
general see posterior distribution parameters linear combination dirichlet distributions dirichlet mixture mixing coefficients need computed prior distributions 
doing computing joint posterior distribution possible completion database mixing possible completions 
note means posterior parameters longer independent 
addition process continues cases observed faces combinatorial explosion general complexity exact computation exponential number missing variable entries data base 
practice approximation required 
number approximations described literature 
broadly divide classifications deterministic stochastic 
early example referred quasi bayes fractional updating unobserved variable assumed fractional number instances state variable observed 
early criticised falsely increases equivalent sample sizes implicit precision dirichlet distributions 
consequently approach subsequently refined approximating distribution attempts match moments correct mixture 
deterministic approximations process data database sequentially assumption parameter independence properties dirichlet distribution 
contrast various stochastic methods process data handle continuous domain variables dependent parameters 
mentioned comparison reported indicates deterministic methods perform comparison stochastic methods 
widely studied stochastic method gibbs sampling 
special case general markov chain monte carlo methods approximate inference 
gibbs sampling approximate function initial joint distribution provided certain conditions met 
firstly gibbs sampler irreducible 
distribution sample possible state possible initial state condition satisfied example full joint distribution zeros variable instantiation possible 
secondly strictly instantiation chosen infinitely 
practice algorithm deterministically rotating variables meet requirement 
conditions hold average value sampled function approaches expectation respect probability number samples tends infinity 
problem course arises samples take situation estimate error resulting posterior distribution interested 
heuristic strategies exist answering questions easy general solution 
information outline gibbs sampling algorithm whilst contain discussions gibbs sampling including methods initialisation discussion convergence 
alternative approximation algorithm expectation maximization em algorithm 
viewed deterministic version gibbs sampling search maximum posteriori map estimate model parameters 
em algorithm iterates steps expectation step maximisation step 
step complete database unavailable expected sufficient statistics missing entries database computed 
computational effort needed perform computation intense 
bayesian network inference algorithm may evaluation 
second step expected sufficient statistics taken actual sufficient statistics database mean mode parameters calculated 
computation probability observation network structure parameters maximised term maximisation step 
em algorithm fast disadvantage providing distribution parameters 
addition reported substantial amount data missing likelihood function number local maxima leading poor results problem gradient ascent methods 
modifications suggested overcome authors suggest switch alternative algorithms near solution order overcome slow convergence em near local maxima 
algorithms suggested alternative switch gradient descent 
demonstration pure gradient descent reported 
raises interesting point 
firstly introduce notion hidden variable 
case database missing variables particular variable may observed cases may observed 
denotation hidden variable refers situation 
preceding discussion clear learning parameters network hidden variables complex situation variables observable structure 
contrast reported indicates learning parameters network containing hidden variables provided cognitively meaningful statistically efficient learning parameters alternative network parameters observable 
note efficiency refers need relatively small amounts data speed computation 
indicates alternative networks considered 
network structure showed far rapid reduction mean square error output value function number training cases network structure 
lesson learnt soundly structured model easier train having inherently higher diagnostic accuracy 
demonstrates advantages probabilistic networks neural networks exploitation known structural information dramatically reduce amount learning data needed 
basically number parameters needed reduced learning statistically efficient 
systematically missing data methods preceding section assumption datum missing independent state world 
happen assumption invalid certain values variables missing systematically database 
lead bias parameter estimates 
problem particular interest study causal inference statistics outline widely solution problem 
suppose example interested studying effectiveness new treatment population suffering specific disease syndrome 
think terms trial new drug number forms treatment take 
effect treatment individual defined comparison usually difference value outcome individual treated value outcome individual treated 
assess influence treatment randomised trial typically carried 
individuals population randomly assigned treatment comparison remaining individuals undergo treatment 
difficulty individual assigned treatment value outcome individual treated available 
vice versa 
difficulty met introducing notion potential outcomes tries imagine observing outcomes individual circumstances individual exposed 
rubin provided foundational approach 
basic idea build prior model data missing update model realworld data 
rubin generalises complex situations assignment mechanism complex simple randomised trial widely statistics epidemiology see general discussion rubin causal model rcm 
analysis rcm illustration rcm diagrams nodes valued 
valued 
represents probabilistic network hidden variable requires parameters 
corresponding fully observable network 
requires parameters 
estimate effect veteran status vietnam era mortality 
training versus adaptation stage worth making clear distinction production initial model revision parameters extant model 
domains sufficient data available learn parameters network database 
may refer activity training 
usual case field expert systems combination expert knowledge statistical data elicit required parameters 
wish incoming case data revise parameters network parameters may initially imprecisely specified incorrect 
olesen distinguish training preferring term adaptation 
example certain epidemiological data available obtain probabilities parameters medical expert system 
data provide reliabilities probabilities 
expert judgement provide remaining parameters harder sure trust judgements 
learning techniques described enable parameters adapted data passes system model closely represents true state world 
process adaptation brings additional requirements 
initial parameters may unreliable may wish rapidly discount prior distribution parameter incoming case data wide disagreement 
shows example taken spiegelhalter expert assessment significant disagreement observed data 
parameter interest taken system diagnosing lung disease heart disease babies 
expert opinion proportion cases lung disease exhibited mean value 
sixteen cases lung disease admitted fact exhibited 
demonstrates sixteen cases predictive probability expert judgement starting point slowly revised downward whilst prior stabilise appears true value 
spiegelhalter lauritzen spiegelhalter cowell describe significance tests developed provide measure discrepancy expert assessment observed data 
provides formal basis rapid rejection expert prior assessment favour value provide better predictions 
modification expert system shell hugin uses technique fading discount things learnt long time ago system prone adapt 
adaptive hugin equivalent sample size discounted fading factor time new case taken account 
real number typically close 
dirichlet density decreasing equivalent sample size corresponds flattening density 
rapidly reduces influence case data passes system 
reading learning parameters spiegelhalter lauritzen describes initial extending reported lauritzen spiegelhalter allow imprecision probabilities network accommodated revised database cases accumulates 
discusses probability revisions predictive probabilities case lung disease reported 
starting point top curve assessed prior bottom curve prior 
asterisks mark positive observations 
spiegelhalter 
discrete models models dirichlet distributions models logistic regression type 
spiegelhalter continues development lauritzen spiegelhalter algorithm discussion data revise conditional probabilities structure 
details dirichlet priors learning parameters 
data assumed available nodes network 
includes discussion data compare models 
dawid lauritzen introduced notion hyper markov law 
probability distribution set probability measures multivariate space 
philosophy similar role expert seen provider prior distribution expressing uncertainty numerical parameters graphical model 
parameters may revised eventually superseded case data 
major distinct contribution explore details bayesian approach learning parameters context undirected directed graphs 

learning structure data observ ation number previous section focused situation uncertain physical probabilities network certain network structure 
general allow uncertainty structure probabilistic model 
section look techniques learning structure parameters 
general comments look specific techniques developed 
assessed prior prior sm sm sm sm possible network structures describing influences smoking sex lung cancer 
naive approach enumerate possible network structures select maximise suitable criterion 
bayesian criterion example estimate posterior probability network structure observed database 
criteria minimum message length see explored 
unfortunately number nodes network increases rapidly infeasible enumerate exhaustively possible structures 
number nodes small number possible structures approximately number structures number nodes determined function published robinson 
general prior grounds eliminating significant classes possible structures statistically efficient search strategy required 
simple example illustrate important point 
suppose interested studying relationships smoking sex lung cancer 
studying small number cases database expressed relational table table table example database cases 
case sm different networks describe problem directed graph 
shown 
meaning variable names possible structures intuitively plausible consider directed influence smoking sex realistic example 
sm sm ignore semantics nodes stage additional point wish possible networks equivalent sense represent equivalent independence statements 
consider example networks 
probability distributions decompose respectively sm sm sm sm sm sm sm sm apply bayes rule repeatedly decomposition rearrange sm sm sm sm clear networks equivalent functional decompositions express equivalent independence properties 
notion equivalence network structures important kept mind remaining discussion leads important property expected bs hold bayesian network structures 
represent hypothesis physical proba bilities specified joint space encoded network structure hypotheses associated equivalent network structures identical structures consequently having prior posterior probabilities 
property referred hypothesis equivalence 
implication associate hypothesis equivalence class hypotheses single hypothesis 
born mind learning methods strictly pertain learning equivalence classes structures 
example expert knowledge categorically eliminate large number proposed network structures consideration 
arrows lung cancer sex smoking sex certainly go 
reasonably safe remove arrows lung cancer smoking impose judgement technological constraint 
remaining possible network structures represent hypothesis database table random sample equivalence class network structure simply select hypothesis posterior probability data maximum 
represents current background knowledge normalisation constant calculated data table bs bs bs eqn 

full details computation expression 
high level view learning structure probabilistic networks 
practice user believes network structures possible directly assess priors possible network structures parameters compute posterior probabilities described 
trying learn model situation little prior knowledge extra mechanism place control explosion possible structures 

testing characterising equivalent network structures 
bs bs note contrast previous section remainder section covers situation complete data available 
search score approaches learning key computational problem rigorous bayesian approach learning structure involves averaging possible models 
number possible structure hypotheses exponential number variables clearly intractable general 
cases model averaging monte carlo methods statistically efficient yield predictions 
problem simplified positive answers provided questions 
sufficiently accurate approximation provided including small fraction hypotheses sum 
hypotheses 
hard questions address theoretically 
progress learning probabilistic networks significantly advanced workers showed experimentally single network structure offer sufficiently accurate approximation :10.1.1.52.2692:10.1.1.52.2692
approach statisticians decades context types models referred model selection 
alternative selective model averaging manageable number models selected possible models 
complex advantageous identify network structures significantly different representative distribution models 
partly reason partly model selection far widely discussed literature discuss model selection 
reader referred discussion selective model averaging 
question identify hypotheses 
widely general strategy augment scoring criterion search algorithm 
prior knowledge database set network structures taken goodness fit structures prior knowledge data computed criterion 
search algorithm identify structures scored 
earlier discussions obvious choice scoring criterion network structure equivalence class relative posterior probability structure database 
bs example compute relative pos bs probability network structure 
computed assumption dirichlet distributions referred bayesian dirichlet bd criterion 
number alternative criteria proposed 
scoring metric uses relative posterior probability conjunction heuristics principle occam razor proposed madigan 
include information criterion aic bayesian information criterion bic minimum description length mdl minimum message length 
differ minus sign asymptotically approximate bd criterion 
limit number cases database approaches infinity bic mdl give scores bd metric uniform priors structures 
rarely achieved practice due limits sizes datasets available 
bic easy require evaluation prior distributions 
consequently practical criterion appropriate cir bs bs 
potential difficulty posterior probability criteria need assign prior probabilities possible network structure 
statistically efficient terms data demands method doing described heckerman 
requires assessment prior network structure domain user best guess single constant 
user willing provide detailed knowledge assessing different penalties different nodes different parent configurations node 
alternatively categorically assert arcs prior network 
similar practical approaches necessary assignment priors parameters possible network structures discussed :10.1.1.52.2692
search strategies having selected scoring criterion bd assessed relevant priors needs search strategy seek preferred network structure 
case network structure highest posterior probability called maximum posteriori map structure strictly equivalence class structures 
search methods decomposability property scoring criteria 
network structure domain say measure structure separable written product measures function node parents 
metrics complete databases separable 
bd criterion example separable criterion 
write bs xi eqn 

function parents 
separable criterion compare score network structures differ addition deletion arcs pointing computing term structures 
arc example nodes reversed terms need calculated 
general technique search strategies successive arc changes network employ property decomposability evaluate merit change 
cooper herskovits describes greedy search algorithm identifying probable structure test data 
aliferis cooper evaluates accuracy specific instantiation greedy search algorithm simulated data 
simulated data specified gold standard models allows accuracy specific technique learns model structure measured directly 
alternative measure accuracy indirectly assessing predictive accuracy resulting model 
direct measurements aliferis cooper report mean percentage correctly arcs whilst mean ratio superfluous arcs 
empirical studies leading refined search algorithms 
assumptions enable computation criteria general case database cases wish learn structure directed acyclic graph parameters conditional probability distributions network node structure corresponds element domain variables 
order compute bd criterion closed form number hypotheses need database multinomial sample network network parameters associated node independent parameters associated nodes global independence parameters associated node instance parents independent parameters node instances parent nodes node parents distinct networks distribution parameters associated node identical networks parameter modularity case complete distribution parameters associated node dirichlet 
indicated earlier assumptions distributions parameters stay conjugate family distributions sampling 
interestingly assumptions additional assumption fact imply fifth assumption 
demonstrated geiger heckerman 
additional assumption data help discriminate network structures encode sets probability distributions likelihood equivalence 
indicated wide variety learning algorithms extant 
buntine provides framework wide variety data analysis learning algorithms constructed 
graphical models meta level represent task learning object level models 
buntine hopes form basis computational theory bayesian learning 
refinement local model structure global structure probabilistic networks exploited reduce number parameters need elicited learned broad search strategies preceding section assume number parameters locally exponential number parents node 
illustrated simple example think nodes corresponding events alarm armed burglary earthquake load alarm sound respectively 
naive representation conditional simple network structure associated conditional probability table 
alternative representations local cpt structure 
default table requires parameters whilst decision tree requires 
probability table cpt right hand side requires parameters 
inspecting table see scope representation uses fewer parameters 
firstly default condition alarm armed alarm sound state parents 
addition alarm armed burglary load alarm sound probability occurrence earthquake happening 
shows alternative representations local structure fewer parameters original structure default table requiring parameters decision tree 
default table single default probability provided parameters explicitly listed 
case decision tree leaf describes probability node arcs labelled possible states parent node travels tree appropriate branches find probability selected states parent nodes 
default table captures default condition whilst decision table captures overriding impact burglary alarm armed difference parameters required 
important benefit gained modifying learning algorithms recognise local structure appropriate 
fewer parameters needed estimation parameters efficient amount data 
furthermore scoring criteria aim balance complexity preferred network accuracy represents frequencies database learnt 
means example just simpler network preferred say just possible parents reduce number parameters needed naive tabular representation cpt 
search algorithm may show bias true complex structure 
friedman goldszmidt proposed modifications minimum description length mdl criterion learning probabilistic networks :10.1.1.29.6166
allow compact encoding default tables decision trees respectively 
criteria separable see section may conjunction search strategy employing local additions removals reversals edges learn structure similar way discussed 
friedman goldszmidt carried series experiments evaluate effectiveness new criteria 
experiments supported hypotheses structures learnt new criteria reduce error preferring complex structures situations associated cpt described fewer exponential parameters 
addition estimation parameters robust relatively larger samples 
analogous changes bd metric proposed explored 
compact representation local cpt structure important topic 
discussions fundamental knowledge representation issues 

learn causation just correlation data 
final section somewhat controversial described preceding sections 
wish discuss possibility learning causation data 
controversy arises partly term causation open interpretation subject philosophical debate years 
statistics regularly demonstrate causality sense 
example controlled experiments see new drug causes side effects 
micro molecular mechanisms underlying causality may understood level statistical correlations may sufficiently convincing drug withdrawn 
concrete example inference causal relationship tuberculosis certain sources drinking water london turn century 
led improvements far significant impact health population discoveries health care 
causal models implied data extremely valuable advances development techniques deriving welcomed 
regard aforementioned controversy author past somewhat confused word causality connection graphical models 
due may termed specification causal connections fine levels detail fine timescales 
helpful remember causal links graphical model learnt provided expressed level terms large complex objects populations distributions 
cook campbell provide set statements captures key features view causality causal assertions meaningful level ultimate known 
contingent conditions causal laws causal laws fallible probabilistic 
effects laws result multiple causes casually speak cause having learnt necessarily imply cause 
easiest causal laws detected closed systems controlled conditions field research involves open systems 
effects follow causes time may instantaneous level 
causal laws reversed cause effect interchangeable 
paradigmatic assertion causal relationships manipulation cause result manipulation effect 
statement particular significance 
causal information kind needed order predictions face intervention 
conversely case experimental science causal information learned interventional data 
cases social sciences medicine interventional data obtained example randomised trials 
cases impractical 
alternative try learn causal information data 
notion conditional independence provides link learning graphical models data learning causality 
domains direct connection lack cause conditional independence 
connection called causal markov assumption domain causal relationships graph exhibits conditional independence assumptions determined separation applied example measles causes spots measles causes red spots probabilities symptoms independent measles confirmed 
authors reported causal markov assumption appropriate domains 
learning causality essentially just inversion certain conditional independence assertions learned data determine graphs produced assertions 
causal markov assumption graphs include true causal graph 
caution needed 
possible alternative network structures may record conditional independence assertions 
consider example set variables possible network structures structures represent independence assertion conditionally independent sense equivalent structures 
illustrated section 
verma pearl define bayesian network structures set variables independence equivalent represent set independence assertions 
verma pearl provide simple condition assessing network structures independence equivalent 
strictly need stronger notion distribution equivalence network structures distribution equivalent encode set probability distributions purposes discussion important point remember strictly learning equivalence classes structures individual structures 
discussion forms equivalence 
meek defines complete causal explanation dependency model set conditional independence statements directed acyclic graph set conditional independence facts entailed exactly set facts follows complete causal explanation dependency model equivalent complete causal explanation discriminate equivalent causal explanations usefully ask causal relationships common causal explanation set independence facts sample 
meek provides algorithms answering question case causal explanations consistent background knowledge consisting set directed edges required set directed edges forbidden 
import epidemiology economics social sciences soft sciences learning causal networks long history 
seminal topic book spirtes verma pearl 
shall refer general approach independence search techniques learning causality isc discriminate bayesian approaches learning structure previous section 
techniques appear different 
data set approaches infinity asymptotic correspondence 
foolish regard approaches competitors learning causality deep problem warrants viewing different perspectives 
possible concern collected data learn causality fundamental premise scientific methodology form intervention experimentation necessary order discover causality 
example case smoking lung cancer section conclude smoking causal relationship lung cancer intervening smoking reduced incidence lung cancer 
spirtes pearl verma shown certain assumptions passive observation sufficient determine part equivalence classes causal structure system study mentioned earlier section goal aiming 
wermuth lauritzen discuss graphical chain models formulation substantive research hypotheses 
interest social sciences produce statistical models capturing characteristics behaviour abilities attitudes people historical environmental conditions 
describe simple criteria identifying equivalent statistical models graphs 
important point take board discussing learning causality possible differentiate alternative substantive research structures correspond equivalent graphical models 
bayesian learning techniques discussed viewed providing soft version independence search techniques expert judgement initiate search 
required technical material bayesian approach essentially covered preceding sections 
main motivation section air issues describe technology 
reading small scale informative example learning causality real world data 
data studied non bayesian techniques spirtes 
papers rubin causal model referred section relevant significant graphical models 
number problems remain solved 
example techniques best variables domain interest observable 
difficulties arise hidden variables permitted 
way illustration graph implies set conditional independence statements variable graph cause whilst 
independence techniques identify association attributed hidden common cause 
bayesian techniques hand difficulties hidden variables involved 

causal markov assumption assumption called faithfulness 
bayesian learning case follows assumption parameters probability density function 

described area research expert systems rapidly maturing find significant real world application 
application techniques described described madigan raftery lauritzen singh provan friedman goldszmidt :10.1.1.29.6166
addition number research groups developed software systems specifically learning graphical models 
described spirtes developed program called tetrad ii learning cause effect 
reported scheines built systems learn mixed graphical models chain graphs variety criteria model selection :10.1.1.56.5057
widely benchmark learning literature system called bugs created thomas spiegelhalter gilks 
bugs takes learning problem specified bayesian network compiles problem gibbs sampler computer program 
software package developed radford neal available internet www cs toronto edu radford fbm software html 
neal package supports bayesian regression classification models 
web page listing currently available commercial research software learning belief networks bayes stat washington edu html 
goes show learning probabilistic networks data healthy research area great potential application 
hope go way raising awareness possibilities exploiting 

akaike 
new look statistical model identification 
ieee trans 
automatic control 
aliferis cooper evaluation algorithm inductive learning bayesian belief networks simulated data sets 
uncertainty artificial intelligence proceedings th conference 
allen fikes 
sandewall kr principles knowledge representation reasoning proceedings second international conference 
cambridge ma morgan kauffman 
rubin identification causal effects instrumental variables 
journal american statistical association 
:10.1.1.56.5057
model search contingency tables coco 
dodge 
eds computational statistics heidelberg physica verlag 
bernardo 
bayesian analysis simple mixture problems 
bayesian statistics bernardo degroot lindley smith eds oxford university press 
bernardo smith bayesian theory 
chichester john wiley 
besnard hanks 
uncertainty artificial intelligence proceedings eleventh conference san fransisco morgan kaufmann 
buntine 
theory refinement bayesian networks 
proc 
seventh conference uncertainty artificial intelligence san francisco morgan kaufmann 
buntine operations learning graphical models 
journal artificial intelligence 
buntine guide literature learning probabilistic networks data 
ieee transactions knowledge data engineering 
chickering transformational characterisation equivalent bayesian network structures 
besnard hanks eds 
chickering learning equivalence classes bayesian network structures 
horvitz jensen 
eds 
chung elementary probability theory stochastic processes 
new york springer verlag 

stochastic processes 
prentice hall 
cook campbell 
quasi experimentation design analysis issues field settings 
chicago rand college publishing 
cooper herskovits bayesian method induction probabilistic networks data 
technical report smi section medical informatics stanford university 
cooper herskovits bayesian method induction probabilistic networks data 
machine learning 
cowell dawid sebastiani 
comparison sequential learning methods incomplete data 
bayesian statistics pp 
oxford clarendon press 
cox 
causality graphical models 
bull 
int 
stat 
inst proc 
th session 
dawid conditional independence statistical theory 

statist 
soc 

dawid lauritzen hyper markov laws statistical analysis decomposable graphical models 
annals statistics 
degroot optimal statistical decisions 
mcgraw hill new york 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
roy 
statist 
soc 

druzdzel van der gaag elicitation probabilities belief networks combining qualitative quantitative information 
besnard hanks eds 
dubois wellman ambrosio smets uncertainty artificial intelligence proceedings eighth conference san mateo ca morgan kaufmann 
edwards graphical modelling 
new york springer verlag 
friedman goldszmidt 
learning bayesian networks local structure 
uai 
friedman goldszmidt 
building classifiers bayesian networks 
proceedings aaai menlo park ca aaai press 
geiger pearl 
logic causal models 
proc 
th workshop uncertainty ai st paul 

geiger heckerman characterisation dirichlet distribution application learning bayesian networks 
besnard hanks eds 
estimation probabilities 
cambridge ma mit press 
hastings 
monte carlo sampling methods markov chains applications 
biometrika 
heckerman probabilistic similarity networks 
networks 
heckerman mamdani wellman real world applications bayesian networks comm 
acm 
heckerman bayesian networks knowledge discovery 
fayyad piatetsky shapiro smyth uthurusamy 
eds advances knowledge discovery data mining 
cambridge ma mit press 
heckerman 
tutorial learning bayesian networks 
technical report msr tr microsoft redmond usa 
heckerman breese 
new look causal independence 
lopez de mantaras poole eds 
heckerman shachter decision theoretic foundations causal reasoning 
journal artificial intelligence research 
heckerman geiger chickering 
learning bayesian networks combination knowledge statistical data 
machine learning 
henrion shachter kanal lemmer 
eds 
uncertainty artificial intelligence 
amsterdam elsevier science publishers 
north holland 
herskovits computer probabilistic network construction 
phd thesis stanford university 
skj th thiesson 
user guide 
technical report department mathematics computer science aalborg denmark 
horvitz jensen 
eds 

uncertainty artificial intelligence proceedings twelfth conference san fransisco morgan kaufmann 
jensen bayesian networks 
london ucl press jensen lauritzen olesen bayesian updating recursive graphical models local computations 
technical report department mathematics computer science university aalborg denmark 
jensen olesen andersen algebra bayesian belief universes knowledge systems 
networks 
holland statistics causal inference 
journal american statistical association 
kass raftery bayes factors model uncertainty 
technical report dept statistics carnegie mellon university 
kass raftery bayes factors 
journal american statistical association 
lauritzen 
em algorithm graphical association models missing data 
computational statistics data analysis 
lauritzen 
graphical models 
oxford clarendon press 
lauritzen spiegelhalter 
local computations probabilities graphical structures application expert systems discussion 
stat 
soc 
ser 

lauritzen dawid larsen 
independence properties directed markov fields 
networks 
lauritzen thiesson spiegelhalter 
diagnostic systems created model selection methods case study 
cheeseman eds ai statistics iv new york springer verlag lecture notes statistics vol 
lopez de mantaras poole 
eds 
uncertainty artificial intelligence proceedings tenth conference san fransisco morgan kaufmann 
madigan raftery 
model selection accounting model uncertainty graphical models occam window 
am 
statist 
association 
madigan york 
bayesian graphical models discrete data 
international statistical review 
madigan raftery 
bayesian model averaging 
proceedings aaai workshop integrating multiple learned models portland 
meek causal inference causal explanation background knowledge 
besnard hanks eds 

fast improvement em algorithm terms 
roy 
statist 
soc 

neal probabilistic inference markov chain monte carlo methods 
technical report crg tr dept computer science university toronto 
neal bayesian learning neural networks 
new york springer verlag 
nilsson probabilistic logic 
artificial intelligence 
lauritzen jensen system creating adaptive causal probabilistic networks 
dubois 
eds 
pearl constraint propagation approach probabilistic reasoning 
kanal lemmer 
eds uncertainty artificial intelligence amsterdam north holland 
pearl probabilistic reasoning intelligent systems networks plausible inference 
san mateo ca morgan kauffman 
pearl testability causal models latent instrumental variables 
besnard hanks eds 
pearl causal diagrams empirical research 
biometrika 
pearl geiger verma 
logic influence diagrams 
oliver smith 
eds influence diagrams belief nets decision analysis chichester john wiley 
pearl verma theory inferred causation 
allen fikes sandewall 
eds 
richardson 
extensions undirected acyclic directed graphical models 
proc th conference artificial intelligence statistics ft lauderdale 
ripley 
stochastic simulation 
chichester john wiley sons 
rissanen 
stochastic complexity discussion 
roy 
statist 
soc 

robins new approach causal inference mortality studies sustained exposure results 
mathematical modelling 
robinson counting unlabelled acyclic digraphs 
dold eckmann lecture notes mathematics combinatorial mathematics berlin springer verlag 
rubin estimating causal effects treatments randomized studies 
journal educational psychology 
rubin bayesian inference causal effects role 
annals statistics 
rubin practical implications modes statistical inference causal effects critical role assignment mechanism 
biometrics 
russell norvig 
artificial intelligence modern approach 
new jersey prentice hall 
russell binder koller kanazawa local learning probabilistic networks hidden variables 
proceedings fourteenth international joint conference artificial intelligence san fransisco morgan kaufmann 
schwarz 
estimating dimensions model 
annals statistics 
shachter 
evaluating influence diagrams 
operations research 
shachter evidence absorption propagation evidence reversals 
uncertainty artificial intelligence 
henrion 
eds 
singh provan 
efficient learning selective bayesian network classifiers 
technical report ms cs computer information science department university pennsylvania philadelphia pa smith makov quasi bayes sequential procedure mixtures 
statist 
soc 
ser 

smyth heckerman jordan probabilistic independence networks hidden markov probability models 
microsoft research technical report msr tr 
spiegelhalter lauritzen sequential updating conditional probabilities directed graphical structures 
networks 
spiegelhalter cowell learning probabilistic expert systems 
bayesian statistics bernardo berger dawid smith eds oxford university press 
spiegelhalter harris bull franklin empirical evaluation prior beliefs frequencies methodology case study heart disease 
report br mrc biostatistics unit cambridge england 
spiegelhalter dawid lauritzen cowell bayesian analysis expert systems discussion 
statistical science 
spirtes meek learning bayesian networks discrete variables data 
proc 
international conference knowledge discovery data mining montreal qu morgan kaufmann 
spirtes glymour scheines causation prediction search 
new york springer verlag 
srinivas 
generalisation noisy model 
uncertainty artificial intelligence proceedings ninth conference san fransisco morgan kaufmann 
taylor measure integration 
cambridge university press 
thomas spiegelhalter gilks 
bugs program perform bayesian inference gibbs sampling 
bayesian statistics bernardo berger dawid smith eds oxford university press 
titterington updating diagnostic system unconfirmed cases 
applied statistics 
titterington smith makov 
statistical analysis finite mixture distributions 
chichester john wiley 
verma pearl 
equivalence synthesis causal models 
proc 
sixth conference uncertainty artificial intelligence san francisco morgan kaufmann 
wallace korb 
learning linear causal model mml 
proc 
uni com seminar intelligent data management chelsea village london uk 
wermuth lauritzen substantive research hypotheses conditional independence graphs graphical chain models 
stat 
soc 

wilks 
mathematical statistics 
new york john wiley 
winkler assessment prior distributions bayesian analysis 
american statistical association journal 

graphical models applied multivariate statistics 
chichester john wiley 

