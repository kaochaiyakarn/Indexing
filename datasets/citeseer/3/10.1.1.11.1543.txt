graphical models kevin murphy may quotation preface jor provides concise graphical models 
graphical models marriage probability theory graph theory 
provide natural tool dealing problems occur applied mathematics engineering uncertainty complexity particular playing increasingly important role design analysis machine learning algorithms 
fundamental idea graphical model notion modularity complex system built combining simpler parts 
probability theory provides glue parts combined ensuring system consistent providing ways interface models data 
graph theoretic side graphical models provides intuitively appealing interface humans model highly interacting sets variables data structure lends naturally design efficient general purpose algorithms 
classical multivariate systems studied fields statistics systems engineering information theory pattern recognition statistical mechanics special cases general graphical model formalism examples include mixture models factor analysis hidden markov models kalman filters ising models 
graphical model framework provides way view systems instances common underlying formalism 
view advantages particular specialized techniques developed field transferred research communities exploited widely 
graphical model formalism provides natural framework design new systems 
flesh discussing topics representation graphical model compactly represent joint probability distribution 
inference efficiently infer hidden states system partial possibly noisy observations 
learning estimate parameters structure model 
decision theory happens time convert beliefs actions 
applications machinery 
representation cloudy sprinkler rain simple bayesian network adapted rn 
probabilistic graphical models graphs nodes represent random variables lack arcs represent conditional independence assumptions 
provide compact representation joint probability distributions see 
example binary random variables atomic representation joint 
xn needs parameters graphical model may need exponentially fewer depending conditional assumptions 
help inference learning explain 
main kinds graphical models undirected directed 
undirected graphical models known markov networks markov random fields mrfs popular physics vision communities 
log linear models special case undirected graphical models popular statistics 
directed graphical models known bayesian networks bns belief networks generative models causal models popular ai machine learning communities 
note despite name bayesian networks necessarily imply commitment bayesian methods called bayes rule inference see 
possible model directed undirected arcs called chain graph 
directed graphical models directed graphical model bayesian network arc informally interpreted indicating causes directed cycles disallowed 
consider example 
nodes represent binary random variables 
see event grass wet true possible causes water true raining true 
strength relationship shown table called conditional see books detailed discussion bayes nets causal modelling pea sgs gc shi 

xn 
ym factor analysis represented bayesian network 
probability table cpt 
example see true true false second entry second row false true false row sum 
node parents cpt specifies prior probability cloudy case 
simplest statement conditional independence relationships encoded bayesian network stated follows node independent ancestors parents ancestor parent relationship respect fixed topological ordering nodes 
see fact specify joint distribution compactly 
chain rule probability joint probability nodes conditional independence relationships rewrite allowed simplify third term independent parent written term see conditional independence relationships allow represent joint compactly 
savings minimal general binary nodes full joint require parameters factored form require parameters maximum fan node 
show popular statistical models represented directed graphical models 
pca ica example nodes represented discrete random variables 
possible represent continuous random variables 
case specify conditional probability distribution cpd node parents parametric form 
common example gaussian 
consider example represents model 
observed shown shaded hidden 
goal infer invert causal arrow discuss section 
goal estimate parameters discuss section 
just concentrate semantics model 
xn qn 
ym mixtures factor analysis 
independent factor analysis 
prior distribution conditional distribution diagonal matrix model called factor analysis 
notation means value gaussian pdf mean covariance evaluated point vector 
usually assume ir ir model trying explain high dimensional vector linear combination low dimensional features 
common simplification assume isotropic noise scalar 
case components vectors uncorrelated explicitely represent model 
turns maximum likelihood estimate factor loading matrix case principle eigenvectors sample covariance matrix scalings determined eigenvalues sigma 
classical pca obtained limit tb rg 
lift restrictive assumption linearity modelling mixture linear subspaces tb 
model shown 
discrete latent hidden variable indicates subspaces generate mathematically written similarly lift restrictive assumption gaussian noise modelling prior mixture gaussians 
way known independent factor analysis ifa att shown 
technically chain graph undirected arcs components indicate correlated diagonal 
model closely related independent components analysis ica 
hmms kalman filters consider bayesian network represents hidden markov model hmm 
encodes joint distribution qt qt yt qt adopt non standard convention square nodes represent discrete random variables circles represent continuous random variables 
standard convention shaded nodes observed clear nodes usually hidden 


hidden markov model hmm unrolled time slices 
hmm parameters explicitely shown dotted circles 
represents represents transition matrix represents parameters observation model 
sequence length simply unroll model time steps 
general dynamic bayesian network dbn specified just drawing time slices called tbn structure parameters assumed repeat 
markov property states independent past qt qt qt 
parameterize markov chain transition matrix mij qt qt prior distribution 
assumed homogeneous markov chain parameters vary time 
assumption explicit representing parameters nodes see 
think parameters random variables bayesian approach parameter estimation equivalent inference 
think parameters fixed unknown quantities parameter estimation requires separate learning procedure discussed section 
case typically represent parameters graph shared parameters example implemented specifying corresponding cpds tied 
hmm hidden markov model don see states markov chain qt just function yt 
example yt vector define yt qt 
richer model widely speech recognition model output conditioned hidden state mixture gaussians 
shown 
popular variations basic hmm theme illustrated 
input output model cpd softmax function neural network 
software handle inference learning general bayesian networks models trivial implement 
linear dynamical system lds topology bayesian network hidden nodes linear gaussian cpds 
replacing qt xt model xt xt ut xt xt ax bu yt xt ut cx du input output hmm 
factorial hmm 
coupled hmm 
hmm mixture gaussians output 
kalman filter algorithm computing xt online 
rauch tung rts smoother algorithm computing xt offline 
algorithms turn special cases general inference algorithms discuss section 
summary summary relationship various popular statistical models 
undirected graphical models joint distribution markov random field mrf defined xc set maximal cliques graph xc potential function positive arbitrary real valued function clique xc normalization factor xc 
consider example 
case joint distribution xi yi ising model graph grid cliques edges xi nodes binary 
potentials xi xj neighboring nodes penalize differences xi xj xi xj xi xj xi xj xi xj 
gaussian boltzmann machines hier cooperative vector quantization distrib nonlin hier mixture gaussians vq factor analysis pca ica mix nonlinear gaussian belief nets red dim dyn dyn dyn red dim factorial hmm hmm mixture factor analyzers mix dyn linear dynamical systems ssms nonlin nonlinear dynamical systems distrib mix dyn mix mixture red dim reduced dimension dyn dynamics distrib distributed representation hier hierarchical nonlin nonlinear switch switching mixture hmms switching state space models switch mix mixture generative model generative models 
roweis ghahramani 
markov random field low level vision 
xi hidden state world grid position yi corresponding observed state 
fp 
low level vision problems gg fpc xi usually hidden xi node private observation node yi 
potential xi yi yi xi encodes local likelihood conditional gaussian yi image intensity pixel xi underlying discrete scene label 
inference main goal inference estimate values hidden nodes values observed nodes 
observe leaves generative model try infer values hidden causes called diagnosis bottom reasoning observe roots generative model try predict effects called prediction top reasoning 
bayes nets tasks 
example consider water sprinkler network suppose observe fact grass wet denote represents true represents false 
possible causes raining sprinkler 

bayes rule compute posterior probability explanation 
bayes rule states hidden nodes observed evidence 
follow standard practice represent random variables upper case letters sampled values lower case letters 
words formula posterior current example conditional likelihood prior likelihood normalizing constant equal probability likelihood data 
see grass wet raining sprinkler likelihood ratio 
note computation considered scenarios cloudy cloudy 
general computing posterior bayes rule computationally intractable 
way see just consider normalizing constant general involves sum exponential number terms 
continuous random variables sum integral certain notable cases gaussians analytically tractable 
see conditional independence assumptions encoded graph speed exact inference 
variable elimination consider problem computing normalizing constant water sprinkler model 
factored representation joint implied graph may write pr pr pr pr pr pr key idea variable elimination algorithm push sums far possible pr pr pr pr perform innermost sum pr pr pr create new term depend term summed pr pr performing second sum get pr pr pr principle distributing sums products generalized greatly apply commutative semi ring 
forms basis common algorithms viterbi decoding fast fourier transform am 
amount perform bounded size largest term create 
choosing summation elimination ordering minimize np hard greedy algorithms practice 
dynamic programming cloudy spr rain wet grass tt tf ft ff clustered version 
rn 
wish compute marginals time dynamic programming avoid redundant computation involved variable elimination repeatedly 
underlying undirected graph bn acyclic tree local message passing algorithm due pearl pea ps 
generalization known forwards backwards algorithm hmms chains rab 
bn undirected cycles loops water sprinkler example local message passing algorithms run risk double counting may converge 
example information flowing independent came common cause analogous problems plague undirected graphical models loops 
assuming interested exact inference common approach convert graphical model tree clustering nodes see 
algorithms convert possibly directed graph tree somewhat complicated involving various graph theoretic operations see hd details 
creating tree clusters run local message passing algorithm 
message passing scheme pearl algorithm common variant designed undirected models called junction tree algorithm hd 
need approximate inference running time exact algorithms exponential size largest cluster assuming hidden nodes discrete size called induced width graph minimizing np hard 
graphs grids directed graphs contain nodes high fan induced width large necessary approximate inference 
approximate inference necessary graphs low induced width chains trees nodes represent continuous random variables cases corresponding integrals needed implementing bayes rule performed closed form 
instance nearly case bayesian approaches trying estimate distribution continuous valued parameter node see 
summary approximate inference methods provide brief summary popular approximate inference methods 
sampling monte carlo methods 
simplest kind importance sampling draw random samples unconditional distribution hidden variables weight samples likelihood evidence 
efficient approach high dimensions called markov chain monte carlo mcmc includes special cases gibbs sampling metropolis hastings algorithm 
see grs 
variational methods 
simplest example mean field approximation exploits law large numbers approximate large sums random variables means 
particular essentially decouple nodes introduce new parameter called variational parameter node 
iteratively update variational parameters minimize cross entropy kl distance approximate true probability distributions 
updating variational parameters proxy inference 
mean field approximation produces lower bound likelihood 
sophisticated methods possible give tighter lower upper bounds 
see tutorial 
loopy belief propagation 
entails applying pearl algorithm see section original graph loops undirected cycles 
algorithm inspired outstanding empirical success turbo codes shown mmc instance algorithm 
empirical fp shown algorithm works contexts low level vision 
lot theoretical analysing algorithm wf wei fw figuring connections methods statistical mechanics 
learning learning refer structure topology model parameters 
parameter estimation called systems identification 
important distinction variables observed hidden 
gives rise way table classifying learning methods em stands expectation maximization 
observability structure full partial known closed form em unknown local search structural em third distinction goal find single best model set parameters point estimation return posterior distribution models parameters bayesian estimation 
case known structure bayesian parameter estimation inference hidden nodes represent parameters briefly discussed section 
bayesian structure learning discussed section 
going details mention learning bns huge subject touches tip iceberg 
details see review papers bun bun hec 
known structure full observability directed graphical models assume goal learning case find maximum likelihood estimates parameters cpd parameter maximize likelihood training data contains cases assumed independent 
normalized log likelihood training set 
dm sum terms node log pr dm log xi xi dm xi parents xi 
see log likelihood scoring function decomposes structure graph maximize contribution log likelihood node independently 
simple modify handle tied shared parameters 
small number training cases compared number parameters prior regularize problem 
case call estimates maximum map estimates opposed maximum likelihood estimates 
practice harder computationally see 
example consider estimating cpt node water sprinkler example 
set training data just count number times grass wet raining number times grass wet raining sprinkler counts sufficient statistics multinomial distribution find mle cpt follows pml denominator 
learning just amounts counting case multinomial distributions 
particular event seen training set assigned probability 
avoid dirichlet prior just amounts adding pseudo counts empirical counts 
example uniform dirichlet prior map estimate node pmap gaussian nodes mle mean covariance sample mean covariance mle weight matrix squares solution normal equations 
kinds cpds neural networks complex procedures necessary computing ml map parameter estimates 
undirected graphical models parameters undirected graphical model clique potentials 
maximum likelihood estimates computed iterative proportional fitting ipf jp 
known structure partial observability nodes hidden em expectation maximization algorithm find locally optimal mle 
basic idea em knew values nodes learning step easy just techniques section 
step compute expected values nodes inference algorithm treat expected values observed 
example case node replace observed counts events number times expect see event expected number times event occurs training set current guess parameters 
expected counts computed follows dm dm dm indicator function event occurs training case 
expected counts maximize parameters recompute expected counts iterative procedure guaranteed converge local maximum likelihood surface 
note baum welch algorithm training hmms rab identical em algorithm 
possible gradient ascent likelihood surface gradient expression involves expected sufficient statistics em usually faster uses natural gradient simpler step size parameter takes care parameter constraints automatically 
em gradient ascent inference subroutine called learning procedure fast inference algorithms crucial learning 
unknown structure full observability maximum likelihood structure complete graph greatest number parameters achieve highest likelihood 
principled way avoid kind overfitting put prior models 
bayes rule map model maximizes pr pr pr pr case multinomials constraint parameters row cpt sum case gaussians constraint covariance matrix positive semi definite logs find log pr log pr log pr log constant independent prior probability higher simpler models ones sparser sense term effect penalizing complex models 
fact necessary explicitly penalize complex structures structural prior 
marginal likelihood automatically penalizes complex structures parameters give probability mass region space data lies 
words complex model right chance believable 
phenomenon called ockham razor see mac 
assume parameters independent marginal likelihood decomposes product local terms xi xi cases priors parameters conjugate integrals performed closed form marginal likelihood computed efficiently 
typically resort sampling methods 
search algorithms having defined scoring function need way search highest scoring graph 
unfortunately number dag variables super exponential 
closed form formula give idea dags nodes dags nodes 
usual approach local search algorithms greedy hill climbing possibly multiple restarts branch bound techniques search space graphs 
directed graphical models ensure graphs directed cycles undirected graphical models ensure graphs decomposable 
fact scoring function product local terms local search efficient compute relative score models differ arcs neighbors space necessary compute terms common terms cancel ratio 
bayesian methods bayesian approach goal compute posterior 
super exponentially large realistic goal return set graphs sampled distribution 
standard approach mcmc search procedure see mur review 
bn hidden variable 
simplest network capture distribution hidden variable created arc reversal node elimination 
binary nodes assume full cpts network independent parameters second 
unknown structure partial observability come hardest case structure unknown hidden variables missing data 
problem marginal likelihood intractable requiring sum latent variables integrate parameters score decompose product local terms 
approach laplace approximation posterior parameters ch 
algebra hec leads expression log pr log pr log number samples ml estimate parameters computed em dimension model 
fully observable case dimension model number free parameters 
model hidden variables 
called bayesian information criterion bic equivalent minimum description length mdl approach 
term just likelihood second term penalty model complexity 
note bic score independent parameter prior 
bic score decomposes local search algorithms hill climbing expensive need run em step compute 
alternative approach local search steps inside step em called structural em provably converges local maximum bic score fri 
inventing new hidden nodes far structure learning meant finding right connectivity pre existing nodes 
challenging problem inventing hidden nodes demand 
hidden nodes model compact see 
standard approach keep adding hidden nodes time performing structure learning step score drops 
intelligent heuristics 
example dense clique graphs suggest hidden node added 
decision making uncertainty said decision theory probability theory utility theory dw rn 
outlined model joint probability distributions compact way sparse graphs reflect conditional independence relationships 
possible decompose multi attribute utility functions similar way 
global utility sum local utilities ui 
create node ui term parents attributes random variables depends typically utility nodes action control nodes parents utility depends state world action perform 
resulting graph called influence diagram 
algorithms similar inference algorithms discussed section compute optimal sequence action perform expected utility 
see kls application multi person game theory 
sequential decision theory agent decision maker assumed interacting environment modelled dynamical system see 
dynamical system linear gaussian noise utility function negative quadratic loss techniques control theory compute optimal policy mapping percepts actions 
system non linear standard approach locally linearize system 
linear dynamical systems enjoy separation property states optimal behavior obtained doing state estimation infer hidden states expected value hidden states input regular lqg linear quadratic gaussian controller 
general separation property hold 
example consider controlling mobile robot 
optimal action take account robot uncertainty state world location just mode posterior true state world 
strategy called certainty equivalent controller perform information gathering moves 
words robot choose look matter uncertain 
general finding controllers non linear partially observed systems usually unknown parameters extremely challenging 
approach shows promise reinforcement learning 
systems fully observed sb partially observed systems 
policy search methods nj show particular promise 
applications special cases bayes nets independently invented different communities years ago genetics linkage analysis speech recognition hmms tracking kalman data compression density estimation channel coding general framework developed pearl pea various european researchers jlo probabilistic expert systems 
systems medical diagnosis 
example consider missile tracking airplane goal minimize squared distance target 
kind diagnosis technology widely adopted microsoft answer wizard office office assistant paperclip guy office technical support 
fact microsoft offers bayesian network api windows developers 
interesting fielded application vista system developed eric horvitz 
vista system decision theoretic system nasa mission control center houston years 
system uses bayesian networks interpret live telemetry provides advice likelihood alternative failures space shuttle propulsion systems 
considers time criticality recommends actions highest expected utility 
vista system employs decision theoretic methods controlling display information dynamically identify important information highlight 
horvitz gone attempt apply similar technology microsoft products lumiere user interface project hbh 
charles comments manuscript 
am aji mceliece 
generalized distributive law 
ieee trans 
info 
theory march 
att attias 
independent factor analysis 
neural computation 
press 
bun buntine 
operations learning graphical models 
ai research pages 
bun buntine 
guide literature learning probabilistic networks data 
ieee trans 
knowledge data engineering 
cowell dawid lauritzen spiegelhalter 
probabilistic networks expert systems 
springer 
ch chickering heckerman 
efficient approximations marginal likelihood incomplete data bayesian network 
machine learning 
dw dean wellman 
planning control 
morgan kaufmann 
friedman koller 
discovering hidden variables approach 
advances neural info 
proc 
systems 
fp freeman pasztor 
markov networks super resolution 
proc 
th annual conf 
information sciences systems ciss 
fpc freeman pasztor 
learning low level vision 
intl 
computer vision 
fri friedman 
learning bayesian networks presence missing values hidden variables 
proc 
conf 
uncertainty ai 
fw freeman weiss 
fixed points max product algorithm 
ieee trans 
info 
theory 
appear 
gc glymour cooper editors 
computation causation discovery 
mit press 
gg geman geman 
stochastic relaxation gibbs distributions bayesian restoration images 
ieee trans 
pattern analysis machine intelligence 
grs gilks richardson spiegelhalter 
markov chain monte carlo practice 
chapman hall 
hbh horvitz breese heckerman 
lumiere project bayesian user modeling inferring goals needs software users 
proc 
conf 
uncertainty ai 
hd huang darwiche 
inference belief networks procedural guide 
intl 
approx 
reasoning 
hec heckerman 
tutorial learning bayesian networks 
jordan editor learning graphical models 
mit press 
jordan ghahramani jaakkola saul 
variational methods graphical models 
jordan editor learning graphical models 
mit press 
jlo jensen lauritzen olesen 
bayesian updating causal probabilistic networks local computations 
computational statistics quarterly 
jor jordan editor 
learning graphical models 
mit press 
jp 
effective implementation iterative proportional fitting procedure 
computational statistics data analysis 
kaelbling littman cassandra 
planning acting partially observable stochastic domains 
artificial intelligence 
kls kearns littman singh 
graphical models game theory 
proc 
conf 
uncertainty ai 
mac mackay 
probable networks plausible predictions review practical bayesian methods supervised neural networks 
network 
mmc mceliece mackay cheng 
turbo decoding instance pearl belief propagation algorithm 
ieee selected areas comm 
mur murphy 
learning bayes net structure sparse data sets 
technical report comp 
sci 
div uc berkeley 
murphy weiss jordan 
loopy belief propagation approximate inference empirical study 
proc 
conf 
uncertainty ai 
nj ng jordan 
pegasus policy search method large mdps pomdps 
proc 
conf 
uncertainty ai 
pea pearl 
probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann 
pea pearl 
causality models reasoning inference 
cambridge univ press 
ps peot shachter 
fusion propogation multiple observations belief networks 
artificial intelligence 
rab rabiner 
tutorial hidden markov models selected applications speech recognition 
proc 
ieee 
rg roweis ghahramani 
unifying review linear gaussian models 
neural computation 
rn russell norvig 
artificial intelligence modern approach 
prentice hall englewood cliffs nj 
sb sutton barto 
learning 
mit press 
sgs spirtes glymour scheines 
causation prediction search 
mit press 
nd edition 
shi 
cause correlation biology user guide path analysis structural equations causal inference 
cambridge 
tb tipping bishop 
mixtures probabilistic principal component analyzers 
neural computation 
wei weiss 
correctness local probability propagation graphical models loops 
neural computation 
wf weiss freeman 
correctness belief propagation gaussian graphical models arbitrary topology 
nips 
yedidia freeman weiss 
generalized belief propagation 
nips 

