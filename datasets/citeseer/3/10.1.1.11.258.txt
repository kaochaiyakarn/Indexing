learning positive unlabeled examples weighted logistic regression wee sun lee comp nus edu sg department computer science singapore mit alliance national university singapore singapore bing liu cs uic edu department computer science university illinois chicago south morgan st chicago il problem learning positive unlabeled examples arises frequently retrieval applications 
transform problem problem learning noise labeling unlabeled examples negative linear function learn noisy examples 
learn linear function noise perform logistic regression weighting examples handle noise rates greater half 
appropriate regularization cost function logistic regression problem convex allowing problem solved efficiently 
propose performance measure estimated positive unlabeled examples evaluating retrieval performance 
measure proportional product precision recall validation set select regularization parameters logistic regression 
experiments text classification corpus show methods proposed effective 

retrieval applications common situations positive unlabeled examples available negative examples obtained paying additional cost 
example trying learn classifier user preference web pages user bookmarks considered positive examples unlabeled examples sampled web 
direct marketing desirable classifier identify customers customer profiles 
current list customers considered positive examples new databases unlabeled examples purchased low cost compared cost obtaining negative examples 
simple model learning positive unlabeled examples positive exam ples randomly labeled positive probability left unlabeled probability see denis 
assumption labeled unlabeled examples negative error negative example randomly label positive examples negative probability problem formulation value unknown 
blum mitchell blum mitchell observed function noisy observed la bel actual label input linearly related ex pression expected sum observed false positive false negative error frequencies 
target function zero actual error minimizing sum observed false positive false negative frequencies give approximation target function target function function class sample size large 
avoids need know value minimizing expected sum false positive false negative error frequencies shown equivalent minimizing expected weighted error false positives multiplied false negatives multiplied unfortunately minimizing weighted errors np hard linear functions 
minimizing weighted errors learn real valued conditional probability observing positive label input performing logistic regression 
perform regularization forming cost function optimize sum squared weights linear function sum weighted logit losses 
resulting cost function convex optimize simple gradient descent 
practice function class may correct function class learning 
correct target function class known may want simpler approximating function example regularization training sample small order proceedings twentieth international conference machine learning icml washington dc 
obtain better generalization 
show precision recall estimated positive unlabeled examples 
function maximized target function generally want precision recall high retrieval situation 
able estimate function positive unlabeled examples allows performance validation set select appropriate regularization parameter learning 
performed experiments validation set select regularization parameter logistic regression text classification task 
results show method effective 
main contributions 
real valued output thresholded binary outputs linear function weighted examples learning positive unlabeled examples 
allows maximum likelihood gives convex cost function optimization 
second contribution performance measure estimated positive unlabeled examples 
performance measure selecting regularization parameter validation set positive unlabeled examples available 
section discuss related 
section describes detail algorithm learning linear functions 
derive estimate positive unlabeled data section give experimental results section 
related works theoretical study probably approximately correct pac learning positive unlabeled examples done denis 
model positive example left unlabeled constant probability shown function classes learnable statistical queries model kearns learnable positive unlabeled examples 
learning positive example studied theoretically muggleton bayesian framework distribution functions examples assumed known 
sample complexity case positive unlabeled examples sampled liu shown maximizing number examples classified negative constraining function correctly classify positive examples give performance large sample size :10.1.1.11.9010
liu expectation maximization em algorithm naive bayes model approximately maximize number examples classified negative approximately constraining positive examples classified correctly :10.1.1.11.9010
done initializing generative model negative examples sub set unlabeled examples highly negative followed performing em iterations labels positive examples kept positive labels unlabeled examples allowed change 
similar algorithm support vector machines svm proposed yu 
algorithm finds examples confidently labeled negative trains support vector machine positive examples examples confidently labeled negative 
support vector machine tries find maximal margin hyperplane unlabeled examples may classified negative initially labeled negative examples 
forms new negative set train support vector machine positive set 
process iterated order label unlabeled examples negative retaining positive examples correctly labeled 
em iterated svm algorithms guaranteed find functions label large number unlabeled examples negative functions exists function class 
naive bayes algorithm modified learn positive unlabeled examples denis 
done subtracting estimate positive examples negative model 
algorithm requires prior knowledge probability positive examples 
alternative knowledge performance estimates estimated positive unlabeled examples function proposed validation set 
probability positive example left unlabeled constant possible modify perceptron algorithm able learn positive unlabeled examples ideas bylander blum 
far attempts algorithms successful probably due lack regularization criterion experimenting datasets large input dimensions 
possible discard unlabeled data learn positive data 
done class svm scholkopf tries learn support positive distribution :10.1.1.39.912
method appears highly sensitive input representation dimensionality perform feature set 
learning positive unlabeled examples considerable interest learning small number labeled positive negative examples large number unlabeled examples 
works topic include nigam uses naive bayes em algorithm joachims uses transductive svm blum mitchell exploits conditional independence multiple views data training 

learning linear functions linear functions form components input vector bias practically effective commonly machine learning 
model process generating positive unlabeled examples assume positive examples randomly left unlabeled probability negative examples left unlabeled 
labeling unlabeled examples negative positive examples wrongly labeled probability negative examples wrongly labeled 
random variable representing input vector actual label observed noisy label 
function define expected observed sum false positive false neg ative error frequencies expected actual sum false positive false negative error frequencies mitchell blum mitchell showed positive examples constant noise rate negative examples constant noise rate blum label unlabeled examples negative positive examples noise rate negative examples noise rate minimizing minimize observing minimizing equivalent minimizing expected weighted er false negatives weighted false positives weighted minimizing weighted errors np hard 
minimizing false positive false negative error frequencies assume function class real valued function class powerful represent conditional probability label positive input 
show multiply examples labeled positive examples labeled negative conditional probability positive label example positive example greater conditional probability positive label example negative example 
allows threshold real valued conditional probability obtain correct classification 
expected fraction examples labeled positive similarly expected fraction examples labeled negative consider behaviour positive instance probability labeled positive positively labeled example multiplied weight giving expected positive weight similarly expected negative weight normalizing equate weights probabilities see conditional probability positive label transformed greater long weighting see set threshold obtain correct classification 
probability negative example labeled positive zero thresholding give correct classification negative examples 
learn conditional probability compose lin ear function sigmoid function obtain perform maximum estimation weighted positive negative examples weighting interpreted multiple copies examples 
unweighted example obtain logit loss summing weighted examples obtain number positive examples number negative examples set examples 
equivalent minimizing cost prevent overfitting add sum squared values weights regularization term obtain final cost function regularization parameter adjusted prevent overfitting 
cost function convex 
function class powerful represent conditional probability maximum likelihood estimation give accurate approximation sample size large 
case function class powerful useful view logit loss upper bound loss mason 
case trying minimize upper bound sum false positive false negative frequencies sense function class powerful give approximation real valued conditional probability give approximation classifier 
optimize cost simple gradient descent 
gradient loss function simply respect th component weight vector epoch th component negative gradient weighted sum losses epoch th component negative gradient sum squared weights simply add momentum term gradient sum losses see 
mitchell accelerate convergence gradient descent 
update epoch learning rate 
bias implemented extending feature vector additional component setting additional component constant examples 
updates bias treated way updates weights 

estimating performance positive unlabeled examples prevention overfitting crucial learning noise 
done validation set select regularization parameter target function minimizes sum positive negative error frequencies necessarily selecting regularization parameter function class able represent target function accurately 
need learn positive unlabeled examples arise retrieval situations collection positive examples retrieve positive examples source unlabeled examples 
scenarios ratio positive negative examples quite small 
commonly performance measure retrieval situations score precision recall score harmonic mean precision recall 
get high score precision recall high 
unfortunately know estimate score positive unlabeled examples 
propose performance criteria comparing models regularization parameters estimated directly validation set making additional assumptions 
see note multiplying sides find note recall estimated performance hypothesis positive labeled examples validation set estimated validation set giving estimate desired model selection criteria 
performance measure proportional square geometric mean precision recall 
roughly behaviour score sense large large small small 

experiments performed experiments newsgroup dataset lang 
dataset consists documents newsgroups roughly documents group 
preprocessing follows removal headers document 
removal words 
removal words times entire corpus 
document represented vector components vector term frequencies bag words document 
vectors normalized length naive bayes methods raw word counts 
additional component value added im bias linear function 
positive set best crit 
crit 
ii best crit 
crit 
ii best crit 
crit 
ii atheism autos space graphics motorcycles christian ms windows baseball guns pc hockey mideast mac crypt politics electronics religion med average table 
scores weighted logistic regression learn positive unlabeled examples 
criteria uses selecting regularization parameter 
criteria ii uses sum false positive false negative frequencies selecting regularization parameter 
best indicates performance best regularization parameter test set 
performed random splitting data sets training set containing documents validation set containing documents test set containing documents 
experiment newsgroup alternately positive group remaining newsgroups negative group 
linear function trained training data epochs learning rate number examples momentum parameter decay parameters tested sum false positive false negative frequencies estimate vali dation set select best parameter 
linear function retrained combined training validation set selected parameter tested test data 
sets experiments performed 
compares performance linear function trained logit loss weighted examples henceforth refered weighted logistic regression methods selecting regularization parameter sum false pos itive false negative frequencies estimate values 
case different values give different number positive examples 
second set experiments gives result em liu class svm scholkopf feature set comparison purposes :10.1.1.39.912:10.1.1.11.9010
set experiments uses slightly different set number positive examples held constant different number positive examples added negative set form unlabeled set 
simulates situation set positive examples try draw unlabeled examples source unknown probability obtaining positive example 
measure performance test set terms score commonly familiar information retrieval practitioners 

experiment created different datasets 
set additional errors added 
second positive documents training validation sets respectively unlabeled documents labeled negative third data set training validation documents unlabeled documents 
results shown table compare new criteria selecting regularization parameter criteria best regularization pa rameter test set best sum positive positive set svm nb em cls em cls atheism autos space graphics motorcycles christian ms windows baseball guns pc hockey mideast mac crypt politics electronics religion med average table 
scores errors svmlight default parameters naive bayes additive smoothing 
performance em class support vector machine different number positive examples 
negative error frequencies criteria ii different values results show selecting regularization parameter gives slight dation performance compared best regularization parameter test set 
comparison sum false positive false negative frequencies perform selecting regularization parameter performance measured terms score 
increases degradation performance slight positive examples labeled positive 
positive examples unlabeled classifier achieves reasonable performance 

experiment comparison see table tried methods data feature sets 
validation sets methods combine training validation sets previous experiments form new training sets set experiments 
ran em algorithm liu tries find initialization generative model negative examples uses em naive bayes order label unlabeled examples :10.1.1.11.9010
implementation described liu additive smoothing parameter agrawal laplacian smoothing naive bayes model performs better :10.1.1.11.9010
method class support vector machines scholkopf unlabeled examples tries learn support distribution positive examples :10.1.1.39.912
class svm solve text classification problem positive examples available 
class support vector machines software libsvm chang lin experiment 
combined positive sets training validation sets positive set 
default parameters libsvm appear simple method tuning parameters positive examples 
performance linear kernel shown 
results gaussian kernels poorer 
compare performance weighted logistic regression noiseless data ran svm algorithm svm light joachims default parameters naive bayes additive smoothing parameter noiseless dataset feature set 
average score svm average score naive bayes 
average score weighted logistic regression case 
parameters features svm naive bayes tuned validation set give fair comparison merely gives indication weighted table 
scores varied 
positive set best crit nb best crit 
em atheism autos space graphics motorcycles christian ms windows baseball guns pc hockey mideast mac crypt politics electronics religion med average logistic regression perform poorly relative methods noiseless case 
em algorithm performs reasonably weighted logistic regression 
probably mismatch generative model uses single model negative examples data composed news groups negative examples 
class svm performs poorly feature set 
experiments indicate class svm highly sensitive features 
better performance obtained class svm reuters data set 
number features experiments 
try feature selection improving performance class support vector machines 
general think utilizing unlabeled examples noisy information better throwing away 

experiment experiments number positive examples decreases noise level increases 
practice number positive examples constant learner positive examples may different source unlabeled examples may contain different fraction positive examples unlabeled set 
perform set experiments number positive set remains constant number positive unlabeled positive examples held constant number positive unlabeled set varied 
started number positive examples case set experiments 
number positive examples unlabeled set varied create situations case ex periment identical case set experiments 
test set previous experiments algorithm having different test distribution compared training distribution positive examples removed unlabeled set 
result table shows regularization parameter selection criteria performs situation 
comparison previous experiment shows performance improves larger positive set 
interestingly naive bayes performs noiseless case number positive examples small table performance improve number positive examples increased table 

discussion method effective linear function classifier score appropriate performance measure 
adding regularization parameter selected validation set fairly tolerant high dimensional data text 
able perform better methods naive bayes assumption em datasets naive bayes assumption satisfied 
uses information methods class svm may preferable unlabeled data available 
experiments need done text data sets reuters data set non text data direct marketing data confirm effectiveness method 

studied problem linear functions learn positive unlabeled examples 
propose logistic regression weighted examples performance measure estimated positive unlabeled examples select regularization parameter validation set 
experiments show method performs 
authors li performed experiments em algorithm 
bing liu supported part national science foundation nsf iis agrawal bayardo jr srikant 

athena mining interactive management text databases 
international conference extending database technology 
blum frieze kannan vempala 

polynomial time algorithm learning noisy linear threshold functions 
ieee symposium foundations computer science pp 

blum mitchell 

combining labeled unlabeled data training 
colt proceedings workshop computational learning theory 
bylander 

learning linear threshold functions presence classification noise 
colt proceedings workshop computational learning theory 
chang lin 

libsvm library support vector machines 
software available www csie ntu edu tw cjlin libsvm 
denis 

pac learning positive statistical queries 
proc 
th international conference algorithmic learning theory alt pp 

denis gilleron 

text classification positive unlabeled examples 
th international conference information processing management uncertainty knowledge systems 
simon horn 

robust single neurons 
journal computer system sciences 
joachims 

making large scale svm learning practical 
ls report universitat dortmund ls viii report 
joachims 

transductive inference text classification support vector machines 
proceedings icml th international conference machine learning pp 

kearns 

efficient noise tolerant learning statistical queries 
journal acm 
lang 

newsweeder learning filter netnews 
international conference machine learning pp 

liu lee yu li 

partially supervised classification text documents 
international conf 
machine learning pp 



class svms document classification 
journal machine learning research 
mason baxter bartlett frean 

boosting algorithms gradient descent function space 
technical report australian national university 
mitchell 

machine learning 
new york mcgraw hill 
muggleton 

learning positive data 
machine learning appear 
nigam mccallum thrun mitchell 

learning classify text labeled unlabeled documents 
aaai pp 

madison aaai press menlo park 
scholkopf platt shawe taylor smola williamson 

estimating support high dimensional distribution 
technical report 
microsoft research msr tr 
yu han chang 

pebl positive example learning web page classification svm 
proc 
int 
conf 
knowledge discovery databases kdd 
