ieee transactions signal processing vol 
october online learning kernels kivinen alexander smola robert williamson member ieee kernel algorithms support vector machines achieved considerable success various problems batch setting training data available advance 
support vector machines combine called kernel trick large margin idea 
little methods online setting suitable real time applications 
consider online learning reproducing kernel hilbert space 
considering classical stochastic gradient descent feature space straightforward tricks develop simple computationally efficient algorithms wide range problems classification regression novelty detection 
addition allowing exploitation kernel trick online setting examine value large margins classification online setting drifting target 
derive worst case loss bounds show convergence hypothesis regularised risk functional 
experimental results support theory illustrating power new algorithms online novelty detection 
index terms reproducing kernel hilbert spaces stochastic gradient descent large margin classifiers tracking novelty detection condition monitoring classification regression 
kernel methods proven successful batch settings support vector machines gaussian processes regularization networks :10.1.1.11.2062
whilst apply batch algorithms utilising sliding buffer better online algorithm 
extension kernel methods online settings data arrives sequentially proven provide hitherto unsolved challenges 
challenges online kernel algorithms standard online settings linear methods danger overfitting applied estimator hilbert space method high dimensionality weight vectors 
handled regularisation exploitation prior probabilities function space gaussian process view taken 
second functional representation classical kernel estimators complex number observations increases 
representer theorem implies number kernel functions grow linearly manuscript received july revised july 
supported australian research council 
parts th international conference algorithmic learning theory november th annual conference neural information processing systems december 
authors research school information sciences engineering australian national university 
williamson national ict australia 
number observations 
depending loss function happen practice cases :10.1.1.2.6040
complexity estimator prediction increases linearly time restricted situations reduced cost constant cost linear storage requirements 
clearly satisfactory genuine online applications 
third training time batch incremental update algorithms typically increases number observations 
incremental update algorithms attempt overcome problem guarantee bound number operations required iteration :10.1.1.21.1720
projection methods hand ensure limited number updates iteration keep complexity estimator constant 
computationally expensive require matrix multiplication step 
size matrix number kernel functions required step typically hundreds smallest dimension 
solving challenges highly desirable able theoretically prove convergence rates error bounds algorithms developed 
want able relate performance online algorithm seeing observations quality achieved batch setting 
desirable able provide theoretical insight drifting target scenarios comparison batch algorithm little sense 
algorithms deal effectively challenges satisfying desiderata 
related algorithms proposed perform perceptron updates classification step 
algorithms noise free case moving targets assume upper bound complexity estimators 
simple method allows kernel estimators classification regression novelty detection copes large number kernel functions efficiently 
stochastic gradient descent algorithms propose collectively called norma differ tracking algorithms warmuth herbster auer insofar require norm hypothesis bounded 
importantly explicitly deal issues described earlier arise applying kernel representations 
concerning large margin classification obtain performing stochastic gradient descent soft margin loss ieee transactions signal processing vol 
october function algorithm similar gentile alma obtain similar loss bounds obtained alma 
advantages large margin classifier allows track changing distributions efficiently :10.1.1.39.912
context gaussian processes alternative theoretical framework develop kernel algorithms related 
key difference algorithm csat opper repeatedly project low dimensional subspace computationally costly requiring matrix multiplication 
considered tracking arbitrary linear classifiers variant winnow bousquet warmuth studied tracking small set experts posterior distributions :10.1.1.2.6040:10.1.1.41.3139
note whilst originally developed online algorithm sequential minimal optimization smo algorithm closely related especially bias term case effectively perceptron algorithm 
outline section ii develop idea stochastic gradient descent hilbert space 
provides basis algorithms 
subsequently show general form algorithm applied problems classification novelty detection regression section iii 
establish mistake bounds moving targets linear large margin classification algorithms section iv 
proof stochastic gradient algorithm converges minimum regularised risk functional section conclude experimental results discussion sections vi vii 
ii 
stochastic gradient descent hilbert space consider problem function estimation goal learn mapping sequence 
xm ym examples xt yt assume exists loss function penalises deviation estimates observed labels common loss functions include soft margin loss function logistic loss classification novelty detection quadratic loss absolute loss huber robust loss insensitive loss regression :10.1.1.41.3139:10.1.1.39.912
shall discuss section iii 
reason allowing range allows refinement evaluation learning result 
example classification interpret sgn prediction class confidence classification 
call output learning algorithm hypothesis denote set possible hypotheses assume reproducing kernel hilbert space rkhs :10.1.1.11.2062
means exists kernel dot product reproducing property closure span words linear combinations kernel functions 
inner product induces norm usual way interesting special case normal dot product corresponds learning linear functions varied function classes learned different kernels 
risk functionals batch learning typically assumed examples immediately available drawn independently distribution natural measure quality case expected risk 
unknown drawn standard approach minimise empirical risk remp xt yt :10.1.1.11.2062:10.1.1.11.2062
minimising remp may lead overfitting complex functions fit training data generalise unseen data 
way avoid penalise complex functions minimising regularised risk remp measure complexity sensible way :10.1.1.11.2062
constant needs chosen appropriately problem 
parameters example see write remp 
interested online algorithms deal example time define instantaneous approximation instantaneous regularised risk single example 
online setting interested online learning examples available desired learning algorithm produces sequence hypotheses 
fm 
arbitrary initial hypothesis fi hypothesis chosen seeing th example 
ft xt yt loss learning algorithm tries predict yt xt previous examples 
xt yt 
kind learning framework appropriate real time learning problems course analogous usual adaptive signal processing framework 
may online algorithm simply efficient method approximately solving batch problem 
algorithm propose ieee transactions signal processing vol 
october effectively run huge data sets machines limited memory 
suitable measure performance online algorithms online setting cumulative loss ft xt yt 
write notice ft tested example xt yt available training ft guarantee low cumulative loss guarding overfitting 
regularisation useful online setting target learning changes time regularisation prevents hypothesis going far direction hopefully helping recovery change occurs 
furthermore interested large margin algorithms kind complexity control needed definition margin meaningful 
general idea algorithm algorithms study classical stochastic gradient descent perform gradient descent respect instantaneous risk 
general form update rule ft ft xt yt ft fi short hand gradient respect learning rate constant 
order evaluate gradient note evaluation functional xi xt yt xt yt xt zl 
update ft ft tl ft xt yt xt 
clearly needs satisfy algorithm 
allow loss functions piecewise differentiable case stands subgradient 
subgradient unique choose arbitrarily choice difference practice theoretical analyses 
loss functions consider convex argument 
choose zero initial hypothesis 
purposes practical computations write ft kernel expansion cf 
ft ik xi coefficients updated step tl ft xt yt 
sequence xi yi regularisation parameter truncation parameter learning rate piecewise differentiable convex loss function reproducing kernel hilbert space reproducing kernel norma outputs sequence hypotheses 
initialise 
loop ft pt max xi ft xt yt loop fig 

norma constant learning rate exploiting truncation approximation 
step th coefficient may receive non zero value 
coefficients earlier terms decay factor constant constant 
notice cost training step larger prediction cost computed ft xt obtained value derivative ft xt yt 
speedups truncation ways speeding algorithm 
updating old coefficients 
may simply cache power series 
pick suitable terms needed 
particularly useful derivatives loss function assume discrete values say case soft margin type loss functions see section iii 
alternatively store compute ft ik xi xt requires rescaling large machine precision exploits exponent standard floating point number representation 
major problem additional measures kernel expansion time contains terms 
amount computations required predicting grows linearly size expansion undesirable 
regularisation term helps 
iteration coefficients shrunk 
iterations coefficient reduced drop small terms incur little error proposition shows 
proposition truncation error suppose loss function satisfying zl kernel bounded norm denotes max ik xi denote kernel expansion truncated terms 
truncation error satisfies cx cx 
obviously approximation quality increases exponentially number terms retained 
ieee transactions signal processing vol 
october regularisation parameter control storage requirements expansion 
addition naturally allows distributions change time cases desirable forget instances xi yi older average time scale distribution change 
call algorithm norma naive online minimisation algorithm explicitly write parameter norma 
norma summarised 
applications discussed section necessary introduce additional parameters need updated 
refer somewhat loosely family algorithms norma 
iii 
applications general idea norma applied wide range problems 
utilise standard addition constant offset function expansion update bt bt xt yt classification ft bt binary classification :10.1.1.11.2062:10.1.1.11.2062
obvious loss function context yf 
loss incurred sgn correct prediction say mistake charge unit loss 
mistake loss function drawbacks fails take account margin yf considered measure confidence correct prediction non positive margin meaning actual mistake mistake loss discontinuous non convex unsuitable gradient algorithms 
order deal drawbacks main loss function classification soft margin loss max yf margin parameter 
soft margin loss positive fails achieve margin case say margin error 
actual mistake 
indicator ft margin error xt yt xt zero 
ft xt yt update yt xt ft ft xt bt bt 
suppose bound xt xt holds ft ft xt ft xt xt obtain ft furthermore ft xt ft xt 
offset parameter omitted consider particularly sections iv reasonable require 
loss function effectively bounded ft xt yt update terms 

recover kernel perceptron 
kernel perceptron regularisation 
classification trick take care margin recall max yg :10.1.1.2.6040
show specific choice influence estimate sv classification may set obtain update rule :10.1.1.2.6040

novelty detection novelty detection classification labels :10.1.1.39.912
useful condition monitoring tasks network intrusion detection 
absence labels yi means algorithm precisely special case norma earlier derive variant spirit 
setting useful allows specify upper limit frequency alerts 
loss function utilised max usually uses order avoid trivial solutions 
update rule 

consideration update shows average fraction observations considered updates 
necessary store small fraction xis 
regression consider settings squared loss insensitive loss trick huber robust loss function trimmed mean estimators 
convenience estimates extension case straightforward 
squared loss consequently update equation 
yt xt 
means store observation precisely prediction error observation 
ieee transactions signal processing vol 
october insensitive loss loss function max introduces new parameter width insensitivity zone 
making variable optimisation problem max 
update equations stated terms allowed change optimisation process 
setting yt xt updates 
sgn 
means time prediction error exceeds increase insensitive zone 
smaller insensitive zone decreased 
huber robust loss loss function proposed robust maximum likelihood estimation family unknown densities 

setting yt xt updates 
sgn 
comparing leads question adjusted adaptively 
desirable goal may know amount noise data 
setting allowed formation adaptive estimators batch learning insensitive loss goal proven elusive estimators standard batch setting 
online situation extension quite natural see 
merely necessary variable optimisation problem updates 
sgn 
iv 
mistake bounds non stationary targets section theoretically analyse norma classification soft margin loss margin 
process establish relative bounds soft margin loss 
detailed comparative analysis norma gentile alma :10.1.1.39.912
definitions consider performance algorithm fixed sequence observations 
xm ym study sequence hypotheses 
fm produced algorithm key quantities number mistakes xt number margin errors xt 
notice margin errors examples gradient soft margin loss non zero gives size kernel expansion final hypothesis fm 
denote margin error trial xt 
soft margin loss written ft xt yt xt consequently denotes total soft margin loss algorithm 
bounds compare performance norma performance function sequences 
gm comparison class notice different margin comparison sequence refers margin errors actual algorithm respect margin 
yg 
extend notations gt yt comparison sequences obvious manner 
preview understand form bounds consider case stationary target comparison constant sequence 

algorithm perceptron algorithm 
assuming achieves version perceptron convergence theorem gives max xt xt consider general case sequence linearly separable feature space 
ideally wish bounds form min mean mistake rate algorithm converge mistake rate best comparison function 
unfortunately approximately minimising number mistakes training sequence difficult strong bounds simple online algorithms 
settle weaker bounds form min upper bound norm bound appears constant term 
earlier bounds form see 
ieee transactions signal processing vol 
october non stationary case consider comparison classes allowed change slowly 
gm gt gt gt gt gt parameter bounds total distance travelled target 
ideally wish target movement result additional term bounds meaning constant cost unit step target 
unfortunately technical reasons need parameter restricts changes speed target 
meaning parameter clearer state bounds discuss 
choosing parameters issue bounds 
bounds depend choice learning rate margin parameters optimal choices depend quantities ming available algorithm starts 
bounds handle assuming upper bound ming tuning 
substituting ming obtain kind bound discussed estimate replaces ming bound 
practical application probably best served ignore formal tuning results bounds just tune parameters empirical methods preferred 
online algorithms suggested dynamically tune parameters optimal values algorithm runs 
applying techniques analysis remains open problem 
relative loss bounds recall update case consider ft ft xt 
convenient give parameter terms function assume positive 
notice holds 
accordingly define 
start analysing margin errors respect margin 
theorem suppose generated sequence length suppose xt xt fix 
md parameters choose regularisation parameter learning rate parameter 
proof appendix consider obtaining mistake bounds margin error result 
obvious method set turning margin errors directly mistakes 
interestingly turns subtly different choice parameters allows obtain mistake bound non zero margin 
theorem suppose generated sequence length suppose xt xt fix define 
choose regularisation parameter learning rate set margin margin settings exists comparison sequence proof theorem appendix gain intuition theorems consider separable case stationary target 
special case theorem gives familiar bound perceptron convergence theorem 
theorem gives upper bound margin errors 
choices theorem purpose minimising mistake bound case 
notice choice results bound margin errors 
generally choose assume largest margin separation possible see algorithm achieves iterations margin factor optimal 
bound similar alma alma sophisticated automatically tunes parameters 
removing separability assumption leads additional term mistake bound expected 
see effects terms assume target constant speed gt gt constant 
md 
speed constant md 
extreme case gt gt 
md md 
term increases bound case changing target speed 
preview convergence norma study performance norma comes minimising regularised risk functional xt yt stochastic approximation time show mild assumptions loss function ieee transactions signal processing vol 
october average instantaneous risk ft xt yt hypotheses ft norma converges minimum regularised risk ming rate 
requires probabilistic assumptions 
examples high probability expected regularised risk average hypothesis ft similarly converges minimum expected risk 
convergence guaranteed truncated version algorithm keeps kernel expansion sublinear size 
assumptions notation assume bound xt xt xt xt assume loss function convex argument satisfies constant lipschitz condition fix 
hypotheses ft produced ft ft tl xt yt xt ft bound ft cx 
xt yt xt yt cx xt yt cx cx fix sequence define argmin xt yt xt yt cx cx considering limit shows 
basic convergence bounds start simple cumulative risk bound 
achieve convergence decreasing learning rate 
theorem fix 
assume convex satisfies 
example sequence xt yt xt xt holds 
fm hypothesis sequence produced norma learning rate ft xt yt am 
proof appendix analysing progress ft update basic technique shows adjust learning rate complicated setting 
note holds particular ft xt yt constants depend parameters algorithm 
bound depend probabilistic assumptions 
example sequence fixed predictor small regularised risk average regularised risk line algorithm small 
consider implications theorem situation assume examples xt yt fixed distribution bound cumulative risk transformed probabilistic bound standard methods 
assume probability say risk bounded probability xt yt 
fm 
example consider soft margin loss 
preceding remarks assume 
implies xt interesting values satisfy 
xt yt take 
wish offset parameter bound needs obtained incorporated similarly regression type loss functions may need bound yt 
result cesa bianchi bounded convex loss functions theorem directly gives 
corollary assume probability distribution holds probability example sequence xt yt drawn fix 
assume convex satisfies risk bounded fm ft ft th hypothesis produced norma learning rate theorem fm ln probability random draws apply corollary choose argmin 
ieee transactions signal processing vol 
october high probability close high probability fm close minimum expected risk 
effects truncation consider version time hypothesis consist kernel expansion size st allow st slowly sublinearly increase function st ft tk xt coefficient xt kernel expansion time simplicity assume st st st include expansion terms 
update add new term kernel expansion st st drop oldest previously remaining term 
write ft ft xt yt ft st st st tk xt st 
see kernel expansion coefficients decay geometrically 
need decreasing learning rate factor approaches 
somewhat complicated choose expansion sizes st large guarantee cumulative effect terms remains control 
theorem assume convex satisfies 
example sequence xt yt xt xt holds fix 
value holds define st st 

fm hypothesis sequence produced truncated norma learning rate expansion sizes st ft xt yt am 
proof definition appendix conversion result probabilistic setting done previously additional step needed estimate terms may affect maximum norm ft omit details 
vi 
experiments mistake bounds section iv course worstcase upper bounds constants may tight 
performed experiments evaluate performance stochastic gradient descent algorithms practice 
classification bounds suggest form regularisation useful target moving forcing positive margin may give additional benefit 
hypothesis tested artificial data mixture dimensional gaussians positive examples negative ones 
removed examples misclassified bayes optimal classifier actual distribution known close decision boundary 
gave data cleanly separable gaussian kernel 
order test ability norma deal changing underlying distributions carried random changes parameters gaussians 
movement schedules drifting case relatively small parameter change trials 
switching case large parameter change trials 
form bounds things equal mistake bound better drifting switching case 
case ran algorithm trials cumulatively summed mistakes 
experiments compared norma alma basic perceptron algorithm stochastic gradient descent margin loss function weight decay parameter set zero 
considered variants norma alma margin fixed zero 
algorithms included see regularisation weight decay norma norm bound alma helps predicting moving target aiming large margin 
gaussian kernels handle non linearity data 
experiments parameters algorithms tuned hand optimally example distribution 
shows cumulative mistake counts algorithms 
decisive differences algorithms 
particular norma works quite switching data bound suggests probably due slack bound 
general positive margin better fixing margin zero regularisation zero margin better basic perceptron algorithm 
novelty detection experiments studied performance novelty detection variant norma various kernel parameters values 
performed experiments usps database handwritten digits scanned images handwritten digits resolution pixels chosen training testing purposes 
pass database took matlab mhz celeron results badly written digits cf 
left plot ieee transactions signal processing vol 
october mistakes mistakes perceptron alma norma alma norma trials perceptron alma norma alma norma trials fig 

mistakes algorithms drifting data top switching data bottom 

chose allow fixed fraction detected outliers theoretical analysis section decreasing learning rate shows algorithm improves assessment unusual observations digits left table quite regular degrade rapidly 
online data filter 
vii 
discussion shown careful application classical stochastic gradient descent lead novel practical algorithms online learning kernels 
regularisation essential capacity control rich hypothesis spaces generated kernels allows truncation basis expansion computationally efficient hypotheses 
explicitly developed parameterisations algorithm classification novelty detection regression 
algorithm aware online novelty detection 
furthermore general form efficient computationally allows easy application kernel methods enormous data sets course real time online problems 
theoretical analysis algorithm applied classification problems soft margin goal understanding advantage securing large margin tracking drifting problem 
positive side obtained theoretical bounds give guidance effects margin case 
negative side bounds corroborated experiments performed 
acknowledgments supported australian research council 
paul help implementation ingo ralf herbrich comments suggestions 
appendix proofs theorems technical lemma proved simple differentiation proofs choosing optimal parameters 
lemma define 
maximised maximum value main idea proofs lower bound progress update define gt ft gt ft notational convenience introduce gm gm 
proof theorem define ft xt 
split progress parts gt ft gt ft gt ft gt gt gt ft gt ft gt ft 
substituting definition applying tl gt xt yt gt xt yt estimate part gt ft gt xt gt ft ft gt xt ft xt tk xt xt gt xt yt ft xt yt tx 
second part gt gt ft ft ft ft gt ft ft ft ft ieee transactions signal processing vol 
october fig 

results online novelty detection pass usps database 
learning problem discover online novel patterns 
gaussian rbf kernels width 
learning rate left patterns incurred margin error seen algorithm finds formed digits novel finds unusually written ones middle worst patterns training set badly written digits right worst patterns unseen test set 
ft ft gt ft ft gt recalling definition get gt gt ft ft ft gt third part gt ft gt ft gt gt gt gt ft 
substituting gives gt ft gt ft gt xt yt ft xt yt tx gt gt ft gt gt gt bound ft write gt gt 
ft gt gt gt gt gt gt gt gt gt give gt ft gt ft ft xt yt gt xt yt tx gt gt gt gt gt gt gt gt 
summing 
assumption obtain gm fm gm bd 
appears subexpression function maximised mb choose gives md 
assume gm fm moving terms estimating gm get md 
get bound margin errors notice value theorem satisfies 
trivial estimate gives md bound follows applying lemma 
ieee transactions signal processing vol 
october proof theorem claim follows directly theorem 
non zero take starting point 
choose term vanishes get md 
implies md 
claim follows lemma 
proof theorem loss generality assume particular notice ft ft ft ft ft ft ft xt yt ft xt yt ft ft xt yt ft xt yt lipschitz property convexity argument 
leads ft ft ft ft ft tc xt yt ft xt yt ft 
summing 
noticing terms telescope get fm xt yt ft xt yt claim follows rearranging terms estimating fm proof theorem define smallest possible hold exp 
estimate st clearly consider case 
st 
rc 
st 
rc st rc exp rc exp rc cx 
particular ft ft ft xt xt ft 
get ft cx 
loss generality assume particular ft cx 
estimate progress trial ft ft new hypothesis truncation 
write ft ft ft ft estimate write ft ft ft ft 
ft ft ft ft ft ft combining estimate get ft ft xt yt ft xt yt notice similarity 
rest follows proof theorem 
sch lkopf smola learning kernels :10.1.1.11.2062
cambridge ma mit press 
support vector machine techniques nonlinear equalization ieee transactions signal processing vol 
pp 
november 
kimeldorf wahba results spline functions math 
anal 
applic vol 
pp 

sch lkopf smola williamson bartlett new support vector algorithms neural computation vol :10.1.1.2.6040
pp 

ieee transactions signal processing vol 
october herbster learning additive models online fast evaluating kernels proceedings fourteenth annual conference computational learning theory colt ser 
lecture notes computer science helmbold williamson eds vol 

springer pp 

vishwanathan smola fast kernels strings trees proceedings neural information processing systems press 
poggio incremental decremental support vector machine learning advances neural information processing systems leen dietterich tresp eds :10.1.1.21.1720
mit press pp 

csat opper sparse representation gaussian process models advances neural information processing systems leen dietterich tresp eds 
mit press pp 

gentile new approximate maximal margin classification algorithm journal machine learning research vol 
pp 
dec 
graepel herbrich williamson margin sparsity advances neural information processing systems leen dietterich tresp eds 
cambridge ma mit press pp 

li long relaxed online maximum margin algorithm machine learning vol 
pp 
jan 
herbster warmuth tracking best linear predictor journal machine learning research vol 
pp 

auer warmuth tracking best disjunction journal machine learning vol 
pp 

kivinen smola williamson large margin classification moving targets proceedings th international conference algorithmic learning theory cesa bianchi reischuk eds :10.1.1.39.912
berlin springer lnai nov pp 

tracking linear threshold concepts winnow proceedings th annual conference computational learning theory kivinen sloan eds :10.1.1.2.6040
berlin springer lnai july pp 

littlestone learning quickly irrelevant attributes abound new linear threshold algorithm machine learning vol :10.1.1.41.3139
pp 

bousquet warmuth tracking small set experts mixing past posteriors journal machine learning research vol 
pp 
nov 
platt fast training support vector machines sequential minimal optimization advances kernel methods support vector learning sch lkopf burges smola eds 
cambridge ma mit press pp 

vogt smo algorithms support vector machines bias term technische universit darmstadt institute automatic control laboratory control systems process automation tech 
rep july 
bennett mangasarian robust linear programming discrimination linearly inseparable sets optimization methods software vol 
pp 

sch lkopf platt shawe taylor smola williamson estimating support high dimensional distribution neural computation vol 

huber robust statistics review annals statistics vol 

vapnik smola support vector method function approximation regression estimation signal processing advances neural information processing systems mozer jordan petsche eds 
cambridge ma mit press pp 

haykin adaptive filter theory 
englewood cliffs nj prentice hall second edition 
sch lkopf herbrich smola generalized representer theorem proceedings annual conference computational learning theory pp 

kivinen smola williamson online learning kernels advances neural information processing systems dietterich becker ghahramani eds 
cambridge ma mit press pp 

herbrich learning kernel classifiers theory algorithms 
mit press 
friedman hastie tibshirani additive logistic regression statistical view boosting stanford university dept statistics tech 
rep 
convergence proofs perceptrons proceedings symposium mathematical theory automata vol 

polytechnic institute brooklyn pp 

gentile littlestone robustness norm algorithms proc 
th annu 
conf 
comput 
learning theory 
acm press new york ny pp 

freund schapire large margin classification perceptron algorithm machine learning vol 
pp 

auer cesa bianchi gentile adaptive self confident line learning algorithms journal computer system sciences vol 
pp 
feb 
cesa bianchi long warmuth worst case quadratic loss bounds line prediction linear functions gradient descent ieee transactions neural networks vol 
pp 
may 
warmuth jagota continuous discrete time nonlinear gradient descent relative loss bounds convergence electronic proceedings fifth international symposium artificial intelligence mathematics ed 
electronic rutgers edu 
cesa bianchi gentile generalization ability line learning algorithms advances neural information processing systems dietterich becker ghahramani eds 
cambridge ma mit press pp 

kivinen received msc degree phd computer science university helsinki finland 
held various teaching research appointments university helsinki visited university california santa cruz australian national university postdoctoral fellow 
professor university helsinki 
scientific interests include machine learning algorithms theory 
alexander smola received masters degree physics technical university munich phd technical university berlin machine learning 
australian national university fellow research school information sciences engineering 
coauthor learning kernels mit press 
scientific interests machine learning vision bioinformatics 
robert williamson received phd electrical engineering university queensland 
australian national university professor research school information sciences engineering 
director canberra node national ict australia president association computational learning theory member editorial boards jmlr 
scientific interests include signal processing machine learning 
