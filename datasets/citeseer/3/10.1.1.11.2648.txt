bayes meets bellman gaussian process approach temporal di erence learning engel alice nc huji ac il interdisciplinary center neural computation hebrew university jerusalem israel mit edu laboratory information decision systems massachusetts institute technology cambridge ma ron meir ee technion ac il department electrical engineering technion institute technology haifa israel novel bayesian approach problem value function estimation continuous state spaces 
de ne probabilistic generative model value function imposing gaussian prior value functions assuming gaussian noise model 
due gaussian nature random processes involved posterior distribution value function gaussian described entirely mean covariance 
derive exact expressions posterior process moments utilizing ecient sequential cation method describe line algorithm learning 
demonstrate operation algorithm dimensional continuous spatial navigation domain 

gaussian processes gps extensively years supervised learning tasks classi cation regression gibbs mackay williams 
probabilistic generative model gp methods theoretically attractive allow bayesian treatment problems yielding full posterior distribution point estimate non bayesian methods 
gaussian structure yields closed form expressions posterior moments completely avoiding diculties associated optimization algorithms convergence behavior 
gaussian process nite countably uncountably nite set jointly gaussian random variables 
special case main focus case random variables indexed vector case instantiation process simply function random variable distribution jointly gaussian components process prior distribution process fully speci ed mean take zero covariance def 
denotes expectation 
order 
legitimate covariance function required symmetric positive de nite 
interestingly exactly requirements mercer kernels extensively kernel machines sch olkopf smola :10.1.1.11.2062:10.1.1.11.2062:10.1.1.11.2062
reinforcement learning rl eld machine learning concerned problems formulated markov decision processes mdps bertsekas tsitsiklis sutton barto 
mdp tuple fs pg state action spaces respectively immediate reward may random process conditional transition distribution 
stationary policy mapping states action selection probabilities 
objective rl problems usually learn optimal suboptimal policy simulated real experience exact knowledge mdp transition model fp rg 
optimality de ned respect value function 
value state de ned xed policy total expected possibly discounted payo col sequel capital letters denote random variables processes respectively lower case letters instantiations 
bold lower case letters denote vectors indexed lower case components denotes vector matrix zeros dimensions clear context 
general case simplify exposition ignore dependence action 
proceedings twentieth international conference machine learning icml washington dc 
lected trajectories starting state value function policy de ned jx expectation taken policy dependent state transition distribution jx ju 
discount factor expected reward 
value function shown solution bellman equation satis ed jx dx state space nite integral eq 
replaced sum generally speaking rl concerned nding policy delivering highest possible total payo state 
algorithms solving problem policy iteration method value function estimated sequence xed policies making value estimation crucial algorithmic component 
undoubtedly best known method value estimation family algorithms known td sutton utilize temporal di erences line updates estimate value function 
simplest member td family td 
td update transition simply step temporal di erence time reward sampled transition time dependent learning rate 
kernel methods largely ignored eld rl 
due reasons 
foremost kernel algorithms generally scale number samples terms space time 
second algorithms require random repeated access training samples 
reasons kernel algorithms gps included considered ill tting rl domain value strictly denoted deals xed policy exception section retain simpler notation policy proper see bertsekas tsitsiklis 
degenerate special case bellman equation results problem statement evaluating value function xed policy 
normally requires line operation 
far aware exception dietterich wang value function estimated line support vector machine 
rest organized follows 
section generative model value function derive exact posterior value distribution 
section describe line cation method section uni es results preceding sections description line sparse algorithm gp temporal di erence learning 
section experiments continuous dimensional mazes conclude section 
gaussian processes td learning analogy gp regression impose gaussian prior value functions 
means gp priori form function re ect prior knowledge concerning similarity states domain hand 
recalling de nition temporal di erences propose generative model sequence rewards corresponding trajectory white gaussian noise process depend may input dependent denotes dirac delta function 
simplify matters assume constant denote simply eq 
may viewed latent variable model value process plays role latent hidden variable reward process plays role observable output variable 
nite trajectory length de ne nite dimensional random processes vector matrices respectively diag diag 
denotes diagonal matrix diagonal argument vector 
de nitions may write de ning matrix 
may write concisely standard results jointly gaussian random variables obtain posterior distribution value point conditioned observed sequence rewards jr xx xx expressions written somewhat familiar way separating input dependent term learned terms xx concludes derivation value gp posterior points worth noting 
note addition value estimate provided uncertainty measure estimate 
opens way wide range possibilities touch section 
second assumptions noise process questionable 
consider validity assumptions 
value process satis es bellman equation assume model 
equating expressions equations get jx dx jx dx state transitions deterministic jx rst integral vanishes left exact state transitions move away equality turns approximation 
section experimentally demonstrate ect non deterministic transitions value estimate 
note computing exact gp estimators computationally unfeasible smallest simplest domains need compute store incurring cost time space dependence arises inverting matrix 
section describe ecient line cation method reduces computational burden level allows compute approximation gp posterior line 

line cation order render algorithm described practical need reduce computational burden associated computation functions 
engel 
line cation method kernel algorithms proposed context support vector regression 
method observation due mercer theorem kernel function 
may viewed inner product high possibly nite dimensional hilbert space see sch olkopf smola details :10.1.1.11.2062:10.1.1.11.2062:10.1.1.11.2062:10.1.1.11.2062
means exists generally non linear mapping 
dimension may exceedingly high ective dimensionality manifold spanned set vectors may signi cantly lower 
consequently expression describable linear combination vectors may expressed arbitrary accuracy smaller set linearly independent feature vectors approximately span manifold similar ideas cation algorithms previously proposed burges advantage insight maintain dictionary samples suce approximate training sample feature space prede ned accuracy threshold :10.1.1.54.9934
method starts empty dictionary fg 
observes sequence states state time admits dictionary feature space image approximated suciently combining images states xm set linearly independent feature vectors corresponding dictionary states time seek squares approximation terms set 
easily veri ed approximation solution quadratic problem min tt kernel matrix dictionary states time tt 
solution substituting back obtain minimal squared error incurred approximation tt tt accuracy threshold parameter add dictionary set exactly represented 
dictionary remains unchanged 
way assured feature vectors corresponding states seen time approximated dictionary time maximum squared error res res order able compute time step need update dictionary appended new state 
done partitioned matrix inversion formula tt 
note equals vector computed solving need recompute 
smola bartlett williams seeger csat opper mention 
inapplicable line setting 
de ning matrices res res res may write eq 
time steps concisely res pre multiplying transpose obtain decomposition full kernel matrix matrices res matrix rank approximation shown norm residual matrix res bounded factor linear 
consequently approximations symbol implies di erence sides equation 
note computational cost time step cation algorithm assuming depend asymptotically independent time allowing algorithm operate line 
line version algorithm draws computational leverage low rank approximation provided cation algorithm 
section show 

line ready combine ideas developed sections 
substituting approximations exact gp solution obtain xx de ne note parameters required store update order evaluate posterior mean due sequential nature algorithm 
bound mild assumptions kernel space space allow pursue interesting issues concerning cation method 
covariance dimensions respectively 
derive recursive update formulas time step may faced cases 
fx case 
de ning de ning 
def get 


obtain partitioned matrix inversion formula 


compute 
similarly get case 
fx 
furthermore exactly representable going algebra get 


tt 
tt def 
tt denoting 
tt 

partitioned matrix inversion formula get 
de ning compute 
point time say compute prediction value variance arbitrary state analog xx note 
term appearing update rules sign temporal di erence de ned 
point worth noting evaluating costs time respectively 
table outlines algorithm pseudo code 
far described line algorithm nite horizon discounted problems 
straightforward modify algorithm handle episodic tasks 
kind tasks learning agent experience composed series learning episodes trials 
trial agent normally placed random starting position follows trajectory reaches absorbing terminal states having zero outgoing transition probability value xed zero 
algorithmically required temporarily set zero goal state reached zero reward transition starting point trial 
extension semi markov decision processes transitions may require di erent amounts time possible 

experiments section experiments meant demonstrate strengths weaknesses table 
line algorithm parameters initialize fx observe transition cation test add dictionary compute compute dictionary unchanged compute output algorithm 
start simple maze order demonstrate ability provide uncertainty measure value estimate data eciency ability extract reasonable value maps little data 
move value estimation challenging task learning optimal policy approximation thereof 
dicult maze experiment deterministic stochastic state transitions 
experimental test bed continuous dimensional square world unit length sides 
agent roaming world may located point perform nite number actions 
actions long steps major compass winds added gaussian noise standard deviation 
time discrete 
world may rectangular goal regions possibly obstacles piecewise linear curves agent cross 
long agent goal region receives reward time step 
reaching goal state agent zero reward placed random state new trial 
simple experiment 
maze shown fig 
single goal region stretching entire length south wall maze 
chose non homogeneous polynomial kernel hx corresponds features monomials degree coordinates sch olkopf smola subtracted coordinate avoid asymmetric bias :10.1.1.11.2062:10.1.1.11.2062:10.1.1.11.2062
exploration policy stochastic move taken probability random move performed 
fig 
show results single trial states visited including nal goal state 
example illustrates eciency algorithm kernel function chosen judiciously 
seen bottom fig 
single policy iteration sweep choosing greedy action respect value function estimate extracts near optimal policy large section maze surrounding states visited 
looking variance map seen proximity visited states uncertainty value prediction signi cantly lower regions 
mentioned value function estimation algorithm usually component larger rl system aim learn optimal policy maximizing total payo trial case minimizing time takes agent reach goal region 
rl algorithm worked surprisingly certain domains tesauro possess theoretical guarantees convergence optimistic policy iteration opi bertsekas tsitsiklis 
opi agent follow xed stationary policy time step utilizes model environment current value estimate guess expected payo actions available 
greedily chooses highest ranking action 
ran agent utilizing opi maze shown fig 
trials deterministic transitions second time noisy transitions described 
kernel gaussian exp kx 
feature space induced choice sch olkopf smola :10.1.1.11.2062:10.1.1.11.2062:10.1.1.11.2062
value maps learned corresponding greedy policies shown fig 

note policies learned similar quite close optimal value estimates di erent 
speci cally value estimates stochastic case 
variance maps omitted entire maze variance estimate close zero 

discussion novel temporal di erence learning algorithm gaussian processes ecient cation scheme 
essential ingredients able suggest practical ecient method value function approximation large nite state spaces 
approaches rl continuous space considered date parametric function approximation architecture usually employed value value variance policy 
results single step trial simple maze shown gures sampled grid 
top bottom top points visited trial contour lines value function estimate 
center variance value estimates 
bottom greedy policy respect value estimate 
value policy value policy 
results trials dicult maze shown gures 
opi nd near optimal policy 
upper gures show results deterministic state transition lower stochastic ones 
pair nal value function shown top corresponding greedy policy bottom 
results shown samples grid 
conjunction gradient learning rules neural networks see bertsekas tsitsiklis sutton barto review 
architectures choose complexity number tunable parameters model priori 
approximation methods include fuzzy sets hard soft state aggregation non parametric methods variable resolution instance learning viscosity solutions local averaging ormoneit sen refer reader sutton barto review methods 
approach fundamentally di erent 
postulate probabilistic generative model value bayesian analysis extract posterior model 
approach nonparametric complexity value representation size dictionary adapted line match task hand 
method goes reinforcement learning applied solve control problem dynamics known dif cult capture 
speci cally method replace discretization methods see rust signi cant advantage gaussian process approach error estimate provided extra cost 
apart bene having con dence intervals value estimate ways may put 
instance rl agent may information actively direct exploration see dearden possibility con dence measure provide stopping conditions trials episodic tasks suggested bertsekas tsitsiklis 
natural directions research 
extension called 
conjecture extension help alleviate problems experienced stochastic environments 
second mentioned plan test new rl methods value uncertainty measure provided 
third establish convergence guarantees spirit tsitsiklis van roy 
aim reliably solving complete rl problem nding optimal policy 
may achieved actor critic architecture sutton barto 
acknowledgments authors grateful helpful discussions 
partially supported mit merrill lynch partnership 
bertsekas tsitsiklis 

neuro dynamic programming 
athena scienti burges 

simpli ed support vector decision rules 
proceedings thirteenth international conference machine learning pp 

csat opper 

sparse representation gaussian process models 
advances neural information processing systems pp 

dearden friedman russell 

bayesian learning 
proceedings fifteenth national conference arti cial intelligence pp 

dietterich wang 

batch value function approximation support vectors 
advances neural information processing systems pp 

engel meir 

sparse online greedy support vector regression 
th european conference machine learning pp 

gibbs mackay 

ecient implementation gaussian processes 
draft 


study reinforcement learning continuous case means viscosity solutions 
machine learning 
ormoneit sen 

kernel reinforcement learning 
machine learning 
rust 

randomization break curse dimensionality 
econometrica 


statistical signal processing 
addisonwesley 
sch olkopf smola 

learning kernels 
mit press 
smola bartlett 

sparse greedy gaussian process regression 
advances neural information processing systems pp 

sutton 

learning predict methods temporal di erences 
machine learning 
sutton barto 

reinforcement learning 
mit press 
tesauro 

temporal di erence learning 
comm 
acm 
tsitsiklis van roy 

analysis temporal di erence learning function approximation 
ieee trans 
automatic control 
williams 

prediction gaussian processes linear regression linear prediction 
learning graphical models 
cambridge massachusetts 
williams seeger 

nystr om method speed kernel machines 
advances neural information processing systems pp 

