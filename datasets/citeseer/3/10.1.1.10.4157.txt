int computer vision marr prize issue 
image parsing unifying segmentation detection recognition tu chen alan yuille song chun zhu departments statistics psychology computer science university california los angeles los angeles ca 
emails yuille stat ucla edu bayesian framework parsing images constituent visual patterns 
parsing algorithm optimizes posterior probability outputs scene representation parsing graph spirit similar parsing sentences speech natural language 
algorithm constructs parsing graph re configures dynamically set reversible markov chain jumps 
computational framework integrates popular inference approaches generative top methods discriminative bottom methods 
formulates posterior probability terms generative models images defined likelihood functions priors 
computes discriminative probabilities sequence cascade bottom tests filters 
markov chain design posterior probability defined generative models invariant target probability markov chain discriminative probabilities construct proposal probabilities drive markov chain 
intuitively bottom discriminative probabilities activate top generative models 
focus types visual patterns generic visual patterns texture shading object patterns including human faces text 
types patterns compete cooperate explain image image parsing unifies image segmentation object detection recognition generic visual patterns image parsing correspond image segmentation :10.1.1.21.2979
illustrate algorithm natural images complex city scenes show examples image segmentation improved allowing object specific knowledge disambiguate low level segmentation cues conversely object detection improved generic visual patterns explain away shadows occlusions 
keywords 
image parsing image segmentation object detection object recognition data driven markov chain monte carlo adaboost 
objectives image parsing define image parsing task decomposing image constituent visual patterns 
output represented hierarchical graph called parsing graph 
goal optimize bayesian posterior probability 
illustrates typical example football scene divided parts coarse level person foreground sports field 
parts decomposed visual patterns second level face texture regions text point process band field curve process markings field color region region nearby people 
principle continue decomposing parts reach resolution criterion 
parsing graph similar spirit parsing trees speech natural language processing include horizontal connections see dashed curves specifying spatial relationships boundary sharing different visual patterns 
face person texture text point process football match scene sports field color region curve groups texture spectator image parsing example 
parsing graph hierarchical combines generative models downward arrows horizontal connections dashed lines specify spatial relationships visual patterns 
see representation including variables node attributes 
texture persons natural language processing parsing graph fixed depends input image 
image parsing algorithm construct parsing graph fly image parsing algorithm consists set reversible markov chain jumps type jump corresponding operator reconfiguring parsing graph creating deleting nodes changing values node attributes :10.1.1.57.5225
jumps combine form ergodic reversible markov chain space possible parsing graphs 
markov chain probability guaranteed converges invariant probability markov chain simulate fair samples probability 
approach built previous data driven markov chain monte carlo ddmcmc recognition segmentation grouping graph partitioning :10.1.1.59.3042:10.1.1.21.2979
image parsing seeks full generative explanation input image terms generative models diverse visual patterns occur natural images see 
differs computer vision tasks segmentation grouping recognition 
usually performed isolated vision modules seek explain parts image 
image parsing approach enables different modules cooperate compete give consistent interpretation entire image 
integration visual modules increasing importance progress individual modules starts approaching performance ceilings 
particular segmentation edge detection reached performance levels little room improvement low level cues :10.1.1.160.2324:10.1.1.21.2979
example segmentation failures resolved combining segmentation object detection recognition 
successful detection recognition objects classification natural scenes broadly speaking discriminative methods local bottom tests :10.1.1.18.7575
combining different visual modules requires common framework ensures consistency 
despite effectiveness discriminative methods computing scene components object labels categories generate redundant conflicting results 
mathematicians argued discriminative methods followed sophisticated processes graphical inference algorithms literature assume fixed graphs see belief propagation 
natural images posterior probabilities strongly peaked stochastic samples close maxima posterior 
distinguish sampling inference optimization 
input image segmentation synthesized image manual segmentation examples image segmentation failure algorithm uses generic visual patterns low level visual cues :10.1.1.21.2979
results show low level visual cues sufficient obtain intuitive segmentations 
limitations generic visual patterns clear synthesized images obtained stochastic sampling generative models parameters estimated ddmcmc 
right panels show segmentations obtained human subjects contrast algorithm appear object specific knowledge doing segmentation instructed :10.1.1.129.2941
conclude achieve segmentation types images requires combining segmentation object detection recognition 
remove false alarms ii amend missing objects global context information iii reconcile conflicting overlapping explanations model comparison 
impose processes generative models entire image 
show image parsing algorithm able integrate discriminative generative methods take advantage complementary strengths 
couple modules segmentation object detection choice set visual patterns parse image 
focus types patterns generic visual patterns low middle level vision texture shading object patterns high level vision frontal human faces text 
types patterns illustrate different ways parsing graph con structed see related discussion 
object patterns face text comparatively little variability effectively detected bottom tests parts located 
parsing sub graphs con structed decompositional manner parts 
contrast generic texture region arbitrary shape intensity pattern high entropy 
detecting region bottom tests require enormous number tests deal variability computationally impractical 
parsing subgraphs built grouping small elements compositional manner :10.1.1.30.3515
illustrate algorithm natural images complex city scenes give examples image segmentation improved allowing object specific knowledge disambiguate low level cues conversely object detection improved generic visual patterns explain away shadows occlusions 
structured follows 
section give overview image parsing framework discuss theoretical background 
section describe parsing graph generative models generic visual patterns text faces 
section give control structure image parsing algorithm 
section gives details components algorithm 
section shows combine adaboost tests get proposals detecting objects including text faces 
section experimental results 
section addresses open problems developing image parser general inference engine 
summarize section 
overview image parsing framework bottom top processing major element integrate discriminative generative methods inference 
computer vision literature top bottom procedures broadly popular inference paradigms generative methods top discriminative methods bottom illustrated 
perspective integrating generative discriminative models equivalent combining bottom top processing 
role bottom top processing vision discussed 
growing evidence see humans perform high level scene object categorization tasks fast low level texture discrimination called pre attentive vision tasks 
suggests humans detect low high level visual patterns early stages visual processing 
contrasts traditional bottom feedforward architectures start edge detection followed segmentation grouping proceeding object recognition high level vision tasks 
experiments relate long standing conjectures role bottom top loops visual cortical areas visual routines pathways binding visual cues neural network models helmholtz machine 
combining bottom top processing clearly important rigorous mathematical framework achieve 
unify generative discriminative approaches designing ddmcmc algorithm uses discriminative methods perform rapid inference parameters gen models 
computer vision perspective ddmcmc combines bottom processing implemented discriminative models top processing generative models 
rest section gives overview approach 
generative discriminative methods generative methods specify image generated scene representation 
combines prior likelihood function give joint posterior probability 
expressed probability probabilities graphs input image represented leaf nodes denotes remaining nodes node attributes graph 
structure graph example number nodes unknown estimated input image 
inference performed stochastic sampling posterior 
enables estimate arg max 
dimension sample space high standard sampling techniques computationally expensive 
contrast discriminative methods fast compute 
specify mod els image generated 
give discriminative conditional probabilities wj components wj sequence bottom tests performed image 
tests local image features fj computed image cascade manner adaboost filters see section fj fj fj 
assuming known algorithms estimating directly 
theorem shows kl divergence true marginal posterior wj optimal discriminant approximation wj tst test tst decrease monotonically new tests added theorem information gained variable new test tst decrease kullback leibler divergence best discriminative estimate increase mutual information tests 
ei kl tst ei kl tst tst mi tst tst mi tst tst kl tst ei expectation respect tst expectation respect probability test responses tst tst induced 
decrease kullback leibler divergence equals zero tst sufficient statistics respect practice discriminative methods particularly standard computer vision algorithms see subsection typically small number features computational practicality 
discriminative probabilities wj tst optimal 
fortunately image parsing algorithm requires discriminative probabilities wj tst rough approximations wj 
wk tst comparison inference paradigms top generative methods versus bottom discriminative methods 
generative method specifies image synthesized scene representation contrast discriminative methods performing tests guaranteed yield consistent solutions see crosses explained text 
optimal approximation occurs wj tst equals probability wj tst induced 
difference discriminative generative models illustrated 
dis models fast compute run parallel different components computed independently see arrows 
components wi may yield consistent solution may specify consistent model generating observed image inconsistencies indicated crosses 
generative models ensure consistency require solving difficult inference problem 
open problem discriminative models designed infer entire state complicated generative models dealing 
mathematicians argued practical discriminative models require additional post processing 
markov chain kernels sub kernels formally ddmcmc image parsing algorithm simulates markov chain mc kernel space probability starting state 
element parsing graph 
set parsing graphs finite images finite pixels grey levels 
proceed defining set moves reconfiguring graph 
include moves create nodes ii delete nodes iii change node attributes 
specify stochastic dynamics moves terms transition kernels move define markov chain sub kernel transition matrix ka index 
represents probability system transition state state sub kernel applied ka 
kernels alter graph structure grouped reversible pairs 
example sub kernel node creation ka paired sub kernel node deletion ka 
combined paired sub kernel ka ar 
pairing ensures ka ka states 
sub kernels pairing constructed obey detailed balance condition ka ka 
choose stochastic dynamics markov chain probability guaranteed converge posterior 
complexity problem means deterministic algorithms implementing moves risk getting stuck local minima 
full transition kernel expressed ka 
implement kernel time step algorithm selects choice move probability move uses kernel ka select transition state state note probabilities ka depend input image distinguishes ddmcmc methods conventional mcmc computing 
full kernel obeys detailed balance equation sub kernels 
ergodic provided set moves sufficient transition states moves 
conditions ensure invariant target probability markov chain finite space 
applying kernel updates markov chain state probability step ka 
summary ddmcmc image parser simulates markov chain mc unique invariant probability 
time markov chain state parse graph follows probability product sub kernels selected time wo wo 
indexes sub kernel selected time time increases approaches posterior monotonically geometric rate 
convergence theorem useful image parsing helps quantify effectiveness different sub kernels 
theorem kullback leibler divergence posterior markov chain state probability decreases monotonically sub kernel applied kl kl decrease kl divergence strictly positive equal zero markov chain stationary algorithms belief propagation derived approximations update equation gibbs sampler making independence assumptions 
proof see appendix theorem related second law thermodynamics proof detailed balance equation 
kl divergence gives measure power sub kernel suggests efficient mechanism selecting sub kernels time step see section 
contrast classic convergence analysis see appendix show convergence markov chain exponentially fast give measures power sub kernels 
ddmcmc proposal probabilities describe design sub kernels proposal probabilities discriminative models 
heart ddmcmc 
sub kernel designed metropolis hastings form ka qa min qa qa transition proposed stochastically proposal probability qa accepted stochastically acceptance probability min qa qa 
metropolis hastings ensures sub kernels obey detailed balance pairing 
proposal probability qa product factorized discriminative probabilities wj respective elements wj changed move see section 
set bottom tests proposal probabilities qa qa 
proposal probabilities fast compute evaluated possible state sub kernel reach propose transitions states posterior high 
acceptance probabilities computationally expensive dependence need evaluated proposed state 
design proposals trade 
ideally proposals sampled posterior impractical 
trade requires possibility evolves region boundaries 
making large moves time step ii proposals encourage moves states high posterior probability iii proposals fast compute 
formally define scope ka set states reached time step sub kernel want scope sa large large moves space time step jump solution crawl 
scope possible include states high posterior scope large right part 
proposals qa chosen approximate 
proposals functions discriminative models components generative models current state computationally cheap evaluate generative models current state 
details model determine form proposals large scope keeping proposals easy compute able approximate equation 
see detailed examples section 
description gives bare bones ddmcmc 
refer sophisticated discussion issues mcmc perspective 
discussion section describe strategies improve 
preliminary theoretical results convergence ddmcmc encouraging special case see appendix 
appendix address important practical issue maintain detailed balance multiple routes transition state describe ways trade offs involved 
generative models bayesian formulation section describes parsing graph generative models image parsing algorithm 
illustrates general structure parsing graph 
take simplified layer graph illustrated fully specified generative sense 
top node root graph represents scene label 
intermediate nodes visual patterns face text texture shading 
visual pattern number pixels text face scene representation parsing graph 
intermediate nodes represent visual patterns 
child nodes correspond pixels image 
bottom leaves 
graph horizontal connections considered visual patterns share boundaries form partition image lattice 
number intermediate nodes random variable node set attributes li defined follows 
li shape descriptor determines region ri li image pixels covered visual pattern intermediate node 
conceptually pixels ri child nodes intermediate node 
regions may contain holes case shape descriptor internal external boundaries 
remaining attribute variables specify probability models li li generating sub image li region li 
variables indicate visual pattern type types generic visual patterns face pattern text character patterns denotes model parameters corresponding visual pattern details sections 
complete scene description summarized li 
shape descriptors li required consistent pixel image child intermediate nodes 
shape descriptors provide partition image lattice height satisfy condition li li lj generation process scene description governed likelihood function prior probability defined ir li li 
li li 
bayesian formulation parsing image corresponds computing maximizes posteriori probability solution space arg max arg max 
remains specify prior likelihood function 
set prior terms uniform probabilities 
term li penalize high model complexity estimated generic visual patterns training data :10.1.1.21.2979:10.1.1.21.2979
shape models types shape descriptor 
define shapes generic visual patterns faces 
second defines shapes text characters 

shape descriptors generic visual patterns faces case shape descriptor represents boundary image region list pixels li ri 
prior defined li exp li li 
set 
computational reasons prior face shapes complicated priors applied 

shape descriptors text characters model text characters deformable templates corresponding digits letters upper lower cases 
deformable templates defined boundary include internal boundary hole inside image region explained different visual pattern 
random samples drawn shape descriptors text characters 
prototype characters set deformations 
prototypes represented outer boundary inner boundaries 
boundary modeled spline control points 
prototype characters indexed ci control points represented matrix ci 
define types deformations templates 
global affine transformation local elastic deformation 
allow letters deformed affine transform mi 
put prior mi penalize severe rotation distortion 
obtained decomposing mi mi cos sin sin cos rotation angle denote scaling shearing 
prior mi parameters 
mi exp ch allow local deformations adjusting positions spline control points 
digit letter ci affine transform mi contour points template gt mi ci ms mi ci 
similarly contour points shape control points si gs mi ci ms si ms spline matrices 
define probability distribution si mi ci elastic deformation si si mi ci exp li gs mi ci gt mi ci gs mi ci gt mi ci distance contour template de formed contour deformations small correspondence points curves obtained nearest neighbor matches see refine 
shows samples drawn model 
summary deformable template indexed ci shape descriptor prior distribution li specified li ci mi si li ci mi si mi ci 
ci uniform distribution digits letters place prior distribution text strings possible 
generative intensity models families generative intensity models describing intensity patterns approxi mately constant intensity clutter texture shading face 
similar defined :10.1.1.21.2979:10.1.1.21.2979

constant intensity model 
assumes pixel intensities region subject independently identically distributed iid gaussian distribution 
clutter texture model 
iv non parametric intensity histogram discretized take values expressed vector hg 
nj number pixels intensity value 
shading model 
iv nj hg 
family models describe generic shading patterns text characters 
quadratic form ax cy dx ey parameters 
generative model pixel ir 
pca face model 
iv jv 
random samples drawn pca face model 
generative model faces simpler uses principal component analysis pca obtain representations faces terms principal components bi covariances 
lower level features modeled pca added 
shows faces sampled pca model 
add features occlusion process described hallinan 
ir ir ibi 
overview algorithm section gives control structure image parsing algorithm strategy de scribed section shows diagram 
algorithm construct parse graph fly estimate scene interpretation illustrates algorithm selects markov chain moves dynamics sub kernels search space possible parse graphs image altering graph structure deleting adding nodes changing node attributes 
equivalent way visualizing algorithm terms search solution space see details viewpoint 
diffusion diffusion split merge examples markov chain dynamics change graph structure node attributes graph giving rise different ways parse image 
define set moves reconfigure graph 
birth death face nodes ii birth death text characters iii splitting merging regions iv switching node attributes region type model parameters boundary evolution altering shape descriptors li nodes adjacent regions 
moves implemented sub kernels 
moves reversible jumps implemented metropolis hastings equation :10.1.1.57.5225
fifth move boundary evolution implemented stochastic partial differential equation 
sub kernels moves require proposal probabilities driven elementary native methods review subsection 
proposal probabilities designed criteria subsection full details section 
control structure algorithm described section 
full transition kernel image parser built combining sub kernels described subsection 
algorithm proceeds stochastically selecting sub kernel selecting graph apply deciding accept operation 
discriminative methods discriminative methods give approximate posterior probabilities wj components wj computational efficiency probabilities small number simple tests 
briefly overview classify discriminative methods implementation 
sec tion shows discriminative methods composed see crosses give proposals making moves parsing graph 

edge cues 
cues edge detectors 
give proposals region boundaries shape descriptor attributes nodes 
specifically run canny detector scales followed edge linking give partitions image lattice 
gives finite list candidate partitions assigned weights see section :10.1.1.21.2979
discriminative probability represented weighted list particles 
prin statistical edge detectors preferable canny give discriminative probabilities wj learnt training data 

binarization cues 
cues computed variant niblack algorithm 
propose boundaries text characters shape descriptors text nodes conjunction proposals text detection 
binarization algorithm example output section 
edge cues algorithm run different parameters settings represents discriminative probability weighted list particles indicating candidate boundary locations 

face region cues 
cues learnt variant adaboost outputs discriminative probabilities see section :10.1.1.30.3515:10.1.1.24.5565
propose presence faces sub regions image 
cues combined edge detection propose localization faces image 

text region cues 
cues learnt probabilistic version adaboost see section 
algorithm applied image windows range scales 
outputs discriminative probability presence text window 
text region cues combined binarization propose boundaries text characters 

shape affinity cues 
act shape boundaries produced binarization propose text characters 
shape context cues information features propose matches shape boundaries deformable template models text characters :10.1.1.133.1040:10.1.1.18.7575

region affinity cues 
estimate regions ri rj generated visual pattern family model parameters 
affinity similarity measure intensity properties iri :10.1.1.160.2324
model parameter visual pattern family cues 
propose model parameters visual pattern family identity 
clustering algorithms mean shift 
clustering algorithms depend model types described :10.1.1.21.2979:10.1.1.21.2979
current implementation conduct bottom tests early stage discriminative models qj wj combined form composite tests ka equations 
may efficient perform test required see discussion section 
control structure algorithm control strategy image parser illustrated 
image parser explores space parsing graphs markov chain monte carlo sampling algorithm 
algorithm uses transition kernel composed sub kernels ka corresponding different ways reconfigure parsing graph 
sub kernels come reversible pairs birth death designed target probability distribution kernel generative posterior 
time step sub kernel selected stochastically 
sub kernels metropolis sampling algorithm see equation proceeds stages 
proposes reconfiguration graph sampling proposal probability 
accepts rejects reconfiguration sampling acceptance probability 
summarize outline algorithm 
time step specifies stochastically move select sub kernel apply graph accept move 
probability select moves set independent got better performance adapting discriminative cues estimate number faces text characters image see details 
choice apply move specified stochastically sub kernel 
sub kernels selected randomly chosen fitness factor see details section measures current model fits image data 
annealing required start algorithm limited boundary evolution sub kernel described separately see section 
weighted particles text sub kernel face sub kernel markov kernel generic sub kernel birth death birth death split merge input image model switching sub kernel text detection face detection edge partition parameter clustering generative inference discriminative inference integrating generative top discriminative bottom methods image parsing 
diagram illustrates main points image parser 
dynamics implemented ergodic markov chain invariant probability posterior composed reversible sub kernels ka making different types moves parse graph giving birth new nodes merging nodes 
time step algorithm selects sub kernel stochastically 
selected sub kernel proposes specific move create delete specific nodes move evaluated accepted stochastically see equation 
proposals bottom discriminative top generative processes see subsection 
bottom processes compute discriminative probabilities wj input image feature tests 
additional sub kernel boundary evolution uses stochastic partial differential equation described 
scope moves current implementation need annealing reduced compositional techniques described 
improved algorithm making move selection adapt image making depend 
particular increased probability giving birth death faces text bottom adaboost proposals suggested objects scene 
example number proposals faces text threshold ta 
modify probabilities table kg kg tb tb tb chosen normalize probability 
basic control strategy image parsing algorithm 
initialize dividing image regions setting shape descriptors assigning remaining node attributes random 

set temperature 

select type move sampling probability faces text splitting merging switching region model type model parameters boundary evolution 
modified slightly adaptively see text 

selected move boundary evolution select adjacent regions nodes random apply stochastic steepest descent see section 

jump moves selected new solution randomly sampled follows birth death face see section propose create delete face 
includes proposal image 
birth death text see section propose create text character delete existing 
includes proposal 
region splitting see section region node randomly chosen biased fitness factor 
proposals split attributes resulting nodes 
region merging see section neighboring regions nodes selected proposal probability 
proposals attributes resulting node 
switching see section region selected randomly fitness factor new region type model parameters proposed 
full proposal probabilities computed 
metropolis hastings algorithm equation applied accept reject proposed move 

reduce temperature exp current iteration step constant size image 

repeat steps convergence criterion satisfied reaching maximum number allowed steps lack decrease negative log posterior 
markov chain kernels section gives detailed discussion individual markov chain kernel proposal probabilities fitness factors 
template evolution region boundaries implemented stochastic partial differential equations driven models competing ownership regions 
boundary evolution moves evolve positions region boundaries preserve graph structure 
implemented stochastic partial differential equation langevin equation driven brow noise derived markov chain 
deterministic component pde control points obtained performing steepest descent negative log posterior derived 
illustrate approach deriving deterministic component pde evolution boundary letter tj generic visual pattern region ri 
boundary expressed terms control points sm shape descriptor letter 
denote point boundary ri rj 
deterministic part evolution equation obtained derivative negative log posterior log respect control points 
relevant parts negative log posterior see equation ri tj ri tj lj ri log dxdy ri ri 
log dxdy lj log lj 
differentiating ri tj respect control points sm yields evolution pde dsm dt ri sm tj sm ri tj ds log dj di gt ds jacobian matrix spline function 
recall implementa tion 
log likelihood ratio term log generic region models ownership boundary pixels 
markov chain sub kernels implements competition letter changes graph structure realized markov chain jumps implemented different sub kernels 
sub kernel birth death text proposals birth text proposed proposed ext proposals death text example birth death text 
state consists generic regions character 
proposals computed candidate characters obtained adaboost binarization methods see section 
selected see arrow changes state conversely candidate state selected see arrow returns system state pair jumps create delete text characters 
start parse graph transition parse graph creating character 
conversely transition back deleting character 
proposals creating deleting text characters designed approximate terms equation 
obtain list candidate text character shapes adaboost detect text regions followed binarization detect candidate text character boundaries text regions see section 
list represented set particles weighted similarity deformable templates text characters see 
similarly specify set weighted particles removing text characters 
represent possible discretized shape positions text character deformable templates creating removing text particles compute proposal probabilities weights corresponding weights 
creating new text characters specified shape affinity mea sures shape contexts informative features :10.1.1.133.1040:10.1.1.18.7575
deleting text characters calculate directly likelihood prior text character 
ideally weights approximate ratios sub kernel ii birth death face sub kernel birth death faces similar sub kernel birth death text 
adaboost method discussed sect 
detect candidate faces 
face boundaries obtained directly edge detection give candidate face boundaries 
proposal probabilities computed similarly sub kernel sub kernel iii splitting merging regions pair jumps create delete nodes splitting merging regions nodes 
start parse graph transition parse graph splitting node nodes conversely transition back merging nodes node selection region split robust function iri li worse model region ri fits data split 
merging region affinity measure propose merges regions high affinity :10.1.1.160.2324:10.1.1.160.2324
proposed compute proposal probabilities split proposed compute proposal probabilities merge example split merge sub kernel 
state consists regions proposals computed candidate splits 
selected see arrow changes state conversely candidate merges state selected see arrow returns system state formally define lk li lj denotes attributes remaining nodes graph 
obtain proposals seeking approximations equation follows 
obtain edge maps 
canny edge detectors different scales see details :10.1.1.21.2979
edge maps create list particles splitting 
list particles merging denoted 

represent possible discretized positions splitting merging weights defined shortly 
words split region regions contour forms new boundary 
similarly merge regions region deleting boundary contour example shows candidate sites splitting sites merging define weights 
weights determine probabilities splits merges approximate ratios respectively 
iri li lj lk li lj lk expensive compute approximate ri rj lk li lj lk ri rj iri li lj lk li lj ri rj affinity measure similarity regions ri rj weighted sum intensity difference ii ij chi squared difference intensity histograms li priors shape descriptors obtained clustering parameter space see :10.1.1.160.2324:10.1.1.21.2979
jump ii switching node attributes moves switch attributes node involves changing region type model parameters move transitions states li proposal see equation approximate approximate weight iri iri li li iri li li functions split merge moves 
proposal probability weight normalized candidate set adaboost discriminative probabilities face text section describes adaboost techniques compute discriminative probabilities detecting faces text strings letters 
describe binarization algorithm detect boundaries text characters 
computing discriminative probabilities adaboost standard adaboost algorithm example distinguishing faces non faces learns binary valued strong classifier combining set binary valued weak classifiers feature tests hn set weights ada sign ihi sign ada 
features selected pre designed dictionary ada 
selection features tuning weights posed supervised learning problem 
set labeled examples ii adaboost learning formulated greedily optimizing function ada tst ada arg min arg min ada ada exp ada ii :10.1.1.24.5565
obtain discriminative probabilities theorem states features test learnt adaboost give asymptotically posterior probabilities object labels face non face :10.1.1.30.3515:10.1.1.30.3515
adaboost strong classifier log posterior ratio test 
theorem friedman sufficient training samples features adaboost learning selects weights ada tests tst ada satisfy ada ii ada ii ada ii strong classifier converges asymptotically posterior probability ratio test sign ada sign 
practice adaboost classifier applied windows image different scales 
window evaluated face non face text versus non text 
images posterior probabilities faces text negligible parts image 
cascade tests enables rapidly reject windows setting marginal probabilities zero 
course adaboost converge approximations true posterior probabilities limited number tests limited amount training data 
note adaboost way learn posterior probability see theorem 
effective object patterns relatively rigid structures faces text shapes letters variable patterns sequence fairly structured 
adaboost training standard adaboost training methods combined cascade approach asymmetric weighting :10.1.1.30.3515:10.1.1.21.2979:10.1.1.133.1040
cascade enables algorithm rule image face text locations tests allows computational resources concentrated challenging parts images 
scenes training text patches extracted 
adaboost text designed detect text segments 
test data extracted hand images san francisco see contained text images seen 
half images taken blind volunteers reduces bias 
divided text image overlapping text segments fixed width height ration typically containing letters 
total text segments positive training set 
negative examples obtained bootstrap process similar drucker 
selected negative examples randomly sampling windows image dataset 
training samples applied adaboost algorithm range scales classify windows training images 
misclassified text negative examples stage adaboost 
image regions easily confused text vegetation repetitive structures building facades 
features adaboost image tests corresponding examples text image extracted text segments 
examples faces training 
positive training examples adaboost 
statistics elementary filters 
features chosen detect properties text segments relatively invariant shapes individual letters digits 
included averaging intensity image windows statistics number edges 
refer details 
shows failure examples adaboost text detection 
correspond situations heavy shading blurred images isolated digits vertical commercial signs non standard fonts 
included training examples shown 
adaboost posteriors faces trained similar way 
time haar basis functions elementary features 
feret database positive examples see allowing small rotation translation transformation positive examples 
strategy described text obtain negative examples 
failure examples shown 
cases evaluated log posterior ratio test testing datasets number different thresholds see 
agreement previous faces adaboost gave high performance false positives false negatives see table 
low error rates slightly misleading enormous number windows image see examples difficult text examples difficult faces failure examples text faces adaboost misses 
text image failures challenging included positive training examples 
face failures include heavy shading occlusion facial features 
table 
small false positive rate may imply large number false positives regular image 
varying threshold eliminate false positives false negatives time 
illustrate showing face regions text regions proposed adaboost 
attempt classification putting threshold correctly detect faces text expense false positives 
object false positive false negative images subwindows face face face text text table performance adaboost different thresholds 
adaboost integrated generic region models image parser generic region proposals remove false positives find text adaboost misses 
example right panel detected adaboost algorithm trained text segments 
detected generic shading region recognized letter boxes show faces text detected adaboost log posterior ratio test fixed threshold 
observe false positives due vegetation tree structure random image patterns 
impossible select threshold false positives false negatives image 
shown experiments generative models remove false positives recover missing text 
see 
false positive text faces removed figures 
adaboost algorithm text needs supplemented binarization algorithm described determine text character location 
followed shape contexts informative features binarization results proposals presence specific letters digits :10.1.1.133.1040:10.1.1.18.7575
cases see results binarization letters digits detected proposals binarization stage automatically accepted 
case 
note binarization gives far better results alternatives edge detection 
example binarization detected text 
binarization algorithm variant proposed niblack 
image intensity adaptive thresholding adaptive window size 
adaptive methods needed image windows containing text shading shadow occlusion 
binarization method determines threshold tb pixel intensity distribution local window centered 
tb std std intensity mean standard deviation local window 
size local window selected smallest window intensity variance fixed threshold 
parameter allows cases foreground brighter darker background 
experiments image parsing algorithm applied number outdoor indoor images 
speed pcs pentium iv comparable segmentation methods normalized cuts ddmcmc algorithm :10.1.1.14.1476:10.1.1.21.2979:10.1.1.21.2979
typically runs minutes 
main portion computing time spent segmenting generic regions boundary diffusion 
input image segmentation object recognition synthesized image results segmentation recognition images 
results improved compare purely bottom adaboost results displayed 
input image synthesis synthesis close look image 
dark glasses explained generic shading model face model fit part data 
face model difficulty try fit glasses eyes 
standard adaboost correctly classifies faces expense false positives see 
show examples synthesized faces synthesis dark glasses modelled shading regions synthesis dark glasses removed generative face model sample parts face eyes obscured dark glasses 
figures show challenging examples heavy clutter shading effects 
results parts 
shows segmentation boundaries generic regions objects shows text faces detected text symbols indicate text recognition letters correctly read algorithm 
synthesize images sampled likelihood model parsing graph faces text regions parameters boundaries obtained parsing algorithm 
synthesized images visualize parsing graph image content computer understand 
experiments observed face text models improved image segmentation results comparison previous generic region models :10.1.1.21.2979
conversely generic region models improve object detection removing false alarms recovering objects initially detected 
discuss specific examples 
showed images text faces detected purely bottom adaboost 
impossible select threshold adaboost algorithm false positives false negatives 
ensure false negatives apart lower threshold admit false positives due vegetation heavy shadows shadow sign heights optical 
letter detected threshold 
adaboost algorithm trained detect text segments respond single digit 
comparison shows image parsing results images 
see false alarms proposed adaboost removed better explained generic region models 
generic shading models help object detection explaining away heavy shading text heights optical dark glasses women see 
missing digit correctly detected 
algorithm detected generic shading region digit sub kernel switches node attributes 
input image segmentation object recognition synthesized image results segmentation recognition outdoor images 
observe ability detect faces text multiple scale 
ability synthesize image parsing graph advantage bayesian approach 
synthesis helps illustrate successes weaknesses gener ative models 
synthesized images show information image captured models 
table give number variables represen tation show roughly proportional jpeg bytes 
variables represent points segmentation boundary counted independently 
reduce coding length substantially encoding boundary points effectively example spatial proximity 
image encoding goal cur rent sophisticated generative models needed synthesize realistic images 
image soccer parking street westwood jpg bytes table number variables image compared jpg bytes 
discussion section describe challenging technical problems image parsing 
current addresses issues 

mechanisms constructing parsing graph stated parsing graph constructed compo decompositional modes 
compositional mode proceeds grouping small elements decompositional approach involves detecting object locating parts see 
compositional mode appears effective 
detecting cheetah bottom tests learnt adaboost difficult owing large variability shape photometric properties 
contrast quite practical swendsen wang cuts segment image obtain boundary cheetah bottom compositional approach parsing tree multiple levels 
parsing graph constructed starting pixels leaves pixels 
level graph obtained local image texture similarities construct graph nodes corresponding atomic regions image 
algorithm nodes texture regions level grouping atomic regions atomic region node child texture region node 
level compute discriminative proposal object regions atomic regions pixels pca faces image pixels composition decomposition mechanisms constructing parsing graph 
see text explanation 
probability adjacent nodes pixels atomic regions belong object pattern 
apply transition kernel implementing split merge dynamics proposals 
refer detailed discussion 
objects little variability faces shown bottom proposals adaboost activate node represents entire face 
parsing graph constructed downwards decompositional mode expanding face node create child nodes parts face 
child nodes turn expanded grandchild nodes representing finer scale parts 
amount node expansion adaptive depend resolution image 
example largest face expanded child nodes sufficient resolution expand face nodes corresponding smaller faces 
major technical problem develop mathematical criterion mode effective types objects patterns 
enable algorithm adapt search strategy accordingly 

optimal ordering strategy tests kernels control strategy current image parsing algorithm select tests sub kernels optimal way 
time step choice sub kernel independent current state choice graph apply sub kernel depend 
parts bottom tests performed algorithm 
efficient control strategy selects sub kernels tests adaptively provided selection process requires low computational cost 
seek find optimal control strategy selection effective large set images visual pat terns 
selection criteria select tests sub kernels maximize gain information 
propose information criteria described section 
stated theorem 
measures information gained variable parsing graph performing new test tst 
information gain tst kl tst kl tst denotes previous tests kl kullback leibler divergence 
second stated theorem 
measures power sub kernel ka decrease kl divergence ka kl kl 
amount decrease gives measure power sub kernel ka informed 
need take account computational cost selection procedures 
see case study optimally select tests account computational costs 
summary introduces computational framework parsing images basic visual patterns 
formulated problem bayesian probability theory designed stochastic ddmcmc algorithm perform inference 
framework gives way combine segmentation object detection recognition 
give proof concept implementing model visual patterns include generic regions texture shading objects text faces 
approach enables different visual patterns compete cooperate explain input images 
provides way integrate discriminative generative methods inference 
methods extensively vision machine learning communities corre distinction bottom top processing 
discriminative methods typically fast give sub optimal inconsistent results see 
contrast gen methods optimal sense bayesian decision theory slow require extensive search 
ddmcmc algorithm integrates methods illustrated discriminative methods propose generative solutions 
goal algorithm construct parse graph representing image 
structure graph fixed depend input image 
algorithm proceeds constructing markov chain dynamics implemented sub kernels different moves configure parsing graph creating deleting nodes altering node attributes 
approach scaled adding new sub kernels corresponding different vision models 
similar spirit ullman concept visual routines 
ideas applied inference problem formulated probabilistic inference graphs 
group deals related series visual inference tasks similar framework 
includes image segmentation curve grouping shape detection motion analysis scene reconstruction :10.1.1.21.2979
plan integrate visual modules develop general purpose vision system 
working ways improve speed image parsing algorithm discussed section 
particular expect swendsen wang cut algorithms drastically accelerate search :10.1.1.59.3042
anticipate improvements reduce running time ddmcmc algorithms minutes minute :10.1.1.21.2979
appendix proof theorem proof 
notational simplicity ignore dependencies kernels probabilities input image wt state probability time step applying sub kernel ka wt wt state wt probability wt wt ka wt wt 
wt joint probability wt wt wt wt expressed ways wt wt wt ka wt wt wt pmc wt wt pmc wt wt posterior probability state wt time step conditioned state wt time step 
joint probability wt wt compared joint probability wt wt equilibrium wt wt 
express wt wt ways wt wt wt ka wt wt wt ka wt wt second equality obtained detailed balance condition ka 
calculate kullback leibler divergence joint probabilities wt wt wt wt ways second equalities equations 
obtain expressions divergence kl wt wt wt wt wt wt wt wt wt ka wt wt log wt ka wt wt wt ka wt wt wt wt log wt wt wt wt kl ka wt wt wt log wt ka wt wt wt wt wt pmc wt wt kl wt wt kl ka wt wt pmc wt wt equate alternatives expressions kl divergence equations obtain kl kl wt kl wt wt pmc wt wt proves divergence decreases monotonically 
decrease zero wt wt pmc wt wt kl equality 
occurs definition pmc wt wt equation detailed balance conditions wt wt 
proof 
appendix classic markov chain convergence result traditional markov chain convergence analysis concerned total variance invariant target probability markov chain state probability tv 
finite state space classic result relates tv measure second largest eigenvalue modulus kernel theorem adopted 
theorem diaconis irreducible kernel finite space invariant target probability initial state wo step tv appendix hitting time analysis wo wo 
include second result bounds expected hitting time arbitrary state number time steps markov chain mc visit state 
min wt 
expected hitting time higher resolution tv norm theorem kl divergence theorem concerned individual states entire space 
particular interested expected hitting time states high probability 
theorem shows informed proposal improve expected hitting time special class markov chains 
theorem zhu invariance target probability markov chain mc proposal probability metropolis equation min 
min tv markov chain design proposal probability depends called independence sampler mis 
similar result obtained gibbs sampler mgs 
result shows choose proposal probability overlaps large states high probability 
contrast choosing uniform probability result expected hitting times length greater state 
appendix multiple routes sub kernels section addresses practical issue detailed balance leads trade computation efficiency algorithm design 
notational simplicity write terms section 
situations sub kernels give multiple routes states example region split sub regions different bottom pro 
section show detailed balance maintained extra computation integrate multiple routes treating alternative routes separately different sub kernels price reduced efficiency 
path path 
pairs sub kernels 
sub kernels may multiple routes states creates additional computational burden computing acceptance rates 
suppose pairs routes qi qi states pairs moves 
represent transition kernel qi min 
time move considered consider possible routes result heavy computation 
treat route sub kernel described section 
markov chain satisfies detailed balance pair ki ki speci fied metropolis hastings obey detailed balance equation ki ki reduces computational cost 
theorem shows reduction comes price efficiency 
theorem kernels computed eqn 
satisfy ki ki theorem says markov chain obeys detailed balance pair moves effective combines routes 
markov chain design balance computation cost computing proposals effectiveness markov kernel 
situation shown treated special case 
acknowledgments supported nih nei ro ey nsf iis nsf iis onr 
authors smith research institute providing text training images 
wu stimulating discussions markov chain convergence theorems 
zhu graph partition swendsen wang cut proc 
int conf 
computer vision nice france october 
zhu multi grid multi level swendsen wang cuts hierarchic graph partition proc 
ieee conf 
computer vision pattern recognition washington dc june 
barnard forsyth learning semantics words pictures iccv 
belongie malik puzicha :10.1.1.133.1040:10.1.1.18.7575
shape matching object recognition shape contexts ieee trans 
pattern analysis machine intelligence 
bienenstock geman potter compositionality mdl priors object recognition nips :10.1.1.30.3515
geman hierarchical testing designs pattern recognition technical report math 
science johns hopkins university 
markov chains gibbs fields monte carlo simulation queues springer 
chapter 
bowyer dougherty edge detector evaluation empirical roc curves computer vision image understanding pp oct 
canny computational approach edge detection ieee trans 
pami vol nov 
chen yuille 
adaboost learning detecting reading text city scenes 
proc 
ieee conf 
computer vision pattern recognition 
washington dc june 
cootes edwards taylor active appearance models ieee trans 
pami vol 

comaniciu meer mean shift analysis applications proc 
iccv 
cover thomas elements information theory pp john wiley sons ny 
dayan hinton neal zemel 
helmholtz machine 
neural computation pp 

diaconis eigenanalysis examples metropolis algorithms contemporary mathematics vol 
pp 
drucker schapire simard boosting performance neural networks intl pattern rec 
artificial intelligence vol 

fowlkes malik globalization help segmentation cvpr 
freund schapire experiments new boosting algorithm proc 
th int conference machine learning 
friedman hastie tibshirani additive logistic regression statistical view boosting dept statistics stanford univ technical report :10.1.1.30.3515

geman huang diffusion global optimization siam control optimization vol 

green reversible jump markov chain monte carlo computation bayesian model determination biometrika vol :10.1.1.57.5225
pp 

hallinan gordon yuille giblin mumford dimensional patterns face peters 
han zhu bayesian reconstruction shapes scenes single image proc 
int workshop high level knowledge modeling motion nice france october 
hastings monte carlo sampling methods markov chains applications biometrika 
klein manning generative constituent context model improved grammar induction 
th annual meeting assoc 
computational linguistics july 
konishi yuille zhu statistical edge detection learning evaluating edge cues ieee trans 
pattern analysis machine intelligence vol 
pp 
li koch perona rapid natural scene categorization near absence attention proc 
national academy sciences vol 
liu monte carlo strategies scientific computing springer lowe distinctive image features scale invariant keypoints ijcv 
zhu heuristics expedite markov chain search proc 
rd workshop statistical computational theory vision 
malik belongie leung shi contour texture analysis image segmentation int journal computer vision vol :10.1.1.14.1476
manning sch tze 
foundations statistical natural language processing 
mit press 

marr 
vision 
freeman san francisco 
martin fowlkes tal malik database human segmented natural images application evaluating segmentation algorithms measuring ecological statistics proc :10.1.1.129.2941
th int conference computer vision 
metropolis rosenbluth rosenbluth teller teller equations state calculations fast computing machines chem 
phys 

moghaddam pentland probabilistic visual learning object representation ieee trans 
pami vol 
mumford 
neuronal architectures pattern theoretic problems 
large scale neuronal theories brain 
eds 
koch davis 
mit press 
bradford book 

murphy torralba freeman forest see tree graphical model relating features objects scenes nips 
niblack 
digital image processing 
pp 
prentice hall 
phillips wechsler huang rauss feret database evaluation procedure face recognition algorithms image vision computing journal vol 

ponce lazebnik schmid true object recognition reconnaissance de formes intelligence artificielle fr 

schapire boosting approach machine learning overview msri workshop nonlinear estimation classification :10.1.1.24.5565
shi malik normalized cuts image segmentation ieee trans :10.1.1.160.2324
pami vol 
aug 
thorpe speed processing human visual system nature vol 

treisman features objects visual processing scientific american november 
tu zhu image segmentation data driven markov chain monte carlo ieee trans :10.1.1.21.2979
pami vol 
pp 

tu zhu parsing images regions curves curve groups int journal computer vision review short version appeared proc 
eccv 
tu yuille 
shape matching recognition generative models informative features 
proceedings european conference computer vision 
eccv 
prague 

turk pentland eigenfaces recognition cognitive neurosciences vol pp 

ullman visual routines cognition vol pp 
ullman sequence seeking model bidirectional information flow cortex 
large scale neuronal theories brain 
eds 
koch davis 
mit press 
bradford book 

viola jones fast robust classification asymmetric adaboost detector cascade proc 
nips 
weber welling perona automatic discovery object categories proc 
cvpr 
wu mullin learning rare event detection cascade direct feature selection nips 
yedidia freeman weiss generalized belief propagation 
advances neural information processing systems pp 

zhu yuille region competition unifying snakes region growing bayes mdl multiband image segmentation ieee trans 
pami vol 

zhu zhang tu integrating top bottom object recognition datadriven markov chain monte carlo proc 
ieee conf 
computer vision pattern recognition hilton head sc 


