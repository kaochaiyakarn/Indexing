advances algorithms inference learning complex probability models vision brendan frey jojic university toronto technical report psi nov 
submitted ieee trans 
pami 
computer vision currently exciting areas artificial intelligence research largely possible record store process large amounts visual data 
impressive achievements pattern classification problems handwritten character recognition face detection exciting researchers may verge introducing computer vision systems perform scene analysis decomposing video constituent objects lighting conditions motion patterns 
main challenges computer vision finding efficient models physics visual scenes finding efficient algorithms inference learning models 
advocate graph generative probability models associated inference learning algorithms computer vision scene analysis 
review exact techniques various approximate computationally efficient techniques including iterative conditional modes expectation maximization algorithm mean field method variational techniques structured variational techniques gibbs sampling sum product algorithm loopy belief propagation 
describe technique applied illustrative example inference learning models multiple occluding objects compare performances techniques 
keywords graphical models bayesian networks probability models probabilistic inference reasoning learning bayesian methods variational techniques sum product algorithm loopy belief propagation em algorithm aristotle conjectured natural vision active process eyes connected invisible touch sensitive reach sense visual scene 
aristotle emphasize importance brain computational tool interpreting scene conjecture indicates early appreciation importance exploring understanding visual scene eliminate uncertainties environment effectively act 
eye ball ox rene descartes demonstrated th century eye contains dimensional retinal image dimensional scene theory existed computational mechanism understanding scene 
th century computational approach sorting plausible explanations data pioneered thomas bayes pierre simon laplace 
showed probability models data updated account new observations bayes rule 
techniques efficiently computing sums integrals particular calculus vastly sped computations fact computations carried hand restricted size models consideration 
mid th century controversy vision consequence lower nervous system optics eye empiricist consequence learned models created physical visual experiences 
hermann von helmholtz researchers define support empiricist view 
helmholtz established thesis vision involves psychological inferences higher nervous system learned models gained experience 
conjectured brain learns models scenes put explain visual input call generative models vision inference models 
went far conjecture individual carries physical experiments moving object front eyes order build better visual model object interactions objects environment 
computers th century enabled researchers formulate realistic models natural artificial vision perform experiments evaluate models 
particular bayes rule probabilistic inference probability models computationally feasible 
availability computational power motivated researchers tackle problem specify complex hierarchical probability models perform probabilistic inference learning models 
general types probability model generative probability models discriminative probability models 
discriminative model provides way compute distribution target class label input 
generative probability model accounts entire input image possibly additional hidden variables help explain input 
example model image foreground transparency background lighting orientation may explain input image composition foreground image background image transparency map foreground image depends orientation lighting foreground object transparency depends orientation foreground object 
discriminative models situations input preprocessed produce data fits statistical assumptions train model 
generative models potentially useful discriminative models 
accounting input data generative model help solve problem face detection solving related problem identifying foreground obstruction explain part face visible 
generative model probability model observed data video sequence event sample space 
means randomly sample probability model generate sample possible observed data 
contrast generative models discriminative models provide way generating training data 
generative model fit training data training data high probability 
goal find generative model best fit data 
easy defining model probability data 
goal find generative model fits data consistent prior knowledge 
example model video sequence construct set state variables time step require state time independent state time state time markov property 
purposes firstly advocate graph probability models computer vision secondly describe compare latest inference learning algorithms 
tutorial illustrative example model learns describe image explained composition foreground background images 
show various algorithms applied model compare performances sec 

graphical models reasoning uncertainty graphical models describe topology sense dependencies components complex probability model clarify assumptions representation lead algorithms topology achieve exponential speed ups 
constructing complex probability model faced challenges ensuring model reflects prior knowledge deriving efficient algorithms inference learning translating model different form communicating model researchers users 
graphical models graphical representations probability models offer way overcome challenges wide variety situations 
briefly addressing issues review kinds graphical model bayesian networks markov random fields factor graphs 
briefly review graphical models 
extensive treatment see 
prior knowledge usually includes strong beliefs existence hidden variables relationships variables system 
notion modularity central aspect graphical models 
example suppose constructing model motion fields foreground object background object video sequence 
particular frame motion vector associated small foreground patch related corresponding patch temporally proximal frames nearby motion vectors foreground 
contrast motion vector directly related patches motion vectors background directly related foreground motion vectors distant patches directly related patches motion vectors video frames temporally distant 
graphical model existence relationship depicted path connects variables 
probabilistic inference probability model principle carried bayes rule 
complex probability models accurately describe visual scene direct application bayes rule leads intractable number computations 
graphical models provide framework deriving efficient inference learning algorithms 
example suppose computed current estimates image patches motion vectors update motion vector small foreground patch 
graphical model indicates variables directly relevant case corresponding patch temporally proximal frames nearby motion vectors foreground 
examining variables update motion vector regard variables 
generally variables directly relevant updating particular variable form markov blanket determined graph 
complex probability model computational inference interpretation usually benefit judiciously groupings variables clusters take account dependencies variables 
types useful transformation include splitting variables eliminating integrating variables conditioning variables 
examining graph easily identify transformations steps lead simpler models models better suited goals particular choice inference algorithm 
example may able transform graphical model contains cycles tree exact efficient inference algorithm 
examining picture graph researcher user quickly identify dependency relationships variables system understand influence variable flows subset training images train model sec 

image created randomly selecting different background images different foreground objects yale face database combining layer image adding normal noise std 
dev 
dynamic range 
foreground object appears location image different foreground objects appear different places pixel background seen training images 
system change distributions variables 
block diagrams enable efficiently communicate computations signals flow system graphical models enable efficiently communicate dependencies components modular system 
illustrative example model occluding image patches probability models vision applications course extensive sample applications 
introduce model simple study review correctly accounts important effect vision occlusion 
fig 
illustrates training data goal model separate foreground objects background objects images 
important problem vision broad applicability 
example identifying pixels belong background possible improve performance foreground object classifier errors noise background avoided 
model explains input image pixel intensities composition foreground generative process explains image composition image foreground object image background transparency map mask 
foreground background mask selected stochastically library 
refer model patch model 
image background image images selected library possible images mixture model 
review refer model patch model generally model sub component larger model accounts image patches 
generative process illustrated fig 

foreground patch randomly selected library choosing class index distribution 
depending class foreground binary mask mk randomly chosen 
indicates pixel foreground pixel indicates pixel background pixel 
foreground class mask elements chosen independently jf 
class background jg randomly chosen 
intensity pixels patch selected independently mask class foreground class background jm 
joint distribution product distributions jf jm fact product factors broken noting class variable class variable write jm jf jb jf distribution ith pixel intensity class foreground model jb background model 
distributions account dependence pixel intensity mixture index independent observation noise 
joint distribution written jf jf jb note factorization reduces number arguments factors 
representational computational efficiency useful specify model parametric distributions 
parameterize jf jb assuming gaussian class 
foreground background models separate sets means variances assume share parameters ki ki mean variance ith pixel class particular mean patch may act foreground patch instance background patch instance 
desirable foreground background models separate sets means variances class variables constrained ng fn kg 

foreground means 

background means 
denote probability class probability foreground class fi probability fi jf fi fi parametric forms joint distribution fi fi fi fi bi bi normal density function mean variance remainder review patch model example describing graphical models inference algorithms learning algorithms 
model quite simple need advanced techniques complex useful shedding light advantages disadvantages type graphical model inference algorithm learning algorithm 
addition appeals generative models modularity simple model extended various ways apply complex situations 
example shown previously transformations added mixture models layered models scenes 
bayesian network patch model bayesian network variables directed acyclic graph set variables conditional probability function variable parents js bayesian network patch model index foreground patch index background patch binary mask variable specifies pixel foreground patch background patch 
simpler explicit bayesian network obtained grouping mask variables pixels 
markov random field mrf patch model 
mrf corresponding bayesian network 
factor graph patch model 
factor graph corresponding bayesian network 
set indices parents 
joint distribution product conditional probability functions js 
directed acyclic graph directed graph contain directed cycles 
fig 
shows bayesian network joint distribution pixels 
bayesian network don parents distributions conditioned variables grouping mask variables pixels obtain bayesian network shown fig 

graph simpler graph fig 
explicit conditional independencies 
markov random field patch model markov random field mrf variables undirected graph set variables potential function maximal clique set indices variables kth maximal clique 
joint distribution product potential functions divided normalizing constant called partition function 
clique fully connected subgraph maximal clique clique larger clique 
brevity refer maximum cliques cliques potentials maximal cliques usually called clique potentials 
factorization joint distribution similar factorization bayesian network conditional probability function viewed clique potential 
important difference conditional probability functions individually normalized respect child product conditional probabilities automatically normalized 
mrf patch model shown fig 
version mask variables grouped pixels grouped shown fig 

factor graph patch model factor graphs subsume bayesian networks mrfs :10.1.1.54.1570
bayesian network easily converted factor graph loss information 
mrf easily converted factor graph loss information 
exists models independence relationships expressed bayesian network mrf expressed factor graph 
belief propagation takes simple form factor graphs inference bayesian networks mrfs simplified single unified inference algorithm 
factor graph variables local functions ck bipartite graph set variables set nodes corresponding functions function node connected variables argument joint distribution product functions 
factor graph directed graph described ensures distribution normalized 
note local functions may positive potentials mrfs conditional probability functions bayesian networks 
fig 
shows factor graph patch model fig 
shows factor graph version mask variables grouped pixels grouped 
parameterized models bayesian learning far studied graphical models representations structured probability models computer vision 
turn general problem learn models training data 
purpose learning convenient express conditional distributions potentials graphical model parameterized functions 
choosing forms parameterized functions usually restricts model class computations easier 
example sec 
shows parameterize conditional probability functions patch model 
recall joint distribution fi fi fi fi bi bi parameters 
parameters variables frequently case model parameters known exactly prior knowledge experimental results provide evidence plausible values model parameters 
interpreting parameters random variables include conditional distributions potentials specify graphical model encode prior knowledge form distribution parameters 
including parameters variables path model obtain conditional distributions bj jf ji fi fi jf ji ji fi fi jb ji ji bi bi 
obtain simpler model specific independencies clustering mask variables pixels mask parameters pixel means variances 
resulting conditional distributions bj fi fi fi fi bi bi interpreting parameters random variables specify distribution 
generally distribution parameters quite complex simplifying assumptions sake computational 
assumed various parameter sets independent 
assume mixing proportions mask probabilities means variances independent 
bayesian network parameterized model shown fig 
joint distribution variables parameters bj 
introducing training data set training data infer plausible configurations model parameters 
imagine setting parameters produced training data 
see training data settings parameters matches training data 

parameter sets included bayesian network random variables 
training set cases parameters shared training cases 
training cases time series data video sequence may create parameter set time instance require parameters change slowly time 
best compute distribution parameters 
denote hidden variables visible variables hidden variables divided parameters denoted set hidden variables training cases 
similarly set visible variables training case 
assuming training cases independent identically drawn distribution visible variables hidden variables including parameters written jh expression called parameter prior jh called likelihood 
sections describe forms parameter prior lead computationally efficient inference learning algorithms 
patch model described bayesian network training cases shown fig 

training cases consist time series data video sequence parameters thought variables change slowly time 
fig 
shows model different set parameters training case assume parameters coupled time 
denote training case time distributions couple parameters time 
uncertainty distributions specifies quickly parameters change time 
uniform parameter priors parameter prior complex inference learning usually difficult 
possible derive efficient inference learning algorithms assume parameter prior uniform const 
case joint distribution parameters variables jh 
dependence parameters data determined solely likelihood tractable form 
uniform parameter prior justified amount training data large 
case prior tends little effect model exclude regions parameter space zero density prior 
logarithm distribution visible variables hidden variables log log log jh 
number training cases goes infinity term insignificant regions parameter space 
assume effect prior ignored 
justifies non zero prior particular uniform prior const 
training data limited uniform prior simplify inference learning 
assuming uniform prior parameters patch model joint distribution variables parameters note uniform priors parameter constraints taken account inference learning 
conjugate parameter priors conjugate prior form prior offers computational advantage uniform prior allows specification stronger prior knowledge 
idea choose prior form likelihood 
conjugate prior thought likelihood term associated fake user specified data 
result uniform prior joint distribution parameters variables likelihood 
suppose fake data 
setting jh 
combining prior likelihood obtain joint distribution parameters variables jh 
computationally inference learning model equivalent inference learning uniform prior extra fake data 
addition specifying fake training cases useful specify times occurs 
number times tth training case occurs 
real training case 
contribution tth case likelihood jh joint distribution jh values weights usually little influence computational efficiency inference learning provide control impact fake data 
fact set weight real number including fractional numbers 
patch model imagine seeing training data observe total examples patch class follows likelihood fake data parameter conjugate prior 
dirichlet distribution called dirichlet prior 
conjugate prior mean gaussian distribution gaussian distribution random variable mean appear symmetrically density function gaussian 
conjugate prior inverse variance gaussian distribution gamma distribution 
see imagine fake data consisting examples squared difference random variable mean likelihood fake data proportional setting prior proportional likelihood see conjugate prior gamma distribution mean variance 
algorithms inference learning generative model describing image rendering process specified vision consists inference generative model 
exact inference intractable turn approximate algorithms try find distributions close correct posterior distribution 
accomplished minimizing pseudo distances distributions called free energies 
interesting helmholtz researchers propose vision inference generative model nature seeks correct probability distributions physical systems minimizing called helmholtz free energy 
record helmholtz saw brain perform vision minimizing free energy help wonder 
parameterized model training data vision consists inferring parameters describe entire training set variables explain training case 
model shown fig 
training images vision consists inferring set model patches variance maps mixing proportions model patches set binary mask probabilities foreground class training case class foreground patch class background patch binary mask combine patches tutorial parameters variables considered random variables 
difference parameters variables parameters constant training cases data change slowly time time series data videos 
difference leads terminology refer inference model parameters machine learning just learning 
important treat parameters variables differently inference 
variable plays role single training case parameters shared training cases 
parameters impacted evidence variables pinned tightly data 
observation relevant study approximate inference techniques obtain point estimates parameters expectation maximization algorithm 
turn general problem inferring values unobserved hidden variables values observed visible variables 
denote hidden variables visible variables hidden variables usually divided parameters denoted set hidden variables training cases 
similarly set visible variables training case 
assuming training cases distribution hidden visible variables written jh patch model exact inference consists computing estimates making decisions posterior distribution hidden variables including parameters hjv 
bayes rule hjv notation include summing discrete hidden variables 
denominator serves normalize distribution various types inference various inference algorithms need function proportional posterior distribution 
cases suffices hjv note case graphical model equal product conditional distributions product potential functions divided partition function 
exact inference patch model known parameters model parameters known distribution foreground class background class mask variables proportional joint distribution fi fi fi fi bi bi take values binary mask variables total number configurations moderate model sizes compute posterior store posterior probability configuration 
bayesian network fig 
see independent markov blanket 
represent posterior distribution follows jf form posterior stored order numbers configuration order numbers probabilities jf giving total storage requirement order kj numbers 
computed follows fi fi fi fi bi bi fi fi fi fi bi bi value computed order multiply adds 
computed combinations result normalized give 
total number multiply adds needed compute order kj jf rewritten jf jf jf jf jm jf jm substituting definitions conditional distributions jf fi fi fi fi bi bi configuration computed normalized small number multiply adds 
total number multiply adds needed compute jf order kj combining technique model parameters exact posterior computed order kj multiply adds stored order kj numbers 
exact inference variables parameters patch model assuming uniform parameter prior joint distribution parameters variables patch model fig 

posterior distribution proportional joint distribution jz posterior thought large mixture model 
kt discrete configurations class variables mask variables configuration distribution real valued parameters 
mixture component class probabilities dirichlet distributed mask probabilities beta distributed 
pixel means variances coupled posterior variances means normally distributed means inverse variances gamma distributed 
quite simple example exact posterior intractable number posterior mixture components exponential number training cases posterior distribution pixel means variances coupled 
remainder describe variety approximate inference techniques discuss advantages disadvantages approach 
discussing approximations discuss practical ways interpreting results inference 
approximate inference minimizing helmholtz free energies usually techniques applied directly hjv distribution computed tractable manner 
turn various approximations 
approximate inference techniques viewed minimizing cost function called free energy measures accuracy approximate probability distribution 
include iterative conditional modes expectation maximization em algorithm variational techniques structured variational techniques gibbs sampling sum product algorithm loopy belief propagation :10.1.1.33.2557:10.1.1.54.1570
idea approximate true posterior distribution hjv simpler distribution making decisions computing estimates summarizing data approximate inference consists searching distribution closest hjv 
natural choice measure similarity distributions relative entropy kullback leibler divergence log hjv divergence distance symmetric general swapping give different value 
similar distance approximating distribution exactly matches true posterior hjv 
approximate inference techniques derived examining ways searching minimize 
fact directly computing usually intractable depends hjv 
tractable form hjv insert expression probably don need approximate inference 
fortunately modified way alter structure search space computations tractable 
subtract log obtain log log hjv log log hjv log notice log depend subtracting log influence search 
bayesian networks directed factor graphs tractable expression product conditional distributions 
called helmholtz free energy gibbs free energy just free energy 
interpret log energy function physical system distribution state system expression identical expression helmholtz gibbs free energy defined physics textbooks 
case minimizing free energy corresponds finding equilibrium distribution physical system boltzmann distribution 
way derive free energy jensen inequality bound log probability visible variables 
jensen inequality states concave function convex combination points vector space greater equal convex combination concave function applied points 
log probability visible variables log log 
introducing arbitrary distribution provides set convex weights obtain convex combination inside log function bounded log log log log see free energy upper bound negative log probability visible variables log 
seen noting 
free energy training cases training set training cases hidden variables visible variables jh 
free energy log log log log log jh decomposition sum term training case simplifies learning 
exact inference revisited idea approximate inference search space models simpler true posterior hjv 
instructive assume simplified derive minimizer 
constraint put normalized 
account constraint form lagrangian lagrange multiplier optimize log log 
setting derivative solving find hjv 
minimizing free energy simplifying assumptions produces exact inference 
minimum free energy hjv log hjv log 
minimum free energy equal negative log probability data 
minimum achieved hjv 
revisiting exact inference patch model patch model allow approximating distribution unconstrained find minimum free energy obtained jf 
course gained computationally distribution 
sections see various approximate forms lead tremendous speed ups 
point inference discrete variables continuous variables standard techniques search single configuration hidden variables 
particular researchers formulate problems searching configurations hidden variables minimize cost energy function 
discrete hidden variables understand procedure minimizing degenerate distribution 
iverson equality indicator function define 
case free energy simplifies log log 
minimizing corresponds searching values maximize 
global minimum equal global minimum log 
continuous hidden variables distribution point estimate dirac delta function centered estimate infinite spike density 
properties 
free energy reduces log log entropy dirac delta 
entropy depend minimizing corresponds searching values maximize 
unfortunately entropy dirac delta goes negative infinity 
optimal value results clearly equal global minimum log 
fact indication theoretical absurdity making point inferences continuous variables 
probability continuous variable value zero inferring single value probabilistic nonsense 
point inference continuous variables lead useful results 
review popular techniques point inferences iterative condition value interval 
entropy log goes 
modes expectation maximization algorithm 
iterative conditional modes icm main advantage technique usually easy implement 
main disadvantage take account uncertainties values hidden variables inferring values hidden variables 
causes icm find poor local minima 
algorithm works searching configuration maximizes hjv 
simplest version icm proceeds follows 
initialization 
pick values hidden variables randomly cleverly 
icm step 
select hidden variables holding variables constant set map value argmax jh argmax set hidden variables repeat fixed number iterations convergence 
hidden variables kept constant variables markov blanket relevant update 
denote variables markov blanket denote product conditional distributions potentials depend note product depend variables markov blanket update simplifies argmax kept constant 
discrete procedure straightforward 
continuous exact optimization possible current value initial point search algorithm newton method gradient method 
problem icm step choosing value information values discarded 
imagine case value different values icm pick value discarding information fact value essentially equally 
problem partly avoided optimizing entire subsets single elements step icm tractable subgraph graphical model selected variables subgraph updated maximize 
done efficiently viterbi algorithm generalized version 
free energy icm free energy described general point inferences 
icm mixture model means clustering probably famous example icm means clustering 
hidden variables cluster centers class label training case 
algorithm iterates assigning training case closest cluster center setting cluster center equal average training cases assigned 
icm patch model model parameters patch model known computational cost exact inference high 
number clusters large say examining configurations foreground class background class computationally burdensome 
icm patch model distribution entire training set ki ki ki ki ki ki substituting distribution distribution expression free energy obtain log log log log log log term entropy functions constant optimization 
intuitively cost measures mismatch input image image obtained combining foreground background patches mask 
order minimize free energy respect variables parameters iteratively solve variable keeping fixed convergence 
done order model parameters depend values hidden variables optimize hidden variables optimize model parameters 
furthermore observation class variables depend pixels updating hidden variables visit mask values pixels class variables 
parameters variables set random values updates proceed follows notation dropped convenience argmax argmax ji ji ji ji iverson notation true false 
expectation maximization algorithm discussed problem icm account uncertainty values hidden variables 
em algorithm accounts uncertainty variables performing updates variables 
particular parameters remaining variables em obtains point estimates computes exact posterior variables distribution recall data jh 
variables associated different training cases independent 
factorized form exact em restrictions placed distributions 
substituting expressions obtain free energy log log em iterative algorithm alternates minimizing respect set distributions step minimizing respect step 
updating distribution hidden variables training case constraint 
described earlier account constraint including lagrange multiplier 
setting derivative zero solving obtain solution jv 
derivative obtain log log parameters set equations 
solutions give em algorithm initialization 
choose values parameters randomly cleverly 
step 
minimize setting jv training case parameters data step 
minimize model parameters solving log log parameters system equations 
prior parameters assumed uniform const case term expression vanishes 
repeat fixed number iterations convergence 
sec 
showed hjv log 
em algorithm alternates obtaining tight lower bound log maximizing bound model parameters 
means iteration log probability data log increase stay 
em patch model icm approximate distribution parameters 
described step set training case 
independent distribution expressed jb 
distribution step minimize free energy model parameters step values probability tables jb determined minimize free energy subject normalization constraints jb jb leading updates variational distributions single training case jb fi fi fi fi fi fi fi bi bi exp jb fi fi log fi jb bi bi log bi computed normalize distribution 
compute distributions needed step jb jb 
derivatives free energy model parameters give parameter updates indexes training cases ki ki ki ki updates iterated variety ways 
example iteration may consist repeatedly updating variational distributions convergence updating parameters 
iteration may consist updating variational distribution updating parameters 
possibilities update order best avoiding local minima depends problem 
variety ways minimizing free energy leads generalization em 
generalized em derivation em algorithm obvious generalizations attempt decrease 
complex function parameters may possible exactly solve minimizes step 
modified decrease step downhill gradient 
contains parameters may optimized respect parameter holding constant 
doing solve system equations decrease 
generalization em arises posterior distribution hidden variables complex perform exact update jv minimizes step 
distribution previous step modified decrease 
fact icm special case em step decreased finding value minimizes subject 
variational techniques mean field method problem icm account uncertainty variables 
variable updated current guesses neighbors 
clearly neighbor untrustworthy count updating variable 
exact em applied exact posterior distribution subset variables 
exact em possible exact posterior intractable 
exact em account uncertainty parameters 
variational techniques assume comes family probability distributions parameterized 
substituting expression obtain variational free energy log note depends variational parameters 
inference proceeds minimizing respect variational parameters 
term variational refers process minimizing functional respect function 
notational simplicity refer parameterized distribution 
proximity minimum possible value log depend family distributions parameterized 
practice family usually chosen closed form expression obtained optimized 
starting point deriving variational techniques product form fully factorized mean field distribution 
consists hidden variables hm product form distribution variational parameter set variational parameters specifies marginal hidden variable advantage product form approximation readily seen described bayesian network 
suppose kth conditional probability function function variables conditional distributions may depend hidden variables case empty 
conditional distributions may depend visible variables case empty 
kth conditional probability function 
substituting obtain log hc log high dimensional integral hidden variables simplifies sum conditional probability functions low dimensional integrals small collections hidden variables 
term sum negative entropies distributions individual hidden variables 
scalar random variables bernoulli gaussian entropy written closed form quite easily 
second term sum expected log conditional distributions conditional distribution expectation taken respect product distributions hidden variables 
appropriate forms conditional distributions term written closed form 
example suppose jh exp log ah gaussian mean ah gaussian means variances entropy terms log log 
expected log conditional distribution log expressions functions variational parameters 
derivatives needed minimizing computed quite easily 
general variational inference consists searching value minimizes 
convex problems optimization easy 
usually convex iterative optimization required initialization 
pick values variational parameters randomly cleverly 
optimization step 
decrease adjusting parameter vector subset 
repeat fixed number iterations convergence 
variational technique accounts uncertainty hidden variables hidden model parameters 
variational techniques approximate distribution hidden variables step em algorithm point estimates model parameters 
variational em algorithms distribution note set variational parameters training case 
case generalized em steps initialization 
pick values variational parameters model parameters randomly cleverly 
generalized step 
starting variational parameters previous iteration modify decrease generalized step 
starting model parameters previous iteration modify decrease repeat fixed number iterations convergence 
variational inference learning patch model fully factorized distribution hidden variables single data sample patch model 
defining substituting distribution free energy single observed data sample patch model obtain log log log log log fi log fi fi fi log fi bi bi log bi setting derivatives zero obtain updates distributions step exp bi bi log bi exp nx log fi log fi fi fi log fi fi fi exp nx fi fi log fi bi bi log bi update distribution normalized 
updates computed order kj time fold speed exact inference exact em 
variational parameters computed observed images total free energy optimized respect model parameters obtain variational step ki ki qf ki ki updates similar updates exact em exact posterior distributions replaced factorized surrogates 
structured variational techniques product form mean field approximation describe joint probabilities hidden variables 
example posterior distinct modes variational technique product form approximation find mode 
different initialization technique may find mode exact form dependence revealed 
structured variational techniques distribution specified graphical model optimized 
fig 
shows original bayesian network patch model fig 
shows bayesian network fully factorized mean field distribution 
network gives variational inference learning technique described 
fig 
shows complex distribution leads variational technique described detail section 
previously saw exact posterior written jf 
follows distribution form jf capable representing posterior distribution exactly 
graph distribution shown fig 

generally increasing number dependencies distribution leads exact inference algorithms increases computational demands variational inference 
shown fully factorized approximation fig 
leads inference algorithm takes order kj time iteration 
contrast exact distribution fig 
takes order kj numbers represent clearly inference algorithm take order kj time 
starting bayesian network original patch model variational techniques ranging fully factorized approximation exact inference derived 
shows bayesian network factorized mean field distribution 
note inference observed included graphical model distribution 
shows network distribution infers dependence mask variables foreground class 
shows network distribution capable exact inference 
level structure increases computational demands inference turns approximation computationally efficient approximation accounts dependencies posterior 
increasing complexity distribution usually leads slower inference algorithms carefully choosing structure possible obtain accurate inference algorithms significant increase computation 
example shown structured variational distribution fig 
leads inference algorithm exact fully factorized mean field variational technique takes order time kj structured variational inference patch model distribution corresponding network fig 
jf 
defining fi jf fi fi substituting distribution free energy patch model obtain log log fi log fi fi fi log fi fi fi fi fi log fi fi bi bi log bi setting derivatives zero obtain updates distributions exp fi bi bi log bi exp fi log fi fi fi log fi fi fi fi fi log fi fi bi bi log bi fi fi fi exp fi fi log fi bi bi log bi care updates computed order kj time fold speed exact inference 
dependencies accounted dependence accounted fi 
parameter updates step similar form exact em exact posterior replaced structured distribution 
sum product algorithm belief propagation sum product algorithm belief propagation probability propagation performs approximate probabilistic inference generalized step passing messages edges graphical model 
message arriving variable probability distribution function proportional probability distribution represents inference variable part graph message came 
pearl showed algorithm exact graph tree 
graph contains cycles sum product algorithm loopy belief propagation generally exact diverge 
shown produce excellent results problems initial results look promising applications computer vision 
impressively shown give best known algorithms solving difficult instances problems known generally np hard including random satisfiability problems decoding error correcting codes phase unwrapping dimensional images :10.1.1.15.6659
graph contains cycles shown stable fixed points sum product algorithm correspond minima bethe free energy approximation free energy 
shown max product variant sum product algorithm converges configuration configurations differ perturbing variables subgraphs contain cycle lower posterior probabilities :10.1.1.54.1570
sum product algorithm thought variational technique 
recall contrast product form variational techniques structured variational techniques account direct dependencies edges original graphical model finding distributions disjoint substructures sub graphs 
problem structured variational techniques dependencies induced edges connect sub graphs accounted quite weakly variational parameters distributions sub graphs 
contrast sum product algorithm uses set sub graphs cover edges original graph accounts direct dependence approximately distributions 
sum product algorithm applied directed undirected models describe algorithm factor graphs subsume bayesian networks mrfs 
comes probabilistic inference factor graph observed variables deleted graph 
potential depends visible variables observed values variables thought constants potential function 
modified factor graph graphical model hidden variables factorization set variables jth local function 
mrf normalizing constant constant depend disregarded purpose probabilistic inference 
message sent edge factor graph function neighboring variable 
discrete variables messages stored vectors continuous variables parametric forms desirable discretization monte carlo approximations 
initially messages set uniform sum elements equals 
messages marginals updated follows 
sending messages variable nodes 
message sent product incoming message edges connected variable 
sending messages function nodes 
message sent obtained product incoming messages edges function summing variables appear outgoing message 
recall message function neighboring variable 
fusion rule 
compute current estimate posterior marginal distribution variable take product incoming messages normalize 
compute current estimate posterior marginal distribution variables local function take product local function messages arriving outside local function normalize 
repeat fixed number iterations convergence 
numerical stability idea normalize message sum elements equals 
graph tree messages flowed node node estimates posterior marginals exact 
graph edges exact inference accomplished propagating messages follows 
select node root arrange nodes layers beneath root 
propagate messages leaves root messages propagate messages root leaves messages 
procedure ensures messages flowed node node 
graph contains cycles messages passed iterative fashion fixed number iterations convergence detected divergence detected 
various schedules updating messages 
sum product algorithm belief propagation patch model patch model pixels assume model parameters known show compute approximations jz jz discussed exact inference requires examining possible combination takes order time 
contrast loopy belief propagation takes order time assuming number iterations needed convergence constant 
generally computational gain loopy belief propagation exponential number variables combine explain data 
pixels observed obtain factor graph shown fig 

pixels deleted graph pixel local function jm jf fi fi bi bi fi fi factor graph cycles belief propagation exact 
note mask variable jf included reduces number cycles may improve accuracy inference 
fig 
shows labeled messages edges factor graph 
message passing messages 
particular message leaving singly connected function node equal function 
messages leaving nodes corresponding equal shown fig 

message leaving singly connected variable node equal constant 
messages leaving mask variables 
initially messages set value 
updating messages graph specify order messages updated 

factor graph patch model pixels observations absorbed function nodes jm jf 
sum product algorithm belief propagation passes messages edge graph 
graph fragment shows different types messages propagated patch model 
choice influence quickly algorithm converges graphs cycles influence converges 
messages passed convergence fixed amount time 
define iteration consist passing messages 
iteration ensures variable propagates influence variable 
graph cycles procedure repeated 
recipe belief propagation see message sent updated follows 


note resulting message function summed 
substituting assuming normalized update simplified fi fi fi fi bi bi 
step computing message normalize 
message sent product incoming messages normalized 
message sent 

simplifies fi fi fi bi bi fi note terms large parentheses don depend need computed updating message 
proceeding message normalized 
message sent normalized 
message sent updated follows 


update simplifies fi fi fi bi bi fi normalization performed setting 
point message passing fusion rule estimate posterior marginal distribution unobserved variable 
resulting estimates jz jz jz jz compute iteration 
fact computing posterior marginals useful intermediate step efficiently computing messages 
example updated jz followed normalization 
pixels jz computed order time messages computed order time 
update previously computing messages takes order time 
gibbs sampling way approximate intractable distribution represent collection samples 
example need computing expectations function probability distribution expectation approximated average function value computed samples distribution 
sampling techniques numerous frequently due space constraints describe technique gibbs sampling 
overview sampling techniques see 
premise gibbs sampling posterior hidden variables hjv may tractable sampling conditional distributions individual variables jh set hidden variables tractable 
gibbs sampling works follows initialization 
pick values hidden variables randomly cleverly 
sampling step 
choose variable random order sample jh 
repeat fixed number iterations convergence 
counts number sampling steps nth configuration hidden variables guaranteed unbiased sample exact posterior hjv 
simulating single chain guaranteed minimize free energy step simulating ensemble sampling chains ensemble samples step suppose obtain new ensemble sampling chain 
jh 
substituting find contrast variational techniques described sum product algorithm gibbs sampling accounts uncertainty samples hidden variables 
updating variable gibbs sampling viewed variational distribution hk 
step computed minimize free energy distribution giving jh 
distribution represented sample 
context icm viewed technique picks maximize gibbs sampling draws distribution 
evident experiments discussed icm inferior mean field variational posterior captures uncertainty hidden variable focusing mode 
interesting experiment show order keep computational advantages icm technique avoids averaging different configurations hidden variable incorporate uncertainty posterior possible run grossly simplified version gibbs sampler single sample hidden variable re estimate model parameters 
opposed icm sample mode distribution just sample follows distribution described 
technique named iterative conditional samples ics computationally complexity icm shares steps icm sampling maximizing 
performs better icm suffer local minima problem 
gibbs sampling patch model patch model generalized em works randomly selecting parameters hidden variables iterating steps number steps gibbs sampling compute minimizes free energy sample set compute minimizes new free energy depends take sample set pixel mask variables obtain sample adjust model parameters minimize free energy log note parameter updates similar ones icm single configuration hidden variables replaced sample configurations 
gibbs sampler allowed burn find equilibrium 
corresponds discarding samples obtained early updating parameters 
annealing techniques searching local minima problem 
way try avoid local minima introduce inverse temperature log uniform inference easy 
free energy want minimize 
searching annealing system adjusting search may avoid local minima 
annealing best avoided clear adjust strategy adjusting requires extra time 
discussion inference learning algorithms explored algorithms learning parameters patch model described sec 
exact em variational em fully factorized posterior iterative conditional modes icm form gibbs sampling call iterative conditional samples ics sum product algorithm loopy belief propagation 
technique tweaked variety ways improve performance goal provide reader peek hood inference engine convey qualitative sense similarities differences techniques 
cases inference variable parameter initialized random number drawn uniformly range variable parameter 
training data described illustrated fig 

techniques tested best guaranteed converge local minimum free energy necessarily find global maximum log likelihood data upper bounded negative free energy 
typical local minimums free energy set clusters true classes data repeated merged blurry clusters 
avoid type local minimum clusters model total number different foreground background objects 
note clusters model tends overfit learn specific combinations foreground background 
learning algorithm applied training data starting different random initializations solution best total log likelihood kept 
part initialization pixels class comparison learned parameters model section various learning techniques 
techniques show prior mask mean variance class black indicates variance 
exact variational em show total posterior probability class modeling foreground background 
indicate approximate technique may accounting data high posterior probability 
note reason class index techniques correspond object row pictures different techniques don correspond 
means independently set random intensities pixels variances set mask prior pixel set 
classes allowed foreground background layers 
order avoid numerical problems model variances prior posterior probabilities discrete variables allowed drop learned parameters convergence shown fig 
computational costs speed convergence associated algorithms shown fig 

computational requirements varied orders magnitude techniques eventually managed find classes appearance 
technique icm failed find classes ability disambiguate foreground background classes indicated estimated mask priors see example fig 
separating foreground background classes model speeds training introduces local minima 
different parameterization model icm technique better 
example real valued mask binary mask icm technique estimating real valued mask making closer mean field technique described 
flops pixel free energy function computation time exact em variational em icm sum product algorithm 
total posterior probability class background foreground 
exact em part correctly infers classes foreground background 
error evident learned classes swapped model combination background foreground layers shown example training set fig 

particular combination total images dataset modeled class background class foreground 
consequence required classes 
class repeated version class class correctly foreground class examples 
redundancy class ended prior probability zero indicating model 
hand variational technique disambiguate foreground background classes evident computed total posterior probabilities class layer classes exact em inferred background classes variational technique learned masks priors allow cutting holes various places order place classes foreground show faces 
mask priors classes show outlines faces values zero indicating corresponding pixels consistently class picked foreground 
mask values reduce likelihood data increase variational free energy mask distribution jf fi fi highest value fi value 
variational free energy iteration point estimate negative log likelihood variational free energy iteration point estimate negative log likelihood variational free energy iteration free energy negative log likelihood iteration free energy negative log likelihood free energy approximations negative log likelihood 
compare variational free energy point estimate free energy negative log likelihood variational em 
compare approximations negative log likelihood exact em 
illustrate advantage modeling uncertainty posterior compare icm approximates factored piece posterior mode compare form gibbs sampling call iterative conditional samples ics mode picks random sample distribution 
somewhat negative likelihood data parameters see fig 

similar behavior evident results approximate learning techniques effectively decouple posterior foreground background classes loopy belief propagation column fig 
structured variational technique results shown conserve space 
concern raised minimizing free energy bounds negative loglikelihood approximation posterior weak fully factorized bound may lose useful optimization 
discussed earlier theory minimizing free energy tend select models approximation posterior exact 
see effect experimentally plots fig 

fig 
show free energy estimated variational method iterations learning 
case single iteration corresponds shortest sequence steps update variational parameters training case model parameters 
plot show negative true log likelihood computed model parameters iteration 
show point estimate free energy evaluated modes variational posterior 
parameters updated variational technique variational bound curves theoretically monotonic 
negative log likelihood consistently better estimates bound appear relatively tight time 
note early learning point estimate gives poor bound learning essentially finished point estimate gives bound 
fact icm performs poorly learning performs inference learning better technique indicates importance accounting uncertainty early learning process 
energies plotted parameters iteration exact em curves converge th iteration fig 

variational free energy plot computed factorized posterior jf fitted minimizing kl distance exact posterior point estimate computed discarding peaks variational posterior 
posterior broad due high variances early iterations variational posterior leads better approximation free energy point estimate 
point estimate catches quickly em algorithm converges true posterior peaked 
contrast parameters updated icm technique fig 
uses point estimates learning reestimate parameters iteration model parameters get close solution obtained exact variational em 
free energy stays substantially higher energy variational technique converges 
fact log likelihood data computed exact posterior parameters learned icm worse optimum 
plots meant illustrate fairly severe approximations posterior provide tight bound near local optimum log likelihood behavior learning algorithm early iterations determines close approximate technique get true local optimum likelihood 
early iterations give model chance get local optimum model parameters typically initialized model broad distributions allowing learning techniques explore broadly space possibilities relatively flat posteriors case initialize variances equal corresponding standard deviation dynamic range image 
approximate posterior greedy decisions early learning process difficult correct errors iterations 
icm technique fast greedy techniques 
model initialized high variances icm technique greedy decisions configuration hidden variables progress 
importantly computational efficiency necessarily demand extreme greediness 
illustrate fig 
show free energy icm technique modified take uncertainty account performing gibbs sampling step variable picking probable value 
increase computation cost 
doing may counterintuitive sampling suboptimal decision terms improving free energy resulting algorithm ends better values free energy 
log likelihood data considerably better 
sample free energy worse learning allows algorithm account uncertainty early learning distributions individual variables broad 
note single step gibbs sampling technique achieve low free energy exact em variational em 
effect approximate probabilistic inference progress learning algorithm deserves illustration 
fig 
show model parameters change iterations sum product algorithm learning technique 
illustrate inference hidden variables foreground class background class mask cases samples training set 
iteration finding guesses classes took part formation process foreground background incorrectly inverted posterior sample situation persists convergence 
applying additional iterations em learning inferred posterior leaves local minimum training sample rest training data indicated erasure holes estimated mask prior background classes 
improvement observed variational technique 
fact adding small number exact em iterations improve results variational learning seen part framework optimizing variational free energy parameters variational posterior form varied increase bound step 
nature local minima learning technique susceptible understood model parameters iteration mask prior mean appearance variance class posterior data samples em illustration learning loopy belief propagation sum product algorithm 
iteration show model parameters including mask priors mean variance parameters class inferred distribution mask foreground background class training images 
algorithm sec 
converges quickly escape local minimum caused decision iteration foreground object placed background layer illustrated training case 
additional iterations exact em sec 
uses exact posterior allows inference process flip foreground background needed escape local minimum 
possible change model form approximation posterior avoid minima extra computation 
patch model problem inversion avoided simply testing inversion hypothesis switching inferred background foreground classes check lowers free energy exploring possible combinations classes exact posterior 
elegant way doing variational framework add additional switch variable model generative process switch classes 
mean field posterior component models uncertainty foreground background inversion 
render variational learning times slower faster exact em 
directions view interesting potentially high impact areas current research include introducing effective representations models visual data inventing new inference learning algorithms efficiently infer combinatorial explanations visual scenes developing real time near realtime modular software systems enable researchers developers evaluate effectiveness combinations inference learning algorithms solving vision tasks advancing techniques combining information multiple sources including multiple cameras multiple spectral components multiple features modalities audio textual tactile information developing inference algorithms active vision effectively account uncertainties sensory inputs model scene making decisions investigating environment 
view core requirement directions research uncertainty properly accounted representations problems adapting new data 
large scale hierarchical probability models efficient inference learning algorithms play large role successful implementation systems 
adelson anandan 
ordinal characteristics transparency 
proceedings aaai workshop qualitative vision 
besag 
statistical analysis dirty pictures 
journal royal statistical society 
black fleet 
probabilistic detection tracking motion discontinuities 
international journal computer vision 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
proceedings royal statistical society 
editor 
hermann von helmholtz 
university california press los angeles ca 
frey 
factor graphs unification directed undirected graphical models 
university toronto technical report psi 
frey jojic 
transformation invariant clustering em algorithm 
ieee transactions pattern analysis machine intelligence 
frey koetter petrovic 
loopy belief propagation unwrapping phase images 
conference advances neural information processing systems volume 
mit press 
frey kschischang 
probability propagation iterative decoding 
proceedings allerton conference communication control computing 
frey mackay 
revolution belief propagation graphs cycles 
jordan kearns solla editors advances neural information processing systems volume pages 
mit press 

schrodinger search reality solving quantum mysteries 
little brown 
heskes 
stable fixed points loopy belief propagation minima bethe free 
becker thrun obermayer editors advances neural information processing systems 
mit press cambridge ma 
hinton sejnowski 
learning relearning boltzmann machines 
rumelhart mcclelland editors parallel distributed processing explorations microstructure cognition volume pages 
mit press cambridge ma 
jepson black 
mixture models optical flow computation 
proceedings ieee conference computer vision pattern recognition pages june 
jojic frey 
learning flexible video layers 
proceedings ieee conference computer vision pattern recognition 
jordan ghahramani jaakkola saul 
variational methods graphical models 
jordan editor learning graphical models 
kluwer academic publishers norwell ma 
kschischang frey 
loeliger 
factor graphs sum product algorithm 
ieee transactions information theory special issue codes graphs iterative algorithms february 
lauritzen 
graphical models 
oxford university press new york ny 
mackay neal 
near shannon limit performance low density parity check codes 
electronics letters august 
reprinted electronics letters vol 
march 
parisi 
analytic algorithmic solution random satisfiability problems 
science 
neal 
probabilistic inference markov chain monte carlo methods 
university toronto technical report 
neal hinton 
view em algorithm justifies incremental sparse variants 
jordan editor learning graphical models pages 
kluwer academic publishers norwell ma 
pearl 
probabilistic reasoning intelligent systems 
morgan kaufmann san mateo ca 
weiss 
smoothness layers motion segmentation nonparametric mixture estimation 
proceedings ieee conference computer vision pattern recognition pages june 
weiss freeman :10.1.1.54.1570
solutions max product belief propagation algorithm graphs 
ieee transactions information theory special issue codes graphs iterative algorithms february 
yedidia freeman weiss 
generalized belief propagation 
advances neural information processing systems 
mit press cambridge ma 

