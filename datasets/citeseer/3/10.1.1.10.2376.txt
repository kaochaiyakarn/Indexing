beehive exploiting power law query distributions lookup performance peer peer overlays structured peer peer hash tables provide decentralization self organization failure resilience worst case lookup performance applications suffer high la emin sirer average case 
high latencies pro relevant demanding applications dns 
proactive replication framework achieve lookup performance common zipf query distributions 
framework closed form solution achieves lookup performance low storage requirements bandwidth overhead network load 
simulations show replication framework realistically achieve latencies outperform passive caching adapt efficiently sudden changes object popularity known flash crowds 
framework provides feasible substrate high performance low latency applications peer peer domain name service 
peer peer distributed hash tables dhts emerged building block distributed applications 
unstructured dhts freenet gnutella network offer decentralization simplicity system construction may take hops perform lookups networks nodes :10.1.1.10.4919
structured dhts chord pastry tapestry particularly wellsuited large scale distributed applications self organizing resilient denial service attacks provide log lookup performance worst average case :10.1.1.17.4065:10.1.1.142.752:10.1.1.105.3673:10.1.1.10.1904
large scale high performance latency sensitive applications domain name service dns world wide web logarithmic performance bound translates high latencies 
previous serving dns peer peer lookup service concluded high average case lookup costs render current structured dhts unsuitable latency sensitive applications dns 
describe proactive replication achieve lookup performance efficiently top standard log peer peer distributed hash table certain commonly encountered query distributions 
wellknown query distributions popular applications including dns web follow power law distribution 
characterized query distribution presents opportunity optimize system expected query stream 
critical insight query distributions power law proactive replication enable dht system achieve small constant lookup latency average 
contrast show common techniques passive demand driven replication caching objects lookup path fail significant impact average case behavior system 
outline design replication framework called beehive goals high performance enable average case lookup performance effectively decoupling performance peer peer dht systems size network 
provide log worst case lookup performance 
high scalability minimize background traffic network reduce aggregate network load node bandwidth consumption 
ensure amount memory disk space required peer network kept minimum 
high adaptivity promptly adjust performance system response changes aggregate popularity distribution objects 
cheaply track maintain popularity individual objects system quickly respond certain object highly popular flash crowds slashdot effect beehive achieves goals efficient proactive replication 
proactive replication mean actively propagating copies objects nodes participating network 
fundamental tradeoff replication resource consumption copies object generally improve lookup performance cost space bandwidth aggregate network load 
limit proactively copying objects dht nodes enable query satisfied constant time 
scale large systems require prohibitive amounts space node network overloaded replica creation changes mutable objects require updates 
contrast beehive performs tradeoff analytical model provides closedform optimal solution achieves lookup performance power law query distributions minimizing number object copies reducing storage bandwidth load network 
beehive relies cheap local measurements efficient lease protocols replica coordination 
node beehive continually performs local measurements determine relative popularity objects system estimate global properties aggregate query distribution function 
beehive nodes decide replicas object propagated combining closed form solutions analytical model measurements aggregate query distribution function estimates object rank 
estimation performed independently periodically node replica management protocol efficiently propagates removes cached objects excessive messaging global synchronization agreement 
objects beehive may modified dynamically 
general mutable objects pose cache coherency problems replication technique older date copies object may remain cached system keep clients accessing versions 
provide date views presence updates system needs track replicas object invalidate old copies propagate changes object modified 
beehive structured nature underlying dht allows system keep track placement replicas single integer 
enables beehive efficiently find update replicas object modified 
consequently objects may updated time beehive lookups performed update completed return latest copy object 
describes beehive proactive replication framework general form domain name system target application perform evaluation dns data demonstrate serving dns lookups peerto peer distributed hash table feasible 
shortcomings current hierarchical structure dns ideal application candidate beehive 
dns highly poses significant challenge serve efficiently 
second hierarchical organization dns leads disproportionate amount load placed higher levels hierarchy 
third higher nodes dns hierarchy serve easy targets distributed denial service attacks form security vulnerability entire system 
nameservers required internal leaves dns hierarchy incur expensive administrative costs need manually administered secure constantly online 
peer topeer dhts address critical problem show beehive replication strategy address 
implemented prototype beehive dns server layered top pastry peer peer hash table :10.1.1.142.752
prototype implementation compatible current client resolver libraries deployed internet 
envision dns nameservers currently serve small dedicated portions naming hierarchy form beehive network collectively manage entire namespace 
implementation supports existing naming scheme falling back legacy dns beehive dns lookups fail 
legacy dns relies cache timeouts loose coherency incurs ongoing cache expiration refill overheads beehive dns enables resource records updated time 
dns guiding application evaluating system note full treatment implementation alternative peer peer dns system scope focus general purpose beehive framework proactive replication 
framework sufficiently general achieve lookup performance settings including web caching aggregate query distribution follows power law similar dns 
describes design replication framework enables lookup performance structured dhts common query distributions applies dns implementation contributions 
proposes proactive replication objects provides closed form analytical solution number replicas needed achieve constant time lookup performance low costs 
storage bandwidth load placed network scheme modest 
contrast show simple caching strategies passive replication incur large ongoing costs 
second outlines design complete system analytical model 
system layered top pastry existing peer peer substrate 
includes techniques estimating requisite inputs analytical model mechanisms replica propagation deletion strategy mapping continuous solution analytical model discrete implementation pastry 
presents results prototype implementation peer peer dns service show system achieves performance low overhead adapt quickly flash crowds 
turn approaches enable benefits systems self organization resilience denial service attacks applied latency sensitive applications dns 
rest organized follows 
section provides broad overview approach describes storage bandwidth efficient replication components beehive detail 
section describes implementation beehive pastry 
section presents results expected benefits beehive serve dns queries 
section surveys different dht systems summarizes approaches caching replication peer peer systems section describes section summarizes contributions 
beehive system beehive general replication framework applied structured dhts prefix routing chord pastry tapestry kademlia :10.1.1.38.1850
dhts operate manner 
node unique randomly assigned identifier circular identifier space 
object unique randomly selected identifier assigned space stored closest node called home node 
routing performed successively matching prefix object identifier node identifiers 
generally step query processing takes query node matching prefix previous node 
query traveling hops reaches node prefixes matching strictly speaking nodes encountered query routing process sparsely populated dht may share progressively pre lookup home node nodes illustrates levels replication beehive 
query object takes hops node node home node object 
replicating object level query latency reduced hops 
general object replicated level incurs hops lookup 
search space reduced exponentially query routing approach provides lookup performance average number nodes dht base fanout system 
central observation beehive length average query path reduced hop object proactively replicated nodes logically preceding node query paths 
apply iteratively disseminate objects widely system 
replicating object nodes hops lesser home node reduce lookup latency hops 
beehive replication mechanism general extension observation find appropriate amount replication object popularity 
beehive strives create minimal number replicas expected number nodes traversed query match targeted constant uses analytical model derive number replicas required achieve lookup performance minimizing node storage bandwidth requirements network load 
note model driven estimates object popularity real implementation describe may deviate optimal due sampling errors 
beehive controls extent replication system assigning replication level object 
object level replicated nodes matching prefixes object 
queries objects replicated level incur lookup latency hops 
objects stored home nodes level objects replicated level cached nodes system 
illustrates fixes object remain numerically close 
detail significantly impact time complexity standard dht replication algorithm 
section discusses issue detail 
concept replication levels 
goal beehive replication strategy find minimal replication level object average lookup performance system constant number hops 
naturally optimal strategy involves replicating popular objects lower levels nodes popular objects higher levels 
judiciously choosing replication level object achieve constant lookup time minimal storage bandwidth overhead 
beehive employs mechanisms protocols find maintain appropriate levels replication objects 
analytical model provides beehive closed form optimal solutions indicating appropriate levels replication object 
second monitoring protocol local measurements limited aggregation estimates relative object popularity global properties query distribution 
estimates independently distributed fashion inputs analytical model yields locally desired level replication object 
replication protocol proactively copies desired objects network 
rest section describes components detail 
analytical model section provide model analyzes zipf query distributions provides closed form optimal replication levels objects order achieve constant average lookup performance low storage bandwidth overhead 
zipf power law query distributions number queries popular object proportional parameter distribution 
query distribution heavier tail smaller values parameter zipf distribution parameter corresponds uniform distribution 
total number queries popular objects approximately estimate number queries received objects pose optimization problem minimize total number replicas constraint average lookup latency constant base underlying dht system num ber objects number nodes system 
ini tially objects system stored home nodes replicated level denote fraction objects replicated level lower 
definition objects replicated level popular objects replicated nodes system 
object replicated level cached nodes 
objects replicated nodes exactly matching prefixes 
average number objects replicated node simplifying expression average node storage requirement replication fraction queries arrive popular objects approximately number objects replicated level number queries travel hops average lookup latency entire system constraint aver age latency required constant lookup performance 
ing approximation simplifying arrive optimization problem 
minimize note second constraint effectively reduces optimal solution problem just con straint satisfy lagrange multiplier technique find analytical closed form optimal solution problem just constraint defines convex feasible space 
resulting solution may guarantee second constraint obtained solution violates second constraint force apply lagrange mul technique modified problem 
obtain optimal solution repeating process iteratively second constraint satisfied 
symmetric property constraint facilitates easier analytical approach solve optimization problem iterations 
assume optimal solution problem restate optimization problem follows minimize lagrange multiplier technique solve optimization problem get closed form solution derive value satisfying condition 
objects 
applying example consider dht base nodes ical method achieve average lookup time hop obtain solution popular objects replicated level popular objects replicated level remaining objects level average node storage requirement system objects 
optimal solution obtained model applies case closed form solution yield level replication achieve target lookup performance amount replication may optimal feasible space longer convex 
obtain optimal solution approximation applying technique 
optimal solution case follows analytical solution properties useful guiding extent proactive replication 
analytical model provides solution achieve desired constant lookup performance 
system tailored amount replication controlled level performance adjusting continuous range 
structured dhts preferentially keep physically nearby hosts toplevel routing tables consequently pay highest hop latency costs get closer home node selecting large target value dramatically speed query latencies 
second large class query distributions solution provided model achieves optimal number object replicas required provide desired performance 
minimizing number replicas reduces node storage requirements bandwidth consumption aggregate load network 
serves upper bound worst case lookup time successful query objects replicated level assumptions analytical model objects incur similar costs replication objects change frequently 
applications dns essentially homogeneous object sizes traffic small fraction replication overhead analytical model provides efficient solution 
applying beehive approach applications web wide range object sizes frequent object updates may require extension model incorporate size update frequency information object 
popularity zipf parameter estimation analytical model described previous section requires knowledge parameter query distribution relative objects 
order obtain accurate estimates popularity objects parameter query distribution beehive needs efficient mechanisms continuously monitor access frequency objects 
beehive employs combination local measurement limited aggregation keep track changing parameters adapt replication appropriately 
node locally measures number queries received object replicated node order estimate relative popularity 
objects replicated home nodes queries object routed home node local measurement access frequency sufficient estimate relative popularity 
object replicated level queries object distributed approximately nodes base dht nodes 
order estimate relative popularity accu racy need fold increase measurement 
prevents system reacting quickly changes popularity objects 
beehive performs limited aggregation order alleviate problem improve responsiveness system 
aggregation beehive takes place periodically aggregation interval 
node sends node level routing table aggregation message containing access frequency object replicated level lower having matching prefixes node receives aggregation messages nodes level shares prefixes 
aggregates estimates access frequencies received nodes local estimate sends aggregated access frequency nodes level routing table round aggregation rounds aggregation home node object replicated level obtains accurate estimate access frequency 
beehive node responsible replicating object level lower 
nodes level responsible replicating object level nodes level need get aggregated access frequencies objects replicated level home nodes 
enable reverse information flow sending aggregated access frequencies response aggregation messages 
home node object sends latest aggregated estimate access frequency response aggregation message node node receives aggregation message sends reply containing aggregated access frequency objects listed aggregation message 
manner access frequency object aggregated home node aggregated estimate disseminated nodes containing replica object 
object replicated level takes rounds aggregation complete information flow 
addition popularity objects analytical model needs estimate parameter query distribution 
zipf parameter estimated local measurement limited aggregation 
node locally com aggregated access frequency different objects replicated node 
estimate linear sion techniques compute slope best fit line zipf popularity distribution straight line log scale 
local estimate small subset objects system estimate refined aggregating local estimates nodes communicates aggregation 
fluctuations estimation access frequency zipf parameter due randomness query distribution 
order avoid large discontinuous changes estimates age follows replication protocol beehive requires protocol replicate objects levels computed analytical model 
order deployable wide area networks replication protocol asynchronous require expensive mechanisms distributed consensus agreement 
section develop efficient protocol enables beehive replicate objects dht 
beehive replication protocol uses asynchronous distributed algorithm implement optimal solution provided analytical model 
node responsible replicating object nodes hop away nodes share prefix current node 
initially object replicated home node level number nodes system base dht shares prefixes object 
object needs replicated level home node pushes object nodes share prefix home node 
level nodes object currently replicated may independently decide replicate object push object nodes share prefix 
nodes continue process independent distributed replication objects replicated appropriate levels 
algorithm nodes share prefixes object responsible replicating object level called level deciding nodes object 
object replicated level node level deciding node node routing table level matching prefixes object 
objects deciding node may node 
distributed replication algorithm illustrated 
initially object identifier replicated home node level shares prefixes 
analytical model indicates object replicated level node pushes objects nodes shares prefixes 
node level deciding node object nodes popularity object level nodes may independently decide replicate object level node decides pushes copy object nodes shares prefix level deciding node object nodes similarly node may replicate object level pushing copy nodes home node level level level illustrates object home node replicated level 
nodes numbers indicate prefixes match object identifier different levels 
node pushes object independently nodes matching digit 
node replication algorithm require agreement estimation relative popularity nodes 
consequently objects may replicated partially due small variations estimate relative popularity 
exam ple node decide push object level tolerate inaccuracy keep replication protocol efficient practical 
evaluation section show inaccuracy replication protocol produce noticeable difference performance 
beehive implements distributed replication algorithm phases analysis phase replicate phase follow aggregation phase 
analysis phase node uses analytical model latest known estimate zipf parameter obtain new solution 
node locally changes replication levels objects solution 
solution specifies level fraction objects need replicated level lower 
fraction objects replicated level lower replicated level lower 
current popularity node sorts objects level lower level deciding node 
chooses popular fraction objects locally changes replication level chosen objects current repli cation level node changes replication level objects chosen current replication level lower 
analysis phase replication level objects increase decrease popularity objects changes time 
replication level object decreases level needs replicated nodes share prefix 
replication level object increases level nodes matching prefixes need delete replica 
replicate phase responsible enforcing correct extent replication object determined analysis phase 
replicate phase node sends node level routing table replication message listing identifiers objects level deciding node 
receives message checks list identifiers pushes node object current level replication lower 
addition sends back identifiers objects longer replicated level receiving message removes listed objects 
beehive nodes invoke analysis replicate phases periodically 
analysis phase invoked analysis interval replicate phase replication interval 
order improve efficiency replication protocol reduce load network integrate replication phase aggregation protocol 
perform integration setting durations replication interval aggregation interval combining replication aggregation messages follows node sends aggregation message message implicitly contains list objects replicated level deciding node similarly node replies replication message adds aggregated access frequency information objects listed replication message 
analysis phase estimates relative popularity objects estimates access frequency obtained aggregation protocol 
recall object replicated level takes rounds tion obtain accurate estimate access frequency 
order allow time information flow aggregation set replication interval times aggregation interval 
random variations query distribution lead fluctuations relative popularity estimates objects may cause frequent changes replication levels objects 
behavior may increase object transfer activity impose substantial load network 
increasing duration aggregation interval efficient solution decreases responsiveness system changes 
beehive limits impact fluctuations employing hysteresis 
analysis phase node sorts objects level popularity access frequencies objects replicated level increased small fraction 
biases system maintaining existing replicas popularity difference objects small 
replication protocol enables beehive maintain appropriate replication levels objects new nodes join leave system 
new node joins system obtains replicas objects needs store initiating replicate phase replication protocol 
new node objects replicated previously part system objects need fetched deciding nodes 
node leaving system directly affect beehive 
leaving node deciding node objects underlying dht chooses new deciding node objects repairs routing table 
mutable objects beehive directly supports mutable objects proactively disseminating object updates replicas system 
semantics read update operations objects important issue consider supporting object mutability 
strong consistency semantics require object updated subsequent queries object return modified object 
achieving strong consistency challenging distributed system replicated objects copy replicated object updated invalidated object modification 
beehive exploit structure underlying dht efficiently disseminate object updates nodes carrying replicas object 
scheme guarantees object modified replicas consistently updated short time system stable nodes joining leaving system 
beehive associates bit version number object identify modified objects 
object replica higher version number replica lower version number 
owner object system modify object inserting fresh copy object higher version number home node 
home node proactively multicasts update replicas objects routing table 
object replicated level home node sends copy updated object node level routing table 
node propagates update node level routing table 
update propagation protocol ensures node sharing prefixes object obtain copy modified object 
object update reaches node exactly path query issued object home node node identifier follow 
property nodes replica object get exactly copy modified object 
scheme efficient provides guaranteed cache coherency absence nodes leaving system 
nodes leaving system may cause temporary inconsistencies routing table 
consequently updates may reach nodes objects replicated 
similarly nodes joining system having older versions object replicated need update copy objects 
modify beehive replication protocol slightly disseminate updates nodes older versions due churn system 
replicate phase node includes version number addition object identifiers listed replication message 
receiving message deciding node object pushes copy object version object 
implementation beehive general replication mechanism applied prefix distributed hash table 
layered implementation top pastry freely available dht log lookup performance 
implementation structured transparent layer top freepastry supports traditional insert modify delete query dht interface applications required modifications underlying pastry 
converting preceding discussion concrete imple mentation beehive framework building dns application top combining framework pastry required practical considerations identified optimization opportunities 
beehive needs maintain additional modest amount state order track replication level freshness popularity objects 
beehive node stores replicated objects object repository 
beehive associates meta information object system beehive node maintains fields object repository object id bit field uniquely identifies object helps resolve queries 
object identifier derived hash key time insertion just pastry 
version id bit version number differentiates fresh copies object older copies cached network 
home node single bit specifies current node home node object 
replication level small integer specifies current local replication level object 
access frequency small integer monitors number queries reached node 
incremented locally observed query reset aggregation 
aggregate popularity small integer gation phase collect sum access frequencies dependent nodes node deciding node 
maintain older aggregate popularity count aging 
addition state associated object beehive nodes maintain running estimate zipf parameter 
updates estimate batched occur relatively infrequently compared query stream 
storage cost consists bytes object processing cost keeping meta data date small 
pastry query routing deviates model described earlier entirely prefix uniform 
pastry maps object numerically closest node identifier space possible object share prefixes home node 
example network nodes pastry store object identifier query ob ject home node pastry completes query aid auxiliary data structure called leaf set 
leaf set hops directly locate numerically closest node queried object 
pastry initially routes query entries routing table may route couple hops leaf set entries 
required modify beehive replication protocol replicate objects leaf set nodes follows 
leaf set node propagated prefix matching reach hop replicate objects leaf set nodes high highest replication est replication levels 
level beehive default replication level object replicated home node 
part maintain phase node sends maintenance message nodes routing table leaf set list identifiers objects replicated level deciding node deciding node object homed node forward query object node 
receiving maintenance message level node push object node node object matching prefixes 
object replicated leaf set node level replication lower levels follow replication protocol described section 
slight modification beehive enables top pastry 
routing metrics dht substrates xor metric proposed exhibit non uniformity beehive implementation simpler 
pastry implementation provides opportunities optimization improve beehive impact reduce overhead 
pastry nodes preferentially populate routing tables nodes physical proximity 
instance node identifier opportunity pick nodes routing digit 
pastry selects node lowest network latency measured packet round trip time 
prefixes get longer node density drops node progressively freedom find choose nearby nodes 
means significant fraction lookup latency experienced pastry lookup incurred hop 
means selecting large number constant hops beehive performance target significant effect real performance system 
pick implementation note continuous variable may set fractional value get average lookup performance fraction hop 
yields solution replicate objects hops suitable total hash table size small 
second optimization opportunity stems maintenance messages beehive pastry 
beehive requires inter node communication replica dissemination data aggregation 
communication confined pairs nodes member pair appears member routing table 
highly stylized communication pattern suggests possible optimization 
pastry nodes periodically send heart beat messages nodes routing table leaf set detect node failures 
perform periodic network latency measurements nodes routing table order obtain closer routing table entries 
improve beehive efficiency combining periodic heart beat messages sent pastry periodic maintenance messages sent beehive 
piggy backing row routing table entries beehive maintenance message level replication single message simultaneously serve heart beat message pastry maintenance message beehive maintenance message 
built prototype dns name server top bee hive order evaluate caching strategy proposed 
beehive dns uses beehive framework proactively disseminate dns resource records containing name ip address bindings 
beehive dns server currently supports udp name queries compatible resolver libraries designed provide migration path legacy dns 
queries satisfied beehive system looked legacy dns home node inserted beehive framework 
beehive system stores disseminates resource records appropriate replication levels monitoring dns query stream 
clients free route queries node part beehive dns 
dns system relies entirely aggressive caching order scale provides loose coherency semantics limits rate updates performed 
recall beehive system enables resource records modified time disseminates new resource records caching name servers part update operation 
process initiated name owners directly notify home node changes name ip address binding 
expect time come beehive adjunct system layered top legacy dns name owners part beehive know contact system 
reason current implementation delineates names exist solely beehive versus resource records originally inserted legacy dns 
current implementation home node checks validity legacy dns entry issuing dns query domain time live field entry expired 
dns mapping changed home node detects update propagates usual 
note strategy preserves dns semantics quite efficient home nodes check validity entry replicas retain mappings invalidated 
beehive implementation adds modest amount overhead complexity peer peer distributed hash tables 
prototype implementation beehive dns lines code compared lines code pastry 
evaluation section evaluate performance costs benefits beehive replication framework 
examine beehive performance context dns system show beehive robustly efficiently achieve targeted lookup performance 
show beehive adapt sudden drastic changes popularity objects global shifts parameter query distribution continue provide lookup performance 
compare performance beehive pure pastry pastry enhanced passive caching 
passive caching mean caching objects nodes query path similar scheme proposed 
impose restrictions size cache passive caching 
follow dns cache model handle mutable objects associate time live object 
objects removed cache expiration time live 
setup evaluate beehive simulations driven dns survey trace data 
simulations performed source code implementation 
simulation run started seeding network just single copy object querying objects dns trace 
compared proactive replication beehive passive caching pastry pc pastry regular pastry 
passive caching relies expiration times coherency beehive pastry need perform extra presence updates conducted large scale survey determine distribution ttl values dns resource records compute rate change dns entries 
survey spanned july september periodically queried web servers resource records unique domain names collected crawling yahoo 
dmoz org web directories 
distribution returned time live values determine lifetimes resource records simulation 
measured rate change dns entries repeating dns survey periodically derived object lifetime distribution 
dns trace collected mit december 
trace spans lookups days featuring distinct clients distinct fully qualified names 
order reduce memory consumption simulations scale number distant objects issue queries rate queries sec 
rate issue requests little impact hit rate achieved beehive dominated performance analytical model parameter estimation rate updates 
query distribution trace follows approximate zipf distribution parameter separately evaluate beehive robustness face changes parameter 
performed evaluations running beehive implementation pastry simulator mode nodes 
pastry set base leaf set size length identifiers recommended 
evaluations beehive maintenance interval minutes replication interval minutes 
replication phases node randomly staggered approximate behavior independent non synchronized hosts 
set target lookup performance beehive average hop 
beehive performance shows average lookup latency pastry pc pastry beehive query period spanning hours 
plot lookup latency moving average minutes 
average lookup latency pure pastry hops 
average lookup latency pc pastry drops steeply hours 
average lookup performance beehive decreases steadily hours averages latency hops pastry pc pastry beehive time hours latency hops vs time 
average lookup performance beehive converges targeted hop replication phases 
object transfers pc pastry beehive time hours object transfers cumulative vs time 
total amount object transfers imposed beehive significantly lower compared caching 
passive caching incurs large costs order check freshness entries presence conservative timeouts 
converges hops target lookup performance 
beehive achieves target performance hours minutes time required replication phases followed maintain phase node 
phases combined enable beehive propagate popular objects respective replication levels 
level objects disseminated beehive proactive replication achieves expected payoff 
contrast pc pastry provides limited benefits despite infinite sized cache 
reasons relative ineffectiveness passive caching 
heavy tail zipf distributions implies objects requests queries take disjoint paths network collide node object cached 
second pc pastry relies time live values cache coherency tracking location cached objects 
time live values need set conservatively order reflect worst case scenario record may updated opposed expected lifetime object 
consequently passive caching suffers low hit rate entries evicted due small values ttl set name owners 
examine bandwidth consumed network load incurred pc pastry beehive caching objects show beehive generates significantly lower background traffic due object transfers compared passive caching 
shows total amount objects transferred beehive pc pastry experiment 
pc pastry rate object transfer proportional lookup latency transfers object node query path 
beehive incurs high rate object transfer initial period beehive achieves target lookup performance incurs considerably lower overhead needs perform transfers response changes object popularity relatively infrequently dns object updates 
beehive continues perform limited amounts object replication due fluctuations popularity objects estimation errors hysteresis 
avg objects node storage latency target lookup performance hops storage requirement vs latency 
graph shows average node storage required beehive estimated latency different target lookup performance 
graph captures trade overhead incurred beehive lookup performance achieved 
average number objects stored node hours beehive passive caching 
pc pastry caches objects beehive lookup performance worse due heavy tailed nature zipf distributions 
evaluation shows beehive provides hop average lookup latency low storage bandwidth overhead 
estimated latency ms beehive efficiently trades storage bandwidth improved lookup latency 
replication framework enables administrators tune trade varying target lookup performance system 
shows trade storage requirement estimated latency different target lookup performance 
analytical model described section estimate storage requirements 
estimated expected lookup latency round trip time obtained pinging pairs nodes planetlab adding ms accessing local dns resolver 
average hop round trip time nodes planetlab ms large scale dns survey average dns lookup latency ms beehive target performance hop provide better lookup latency dns 
latency hops pastry pc pastry beehive time hours latency hops vs time 
graph shows beehive quickly adapts changes popularity objects brings average lookup performance hop 
flash crowds examine performance proactive passive caching response changes object popularity 
modify trace suddenly reverse objects system 
popular object popular object second popular object second popular object 
represents worst case scenario proactive replication objects replicated suddenly need replicated widely vice versa simulating essence set flash crowds issue popular objects 
switch occurs queries reversed popularity distribution hours 
shows lookup performance pastry pc pastry beehive response flash crowds 
popularity reversal causes temporary increase average latency beehive pc pastry 
beehive adjusts replication levels objects appropriately reduces average lookup performance hop replication intervals 
lookup object transfers sec pc pastry beehive time hours rate object transfers vs time 
graph shows popularity objects change beehive imposes extra bandwidth overhead temporarily replicate newly popular objects maintain constant lookup time 
formance passive caching decreases shows instantaneous rate object transfer induced popularity reversal beehive pc pastry 
popularity reversal causes temporary increase object transfer activity beehive adjusts replication levels objects appropriately 
beehive incurs high rate activity response worst case scenario consumes bandwidth imposes aggregate load compared passive caching 
latency hops latency alpha hops 
time hours latency hops vs time 
graph shows beehive quickly adapts changes parameter query distribution brings average lookup performance hop 
alpha avg objects node objects alpha time hours objects stored node vs time 
graph shows parameter query distribution changes beehive adjusts number replicated objects maintain lookup performance storage efficiency 
zipf parameter change examine adaptation beehive global changes parameter query distribution 
issue queries zipf distributions generated differ ent values parameter hour interval 
start increase hours de crease value starting value order shorten com alpha increase time simulations performed experiment queries objects issued queries rate sec 
shows lookup performance beehive adapts changes parameter query distribution 
started experiment average query latency converges rapidly target hop 
hours increase value causes temporary decrease average query latency beehive adapts change zipf parameter brings lookup performance close target 
similarly beehive refines replication levels objects meet target lookup performance zipf parameter changes hours back hours 
shows average number objects replicated node system beehive 
parameter query distribution beehive achieves hop lookup performance replicating objects node average 
beehive observes increase zipf parameter decreases node storage requirement objects order meet target lookup performance efficiently 
similarly parameter increases beehive increases number objects stored node order achieve target 
continu ously monitoring estimating query distribution enables beehive adjust extent level replication compensate global changes 
summary section evaluated performance beehive replication framework different scenarios context dns 
evaluation indicates beehive achieves lookup performance low storage bandwidth overhead 
particular outperforms passive caching terms average latency storage requirements network load bandwidth consumption 
beehive continuously monitors popularity objects parameter query distribution quickly adapts performance changing conditions 
related peer peer lookup systems proposed date fall categories unstructured systems dht constructs unconstrained graph participating nodes structured systems dht imposes structure underlying network 
unstructured peer peer systems freenet gnutella perform lookups objects graph traversal algorithms 
gnutella uses flooding breadth freenet uses iterative depth search technique 
gnutella freenet cache queried objects search path improve efficiency search algorithms 
lookup protocols inefficient scale provide bounds average worst case lookup performance 
structured peer peer systems appealing provide worst case bound lookup performance 
structured peer peer systems designed years 
maps objects nodes ddimensional torus provides lookup performance searching multi dimensional space 
plaxton introduce randomized lookup algorithm prefix match ing locate objects distributed network prob time :10.1.1.38.1850
chord pastry tapestry consistent hashing map objects nodes route lookup requests plaxton prefix matching algorithms search objects 
internal database entries enables systems route lookup requests achieve worst case lookup performance 
kademlia provides lookup performance similar search technique uses xor metric compute closeness objects nodes 
viceroy provides lookup performance constant degree routing graph 
de bruijn graphs achieve lookup performance neighbors node degree node 
bee hive applied overlay prefix matching 
introduced dhts provide lookup performance tolerating increased storage bandwidth consumption 
kelips provides lookup performance probabilistic guarantees replicating object nodes 
divides nodes groups nodes maintains information network membership object updates gossip protocols 
maps object group replicates object nodes group regardless popularity 
background gossip communication consumes constant amount bandwidth incurs long convergence time 
consequently kelips may disseminate object updates replicas quickly 
alternative method achieve hop lookups described relies maintaining full routing state complete description system membership node 
space bandwidth costs approach scale linearly size network 
farsite routes constant number hops address rapid membership changes 
beehive differs systems fundamental ways 
beehive operates separable layer dhts requiring structural changes 
second exploits popularity distribution objects minimize amount replication 
unpopular objects replicated reducing storage overhead bandwidth consumption network load 
beehive provides fine grain control trade lookup performance overhead allowing users choose target lookup performance continuous range 
peer peer applications examined caching replication improve lookup performance increase availability provide better failure resilience 
past cfs examples file backup applications built top pastry chord respectively 
reserve part storage space node cache queried results lookup path provide faster lookup 
maintain constant number replicas object system order improve fault tolerance 
passive caching schemes provide performance bounds 
systems employ combination caching proactive object updates 
authors describe proactive cache dns records 
cached dns record expire cache issues fresh query check validity dns record result query stored cache 
technique reduces impact short expiration times lookup performance introduces large amount overhead due background object transfers providing bounded lookup performance 
cup cache update propagation demand caching mechanism proactive object updates 
cup process querying object updating cached replicas object forms tree structure rooted home node object 
cup nodes propagate object updates away home node accordance popularity incentive flows leaf nodes home node 
similarities replication protocols cup beehive 
decision cache objects propagate updates cup heuristics replication beehive driven analytical model enables provide constant lookup performance power law query distributions 
closest beehive presents study optimal strategies replicating objects unstructured systems 
employs analytical approach find best possible replication strategy unstructured peer topeer systems subject storage constraints 
observations directly applicable structured dhts cause assumes lookup time object depends number replicas placement strategy 
beehive exploits structure overlay place replicas appropriate locations network achieve desired performance level 
investigated potential performance benefits model driven proactive caching shown feasible peer peer systems cooperative low latency high performance environments 
deploying full blown applications complete peer peer dns replacement top substrate require substantial effort 
notably security issues need addressed peerto peer systems deployed widely 
application level involves authentication technique securely delegate name service nodes peer peer system 
underlying dht layer secure routing techniques required limit impact malicious nodes dht 
techniques add additional latencies may offset cost additional bandwidth storage load setting beehive target performance level lower fractional value 
beehive layer proactive replication layer needs protected nodes popularity objects 
malicious peer beehive replicate object indirectly cause object replicated nodes malicious node routing tables expect limit amount damage attackers cause object 
structured dhts offer unique properties desirable large class applications including self organization failure resilience high scalability worst case performance bound 
average case performance prohibited deployed latency sensitive applications including dns 
outline framework proactive replication improve lookup performance prefix dhts frequently encountered class query distributions 
beehive framework consists components layered top standard dht substrate pastry 
analytical model provides closed form solution computing requisite level replication order achieve targeted lookup performance 
analytical solution optimal number replicas zipf distributions estimation technique local measurements limited aggregation address statistical fluctuations derives input parameters model 
estimation process integrated background traffic dht 
computing level replication object performed independently node costly consensus synchronization 
replication algorithm proactively disseminates objects system routing tables maintained underlying dht 
analysis beehive performance context dns application indicates achieve targeted performance level low overhead 
beehive adapts quickly flash crowds alter relative objects system 
detects qualitative shifts global query distribution adjusts replication parameters accordingly compensate 
implementation small beehive approach applied latency sensitive applications 
system derives efficiency advantage underlying structure lower layer dht feasible dhts low latency applications query distribution follows power law decoupling lookup performance size network 
gnutella protocol specification www limewire com developer gnutella protocol pdf march 
lee breslau pei cao li fan graham phillips scott shenker 
web caching zipf distributions evidence implications ieee international conference computer communications infocom new york ny march 
miguel castro peter druschel ganesh antony rowstron dan wallach 
secure routing structured peer peer overlay networks symposium operating systems design implementation osdi boston ma december 
miguel castro peter druschel charlie hu antony rowstron 
exploiting network proximity peer peer overlay networks technical report msr tr microsoft research may 
ian clarke oskar sandberg brandon wiley theodore hong 
freenet distributed anonymous information storage retrieval system lecture notes computer science vol pp 
cohen haim kaplan 
proactive caching dns records addressing performance bottleneck symposium applications internet saint san diego mission valley ca january 
cohen scott shenker 
replication strategies unstructured peer peer networks acm sigcomm pittsburgh pa august 
russ cox muthitacharoen robert morris 
serving dns peer peer lookup service 
international workshop peer peer systems cambridge ma march 
frank dabek frans kaashoek david karger robert morris ion stoica 
wide area cooperative storage cfs acm symposium operating system principles sosp banff alberta canada october 
john douceur atul adya william bolosky dan simon marvin theimer 
reclaiming space duplicate files serverless distributed file system international conference distributed computing systems icdcs vienna austria july 

domain name system security extensions 
request comments rfc ed march 
gupta ken birman prakash demers robert van 
kelips building efficient stable dht increased memory background overhead second international peer peer systems workshop iptps berkeley ca february 
gupta barbara liskov rodrigo rodrigues 
hop lookups peer peer overlays ninth workshop hot topics operating systems 
hawaii may 
nicholas harvey michael jones stefan saroiu marvin theimer alec wolman 
skipnet scalable overlay network practical locality properties fourth usenix symposium internet technologies systems usits seattle wa march 
jung emil sit hari balakrishnan robert morris 
dns performance effectiveness caching acm sigcomm internet measurement workshop san francisco ca november 
frans kaashoek david karger 
koorde simple degree optimal distributed hash table second international peer peer systems workshop iptps berkeley ca february 
malkhi moni naor david 
viceroy scalable dynamic emulation butterfly acm symposium principles distributed computing podc monterey ca august 
maymounkov david mazi res 
kademlia peer peer information system xor metric international peer peer systems workshop iptps cambridge ma march 
greg plaxton rajaraman andrea richa :10.1.1.38.1850
accessing nearby copies replicated objects distributed environment theory computing systems vol pg 
roussopoulos mary baker 
cup controlled update propagation peer peer networks usenix annual technical conference san antonio tx june 
sylvia ratnasamy paul francis mark hadley richard karp scott shenker 
scalable content addressable network acm sigcomm san diego ca august 
antony peter druschel 
pastry scalable decentralized object location routing large scale peer peer systems ifip acm international conference distributed systems platforms middleware heidelberg germany november 
antony peter druschel 
storage management caching past large scale persistent peer topeer storage utility acm symposium operating system principle sosp banff alberta canada october 
ion stoica robert morris david karger frans kaashoek hari balakrishnan 
chord scalable peer peer lookup service internet applications acm sig comm san diego ca august 
udi wieder moni naor 
simple fault tolerant distributed hash table second international peer peer systems workshop iptps berkeley ca february 
ben zhao ling huang jeremy stribling sean rhea anthony joseph john kubiatowicz 
tapestry resilient global scale overlay service deployment ieee journal selected areas communications jsac 
