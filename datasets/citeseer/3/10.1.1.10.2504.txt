probabilistic logic learning luc de raedt institut informatik albert university georges koehler building freiburg germany informatik uni freiburg de past years witnessed significant interest probabilistic logic learning research lying intersection probabilistic reasoning logical representations machine learning 
rich variety different formalisms learning techniques developed 
provides introductory survey overview stateof art probabilistic logic learning identification number important probabilistic logical learning concepts 
keywords data mining machine learning inductive logic programming multi relational data mining uncertainty probabilistic reasoning 
central open questions data mining artificial intelligence concerns probabilistic logic learning integration relational logical representations probabilistic reasoning mechanisms machine learning data mining principles 
past years question received lot attention various different approaches developed cf 
:10.1.1.50.603:10.1.1.30.3832:10.1.1.1.8783:10.1.1.38.9253:10.1.1.35.951
provides introductory survey overview developments lie intersection logical relational representations probabilistic reasoning learning cf 

doing identify key concepts techniques underlying probabilistic logic learning 
turn hope allow reader appreciate differences commonalities various approaches time provide insight remaining challenges probabilistic logic learning 
starting survey specify mean probabilistic logic learning 
term probabilistic context refers probabilistic representations reasoning mechanisms grounded probability theory bayesian networks hidden markov models stochastic grammars 
representations successfully wide range applications resulted number robust models reasoning uncertainty 
application areas include genetics computer vision speech recognition understand kristian kersting institut informatik albert university georges koehler building freiburg germany kersting informatik uni freiburg de probability logic learning probabilistic logic learning probabilistic logic learning intersection probability logic learning 
ing diagnostic troubleshooting information retrieval software debugging data mining user modelling 
term logic overview refers order logical relational representations studied field computational logic 
primary advantage order logic relational representations allows elegantly represent complex situations involving variety objects relations objects 
consider problem building logical troubleshooting system set computers 
computers complex objects composed complex objects 
furthermore assume structure computers individual components highly different 
situations elegantly modelled relational order logic representations 
relation ram specify computer ram component type cpu cpu denote multi processor machine cpu 
addition base extensional relations add virtual intensional relations describe generic knowledge domain 
consider rule multiprocessor cpu cpu defines concept multi processor machine machine possesses different cpu 
representing information employing propositional logic require specify machine separate model 
term learning context probabilistic logic refers deriving different aspects probabilistic logic basis data 
typically distinguishes various learning algorithms basis data fully partially observable variables aspect learned parameters probabilistic representation logical structure 
motivation learning sigkdd explorations 
volume issue page easier obtain data application domain learn model build model traditional knowledge engineering techniques 
probabilistic logic learning aims combining underlying constituents learning probabilistic reasoning order logic representations 
interest probabilistic logic learning may explained growing body addressed pairwise intersections domains various techniques probabilistic learning gradient methods family em algorithms markov chain monte carlo methods developed exhaustively investigated different communities uncertainty ai community bayesian networks computational linguistic community hidden markov models 
techniques theoretically sound resulted entirely new technologies revolutionary novel products computer vision speech recognition medical diagnostics troubleshooting systems overviews introductions probabilistic learning :10.1.1.112.8434
inductive logic programming multi relational data mining studied logic learning learning data mining order logical relational representations 
inductive logic programming significantly broadened application domain data mining especially chemo informatics cf 
papers volume represent best known examples scientific discovery ai systems literature 
overviews inductive logic learning multi relational data mining volume :10.1.1.27.4632
early incorporating probabilities logic programs done clark mccabe shapiro 
subsequently researchers nilsson halpern ng subrahmanian poole studied probabilistic logics knowledge representational perspective :10.1.1.50.603
aim research probabilistic characterization logic suitable representations learning 
past years important attempts address full problem probabilistic logic learning :10.1.1.18.9092:10.1.1.101.3165:10.1.1.141.6461:10.1.1.35.951
goal provide overview works 
doing restrict attention developments address intersection probabilistic logic learning cf 

consequently discuss interesting lying boundaries flach lachiche na bayes structured terms craven slattery combining na bayes relational rule learner 
giving overview pairwise intersections underlying domains lead far require book scope 
probabilistic logic learning involves domains approach topic various directions 
motivated topic special issue start multi relational inductive logic programming perspective 
terminology notation employed computational logic relational databases alarm burglary earthquake 
alarm 
alarm 
alarm program 
relational model expressive model logical component probabilistic logical representations :10.1.1.35.951
attempted self contained reduce required logical machinery background possible 
organized follows sections introduce key concepts underlying computational logic probabilistic reasoning formalisms respectively section studies order probabilistic logics probabilistic reasoning computational logic combined section surveys various approaches learning probabilistic logics section conclude 

logic logic programs underlying knowledge representation framework briefly introduce key concepts 
show state art probabilistic representations employ concepts 
logic choice logic programs underlying representation motivated expressive power generality 
logic programs allow represent relational databases grammars programs 
turn useful discussing commonalities differences probabilistic relational logical representations 
addition logic programs understood clear semantics wide spread 
example logic programs figures specifies regularities alarm program inspired judea pearl famous example bayesian networks 
genetic program simple model mendel law applied color plants 
grammar example encodes context free grammar 
corresponding type logic program known term definite clause grammar 
introduce terminology 
predicates examples include alarm burglary earthquake 
pc mc pt mother father vp 
arity number arguments listed explicitly 
predicates arity alarm called propositions 
furthermore mf 
green 
dog 
constants 
variables 
constants variables terms 
addition exist structured terms shorthand nil contains list functor terms nil empty list 
constants considered functors arity 
atoms predicate symbols followed necessary number terms pt white able define key concept definite clause 
definite clauses formulas form 
bm bi atoms variables understood universally quantified 
sigkdd explorations 
volume issue page mc mother mc pc 
mc mother mc pc 
pc father mc pc 
pc father mc pc 
mc mf 
mother 
pc mf 
father 
mc mm 
mother mm 
pc mm 
father mf 
mc 
pc 
pt green mc pc 
pt red mc 
pt red pc 
np vp 
vp genetic program 
np det noun 
det 
noun dog 

np vp 
vp 
np det noun 
det 
noun dog 

context free grammars context free grammar corresponding grammar program 
definite clause pt green mc pc read phenotype green chromosome contains gene chromosome contains gene restrict attention definite clauses omit term definite long ambiguities arise 
call pt green head clause mc pc body 
clauses empty body called facts 
clause program logic program short consists set clauses 
thorough logic programming refer 
logic types semantics distinguished model theoretic proof theoretic 
distinction useful probabilistic point view briefly introduce concepts 
model theoretic view consider alarm program 
example propositions occur alarm earthquake burglary 
called herbrand base hb 
consists facts construct predicate constant function symbols program 
herbrand base sense specifies set possible worlds described program 
alarm program possible assignments truth values herbrand base 
assignments called herbrand interpretations describe world 
model theoretic point view logic program restricts set possible worlds 
interpret clause alarm burglary earthquake implication interpretations burglary earthquake true alarm satisfy clause 
idea accept clause axiom worlds impossible 
formally herbrand interpretation satisfies clause substitutions body head interpretation called model satisfies substitution 
vn tn mm assignment terms ti variables vi 
applying substitution term atom clause yields instantiated term atom clause occurrences variables vi simultaneously replaced term ti 
consider clause pt green mc pc substitution 
applying yields instantiated clause pt mm green mc mm pc mm 
furthermore reader may want verify interpretation mc pc model clause interpretation satisfies set clauses satisfies clauses set 
important point model theory purposes specifies worlds interpretations possible respect satisfy logical theory 
proof theoretic view way looking logic programs comes proof theory 
perspective logic program prove certain atoms goals see clauses logically entailed program 
consider genetic program 
program fact pt provable possible colors plant axioms encoded program 
proofs typically constructed sld resolution procedure briefly introduce 
formally goal gn clause lm applying resolution results goal lm gn 
successful resolution refutation goal sequence resolution steps leads empty goal failure empty goal 
instance genetic example prove pt green true resolution derivation pt green 
mc pc 
pc 
resolution employed theorem provers prolog 
goal pt green prolog compute resolution derivation answer 
furthermore goal pt prolog answer substitutions goal true green 
set possible resolution steps goal captured sld tree 
examples genetic domain grammar domain shown 
point important see connection sld derivation grammar domain traditional derivation context free grammar 
mapping concepts grammar 
sigkdd explorations 
volume issue page dog dog np dog vp 
det dog noun vp mother mm mc pc 
father mm mc pc 
dog noun dog vp vp pt mm red 
mc mm 
pc mm 
mother mm mc pc 
father mm mc pc 
examples sld trees 
root nodes shaded 
upper tree sld tree querying grammar program dog 
lower tree sld tree querying genetic program pt mm red 
cases refutation successful derivation 
ways viewing logic program 
view illustrated alarm program model theoretic view 
determines possible worlds interpretations satisfy program 
second view illustrated genetic program proof theoretic view 
determines possible proofs respect goal 
course views intimately connected 
clause programs ground fact resolution refutation belongs herbrand models program 

probabilistic logics model theoretic proof theoretic views logic useful probabilistic perspective 
probabilistic representations regarded defining probabilities possible worlds bayesian networks proofs stochastic context free grammars 
probabilistic representations form basis probabilistic logics provide short overview 
denote probability distribution denote probability burglary alarm earthquake graphical structure alarm bayesian network 
value 
probabilities possible worlds popular formalism defining probabilities possible worlds bayesian networks 
example bayesian network consider judea pearl famous alarm network graphically illustrated 
formally speaking bayesian network augmented directed acyclic graph node corresponds random variable xi alarm earthquake burglary edge indicates direct influence random variables 
denote set parents node xi pa xi pa alarm earthquake burglary 
bayesian network specifies joint probability distribution 
xn fixed finite set 
xn random variables 
simplicity assume random variables boolean domain true false amounts specifying probability distribution set possible interpretations 
alarm example bayesian network defines probability distribution truth assignments alarm earthquake burglary 
bayesian network stipulates conditional independency assumption node xi graph conditionally independent subset nodes descendants xi joint state pa xi xi pa xi xi pa xi 
example conditionally independent earthquake joint state alarm 
conditional independence assumption write joint probability density 
xn xi pa xi applying independency assumption chain rule expression joint probability distribution 
associate node xi graph conditional probability distribution xi pa xi denoted cpd xi 
conditional probability distributions alarm examples include alarm earthquake burglary earthquake illustrated table 
bayesian networks define probability distribution possible worlds interpretations 
furthermore general domains random variable continuous discrete cf 

sigkdd explorations 
volume issue page burglary earthquake burglary earthquake alarm true true true false false true false false alarm true false alarm true false table conditional probability distributions associated nodes alarm network cf 

distributions specified true false 
viewing child parents pairs clauses clauses alarm program cf 
specify regularities alarm network 
corresponding conditional probability distributions associated clauses 
resulting probabilities induce probability distribution level propositions propositional formulae 
reconsider alarm example assume interested probability alarm true 
probability obtained marginalizing variables abbreviated variables states appropriately 
probability propositional formula obtained similarly simply sum probabilities interpretations model formula 
key limitation bayesian networks lack notion object 
possible worlds considered propositional 
bayesian networks impossible model relations objects relational logical worlds 
problem similar traditional propositional mining learning systems 
probabilities proofs closeness proofs logical formulae derivations grammar start discussing stochastic grammars see 
consider stochastic context free grammar adapted omitted terminal symbols brevity 
context free grammar stochastic grammar rule probability associated 
non terminal symbol sum probabilities rules left hand side equals np 
stochastic contextfree grammars define probabilities derivations mapped logic programs proofs 
proofs associated probabilities sentence rice flies sand goes follows np vp 
vp 
rice vp 
rice flies vp 
np vp 
np 
np 
vp np 
vp pp 
pp np 
rice 
flies 

flies 

example stochastic context free grammar 
numbers associated grammar rules denote probability values 
note rice denote variable string rice 
rice flies np 
rice flies np 
rice flies rice flies sand 
time rule applied proof probability rule multiplied probability 
easy verify induces probability distribution possible proofs successful derivations nonterminal symbols 
failures due terminals considered probability label 
furthermore probabilities proofs induce probability accepted sentences non terminals 
consider sentence rice flies sand proofs probability illustrated probability 
sum probabilities specifies probability random sentence generated non terminal symbol rice flies sand 
distributions useful various purposes 
instance natural language processing may interested parse tree derivation dealing ambiguity total probability particular sentence derived 
bioinformatics numerous applications stochastic grammars notably hidden markov models essentially stochastic regular grammars 
determine protein belongs fold 
computational logical perspective key limitation stochastic context free grammars concerns expressive power 
context free grammars stochastic context free grammars programming language 
useful lift underlying grammar representation logic programs turing equivalent language cf 
:10.1.1.35.951
sigkdd explorations 
volume issue page 
order probabilistic logics place introduce key representational frameworks combine probabilistic reasoning logical relational representations 
furthermore spirit presentation distinguish define probabilities interpretations proofs 
probabilistic logical models class representations extends bayesian networks abilities define probabilities order logical relational interpretations cf 
:10.1.1.50.603
important predecessors logical bayesian networks discussed include breese charniak bacchus goldman poole 
predecessors largely idea knowledge model construction 
knowledge model construction knowledge base form annotated logic program describe sets probabilistic models 
query results constructing particular model answer query 
approaches represent bayesian networks 
direct upgrades bayesian networks way haddawy may explain authors knowledge bayesian network learning techniques applied representations 
central contribution combining clausal logic bayesian networks due peter haddawy 
observation structure bayesian network essentially represented set propositional clauses clause node graph cf 
alarm program bayesian network 
observation allows upgrade bayesian networks order logic upgrading corresponding propositional clauses definite clauses 
clauses specify called intensional part regularities 
addition need extensional part specifies specific objects situations consideration 
distinction extensional intensional akin database theory computational logic 
genetic program extension corresponds facts intension non fact clauses 
extensional part intensional part central ideas view atoms herbrand interpretation random variables constitute nodes induced bayesian network view instantiated clauses 
bm defining probabilistic dependencies bi correspond parents node conditional probability distribution cpd needs associated clause 
continuing discuss problems extensions illustrate ideas genetic example 
doing slightly redefine predicates pc mc specifically sense talk atoms logically entailed program true possible herbrand interpretations atoms belonging called herbrand model 
mc mother mc pc 
pc father mc pc 
mc mf 
mother 
pc mf 
father 
mc mm 
mother mm 
pc mm 
father mf 
mc 
pc 
pt mc pc 
truncated genetic program 
pc mf mc mf father mf mother mm pc mm mc mm pt mf pc mc mother father pc mc pt mm pt mc pc pt pt structure bayesian network induced truncated genetic program see 
nodes nodes non empty set parents predicate share associated cpds 
pt 
turn place predicates predicates arity idea pt state true phenotype person green 
predicates dealt similar way 
facts mother father listed truncated facts mc pc result truncated genetic program shown 
program induces bayesian network graphical structure shown 
nodes non empty set parents predicate mc pc pt share local probability model 
cpd mc mc mm 
problems approach 
notable ones 
bayesian network obtained required acyclic 
required semantics cyclic bayesian networks defined 
virtually approaches require acyclicity enforce restrictions guarantee acyclicity 
second domains random variables may boolean may possible values continuous 
ways realize 
simply allow domains random variables arbitrary cf 

side effect boolean logical random variables get mixed sigkdd explorations 
volume issue page ones 
may cause confusion 
second simply add extra argument atoms cf 

genetic domain correspond encoding 
disadvantage approach needs add extra constraints state phenotype red white cf 

turn results complicated inference engine 
case exist multiple ground clauses head 
consider program consisting clauses clause states pa second pa 
problem arise program consisting single clause 
consider may multiple true instantiations single various solutions combined developed dealing situation 
introduce called combination rules cf 

combination rules functions case take corresponding conditional probability distributions inputs produce desired output 
classical examples combining rules noisy noisy 
illustration purposes consider clauses cold 
fever cold 
flu 
fever flu 
malaria 
fever malaria 
noisy combining rule 
bayesian network induced cold flu malaria fever noisy cpd fever flu cpd fever cold cpd fever malaria associated fever cf 
page 
fever false parents independently false 
probability fever true depends parents true 
precisely fever true true cold flu malaria true second employ aggregate function done probabilistic relational models prms 
aggregate functions known databases examples include min max sum average count illustrate context reconsider clause consider multiple substitutions say holds 
random variables influence 
furthermore random variables values say 
aggregate function map set values single value notation denote value random variable depends course random variable 
multiple atoms appeared clause atoms aggregated independently 
clause represents probabilistic dependence cpd associated clauses specifies value distribution 
effect aggregate functions reduce information natural way 
advantageous reduces size conditional probability distributions natural way hand may result loss information 
introduced principles underlying logical extensions bayesian networks 
briefly survey key representational frameworks developed principles 
mentioned approach directly upgrade bayesian networks relational representations due peter haddawy 
initial approach random variables correspond ground atoms 
furthermore need combining rules alleviated requiring random variable instantiation clause having random variable head 
haddawy ngo combination rules deal problem values random variables written extra argument atoms necessitates need add constraints cf 

language called probabilistic logic programs extended knowledge model construction tradition distinguishing logical probabilistic conditions clauses 
specified logical conditions probabilistic dependencies held 
increased expressive power certainly useful specifying probabilistic models hand hand significantly extended complicated description language 
complications may explain authors knowledge structural learning results known formalism 
framework applied medical domains 
building ngo haddawy kersting de raedt introduced framework bayesian logic programs blps 
framework viewed simplification ngo haddawy blps employ minimal set primitives blps definite clause logic pure prolog bayesian networks direct special case 
turn facilitates learning cf 
:10.1.1.18.9092
bayesian logic programs view atoms random variables 
precisely atoms herbrand model constitute set random variables induced bayesian network 
blps deal continuous random variables employ simpler form combination rule 
quite popular formalism probabilistic relational models prms :10.1.1.101.3165
formalism extensions bayesian network developed pfeffer introducing probabilistic relational models 
extensions termed frame object oriented relational logical issues direct 
base discussion probabilistic relational models sigkdd explorations 
volume issue page definite clause logic pure prolog underlying representational framework entityrelationship model 
prms idea information entity type stored relation 
genetic illustration realized relation person person mc pc pt 
ground atom store information multiple attributes dependencies defined level attributes 
types dependencies allowed direct dependencies attributes entity blood type person depend chromosomes person dependencies called slot chains 
slot chains binary relations binary projections relations relate attributes entity ones 
genetic example mc attribute person depend slot chain mother person mother attributes mc pc entity corresponding mother 
furthermore basic prm setting relations entities mother father genetic example assumed deterministic cf 

implies impossible specify probability jef father mary 
partial solutions dealing probabilistic relations developed cf 

genetic example shows logical level prms essentially define single clauses probabilistic dependencies attributes various entities 
shows probabilistic dependencies relations harder represent require special treatment 
logical approaches sketched issues dealt uniform manner random variables represented ground atoms 
logically oriented version genetic example introduced shows person relation prms split normalized atoms directly correspond random variables 
hand advantages prms possess elegant graphical representation see may efficient implement learn test acyclicity requirement easily 
differences mentioned logical notation include aggregate functions deal multiple instantiations problem purely relational language allow functors needed representing infinite structures see 
prms successfully applied various problems relational clustering hypertext classification selectivity estimation databases modelling identity uncertainty led impressive results computational biology 
getoor introduced stochastic relational models srms 
syntactic level srms identical prms srms possess different semantics allow prms answer statistical queries probability randomly chosen patient elderly contact key idea underlying srms probability distributions defined level determined database schema 
level interested specific entities specific patients contacts workers properties probability randomly chosen patient elderly presentation getoor 
graphical representation probabilistic relational model genetic domain 
randomly chosen contact 
prms define dependencies different entities slot chains srms define chains joins 
issues srms quite similar prms explains provide details srms 
information see efficient algorithms construct desired bayesian network estimate particular join parameters learn dependency structure database 
srms selectivity estimation databases 
probabilistic proofs define probabilities proofs different options 
correspond approaches taken stochastic logic programs prisms respectively :10.1.1.35.951
start introducing stochastic logic programs slps 
stochastic logic programs direct upgrade stochastic context free grammars 
realized associating clause probability value 
furthermore case stochastic context free grammars sum probabilities associated clauses predicate equals 
illustrate stochastic logic programs reconsider derivation rice flies sand definite clause grammar associated probabilities 
give probabilities ground queries 
stochastic context free grammars constitute simple stochastic logic programs 
failures due non matching terminals 
general slps intensional clauses may cause failures 
consider genetic example assume associated uniform probabilities clauses predicate 
assumption show probability single proof mc 
computing probabilities ground atoms mc shows sum see refutations fail clause level 
stochastic logic programs account failures normalization 
normalization constant atom predicate mc sum probabilities refutations gen general definitions stochastic logic programs probability values sum discussed cussens 
learning stochastic logic programs considered normalized programs restrict attention 
sigkdd explorations 
volume issue page eral goal mc sum probabilities mc mf mc mm mc mc mc normalized probability mc 
details inference including approximative inference 
stochastic logic programs specify declarative priors probabilistic models 
stochastic logic programs attach probability labels clauses prisms earlier representation david poole attach probability labels facts :10.1.1.50.603
logic program consist set facts considered extensional part set proper clauses intension 
frameworks poole sato associate probability label facts genetic example take truncated encoding associate probability facts mother father addition probabilities associated facts mc pc mc mf 
pc mf 
mc mm 
pc mm 
mc 
pc 
labelled fact meaning probability fact true 
induces probability level proofs 
consider atom pt mm 
proof pt mm succeeds mc mm pc mm true 
facts true probability 
probability proof pt mm succeeds 
probability values associated facts normalization needed opposed stochastic logic programs 
point reader probably observed probability distribution proofs induces probability distribution atoms interpretations 
possible proof pt mm probability atom true 
proofs sum probabilities 
turn defines probability distribution level interpretations formalisms sato poole interpreted model theoretic perspective 
sato called distributional semantics cf 
stressing declarative character prisms 
key difference bayesian network approaches approaches rely logical inference methods rely probabilistic inference mechanisms 
sense assumptions proofs rely probabilistic bayesian networks inference mechanism turn explains strong link department course lecturer graph structure markov model web navigation 
abductive inference methods cf 
:10.1.1.50.603
discussed simplified versions poole sato frameworks 
originally introduced deal situations facts mutually exclusive 
genes involved say corresponding facts mc pc mutually exclusive 
specified logical constraints false mc mc 
false mc mc 
false mc mc 
mc mc mc person 
compact notation sato poole disjoint mc mc mc pi denotes probability value 
intermediate representations introduced frameworks integrate probabilistic models expressive relational representations logic programs 
comes computational cost 
approaches relational markov models rmms hidden tree markov models logical hidden markov models tried upgrade understood probabilistic representations hidden markov models :10.1.1.33.7666:10.1.1.1.8783
approaches viewed downgrading expressive probabilistic logics 
resulting approaches typically expressive introduced ones advantages closely related underlying representations principle efficient learning algorithms underlying representations directly applied 
illustrate idea markov models follow ideas underlying rmms example anderson :10.1.1.1.8783
markov model essentially finite state automaton probabilities associated edges symbols alphabet 
alternatively regarded stochastic regular grammar toy example web navigation academic site 
model denotes probabilities web page type web page type graph listed probability moving department lecturer 
clear model simple useful web user modelling abstraction level high 
alternative build model special case stochastic context free grammar rules exactly symbol right hand 
sigkdd explorations 
volume issue page node web pages 
model huge hardly usable learnable 
better way supported relational logical markov models proper relational logical atoms represent state propositional symbols 
sequence possible states dept cs course cs dm lecturer cs pedro course cs stats 
sequence pages represented include web page department cs dm course lecturer pedro course stats taught lecturer 
type relational logical sequence modelled type grammar rules dept course 
dept lecturer 
course lecturer 
course dept 
course 
lecturer course 
starting ground state dept cs determine possible transitions probabilities 
logical markov model specifying probability transition state state ground contains variables example states course cs lecturer cs 
able determine corresponding real states needs know domains various arguments examples set lecturers set courses say dom dom probability distribution domains pl pc specifies probability selecting particular instance domains probability selecting pedro lecturer pl pedro 
domains corresponding distributions instantiate transitions 
starting dept cs probability pl pedro going lecturer cs pedro 
illustrates key ideas underlying representations proof steps correspond time steps 
various differences approaches exist relational markov models allow variables unification :10.1.1.1.8783
employ taxonomy define domains probability estimation trees specify domain distributions 
anderson impressive experimental results adaptive web navigation 
logical hidden markov models allow variables unification upgrade hidden markov models simpler markov models 
kersting bioinformatics application predicting fold class proteins 
hidden tree markov models hidden markov models data structures focus modelling probability distributions defined spaces trees graphs 
logical concepts predicates 
adapt concepts deterministic transduction generically define dependency structure random variables 
diligenti application document image classification 
possible choices approaches exist 
concerned level specify domain probabilities 
specify locally predicate independently globally 
secondly combine probability distributions account larger substitutions cs pedro lecturer 
take na approach assuming domains independent similar na bayes cs pedro cs pedro employ advanced models bayesian networks probabilistic models 
experience research necessary order fully understand appreciate different options opportunities limitations intermediate approaches 
intermediate representations section viewed belonging proof theoretic approach probability distributions defined level derivations partial proofs respectively 

learning probabilistic logics learning denotes process adapts probabilistic models basis data 
learning probabilistic logics traditional probabilistic representations typically distinguishes tasks 
task concerned probabilistic part question numbers come called parameter estimation problem 
task assumed structure underlying logic program representation fixed known parameters probability labels estimated 
second task called structure learning model selection assumes logic program parameters fixed learned 
turn useful distinguish modelbased proof representations 
turn inductive logic programming perspective distinctions akin learning entailment learning interpretations cf 

start section discussing types data employed learning 
needs estimate parameters learning structure probabilistic model introduce parameter estimation structure learning 
evidence model proof representations employ different forms data sketch 
model theoretic example data case learning bayesian network setting consists random variables specified network possibly partial assignment values states illustrate reconsider earlier alarm assume random variables sigkdd explorations 
volume issue page example 
possible example network burglary false earthquake true alarm true example partially describes particular situation arose alarm 
judea called mary earthquake took place 
judea knew alarm went john tried call 
working logical relational upgrades bayesian networks natural examples form interpretations 
atoms interpretations specify random variables values specify state 
genetic illustration example pc true mc true pc true mc father true mother true pc mc pt true logical perspective constraints type examples 
random variables specified single example considered herbrand interpretation interpretation model unknown target program 
akin learning interpretations setting field inductive logic programming 
second bayesian network structure induced target program particular example acyclic 
properties turn important learning structure 
proof theoretic obtain insight nature examples proof theoretic setting idea look nature data learning stochastic context free grammars 
learning stochastic context free grammars examples strings accepted unknown probability target grammar 
learning stochastic context free grammar natural language domain english examples english sentences 
furthermore sentences derivable target grammar 
working definite clause grammars sentences represented facts 
sentence non terminal symbol represented dog 
proof oriented setting probabilistic logics evidence correspond facts clauses 
furthermore examples logically entailed target program akin traditional learning entailment setting inductive logic programming cf 
:10.1.1.27.4632
intermediate representations intermediate representations hidden markov models typically learned examples correspond sequences states arose particular situations 
markov model web site domain learned examples department lecturer course lecturer order case sequences specified sec known latent variables states observed introduced learning algorithm 
tion dept cs course cs dm lecturer cs pedro course cs stats 
sequences correspond kind partial trace derivation underlying proof system 
learning traces considered field inductive logic programming see known examples traces induce logic programs 
traces impose strong logical constraint underlying program 
subsequent states trace logical markov model required exists clause substitution 
parameter estimation parameter estimation methods start set examples logic program probabilistic logical model aims computing values parameters best explain data 
set parameters quantitative part model represented vector 
measure extent model fits data usually employs likelihood data 
likelihood data probability observed data function unknown parameters respect current model 
maximum likelihood estimation mle aims finding argmax 
argmax argmax log works log likelihood log easier manipulate 
assumes examples independently sampled identical distributions log log 
variants extensions mle criterion 
include bayesian approach take account parameter priors minimum description length mdl principle structure learning cf 
section 
illustrate mle consider tossing coin 
coin heads tails 
assume evidence toss head toss tail toss tail respectively head tail tail proof notation current quantitative part toss head log log log log maximize log likelihood setting partial derivative equal log solving give intuitive estimate proportion heads seen examples 
formally count toss head count toss head count toss tail count denotes frequency event 
mle reduces frequency counting 
sigkdd explorations 
volume issue page presence missing data maximum likelihood estimate typically written closed form 
numerical optimization problem known algorithms involve nonlinear optimization commonly adapted technique probabilistic logic learning expectation maximization em algorithm em observation learning easy correspond frequency counting values random variables known 
estimates values uses maximize likelihood iterates 
specifically em assumes parameters initialized random iteratively perform steps convergence step basis observed data parameters model compute distribution possible completions partially observed data case 
step completion fully observed data case weighted probability compute updated parameter values weighted frequency counting 
frequencies completions called expected counts 
illustrate em evidence toss head toss toss tail assume current model toss head completions data case toss head toss tail toss head toss tail updated maximum likelihood estimation probability coin head assuming gives iterating yields 
converges 
discuss em applied various probabilistic logics introduced earlier 
sake simplicity state key ideas address advanced issues efficiency concerns 
model theoretic parameter estimations probabilistic logic programs probabilistic relational models bayesian logic programs follow principle data current model induce bayesian network explaining data cases :10.1.1.18.9092:10.1.1.101.3165
parameters induced bayesian network estimated standard bayesian network parameter estimation methods discussed heckerman tutorial 
instance examples section genetic program induce bayesian network shown 
nodes bayesian network occur evidence introduced program approaches employ gradient approaches em muggleton structural learning slps kersting de raedt developed em gradient approaches parameter estimation blps :10.1.1.18.9092:10.1.1.35.951
underlying idea em 
formally step consists computing expectation likelihood old parameters observed data 
step consists maximizing expected likelihood parameters 
observed assigned question mark 
evidence corresponds partial joint state resulting bayesian network 
completed step relies traditional bayesian network inference determine distribution values unobserved states 
improved estimates parameters node values cpds computed step dividing expected counts corresponding node parents parents joint states 
difference standard bayesian network parameter estimation parameters different nodes network corresponding different ground instances clause forced identical 
technique akin recurrent neural network dynamic bayesian networks 
requires unique assignment parent nodes clauses 
aggregate functions guarantee prms aggregated examples constitute states nodes induced bayesian network 
blps uniqueness enforced decomposable combining rules 
effect decomposable combining rule represented extra nodes induced bayesian network 
combining rules commonly employed bayesian networks noisy noisy linear regression decomposable 
proof theoretic model approaches complete data terms bayesian network proof theoretic approaches complete data refutations failures 
stochastic logic programs cussens introduced failure adjusted maximization fam algorithm 
fam logical part slp fixed parameters learned basis evidence 
evidence examples atoms predicate assumed generated target slp probability distribution induces implies examples logically entailed slp 
parameters estimated computing example sld tree treating path root leaf sld tree possible completion 
instance evidence pt mm red yields possible completions paths lower sld tree failed constitutes proof secondly completions weighted product probabilities presently associated clauses corresponding derivations cf 
section 
improved estimates clause obtained dividing clause expected counts sum expected counts clauses predicate 
em algorithm prism programs similar spirit 
main differences probability values associated intensional clauses failed derivations probability zero disjoint representation introduced section assumed 
example explanations example computed 
similar abductive reasoning explanation example set facts allows example proven 
instance pc mm explanation pt mm red failure derivation neglected 
approach called failure adjusted maximization failed successful derivations taken account 
sigkdd explorations 
volume issue page tions constitute completions data weighted probability specified current parameters 
maximum likelihood estimation probability value associated fact say pc mm fact expected count explanations true divided expected counts corresponding mutually exclusive facts declaration disjoint pc mm pc mm pc mm pc mm mutually exclusive 
speed computations sato kameya employ known tabulation techniques field computational logic 
argued intermediate representations rmms viewed belonging proof theoretic approach 
omit detailed discussion parameter estimation methods 
structure learning structure learning starts set examples language bias determines set possible hypotheses aims computing hypothesis 
logically covers examples cover 
hypothesis optimal scoring function score argmax score 
hypotheses form logical part vector parameter values section 
coverage criterion typically depend type data representation formalisms considered cf 
section discussed different types data types representation formalisms 
addition mentioned section various scoring functions employed 
simple example score likelihood criterion 
type data coverage criterion language bias scoring function central components structure learning method 
addition various enhancements variants considered 
include possible initial hypothesis serve starting point guiding search incorporation background knowledge 
background knowledge consist set fixed clauses possibly attached probability labels provide useful information problem domain 
initial hypothesis common probabilistic learning theory revision approaches inductive logic programming :10.1.1.45.2389
background knowledge typically employed multi relational data mining inductive logic programming cf 
:10.1.1.27.4632
distinctions data initially fixed gathered incrementally 
distinctions enhancements quite similar logic learning probabilistic learning 
provide details aspects doing lead far 
concentrate generalized problem sketched 
nearly score approaches structure learning perform heuristic search space possible hypotheses determined typically hill climbing beam search applied candidate hypothesis burglary 
earthquake 
alarm 
burglary 
earthquake 
alarm burglary 
burglary 
earthquake 
alarm burglary earthquake 
refinement operators structure learning bayesian networks 
add proposition body clause respectively fact delete body 
pt mc pc 
pt mc pc 
mc 
pt mc pc 
mc pc 
pt pc 
mc 
pt mc pc 
mc pc 
order refinement operators structure learning 
atoms added deleted body clause 
facts added deleted program 
satisfies cover score longer improving 
steps search space application called refinement operators perform small modifications hypothesis 
logical perspective refinement operators typically realize elementary generalization specialization steps usually subsumption 
probabilistic learning certainly working graphical models larger steps taken see 
illustrate refinement operators 
assume current candidate hypothesis consists set clauses 
cn 
results applying specialization refinement operator hypothesis imagined 
notice operates hypothesis theory level employs refinement operator clause level ci ci ci fact 
bm 
bm atom 
addition typically employs dual generalization operator 
operator generalize hypotheses deleting atoms clauses facts hypotheses 
refinement operator illustrated propositional clauses order ones 
able describe key approaches structure learning probabilistic logics 
bayesian networks directly correspond propositional clauses 
typical structure learning approach propositional generalization specialization operator applied 
step atoms added deleted clauses 
furthermore exactly clause proposition 
special care taken resulting network acyclic 
state art point note employ simplified idealized refinement operator 
essential features substitutions taken account reasons simplicity cf 

sigkdd explorations 
volume issue page learning algorithm structural em sem 
adapts standard em algorithm structure learning 
key idea expected counts computed anew structure proposed iterations 
leads improved efficiency 
prms srms blps application refinement operator hypothesis level acts kind macro level induced bayesian network 
literal added logical hypothesis corresponds adding multiple edges induced bayesian network data 
consequently bayesian network learning techniques sem adopted 
probabilistic relational models represented clauses denote different aggregate functions clause defining predicate 
structure learning probabilistic relational models occurs refining type clauses aggregated literals added deleted clauses :10.1.1.101.3165
point graphical notation probabilistic relational models illustrated convenient visualizing process 
adding deleting aggregated literals clause directly corresponds operation arcs 
turn clarifies connection traditional bayesian network learning 
bayesian networks special care taken guarantee regardless extension probabilistic relational model acyclic 
bayesian logic programs represented regular clauses typical refinement operators inductive logic programming applied 
bayesian logic programs take account covers constraint 
formally required examples models bayesian logic programs cover true model requirement needed argued section set random variables defined bayesian logic program corresponds herbrand model 
akin learning interpretations setting inductive logic programming 
requirement enforced learning structure bayesian logic programs starting initial bayesian logic programs satisfies requirement hypothesis computed clausal discovery engine considering refinements result violation 
addition acyclicity enforced checking refinement example induced bayesian network acyclic 
framework stochastic logic programs representative proof theoretic probabilistic logics 
differs frameworks previously discussed section learning problem related bayesian networks 
stochastic logic programs similar bayesian logic programs represented regular clauses typical refinement operators apply 
bayesian logic programs learned interpretations stochastic logic programs learned entailment 
examples stochastic logic programs facts please note original prms learning problem terms clauses 
facts logically entailed target stochastic logic program 
coverage requirement states stochastic logic program 
solving general structure learning problem stochastic logic programs involves applying refinement operator theory level considering multiple predicates entailment 
problem studied field inductive logic programming term theory revision known hard problem 
may explain general form structure learning stochastic logic programs related prisms addressed 
far contributions structure learning stochastic logic programs restricted learning missing clauses single predicate 
muggleton introduced phase approach separates structure learning aspects parameter estimation phase 
approach muggleton presents initial attempt integrate phases single predicate learning 
learn models intermediate representations take advantage simplified underlying logic program 
logical relational markov models represented transition rules form atoms representing states 
principle consider possible transition estimate corresponding transition probability directly data 
number transitions way large approach practice 
approaches select informative transitions needed 
rmms realized employing transition predicate pair assigning probability estimation tree kind decision tree predicate 
probability estimation tree assign concrete state body part transition probability distribution possible resulting states 
probability estimation trees encode kind abstraction hierarchy 
kersting propose different approach essentially structural generalized em algorithm refinement operators formalism 

overview survey new exciting area probabilistic logic learning 
combines principles probabilistic reasoning logical representations statistical learning coherent 
techniques probabilistic logic learning analyzed starting logical inductive logic programming perspective 
turned quite useful obtaining appreciation differences similarities various frameworks formalisms contributed date 
particular distinction model view clarifying relation logical upgrades bayesian networks prms blps grammars prisms slps 
distinction relevant representational level learning 
turned learns interpretations model theoretic approaches entailment proof theoretic ones traces intermediate ones rmms 
furthermore principles statistical learning inductive logic programming multi relational data mining sigkdd explorations 
volume issue page employed learning parameters structure probabilistic logics considered 
authors hope survey provides useful perspective probabilistic logic learning may inspire reader contribute challenging exciting research area 
benefited european union ist project ist application probabilistic inductive logic programming april 
authors lise getoor providing graphical representation prm 

allen 
natural language understanding 
benjamin cummings series computer science 
benjamin cummings publishing 
anderson domingos weld :10.1.1.1.8783
relational markov models application adaptive web navigation 
hand keim za ne goebel editors proceedings eighth international conference knowledge discovery data mining kdd pages edmonton canada 
acm press 
cussens 
markov chain monte carlo tree priors model structure 
breese koller editors proceedings seventeenth annual conference uncertainty artificial intelligence uai pages seattle washington usa 
morgan kaufmann 
bacchus 
order probability logic construction bayesian networks 
heckerman mamdani editors proceedings ninth annual conference uncertainty artificial intelligence uai pages providence washington dc usa 
morgan kaufmann 
bergadano 
inductive logic programming machine learning software 
mit press 
breese 
construction belief decision networks 
computational intelligence 
breese goldman wellman 
special section knowledge construction probabilistic decision models 
cybernetics 
buntine 
guide literature learning probabilistic networks data 
ieee transaction knowledge data engineering 
clark mccabe 
prolog language implementing expert systems 
hayes michie 
pao editors machine intelligence volume pages 
ellis horwood chichester 
cowell dawid lauritzen spiegelhalter 
probabilistic networks expert systems 
springer verlag 
craven slattery 
relational learning statistical predicate invention better models hypertext 
machine learning 
cussens 
loglinear models order probabilistic reasoning 
laskey prade editors proceedings fifteenth annual conference uncertainty artificial intelligence uai pages stockholm sweden 
morgan kaufmann 
cussens 
stochastic logic programs sampling inference applications 
boutilier goldszmidt editors proceedings sixteenth annual conference uncertainty artificial intelligence uai pages stanford ca usa 
morgan kaufmann 
cussens 
parameter estimation stochastic logic programs 
machine learning 
cussens 
statistical aspects stochastic logic programs 
jaakkola richardson editors proceedings eighth international workshop artificial intelligence statistics pages key west florida usa 
morgan kaufmann 
de raedt 
interactive theory revision inductive logic programming approach 
academic press 
de raedt 
logical settings concept learning 
artificial intelligence 
de raedt dehaspe 
clausal discovery 
machine learning 
dean kanazawa 
probabilistic temporal reasoning 
mitchell smith editors proceedings seventh national conference artificial intelligence aaai pages st paul mn usa 
aaai press mit press 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
royal stat 
soc 
diligenti frasconi gori 
hidden tree markov models document image classification 
ieee transactions pattern analysis machine intelligence 
durbin eddy krogh mitchison 
biological sequence analysis probabilistic models proteins nucleic acids 
cambridge university press 
zeroski lavra 
relational data mining 
springer verlag 

probabilistic extensions grammars 
editor computational aspects constraint linguistics ii 
dyna deliverable 
sigkdd explorations 
volume issue page william feller 
probability theory applications volume 
wiley series probability mathematical statistics 
john wiley sons rd edition 
flach 
simply logical intelligent reasoning example 
john wiley sons 
flach lachiche 
bc order bayesian classifier 
zeroski flach editors proceedings ninth international workshop inductive logic programming ilp volume lnai pages bled slovenia 
springer 
frasconi gori sperduti 
general framework adaptive processing data structures 
ieee transactions neural networks 
friedman 
bayesian structural em algorithm 
cooper moral editors proceedings fourteenth annual conference uncertainty artificial intelligence uai pages madison wisconsin usa 
morgan kaufmann 
friedman getoor koller pfeffer 
learning probabilistic relational models 
dean editor proceedings sixteenth international joint conferences artificial intelligence ijcai pages stockholm sweden 
morgan kaufmann 
getoor 
learning statistical models relational data 
phd thesis stanford university 
getoor friedman koller 
learning structured statistical models relational data 
link ping electronic articles computer information science 
getoor friedman koller pfeffer 
learning probabilistic relational models 
zeroski lavra editors relational data mining 
springer verlag 
getoor friedman koller taskar 
learning probabilistic models relational structure 
brodley editors proceedings eighteenth international conference machine learning icml pages ma usa 
morgan kaufmann 
getoor koller taskar 
statistical models relational data 
zeroski de raedt editors workshop notes kdd workshop multi relational data mining 
getoor koller taskar friedman 
learning probabilistic relational models structural uncertainty 
getoor jensen editors proceedings aaai workshop learning statistical models relational data pages 
getoor segal taskar koller 
probabilistic models text link structure hypertext classification 
workshop notes ijcai workshop text learning supervision washington usa 
getoor taskar koller 
selectivity estimation probabilistic relational models 
aref editor proceedings acm special interest group management data conference sigmod santa barbara ca usa 
koller 
constructing flexible dynamic belief networks order probabilistic knowledge bases 
ch 
froidevaux editors proceedings european conference symbolic quantitative approaches reasoning uncertainty ecsqaru volume lncs pages fribourg switzerland 
springer verlag 
goldman charniak 
dynamic construction belief networks 
bonissone henrion kanal lemmer editors proceedings sixth annual conference uncertainty artificial intelligence uai pages cambridge ma usa 
elsevier 
haddawy 
generating bayesian networks probabilistic logic knowledge bases 
pez de poole editors proceedings tenth annual conference uncertainty artificial intelligence uai pages seattle washington usa 
morgan kaufmann 
haddawy ngo krieger 
clinical simulation context sensitive temporal probability models 
proceedings nineteenth annual symposium computer applications medical care scamc 
halpern 
analysis order logics probability 
artificial intelligence 
heckerman 
tutorial learning bayesian networks 
technical report msr tr microsoft research 
jaeger 
relational bayesian networks 
geiger shenoy editors proceedings thirteenth annual conference uncertainty artificial intelligence uai pages providence rhode island usa 
morgan kaufmann 
jensen neville 
linkage autocorrelation cause feature selection bias relational learning 
sammut hoffmann editors proceedings nineteenth international conference machine learning icml pages sydney australia 
morgan kaufmann 
jensen 
bayesian networks decision graphs 
springer verlag new 
jordan editor 
learning graphical models 
kluwer academic publishers 
reprinted mit press 
kameya sato 
efficient em learning tabulation parameterized logic programs 
lloyd dahl furbach kerber 
lau palamidessi pereira sagiv stuckey editors proceedings international sigkdd explorations 
volume issue page conference computational logic cl volume lnai pages 
springer verlag 
kameya ueda sato 
graphical method parameter learning symbolic statistical models 
proceedings second international conference discovery science ds volume lnai pages japan 
springer verlag 
kersting de raedt 
adaptive bayesian logic programs 
rouveirol sebag editors proceedings eleventh conference inductive logic programming ilp volume lncs strasbourg france 
springer 
kersting de raedt 
bayesian logic programs 
technical report university freiburg institute computer science april 
submitted 
kersting de raedt 
combining inductive logic programming bayesian networks 
rouveirol sebag editors proceedings eleventh conference inductive logic programming ilp volume lncs strasbourg france 
springer 
kersting de raedt 
principles learning bayesian logic programs 
technical report university freiburg institute computer science june 
submitted 
kersting kramer de raedt 
discovering structural signatures protein folds logical hidden markov models 
altman hunter jung klein editors proceedings pacific symposium biocomputing pages kauai hawaii usa 
world scientific 
kersting de raedt 
structural gem learning logical hidden markov models 
zeroski de raedt wrobel editors workshop notes kdd workshop multi relational data mining 
appear 
koller 
probabilistic relational models 
zeroski flach editors proceedings ninth international workshop inductive logic programming ilp volume lnai pages bled slovenia 
springer 
koller levy pfeffer 
classic tractable probabilistic description logic 
proceedings fourteenth national conference ai pages providence rhode island august 
koller pfeffer 
learning probabilities noisy order rules 
proceedings fifteenth joint conference artificial intelligence ijcai pages nagoya japan 
koller pfeffer 
object oriented bayesian networks 
geiger shenoy editors proceedings thirteenth annual conference uncertainty artificial intelligence uai pages providence rhode island usa 
morgan kaufmann 
koller pfeffer 
probabilistic frame systems 
proceedings fifteenth national conference artificial intelligence pages madison wisconsin usa july 
aaai press 
lachiche flach 
bc true order bayesian classifier 
matwin sammut editors proceedings twelfth international conference inductive logic ilp volume lncs pages sydney australia 
springer 
lloyd 
foundations logic programming 
springer berlin 
edition 
manning sch tze 
foundations statistical natural language processing 
mit press 
krishnan 
em algorithm extensions 
john sons 
muggleton :10.1.1.35.951
stochastic logic programs 
de raedt editor advances inductive logic programming 
ios press 
muggleton 
learning stochastic logic programs 
electronic transactions artificial intelligence 
muggleton 
learning structure parameters stochastic logic programs 
matwin sammut editors proceedings twelfth international conference inductive logic ilp volume lncs pages sydney australia 
springer 
muggleton de raedt :10.1.1.27.4632
inductive logic programming theory methods 
journal logic programming 
muggleton 
learning stochastic logic programs 
getoor jensen editors working notes aaai workshop learning statistical models relational data srl pages austin texas 
aaai press 
ng subrahmanian 
probabilistic logic programming 
information computation 
ngo haddawy 
knowledge model construction approach medical decision making 
proceedings twentieth american medical informatics association annual fall symposium amia washington dc usa 
ngo haddawy 
answering queries context sensitive probabilistic knowledge bases 
theoretical computer science 

cheng de wolf 
foundations inductive logic programming 
springer verlag 
sigkdd explorations 
volume issue page nilsson 
principles artificial intelligence 
springer verlag new york 
reprint originally published tioga publishing 
pasula russell 
identity uncertainty citation matching 
becker thrun obermayer editors advances neural information processing systems 
mit press 
pasula russell 
approximate inference order probabilistic languages 
nebel editor seventeenth international joint conference artificial intelligence ijcai pages seattle washington usa 
morgan kaufmann 
pearl 
reasoning intelligent systems networks plausible inference 
morgan kaufmann 
edition 
pfeffer koller 
semantics inference recursive probability models 
kautz porter editors proceedings seventeenth national conference artificial intelligence aaai pages austin texas usa 
aaai press 
pfeffer koller 
system probabilistic object oriented knowledge representation 
proceedings fifteenth annual conference uncertainty artificial intelligence uai 
pfeffer 
probabilistic reasoning complex systems 
phd thesis stanford university 
poole :10.1.1.50.603
probabilistic horn abduction bayesian networks 
artificial intelligence 
provost domingos 
tree induction probability ranking 
machine learning 
appear 
rabiner 
tutorial hidden markov models selected applications speech recognition 
proceedings ieee 
rabiner juang 
hidden markov models 
ieee assp magazine pages january 
russell norvig 
artificial intelligence modern approach 
prentice hall 
santos costa page cussens 
clp bn constraint logic programming probabilistic knowledge 
proceedings nineteenth annual conference uncertainty artificial intelligence uai mexico 
morgan kaufman 
appear 
sato 
statistical learning method logic programs distribution semantics 
sterling editor proceedings twelfth international conference logic programming iclp pages tokyo japan 
mit press 
sato 
parameterized logic programs computing meets learning 
ueda editors proceedings fifth international symposium functional logic programming flops volume lncs pages tokyo japan 
springer verlag 
sato kameya 
prism symbolic statistical modeling language 
proceedings fifteenth international joint conference artificial intelligence ijcai pages nagoya japan 
morgan kaufmann 
sato kameya 
viterbi algorithm em learning statistical abduction 
myers parsons editors workshop notes uai workshop fusion domain knowledge data decision support stanford ca usa 
sato kameya 
parameter learning logic programs symbolic statistical modeling 
journal artificial intelligence research 
segal simon friedman koller 
promoter sequence expression probabilistic framework 
proceedings sixth international conference research computational molecular biology recomb pages washington dc usa 
segal battle koller 
decomposing gene expression cellular processes 
altman hunter jung klein editors proceedings pacific symposium biocomputing pages kauai hawaii usa 
world scientific 
segal taskar friedman koller 
rich probabilistic models gene expression 
bioinformatics 
shapiro 
algorithmic program debugging 
mit press 
shapiro 
logic programs uncertainties tool implementing expert systems 
bundy editor proceedings eighth international joint conference artificial intelligence ijcai pages karlsruhe germany 
william kaufmann 
taskar segal koller 
probabilistic clustering relational data 
nebel editor seventeenth international joint conference artificial intelligence ijcai pages seattle washington usa 
morgan kaufmann 
williams zipser 
gradient learning algorithms recurrent networks complexity 
chauvin rumelhart editors back propagation theory architectures applications 
lawrence erlbaum hillsdale nj erlbaum 
wrobel :10.1.1.45.2389
order theory refinement 
de raedt editor advances inductive logic programming 
ios press 
sigkdd explorations 
volume issue page 
