context policy search transfer experience problems leonid peshkin ai mit edu harvard center arti cial intelligence cambridge ma edwin de jong edwin cs brandeis edu computer science department brandeis university waltham ma important question reinforcement learning generalization may performed 
problem especially important learning agent receives partial information state environment 
typically bias required generalization chosen experimenter 
investigate way learning method extract bias learning problem apply subsequent problems 
gradient policy search method look controllers consist context component action component 
empirical results agent coordination problem reported 
learning bias possible address problems solved 

reinforcement learning problems large state spaces typically require generalization 
form generalization implies choices regarding similarity di erent situations choices constitute bias 
success particular method generalization depends bias appropriate problem hand :10.1.1.19.5466
standard markov decision processes mdps observation learner captures relevant information state environment 
partially observable mdps pomdps choice appropriate action may principle depend pre version available www eecs harvard edu papers html vious observations 
learner extract relevant information history interactions environment 
renders need appropriate generalization pressing 
general may possible recover required information state environment observations certain class pomdps reduced mdps distinguishing di erent contexts agent may nd 
contexts extracted interaction history remaining problem addressed standard reinforcement learning methods 
information reinforcement learning general reader may consult 
problems having structure optimal policy described set behaviors corresponds particular context 
suggest structure potential meta learning 
related environments require similar behaviors di ering correspondence contexts input observations behaviors transferred previously solved problems dicult variants problems 
identi ed particular mechanism extracting useful bias related problems see corresponding class problems bene mechanism :10.1.1.37.8738
investigate method simple case report empirical results 

architecture learning mechanism partial observability environmental state requires policy representation include form memory order specify optimal policy 
case achieved nite state con see :10.1.1.102.8052
furthermore order nd optimal policies partially observable problems learning method able search policies memory 
employ method policy search see general method discussion cooperation explicit coordination agents controlled fscs 
main idea search policies described set contexts corresponding behaviors behaviors transferred problems 
possible construct agent components see gure 
rst component called context component determine current context interaction history 
general history may include past observations actions rewards 
particular problem context component uses current observation context previous time step determine current context 
action component receives current context optionally observations input speci es behavior partial policy context 
component action component context action observation context 
proposed architecture 
context component determines current context previous observation context 
action component uses information possibly combination current observation produce corresponding behavior 
separating context action components representation policy allows components learned separately 
examples advantage controller structure 
uses controller consisting disjoint neural networks demonstrates natural continuity euclidean coordinate system generalize observation space 
order transfer experience nd optimal controller small instance problem hand retain action component controller scaling larger instances problem 
provides natural way incorporate bias learning process 
nite state controller described detail 
controller consists internal state transition function determines context observations action function associates context corresponding behavior 

uence diagram agent fscs pomdp 
complete controller agent action space observation space tuple hm nite set internal controller states contexts internal state transition function maps internal state observation probability distribution internal states 
experiments action component maps internal state probability distribution actions 
assume internal state transition function action function stochastic derivatives exist bounded away zero 
depicts uence diagram agent controller 
consider series problems require similar behaviors 
retaining part policy speci es behaviors learned small problem useful starting point addressing larger similar problems obtained 
context component hand learned anew particular observations identify context may vary di erent environments 
sections rst describe task detail experiments reported illustrate advantage aforementioned learning bias 

task description block moving metaphor task experiments block moving multi agent problem derived load unload problem 
requires cooperation agents constitutes coor problem sense agents need adjust actions 
real world version problem activity people lifting heavy block synchronized satisfactory results requires coordinating moment start moving 
information state agent exchanged possibly non verbal communication 

state transition function 
simple task study process modeled follows 
agent move independently locations see gure locations example 
agents state automatically load block 
move synchrony order drop load 
agents perceive location agent communication necessary 
reward function task depicted gure 
states diagram collective states specifying state rst second agent 
connections arrows mark transitions direction 
projection diagram horizontal axis yields state transition diagram single agent shown gure 
reward function depends current state agents history states 
aspect history relevant reward function load lifted dropped lifted 
collective states property upper level diagram marked loaded 
remaining collective states lower unloaded level 
potential communication provided follows 
addition action agent select move states agent single bit set reset 
agent reads bits agents addition sensor information specifying location 
agents start state learn move state quickly possible return 
ballistic policy cause move synchrony need communication 
order disallow strategy initial position agent selected randomly locations unloaded loaded load unload 
reward structure block moving domain 
load 
location instance depicted gure optimal strategy form 
agent moves state informs agent presence necessary waits agent arrive 
agent moves back 
agents execute policy move picking load 
communication agents synchronize behavior collect maximum amount reward 
part diculty task requires establishing convention 
clear possibility communication agent set reset bit sending receiving agent converge choice setting case 
agents equivalent may function sender receiver problem needs solved twice necessarily way 
greater diculty selection convention settling sending receiving behavior simultaneously rst agent arrive determine second agent arrived communication second agent discover signaling information useful rst agent acts convention 
necessity simultaneously arrive compatible communication action policies block moving problem challenging 
diculty problem strongly depends size problem instance 
version problem states described involving load state intermediate state size biased size unbiased size unbiased size biased 
empirical comparison biased unbiased learning policy search fscs block lifting environment various size 
unload state 
obtain larger versions problem introduce additional intermediate states 
part policy space considered grows exponentially length smallest optimal policy adding states problem considerably dicult 

empirical results experiments learning optimal policy initial bias instance problem locations counting load unload locations 
trivial instance domain exhaustive evaluation policies feasible comes surprise agents rapidly converge policy 
resulting action function bias starting point action functions learning policies larger instances domain 
illustrates advantage learning bias domain 
plots averaged runs 
learning rate kept constant 
policy learned space state nite space controllers 
domain locations biased unbiased learning converge unbiased signi cant delay 
domain locations unbiased learning picks trial length 
bias learned smaller version problem possible learn dif cult instance turns ecient unbiased learning smaller domain locations 
experiments action function smaller problem instance starting point learning process principle modi ed 
practice action component change context component learned anew 

discussion signal agent environment formed part observation 
potential change representation controller separate location signal feed signal part observation directly action function 
justi ed fact increase size domain number semantics messages change 
location numbering hand change needs learned instance 
alternative approach approach investigated action function learn internal state transition function 
plan experiment variants setup experiments 
sense agent develops categorization di erent situations learns behavior situation establishes convention form mapping contexts behaviors 
mapping possible world states internal states corresponding mapping internal states partial policies combination leads optimal complete policy 
issue arising convention pronounced problems multiple agents ability exchange signals 
variant establishing mapping internal states signals produce seen rudimentary form communication development consistent signaling convention sucient allow agents produce optimal behavior speci signaling convention arbitrary arriving convention dicult coordination problem 

investigated approach partially observable reinforcement learning problems representation policy separated context component action component 
useful bias extracted simple version problem retaining action component 
put dicult problems method solved problems unbiased approach unable address 
hope explore mechanisms extracting bias learning utilizing aid subsequent learning 
believe may principle possible address problems practically addressed conventional learning methods 
baird 
reinforcement learning gradient descent 
phd thesis cmu pittsburgh pa 
eric baum 
neural networks machine learning chapter manifesto evolutionary economics intelligence pages 
springerverlag 
eric baum 
model intelligence economy agents 
machine learning 
eric baum 
evolution cooperative problem solving arti cial economy 
neural computation 
jonathan baxter 
model inductive bias learning 
journal arti cial intelligence research 
edwin de jong jordan pollack 
utilizing bias evolve recurrent neural networks 
proceedings international joint conference neural networks volume pages 
ungar 
localizing policy gradient estimates action transitions 
proceedings seventeenth international conf 
machine learning 
morgan kaufmann 
ungar 
localizing search reinforcement learning 
proceedings seventeenth national conf 
arti cial intelligence 
dean maria gini james 
integrated connectionist approach reinforcement learning robotic control 
proceedings seventeenth international conf 
machine learning 
morgan kaufmann 
leslie kaelbling michael littman andrew moore 
reinforcement learning survey 
journal ai research 
nicolas meuleau leonid peshkin kee kim leslie kaelbling :10.1.1.102.8052
learning nite state controllers partially observable environments 
proceedings fifteenth conf 
uncertainty arti cial intelligence pages 
morgan kaufmann 
tom mitchell :10.1.1.19.5466
need biases learning generalizations 
technical report cbm tr department computer science rutgers university new brunswick new jersey may 
leonid peshkin 
reinforcement learning policy search 
phd thesis brown university providence ri 
leonid peshkin kee kim nicolas meuleau leslie kaelbling 
learning cooperate policy search 
proceedings sixteenth conf 
uncertainty arti cial intelligence pages san francisco ca 
morgan kaufmann 
christian shelton 
importance sampling reinforcement learning multiple objectives 
phd thesis mit cambridge ma 
richard sutton andrew barto 
reinforcement learning 
mit press cambridge ma 
