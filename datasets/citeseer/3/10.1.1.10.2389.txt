investigating loss functions optimization methods discriminative learning label sequences altun computer science brown university providence ri altun cs brown edu discriminative models interest nlp community years 
previous research shown advantageous generative models 
investigate different objective functions optimization methods affect performance classifiers discriminative learning framework 
focus sequence labelling problem particularly pos tagging ner tasks 
experiments show changing objective function effective changing features included model 
years generative models common approach nlp tasks 
growing interest discriminative models nlp community models shown successful different tasks lafferty ratnaparkhi collins :10.1.1.120.9821
discriminative models theoretical advantages generative models discuss section shown empirically favorable generative models features objective functions fixed klein manning 
discriminative models investigate optimization different objective functions variety optimization methods 
mark johnson cognitive linguistic sciences brown university providence ri mark johnson brown edu thomas hofmann computer science brown university providence ri th cs brown edu focus label sequence learning tasks 
part speech pos tagging named entity recognition ner studied applications tasks 
chunking pitch accent prediction speech edit detection 
tasks differ aspects nature label sequences chunks individual labels difficulty evaluation methods 
variety think worthwhile investigate optimizing different objective functions affects performance 
varied scale exponential vs logarithmic manner optimization sequential vs pointwise different combinations designed different objective functions 
optimized functions ner pos tagging tasks 
despite intuitions experiments show optimizing objective functions vary scale manner affect accuracy 
selection features larger impact 
choice optimization method important learning problems 
optimization methods handle large number features converge fast return sparse classifiers 
importance features importance ability cope larger number features known 
training discriminative models large corpora expensive optimization method converges fast advantageous 
sparse classifier shorter test time denser classifier 
applications test time crucial optimization methods result sparser classifiers preferable methods hmm crf graphical representation hmms crfs 
shaded areas indicate variables model conditions 
training time longer 
investigate aspects different optimization methods number features training time sparseness accuracy 
cases approximate optimization efficient aspects preferable exact method similar accuracy 
experiment exact versus approximate parallel versus sequential optimization methods 
exact methods shelf gradient optimization routine 
approximate methods perceptron boosting algorithm sequence labelling update feature weights parallel sequentially respectively 
discriminative modeling label sequences learning label sequence learning formally problem learning function maps sequence observations label sequence set individual labels 
example pos tagging words construct sentence labelling sentence part speech tag word interested supervised learning setting corpus order learn classifier 
popular model label sequence learning hidden markov model hmm 
hmm generative model trained finding joint probability distribution observation la bel sequences explains corpus best 
model random variable assumed independent random variables parents 
long distance dependencies natural languages modeled sequences conditional independence assumption violated nlp tasks 
shortcoming model due generative nature overlapping features difficult hmms 
reason hmms standardly current word current label previous label current label features 
incorporate information neighboring words information detailed characteristics current word directly model propagating previous labels may hope learn better classifier 
different models maximum entropy markov models memms mccallum projection markov models pmms punyakanok roth conditional random fields crfs lafferty proposed overcome problems :10.1.1.120.9821
common property models discriminative approach 
model probability distribution label sequences observation sequences best performing models label sequence learning memms pmms known maximum entropy models features carefully designed specific tasks ratnaparkhi toutanova manning 
maximum entropy models suffer called label bias problem problem making local decisions lafferty :10.1.1.120.9821
lafferty 
show crfs overcome label bias problem outperform memms pos tagging 
crfs define probability distribution sequence globally conditioning observation sequence 
condition observation opposed generating overlapping features 
features form 
current label information observation sequence identity spelling features word window word currently labelled 
features corresponds choice half window size 
current label neighbors label features capture inter label dependencies 
features corresponds choice neighbors bigram model conditional probability distribution defined model parameters estimated training corpus normalization term assure proper probability distribution 
order simplify notation number times feature observed pair linear combination features parameterization 
sufficient statis tic rewrite loss functions label sequences theoretical advantages discriminative models generative models empirical support klein manning crfs state art discriminative models label sequences chose crfs model trained optimizing various objective functions respect corpus application models label sequence problems vary widely 
individual labels constitute chunks named entity recognition shallow parsing may single entries pos tagging 
difficulty accuracy tasks different 
evaluation systems differ task nature statistical noise level task corpus dependent 
variety objective functions tailored task result better classifiers 
consider dimensions designing objective functions exponential versus logarithmic loss functions sequential versus pointwise optimization functions 
exponential vs logarithmic loss functions estimation procedures nlp proceed maximizing likelihood training data 
loss exp loss loss penalization loss functions log loss loss values exp log loss functions binary classification problem overcome numerical problems working product large number small probabilities usually logarithm likelihood data optimized 
time systems sequence labelling systems particular tested respect error rate test data fraction times function assigns higher score label sequence correct label sequence observation test data 
rank loss natural objective minimize 
ranks higher correct label sequences training instances corpus optimizing rank loss np complete optimize upper bound exponential loss function total number label sequences exponential loss function studied machine learning domain 
advantage exp loss log loss property penalizing incorrect labellings severely penalizes label sequence correct 
desirable property classifier 
shows property exp loss contrast log loss binary classification problem 
property means disadvantage sensitive noisy data systems optimizing exp loss spends effort outliers tend vulnerable noisy data especially label noise 
sequential vs pointwise loss functions applications difficult get label sequence correct time classifiers perfect sequences get longer probability predicting label sequence correctly decreases exponentially 
reason performance usually measured pointwise terms number individual labels correctly predicted 
common optimization functions literature treat label sequence label penalizing label sequence error label sequence wrong manner 
may able develop better classifiers loss function similar evaluation function 
possible way accomplishing may minimizing pointwise loss functions 
sequential optimizations optimize joint conditional probability distribution pointwise op propose optimize marginal conditional probability distribution loss functions derive loss functions cross product dimensions discussed sequential log loss function function standard maximum likelihood optimization crfs lafferty :10.1.1.120.9821
sequential exp loss function loss func tion introduced collins nlp tasks structured output domain 
sum possible label sequence set best label sequences generated external mechanism 
include possible label sequences require external mechanism identify best sequences 
shown altun possible sum label sequences dynamic algorithm 
note exponential loss function just inverse conditional probability plus constant 
pointwise log loss function function op marginal probability labels position conditioning observation sequence obviously function reduces sequential log loss length sequence pointwise exp loss function parallelism log loss vs exp loss functions sequential optimization log vs inverse conditional probability propose minimizing pointwise exp loss function reduces standard multi class exponential loss length sequence comparison loss functions compare performance loss functions described 
lafferty proposes modification iterative scaling algorithm parameter estimation sequential log loss function optimization methods efficient minimizing convex loss function eq :10.1.1.120.9821
minka 
reason gradient method optimize loss functions 
gradient optimization gradients loss function computed follows sequential log loss function expectations taken optimum empirical expected values sufficient statistics equal 
loss function derivatives calculated pass forwardbackward algorithm 
sequential exp loss function optimum empirical values sufficient statistics equals conditional expectations contribution instance weighted inverse conditional probability instance 
loss function focuses examples lower conditional probability usually examples model labels incorrectly 
computational complexity log loss case 
pointwise log loss function optimum expected value sufficient statistics conditioned observation equal expected value conditioned correct label sequence computations done dynamic programming described computational complexity forward backward algorithm scaled constant 
pointwise exp loss function optimum expected value sufficient statistics conditioned equal value conditioned point weighted com putational complexity logloss case 
experimental setup presenting experimental results comparison loss functions described describe experimental setup 
ran experiments part speech pos tagging named entity recognition ner tasks 
pos tagging penn treebank corpus 
individual labels corpus 
convention pos tagging tag dictionary frequent words 
sections training section testing 
ner spanish corpus provided special session conll ner 
training test data sets training data consists sentences 
individual label set corpus consists labels continuation person organization location miscellaneous names tags 
different feature sets set bigram features current tag current word current tag previous tags 
consists features spelling features current word current word capitalized current tag person 
spelling features adapted bikel letters word letter lower case upper case alphanumeric word capitalized contains dot letters capitalized word contains hyphen 
includes features current word words fixed window size instance example features previous word ends dot current tag organization intermediate 
pos table accuracy pos tagging penn tree bank 
ner window size considered features previous words 
penn treebank large including features incorporating information neighboring words directly model intractable 
limited experiments features pos tagging 
experimental results gradient optimization method shelf optimization tool uses limited memory updating method 
observed method faster converge conjugate gradient descent method 
known optimizing log loss functions may result overfitting especially noisy data 
reason regularization term cost functions 
experimented different regularization terms 
expected observed regularization term increases accuracy especially training data small observe difference different regularization terms 
results report gaussian prior regularization term described johnson 
goal build best tagger recognizer compare different loss functions optimization methods 
spend effort designing useful features results slightly worse comparable best performing models 
extracted corpora different sizes ranging sentences complete corpus ran experiments optimizing loss functions different feature sets 
table table report accuracy predicting individual label 
seen test accuracy obtained different loss functions lie relatively small range best performance depends kind features included model 
ner table measure ner spanish newswire corpus 
window size observed similar behavior training set smaller 
accuracy highest features included model 
results conclude model optimizing different loss functions effect accuracy increasing variety features included model impact 
optimization methods section showed optimizing different loss function large impact accuracy 
section investigate different methods optimization 
conjugate method section exact method 
training corpus large training may take long time especially number features large 
method optimization done parallel fashion updating parameters time 
resulting classifier uses features included model lacks sparseness 
consider approximation methods optimize loss functions described 
perceptron algorithm labelling sequences 
algorithm performs parallel optimization approximation sequential log loss optimization 
boosting algorithm label sequence learning 
algorithm performs sequential optimization updating parameter time 
optimizes sequential exp loss function 
compare methods exact method experimental setup section 
perceptron algorithm label sequences calculating gradients expectations features instance training corpus computationally expensive corpus large 
cases single training instance informative corpus update parameters 
online algorithm updates training example may converge faster batch algorithm 
distribution peaked label contribution label dominates expectation values 
assume case viterbi assumption calculate approximation gradients considering best label sequence current model 
online perceptron algorithm algorithm collins uses approximations algorithm label sequence perceptron algorithm initialize repeat training patterns compute stopping criteria iteration perceptron algorithm calculates approximation gradient sequential log loss function eq 
current training instance 
batch version algorithm closer approximation optimization sequential log loss approximation viterbi assumption 
stopping criteria may convergence fixed number iterations training data 
boosting algorithm label sequences original boosting algorithm adaboost schapire singer sequential learning algorithm induce classifiers single random variables 
altun presents boosting algorithm learning classifiers predict label sequences 
algorithm minimizes upper bound sequential exp loss function eq 

adaboost distribution observations defined ner perceptron boosting table different methods ner distribution expresses importance training instance updated round algorithm focuses difficult examples 
sequence boosting algorithm algorithm optimizes upper bound sequential function convexity exponential function 
maximum difference sufficient statistic label sequence correct label sequence observation similar meaning 
algorithm label sequence boosting algorithm 
initialize repeat features update stopping criteria seen line algorithm feature added ensemble round determined function gradient sequential exp loss function eq 

round pass forward backward algorithm training data sufficient calculate considering sparseness features training instance restrict forward backward pass training instances contain feature added ensemble round 
stopping criteria may fixed number rounds cross validation heldout corpus 
experimental results results summarized table compares perceptron boosting algorithm gradient method 
performance standard perceptron algorithm fluctuates lot average perceptron stable 
report results average perceptron 
surprisingly slightly worse crf approximation crfs 
advantage perceptron algorithm dual formulation 
dual form explicit feature mapping avoided kernel trick large number features efficiently 
seen previous sections ability incorporate features big impact accuracy 
dual perceptron algorithm may large advantage methods 
hmm features boosting sequential algorithm performs worse gradient method optimizes parallel fashion 
information hmm features identity word labeled 
boosting algorithm needs include features ensemble 
just informative features boosting algorithm better 
situation dramatic pos tagging 
boosting gets accuracy features gradient method gets 
gradient method uses available features boosting uses features 
due loose upper bound boosting optimizes estimate updates conservative 
features selected times 
negatively effects convergence time methods outperform boosting terms training time 
investigated different objective functions optimization methods affect accuracy sequence labelling task discriminative learning framework 
experiments show optimizing different objective functions large affect accuracy 
extending feature space effective 
con clude methods large possibly infinite number features may advantageous 
running experiments dual formulation perceptron algorithm property able infinitely features 
includes svms label sequence learning task 
altun hofmann johnson 

discriminative learning label sequences boosting 
proceedings nips 
daniel bikel richard schwartz ralph weischedel 

algorithm learns name 
machine learning 
collins 

discriminative reranking natural language parsing 
proceedings icml 
collins 

ranking algorithms named entity extraction boosting voted perceptron 
proceedings acl 
johnson geman canon chi riezler 

estimators stochastic unification grammars 
proceedings acl 
teh roweis 

alternative objective function markovian fields 
proceedings icml 
dan klein christopher manning 

conditional structure versus conditional estimation nlp models 
proceedings emnlp 
lafferty mccallum pereira 

conditional random fields probabilistic models segmenting labeling sequence data 
proceedings icml 
mccallum freitag pereira 

maximum entropy markov models information extraction segmentation 
proceedings icml 
minka 

algorithms maximum likelihood logistic regression 
technical report cmu department statistics tr 
punyakanok roth 

classifiers sequential inference 
proceedings nips 
adwait ratnaparkhi 

learning parse natural language maximum entropy models 
machine learning 
schapire singer 

improved boosting algorithms confidence rated predictions 
machine learning 
kristina toutanova christopher manning 

enriching knowledge sources maximum entropy pos tagger 
proceedings emnlp 
