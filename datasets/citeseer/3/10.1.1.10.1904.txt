ieee journal selected areas communications vol 
january tapestry resilient global scale overlay service deployment ben zhao ling huang jeremy stribling sean rhea anthony joseph member ieee john kubiatowicz member ieee tapestry peer peer overlay routing infrastructure offering efficient scalable routing messages directly nearby copies object service localized resources 
tapestry supports generic decentralized object location routing dolr api self repairing routing layer 
presents tapestry architecture algorithms implementation 
explores behavior tapestry deployment planetlab global testbed approximately machines 
experimental results show tapestry exhibits stable behavior performance overlay despite instability underlying network layers 
widely distributed applications implemented tapestry illustrating utility deployment infrastructure 
index terms overlay networks peer peer service deployment tapestry 
internet developers constantly proposing new visionary distributed applications 
new applications variety requirements availability durability performance 
technique achieving properties adapt failures changes load migration replication data services 
unfortunately ability place replicas frequency may moved limited underlying infrastructure 
traditional way deploy new applications adapt existing infrastructures imperfect match standardize new internet protocols encountering significant inertia deployment 
flexible standardized substrate develop new applications needed 
article tapestry extensible infrastructure provides decentralized object location routing dolr :10.1.1.111.1818:10.1.1.111.1818:10.1.1.1.4310:10.1.1.1.4310
dolr interface focuses routing messages endpoints nodes supported part national science foundation nsf career award ani career award ani part nsf information technology research itr award part california micro fund award award part ibm sprint 
zhao huang rhea joseph kubiatowicz university california berkeley ca usa mail eecs berkeley edu 
stribling massachusetts institute technology cambridge ma usa mail mit edu 
object replicas 
dolr virtualizes resources endpoints named opaque identifiers encoding physical location 
properly implemented virtualization enables message delivery mobile replicated endpoints presence instability underlying infrastructure 
result dolr network provides simple platform implement distributed applications developers ignore dynamics network optimization 
tapestry enabled deployment storage applications oceanstore multicast distribution systems bayeux return section vi :10.1.1.12.2441
tapestry peer peer overlay network provides high performance scalable routing messages close endpoints localized resources 
focus routing brings desire efficiency minimizing message latency maximizing message throughput 
instance tapestry exploits locality routing messages mobile endpoints object replicas behavior contrast structured peer peer overlay networks 
tapestry uses adaptive algorithms soft state maintain fault tolerance face changing node membership network faults 
architecture modular consisting extensible upcall facility wrapped simple high performance router 
applications programming interface api enables developers develop extend overlay functionality basic dolr functionality insufficient 
pages describe java implementation tapestry macro benchmarks actual deployed system 
normal operation relative delay penalty rdp locate mobile endpoints wide area 
simulations show tapestry operations succeed nearly time constant network changes massive failures joins small periods degraded performance self rdp stretch ratio distance traveled message endpoint minimal distance source endpoint 
ieee journal selected areas communications vol 
january repair 
results demonstrate tapestry feasibility long running service dynamic failure prone networks wide area internet 
section discusses related 
tapestry core algorithms appear section iii details architecture implementation section iv 
section evaluates tapestry performance 
discuss tapestry application infrastructure section vi conclude section vii 
ii 
related generation peer peer systems included file sharing storage applications napster gnutella freenet 
napster uses central directory servers locate files 
gnutella provides similar distributed service scoped broadcast queries limiting scalability 
uses online economic model encourage sharing resources 
freenet file sharing network designed resist censorship 
gnutella freenet guarantee files located functioning network 
second generation peer peer systems structured peer peer overlay networks including tapestry chord pastry :10.1.1.111.1818:10.1.1.111.1818:10.1.1.1.4310:10.1.1.1.4310
overlays implement basic key routing interface supports deterministic routing messages live node responsibility destination key 
support higher level interfaces distributed hash table dht decentralized object location routing dolr layer 
systems scale guarantee queries find existing objects non failure conditions 
differentiating property systems chord take network distances account constructing routing overlay overlay hop may span diameter network 
protocols route shortest overlay hops available runtime heuristics assist 
contrast tapestry pastry construct locally optimal routing tables initialization maintain order reduce routing stretch 
systems fix number location object replicas providing distributed hash table dht interface tapestry allows applications place objects needs 
tapestry publishes location pointers network facilitate efficient routing objects low network stretch 
technique tapestry queries nearby objects generally satisfied time proportional distance query source nearby object replica 
pastry tapestry share similarities plaxton richa static network 
explore distributed object location schemes provably low search overhead require precomputation suitable dynamic networks 
works include systems kademlia uses xor overlay routing viceroy provides logarithmic hops nodes constant degree routing tables 
skipnet uses multi dimensional skip list data structure support overlay routing maintaining dns namespace operational locality randomized namespace network locality 
overlay proposals attain lower bounds local routing state 
proposals differentiate local inter domain routing reduce wide area traffic routing latency 
new generation applications proposed top systems validating novel application infrastructures 
systems application level multicast mc scribe pastry bayeux tapestry 
addition decentralized file systems proposed cfs chord chord tapestry oceanstore tapestry past pastry :10.1.1.12.2441
structured overlays support novel applications attack resistant networks network indirection layers similarity searching 
iii 
tapestry algorithms section details tapestry algorithms routing object location describes network integrity maintained dynamic network conditions 
dolr networking api tapestry provides datagram communications interface additional mechanisms manipulating locations objects 
describing api start couple definitions 
tapestry nodes participate overlay assigned nodeids uniformly random large identifier space 
node may hosted physical host 
application specific endpoints assigned globally unique identifiers guids selected identifier space 
tapestry currently uses identifier space bit values globally defined radix hexadecimal yielding digit identifiers 
tapestry assumes nodeids guids roughly evenly distributed namespace achieved secure hashing algorithm sha 
say node nodeid object guid 
efficiency tapestry generally improves network size advantageous multiple applications share single large tapestry overlay network 
zhao tapestry resilient global scale overlay service deployment ab af fig 

tapestry routing mesh perspective single node 
outgoing neighbor links point nodes common matching prefix 
higher level entries match digits 
links form local routing table 
ac ad fig 

path message 
path taken message originating node destined node ad tapestry mesh 
enable application coexistence message contains application specific identifier select process application message delivery destination similar role port tcp ip upcall handler appropriate 
definitions state part dolr networking api follows publish available object local node 
call best effort receives confirmation 
best effort attempt remove location mappings 
routes message location object guid 
exact route message application node exact specifies destination id needs matched exactly deliver payload 
routing object location tapestry dynamically maps identifier unique live node called identifier root 
node exists node root 
deliver messages node maintains routing table consisting nodeids ip addresses nodes communicates 
refer nodes nexthop return self nil endwhile return nexthop return endif endif fig 

pseudocode nexthop 
function locates hop root previous hop number destination guid 
returns hop self local node root 
neighbors local node 
routing messages forwarded neighbor links nodes nodeids progressively closer matching larger prefixes id space 
routing mesh tapestry uses local tables node called neighbor maps route overlay messages destination id digit digit ad represent wildcards 
approach similar longest prefix routing cidr ip address allocation 
node neighbor map multiple levels level contains links nodes matching prefix digit position id contains number entries equal id base 
primary entry level id location closest node begins prefix entry level node ae closest node id begins 
prescription closest node provides locality properties tapestry 
shows outgoing links node 
shows path message take infrastructure 
router hop shares prefix length destination id route tapestry looks level map entry matching digit destination id method guarantees existing node system reached logical hops system namespace size ids base assuming consistent neighbor maps 
digit matched tapestry looks close digit routing table call surrogate routing non existent id mapped live node similar id details nexthop function chosing outgoing link :10.1.1.111.1818:10.1.1.111.1818
dynamic process maps identifier unique root node 
ieee journal selected areas communications vol 
january phil books fe location mapping ec phil books aa publish path tapestry pointers fig 

tapestry object publish example 
copies object published root node 
publish messages route root depositing location pointer object hop encountered way 
challenge dynamic network environment continue route reliably intermediate links changing faulty 
help provide resilience exploit network path diversity form redundant routing paths 
primary neighbor links shown augmented backup links sharing prefix routing level neighbor links differ digit 
pointers level total size neighbor map 
node stores reverse backpointers nodes point 
expected total number entries 
object publication location shown identifier unique root node assigned routing process 
root node inherits unique spanning tree routing messages leaf nodes traversing intermediate nodes en route root 
utilize property locate objects distributing soft state directory information nodes including object root 
server storing object guid root periodically advertises publishes object routing publish message see 
general nodeid different unique node reached surrogate routing successive calls nexthop :10.1.1.1.4310:10.1.1.1.4310
node publication path stores pointer mapping copy object 
replicas object separate servers server publishes copy 
tapestry nodes store location mappings object replicas sorted order network latency 
client locates routing message see current implementations keep additional backups 
note objects assigned multiple guids mapped different root nodes fault tolerance 
phil books fe location mapping ec phil books aa query path tapestry pointers fig 

tapestry route object example 
nodes send messages object different points network 
messages route root node 
intersect publish path follow location pointer nearest copy object 

node path checks location mapping redirects message forwards message onwards guaranteed location mapping 
hop root reduces number nodes satisfying hop prefix constraint factor identifier base 
messages sent destination nearby nodes generally cross paths quickly hop increases length prefix required hop path root function destination id source nodeid chord neighbor hops chosen network locality usually transitive 
closer network distance client object sooner queries cross paths object publish path faster reach object 
nodes sort object pointers distance queries routed nearby object replicas 
dynamic node algorithms tapestry includes number mechanisms maintain routing table consistency ensure object availability 
section briefly explore mechanisms 
see complete algorithms proofs :10.1.1.1.4310
majority control messages described require acknowledgments retransmitted required 
node insertion components inserting new node tapestry network need know nodes notified fills null entry routing tables 
new object root existing objects 
objects moved maintain object availability 
algorithms construct near optimal routing table zhao tapestry resilient global scale overlay service deployment nodes near notified may consider routing tables optimization 
node insertion begins surrogate root node maps existing network 
finds length longest prefix id shares 
sends acknowledged multicast message reaches set existing nodes sharing prefix traversing tree nodeids 
nodes receive message add routing tables transfer locally rooted pointers necessary completing items 
nodes reached multicast contact initial neighbor set routing table construction 
performs iterative nearest neighbor search routing level 
uses neighbor set fill routing level list closest nodes requests nodes send backpointers see section iii level 
resulting set contains nodes point nodes previous routing level neighbor set 
decrements repeats process levels filled 
completes item 
nodes contacted iterative algorithm optimize routing tables applicable completing item 
ensure nodes inserting network unison fail notify existence node multicast keeps state node multicasting neighbors 
state tell node multicast tree 
additionally multicast message includes list holes new node routing table 
nodes check tables routing table notify new node entries fill holes 
voluntary node deletion node leaves tapestry voluntarily tells set nodes backpointers intention replacement node routing level routing table 
notified nodes send object republish traffic replacement 
routes locally rooted objects new roots signals nodes finished 
involuntary node deletion dynamic network wide area internet nodes generally exit network far gracefully due node link failures network partitions may enter leave times short interval 
tapestry improves object availability routing environment building redundancy routing tables object location backup forwarding pointers routing table entry 
ongoing knob tuning tradeoff resources optimality resulting routing table 
dynamic node management single tapestry node decentralized application level collaborative file system multicast text filtering application interface upcall api routing table object pointer database neighbor link management transport protocols router fig 

tapestry component architecture 
messages pass physical network layers application layers 
router central conduit communication 
shown tapestry viability resilient routing layer 
maintain availability redundancy nodes periodic beacons detect outgoing link node failures 
events trigger repair routing mesh initiate redistribution replication object location 
furthermore repair process augmented soft state republishing object 
tapestry repair highly effective shown section 
despite continuous node turnover tapestry retains nearly success rate routing messages nodes objects 
iv 
tapestry node architecture implementation section architecture tapestry node api tapestry extension details current implementation architecture higher performance implementation suitable network processors 
component architecture illustrates functional layering tapestry node 
shown top applications interface rest system tapestry api 
router dynamic node management components 
processes routing location messages handles arrival departure nodes network 
components communicate routing table 
bottom transport neighbor link layers provide cross node messaging layer 
describe layers 
transport transport layer provides abstraction communication channels overlay node corresponds layer osi layering 
utilizing native operating system os functionality channel implementations possible 
ieee journal selected areas communications vol 
january receive new location msg upcall 
signal application object pointers 
forward nexthop closest forward nexthop fig 

message processing 
object location requests enter neighbor link layer left 
messages forwarded extensibility layer router looks object pointers forwards message hop 
currently support uses tcp ip uses udp ip 
neighbor link transport layer neighbor link layer 
provides secure unreliable datagram facilities layers including fragmentation reassembly large messages 
time higher layer wishes communicate node provide destination physical address ip address port number 
secure channel desired public key remote node may provided 
neighbor link layer uses information establish connection remote node 
links opened demand higher levels tapestry 
avoid overuse scarce operating system resources file descriptors neighbor link layer may periodically close connections 
closed connections demand 
important function layer continuous link monitoring adaptation 
provides fault detection soft state keep alive messages plus latency loss rate estimation 
neighbor link layer notifies higher layers link properties change significantly 
layer optimizes message processing parsing message headers message contents required 
avoids byte copying user data operating system java virtual machine boundary possible 
node authentication message authentication codes macs integrated layer additional security 
router neighbor link layer provides basic networking facilities router implements functionality unique tapestry 
included layer routing table local object pointers 
discussed section iii routing mesh prefix sorted list neighbors stored node routing table 
router examines destination guid messages passed determines hop table local object pointers 
passed back neighbor link layer delivery 
shows flow chart object location process 
messages arrive neighbor link layer left 
messages trigger extension upcalls discussed section iv immediately invoke upcall handlers 
local object pointers checked match guid located 
match message forwarded closest node set matching pointers 
message forwarded hop root 
note routing table object pointer database continuously modified dynamic node management neighbor link layers 
instance response changing link latencies neighbor link layer may reorder preferences assigned neighbors occupying entry routing table 
similarly dynamic node management layer may add remove object pointers arrival departure neighbors 
tapestry upcall interface dolr api section iii provides powerful applications interface functionality multicast requires greater control details routing object lookup 
accommodate tapestry supports extensible upcall mechanism 
expect overlay infrastructures mature need customization give way set tested commonly routing behaviors 
interaction tapestry application handlers occurs primary calls generic id nodeid guid deliver msg invoked incoming messages destined local node 
asynchronous returns immediately 
application generates events invoking route 
forward msg invoked incoming upcall enabled messages 
application call route order forward message 
route msg invoked application handler forward message 
additional interfaces provide access routing table object pointer database 
upcall enabled message arrives tapestry sends message application forward 
handler responsible calling route final destination 
tapestry invokes deliver messages destined local node complete routing 
upcall interface provides sufficient functionality implement instance bayeux multicast system 
messages marked trigger upcalls hop tapestry invokes forward call message 
bayeux handler examines membership list sorts groups forwards copy message outgoing entry 
zhao tapestry resilient global scale overlay service deployment enter leave tapestry applications api calls calls application programming interface state maintenance routing table node membership node insert delete core router maintenance mesh repair route node obj 
link repair control messages node insert delete control messages network stage udp pings periodic beacons patchwork distance measurements seda event driven framework async 
java virtual machine link monitoring requests link quality changes fig 

tapestry implementation 
tapestry implemented java series independently scheduled stages shown bubbles interact passing events 
implementation follow discussion tapestry component architecture detailed look current implementation choices rationale 
tapestry currently implemented java consists roughly lines code source files 
implementation tapestry node tapestry implemented event driven system high throughput scalability 
paradigm requires asynchronous layer efficient model internal communication control components 
currently leverage event driven seda application framework requirements :10.1.1.130.8002:10.1.1.20.2080
seda internal components communicate events subscription model 
shown components core router node membership mesh repair patchwork network stage 
network stage corresponds combination neighbor link layer portions transport layer general architecture 
implements parts neighbor communication abstraction provided operating system 
responsible buffering dispatching messages higher levels system 
network stage interacts closely patchwork monitoring facility discussed measure loss rates latency information established communication channels 
core router utilizes routing object tables handle application driven messages including object publish object location routing messages destination nodes 
router interacts application layer application interface upcalls 
core router critical path messages entering exiting system 
show section implementation reasonably efficient 
tapestry algorithms amenable fast path optimization increase throughput decrease latency discuss section iv 
supporting router dynamic components deterministic node membership stage soft state mesh repair stage 
manipulate routing table object table 
node membership stage responsible handling integration new nodes tapestry mesh graceful voluntary exit nodes 
stage responsible starting new node correct routing table reflecting correctness network locality 
contrast mesh repair stage responsible adapting tapestry mesh network environment changes 
includes responding changes quality network links including links failures adapting catastrophic loss neighbors updating routing table account slow variations network latency 
repair process actively redistributes object pointers network conditions change 
repair process viewed event triggered adjustment state combined continuous background restoration routing object location information 
provides quick adaptation faults evolutionary changes providing eventual recovery problems 
patchwork stage uses soft state beacons probe outgoing links reliability performance allowing tapestry respond failures changes network topology 
supports asynchronous latency measurements nodes 
tightly integrated network native transport mechanisms channel acknowledgments possible 
implemented tcp udp network layers 
tcp supports flow congestion control behaving fairly presence flows 
disadvantages long connection setup tear times sub optimal usage available bandwidth consumption file descriptors limited resource 
contrast udp messages sent low overhead may utilize available bandwidth network link 
udp support flow control congestion control consume unfair share bandwidth causing wide spread congestion widearea 
correct udp layer includes congestion control limited retransmission capabilities 
exploring advantages disadvantages protocol fact udp layer consume file descriptors appears significant advantage deployment stock operating systems 
node virtualization enable wider variety experiments place multiple tapestry node instances physical machine 
minimize memory computational overhead maximizing number instances physical machine run node instances inside single java virtual machine ieee journal selected areas communications vol 
january guid location match bloom filter 
return nil match disk 
pointer cache 
async return match fig 

enhanced pointer lookup 
quickly check object pointers bloom filter eliminate definite non matches memory cache check pointers 
fail asynchronously fall back slower repository 
jvm 
technique enables execution simultaneous instances tapestry single node virtual nodes physical machine share single jvm execution thread virtual node executes time 
virtual instances share code instance maintains exclusive nonshared data 
side effect virtualization delay introduced cpu scheduling nodes 
periods high cpu load scheduling delays significantly impact performance results artificially increase routing location latency results 
exacerbated unrealistically low network distances nodes machine 
node instances exchange messages microseconds making overlay network processing overhead scheduling delay expensive comparison 
factors considered interpreting results discussed section higher performance implementation section show implementation handle messages second 
commercial quality implementation better 
close section important observation despite advanced functionality provided dolr api critical path message routing amenable high performance optimization available dedicated routing hardware 
critical path routing shown consists distinct pieces 
simplest piece computation nexthop similar functionality performed hardware routers fast table lookup 
node network base routing table guid ip address entry expected size kilobytes smaller cpu cache 
simple arguments show network hops involve single lookup final hops require lookups :10.1.1.111.1818:10.1.1.111.1818:10.1.1.111.1818
run virtual nodes machine stress network virtualization limit 
result second aspect dolr routing fast pointer lookup presents greatest challenge high throughput routing 
router route request passes query table pointers 
pointers fit memory simple hashtable lookup provides complexity lookup 
number pointers quite large global scale deployment furthermore fast memory resources hardware router smaller state art workstations 
address issue note routing hops receive negative lookup results receives successful result 
imagine building bloom filter set pointers :10.1.1.20.2080
bloom filter lossy representation set detect absence member set quite quickly 
size bloom filter adjusted avoid go details reasonable size bloom filter pointers bits 
assuming pointers information bytes memory footprint bloom filter orders magnitude smaller total size pointers 
consequently propose enhancing pointer lookup 
addition bloom filter front includes cache active pointers large fit memory router 
primary point split lookup process fast negative check followed fast check objects active followed slower 
say disk fallback repository memory companion processor consulted hardware router fails 
evaluation evaluate implementation tapestry platforms 
run micro benchmarks local cluster measure large scale performance deployed tapestry planetlab global testbed local network simulation layer support controlled repeatable experiments tapestry instances 
evaluation methodology short description experimental methodology 
experiments java tapestry implementation see section iv running ibm jdk node virtualization see section 
micro benchmarks run local cluster machines dual pentium iii ghz servers gbyte ram pentium iv ghz servers gbyte ram 
zhao tapestry resilient global scale overlay service deployment count planetlab internode latency distribution internode rtt ping ms buckets fig 

planetlab ping distribution histogram representation pair wise ping measurements planetlab global testbed 
run wide area experiments planetlab network testbed consisting roughly machines institutions north america europe asia australia 
machines include ghz pentium iii xeon servers gbyte ram ghz pentium iv towers gbyte ram 
roughly thirds planetlab machines connected high capacity internet network 
measured distribution pair wise ping distances plotted histogram 
planetlab real network constant load frequent data loss node failures 
perform wide area experiments infrastructure approximate performance real deployment conditions 
node planetlab tests runs test member stage listens network commands sent central test driver 
note results experiments node virtualization may skewed processing delays associated sharing cpus node instances machine 
instances need large scale repeatable controlled experiments perform experiments simple oceanstore simulator 
event driven network layer simulates network time queues driven single local clock 
injects artificial network transmission delays input network topology allows large number tapestry instances execute single machine minimizing resource consumption 
note model packet loss network queuing effects cross traffic 
performance stable network examine tapestry performance stable static network conditions 
transmission time ms msg processing latency local lan iii ghz local iv ghz local iii scaleup message size kb fig 

message processing latency 
processing latency full turnaround time message single tapestry overlay hop function message payload size 
bandwidth mb maximum routing throughput local lan iv ghz local iii ghz local iv ghz mb message size kb fig 

max routing throughput 
maximum sustainable message traffic throughput function message size 
micro benchmarks stable tapestry microbenchmarks network nodes isolate tapestry message processing overhead 
sender establishes binary network receiver sends stream messages message size 
receiver measures latency size inter arrival time messages 
eliminate network delay measure raw message processing placing nodes different ports machine 
see performance scales processor speed perform tests iii ghz machine iv ghz machine 
latency results show small messages dominant constant processing time approximately milliseconds iv iii 
messages larger kb cost copying data memory buffer network layer dominates processing time linear relative message size 
raw estimate processors ieee journal selected areas communications vol 
january rdp min median th percentile routing relative delay penalty planetlab median th percentile internode round trip ping time ms buckets fig 

rdp routing nodes 
ratio tapestry routing node versus shortest roundtrip ip distance sender receiver 
reported metric linux shows iv times faster 
see routing latency changes proportionally increase processor speed meaning fully leverage moore law faster routing 
measure corresponding routing throughput 
expected shows throughput low small messages processing overhead dominates quickly increases messages increase size 
average kb tapestry message iv process messages second iii processes messages second 
gap estimate get calculating inverse message routing latency attributed scheduling queuing delays layer 
measure throughput ghz iv connected mbit ethernet link 
results show maximum bandwidth utilized kb sized messages 
routing overhead nodes objects examine performance routing node routing object location stable network conditions tapestry nodes evenly distributed planetlab machines 
performance metric relative delay penalty rdp ratio routing overlay shortest ip network distance 
note shortest distance values measured icmp ping commands incur data copying scheduling delays 
graphs see figures plot percentile value median minimum 
compute rdp node routing measuring pairs roundtrip routing latencies tapestry instances dividing correspond rdp min median th percentile location relative delay penalty planetlab th percentile client object round trip ping time ms buckets fig 

rdp routing objects 
ratio tapestry routing object versus shortest way ip distance client object location 
th percentile rdp effect optimization routing objects rdp simulator unoptimized opt back near hop opt back near hop opt back near hop opt back near hop rdp client object round trip ping time fig 

percentile rdp routing objects optimization 
line represents set optimization parameters backups nearest neighbors hops cost additional pointers object brackets 
ing ping roundtrip time see median values node node routing rdp start slowly decrease 
multiple tapestry instances machine means tests heavy load produce scheduling delays instances resulting inflated rdp short latency paths 
exacerbated virtual nodes machine yielding unrealistically low roundtrip ping times 
measure routing object rdp ratio way tapestry route object latency versus way network latency ping time 
experiment place randomly named objects single server planetlab stanford edu 
tapestry nodes unison send messages roundtrip routing tapestry may asymmetric paths direction case ip routing 
zhao tapestry resilient global scale overlay service deployment integration latency ms single node integration latency planetlab size existing network nodes fig 

node insertion latency 
time single node insertion initial request message network stabilization 
objects guid 
rdp values sorted ping values collected millisecond bins percentile median values calculated bin see 
object location optimization object location results large distances diverge significantly optimal ip latency short distances 
variance increases greatly short distances 
reason results quite simple extraneous hops taken routing short distances greater fraction ideal latency 
high variance indicates client server combinations consistently see non ideal performance tends limit advantages clients gain careful object placement 
fortunately greatly improve behavior storing extra object pointers nodes close object 
technique trades extra storage space network faster routing 
investigate tradeoff publishing additional object pointers backup nodes hop publish path nearest terms network distance neighbors current hop 
bound overhead simple optimizations applying hops path 
shows optimization benefits percentile local area routing objects rdp 
explore larger topology generated simulator transit stub topology nodes 
place objects tapestry nodes node route random objects various values demonstrates optimizations significantly lower rdp observed bulk requesters local area network distances 
instance simple addition pointers local area backup nearest hop greatly reduces total bandwidth integration kb node insertion bandwidth planetlab number nodes original network fig 

node insertion bandwidth 
total control traffic bandwidth single node insertion 
convergence time ms min median convergence time parallel node insertion planetlab ratio nodes inserting unison network size fig 

parallel insertion convergence 
time network stabilize nodes inserted parallel function ratio nodes parallel insertion size stable network 
observed variance rdp 
convergence network dynamics analyze tapestry scalability stability dynamic conditions 
single node insertion measure overhead required single node join tapestry network terms time required network stabilize insertion latency control message bandwidth insertion control traffic bandwidth 
shows insertion time function network size 
datapoint construct tapestry network size repeatedly insert delete single node times 
node maintains routing state logarithmically proportional network size expect latency scale similarly network size 
confirms shows latencies scale sublinearly size network 
ieee journal selected areas communications vol 
january successful lookups num 
nodes routing nodes fail join simulator nodes success rate time minutes bw link fig 

route node failure joins 
performance tapestry route node massive network membership change events 
starting nodes nodes fail followed minutes massive join nodes 
bandwidth control messages important factor tapestry scalability 
small networks node knows network size nodes touched insertion corresponding bandwidth scale linearly network size 
shows total bandwidth single node insertion scales logarithmically network size 
reduced guid base order better highlight logarithmic trend network sizes 
control traffic costs include distance measurements nearest neighbor calculations routing table generation 
total bandwidth scales bandwidth seen single link node significantly lower 
parallel node insertion measure effects multiple nodes simultaneously entering tapestry examining convergence time parallel insertions 
starting stable network size nodes repeat parallel insertion times plot minimum median percentile values versus ratio nodes simultaneously inserted see 
note median time converge scales roughly linearly number simultaneously inserted nodes value fluctuate significantly ratios equal greater 
increase attributed effects node virtualization 
significant portion virtual tapestry instances involved node insertion scheduling delays compound result significant delays message handling resulting node insertion 
continuous convergence self repair wanted examine large scale performance bandwidth ip link kbytes successful requests num 
nodes routing objects fail join simulator success rate nodes time minutes bw link fig 

route object failure joins 
performance tapestry route objects massive network membership change events 
starting nodes nodes fail followed minutes massive join nodes 
controlled failure conditions 
experiments measured performance terms latency tests focused large scale behavior failures 
performed experiments simulation framework allows tapestry instances run single machine 
tests wanted examine success rates routing nodes objects modes network change drastic changes network membership slow constant membership churn 
routing nodes test measures success rate sending requests random keys namespace map unique nodes network 
routing objects test sends messages previously published objects located servers guaranteed stay alive network 
performance metrics include amount bandwidth success rate defined percentage requests correctly reached destination 
figures demonstrate ability tapestry recover massive changes overlay network membership 
kill existing network wait minutes insert new nodes equal existing network 
expected small fraction requests affected large portions network fail 
results show faults detected tapestry recovers success rate quickly returns 
similarly massive join event causes dip success rate returns quickly 
note large join event bandwidth consumption spikes nodes exchange control messages integrate new nodes 
bandwidth levels routing tables repaired consistency restored 
bandwidth ip link kbytes zhao tapestry resilient global scale overlay service deployment successful requests num 
nodes routing nodes churn simulator churn churn nodes success rate bw link time minutes fig 

route node churn 
routing nodes churn periods starting nodes 
churn uses poisson process average inter arrival time seconds randomly kills nodes average lifetime minutes 
churn uses seconds minutes 
churn tests measure success rate requests set stable nodes constantly set dynamic nodes insertion failure rates driven probability distributions 
simulations start stable set nodes peaks nodes churn planetlab tests start nodes peaks 
test includes different level dynamicity 
churn insertion uses poisson distribution average inter arrival time seconds failure uses exponential distribution mean node lifetime minutes 
second churn increases dynamic rates insertion failure seconds minutes parameters respectively 
figures show impact constant change tapestry performance 
cases success rate requests constant churn rarely slightly 
imperfect measurements occur independently parameters churn showing tapestry operations succeed high probability high rates turnover 
measure success rate routing nodes different network changes planetlab testbed 
shows requests experience short dips reliability events massive failure large joins 
reliability dips node membership undergoes constant churn inter arrival times seconds average life times seconds recovers 
order support nodes planetlab udp networking layer run instance jvm killed independently 
note additional number jvms increases scheduling delays resulting request timeouts bandwidth ip link kbytes successful requests num 
nodes routing objects churn simulator churn churn nodes success rate bw link time minutes fig 

route object churn 
performance tapestry route objects periods churn starting nodes 
churn uses random parameters node seconds average lifetime minutes 
churn uses seconds minutes 
successful lookups route node planetlab nodes fail churn starts nodes join success rate nodes time minutes fig 

failure join churn planetlab 
impact network dynamics success rate route node requests 
size network virtualization increases 
experiments show tapestry highly resilient dynamic conditions providing nearoptimal success rate requests high churn rates quickly recovering massive membership change events minute 
demonstrate tapestry feasibility long running service dynamic networks wide area internet 
vi 
deploying applications tapestry previous sections explored implementation behavior tapestry 
shown tapestry provides stable interface variety network conditions 
continuing main theme discuss tapestry address challenges facing largescale applications 
bandwidth ip link kbytes number nodes ieee journal selected areas communications vol 
january increasing ubiquity internet application developers begun focus large scale applications leverage common resources network 
examples include application level multicast global scale storage systems traffic redirection layers resiliency security 
applications share new challenges wide area users find difficult locate nearby resources network grows size dependence distributed components means smaller mean time failures mtbf system 
example file sharing user want locate retrieve close replica file avoiding server network failures 
security important concern 
sybil attack attack user obtains large number identities mount collusion attacks :10.1.1.17.1073
tapestry addresses trusted public key infrastructure pki nodeid assignment 
limit damage subverted nodes tapestry nodes pairs routing messages neighbors verifying path taken 
proposes generalized mechanism thwart collusion routing redirection attacks :10.1.1.10.6059
tapestry support message authentication codes macs maintain integrity overlay traffic 
described section iii tapestry supports efficient routing messages named objects endpoints network 
scales logarithmically network size node routing state expected number overlay hops path 
additionally tapestry provides resilience server network failures allowing messages route backup paths 
shown tapestry provide resilient traffic routing layer 
applications achieve additional resilience replicating data multiple servers relying tapestry direct client requests nearby replicas 
variety different applications designed implemented deployed tapestry infrastructure 
oceanstore global scale highly available storage utility deployed planetlab testbed :10.1.1.12.2441
oceanstore servers tapestry disseminate encoded file blocks efficiently clients quickly locate retrieve nearby file blocks id despite server network failures 
applications include file system bayeux efficient self organizing application level multicast system decentralized spam filtering system utilizing similarity search engine implemented tapestry 
vii 
described tapestry overlay routing network rapid deployment new distributed applications services 
tapestry provides efficient scalable routing messages directly nodes objects large sparse address space 
architecture tapestry nodes highlighting mechanisms routing dynamic state repair showed mechanisms enhanced extensible api 
implementation tapestry running simulation global scale planetlab infrastructure 
explored performance adaptability tapestry implementation variety realworld conditions 
significantly tapestry behaves large percentage network changing 
simulations show tapestry performs near optimally faults small portion queries fail faulty wide area network 
routing efficient median rdp stretch starts factor nearby nodes rapidly approaches 
median rdp object location factor wide area 
simple optimizations shown bring median rdp factor 
general purpose applications built top tapestry authors 
believe wide scale tapestry deployment practical efficient useful variety applications 
acknowledgment hildrum roscoe reviewers oceanstore group insightful comments 
zhao kubiatowicz joseph tapestry infrastructure fault tolerant wide area location routing tech :10.1.1.111.1818
rep csd berkeley apr 
hildrum kubiatowicz rao zhao distributed object location dynamic network proceedings spaa canada aug pp :10.1.1.1.4310

dabek zhao druschel kubiatowicz stoica common api structured overlays proceedings iptps berkeley ca feb pp 

rhea eaton geels weatherspoon zhao kubiatowicz pond oceanstore prototype proceedings fast san francisco ca apr pp :10.1.1.12.2441

zhuang zhao joseph katz kubiatowicz bayeux architecture scalable fault tolerant wide area data dissemination proceedings nossdav port jefferson ny june pp 

ratnasamy francis handley karp schenker scalable content addressable network proceedings sigcomm san diego ca aug pp 

rowstron druschel pastry scalable distributed object location routing large scale peer peer systems proceedings middleware heidelberg germany nov pp 

stoica morris karger kaashoek balakrishnan chord scalable peer peer lookup service internet applications proceedings sigcomm san diego ca aug pp 

zhao tapestry resilient global scale overlay service deployment maymounkov mazieres kademlia peer peer information system xor metric proceedings iptps cambridge ma mar pp 

malkhi naor viceroy scalable dynamic emulation butterfly proceedings podc monterey ca pp 

harvey jones saroiu theimer wolman skipnet scalable overlay network practical locality properties proceedings usits seattle wa mar pp 

wilcox hearn experiences deploying large scale emergent network proceedings iptps cambridge ma mar pp 

clarke sandberg wiley hong freenet distributed anonymous information storage retrieval system international workshop design issues anonymity unobservability 
zhao joseph kubiatowicz mechanisms large scale networks proceedings international workshop directions distributed computing italy june 
plaxton rajaraman richa accessing nearby copies replicated objects distributed environment proceedings spaa newport ri june pp 

awerbuch peleg concurrent online tracking mobile users proceedings sigcomm zurich switzerland sep pp 

rajaraman richa data tracking scheme general networks proceedings spaa crete island greece july pp 

kaashoek karger koorde simple hash table proceedings iptps berkeley ca feb pp 

wieder naor simple fault tolerant distributed hash table proceedings iptps berkeley ca feb pp 

zhao duan huang joseph kubiatowicz landmark routing overlay networks proceedings iptps cambridge ma mar pp 

ratnasamy handley karp schenker application level multicast content addressable networks proceedings ngc london uk nov pp 

rowstron 
kermarrec druschel castro scribe design large scale event notification infrastructure proceedings ngc london uk nov pp 

dabek kaashoek karger morris stoica wide area cooperative storage cfs proceedings sosp banff canada oct pp 

hand roscoe peer peer steganographic storage proceedings iptps cambridge ca mar pp 

rowstron druschel storage management caching past large scale persistent peer peer storage utility proceedings sosp banff canada oct pp 

keromytis misra rubenstein sos secure overlay services proceedings sigcomm pittsburgh pa aug pp 

stoica zhuang shenker surana internet indirection infrastructure proceedings sigcomm pittsburgh pa aug pp 

zhou zhuang zhao huang joseph kubiatowicz approximate object location spam filtering peer peer systems proceedings middleware rio de janeiro brazil june pp 

matthew md md md sha hash functions tech 
rep tr rsa labs 
rekhter li architecture ip address allocation cidr rfc www isi edu notes rfc txt 
zhao huang stribling joseph kubiatowicz exploiting routing redundancy structured peerto peer overlays proceedings icnp atlanta ga nov pp 

welsh culler brewer seda architecture conditioned scalable internet services proceedings sosp banff canada oct pp :10.1.1.130.8002:10.1.1.20.2080

bloom space time trade offs hash coding allowable errors communications acm july vol :10.1.1.20.2080
pp 

rhea kubiatowicz probabilistic location routing proceedings infocom new york ny june pp 
vol 

douceur sybil attack proceedings iptps cambridge ma mar pp :10.1.1.17.1073

castro druschel ganesh rowstron wallach security structured peer peer overlay networks proceeding osdi boston ma dec pp :10.1.1.10.6059

