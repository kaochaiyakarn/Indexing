journal machine learning research submitted published generalization error bounds bayesian mixture algorithms ron meir ee technion ac il department electrical engineering technion haifa israel tong zhang watson ibm com ibm watson research center yorktown heights ny usa editors graepel ralf herbrich bayesian approaches learning estimation played significant role statistics literature years 
provably optimal frequentist setting lead excellent performance practical applications precise characterizations performance finite sample sizes general conditions 
consider class bayesian mixture algorithms estimator formed constructing data dependent mixture hypothesis space 
similarly observed practice results demonstrate mixture approaches particularly robust allow construction highly complex estimators avoiding undesirable overfitting effects 
results data dependent nature insensitive underlying model assumptions apply hold 
technical level approach applies unbounded functions constrained certain moment conditions 
bounds derived directly applied non bayesian mixture approaches boosting bagging 

motivation standard approach computational learning theory usually formulated called frequentist approach statistics 
paradigm interested constructing estimator finite sample possesses small loss generalization error 
algorithms constructed analyzed context clear approaches relate standard optimality criteria frequentist framework 
classic optimality criteria approach admissibility characterize optimality estimators rigorous precise fashion robert 
essentially measures performance best estimator worst possible distribution set distributions 
admissibility related extent estimator uniformly dominates estimators 
refer reader robert precise definitions notions play role sequel 
special cases yang known approaches machine learning community lead optimality senses word 
hand known certain regularity conditions bayesian ron meir tong zhang 
meir zhang estimators lead minimax admissible estimators defined optimality classical frequentist sense 
fact shown bayes estimators limits thereof essentially estimators achieve optimality senses robert 
optimality feature provides strong motivation study bayesian approaches frequentist setting 
bayesian approaches widely studied generally applicable finite sample bounds frequentist framework 
approaches attempted address problem 
establish finite sample data dependent bounds bayesian mixture methods optimality properties suggest approaches widely 
consider problem supervised learning attempt construct estimator finite sample pairs examples 
pair drawn independently random unknown distribution 
learning algorithm sample selects hypothesis estimator set hypotheses denoting instantaneous loss hypothesis wish assess true loss 
particular objective provide algorithm data dependent bounds form 
probability empirical assessment true loss complexity term 
example classic vapnik chervonenkis framework vapnik chervonenkis empirical error depends vc dimension independent hypothesis sample algorithm data dependent bounds mean bounds complexity term depends hypothesis chosen algorithm sample main contribution extension pac bayesian framework mcallester unified setting bayesian mixture methods different regularization criteria may incorporated effect performance easily assessed 
furthermore essential bounds obtained dimension independent yield useless results applied methods high dimensional mappings kernel machines 
similar results obtained covering number analysis zhang 
approach current relies direct computation rademacher complexity direct gives better bounds cases 
analysis easier generalize corresponding covering number approach 
analysis applies directly non bayesian mixture approaches bagging boosting 
technical level results remove common limitation bounds learning community assumption boundedness underlying loss functions 
assumption usually inappropriate regression inapplicable classification problems loss function replaced convex upper bound see section 
remainder organized follows 
section description decision theoretic framework bayesian learning 
move section discuss mixture distributions recall basic properties convex functions 
section presents new uniform convergence result unbounded loss functions section establishes bounds generalization error bounds bayesian mixture algorithms rademacher complexity classes functions defined convex constraints 
section applies general results cases interest establishing data dependent bounds 
conclude section technical details appendix 
moving body comments concerning notation 
specified natural base logarithm 
denote random variables upper case letters realizations lower case letters 
expectations respect random variable denoted vectors denoted boldface 

decision theoretic bayesian framework decision theoretic bayesian setting consider spaces 
input space action space output space consider deterministic action performed observing input loss function 
probability measure defined bayes optimal decision rule minimizing inf ease notation suppress dependence expectation 
general access observe sample action selected sample current input refer sample dependent action algorithm 
sample dependent loss 
interested expected loss algorithm averaged samples expectation taken respect sample drawn probability measure consider family measures possesses underlying prior distribution construct averaged risk function respect prior dp dp dp dp dp posterior distribution family induces posterior distribution sample space action algorithm minimizing bayes risk referred bayes algorithm inf 
fact prior sample optimal algorithm return bayes optimal predictor respect posterior measure important practical problems optimal bayes predictor linear functional underlying probability measure 
example loss function quadratic meir zhang optimal bayes predictor conditional mean 
binary classification problems predictor conditional probability optimal classification decision rule corresponds test linear functional clearly bayes predictor linear functional probability measure optimal bayes algorithm respect prior dp dp dp 
case optimal bayesian algorithm regarded predictor constructed averaging predictors respect data dependent posterior 
refer methods bayesian mixture methods 
bayes estimator optimal respect bayes risk shown appropriate conditions appropriate prior minimax admissible estimator robert 
general unknown 
may prior information possible models view consider hypothesis space algorithm mixture hypotheses contrasted classical approaches algorithm selects single hypothesis simplicity consider countable hypothesis space probability distribution 
introduce vector notation 
define probability simplex denote 
observe general may great deal complex single hypothesis example non polynomial ridge functions composite predictor corresponds layer neural network universal approximation power 
main feature establishment data dependent bounds loss bayes mixture algorithm 
flurry activity concerning data dependent bounds non exhaustive list includes bartlett bousquet chapelle shawe taylor zhang 
related vein mcallester provided data dependent bound called gibbs algorithm selects hypothesis random posterior distribution 
essentially result provides bound average error bound error averaged hypothesis may smaller 
langford 
extended result mixture classifiers margin loss function 
general result obtained covering number approach described zhang 
herbrich graepel showed certain conditions bounds gibbs classifier extended bayesian mixture classifier 
bound contained explicit dependence dimension see thm 
herbrich graepel 
approach pioneered mcallester came known pac bayes term somewhat misleading optimal bayesian method decision theoretic framework outline average loss functions hypotheses 
regard 
assumption hypothesis space countable removed 
retain ease presentation 
generalization error bounds bayesian mixture algorithms learning behavior true bayesian method addressed pac bayes analysis 
attempt narrow discrepancy analyzing bayesian mixture methods consider predictor average family predictors respect data dependent posterior distribution 
bayesian mixtures regarded approximation truly optimal bayesian methods 
fact argued equivalent important practical problems 

mixture algorithms convex constraints learning algorithm bayesian mixture framework uses sample select distribution constructs mixture hypothesis order constrain class mixtures forming mixture impose constraints mixture vector non negative convex function define positive 
subsequent sections consider different choices essentially acts regularization term 
mixture define loss empirical loss incurred sample sequel notation stands average sample respect distribution 
formalize assumptions concerning 
assumption constraint function convex non negative 
important tool extensively theory convex duality rockafellar boyd vandenberghe 
discussing issues introduce useful results 
results convex functions duality denote convex function defined convex domain qx 
definition function define sup result follows directly taylor expansion 
meir zhang lemma assume possesses continuous order derivatives 
sup dq qx qx possesses continuous second order derivatives sup dq qx 
proof qx qx 
observe 
generalized mean value theorem theorem known functions continuously differentiable 
replacing setting infer exists qq 
continuously second order differentiable second order taylor expansion remainder shows exists 
function defined domain define conjugate sup noting convex irrespective convexity 
domain consists values supremum finite values bounded simple consequence definition called fenchel inequality states 

concentration inequality unbounded functions general loss functions applications bounded priori 
starting point analysis concentration result similar theorem see theorem bartlett mendelson 
main advantage current formulation functions assumed bounded 
particularly useful context regression 
proof appendix 
theorem class functions mapping domain independently selected probability measure assume exists positive number log sup cosh 
integer probability samples length satisfies sup log generalization error bounds bayesian mixture algorithms note dependence explicit play role sequel 
bound slightly improved functions bounded 
corollary conditions theorem hold addition assume sup sup log proof proof lemma appendix note sup lm 
bounding exp symmetrization argument lemma may apply chernoff bound leads log exp 
spite slightly improved bound case bounded functions bound theorem generality 
great deal dealt rademacher complexity bounds 
denote independent bernoulli random variables assuming values equal probability 
set data points define data dependent rademacher complexity sup 

expectation respect denoted 
note differs standard rademacher complexity defined absolute value argument supremum van der wellner 
current version rademacher complexity merit vanishes function classes consisting single constant function dominated standard rademacher complexity 
definitions agree function classes closed negation classes implies standard symmetrization arguments example lemma van der wellner show sup 
convenient rademacher average due lemma 
lemma sets functions defined domain function probability distribution sup sup meir zhang proof induction 
result holds 
sup sup sup sup sup sup inequality follows induction hypothesis 
lemma refined symmetric version rademacher process comparison theorem theorem ledoux 
proof simpler 
set functions characterized lipschitz constant 
consequence immediate lemma 
theorem functions lipschitz constants sup sup loss function set 
assume lipschitz constant consist functions defined lipschitz constant find theorem kr 
note passing theorem gain factor compared bound corollary ledoux away requirement 
setting obtain bound expected loss 
theorem class functions mapping domain independently selected probability measure assume exists positive real number positive loge sup cosh generalization error bounds bayesian mixture algorithms lipschitz constant 
probability samples length satisfies log 
rademacher complexity classes defined convex constraints consider class functions defined convex constraint function 
wish compute rademacher complexity 
denoting conjugate function 
setting conclude positive sup inequality holds obtain upper bound rademacher complexity inf 
note similar convex duality related context seeger general may difficult compute expectation respect purpose lemma 
note implies sup 
lemma convex function ah 
proof prove claim induction 
ash ah ah ah meir zhang 
assume claim holds 

ah ah ah ah ah ah step definition induction hypothesis 
lemma find inf 

data dependent bounds consider loss bound derived theorem 
bound requires prior knowledge constant characterizing class general able establish bound data dependent assume priori knowledge 
rewriting bound theorem slightly different form 
probability log slightly abuse notation setting 
observe monotonically increasing may upper bound 
example inf eliminating dependence leads fully data dependent bound 
theorem assumptions theorem hold 
consider parameters smax 
probability log log log generalization error bounds bayesian mixture algorithms proof observe final term defined 
sets positive numbers 
theorem multiple testing lemma essentially slightly refined union bound probability log 
pick 
note 
denote smallest index 
log 
substituting log log log log 
combing bounds keeping mind monotonicity probability log log log concludes proof 
note parameter essentially sets scale 
example selected get data independent bound replaces 
observe bounds derived theorem data dependent order select optimal posterior distribution comment section 
observe bounds theorem yields rates techniques refined concentration inequalities bartlett able achieve faster rates convergence favorable circumstances 
example faster rates possible empirical error small 
leave extension results situations 
entropic constraints assume data independent prior distribution assigned hypotheses 
set kullback leibler divergence log 
case conjugate function explicitly calculated yielding log note dq qz qz qz easy see sup dq qz lemma applied 
slightly better bound obtained refined derivation 
derive upper bound rademacher complexity captured lemma 
meir zhang lemma empirical rademacher complexity upper bounded follows sup proof expression sup log exp expectation respect 
chernoff bound exp exp log exp sup loge exp sup sup jensen inequality chernoff bound 
minimizing respect obtain desired result 
result theorem obtain main result section 
theorem conditions theorem hold set smax sup probability log log log note functions uniformly bounded say instructive compare results theorem obtained mcallester gibbs algorithm 
algorithm selects hypothesis random posterior distribution forms prediction mcallester establishes bound expected performance randomized predictor 
probability ln ln 
hypotheses losses bounded value assumed mcallester see small numerical constants leading terms complexity penalties generalization error bounds bayesian mixture algorithms similar 
bound contains extra logarithmic term bound contains extra term order 
note term significantly larger term mixture hypothesis far complex single hypothesis detailed numerical comparison bounds left 
comment similar bound margin loss established langford 
mixture classifiers 
mentioned theorem data dependent bounds order select optimal posterior distribution convex case 
may formulate optimization problem constrained optimization problem form min parameter optimized order obtain best bound 
convex function example quadratic loss obtain convex programming problem solved standard approaches boyd vandenberghe 
note approach similar called maximum entropy discrimination proposed jaakkola 

convex may jensen inequality upper bound 
case mcallester shown exact solution form gibbs distribution obtained 
solution may principle starting point numerical optimization algorithms solving current problem 
norm constraints section entropic term constrain distributions relative prior distribution cases prior information provided terms prior may believe sparser solutions appropriate principle require constraint form close zero 
results hold case indicate principle take account types norms 
hard approach derive bounds support vector machines case replace constraint constraint 
simple case norm 
case simplify notation easy see simple calculation yields substituting result minimizing find theorem jensen inequality obtain bound 
theorem conditions theorem hold set smax meir zhang probability log log log consider case general 
max min consider norm regularization associated conjugate function note case average required cumbersome resort 
rademacher averaging result norm regularization known geometric theory banach spaces type structure banach space example see ledoux follows inequality 
derived general techniques developed bound 
lemma bound valid max proof implying qz qz direct computation second order derivatives required lemma condition yields dq qz qz qz qz qz second inequality follows holder inequality dual pair 
implying qz qz part lemma 
dq qz qz qz qz qz inequality second inequality 
lemma observation max max establishes claim 
generalization error bounds bayesian mixture algorithms obtain bound rademacher complexity inf max lq max combining theorem jensen inequality obtain result 
theorem conditions theorem hold set smax probability log log log max oracle inequalities point obtained data dependent bounds purpose model selection 
general interested knowing empirical estimator compares best possible mixture estimator known underlying probability distribution known 
bounds referred oracle inequalities 
empirically derived posterior distribution 
particular establish oracle inequality relates loss minimal loss inf 
recall theorem probability log log log structural risk minimization vapnik select complexity regularization criterion argmin probability 
meir zhang optimality selection arbitrary hypothesis depend data 
may apply theorem obtain probability greater log 
note case function class consists single element term leading rademacher complexity vanishes 
probability 
arbitrary obtain result 
theorem conditions theorem probability inf 
note uniformly bounded say independently find probability inf 
binary classification far mainly concerned regression 
case binary classification easily incorporated framework 
sample 
consider soft classifier define loss 
lipschitz function lipschitz constant dominates loss 
hard conclude conditions theorem find probability log proceed develop data dependent bounds problem lines theorem 
note possible choices proposed literature 
proof bayes consistency algorithms dominating functions lugosi 
zhang 
extension multi category classification proposed meir 

developed general procedure establishing data dependent bounds mixture approaches regression classification 
discussed section bayesian mixture approaches possess desirable attributes frequentist perspective 
opposition bayesian approaches results hold independently correctness model assumptions 
generalization error bounds bayesian mixture algorithms approach pursued effectively forms prior knowledge may incorporated selection appropriate constraint functions 
additionally results apply general mixture approaches bagging boosting 
technical level replaced boundedness assumptions prevalent learning theory literature general moment constraint conditions 
open issues remain research 
interesting combine current approach methods local rademacher complexities bartlett able attain faster convergence rates 
second particularly interesting question relates data learn appropriate constraint function constraint functions 
clearly important conduct careful numerical studies bounds 
related seeger demonstrated tightness similar bounds context gaussian processes relevance real world problems 
preliminary studies indicate similar behavior bounds systematic numerical investigation needs done 
concerned solely mixture bayesian solutions 
pointed section general optimal bayesian solutions mixture form 
context particularly interesting establish finite sample bounds optimal bayesian procedures appropriate conditions provide tight upper bounds performance learning algorithm selecting hypotheses class hypotheses 
suggested connections established frequentist bayesian approaches conclude quote lehmann casella 
strengths combining bayesian frequentist approaches evident 
bayes approach provides clear methodology constructing estimators frequentist approach provides methodology evaluation 
restricted bayesian mixture algorithms necessarily optimal general hope steps strengthening claim 
acknowledgments partially supported technion fund promotion sponsored research 
support center department electrical engineering technion acknowledged 
appendix examples convex functions conjugates provide examples convex functions conjugates 
examples boyd vandenberghe zhang 
denote convex function variable denotes conjugate dual variable norm vector symmetric positive definite matrix 
ku 
meir zhang real numbers obeying 
assume 
log exp 
appendix proof theorem prove lemma 
lemma consider real valued functions qx 
define sup 
independent random variables 
log exp loge sup cosh 
proof sup 
clear inf sup 
jensen inequality symmetrization obtain exp exp exp exp cosh sup cosh jensen inequality applied symmetrization argument 

independently drawn distribution class functions set sup ne lemma positive loge exp le sup cosh 
generalization error bounds bayesian mixture algorithms proof lemma follows recursively applying lemma 
identifying function parameter value set 
assume 
fixed 
set note 
simplify notation 
positive integers lemma fixed loge exp loge sup cosh exponentiation rewritten exp exp loge sup cosh expectations respect sides inequality followed applying logarithm function find loge loge loge sup cosh 
summing sides 
obtain loge loge zn loge loge zn loge loge sup cosh subtracting identical terms sides inequality find loge le sup cosh establishes claim 

set sup ne sup ne ne chernoff inequality inf exp lx lx nonnegative le meir zhang logarithms sides inequality find logd le loge sup cosh lemma assumption theorem 
arbitrary conclude logd inf ne obtain probability sup sup log 
mathematical analysis 
addison wesley 
bartlett bousquet mendelson 
localized rademacher complexity 
proceedings annual conference computational learning theory volume lnai sydney february 
springer 
bartlett lugosi 
model selection error estimation 
machine learning 
bartlett mendelson 
rademacher gaussian complexities risk bounds structural results 
journal machine learning research 
lugosi massart 
concentration inequalities entropy method 
annals probability 
appear 
bousquet chapelle 
stability generalization 
journal machine learning research 
boyd vandenberghe 
convex optimization 

available www stanford edu boyd html 
meir 
data dependent bounds multi category classification convex losses 
unpublished 
herbrich graepel 
pac bayesian margin bound linear classifiers svms 
advances neural information processing systems pages cambridge ma 
mit press 
jaakkola meila jebara 
maximum entropy discrimination 
kearns editor advances neural information processing systems volume 
generalization error bounds bayesian mixture algorithms 
empirical margin distributions bounding generalization error combined classifiers 
annals statistics 
langford seeger megiddo 
improved predictive accuracy bound averaging classifiers 
proceeding eighteenth international conference machine learning pages 
ledoux 
probability banach spaces processes 
springer press new york 
lehmann casella 
theory point estimation 
springer verlag new york second edition 
lin pinkus 
multilayer feedforward networks activation function approximate function 
neural networks 
lugosi 
consistent strategy boosting algorithms 
proceedings annual conference computational learning theory volume lnai pages sydney february 
springer 
meir zhang 
consistency greedy algorithms classification 
proceedings fifteenth annual conference computational learning theory volume lnai pages sydney 
springer 
meir zhang 
greedy algorithms classification consistency convergence rates adaptivity 
technical report department electrical engineering technion 
mcallester 
pac bayesian theorems 
machine learning 
mcallester 
pac bayesian stochastic model selection 
machine learning 
robert 
bayesian choice decision theoretic motivation 
springer verlag new york second edition 
rockafellar 
convex analysis 
princeton university press princeton 
seeger 
pac bayesian generalization error bounds gaussian process classification 
jmlr 
shawe taylor bartlett williamson anthony 
structural risk minimization data dependent hierarchies 
ieee transaction information theory 
van der wellner 
weak convergence empirical processes 
springer verlag new york 
vapnik chervonenkis 
uniform convergence relative frequencies events probabilities 
theory probability applications 
vapnik 
statistical learning theory 
wiley interscience new york 
yang 
minimax nonparametric classification part rates convergence 
ieee transactions information theory 
zhang 
generalization performance learning problems hilbert functional space 
advances neural information processing systems cambridge ma 
mit press 
meir zhang zhang 
covering number bounds certain regularized linear function classes 
journal machine learning research 
zhang 
dual formulation regularized linear systems convex risks 
machine learning 
zhang 
statistical behavior consistency classification methods convex risk minimization 
annals statistics 
appear 

