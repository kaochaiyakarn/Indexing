genome informatics multi class protein fold classification new ensemble machine learning approach aik tan david gilbert yves deville brc dcs gla ac uk brc dcs gla ac uk info ucl ac bioinformatics research centre department computing science university glasgow gardens glasgow qq scotland united kingdom department computing science engineering universit catholique de louvain place louvain la neuve belgium protein structure classification represents important process understanding associations sequence structure possible functional evolutionary relationships 
structural genomics initiatives high throughput experiments populated biological databases rapid pace 
amount structural data traditional methods manual inspection protein structure impossible 
machine learning widely applied bioinformatics gained lot success research area 
proposes novel ensemble machine learning method improves coverage classifiers multi class imbalanced sample sets integrating knowledge induced different base classifiers illustrate idea classifying multi class scop protein fold data 
compared approach part show method improves sensitivity classifier protein fold classification 
furthermore extended method learning multiple data types preserving independence corresponding data sources show new approach performs traditional technique single joined data source 
experimental results encouraging applied bioinformatics problems similarly characterised multi class imbalanced data sets held multiple data sources 
keywords ensemble machine learning multi class protein fold classification imbalanced data learning multiple data types classification prediction biological entities long central research theme bioinformatics 
years increasing quantities high throughput biological data available understand relationship protein sequence structure function 
data distributed maintained different databases different research groups 
problem database resources contains different subset set specific biological knowledge answer questions queries domain questions span domain boundaries 
current release protein data bank pdb aug contains proteins experimentally determined structures 
number protein structures increasing rapidly result international structural genomics initiatives huge gap compared pir july known protein sequences 
elucidating similarities differences protein structures important understanding relationship protein sequence structure function analysis possible evolutionary relationships 
experimental protein structure determination expensive time consuming sophisticated computational methods developed applied detect search compare multi class protein fold classification new ensemble machine learning approach remote protein homology sequence level hope knowledge transferred new unknown protein inference function 
computational tools developed protein fold prediction primarily sequence similarity searches 
new protein sequence unknown structure high sequence similarity protein known structure new protein may share similar fold structure 
approaches improved prediction accuracy capable detecting proteins high sequence similarity failed perform low sequence similarities closely related structures 
machine learning computational approach widely development automatic protein structure classification prediction 
aims structural genomics enhance understanding relationship amino acid sequence corresponding protein fold 
symbolic machine learning widely applied protein fold recognition especially deriving rules assist human experts understanding folding rules 
statistical learning methods neural networks support vector machines consistently exhibit better performance symbolic machine learning techniques decision trees rule systems resulting complex models hard interpret easily lead new insights problem 
advantages symbolic machine learning approaches purpose generation human understandable classifiers rules biological background knowledge explain relationship sequence structure 
scop database manually derived comprehensive hierarchical classification known protein structures organised evolutionary structural relationships 
database divided hierarchical levels class fold superfamily family 
scop nov protein domains classified folds resulting average domains fold 
number domains fold varies scop folds highly populated tim barrels folds contain examples hsp peptide binding fold contains protein 
performing learning scop folds common versus approach class problem results learning imbalanced data set 
imbalanced proportion examples fold contributes poor performance classical machine learning techniques decision trees 
existing machine learning approaches tend produce strong discriminatory classifier high accuracy low sensitivity called completeness learning types problems 
motivated ding analysis applied support vector machines neural networks construct versus versus methods classifying multi class scop fold sequence data 
observed classical learning methods perform badly due imbalanced proportion data called known false positives problem 
furthermore protein sequence data types may distributed different data sources 
common characteristic bioinformatics necessary data variety independently curated maintained databases 
applying learning techniques infer multiple data sources remains research challenges machine learning bioinformatics communities 
investigate problems context classifying multi class scop folds 
improve coverage classifiers learning imbalanced data sets protein examples class heavily outnumber classes negative examples 

maintain independence different protein sequence data sources time exploit information induced data types combining pattern level 
devised ekiss ensemble knowledge imbalance sample sets ensemble learning method solve types problems 
objective ekiss generate classifiers capable learning multi class examples skewed normal distribution training examples providing explanation user 
addition extended tan method learn multiple data types whilst preserving independence corresponding data sources show new approach performs traditional technique single joined data source 
organisation follows section provides machine learning background related approach section describes ekiss framework section presents training test sets study section describes experimental designs section discusses results section concludes 
machine learning background problem formulation notations multi class supervised classification problem set training data positive negative examples form features classes provided learner 
learner task induce set rules discriminate positive examples negative ones propose classification new instances 
common approach treating multi class learning transform classes set class problems known versus method 
approach faces serious problem learning multi class problems transform classes class problems positive examples class represented compared large number negative examples class 
ck 
presence large amount negative examples training data poses pitfalls classical machine learning systems 
major problem applying discriminative classical machine learning techniques situation generate trivial classifier classifies negative class due negative examples majority class overfit positive examples minority class generating large decision trees highly complex neural networks 
discriminative learning approaches apply recursive partitioning instance space regions labelled majority class region 
furthermore heuristic stopping pruning partitioning procedure constructed avoid overfitting training examples solely accuracy error rate classifier representing weak measurement imbalanced data 
heuristic known occam razor machine learning literature suggests learning algorithm prefer simpler complex classifiers order avoid overfitting training examples 
wolpert free lunch nfl theorems point heuristics fail succeed supervised learning problems 
classical machine learning methods suffer drawbacks perform poorly class imbalanced data situation 
scenario described curse imbalanced data machine learning terminology 
related multi class learning 
approach handling multi class problems generate possible pair wise class classifiers classes training examples 
approach known versus method classes training examples machine learning methods generate class classifiers classifiers 
unseen proteins classified classifiers classifier provides vote class label majority voted class predicted class new proteins 
ideal case correct class get maximum votes class paired classifiers 
case observed approach perform due votes correct class randomly distributed classes 
classifiers trivial votes negative class 
problem observed described votes popular voted class decreasing gradually maximum minimum simply returning class highest vote 
disadvantage approach large number classifiers difficult analyse multi class protein fold classification new ensemble machine learning approach purpose providing insights protein sequence structure relationships 
sampling methods imbalanced data sets 
attempts proposed machine learning community overcome class imbalanced data set problem primarily focus sampling training examples 
due analysis weiss provost concluded natural class distribution best distribution learning classifier 
sampling methods involve sampling reducing negative class randomly removing negative examples training set ii sampling increasing positive class replicating positive examples 
studies observed sampling replication improve minority positive class prediction 
due classifier specific minority class decision region leading overfitting examples 
drummond holte shown sampling approach performs better sampling method 
sampling approach forces learning algorithm focus different degrees class distribution time increasing presence minority class training examples generate robust classifier 
sampling approaches appear appealing solving imbalanced data problems moment techniques mainly experimented class problems artificial synthetic data 
removing increasing training examples suitable research domain due multi class nature training examples limited amount real protein data 
furthermore protein fold classification problem learn sequence fold relationships sequence features non redundant protein examples low sequence similarities 
preserve original training examples propose method capable performing learning multi class imbalanced data 
learning multiple data sources 
primary goals bioinformatics design tools systems integrate multiple data sources perform learning reasoning data support inference annotation mechanisms new sequences 
easy imagine single biological database containing information collected diverse data sources implementation unified database non trivial practice 
problem database resources contain different subsets biological knowledge maintained upgraded regularly different research groups 
various ways bioinformatics groups tried integrate biological databases generally fall categories link integration ii view integration iii data warehousing 
reviews technologies integrating biological data 
common direct approach performing machine learning multiple data sources combine data joint table apply learning techniques joint table discover meaningful discriminative patterns 
approach suffers major problems ignoring destroying data variation increasing learning time memory size 
suggested stein better solution unfortunately non trivial maintain scientific political independence databases allow information contain easily integrated enable cross database queries 
maintaining independence multiple data sources whilst performing integration pattern level motivated undertake study 
investigate possibility performing symbolic machine learning multiple data types combining decision rules way proposed ensemble learning method classify protein folds 
methods devised ekiss ensemble machine learning approach integrates classifiers generated versus versus approaches improve coverage positive protein examples multi class imbalanced data 
ensemble machine learning loosely defined set classifiers individual decisions combined tan way classify new examples :10.1.1.34.4718
empirical studies shown performance ensemble machine learning approaches better single methods due drawbacks discussed background section existing nfl theorems individual learning algorithms :10.1.1.33.353:10.1.1.131.1931
ensemble approach applied ekiss differs classical ones combining decisions different base classifiers combine rules base classifiers generate new classifiers final decision making 
furthermore ekiss preserves original distribution training data making resulting classifiers sensitive imbalanced situation 
ekiss method ekiss approach consists combining rules base classifiers generate new classifiers 
study applied part rule machine learning technique generate base classifiers ensemble learning system 
part rule induction algorithm avoids global optimisation generates accurate compact rule sets combining paradigms conquer separate conquer ripper :10.1.1.50.8204:10.1.1.143.8073
part adopts separate andconquer strategy builds rule removes covered instances continues constructing rules recursively generating partial decision tree remaining instances 
rule generated part fewer number compact compared ripper 
performed procedure generate class classifiers number classes approach produce classifiers 
combined base classifiers generate new classifier class called ensemble classifiers 
protein fold classification problem ensemble combine base classifiers 
part rule learning system part classifier contains set decision rules 
simplify presentation assume base part classifier contains positive decision rules denoted ri ri rik base classifier number classical machine learning methods generate classifier performing heuristic search possible classification rules true hypotheses instance space trying find rules best approximate true classification instance space 
heuristics employed far suitable multi class imbalanced data sets classical machine methods suffer curse learning imbalanced data time return near optimal trivial classifier 
basic idea ekiss consider rule rij potential candidate rule new ensemble classifiers 
main assumption ekiss rules generated part learning algorithm represent possible classification rules enlarging search space 
ekiss search strategy find rules correctly classify examples positive class improving coverage positive examples multi class imbalanced data situation 
believe positive rules useful providing insights human expert understanding relationships protein structure sequence information compared trivial classifier 
technically rule included new ensemble classifier class correctly classifies positive examples class 
decision measure normalised confidence measurement cf norm tp tp fp cut point rule selection 
rules new classifier class ci rules satisfy cut point 
normalised confidence measurement derived applied evaluating goodness decision rules derived decision trees 
measurement takes account ratio positive negative examples produces sensitive measurement computing accuracy rules imbalanced data situation 
obviously rules base classifier class new classifier class rules base classifiers 
system cf norm represents tuning parameter trade coverage positive examples tp rate precision positive predicted value 
class ekiss allows user select classifier best suits classification purpose multi class protein fold classification new ensemble machine learning approach schematic designs ekiss systems 
ekiss sequence data types joined single table ekiss learning strategy 
cb represents base classifiers generated experiment represents scop fold classes 
cc represents final classifiers generated ekiss 
ekiss multiple data types learned independently corresponding base classifiers integrated generate final classifiers cc 
modifying cf norm value 
furthermore order assist user selecting choice classifiers system automatically generate roc receiver operating characteristic curves class providing visualisation tool facilitate selection process 
extending ekiss learning multiple data sources discussed previous sections common approach perform learning multiple data sources join data sources single joined table 
ekiss method applied joined table illustrated 
refer approach ekiss remainder 
possible ekiss basis new ensemble knowledge approach learning multiple data sources motivated 
approach called ekiss multiple illustrated 
maintained data source independent entity integrating instance 
data source versus versus base classifiers generated independently 
rules base classifiers considered potential candidates generation ensemble classifiers 
different data sources potential candidate rules base classifiers expanding total search space ekiss system 
show enlarging search space way specific rules obtained certain classes improving coverage minority classes 
ekiss approaches designed increase sensitivity positive coverage classifiers 
expect methods reduced specificity called soundness 
shown results approach useful ratio low initial classifiers yield little sensitivity 
case loss specificity small compared increase sensitivity yielding useful classifiers 
obviously classes base classifiers may preferred new 
data set applied method protein data set studied ding obtained www nersc gov protein 
original data contains training tan table summary chemical amino acid sequence data types study 
data types amino acid composition polarity predicted secondary structure normalised van der waals volume set test set 
training set extracted pdb select sets comprises proteins populated scop folds examples fold pair wise sequence identities 
test set extracted pdb contains representatives scop folds sequence similarity excluding proteins 
features learning system extracted protein sequences method described protein sequence represented set parameter vectors various chemical structural properties amino acids sequence 
properties polarity predicted secondary structure normalised van der waals volume amino acid composition protein sequence 
sequence properties contained continuous features 
properties extracted individually corresponding protein sequences may treat features different data types stored corresponding individual data sources 
table summarises protein sequence properties study 
exploiting data analysed training test sets interesting errors data sets especially training set 
error inconsistency data sets 
ding protein data pdb selects training set time scop classification exist 
training set early scop database believe domain definitions scop defined 
test set extracted scop database scop dec domain definitions defined clearly contains major changes compared early scop version assign training set 
protein examples training set assigned domains time due earlier scop domain definitions test set different chopped domains 
probably dirty data may contributed poor performance analysis 
time shows domain definition evolved scop database time careful manual assignment automatic intelligent system may facilitate protein fold classification process 
cleaned data set removing errors training testing examples 
applied protein fold classification scop nov astral sequence identity nov removing fold classes examples 
performing cleaning stage protein fold data contained examples distributed fold scop classes 
randomly divided data training set protein examples test set protein examples 
experimental settings performed different experiments ekiss answer objectives study 
experiment joined sequence features single data source performed classification data 
straightforward combination approach provided chemical multi class protein fold classification new ensemble machine learning approach table average performance evaluation ekiss cf norm part 
scop class method training set protein examples test set protein examples sn tpr fpr sn tpr fpr ekiss folds part ekiss folds part ekiss folds part ekiss folds part undef undef small proteins ekiss fold part undef structural properties amino acids learning attributes 
goal experiment fold 
evaluate ekiss learning multi class imbalanced scop folds compare part classical rule system second evaluate ekiss approach perform learning multiple data types single joined data source classifying multi class scop folds second experiment evaluated ekiss multiple preserved individual data sources performed base classification data source combined rules approach 
resulted different sequence data sources containing chemical structural properties amino acids learning features table 
experiments able compare performance ekiss multiple ekiss 
standard measurements applied evaluate goodness classifiers compared part true positive rate positive coverage sensitivity tpr tp tp fn false positive rate fpr fp fp tn specificity positive predicted value tp tp fp measure tpr tpr evaluates trade sensitivity positive predicted value 
applied weka machine learning package generate base classifiers ekiss 
ekiss ensemble system written perl roc curves generated gnuplot 
compared ekiss decision trees support vector machines neural networks package 
results discussion comparison ekiss part 
performed fold cross validation training data tested test set comparing performance ekiss part 
table summarises performance ekiss part training test sets 
results ekiss outperforms part classes measure 
results show ekiss increases sensitivity positive predictive accuracy compared part 
experiments ekiss performs better decision trees classes svm classes artificial neural networks classes 
goodness ekiss classifiers improved tuning cf norm values scop fold lead generation better classifiers 
method increases true positive rate tpr trade increases false positive rate fpr 
objective study improve rule coverage classifying protein folds permit rule set cover false positives consequence improving positive coverage classical machine learning 
results tan show increase tp rate higher corresponding increase fp rate 
comparison ekiss ekiss multiple 
presents roc curves selected classes generated test set comparing ekiss learning single joined data source ekiss multiple learning multiple data sources 
roc curves represent trade coverage tpr axis error rate fpr axis classifier 
classifier located top left corner roc graph illustrating classifier high coverage true positives false positives 
trivial bottom left corner roc graph trivial acceptor top right corner graph 
roc curves represent ekiss multiple ekiss respectively different cf norm values 
higher cf norm curves shifted top left corner 
roc curves clearly show ekiss multiple improved classification performance joined data sources curve constantly higher curve 
figures show classical machine learning tends produce trivial imbalanced data set left bottom corner ekiss approaches greatly improved classifier sensitivity different cf norm values 
figures show ekiss multiple perform better ekiss 
expected classical machine learning methods produce trivial located bottom left corner roc curves 
results observed multiple data types applying majority voting results lead better prediction accuracy 
approach different different ways integrate possible decision rules base classifiers way construct new classifiers base classifiers decisions ii final classifiers represent patterns decision rules learned individual data source compared classifiers formed majority voting system 
observation obviously ekiss multiple performed better part better ekiss shown better part 
believe unique features ekiss provide better rules understanding relationships chemical protein sequence properties corresponding fold 
addition ekiss framework may alternative solution database integration problems currently faced bioinformatics community 
creating unified data warehouse storing data sources different origins approach allows data sources stored different locations perform learning data sources individually 
integration part performed pattern decision rule level separate joined data level 
data source updated ekiss system needs update patterns performing learning updated data source 
furthermore approach easy plug new protein data sources enhance classifier performance 
purposes study assumed data types internally consistent reality need ensured combination mechanised hand consistency checking achieved example approach 
order verify hypothesis set rules base classifier forms useful search space generation new classifiers set random rules obtained applying part randomly generated data set 
performances resulting random classifiers clearly worse performance ekiss 
observation suggested decision rules selected construct ekiss classifiers relevant discriminating classes 
general ekiss performs learning small set positive examples compared negative examples ekiss capable generating softer boundary classifier 
avoids problems connected strong discriminative boundary generated classical learning systems 
essential conditions ensemble methods perform better individual classifier members diversity base classifiers 
reason base classifiers wrong prediction diversity ensemble classifiers improve prediction 
base classifiers different predictions diversity ensemble multi class protein fold classification new ensemble machine learning approach roc curves ekiss system learning multiple sequence data types ekiss multiple joined data ekiss compared part svm neural networks decision trees test data 
may predict correctly considering majority votes diverse base classifier decision 
believe base classifiers ekiss diverse combining part classifiers 
re selecting appropriate rules base classifiers creates diversity ensemble improves positive coverage ekiss 
obviously base classifiers ekiss multiple diverse ekiss generated different data sources 
believe decision rules derived ekiss multiple discriminative due fact represent specific rules induced individual data types contributing better performance combined ensemble stage 
interesting finding experiment rule sets generated ekiss smaller original part system 
expected ekiss rule sets contain rules compared part due collecting additional rules classifiers turns fewer rules 
believe rule sets ekiss useful classifying protein folds assist wet experimental biologists understanding relationships amino acid chemical properties functions 
compared statistical tan learning methods investigated study resulting models hard interpret performance improved specific tuning available parameters different types kernels svm number nodes neural networks 
described ekiss ensemble method specifically designed increase sensitivity positive coverage classifiers losing corresponding specificity learning multi class imbalanced data sets examples class heavily outnumber classes 
applied approach classification scop protein folds results show approach useful ratio low initial classifiers yield little sensitivity 
cases loss specificity small compared increase sensitivity yielding useful classifiers 
shown ekiss capable learning multiple data types attaining performance compared combining data types table 
approach useful learning data obtained independently curated maintained databases 
furthermore advantage rules generated ekiss compared part shorter provide hints understanding relationship amino acid chemical properties sequence constituted fold 
encouraging results approach applied wide range bioinformatics problems plan evaluate ekiss data sets similar characteristics 
extension explore create larger diverse base classifiers incorporating decision rules generated different rule systems employ different inductive biases compared part 
acknowledgments torrance ali useful comments 
ac tan funded university glasgow studentship 
baldi brunak bioinformatics machine learning approach mit press 
bauer kohavi empirical comparison voting classification algorithms bagging boosting variants machine learning 
walker lo conte levitt brenner astral compendium enhancements nucleic acids res 
chawla imbalanced data sets investigating effect sampling method probabilistic estimate decision tree structure workshop learning imbalanced datasets ii icml 
cohen fast effective rule induction proc 
th icml 
cootes muggleton sternberg automatic discovery structural principles describing protein fold space mol 
biol 
dietterich experimental comparison methods constructing ensembles decision trees bagging boosting randomization machine learning 
dietterich ensemble methods machine learning proc st international workshop mcs lncs :10.1.1.34.4718
ding multi class protein fold recognition support vector machines neural networks bioinformatics 
drummond holte class imbalance cost sensitivity sampling beats sampling workshop learning imbalanced datasets ii icml 
muchnik kim protein folding class predictor scop approach global descriptors proc 
th ismb 
multi class protein fold classification new ensemble machine learning approach frank witten generating accurate rule sets global optimisation proc th icml morgan kaufmann 
sander enlarged representative set proteins protein sci 
holm sander protein folds families sequence structure alignments nucleic acids res 
japkowicz class imbalances focusing right issue workshop learning imbalanced datasets ii icml 
king optimisation classes assignment unidentified reading frames functional genomics programmes need machine learning trends biotechnology 
king clark shirazi sternberg machine learning identify topological rules packing strands protein engineering 
kubat holte matwin machine learning detection oil spills satellite radar images machine learning 
lo conte hubbard brenner chothia scop structural classification proteins database nucleic acids res 
lo conte brenner hubbard chothia scop database refinements accommodate structural genomics nucleic acids res 
mitchell machine learning data mining communications acm 
quinlan programs machine learning morgan kaufmann 
quinlan bagging boosting proc 
th national conference artificial intelligence 
rost neural networks predict protein structure hype hit artificial intelligence heuristic methods bioinformatics shamir 
eds ios press 
argos relationships protein sequence structure patterns residue contacts proteins 
stein integrating biological databases nature reviews genetics 
tan gilbert empirical comparison supervised machine learning techniques bioinformatics proc 
st asia pacific bioinformatics conference 
ting low model combination multiple data batches scenario proc 
th ecml 
muggleton automated discovery structural signatures protein fold function mol 
biol 
van rijsbergen information retrieval butterworths 
weiss provost learning training data costly effect class distribution tree induction artificial intelligence research 
witten frank data mining practical machine learning tools techniques java implementations morgan kaufmann 
wolpert supervised learning free lunch theorems proc 
th online world conference soft computing industrial applications 
wong technologies integrating biological data briefing bioinformatics 
yeh karp noy altman knowledge acquisition consistency checking concurrency control gene ontology go bioinformatics 
