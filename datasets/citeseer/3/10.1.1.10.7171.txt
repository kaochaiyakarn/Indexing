university southampton support vector machines classification regression steve gunn technical report faculty engineering science mathematics school electronics computer science may contents nomenclature xi statistical learning theory 
vc dimension 
structural risk minimisation 
support vector classification optimal separating hyperplane 
linearly separable example 
generalised optimal separating hyperplane 
linearly non separable example 
generalisation high dimensional feature space 
polynomial mapping example 
discussion 
feature space kernel functions 
polynomial 
gaussian radial basis function 
exponential radial basis function 
multi layer perceptron 
fourier series 
splines 
splines 
additive kernels 
tensor product 
implicit vs explicit bias 
data normalisation 
kernel selection 
classification example iris data applications 
support vector regression linear regression 
insensitive loss function 
quadratic loss function 
iii iv contents huber loss function 
example 
non linear regression 
examples 
comments 
regression example titanium data applications 
implementation issues support vector classification 
support vector regression 
matlab svm toolbox bibliography list figures modelling errors 
vc dimension illustration 
optimal separating hyperplane 
canonical hyperplanes 
constraining canonical hyperplanes 
optimal separating hyperplane 
generalised optimal separating hyperplane 
generalised optimal separating hyperplane example 
generalised optimal separating hyperplane example 
generalised optimal separating hyperplane example 
mapping input space high dimensional feature space 
mapping input space polynomial feature space 
comparison implicit explicit bias linear kernel 
iris data set 
separating setosa linear svc 
separating polynomial svm degree 
separating polynomial svm degree 
separating radial basis function svm separating polynomial svm degree 
effect separation linear spline svm loss functions 
linear regression 
polynomial regression 
radial basis function regression 
spline regression 
spline regression 
exponential rbf regression 
titanium linear spline regression 
titanium spline regression 
titanium gaussian rbf regression 
titanium gaussian rbf regression 
titanium exponential rbf regression 
titanium fourier regression degree 
titanium linear spline regression 
vi list figures titanium spline regression 
list tables linearly separable classification data 
non linearly separable classification data 
regression data 
vii listings support vector classification matlab code 
support vector regression matlab code 
ix nomenclature column vector zeros positive part svm misclassification tolerance parameter dataset kernel function risk functional remp empirical risk functional xi chapter problem empirical data modelling germane engineering applications 
empirical data modelling process induction build model system hoped deduce responses system ob served 
ultimately quantity quality observations govern performance empirical model 
observational nature data obtained finite sampled typically sampling non uniform due high dimensional nature problem data form sparse distribution input space 
consequently problem nearly ill posed poggio sense hadamard hadamard 
traditional neural network approaches suffered difficulties generalisation producing models overfit data 
consequence optimisation algorithms parameter selection statistical measures select best model 
foundations support vector machines svm developed vapnik gaining popularity due attractive features promising empirical performance 
formulation embodies struc tural risk minimisation srm principle shown superior gunn traditional empirical risk minimisation erm principle employed conventional neural networks 
srm minimises upper bound expected risk opposed erm minimises error training data 
difference svm greater ability generalise goal statistical learning 
svms developed solve classification problem extended domain regression problems vapnik :10.1.1.41.3139
literature terminology svms slightly confusing 
term svm typ ically describe classification support vector methods support vector regression describe regression support vector methods 
report term svm refer classification regression methods terms support vector classification svc support vector regression svr specification 
section continues brief structural risk chapter minimisation principle 
chapter svm introduced setting classifica tion historical accessible 
leads mapping input higher dimensional feature space suitable choice kernel function 
report considers problem regression 
illustrative examples re show properties techniques 
statistical learning theory section brief statistical learning theory 
depth look statistical learning theory see vapnik 
modelling errors goal modelling choose model hypothesis space closest respect error measure underlying function target space 
errors doing arise cases approximation error consequence hypothesis space smaller target space underlying function may lie outside hypothesis space 
poor choice model space result large approximation error referred model mismatch 
estimation error error due learning procedure results tech nique selecting non optimal model hypothesis space 
chapter errors form generalisation error 
ultimately find function minimises risk dxdy unknown 
possible find approximation empirical risk minimisation principle remp minimises empirical risk empirical risk minimisation sense fn arg min remp hn lim remp true law large numbers 
satisfy lim min remp min hn hn valid hn small 
condition intuitive requires minima converge 
bound holds probability remp ln ln remarkably expression expected risk independent probability dis tribution 
vc dimension vc dimension scalar value measures capacity set functions 
vc dimension illustration chapter definition vapnik chervonenkis 
vc dimension set functions exists set points xi points separated possible configurations set xi exists satisfying property 
illustrates points plane shattered set linear indicator functions points 
case vc dimension equal number free parameters general case function sin bx infinite vc dimension vapnik 
set linear indicator functions dimensional space vc dimension equal 
structural risk minimisation create structure sh hypothesis space vc dimension 
srm consists solving problem min remp sh ln ln underlying process modelled deterministic modelling problem consequently chapter restricted deterministic pro cesses 
multiple output problems usually reduced set single output prob lems may considered independent 
appropriate consider processes multiple inputs desired predict single output 
chapter support vector classification classification problem restricted consideration class problem loss generality 
problem goal separate classes function induced available examples 
goal produce classifier unseen examples generalises 
consider example 
possible linear classifiers separate data maximises margin maximises distance nearest data point class 
linear classifier termed optimal separating hyperplane 
intuitively expect boundary generalise opposed possible boundaries 
optimal separating hyperplane optimal separating hyperplane consider problem separating set training vectors belonging separate classes 
chapter support vector classification hyperplane 
set vectors said optimally separated hyperplane separated error distance closest vector hyperplane maximal 
redundancy equation loss generality appropri ate consider canonical hyperplane vapnik parameters constrained min 
constraint parameterisation preferable alternatives simpli fying formulation problem 
words states norm weight vector equal inverse distance nearest point data set hyperplane 
idea illustrated distance nearest point hyperplane shown 
canonical hyperplanes separating hyperplane canonical form satisfy constraints 

distance point hyperplane 
chapter support vector classification optimal hyperplane maximising margin subject constraints equation 
margin min min xi yi min xi yi min xi yi min xi yi min xi yi hyperplane optimally separates data minimises 
independent provided equation satisfied separating hyperplane changing move normal direction 
accordingly margin remains unchanged hyperplane longer optimal nearer class 
consider minimising equation equivalent implementing srm principle suppose bound holds equation accordingly hyperplanes nearer 

data points intuitively seen reduces possible hyperplanes capacity 
constraining canonical hyperplanes chapter support vector classification vc dimension set canonical hyperplanes dimensional space bounded min radius hypersphere enclosing data points 
minimising equation equivalent minimising upper bound vc dimension 
solution optimisation problem equation constraints equation saddle point lagrange functional lagrangian minoux lagrange multipliers 
lagrangian minimised respect maximised respect 
classical lagrangian duality enables primal problem equation transformed dual problem easier solve 
dual problem max max min 
minimum respect lagrangian iyi equations dual problem max max solution problem constraints arg min 
xi xj xi xj 
jyj 
chapter support vector classification solving equation constraints equation determines lagrange multi optimal separating hyperplane xr xs 
xr xs support vector class satisfying hard classifier yr ys 
sgn alternatively soft classifier may linearly interpolates margin may appropriate hard classifier equation produces real valued output classifier queried margin training data resides 
kuhn tucker conditions points satisfy 
non zero lagrange multipliers 
points termed support vectors sv 
data linearly separable sv lie margin number sv small 
consequently hyperplane determined small subset training set points removed training set recalculating hyperplane produce answer 
svm summarise information contained data set sv produced 
data linearly separable equality hold sv sv sv xi xj 
equation vc dimension classifier bounded min sv chapter support vector classification table linearly separable classification data training data normalised lie unit hypersphere min linearly separable example sv illustrate method consider training set table 
svc solution shown dotted lines describe locus margin circled data points represent sv lie margin 
optimal separating hyperplane generalised optimal separating hyperplane far discussion restricted case training data linearly separable 
general case 
approaches generalising problem dependent prior knowledge problem estimate noise data 
case expected possibly known hyperplane correctly separate data method chapter support vector classification generalised optimal separating hyperplane introducing additional cost function associated misclassification appropriate 
alternatively complex function describe boundary discussed chapter 
enable optimal separating hyperplane method generalised cortes vapnik introduced non negative variables penalty function measure misclassification errors 
optimisation problem posed minimise classification error minimising bound vc dimension classifier 
constraints equation modified non separable case 


generalised optimal separating hyperplane determined vector minimises functional value subject constraints equation 
solution optimisation problem equation constraints equation saddle point lagrangian minoux chapter support vector classification lagrange multipliers 
lagrangian minimised respect maximised respect 
classical lagrangian duality enables primal problem equation transformed dual problem 
dual problem max max min 
minimum respect lagrangian iyi 
equations dual problem max max solution problem constraints arg min xi xj xi xj 
jyj 
solution minimisation problem identical separable case modification bounds lagrange multipliers 
uncertain part cortes approach coefficient determined 
parameter introduces additional capacity control classifier 
directly related parameter girosi smola sch lkopf 
blanz 
uses value ultimately chosen reflect knowledge noise data 
warrants practical discussion chapter 
chapter support vector classification table non linearly separable classification data linearly non separable example additional data points added separable data table produce linearly non separable data set table 
resulting svc shown 
sv longer required lie margin orientation hyperplane width margin different 
generalised optimal separating hyperplane example limit solution converges solution obtained optimal separating hyperplane non separable data 
limit solution converges margin maximisation term dominates 
certain point lagrange multipliers take value emphasis minimising misclassification error purely maximising margin producing large width margin 
consequently decreases width margin increases 
useful range lies point lagrange multipliers equal just bounded chapter support vector classification generalised optimal separating hyperplane example generalised optimal separating hyperplane example generalisation high dimensional feature space case linear boundary inappropriate svm map input vector high dimensional feature space choosing non linear mapping priori svm constructs optimal separating hyperplane higher dimensional space 
idea exploits method aizerman 
enables curse dimensionality bellman addressed 
mapping input space high dimensional feature space chapter support vector classification restrictions non linear mapping employed see chap ter turns surprisingly commonly employed functions accept able 
acceptable mappings polynomials radial basis functions certain sigmoid functions 
optimisation problem equation arg min xi xj kernel function performing non linear mapping feature space constraints unchanged 
jyj 
solving equation constraints equation determines lagrange ers hard classifier implementing optimal separating hyperplane feature space sgn sv xi xi iyi xi xr xi xr 
bias computed support vectors computed sv margin stability vapnik :10.1.1.41.3139
kernel contains bias term bias accommodated kernel classifier simply sgn sv ik xi employed kernels bias term finite kernel girosi 
simplifies optimisation problem removing equality constraint equation 
chapter discusses necessary conditions satisfied valid kernel functions 
chapter support vector classification polynomial mapping example consider polynomial kernel form maps dimensional input vector dimensional feature space 
apply ing non linear svc linearly non separable training data table produces classification illustrated 
margin longer constant width due non linear projection input space 
solution contrast training data classified correctly 
svms implement srm principle generalise careful choice kernel function necessary produce classification boundary topologically appropriate 
possible map input space dimension greater number training points produce classifier classification errors training set 
generalise badly 
discussion mapping input space polynomial feature space typically data linearly separable possibly high dimensional feature space 
may sense try separate data exactly particularly finite amount training data available potentially corrupted noise 
practice necessary employ non separable approach places upper bound lagrange multipliers 
raises question determine parameter similar problem regularisation regularisation coefficient determined shown parameter directly related regularisation parameter certain kernels smola sch lkopf 
process cross validation determine chapter support vector classification parameter efficient potentially better methods sought 
removing training patterns support vectors solution unchanged fast method validation may available support vectors sparse 
chapter feature space chapter discusses method construct mapping high dimensional feature space reproducing kernels 
idea kernel function enable operations performed input space potentially high dimensional feature space 
inner product need evaluated feature space 
provides way addressing curse dimensionality 
computation critically dependent number training patterns provide data distribution high dimensional problem generally require large training set 
kernel functions theory reproducing kernel hilbert spaces rkhs aron girosi heckman wahba 
inner product feature space equivalent kernel input space provided certain conditions hold 
symmetric positive definite function satisfies mercer conditions am am kernel represents legitimate inner product feature space 
valid functions satisfy mercer conditions stated valid real chapter feature space polynomial polynomial mapping popular method non linear modelling 

second kernel usually preferable avoids problems hessian zero 
gaussian radial basis function radial basis functions received significant attention commonly gaus sian form exp 
classical techniques utilising radial basis functions employ method determining subset centres 
typically method clustering employed select subset centres 
attractive feature svm selection implicit support vectors contributing local gaussian function centred data point 
considerations possible select global basis function width srm principle vapnik 
exponential radial basis function radial basis function form exp 
produces piecewise linear solution attractive discontinuities acceptable 
multi layer perceptron long established mlp single hidden layer valid kernel represen tation tanh certain values scale offset parameters 
sv correspond layer lagrange multipliers weights 
chapter feature space fourier series fourier series considered expansion dimensional feature space 
kernel defined interval sin sin 
kernel probably choice regularisation capability poor evident consideration fourier transform smola sch lkopf 
splines splines popular choice modelling due flexibility 
finite spline order knots located infinite spline defined interval case 

kernel min min solution piece wise cubic 
splines popular spline formulation 
kernel defined interval attractive closed form 
chapter feature space additive kernels complicated kernels obtained forming summing kernels sum positive definite functions positive definite 
tensor product ki kernels multidimensional kernels obtained forming tensor products kernels ki xi particularly useful construction multidimensional spline kernels simply obtained product univariate kernels 
implicit vs explicit bias remarked previous chapter kernels may may contain implicit bias 
inclusion bias kernel function lead slightly efficient method implementation 
solutions obtained implicit explicit bias may initially come surprise 
difference helps highlight difficulties interpretation generalisation high dimensional feature spaces 
compares linear kernel explicit bias polynomial degree implicit bias 
evident solutions different solutions offer generalisation 
explicit linear implicit polynomial degree comparison implicit explicit bias linear kernel chapter feature space data normalisation data normalisation required particular kernels due restricted domain may advantageous unrestricted kernels 
determine normalisation isotropic non isotropic data necessary requires consideration input features 
additionally normalisation improve condition number hessian optimisation problem 
kernel selection obvious question arises different mappings choose best particular problem 
new question inclusion mappings framework easier comparison 
upper bound vc dimension equation potential avenue provide means comparing kernels 
requires estimation radius hypersphere enclosing data non linear feature space 
final caution strong theoretical method selecting kernel developed validated independent test sets large number problems methods bootstrapping cross validation remain preferred method kernel selection 
chapter classification example iris data iris data set established data set demonstrating performance classification algorithms 
data set contains attributes iris goal classify class iris attributes 
visualise problem restrict features contain information class petal length petal width 
distribution data illustrated 
iris data set setosa classes easily separated linear boundary svc solution inner product kernel illustrated support vectors circled 
support vectors contain important information classification boundary illustrate potential svc data selection 
separation class classes trivial 
fact examples identical petal length width correspond different classes 
illustrates svc solution obtained degree polynomial clear area input space little data classified 
chapter classification example iris data separating setosa linear svc separating polynomial svm degree illustrates higher order polynomial separate additional capacity control 
svc determines hyperplane dimensional feature space 
evidence overfitting due high dimensional nature kernel function emphasised disjoint region top illustration 
separating polynomial svm degree chapter classification example iris data illustrates gaussian radial basis function svc pre specified variance 
result similar degree polynomial 
separating radial basis function svm illustrates svc solution obtained degree polynomial tolerance misclassification errors 
seen produce solution expected generalisation emphasising importance tolerating misclassification errors example 
necessary due non separable nature data just input features 
separating polynomial svm degree visualise effect tolerance misclassification errors topology classifier boundary shows results linear spline svc various degrees misclassification tolerance 
interestingly values offer solutions depending open boundary closed boundary appropriate 
demonstrates parameter may optimal value prior knowledge problem consideration may required select final solution 
chapter classification example iris data effect separation linear spline svm applications larger complex classification problems attacked svc 
notably osuna 
applied svc problem face recognition encouraging results 
svc provides robust method pattern classi fication minimising overfitting problems adopting srm principle 
kernel function enables curse dimensionality addressed solution implicitly contains support vectors provide description significant data classification 
chapter support vector regression svms applied regression problems alternative loss function smola 
loss function modified include distance measure 
illustrates possible loss functions 
quadratic laplace huber insensitive loss functions loss function corresponds conventional squares error criterion 
loss function laplacian loss function sensitive outliers quadratic loss function 
huber proposed loss function robust loss function optimal properties underlying chapter support vector regression distribution data unknown 
loss functions produce sparseness support vectors 
address issue vapnik proposed loss function approximation huber loss function enables sparse set support vectors obtained 
linear regression consider problem approximating set data linear function 

optimal regression function minimum functional pre specified value slack variables representing upper lower constraints outputs system 
insensitive loss function insensitive loss function solution max max alternatively arg min xi xj xi xj 
yi yi yi chapter support vector regression constraints 

solving equation constraints equation determines lagrange multipliers regression function equation xi xr xs karush kuhn tucker kkt conditions satisfied solution 

support vectors points exactly lagrange multipliers greater zero 
get loss function optimisation problem simplified constraints min xi xj regression function equation iyi 
quadratic loss function ixi xr xs quadratic loss function 
chapter support vector regression solution max max yi xi xj 
corresponding optimisation simplified exploiting kkt conditions equation noting imply 
resultant optimisation problems constraints min xi xj iyi regression function equations 
huber loss function 
huber loss function solution max max resultant optimisation problems constraints min xi xj yi xi xj iyi 
chapter support vector regression table regression data regression function equations 
example consider example data set table 
svr solution laplace loss function additional capacity control shown 
non linear regression linear regression similarly classification problems non linear model usually required adequately model data 
manner non linear svc approach non linear map ping map data high dimensional feature space linear regression performed 
kernel approach employed address curse dimensionality 
non linear svr solution insensitive loss function chapter support vector regression max max constraints yi yi xi xj 

solving equation constraints equation determines lagrange multi regression function xi svs xi xj xi xr xi xs svc equality constraint may dropped kernel contains bias term accommodated kernel function regression function xi 
optimisation criteria loss functions chapter similarly obtained replacing dot product kernel function 
insensitive loss function attractive quadratic huber cost functions data points support vectors sv solution sparse 
quadratic loss function produces solution equivalent ridge regression zeroth order regularisation regularisation parameter examples illustrate non linear svr solutions various kernel functions model regression data table insensitive loss function additional capacity control 
shows svr solution degree polynomial sv circled 
dotted line describes insensitive region solution data points lie region chapter support vector regression zero error associated loss function 
result demonstrates support vectors insensitive region 
illustrates svr solution polynomial regression radial basis function 
example model flexible model function zero error associated loss function verified fact data points lie insensitive zone 
shows radial basis function regression svr solution linear spline kernel 
resulting model piecewise cubic spline due high capacity function able model data zero loss function error notice overfitting controlled 
shows svr solution infinite spline kernel similar solution spline kernel endpoints 
shows solution exponential rbf kernel piecewise linear spline 
model high capacity shows sensible behaviour extremity regions 
chapter support vector regression comments spline regression spline regression exponential rbf regression regression method necessary select representative loss function additional capacity control may required 
considerations prior knowledge problem distribution noise 
chapter support vector regression absence information huber robust loss function shown alternative vapnik 
vapnik developed insensitive loss function trade robust loss function huber enables sparsity svs 
implementation computationally expensive insensitive region drawbacks demonstrated section 
chapter regression example titanium data example considers titanium data illustrative example dimensional non linear regression problem 
methods controlling regression model loss function kernel additional capacity control results shown chapter obtained insensitive loss function different kernels different degrees capacity control 
illustrates solution linear spline kernel additional capacity control 
evident solution lies insensitive region 
illustrates titanium linear spline regression solution spline kernel additional capacity control 
particular spline kernel appear prone oscillation insensitive region linear spline kernel alternative loss function preferred 
illustrates solution gaussian rbf kernel additional capacity control 
seen rbf wide accurately model data 
illustrates solution gaussian rbf kernel additional chapter regression example titanium data titanium spline regression titanium gaussian rbf regression capacity control 
seen rbf able accurately model data 
expense oscillation penalised insensitive region 
illustrates solution exponential rbf titanium gaussian rbf regression kernel additional capacity control 
corresponding solution piece wise linear function consequently oscillation avoided 
illustrates chapter regression example titanium data titanium exponential rbf regression solution degree fourier kernel additional capacity control 
solution suffers similar problems wide gaussian rbf kernel kernel accurately model data 
illustrates solution linear spline titanium fourier regression degree kernel additional capacity control 
extra capacity control renders solution incapable accurately modelling peak data contrast 
illustrates solution spline kernel additional capacity control 
extra capacity control renders solution incapable accurately modelling peak data contrast 
examples shown representative set 
insensitive region exaggerated purposes illustration typically careful selection additional capacity control methods cross validation required 
insensitive loss function may inappropriate choice particular kernels causing solution oscillate insensitive region 
chapter regression example titanium data titanium linear spline regression applications titanium spline regression svr applied time series modelling problems mukherjee 
notably ller achieved excellent results applying svr data sets santa fe time series competition 
chapter support vector machines attractive approach data modelling 
combine generalisation control technique address curse dimensionality 
results global quadratic optimisation problem box constraints readily solved interior point methods 
kernel mapping provides unifying framework commonly employed model architectures enabling compar performed 
classification problems generalisation control obtained maximising margin corresponds minimisation weight vector canonical framework 
solution obtained set support vectors sparse 
lie boundary summarise information required separate data 
minimisation weight vector criterion regression problems modified loss function 
directions include tech nique choosing kernel function additional capacity control development kernels invariances 
appendix implementation issues resulting optimisation problems dependent number training exam ples 
data large methods proposed speeding algorithm decomposing problem smaller ones 
approximate stitson weston exact osuna methods proposed 
matlab implementations main support vector routines shown 
note routines optimised sense 
typically quadratic matrix badly conditioned render quadratic program incapable producing accurate solution 
address quick fix zero order regularisation large value perturb solution significantly 
note capacity control svr routines 
support vector classification optimisation problem expressed matrix notation constraints min zz 


matlab implementation yl appendix implementation issues function nsv alpha svc ker svc support vector classification usage nsv alpha bias svc ker parameters training inputs training targets ker kernel function upper bound non separable case nsv number support vectors alpha lagrange multipliers bias term author steve gunn srg ecs soton ac uk check correct number arguments help svc fprintf support vector classification fprintf size inf ker linear construct kernel matrix fprintf constructing zeros ker ones add small amount zero order regularisation avoid problems hessian badly conditioned eye size set parameters optimisation problem zeros set bounds alphas vub ones alphas zeros starting point ker set number equality constraints set constraint ax solve optimisation problem fprintf optimising st alpha lambda qp vub appendix implementation issues fprintf execution time seconds st fprintf status alpha alpha fprintf fprintf margin sqrt fprintf sum alpha sum alpha compute number support vectors epsilon alpha svi find alpha epsilon nsv length svi fprintf support vectors nsv nsv implicit bias explicit bias ker find average support vectors margin svs margin alphas alpha find alpha epsilon alpha epsilon length length sum svi alpha svi fprintf support vectors margin compute bias listing support vector classification matlab code support vector regression optimisation problem insensitive loss function expressed matrix notation constraints min xx xx xx xx xt hx 

xl yl 

appendix implementation issues matlab implementation function nsv beta bias svr ker loss svr support vector regression usage nsv beta bias svr ker loss parameters training inputs training targets ker kernel function upper bound non separable case loss loss function insensitivity nsv number support vectors beta difference lagrange multipliers bias bias term author steve gunn srg ecs soton ac uk check correct number arguments help svr fprintf support vector regressing fprintf size loss inf ker linear construct kernel matrix fprintf constructing zeros ker set parameters optimisation problem switch lower loss case hb ones ones zeros set bounds alphas vub ones alphas zeros starting point ker set number equality constraints ones ones set constraint ax case quadratic hb eye ones appendix implementation issues vub ones zeros starting point ker set number equality constraints ones set constraint ax disp error unknown loss function add small amount zero order regularisation avoid problems hessian badly conditioned rank equal note adding reg solution hb hb eye size hb solve optimisation problem fprintf optimising st alpha lambda qp hb vub fprintf execution time seconds st fprintf status switch lower loss case beta alpha alpha case quadratic beta alpha fprintf beta beta fprintf sum beta sum beta compute number support vectors epsilon abs beta svi find abs beta epsilon nsv length svi fprintf support vectors nsv nsv implicit bias bias explicit bias ker switch lower loss case find bias average support vectors interpolation error svs interpolation error alphas alpha find abs beta epsilon abs beta epsilon length bias length sum sign beta svi beta svi fprintf support vectors interpolation error compute bias bias max min appendix implementation issues case quadratic bias mean beta listing support vector regression matlab code appendix matlab svm toolbox matlab toolbox implementing svm freely available academic purposes 
download www isis ecs soton ac uk resources 
extract tar file svm tar matlab toolbox directory 

add matlab toolbox svm matlab path 

type help svm matlab prompt help 
main user interfaces classification regression respectively 
bibliography aizerman braverman theoretical foundations potential function method pattern recognition learning 
automation remote control 

theory reproducing kernels 
trans 
amer 
math 
soc 
bellman 
adaptive control processes 
princeton university press princeton nj 
blanz sch lkopf burges vapnik vetter 
comparison view object recognition algorithms realistic models 
von der malsburg von seelen sendhoff editors artificial neural networks icann pages berlin 
springer lecture notes computer science vol 

cortes vapnik 
support vector networks 
machine learning 

curve surface fitting splines 
monographs numerical analysis 
clarendon press oxford 
girosi 
equivalence sparse approximation support vector machines 
memo mit artificial intelligence laboratory 
gunn brown 
network performance assessment neuro fuzzy data modelling 
liu cohen berthold editors intelligent data analysis volume lecture notes computer science pages 
hadamard 
lectures cauchy problem linear partial differential equations 
yale university press 
heckman 
theory application penalized squares methods kernel hilbert spaces easy 
minoux 
mathematical programming theory algorithms 
john wiley sons 
bibliography mukherjee osuna girosi 
nonlinear prediction chaotic time series support vector machine 
principe morgan wilson editors neural networks signal processing vii proceedings ieee workshop new york 
ieee 

ller smola tsch sch lkopf kohlmorgen vapnik 
pre time series support vector machines 
sch lkopf burges smola editors advances kernel methods support vector learning pages cambridge ma 
mit press 
short version appeared icann springer lecture notes computer science 
osuna freund girosi 
improved training algorithm support vector machines 
principe morgan wilson editors neural networks signal processing vii proceedings ieee workshop pages new york 
ieee 
poggio torre koch 
computational vision regularization theory 
nature 
smola 
regression estimation support vector learning machines 
master thesis technische universit nchen 
smola sch lkopf 
kernel method pattern recognition regression approximation operator inversion 
algorithmica 
technical report gmd april 
stitson weston 
implementational issues support vector machines 
technical report csd tr computational intelligence group royal holloway university london 
vapnik 
nature statistical learning theory 
springer 
isbn 
vapnik 
statistical learning theory 
springer 
vapnik smola 
support vector method function approxi mation regression estimation signal processing 
mozer jordan petsche editors advances neural information processing systems pages cambridge ma 
mit press 
wahba 
spline models observational data 
series applied mathematics vol 
siam philadelphia 
