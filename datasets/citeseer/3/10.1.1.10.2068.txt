ieee transactions image processing vol 
july automatic image orientation detection aditya vailaya member ieee zhang senior member ieee yang feng liu anil jain fellow ieee algorithm automatic image orientation estimation bayesian learning framework 
demonstrate small codebook optimal size codebook selected modified mdl criterion extracted learning vector quantizer lvq estimate class conditional densities observed features needed bayesian methodology 
show principal component analysis pca linear discriminant analysis lda feature extraction mechanism remove redundancies high dimensional feature vectors classification 
proposed method compared different commonly classifiers nearest neighbor support vector machine svm mixture gaussians hierarchical discriminating regression hdr tree 
experiments database images shown proposed algorithm achieves accuracy approximately training set independent test set 
slight improvement classification accuracy achieved employing classifier combination techniques 
index terms author please supply keywords send blank mail keywords ieee org receive list suggested keywords 
ieee proof content image organization retrieval emerged important area computer vision multimedia computing due technological advances digital imaging storage networking 
development digital photography inexpensive scanners possible store vacation family photographs personal computers 
created need developing image management systems assist user storing indexing browsing retrieving images database 
image management systems require information true image orientation 
user scans picture expects resulting image displayed correct orientation regardless orientation photograph placed scanner 
image management system expected correctly orient input images 
currently operation orientation detection performed manually 
automatic image orientation detection difficult problem 
humans object recognition contextual manuscript received january revised february 
associate editor coordinating review manuscript approving publication dr 
vailaya agilent technologies palo alto ca usa mail aditya vailaya agilent com 

zhang microsoft research china beijing china mail microsoft com 
yang 
liu jain department computer science engineering michigan state university east lansing mi usa mail cse msu edu cse msu edu jain cse msu edu 
publisher item identifier tip 
ieee information identify correct orientation image 
unfortunately state art computer vision techniques infer high level knowledge abstraction objects real world 
alternative method exploit low level features spatial color distributions texture images orientation detection 
fig 
illustrates difficulty image orientation estimation true orientation detected recognize object image 
close images low contrast images images uniform homogeneous texture sunset sunrise indoor images pose additional problems robust orientation estimation 
assume input image restricted possible rotations multiples photograph scanned differ correct orientation rotation reasonable photographs placed scanner usually aligned horizontal vertical boundaries scanner plate 
orientation detection problem defined class classification problem image scanned photograph determine correct orientation possible orientations rotation 
note image arbitrary orientation easily rotated orientations aligning image boundaries horizontally vertically 
fig 
shows image possible orientations fig 
shows true orientation detected algorithms images fig 

literature image orientation detection sparse 
literature emphasized related topics page orientation detection 
bayesian framework image orientation detection 
spatial color moments features classification 
image represented multi dimensional feature vector classification problem uses bayesian learning framework 
class conditional probability density functions observed feature vector estimated learning vector quantization lvq technique 
organized follows 
bayesian learning framework classification results section ii 
section iii discusses methods feature extraction selection improve robustness classifier 
compare pro posed bayesian classifier classifiers commonly reported literature section vi 
section presents results classifier combination conclude section vi 
ii 
proposed method bayesian methods successfully adopted image analysis computer vision problems 
ieee transactions image processing vol 
july fig 

image orientation incorrectly estimated algorithm 
ieee proof fig 

orientation detection possible orientations image correct orientations detected algorithm 
content retrieval image databases just realized 
orientation detection problem formulated addressed bayes decision theory 
image represented feature vector extracted image 
probabilistic models required bayesian approach estimated training phase particular class conditional probability density functions observed feature vector estimated vector quantization vq framework :10.1.1.116.3824
consider training samples class vector quantizer extract codebook vectors training samples 
modified mdl principle described section ii select optimal codebook size class conditional density feature vector class approximated mixture gaussians identity covariance matrices centered codebook vector resulting proportion training samples assigned bayesian classifier defined maximum posteriori map criterion follows set pattern classes represents priori class probability 
number vq techniques developed proposed literature 
include entropy constrained vector quantization bayes vector quantization learning vector quantization lvq 
classification problem hand concentrate supervised algorithms vq 
consider vector quantization 
bayes vq parametric vector quantization technique needing measure posteriori density lvq nonparametric vector quantization technique 
attempt obtain posteriori estimates underlying probability density models generate data 
high significance image classification problems training data limited dimensionality feature set high 
circumstances parametric approaches generally perform worse nonparametric techniques 
experimental results comparing lvq classifier parametric approach mixture gaussians em algorithm estimate parameters mixture henceforth classifier referred mixture gaussian classifier section ii section iv demonstrate point 
mixture gaussian em classifier tends perform significantly worse high dimensional data performance improves dimensionality data reduced considerably ratio number training samples feature dimensionality increases 
due ability nonparametric approaches scale high dimensional data selected lvq vq algorithm approach 
implementation issues potential features represent image 
different features different abilities detect possible orientations scanned image 
global image features invariant image rotation local regional features classification 
image represented terms blocks features extracted local regions 
number features color moments color space color histograms color space edge direction histograms texture features evaluated ability discriminate possible orientations image 
feature saliency evaluated nearest neighbor classifier 
preliminary results showed spatial color moment features especially normalization see section iv re sults nn classifier normalized spatial color moment data yielded higher accuracy class classification problem color histogram edge direction histogram features 
certain image classes outdoor images tend uniformity spatial color distributions sky top typically blue color grass typically green bottom image direction illumination illumination usually top texture features lower variations changes image orientation 
trained bayesian classifier blocks mean variance values vailaya automatic image orientation detection components spatial color moment features 
features normalized scale follows represents th feature component feature vector represent range values th feature component training samples scaled feature component 
selecting classification scheme posed problem estimating underlying posterior probability distribution high dimensional feature vector dimensions relatively small number training samples 
order simplify problem assumed underlying class conditional probability distribution represented mixture gaussians bayes rule applied classification 
estimating parameters class conditional probability distribution difficult problem 
expectation maximization em technique vector quantization vq main techniques employed model data 
em technique attempts estimate parameters mixture gaussians vq attempts quantize data compression classification 
due high dimensionality feature vector nonparametric techniques expected outperform parametric techniques nonparametric techniques better scaling ability estimating class conditional probability distribution see section ii empirical demonstration property 
kohonen lvq ideal choice classifier supervised positive negative examples selecting codebook vectors nonparametric nature 
lvq estimating codebook vectors fitting gaussian kernels codebook vectors classification 
ideally gaussian kernels fitted codebook vectors estimated em algorithm 
due high dimensionality feature vector relatively small number training samples empirically estimate parameters gaussian case variance weight mean gaussian components represented normalized number training samples closest codebook vector codebook vectors respectively 
ieee proof selecting optimal codebook size key issue vector quantization density estimation choice codebook size 
clear training set vq approximated likelihood probability training set monotonically increase dimension codebook grows limit code vector training sample corresponding probability equal 
address issue adopt minimum description length mdl principle select optimal codebook size 
mdl criterion states description code length minimized estimate include data code length code lengths pa rameters 
resulting criterion choice codebook size term represents data code length second term represents code length parameters 
key observation mdl finding ml estimate equivalent finding shannon code observations shortest code length shannon optimal code length set observations obeying joint probability density function simply assumption independent samples joint likelihood eq 
written case marginals likelihood contains codebook vectors weights concerning parameter description length commonly choice sample size number real valued parameters needed specify th order model represents dimensionality feature space 
asymptotically optimal choice valid parameters depend data applicable 
weights fact estimated data estimated samples fall associated cell 
accordingly obtain modified mdl criterion term negative log likelihood observations second accounts weights third corresponds codebook vectors 
experimental results tested orientation detection system database images consists training samples samples class test samples samples class 
database consists professional quality images corel stock photo library 
lvq package described determine codebook vectors 
criterion described section ii determine codebook size 
fig 
shows plot criterion versus codebook size spatial color moment features classes 
code bits nats base natural logarithms respectively 
ieee transactions image processing vol 
july fig 

determining optimal codebook size 
book vectors class minimizes criterion classes 
classifier yielded accuracy training set independent test set 
compared lvq classifier mixture gaussian classifier em algorithm parameter estimation 
employed method described number mixture components automatically estimated mixture parameters incorporating mdl technique em steps 
em method employed supervised fashion learning mixture class separately classifying map classifier diagonal covariance assumed distribution 
mixture gaussian method yielded single gaussian component classes 
feel mainly artifact high dimensional nature feature set dimensions availability small number training samples samples 
mixture gaussian classifier yielded accuracy training data test data 
section iv demonstrates performance mixture gaussian classifier em approach improves significantly dimensionality feature set reduced 
ieee proof iii 
feature extraction performance classifier trained finite number samples starts deteriorating point features added curse dimensionality 
limited salient feature set simplifies pattern representation resulting classifiers built chosen representation 
consequently classifier faster memory able alleviate curse dimensionality 
reduce dimensionality feature vector utilized principal component analysis pca linear discriminant analysis lda 
pca wellknown feature extractor selects largest eigenvectors covariance matrix dimensional patterns new features pca unsupervised linear feature extraction method lda uses category information associated training pattern 
denote class scatter matrix class scatter matrix 
linear discriminant analysis finds subspace maximizes value lda extracts features 
applied methods extract features original dimensional data 
visualized feature extraction results dimensional dimensional projections original data fig 

pca able effectively reduce feature dimensionality maintaining greater variance original data resulting features help improving classification accuracy 
fact performance classifier trained reduced dimensional feature vector extracted pca dropped significantly compared classifier trained original dimensional feature vector training independent test data see fig 

pca unsupervised feature extraction method uncorrelated features extracted pca represent original dimensional patterns necessarily best pattern classification 
classifier trained features extracted lda yielded higher accuracies see fig 

chernoff faces represent mean vectors lda space categories fig 

clearly see images belonging different orientations separated dimensional feature space obtained lda 
method define features derive automatically input image hdr tree method 
relying human expert define features method automatically implicitly derives features training images building classifier 
due complexity extracting features terms time complexity robustness extracted features automatically small set training images employed hdr tree method derive features original feature vectors color moment features directly image 
iv 
classifier comparison large number classification schemes reported literature compare proposed lvq classifier known classifiers nearest neighbor rule nn support vector machine svm mixture gaussians hierarchical discriminating regression hdr tree 
choice classifiers observations nn known simplest implement nonparametric classification scheme svms comprehensive nonparametric classification scheme attempting apply feature normalization occam razor principle classifier design hdr tree powerful technique devised high dimensional data applies implicit feature extraction classifier design svm hdr tree shown specifically high dimensional feature space vailaya automatic image orientation detection ieee proof fig 

dimensional representation data pca lda 
original feature space projected space largest eigenvectors discriminant variables respectively 
dimensional representation lda space 
chernoff faces corresponding mean vectors categories lda space 
choice compare classification performance techniques mixture gaussian classifier suitable situations class conditional densities unimodal especially true high dimensional feature vectors 
nearest neighbor rule nn rule assigns test pattern majority class nearest neighbors performance optimized value separate training procedure nn rule 
robust classifier gives classification accuracy practice 
asymptotic error rate nn rule bounded twice bayes error rate 
drawback nn rule large computational requirement 
data properly scaled nn rule employing euclidean distance perform 
data normalization inevitable cases 
support vector machine support vector machine introduced vapnik utilizes structural risk minimization principle 
primarily dichotomy classifier 
optimization criterion width margin classes empty area decision boundary defined distance nearest training samples 
patterns called support vectors define classification function 
svms optimization methods maximize gap classes 
svm large margin separating classes small vc dimension yields generalization performance 
computational complexity training procedure quadratic minimization problem drawbacks svm 
number classifiers trained different kernels linear polynomial radial basis function sigmoid svm 
best classification accuracy achieved polynomial kernel function degree 
report results svm classifier 
hierarchical discriminating regression tree hdr algorithm casts classification regression problems unified regression framework 
unified view enables classification problems numeric information output space distance metric clustered class labels coarse fine classifications 
clustering performed output space input space internal node regression tree 
clustering output space provides virtual labels computing clusters input space 
features input space automatically derived clusters output space 
discriminating features span subspace internal node tree 
hierarchical probability distribution model applied ieee transactions image processing vol 
july resulting discriminating subspace internal node 
relax class training sample requirement traditional discriminant analysis techniques sample size dependent negative log likelihood nll employed 
mixture gaussian mixture gaussians model data set comprising distinct populations 
gaussian mixture model components written mixing probabilities set parameters mean covariance matrix defining th component gaussian mixture 
expectation maximization em algorithm commonly employed parametric technique estimating parameters mixture gaussians employed method described number mixture components automatically estimated mixture parameters incorporating mdl technique em steps 
em method unsupervised employed supervised fashion learning mixture class separately 
final classifier designed estimated mixture parameters classifying map classifier 
main limitations approach doesn scale large dimensional feature vectors complete training data class separately treated independent estimating mixture parameters 
ieee proof experimental results implemented nn algorithm ways voting distance weighted voting normalized data distance weighted normalized data 
hold method compute classification accuracy samples class training set samples class test set 
notice data normalization improves classification performance 
studied performance nn feature extracting pca lda 
classification results plotted fig 
shows nn lda best performance 
lvq classifier applied original normalized features features extracted pca features extracted lda 
accuracies function codebook size features extracted pca lda shown fig 

lvq lda achieves better performance experiments 
principle extracted total codebook vectors class problem lvq lda classifier 
due space constraints show graph criterion versus codebook size case 
classification accuracy maximized codebook vectors criterion function accuracy code length minimized codebook vectors 
order maintain consistent approach diagonal covariance gaussian kernel placed codebook vector 
assumption necessary fact em algorithm applied estimate parameters gaussian kernel representing codebook vector training samples falling codebook vector 
remove assumption full covariance matrix employ em algorithm lda derived features 
applied svm hdr tree mixture gaussian assumed diagonal covariance matrix component methods original training data 
classification accuracy svm training set test set 
time hdr tree obtained accuracy training data set test data set 
methods implicit data normalization 
mixture gaussian classifier yielded lower results accuracy training data test data 
applied svm mixture gaussian classifiers reduced feature space generated lda 
reduced feature space mixture component entries assumed full covariance matrix 
feature extraction leads speed classification improves classification accuracy 
accuracy mixture gaussian classifier improved training set test set 
table shows performance comparison classifiers sun ultrasparc machine mb ram 
emphasize performance classifiers impressive 
best results terms accuracy speed obtained lvq classifier mixture gaussian classifier lda feature extraction 
lvq mixture gaussian classifiers estimated number components codebook vectors total 
full covariance matrix mixture gaussian classifier increases computation time classification 
note lvq classifier maintains accuracy higher dimensionality mixture gaussian classifier scale appropriately 
feel due lack sufficient number training samples higher dimension affecting parameter estimation provided em algorithm 
major limitation lvq mixture gaussian classifiers attempt feature selection normalization classifier design 
onus left human designer come set features 
svm hdr tree attempt feature normalization extraction classifier design 
detailed results lvq classifier lda feature extraction 
fig 
shows subset images orientations correctly detected 
classifier performance near perfect long distance outdoor images images sky 
classifier performance drops difficult images images uniform texture background close images symmetric images images low contrast 
fig 
shows subset misclassified images 
classifier performance near perfect outdoor images tends decrease images constant background symmetric images classifier bases classification gradation illumination input image 
vailaya automatic image orientation detection fig 

error rates nn classifier scheme classification represented follows voting distance weighted voting normalized data distance weighted normalized data xa results shown feature extraction solid lines feature extraction pca dotted lines feature extraction lda dashed lines 
ieee proof fig 

accuracy lvq classifier features extracted pca lda 
classifier combination large number experimental studies shown classifier combination exploit discrimination ability individual feature sets classifiers 
bagging boosting commonly methods combining classifiers statistical re sampling techniques 
bagging uses bootstrap techniques randomly draw patterns replacement original training data size generate number new training sets 
different classifiers designed training sets 
final classification combined output linear combination various classifiers 
boosting statistical re sampling technique individual classifiers trained hierarchically learn subtle regions classification problem 
classifier hierarchy trained training set assigns weights patterns misclassified earlier stages 
number studies shown bagging boosting improve classification accu fig 

subset images database orientations correctly detected 
input images detected orientations 
racy weak classifiers classifiers near chance ieee transactions image processing vol 
july cies 
classifier performance bagging boosting guarantee improvement bagged boosted classifier may perform worse original classifier 
specifically mao showed boosting fact reduce classification accuracy robust efficient classifiers reject option 
mao argued robust classifiers misclassifications due lack sufficient discrimination ability underlying features classification 
circumstances boosting improve classification accuracies additional features higher discrimination ability misclassified patterns practical approach 
individual classifier performances results combining classifiers bagging ensemble 
bagging bootstrapped data sets created randomly drawing samples training set replacement original training set train component classifiers 
final decision classification output component classifier voting 
table ii shows classification accuracy bagging 
expected individual classifier performances bagging slightly improves classifier performance 
combined classifiers nn lvq hdr svm 
final decision voting output classifiers 
classification accuracy combined classifiers training dataset accuracy test dataset 
result slight improvement classification accuracy individual classifiers 
ieee proof vi 
automatic image orientation detection important problem extremely difficult practice contextual information available 
lvq classifier novel solution problem 
proposed approach gives high classification accuracy extremely fast images msec features extracted 
shown pca lda applied extract relatively small number features high dimensional data 
pca reduces feature dimension ality unsupervised nature improve classification accuracy lower classifier performance 
lda technique greatly reduced dimensionality data maintaining excellent classification accuracy 
compared proposed lvq classifier commonly classifiers hdr tree svm mixture gaussians parameters estimated em nn classifier 
svm classifier achieved accuracies comparable lvq classifier 
mixture gaussian classifier achieved excellent performance features extracted lda 
performance scale high dimensional feature vector 
limitation lvq classifier perform implicit feature selection normalization classifier design onus placed designer 
hand svm hdr tree classifiers built feature extraction normalization 
utilized fig 

subset images database misclassified 
input images detected orientations true orientations 
table performance comparison classifiers table ii performance bagging vailaya automatic image orientation detection classifier combination techniques improve accuracy robustness classifiers 
bailey jain note distance weighted nearest neighbor rules ieee trans 
syst man cybern vol 
smc pp 

breiman bagging predictors mach 
learn vol 
pp 

burges tutorial support vector machines pattern recognition data mining knowledge discovery vol 
pp 

algorithm text page orientation determination pattern recognit 
lett vol 
pp 

chaudhuri pal skew angle detection digitized indian script documents ieee trans 
pattern anal 
machine intell vol 
pp 
feb 
chou gray entropy constrained vector quantization ieee trans 
acoust speech signal processing vol 
pp 

cover thomas elements information theory 
new york wiley 
dempster laird rubin maximum likelihood incomplete data em algorithm statist 
soc 
vol 
pp 

duda hart stork eds pattern classification nd edition 
new york ny wiley 
figueiredo jain unsupervised selection estimation finite mixture models proc 
th int 
conf 
pattern recognition barcelona spain sept pp 

figueiredo leit unsupervised image restoration edge location compound gauss markov random fields mdl principle ieee trans 
image processing vol 
pp 
aug 
figueiredo leit jain fitting mixture models energy minimization methods computer vision pattern recognition hancock eds springer verlag pp 

freund schapire decision theoretic generalization line learning application boosting comp 
syst 
sci vol 
pp 

gersho gray vector quantization signal compression 
norwell ma kluwer 
gonzalez woods digital image processing 
reading ma addison wesley 
ieee proof gray vector quantization ieee assp mag vol :10.1.1.116.3824
pp 
apr 
gray olshen vector quantization density estimation sequences online 
available stanford edu gray compression html 
huang kumar boosting algorithms image classification proc 
taipei taiwan jan 

hwang weng hierarchical discriminant regression ieee trans 
pattern anal 
machine intell vol 
pp 
nov 
jain chandrasekaran dimensionality sample size considerations pattern recognition practice handbook statistics 
amsterdam netherlands north holland pp 

jain dubes algorithms clustering data 
englewood cliffs nj prentice hall 
jain duin mao statistical pattern recognition review ieee trans 
pattern anal 
machine intell vol 
pp 
jan 
kokkinakis skew angle estimation document processing cohen class distributions pattern recognit 
lett vol 
pp 

kittler duin combining classifiers ieee trans 
pattern anal 
machine intell vol 
pp 
mar 
kohonen self organization associative memory rd ed 
berlin germany springer verlag 
improved versions learning vector quantization proc 
int 
joint conf 
neural networks san diego ca june pp 

kohonen kangas laaksonen torkkola lvq pak program package correct application learning vector quantization algorithms proc 
int 
joint conf 
neural networks baltimore md june pp 

le wechsler automated page orientation skew angle detection binary document images pattern recognit vol 
pp 

mao case study bagging boosting basic ensembles neural networks ocr proc 
ieee int 
joint conf 
neural networks anchorage ak may 
mao jain texture classification segmentation multiresolution simultaneous autoregressive models pattern recognit vol 
pp 

gray olshen bayes risk weighted vector quantization posterior estimation image compression classification ieee trans 
image processing vol 
pp 
feb 
rissanen stochastic complexity statistical inquiry 
singapore world scientific 
shapiro computer vision 
englewood cliffs nj prentice hall 
duin bagging linear classifiers pattern recognit vol 
pp 

vailaya figueiredo jain 
zhang content hierarchical classification vacation images proc 
ieee multimedia systems vol 
florence italy june pp 

vailaya figueiredo jain 
zhang image classification content indexing trans 
image processing vol 
pp 
jan 
vailaya jain zhang image classification city images vs landscapes pattern recognit vol 
pp 

vailaya 
zhang jain automatic image orientation detection proc 
ieee icip kobe japan oct 
vapnik nature statistical learning theory 
new york springer verlag 
statistical learning theory 
ny wiley 
vasconcelos lippman library coding representation efficient video compression retrieval data compression conf 
snowbird ut 
bayesian framework semantic content characterization proc 
ieee conf 
computer vision pattern recognition santa barbara ca june pp 

yu jain robust fast skew detection algorithm generic documents pattern recognit vol 
pp 

aditya vailaya received tech 
degree indian institute technology delhi ph degrees michigan state university east lansing respectively 
joined agilent laboratories palo alto ca may currently applying pattern recognition techniques decision support bioscience research 
research interests include bioinformatics pattern recognition classification machine learning image video databases image understanding 
dr vailaya received best student award ieee international conference image processing 
ieee transactions image processing vol 
july zhang sm received ph degree technical university denmark degree university china electrical engineering respectively 
institute systems science national university singapore led projects video image content analysis retrieval computer vision 
worked massachusetts institute technology mit media lab cambridge visiting researcher 
research manager hewlett packard labs responsible research technology transfers areas multimedia management intelligent image processing internet media 
joined microsoft research asia currently senior researcher assistant managing director mainly charge media computing information processing research 
authored books referred papers book chapters special issues international journals multimedia processing content media retrieval internet media numerous patents pending applications 
currently serves editorial boards professional journals dozen committees international conferences 
dr zhang member acm 
ieee proof yang received degree automation university science technology china degree institute automation chinese academy sciences 
currently pursuing ph degree computer vision laboratory university maryland college park 
research interests include computer vision image processing pattern recognition 
feng liu received degree information computer education national taiwan normal university degree computer science michigan state university east lansing 
research interest includes medical images analysis pattern recognition 
anil jain sm university distinguished professor department computer science engineering michigan state university east lansing 
served department chair 
research interests include statistical pattern recognition computer vision biometric authentication 
author algorithms clustering data englewood cliffs nj prentice hall edited book real time object measurement classification berlin germany springer verlag edited books analysis interpretation range images berlin germany springer verlag markov random fields new york academic artificial neural networks pattern recognition amsterdam netherlands elsevier object recognition amsterdam netherlands elsevier biometrics personal identification networked society norwell ma kluwer 
received best awards certificates outstanding contributions ieee pattern recognition society 
received ieee transactions neural networks outstanding award 
editor chief ieee transactions pattern analysis machine intelligence 
fellow iapr 
received fulbright research award named fellow john simon memorial foundation 
