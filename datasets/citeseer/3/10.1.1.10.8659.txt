kernel whitening class classification david tax piotr fraunhofer institute ida berlin germany fraunhofer de pattern recognition group faculty applied science delft university technology cj delft netherlands piotr ph tn tudelft nl 
class classification tries describe class target data distinguish possible outlier objects 
obvious applications areas outliers diverse di cult expensive measure machine diagnostics medical applications 
order distinction target objects outliers representation data essential 
performance class classifiers critically depends scaling data harmed data distributions nonlinear subspaces 
presents simple preprocessing method actively tries map data spherical symmetric cluster insensitive data distributed subspaces 
uses techniques kernel pca rescale data kernel feature space unit variance 
transformed data described support vector data description basically fits hypersphere data 
presents methods preliminary experimental results 
machine learning pattern recognition research assumed training dataset available reflects expected practice 
data classifier regressor fitted generalization instances achieved 
unfortunately hard guarantee training data truly identically distributed sample real application 
data gathering process certain events easily missed low probability measuring costs changing environments 
order detect unexpected ill represented objects new incoming data classifier fitted detects objects resemble bulk training data sense 
goal class classification novelty detection outlier detection concept learning :10.1.1.40.3663
class objects target class distinguished possible objects outlier objects 
common solution outlier novelty detection fit probability density target data classify object outlier object falls region density lower threshold value 
works cases target data sampled 
means sample size su cient distribution representative 
density estimation requires large sample sizes 
boundary target class limited sample size estimated better directly fit boundary estimating complete target density 
vapnik principle avoid solving general problem needed solve 
principle problem changed density estimation domain description 
support vector data description svdd method tries fit directly boundary minimal volume target data performing density estimation 
inspired class support vector classifier 
objects inside hypersphere accepted classified target objects 
objects labeled outliers 
minimizing volume hypersphere hoped chance accepting outliers minimized 
linear class classifier idea separate data maximal margin origin :10.1.1.40.7416
linear classifier problem posed linear programming problem quadratic progamming problem 
general hypersphere model flexible give tight description target class analogous support vector classifier svc svdd flexible transforming objects input space representation representation kernel space 
appears kernels proposed svc svdd 
cases data classes elongated useful discrimination classes harmful class classification 
exception gaussian kernel performances obtained 
unfortunately gaussian kernel homogeneous input feature space assumed means distances directions space comparable 
practice data distributed subspaces resulting small typical distances objects directions perpendicular subspace 
moving inside subspace change objects just slightly moving subspace result illegal object outlier 
comparable distances traveled class memberships objects di er drastically 
homogeneity distances just harm svdd principle class methods rely distances similarities objects 
propose rescaling data kernel feature space robust large scale di erences scaling input data 
data kernel space variances data equal directions 
techniques kernel pca 
section svdd followed example fails 
section rescaling data followed experiments 
svdd describe domain dataset enclose data hypersphere minimum volume minimizing chance accepting outlier objects 
assume dimensional data set containing data objects tr hypersphere described center radius assume sum sum training objects means allow possibility outliers training set distance center need strictly smaller larger distances penalized 
extra parameter introduced trade volume hypersphere errors 
error function containing volume hypersphere distances minimized 
solution constrained requirement data hypersphere 
constraints incorporated error function applying lagrange multipliers 
yields function maximize respect details 
constraint influences ective range hyperparameter 
constraint met practice 
hyperparameter plays role comparable class classifier svc :10.1.1.40.7416
standard quadratic optimization problem 
box constraints free parameters optimization situations 
objects satisfy just objects 
analogous objects called support objects determine center hypersphere 
new object accepted description classified target object 
radius determined calculating distance center support vector boundary 
shape boundary dataset restricting satisfied general case 
analogous method vapnik replace inner products equations kernel functions positive definite kernel mercer kernel 
replacement inner product data implicitly mapped new feature space 
ideally mapping map data spherical constrained domain assumptions svdd fulfilled 
kernels proposed mainly application support vector classifiers 
popular choice polynomial kernel xy maps data feature space spanned monomial features class classification kernel works poorly tends transform data elongated flat structures spherical clusters 
especially larger degrees power stress di erences variances di erent feature directions 
large direction largest variance input space overwhelm smaller variances kernel space 
popular kernel gaussian kernel case exp 
width parameter kernel definition determines scale resolution data considered input space 
data implicitly mapped infinitely dimensional space inner products kernel outputs 
furthermore indicating objects length placing objects ectively hypersphere radius 
performance svdd gaussian kernel properly scaled distances required 
new inner product depends distance inhomogeneous distances result elongated clusters large empty areas target class input feature space accepted 
fig 

decision boundary svdd trained artificial dataset 
scatterplot artificial dimensional dataset show 
svdd trained fit boundary target data boundary 
svdd follows curve data fit tightly subspace structure data 
large strip inside curve classified target object contain target training objects 
caused large scale di erence data parallel perpendicular subspace 
approach linear hyperplane shaped boundary :10.1.1.40.7416
plane separate target data maximal margin origin feature space 
input space incomparable hypersphere approach method gaussian kernel method appears identical svdd 
kernel whitening directly fitting hypersphere kernel space propose rescale data equal variance 
fitting hypersphere rescaled space identical fitting ellipsoid original kernel space 
rescaling easily done derivation kernel pca 
data basically mapped principal components largest eigenvalues data covariance matrix rescaled corresponding eigenvalues 
eigenvectors eigenvalues covariance matrix kernel space estimated 
eigenvectors eigenvalues close equal zero disregarded 
assume data tr mapped kernel space possibly nonlinear mapping assume data centered space covariance matrix mapped dataset estimated 
eigenvectors eigenvalues satisfy cv 
equation shows eigenvectors non zero eigenvalue span mapped data means expanded 
multiplying equation left gives 
kernel matrix ij introduced appears coe cients equation directly obtained solving eigenvalue problem 
normal kernel pca eigenvectors normalized unit length means eigenvector rescaled 
assumed data centered done transforming original kernel matrix 
assume kernel matrix training data tst mn matrix new data possibly training data 
centered kernel matrix computed tst tst matrix matrix entries 
assume centered kernel matrices 
coe cients obtained new object mapped eigenvector means th component vector transform data representation equal variance feature direction directions normalization equation slightly adapted 
variance mapped data component var tr kk 
equation constant features normalization considered components 
dataset tr transformed mapping normalization class classifier 
dimensionality dataset depends principal components taken account 
features equal variances fact data mapped principal components covariance matrix data uncorrelated 
fact data properly scaled ideal estimate normal distribution svdd linear case just fits hypersphere 
fig 

data description sinusoidal distributed dataset 
left shows svdd trained input space right shows decision boundary hypersphere kernel space 
cases gaussian kernel 
artificial dataset shown data distributed sinusoidal subspace 
left subplot output svdd shown right subplot output svdd data scaled unit variance 
order model data class classifier flexible large amounts data available follow large sinusoidal structure tight subspace 
svdd optimized error target set 
decision boundary white line 
clear model subspace structure data 
characteristics whitening cient mapping data new representation unit variance depends choice kernel parameters 
feature extraction captures data structure easy train class classifier data obtain classification performance 
table decision boundaries artificial data di erent choices kernels shown 
left column shows results polynomial kernel degree top bottom 
left column shows results gaussian kernel 
results show large dependence choice free parameter 
rescaling tend overfit high values degree low values 
visually judged polynomial kernel reasonable gaussian kernel 
applying ill fitting kernel results spurious areas input space 
class classifiers rely distances objects input space 
data whitened kernel space significant eigenvectors taken account influence rescaling features eliminated 
table results rescaling features shown 
middle row scatterplot original data shown 
dataset svdd svdd whitened data non zero principal components svdd just principal components trained 
appears data just non zero principal components 
upper row table horizontal feature rescaled original size lower row data feature times enlarged 
svdd kernel whitened data gives tight description table 
influence choice kernel 
left column shows results polynomial kernel varying degrees right column gaussian kernel varying 
robust rescaling single feature 
svdd input space heavily su ers rescaling data 
just principal components mapped data results poorer results spurious areas 
fact data unit variance uncorrelated features normal distribution choice describing dataset kernel space 
sinusoidal data set shown prominent outlier 
furthermore typical decision boundaries fitted normal distribution left support vector data description right shown 
cases di erence decision boundary svdd gaussian model minor 
case training data contains significant outliers svdd tends obtain tighter descriptions ectively ignore prominent outliers data 
normal distribution influenced starts accept superfluous areas feature space 
visible 
cases decision boundary optimized training data rejected 
scaling svdd kernel whitening whitening reduced table 
influence scaling features 
left column shows decision boundary svdd middle column results data description whitening non zero variance directions right column shows output principal components 
middle row shows original data 
upper row horizontal feature times lower horizontal feature times enhanced 
display purposes data scaled show comparable scales 
experiments show results real world datasets standard concordia dataset digits stored black white images 
digit classes designated target class 
digits considered outliers 
training objects class testing objects class available 
typical images rejected objects shown 
class classifier trained class respectively 
images preprocessed retain variance remove pixels zero variance dataset 
data kernel whitened fig 

typical decision boundaries normal distribution left support vector data description right trained normalized data svdd tends tighter especially outliers training data 
fig 

examples rejected handwritten digits concordia dataset 
svdd trained class kernel whitened polynomial kernel 
polynomial kernel degree 
principal components chosen eigenvalues principal components factor smaller largest eigenvalue 
data normal svdd fitted target data rejected 
results show rejected objects skewed written contain big 
results shown compare outliers obtained normal pca kernel whitening polynomial kernel degree 
normal pca objects rejected 
look reasonable human interpretation 
kernel whitening processing objects rejected 
objects rejected methods instance upper left object pca second object kernel whitening 
objects specifically rejected fit particular model instance lower right object pca kernel whitening 
results concordia digit classes shown 
digit classes class classifiers trained roc curve computed 
fig 

svdd trained digit class 
left data preprocessed normal pca right kernel whitening polynomial 
gauss mog svdd ellipse ellipse ellipse gauss mog svdd ellipse ellipse ellipse fig 

auc errors classes concordia handwritten digits 
left shows auc error classes simple gaussian density mixture gaussians svdd whitening polynomial degree 
roc gives error outlier data varying values error target class 
roc curves error derived called area roc curve auc 
low values auc indicates separation target outlier data 
classes class classifiers trained 
methods density models normal density mixture gaussians clusters 
third basic svdd directly trained input space optimized target class rejected 
classifiers data mapped kernel whitening polynomial kernel 
principal components considered retaining variance 
left subplot data preprocessed 
density methods capable estimating density give highest auc error 
cases best performances obtained applying whitening procedure 
svdd perform poorly due relative low sample size complexity boundary high dimensional feature space 
whitening higher polynomial degrees su ers low sample size ects 
right subplot data preprocessed basic retain variance 
reduction dimensionality cases overlap classes introduced performance best whitening procedures deteriorate 
density methods outperform poorer whitening versions 
actual performance increase decrease mainly determined model fits data 
means whitening procedure performance obtained data distributed nonlinear subspace 
presents simple whitening preprocessing class classification problems 
uses idea kernel pca extract non linear principal features dataset 
mapping data new feature space implicitly defined kernel function feature directions zero variance removed features rescaled unit variance 
kernel pca rescaling resulting data zero mean identity covariance matrix 
data principle described class classifier 
preprocessing step class classifiers trained contain large di erences scale input space 
particular data non linear subspaces described 
class classifiers data distributed subspaces problematic data contains large di erences typical scale subspace perpendicular subspace 
suitable kernel kernel pca scale di erences data recognized modeled mapping 
transformed data equal variance feature direction 
subspace modeling comes price 
mapping requires reasonable sample size order extract complex non linear subspaces 
complex mappings principal components combination small sample sizes result overfitting data poor results independent test data 
drawback rescaling kernel pca basis expansion general sparse 
means projection test point principal direction training objects taken account 
large training sets expensive 
fortunately approximations reduce number objects expansion drastically :10.1.1.40.7416
problem choose kernel function values hyperparameters open 
test data available target outlier class evaluation model includes whitening classifier kernel space 
general case class classification just poorly represented outlier class estimating performance dataset give bad indication expected performance 
cases rely instance artificially generated outlier data 
research supported european community marie curie fellowship 
author solely responsible information communicated european commission responsible views results expressed 

bishop neural networks pattern recognition 
oxford university press walton street oxford ox dp 
tax class classification 
phd thesis delft university technology www ph tn tudelft nl thesis pdf 
network contraints multi objective optimization classification 
neural networks 
ritter outliers statistical pattern recognition application automatic chromosome classification 
pattern recognition letters 
bishop novelty detection neural network validation 
iee proceedings vision image signal processing 
special issue applications neural networks 
japkowicz myers gluck novelty detection approach classification 
proceedings fourteenth international joint conference artificial intelligence 

tarassenko hayton brady novelty detection identification masses mammograms 
proc 
fourth international iee conference artificial neural networks 
volume 

worden tomlinson novelty detection approach diagnose damage cracked beam 
proceedings spie 

vapnik statistical learning theory 
wiley 
scholkopf williamson smola shawe taylor sv estimation distribution support 
advances neural information processing systems 

campbell bennett linear programming approach novelty detection 
nips 

scholkopf smola muller nonlinear component analysis kernel eigenvalue problem 
neural computation 
smola learning kernels 
phd thesis university berlin 
cho recognition unconstrained handwritten numerals doubly selforganizing neural network 
international pattern recognition 

metz basic principles roc analysis 
seminars nuclear medicine viii 
