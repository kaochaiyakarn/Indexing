evaluating impact programming language features performance parallel applications cluster architectures konstantin berlin mary jacob jan prins bill pugh sadayappan jaime chau wen tseng department computer science university maryland college park md department computer science university north carolina chapel hill nc department computer information science ohio state university columbus oh 
evaluate impact programming language features performance parallel applications modern parallel architectures particularly demanding case sparse integer codes 
compare number programming languages pthreads openmp mpi upc shared distributed memory architectures 
find language features parallel programs easier write hide underlying communication costs target parallel architecture 
powerful compiler analysis optimization help reduce software overhead features fine grain remote accesses inherently expensive clusters 
avoid large reductions performance language features avoid degrading performance local computations 
parallel computing potentially provide huge amounts computation power solving important problems science engineering 
difficulty writing parallel programs poses major barrier exploiting power parallel architectures 
programming especially difficult applications irregular fine grain memory access patterns current parallel programming languages tools architectures evolving directions suited codes 
vital goals conflict choosing parallel programming paradigm clusters shared memory multiprocessors exploitation maximum machine performance particular platform portability code performance various high performance computing platforms programmability easy creation correct reliable efficient programs parallel programming languages designed making different tradeoffs depending assumptions underlying compiler runtime system hardware support target application characteristics acceptable user effort 
embarrassingly parallel applications coarse grain communication choice parallel programming language important languages achieve performance low programmer effort 
unfortunately current parallel programming paradigm satisfactory complex applications fine grain parallelism irregular remote accesses 
mpi portable achieves best performance distributed memory machines codes difficult program inefficient applications irregular fine grained accesses 
openmp pthreads simple efficient shared memory nodes clusters 
hpf portable limited flexibility applicability 
java popular widely adopted libraries apis efficient parallel execution clusters 
promising approach easing task writing codes fine grain parallel accesses programming languages provide flexible remote accesses support shared address space upc array fortran 
hybrid languages simplify code development programmers rely language support fine grain remote accesses get working version quickly selectively putting effort modifying small subset code enhanced performance 
comparison programming paradigms mpi require explicit communications inserted code correctness 
problem hybrid approach architectural trend building high supercomputers clusters pcs shared memory multiprocessors smps commodity parts approach yields systems expensive high latency inter processor communication 
result users parallel programming paradigms mpi efficiently support coarse grain bulk communications 
parallel programming paradigms upc rely fine grained remote accesses may find difficult achieve performance clusters underlying architecture efficiently support operations 
goal evaluate quantify performance parallel language features experimental evaluations number challenging parallel applications particularly requiring fine grain remote accesses 
identify programming language features reduce programmer effort quantify overhead encountered features 
attempt determine feasibility hybrid fine coarse grain parallel programming model cluster architectures 
pay special attention performance upc widely available commercially supported high level parallel programming language provides flexible non local accesses shared distributed memory paradigms 
attempt place evaluation context ongoing trends parallel architectures applications 
specifically contributions include 
experimental evaluation language features challenging irregular parallel applications 
observations programmability performance pthreads openmp mpi upc 
suggestions achieving programmability performance 
predictions impact architectural developments performance parallel language features findings fine grained parallel applications perform poorly cluster architectures surprising study quantifies performance penalty interesting programming languages challenging irregular benchmarks 
remainder explain choice evaluation parameters applications parallel languages experimental results 
observations programming language features impact performance followed number suggestions usage developing parallel applications 
conclude discussion impact architecture trends comparison related 
applications scientific applications regular memory access patterns easily parallelized implemented efficiently large number parallel architectures 
chose evaluation application classes complex represent challenging test cases parallel programming paradigms 
types parallel applications irregular table update parallel database operations viewed making irregular parallel accesses large distributed table values 
accesses perform associative reduction operations summation application similar large histogram may implemented bucket algorithm 
accesses may perform arbitrary read modify write operations case fine grain algorithms necessary 
amount computation table updates static may distributed evenly compile time 
table update potentially high communication requirements 
irregular dynamic accesses second class challenging parallel applications perform irregular parallel accesses sparse data structures 
application may allow limited amount coarse grained accesses 
amount computation static may distributed evenly compile time high communication requirements 
integer sort large memory sorting third parallel application class surprisingly difficult perform efficiently distributed memory parallel architectures 
parallel implementations possible including coarse fine grained algorithms 
sorting high communication requirements 
types benchmarks characterized irregular memory access large data structures 
depending benchmark coarse finegrained remote accesses may necessary 
programming paradigms broadly speaking parallel paradigms classified shared memory explicit threads pthreads java threads shared memory task data parallelism openmp hpf distributed memory explicit communication mpi global arrays distributed memory special global accesses array fortran upc 
describe paradigms study detail 
pthreads posix threads shared memory programming model parallelism takes form parallel function invocations lb 
parallel function body executed parallel threads access shared global data 
pthreads underlying implementation parallelism programming paradigms 
java general purpose programming language supports parallelism form threads ow 
parallel java programs smps resemble pthreads programs 
pthreads java available smps 
openmp shared memory programming model parallelism takes form parallel directives loops functions cmd 
openmp directives specify loops iterations executed parallel functions may invoked parallel 
additional directives specify data shared private thread 
compilers translate openmp programs code resembles pthreads programs parallel loop bodies parallel functions 
openmp industry standard supported languages platforms 
openmp currently available smps 
mpi message passing interface distributed memory programming model threads explicitly communicate functions mpi run time library send receive messages gls 
includes large selection efficient collective communication routines 
mpi widely available virtually parallel platform tuned performance 
despite programming effort required mpi current programming paradigm choice portability performance 
upc unified parallel shared memory programming model version extended global pointers data distribution declarations shared data cdc 
accesses global pointers translated interprocessor communication upc compiler 
distinguishing feature upc global pointers may cast local pointers efficient local access 
explicit way communication similar 
supported upc run time library routines upc upc 
compiler responsibility translate memory addresses insert inter processor communication 
upc commercially supported parallel paradigm supports flexible remote accesses shared memory abstraction 
performance evaluation believe performance key factor key factor determining success parallel programming paradigms 
gain insight factors underlying performance performed experimental performance evaluation number programming paradigms parallel platforms 
compaq alphaserver sc 
node cluster located ornl 
node smp gb memory es processors single quadrics network adapter 
nodes run alphaserver os mpi implementation built native quadrics libraries 
sun 
processor sun shared memory multiprocessor located university maryland ultrasparc iii processors gb memory crossbar interconnect running sunos 
table update performs irregular updates large distributed hash table 
updates commutative may reordered 
different versions table update mpi 
coarse grain algorithm uses buckets store updates data processors 
buckets synchronously exchanged processors buckets filled 
receiving buckets updates bucket applied local portion table 
upc 
fine grained algorithm uses global pointers update non local table elements 
upc bucket 
coarse grain algorithm uses algorithm mpi code 
way explicit communication transfer buckets processors 
pthreads 
shared memory code uses parallel function calls update table elements 
threads directly access table shared array 

shared memory code loops computing table elements openmp annotations 
java 
shared memory code uses java threads update shared global table 
presents performance table size compaq alphaserver sc mpi upc upc bucket 
performance measured number table updates millisecond processor log scale 
results show mpi greatly outperforms upc upc coarse grain bucket algorithm approach performance mpi 
upc suffers significant performance degradations fine grain access patterns software hardware overhead making pointwise remote accesses 
examine performance sun smp 
results show java pthreads openmp implementations achieve comparable performance java performance slightly higher possibly better tuned performance vendor 
sun upc compiler significantly poorer performance software overhead translating point wise accesses shared data 
conjugate gradient conjugate gradient benchmark nas cg benchmark finds principal eigenvalue sparse real matrix random pattern kn nonzeros inverse power method bbb 
involves solving linear system form ap different vectors solver uses conjugate gradient method repeatedly calculates sparse matrix vector product av dense vectors length benchmark widely stresses memory communication performance 
evaluated versions cg mpi 
fortran version taken nas suite uses explicit mpi communication operations 
implementation uses block block distribution replicates appropriate section dot product corresponding section total size implementation lines 
openmp 
shared memory implementation openmp directives derived nas serial code rwc japan total size lines 
implementation uses static partition processors row loop matrix vector product 
long lived parallel region reduce overheads successive sparse matrix vector products 
openmp distribution directives inserted initializations sparse matrix vector product dot products algorithm 
updates msec processor updates msec processor mflops processor fig 
table update alphaserver table processors fig 
table update sun smp table processors fig conjugate gradient alphaserver class processors mpi upc bucket upc java openmp pthreads upc mpi openmp upc mpi upc openmp upc openmp 
upc implementation derived openmp shared memory version 
rewritten openmp added new 
total size version lines 
distributes matrix block cyclic distribution large block size 
best distribution problem expressed directly upc explicitly partitioning matrix partitioned processors sparse vector matrix product portions held processor 
vector replicated reduce communication default strategy distributing shared vector leads run times orders magnitude larger due repeated fine grain random accesses sparse matrix vector product 
upc mpi 
upc implementation closely follows mpi algorithm 
uses explicit blocked distribution replicates vector coarse grain data movement upc upc replicate result total size version lines 
presents results class problem size cg alphaserver sc 
results reported mflops processor 
total number flops required defined problem size 
openmp results available processors node scale relatively poorly due replication processor caches misses random order 
mpi outperforms versions upc upc mpi implementation closer performance 
sequential performance upc implementations single processor mpi openmp performance 
mpi implementation achieves speedup processors speedup processors 
upc openmp speedup processors processors performs mpi implementation processors 
upc mpi speedup better processors processors performs mpi implementation processors 
performance cg heavily dependent memory system performance 
comparison vectorized implementation cg benchmark achieves mflops single processor nec sx mflops processor processors sx node 
integer sort integer sort performs parallel radix sort large collection integer data 
timed mpi upc implementations alphaserver sc 
implementations coarse grain parallel algorithms employing bulk explicit messages fine grain upc implementation inefficient 
key input data size 
performance reported efficiency 
results show mpi outperforms upc slightly difference increasing larger numbers processors 
upc microbenchmark experimental results entire applications showed fine grain algorithms exceedingly inefficient cluster architectures 
repeated experiments berkeley upc compiler cb amd athlon pc cluster ohio upc performance slightly improved relative mpi 
problem believe caused overhead fine grained accesses upc 
upc provides global shared pointers easily access non local data providing convenient shared memory abstraction parallel programming 
shared data element accessed completely transparent fashion process executing processor overhead direct pointwise access quite significant 
quantify hardware software overheads greater detail upc microbenchmarks evaluate performance wide range parallel architectures 
compaq alphaserver sc system falcon oak ridge national laboratory running version compaq upc compiler 
single node alphaserver marvel university florida running version compaq upc compiler 
amd athlon cluster dual processor nodes myrinet interconnect ohio supercomputer center running berkeley upc compiler 
sun system nodes university maryland running sun upc compiler 
cray system michigan tech university running original upc compiler 
sgi origin university north carolina running upc compiler 
measured cost direct point wise shared data access costs private shared pointers 
shows word access cost read modify write increment operation floating point doubles various modes access private local shared data accessed private data casting upc pointer private 
shared local local shared data accessed directly upc shared pointer shared node non local shared data local process smp node 
shared remote non local shared data different node 
authors dr alan george university florida gainesville providing access machine efficiency time operation ns fig 
integer sort alphaserver keys processors fig 
upc point wise data access costs read modify write double alpha sc alpha marvel sun amd pc cluster cray sgi origin private shared local shared node shared remote mpi upc observed systems significant difference access time private data shared local data data movement involved 
difference represents overhead translating shared upc node address pair 
overhead times local memory access cost compaq alphaserver earlier version compaq upc compiler 
compiler enhancements reduced overhead versions compiler times private data access cost 
area compiler optimization reduce software overhead memory access costs accessing non local data located node belonging thread node 
powerful compiler optimizations efficient local memory accesses situation demonstrated newer compaq upc compiler 
optimizations local shared data node shared data memory access costs orders magnitude higher access private memory upc alphaserver marvel system 
fine grain non local accesses sparingly performance critical sections parallel upc program 
evaluation summary summarizing results find smps threads paradigms closest underlying hardware provide best performance 
clusters paradigms explicit communication lowest overhead achieve best performance 
upc programs achieve performance written similar coarse grain style bulk communication routines performance extremely poor 
language features experimental evaluation observations suggestions high level language features 
number parallel programming languages provide language features providing illusion shared memory 
upc programming model provides access cyclically distributed shared arrays global pointers accessing local portions shared array global pointers may cast back local pointers greater efficiency 
addition upc run time library provides oneway coarse grained explicit communication primitives functions upc upc 
observations language features global shared memory programming model easy 
core upc programming model ability easily access non local data parallel program simply global pointers 
programmers need specify data distributed processors special global pointers 
fine grained upc programming model simple easy 
resulting code cleaner maintainable paradigms mpi require explicit communication program 
user level shared memory reflection clusters 
programming model may allow easy fine grain access non local data supported underlying hardware architecture 
interconnect nodes cluster typically provides high bandwidth long latencies making aggregate coarse grained communication efficient fine grained remote accesses 
problem worsen parallel architectures continue evolve clusters smps 
comparison coarse grain way communication primitives languages accurately reflect actual communication mechanisms supported hardware 
shared memory programming model encourage poor performance clusters 
fine grained shared memory programming model argue leads poor performance encouraging programmers write fine grain codes execute poorly clusters 
programmers code problem usually cost complicating programming model changing coarse grain algorithm 
dubious compiler techniques solve problem 
lack hardware support efficient fine grain communication clusters believe programmers need develop parallel algorithms coarse grain block data movement achieve performance 
compilers remove inefficiencies fine grain communication robustly transform fine grain parallel algorithms efficient block parallel codes clusters 
hybrid programming model combine fine grain coarse grain accesses 
advantage upc programming model allows integration fine grain remote accesses global pointers coarse grain explicit communication library routines upc upc 
stated previously hybrid programming paradigm upc ease development maintenance parallel codes 
program may written cleanly global pointers inserting explicit coarse grain communication performance critical sections 
experimental evaluation shows done resulting codes achieve performance close mpi clusters 
programmers extremely careful cost global pointers remote accesses high 
developing coarse grain parallel algorithms performance critical sections program may require extensive modifications algorithms data structures code 
programming language features avoid degrading local computations 
computations parallel programs performed purely local previously prefetched remote data 
parallel programming languages designed local computations compiled optimized native sequential compiler 
performance degrade significantly 
great deal success mpi attributed rule computations depend local data calls mpi communications functions return 
comparison upc require user inserted explicit copies remote global data local buffers casting global pointers local shared data local avoid excessive overhead 
simply accessing global shared data expensive global data may completely located locally 
instance accessing local data global pointer upc result times slowdown 
local 
advice choosing parallel paradigms summarize observations parallel language features follows 
language upc may support fine grain programming model achieve respectable performance clusters fine grain remote accesses sparingly 
coarse grain parallel algorithms bulk communication essential achieving performance 
fine grain parallel algorithms language compiler support improve performance compared naive implementations absolute performance clusters poor differences insignificant 
experiences believe prime factor choosing parallel paradigms nature algorithm 
coarse grain parallel algorithms clusters choices possible 
peak performance explicit message passing paradigms mpi provide best performance 
program development time issue choosing hybrid upc implementation selectively bulk collective communication upc upc routines computationally intensive portions program useful 
programming effort reduced exploiting existing libraries possible 
fine grain parallel algorithms fewer options 
implementations clusters fine grain language features extremely slow 
data size small codes may executed smps 
coarse grain alternatives developed possible 
cray original platform upc upc appears unqualified success best possible choices programming language paradigm 
suitability fine grain programming languages cluster environments higher latencies message overheads unclear 
obtaining performance shared memory cluster environment requires programming specific convoluted styles discarding easy features language 
advancing compiler technology help cases results environment complicated opaque performance model 
ability programmer write complicated fine grain parallel program confidence achieve performance range platforms distant dream 
impact trends parallel architectures wish evaluate parallel language features context ongoing architectural developments 
examine developments trends parallel computer architectures impact parallel programming paradigms 
faster interconnects 
high speed cluster interconnects continue improve bandwidth latency 
proprietary interconnects quadrics elan compaq alphaserver systems connecting commodity processors sci dolphin myrinet improving performance 
interconnects offer better support shared memory small messages sided communication may improve fine grain communication performance 
hand absolute performance inter processor communication steadily improving cost communication relative computation continues increase due faster nodes processors 
see technological developments reduce slow gap near 
larger memories 
memory latency increasing relative processor speeds memory size increasing due greater chip densities 
memory prices continue drop possible construct parallel systems larger amounts memory past 
cluster mpp systems built terabytes memory smps purchased gigabytes memory 
continuing increases smp memory size may allow run commercial applications previously limited clusters reducing demand vendor support complicated programming models 
processor memory integration 
processor memory pim designs potentially offer enormous improvements specific problems providing efficient parallel operations data 
obviate need interprocessor communication 
general utility designs depend communication performance 
specific aspects pim designs may start appear memory controllers conventional systems probably years away 
general pim systems increase cost non local memory accesses relative computation increasing reducing difficulty efficient parallel programming 
multithreading 
microprocessor design heading greater support multithreading tolerate increasing memory latencies 
increasing levels task level multithreading start single processor nodes mpp systems resemble smps accelerate shift hybrid programming models suitable cluster architectures 
news parallel architectures improve programs able process larger irregular problems quickly 
bad news efficiency parallel programs continue decrease 
related obviously tremendous amount research parallel language design benchmarking 
relevant analyzing performance upc 
el developing benchmarking upc codes cy ec ec discovered performance respectable coarse grain programming style adapted 
yelick developed upc translator compiler cb 
experiments show similar results fine grain accesses significantly expensive performance improves compiler aggregate remote accesses reduce costs 
comparison study wider range parallel languages slightly different set applications 
pugh similar benchmarks evaluate mpjava method developing high performance parallel computations java ps 
evaluated features number parallel programming languages mpi upc openmp java pthreads performance ease 
find languages upc support shared memory flexible non local accesses reduce difficulty parallel programming 
unfortunately parallel applications requiring fine grain accesses achieve poor performance clusters amount inherent software hardware overhead regardless programming paradigm language feature 
language support fine grain non local accesses prove useful reducing difficulty parallel programming 
decent performance achievable coarse grain bulk communication performance critical sections code 
bbb bailey barton browning carter dagum frederickson schreiber simon nas parallel benchmarks technical report rnr nasa ames research center march 
cb chen husbands yelick 
performance analysis berkeley upc compiler proceedings th annual international conference supercomputing ics june 
cdc carlson draper culler yelick brooks warren upc language specification center computing sciences technical report ccs tr may 
cmd chandra menon dagum maydan mcdonald parallel programming openmp morgan kaufmann publishers 
cy yao mohamed el 
performance monitoring evaluation upc implementation numa architecture proceedings international conference parallel distributed parallel systems ipdps april 
ec el chauvin upc benchmarking issues proceedings international conference parallel processing icpp september 
ec el 
upc performance potential npb experimental study proceedings sc baltimore november 
gls gropp lusk skjellum mpi portable parallel programming message passing interface mit press cambridge ma 
lb lewis berg multithreaded programming pthreads prentice hall 
ow oaks wong java threads 
nutshell handbook reilly associates 
ps pugh mpjava high performance message passing java java nio proceedings workshop languages compilers parallel computing college station tx october 
