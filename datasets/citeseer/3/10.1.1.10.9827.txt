pre processing noisy text alexander clark issco tim university geneva uni mail boulevard du pont ch geneva switzerland alex clark issco unige ch existing techniques tokenisation sentence boundary identification extremely accurate data perfectly clean mikheev applied successfully corpora news feeds post edited corpora 
informal written texts readily available growth informal text modalities irc icq sms interesting alternative better suited source lexical resources language models studies dialogue spontaneous speech 
high degree spelling errors irregularities idiosyncrasies punctuation white space capitalisation require specialised tools 
study design implementation tool pre processing normalisation noisy corpora 
argue having separate tools tokenisation segmentation spelling correction organised pipeline unified tool appropriate certain specific sorts errors 
describe noisy channel model character level perform 
describe sequence tokens needs divided various types depending characteristics modelling white space needs conditioned type preceding tokens 
trainable stochastic transducers model typographical errors orthographic changes variety sequence models white space different sorts tokens 
discuss training models various efficiency issues related decoding algorithm illustrate examples word corpus usenet news 

addresses problem pre processing noisy text 
concern just english text note approach applied languages 
texts noisy greater lesser extent post edited contain varying amounts typographical errors 
useful assumption processing texts errors isolated surrounded clean context correct errors 
interested difficult problem processing texts high proportion words may errors may difficult perform tokenisation identify words 
goal produce tool process arbitrarily noisy text text deliberately obfuscated reason example avoid detection discussions criminal nature 
discuss design implementation tool shall extensive formal evaluations partly due current difficulties obtaining suitable annotated data testing 
basic methodology shall describe detail standard noisy channel model employed widely fields speech recognition jelinek areas natural language processing machine translation spelling correction kernighan church gale area quite close 
brief consider text study produced underlying stochastic process language model data put noisy channel process converts sequence symbols representations words word classes produced model sequence characters 
speech recognition noisy channel acoustic model mapping fixed set words vocabulary system sequence phones acoustic features 
radically different process allow system unbounded vocabulary secondly process words mapped sequence characters different needs decomposed carefully sequence different levels incorporating minimum varying amounts white space tokens remembering course tokens written intervening white space various changes capitalisation words including obligatory addition capitalisation sentences optional capitalisation emphasis removal trailing periods abbreviation occurring immediately period 
addition various types typographical errors caused ignorance correct spelling true typing errors due lack manual dexterity keyboard 
note ways producing electronic text include handwriting recognition systems personal digital assistants pdas output speech recognition systems ocr printed documents 
different patterns error suspect handled system appropriate modifications 
suitable sources data investigate moment 

motivation studies involving large corpora news feeds obvious technical reasons exceptions brown corpus british national corpus bnc forthcoming american national corpus 
news feed corpora advantages tend relatively homogeneous register orthographic style mean capitalisation punctuation white space facilitates processing potential users 
news feeds economically important source information 
non news feed sources corpora tended come sources course transcriptions naturally occurring speech represented bnc certain extent alternatively sources highly post edited written text fiction 
corpora problems norms informal spoken language tend change quite rapidly fiction tends marked language quite uncommon registers 
informal written language widespread 
usenet distributed news service forum debate forms electronic communication text growing popularity internet irq icq mobile phones sms 
study language intrinsically interesting deserves study 
additionally useful source studies language 
particular interest extracting adjacency pairs dialogue processing quoted sections usenet posts 
shall provide illustrative examples corpus words usenet posts prepared past years 

pre processing time precise set tasks wish perform 
conceive processing texts sequence operations starting sort computer file stream detailed structured representation 
consider step parsing data format raw text character encoding parsing structured text format xml native file format commercial word processor done preliminary step components 
stages concerned take input sequence characters unicode including white space characters 
wish produce sequence tokens certain simple analyses 
particular wish identify boundaries tokens say sequences characters form token boundaries sentences form texts correct spelling words typographical errors words natural capitalisation altered 
worth pointing exact definition text token far clear matter 
take agnostic view matter 
standard english penn treebank marcus bnc tokenisation different criteria 
differences tokenisation schemes separated preceding word auxiliaries attached negative particles detached example isn decomposed tokens complex prepositions considered single tokens far 
languages similar questions addressed 
german uses noun noun compounding extensively compounds written internal white space 
perfectly legitimate consider segmentation compounds part tokenisation process 
show simple examples illustrate range errors want account explain seemingly simple task requires complex tool 
table examples noisy text 
talks age signed number words think thing getting frustrated wanted communicate 
tell wanted hot drink cold drink hungry tired having said children learn sign communicate different ways 

need street level empower political manipulations 
respect topic understood 
really don know start 
dj losing gear government local state federal years 
part say 
give concrete example output pre processing consider second line example table output produced consist information text consists sentence missing punctuation mark initial word capitalised sentence initial position word extremely misspelling extremely 
terms tokenisation treat string tokens depending finely wishes decompose text 
worth pausing consider problem difficult 
basic problem problems language ambiguity 
orthographic features writing text mean different things different places 
capitalisation mark words mark proper nouns adjectives france british 
period signify removal material abbreviation mark sentence boundaries 
dash minus sign hyphen minus sign signify various ranges parenthesis 
theory represented different characters texts dealing happen 
add problem interpreting noise correcting spelling mistakes clear problem non trivial 
problems interlinked 
identifying sentence boundaries requires resolving ambiguity capitalisation periods 
secondly spelling errors frequently introduce errors tokenisation 
common example english incorrect plurals confusion rare occasions errors introduce ore remove white space example 
addition compounds law english written hyphen space mark compound boundary 
example example difficulties involved resolving 
correcting spelling errors requires tokenisation order get sentential context identify words misspelled 
see tangled problem various processes interlocking complex way 
argue appropriate solution integrated tool simultaneously perform tokenisation spelling correction sentence boundary detection capitalisation processing find optimal near optimal solution computing areas ambiguity time 
number areas processing need done 
notably normalisation non standard words discussed sproat 

interesting important area feel deferred stage processing basically standard hmm tagging framework exhibit integration processes 
similarly part speech tagging shallow parsing word sense disambiguation performed loss precision 
initial area processing considered specific corpus corpora emails task segmentation text blocks requires identification signatures quoting problem areas 
assume performed prior processing described 
previous approaches tokens different levels 
ws raw unsegmented text describe briefly previous approaches related problems 
aware systems integrate processing extent advocate mikheev interesting step direction 
standard techniques spelling correction 
kernighan church gale seminal regard introducing noisy channel model perform spelling correction 
idea find sequence words rise observed sequence tokens 
involves components language model predict sequence correct words error model models conditional probability observed string correct string 
extended notably brill moore sophisticated error model get substantial improvements 
note primarily concerned errors post edited text model designed account plausible errors errors author notice error believe correct 
limitations include fact generally look errors words dictionary 
serious flaw large amount errors fact result correctly spelled errors 
example form dictionary 
authors simple spelling correction tools integrated email clients produce high proportion errors overlooked software 
tokenisation asian languages chinese japanese thai particular problems text processing general written explicit indications token boundaries form white space 
rise series algorithms general simple viterbi decoder choose optimal segmentation teahan 
approach points similarity gao 

english general white space pointed sproat 
analyses corpus real estate advertisements cost character written language model states language model tokens canonical tokens states orthographic transducer correct tokens observed tokens white space compact way frequently removing white space 
algorithms propose algorithm finite state automata viterbi decoding algorithm 
points similarity approach 
tokenisation english extensively studied se largely clean text accurate results obtained basic tools sed perl regular expression tools text tokenisation tool grover 

outline approach straightforward machine learning methodology generative models noisy channel method 
say produce generative model sequence characters noisy text sequence tokens separated white space 
sequence characters decoding algorithm find sequence tokens generated observed text formally sequence tokens highest probability observed unsegmented input 
previously discussed similar standard noisy channel model spelling correction kernighan church gale 
number alternative methods method chunking shallow parsing adapted task just converting letters words 
methods fail take account multi word context required disambiguate sort problems displayed examples 
previously described view problem requires integrated approach treating tokenisation sentence boundary detection spelling correction separate modules joined pipeline necessary processing noisy text integrate problems 
approach take system designed clean text gradually modify handle certain hard cases design system ground able process arbitrarily noisy text 
sections describe basic stochastic model various independence assumptions secondly specific models algorithms chosen implement 
describe model base algorithm 
effectively doing language modelling character level word level incorporating gram word model 
consider sequence characters consisting sequence tokens interspersed white space zero length 
view allow white space occur inside tokens allow non white space characters occur inside white space 
consider sequence tokens generated sort stochastic process model language model 
key element approach model process having sequence levels description tokens ranging observed strings set models govern production specific level 
describe briefly examples clarify definitions 
observed string actual sequence characters observed input may corrupted typographical errors 
correct string sequence characters uncorrupted 
may correct sense conforming norms written language 
canonical string representation string abstracted orthographic features capitalisation 
simplest case levels 
word observed string differ correct string word capitalised altered deliberately way correct string differ canonical string 
orthographic model canonical form word just form normally written sentence internal position 
correct form form written 
changes canonical form word capitalisation altered secondly trailing period removed word occurs period ellipsis 
role orthographic model handle transduction canonical form correct form 
number constraints done deterministic input canonical output correct forms correctly normalised 
technical reasons want program operate look ahead need allow limited non determinism 
error models allow process normal words 
moment model distinct processes typing errors spelling errors 
different properties 
true typing errors caused typist intends produce particular sequence key strokes fails spelling errors caused genuinely believes word particular incorrect way 
differ respects typing errors generally consist transposition adjacent letters substitution errors adjacent key substituted correct example substituted spelling errors hand involve phonological proximity particularly vowels substituted 
second difference typing errors independent stochastically 
spelling errors repeated word concerned re appears text written author 
texts need distinguish third process accidental typographical errors corrected editing process 
case subset typing errors superficially plausible 
glaring errors insertion corrected subtle errors remain 
white space type condition white space model preceding token 
minimum necessary 
fact periods punctuation marks written immediately previous token fact currency symbols written generally immediately preceding quantities modify justify 
aware longer distance effects white space factor 
note white space sentence boundary new lines punctuation marks followed space 
discussion model diagram shows structure model 
complexity model allows certain amount flexibility treat particular phenomena 
example people capitalisation 
modelled orthographic model error model 
model orthographic model add state states represent removal capitalisation add transitions appropriate transductions 
modelling error model involve having higher probability substitution errors replaced modelling errors composition transducers models changes case models errors change characters 
clearly independence assumptions case favour approach accordingly 
number situations model insufficiently general 
limitations decoding algorithm particular model correct words word list fast match 
encountered new word ended having encountered corresponding word ology constraints indicate misspelled 
technically difficult model process clear important practice 
secondly broad range errors errors limited individual token processed model 
particular typographical errors span token handled correctly 
isn ins fairly common error occurs times corpus 
handled separate substitution errors transposition 
errors involve sequence tokens tokens duplicated omitted 
examples easily ground year hold water 
better handled tagging shallow parser component just repetition deliberately worrying handled general 
phenomenon difficult handle model 
fan 
know film 
people expecting martin dawn dead 
spelling mistakes inside composite tokens treated properly 
compound token fast match handled properly need separate component dealing 
component probably necessary anyway feel serious problem interfere tokenisation 

specific algorithms describe algorithm model specifically 
decision treat problems time obviously leads substantially increased complexity 
taken care decompose tool various modules cleanly defined interfaces altered separately causing problems 
main modules system correspond basically various statistical models described sections 
modularity negative aspects 
certain amount overlap lexical resources identifying classes 
example clearly type token defined white space model type orthographic model may substantial overlap 
causes inefficiencies terms time classification may performed component terms space word lists may loaded component 
language model trigram language model interpolated kneser ney smoothing ney 
deterministic model states language model corresponding bigrams language model tokens 
deterministic language model 
having vocabulary oov token ignore called hierarchical language model allen incorporates separate models sequences characters unknown words 
different classes oov token model unknown words precisely 
give exhaustive list classes certain extent application dependent 
merely outline circumstances desirable separate type 
types course just basic words single invariant form 
vast majority forms 
true vocabulary tokens representing words seen 
practice want model occurrence tokens tokens seen include section words seen rare 
divide classes separate classes numbers normal words optionally separate classes different part speech tags example 
technically wish sets strings class disjoint vital result slightly deficient model 
addition situations desirable model token set strings 
variant spellings particular words due dialect variations colour color register variations tonight 
modelling alternations reduces amount data required 
secondly tokens variable length 
tables illustrate 
exclamation marks question marks free seen table shows frequency occurrence strings symbols 
table productive punctuation marks 


















table illustrates word corpus comparison frequency occurrence words penn treebank 
illustrating necessity productive model word highlights vast difference number examples get sort noisy corpus compared small clean corpus comes extremely common word 
table comparison counts variants penn treebank usenet corpus 
string penn treebank count usenet count similarly table shows small subset ways authors express laughter surprise 
class large number different types attested argument showing need model word list sophisticated simple list possibilities 
table productive word class 
ha ha ha hah ha ha hah fast match fast match component identify possible correct spellings observed words 
observed string input normalise removing non alphabetic characters retrieve fast match words sufficiently close terms levenshtein edit distance 
general word list fast match larger vocabulary language model 
experimented various algorithms including burkhard keller trees obtained best results prefix tree acceptor built reversed strings easy compute levenshtein distance suffixes string words word list efficiently 
orthographic transducer orthographic model responsible mapping canonical forms correct forms 
basically sorts changes 
model mapping canonical forms correct forms fully aligned non deterministic stochastic finite state transducer 
slight problems normalisation model 
basically independent processes need modelled capitalisation conversion upper lower cases removal trailing periods abbreviations periods ellipses 
processes performed consistently model stochastic process 
model independent transducers product reduces number parameters complexity considerably 
give concrete example transducer models transduction string canonical tokens way means observed text way means 
particular input string tokens possible output sequences emphasis applied different places 
problem mode analysis assume lose information particular phrases emphasized 
course turned problem practice add emphasized tokens language model 
white space models white space model critical 
basically different circumstances want distinguish 
normal situation white space normal words normally represented single white space character new line character line 
secondly space word invariably length zero 
thirdly space currency symbols numbers modify normally zero frequently spaces 
various classes model white space various different classes punctuation symbols 
specific models character trigram models boundary symbols structure capture limited length dependencies 
error model trainable stateless stochastic transducers model typographical errors 
bootstrapped models data kernighan church gale distribution errors data radically different results report 
intend experiment sophisticated error models moment feel additional computation justified 
focus modelling particular classes errors critical tokenisation incorrect 
viterbi decoder decoder component determines sequence language model tokens just observed strings 
viterbi decoder beam search 
character input store hypotheses terminate point log probabilities 
store best hypothesis ends language model state state orthographic transducer 
addition order reduce search space consider hypotheses sufficiently close best hypothesis 
take form considering best hypotheses considering hypotheses log probabilities lie certain amount best hypothesis 
necessary consider number hypotheses worst equal square vocabulary size 
addition ad hoc heuristics prune set tokens consider character unigram probability error model probability 
precise form algorithm complicated need operate orthographic model directions producing candidate tokens words fast match reversing process reconstructing canonical form new words 
shall describe 

discussion tool performing closely related set low level processing tasks arbitrarily noisy text 
argue intertwining different models integrated tool necessary 
part motivation acquire lexical resources formal registers english processing providing appropriate language models conversational speech newspaper newswire derived ones 
second motivation provide robust front written natural language dialogue accordingly implemented tool way run asynchronously input stream multi threaded environment 
time writing tool fully implemented java engaged tuning models performing various boot strapping operations 
modularity system allows run various different modes example assuming text spelling errors tokens separated white space 
penn tokenisation large corpus english usenet posts 
terms pressing concern perform proper evaluation 
time consuming manually annotate data currently crude produce draft manually correct 
obviously tend report errors significantly 
problem gold standard perfectly clear 
frequently different possible analyses fairly plausible 
check inter annotator agreement necessary 
additionally data mixed 
usenet corpus clean highly edited 
frequently people post excerpts press releases copyrighted news material 
interested segments data additional complexity approach justified 
currently single models components 
improvement mixture models track different styles typographical errors 
challenging problem separate intentional errors due probably ignorance correct spelling accidental typographical errors improved batch processing 
number areas people non standard orthography 
asterisks underscores emphasis really quite common 
sentence boundaries marked specific punctuation mark just white space particular new lines 
line length case important clue 
acknowledgments supported multimodal dialogue processing ip unige ch projects im mdm im project www im ch 
brill moore improved error model noisy channel spelling correction 
proceedings acl 
users guide british national corpus 
allen evaluating hierarchical hybrid language models 
proceedings icslp beijing china october 
grover matheson mikheev moens lt ttt flexible tokenisation tool 
proceedings lrec 
gao wang li lee unified approach statistical language modelling chinese 
proceedings icassp istanbul turkey 
jelinek statistical methods speech recognition mit press 
kernighan church gale spelling correction program noisy channel model 
proceedings coling pp 
mikheev 
periods capitalized words computational linguistics 
marcus santorini marcinkiewicz building large annotated corpus english penn treebank computational linguistics 
ney essen kneser 
structuring probabilistic dependencies stochastic language modelling 
computer speech language 
paul baker 
design wall street journal csr corpus 
proceedings darpa sls workshop february 
sproat black chen kumar ostendorf richards normalization non standard words 
computer speech language 
teahan wen mcnab witten compression algorithm chinese word segmentation 
computational linguistics 

