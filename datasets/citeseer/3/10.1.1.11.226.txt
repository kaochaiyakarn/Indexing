self segmentation sequences automatic formation hierarchies sequential behaviors ron sun chad sessions cecs dept university missouri columbia mo university alabama tuscaloosa cecs missouri edu appeared ieee transactions systems man cybernetics part cybernetics vol pp 

presents approach hierarchical reinforcement learning rely priori domain speci knowledge regarding hierarchical structures 
deals dicult problem compared existing 
involves learning segment action sequences create hierarchical structures example purpose dealing partially observable markov decision processes multiple limited memory memoryless modules 
segmentation reinforcement received task execution di erent levels control communicating sharing reinforcement estimates obtained 
algorithm segments action sequences reduce non markovian temporal dependencies seeks proper con gurations long short range dependencies facilitate learning task 
developing hierarchies facilitates extraction explicit hierarchical plans 
initial experiments demonstrate promise approach 
sequential behaviors sequential decision processes fundamental cognitive agents 
reinforcement learning rl acquiring sequential behaviors appropriate necessary domain speci priori knowledge available agents sutton barto barto kaelbling bertsekas tsitsiklis watkins :10.1.1.134.2462:10.1.1.134.2462
complexity di ering scales events world need hierarchical rl produce action sequences subsequences correspond domain structures 
demonstrated time terms facilitating learning dealing non markovian dependencies dayan hinton kaelbling lin wiering schmidhuber tadepalli dietterich parr russell dietterich 
di erent levels action correspond di erent levels abstraction dayan hinton knoblock tenenberg yang 
facilitates hierarchical planning studied traditional ai sacerdoti knoblock tenenberg yang sun sessions 
noticed shortcomings structurally pre determined hierarchies rl structures derived priori domain knowledge xed 
problems hierarchies include cost costly obtain priori domain knowledge form hierarchies exibility characteristics domain xed hierarchies built change time lack generality domain speci hierarchies vary domain domain 
limited learning ne tune structurally pre determined hierarchies parr russell dietterich problems persist :10.1.1.32.8206:10.1.1.32.8206
general approach automatically develop hierarchies actions scratch generic structures xed number levels automatically tailor details structures parameters reinforcement learning 
process amounts automatically segmenting action sequences self segmentation creating hierarchical organization action subsequences viewed macro actions subroutines 
subsequences resulting may reused sequence di erent sequences shared di erent tasks 
hierarchical organization sequential behaviors 
segmentation evidently important existing hierarchical rl models involve segmentation pre determined hierarchies sutton precup parr russell dietterich involve domain speci segmentation processes :10.1.1.32.8206:10.1.1.32.8206
contrast address issue learning segmentation little domain speci knowledge procedures important domain speci knowledge available costly obtain 
ring schmidhuber weiss thrun schwartz nevill manning witten 
shortcomings limited learning hierarchies include xed hierarchies exibility extent lack generality structures domain speci 
adaptive formation hierarchies carried multiple task context learning di erent sequences uence common subsequences may emerge 
result di erent tasks may share subsequences subroutines thrun schwartz 
models category rely domain speci knowledge procedures segmentation generic autonomous example lin singh reinforcement learning knoblock hierarchical planning kuniyoshi robotic learning 
domain speci knowledge available may advantageous avoid sake generality models widely applied 
dicult issue face rl deal non markovian temporal dependencies situations state transition probabilities determined current state action previous ones due partial observability lin mccallum kaelbling :10.1.1.134.2462:10.1.1.134.2462
dependencies create problems reinforcement learning 
reinforcement learning di erent levels may seek proper con gurations non markovian temporal dependencies goal reducing dependencies segmenting proper places di erent levels facilitate learning task see details 
need deal issues automatic self segmentation sequences segment handled di erently di erent module reduce remove non markovian temporal dependencies automatic development common subsequences subroutines subtasks various places sequence di erent sequences simplify non markovian dependencies form compact representations segmentation little priori domain speci knowledge embodied domain speci structures domain speci ways creating structures relying priori knowledge leads lack generality 
remainder develop approach sequences hierarchical fashion sss 
section review basics rl 
section sss algorithm 
section analyze algorithm various ways 
section experiments sss 
section discuss number issues including comparing sss existing 
note focus demonstrating algorithm examples 
leave depth theoretical analysis major undertaking 
review rl reinforcement learning rl sutton barto watkins barto bertsekas tsitsiklis kaelbling received great deal attention :10.1.1.134.2462:10.1.1.134.2462
rl viewed line variation value iteration dynamic programming dp 
dp general de ned follows tuple set state set actions probabilistic state transition function maps current state current action new state time step 
reactive stationary policy determines action current time step current state cost reward function maps certain states actions real values 
dp discrete time system state transitions depend actions performed agent 
probabilistic terms markovian process working determining new state state transition action performed prob js prob js determined policy assuming 
process costs rewards occur certain states actions 
normally costs rewards accumulate additively discount factor 
situations optimal actions point may dependent current state states actions occurring time ago 
see sondik monahan puterman treatments pomdps partially observable markovian decision processes 
cumulative cost reward estimate denote results optimal policy actions satis es bellman optimality equation max denotes cost reward assumed function states state new state resulting action notation max max bellman optimality equation number line rl algorithms learning functions 
learning algorithm watkins 
updating done completely line explicitly probability estimates max max determined action policy allows sucient exploration bertsekas tsitsiklis 
example boltzmann distribution prob temperature determines degree randomness action selection policies possible bertsekas tsitsiklis 
updating done actual state transition transition probabilities line simulation performed 
sampling transition frequency approach provides estimation 
result values earlier speci cation values 
learning allows completely autonomous learning scratch priori domain knowledge 
sss algorithm approach automatically developing hierarchical reinforcement learning systems sss algorithm stands self segmentation sequences 
different usually markovian domains learning deal non markovian domains partially observable markovian decision processes agent observes local information constitutes observational state observation bertsekas tsitsiklis true state 
true state may determined history observational states 
domains single learner may may consistent markovian policy available 
create hierachical structure involves multiple modules little memory dealing domains priori domain knowledge 
discussion term state general refers observational states 
general idea general idea follows number individual action modules referred modules 
selects actions performed step 
module corresponding controller cq determines step module continue relinquish control 
module currently control relinquishes control higher level controller aq decide module take current point 
state pair cq control 
cq selects continue select action regard current state ect environment generate reinforcement environment 
cq selects control returned aq proceed immediately select regard current state cq corresponding take 
cycle repeats cf 
idea options precup 
component system learned scratch contrast precup dietterich :10.1.1.32.8206:10.1.1.32.8206:10.1.1.32.8206
look component learns module tries receive reinforcement possible forced give 
performs optimization local segment set adjacent states 
local segments change time world static module 
learning self segmentation determined jointly cq aq concurrent local learning modules 
non static situation adds complexity learning 
cq tries determine current point advantageous terminate corresponding module continue terms maximizing reinforcement 
cq considers give way lead reinforcement 
partially determined performance executed subsequently selection aq 
cq deal problem dynamic interaction 
aq tries learn selection modules various points control action evaluating selection lead reinforcement 
static situation states decision points aq partially determined cq determines give control back aq turn determined select actions generate reinforcement 
cq pair learns selection aq may change 
level structure extended levels 
algorithm speci cation denote observational state agent particular moment necessarily complete description 
assume reinforcements associated current observational module continued reinforcement discounted sum reinforcements received module plus discounted sum reinforcement received subsequent active module terminated eventually 
module terminated reinforcement discounted sum reinforcement received subsequent 
state 
may deal discounted cumulative reinforcement undiscounted cumulative reinforcement nite horizon problems average reinforcement relatively straightforward extension 
focus learning illustrate idea rl algorithms applicable 
look minimal structure level hierarchy types learning modules individual action module performs actions learns learning 
individual controller cq cq learns module corresponding cq continue control give control 
learning accomplished separate learning 
controller aq performs learns control actions module select circumstances 
learning accomplished separate learning 
algorithm follows 
observe current state 
currently active cq pair takes control 
active system rst starts go step 

active cq selects performs control action cq ca di erent ca 
action chosen cq go step 
active selects performs action di erent 
active cq performs learning 
go step 

aq selects performs control action aq aa di erent aa select cq pair active 

aq performs learning 
go step 
learning learning rules explanations purposes detailed follows 
individual action modules 
active current action action corresponding cq usual learning rule 
max new state resulting action state explanation cq decides continue learns estimate reinforcement generated current action subsequent actions assuming greedy policy action highest value state plus reinforcement generated modules cq relinquishes control see learning rule 
value sum estimates parts 
note step delay application learning rule 
current action cq continue action cq module receives reward maximum value aq 
max aa aq aa new state resulting action state control returned aq cq aa action aq 
explanation corresponding cq terminates control current module aq take action state value discounted total reinforcement received point system action taken greedy policy action highest value current state followed aq current stochastic policies followed modules 
termination module learns total discounted reinforcement received system thereon greedy policy followed aq current stochastic policies followed modules 
combining explanations rules see module decides action action lead higher reinforcement current state subsequent modules 
remarks reinforcement goal reached learning rules amount assigning reward module number steps goal state terminated 
counting number steps goal simply discounting reinforcement achieve ect total amount discounting determined number steps goal 
terminated estimates way close gotten goal point 
way need separate rewards individual modules require priori domain knowledge set 
said cq 
individual controllers 
corresponding cq separate learning rules di erent actions 
current action cq continue learning rule usual learning 
cq continue max ca cq ca cq continue new state resulting action continue state ca control action cq explanation cq decides continue learns estimate reinforcement generated actions corresponding assuming cq gives control higher value plus reinforcement generated cq relinquishes control see learning rule 
cq learns value continue sum expected reinforcement generated expected reinforcement generated subsequent modules 
current action cq learning rule 
cq max aa aq aa cq note optimistic selections aq 
alternative sarsa learning rule aq aa formula aa actual action taken aq 
change produce estimated average reinforcement learning rules aq changed accordingly 
aa control action aq 
explanation cq ends learns value best action performed aq equal discounted total reinforcement accumulated system current state greedy policy followed aq current stochastic policies followed modules 
combining learning rules ect cq module learns decisions comparing giving continuing control lead reinforcement current state 
controllers 
aq learning rule 
aq aa ag max aa aq aa aq aa aa control action selects module take control new state aq required decision number time steps taken go determining amount discounting ag discounted cumulative reinforcement received control action state control action state ag 
explanation aq learns value control action selects discounted cumulative reinforcement chosen module accrue aq action decision plus accumulation reinforcement action assuming greedy policy followed aq current stochastic policies followed modules 
words aq estimates total reinforcement accrued greedy modules follow current policies 
ect aq learns select lower level modules comparing selections lead reinforcement current point 
technically need cq aq values closely related identical input representation 
discuss keep separate levels 
furthermore introduce di erent input representations di erent levels see section keep separate case levels 
want modules independent autonomous entities agents separation aq justi ed basis separate learning separate entities 
parameters sss sets parameters 
learning rates types modules aq cq parameters concerned randomness temperature stochastic decision making types modules aq cq may gradually lower learning rates temperatures respective schedules see section details 
algorithm due complex interactions necessary tune parameters generate proper performance 
combinations parameters initial conditions lead bad performance 
shortcoming severe popular numerical methods backpropagation algorithms 
experiments described showed wide ranges parameter values generate performance dependency parameter tuning limited 
temporal representation modules sss may need construct temporal representations order avoid excessive segmentation temporal dependency 
possibility recurrent neural networks elman 
rnns represent temporal dependencies various forms previous elman lin whitehead lin giles 
rnns shortcomings long learning time possible inaccuracy possibly improper generalizations 
theoretically rnn memorize arbitrarily long sequences practically due limitations precision length sequence memorized quite limited lin 
viable alternative rnns may decision trees rl line mccallum splits state statistical test shows signi cant di erences cases covered state state divided states adding features preceding steps provide accurate assessment discounted cumulative reinforcement received 
compared priori speci cations non markovian temporal dependencies specifying exactly preceding step relevant ahead time method widely applicable priori knowledge may available 
compared rnns method robust due limited precision temporal representation rnns fade away quickly steps rnns may trouble dealing long range dependencies 
compared full scale temporal representation fold state space method fold state space states corresponding preceding states advantage method may ideally seek relevant preceding steps involved temporal dependencies reduces complexity representation 
shortcoming method unconstrained search relevant historic features exhaustive costly 
extensions levels speci cation respect level hierarchy component denoted aq cq cq aq aq 
call zeroth order system 
easily extend levels 
top aq cq aq aq selecting aq cq determines corresponding aq give 
obtain rst order system way 
top aq aq selecting aq cq determining termination 
number rst order systems develop second order system 
process repeated number times 
step active aq cq pair level 
denote pair controllers active 
algorithm bottom level module 
observe current state 
currently active bottom level pair aq cq takes control 
active set activate controller aqm go step 

active cq selects action cq ca di erent ca 
action chosen cq set go step 
corresponding aq selects performs action aq di erent 
active aq cq perform learning respective ways 
go step 
moving hierarchy 
currently active controller level cq aq takes control 
cq selects action cq ca di erent ca 
selects continue 

cq decision continue cq aq pair selected aq activated 

cq aq perform learning respectively 

set go step moving hierarchy 
go step reaching bottom hierarchy 

cq decision nd currently active controller higher level cq aq 

cq aq perform learning respectively 

set go step moving hierarchy 
new learning rules follows penalty term set experiments 
active aq current action action time step cq 
aq max aq aq new state resulting action state current action cq continue action cq 
aq max aa aq aa aq penalty moving hierarchy new state resulting action state aq active module higher level time step 
active cq separate learning rules di erent actions 
cq continue max ca cq ca cq continue 
cq max aa aq aa cq new state resulting action aq state penalty moving hierarchy 
aq active module higher level current time step 
active aq learning rule current action action corresponding cq continue 
aq aa ag max aa aq aa aq aa aa control action selects lower level module new state encountered aa completed number time steps taken go determining amount discounting ag discounted cumulative reinforcement received current control action state control action state ag 
current action corresponding cq continue action cq 
aq aa ag max aa aq aa aq aa aa control action selects lower level module new state encountered aa completed 
aq active module higher level encountered 
active cq learning rule 
cq max aa aq aa cq penalty moving hierarchy aq active module higher level current time step 
learning rule continue 
cq continue ag max ca cq ca cq continue state cq decision number time steps taken go note di erent levels set di erent learning rates temperatures cq aq 
note level hierarchy easily simpli ed representation examining values cq aq selection lower level modules hierarchy levels easily collapsed level way 
hierarchical decision making advantages 
analysis analysis non markovian dependencies examine special cases non markovian dependencies particularly suitable sss 
simplicity assume level hierarchy 
non markovian tasks refer tasks non markovian dependencies de ned section exist markovian tasks refer tasks dependencies exist 

markovian turn non markovian tasks markovian ones 
simplest case dealing non markovian dependencies addressed similar limited model proposed wiering schmidhuber 
example wiering schmidhuber follows agent supposed turn left rst intersection turn right second 
non markovian sequence second turn dependent rst turn assuming states composed visual indicators intersections features 
switch di erent sub sequence rst turn chain markovian subsequences nonmarkovian sequence 
non markovian sequence may multiple markovian sequences chain higher level chaining provides necessary context local markovian decision making 
action modules handling respective subsequences di erent policies accordance global context respective positions chain 
segmentation turns non markovian sequence multiple markovian sequences chain accomplished sss implicitly comparing reinforcement received segmentation di erent points 
higher level lower level modules atemporal representation case memory sort 
better way segmentation lead higher amount reinforcement eventually better ways segmentation dominate 
extension approach wiering schmidhuber 
wiering schmidhuber model priori determined chain modules module select subgoal keep running goal achieved switched 

non markovian turn non markovian tasks markovian ones 
non markovian dependencies dense limited small temporal segments subsequences isolate deal separately small segments task treated markovian 
diculty lies determine local segments 
level structure lower level uses temporal representation higher level uses atemporal representation sss determine relevant subsequences implicitly comparing reinforcement received segmentation di erent points 
segmentation proper points separate local segments result largest reinforcement segmentation leads capturing dependencies points adopted 
algorithm typically adjusts lengths subsequences di erent modules accommodate di erent lengths non markovian dependencies di erent local segments 
clearly non markovian sequences long range dependencies handled way 
scenario represents portion non markovian situations 
guarantee possible trapped local minimum 
caveat applies 
addition local non markovian processes di erent seg refer process automatic temporal localization non 
refer sequences non markovian dependencies localized having temporal locality property opposed spatial locality property assumed dayan hinton deriving spatially hierarchical model 
local non markovian dependencies handled segmentation temporal representation unwieldy segments may generated modules may needed 

markovian non markovian meta sequencing handling long range sporadic non markovian dependencies 
level structure lower level uses atemporal representation higher level uses temporal representation sss seek handle long range dependencies 
assumption long range dependencies sporadic higher level process determine points sequence involved non markovian dependency lower level processes handle markovian processes 
ideally segmentation subsequences occurs points involved long range dependencies 
sss algorithm capable segmentation implicitly compare reinforcement received segmentation di erent points tend nd best way segmentation results largest reinforcement 
higher level temporal representation lower level atemporal representation segmentation points involved long range dependencies result largest reinforcement segmentation leads capturing dependencies involving states unnecessarily points adopted 

separating handling coexisting short range long range non markovian dependencies 
approach suitable dealing mixture long range short range global local non markovian dependencies 
presence types dependencies sss typically nds way separating di erent global local dependencies treat separately 
temporal representation higher lower level separation types dependencies accomplished comparing reinforcement received segmentation di erent points 
better ways segmentation treat global local nonmarkovian dependencies separately correctly result reinforcement eventually 
separation may various advantages 
example may simplify complexity temporal representation 
may facilitate convergence learning 
sss learns global local contexts seeking proper con gurations temporal structures global local dependencies 
characterize di erent dimensions non markovian temporal dependencies follows may shed light special cases 
mentation local subsequences captures certain global non markovian dependencies rst case 
local non markovian processes di erent segmentation se captures certain global non markovian dependencies 
possible atemporal representation higher level representation sucient handle existent global non markovian dependencies rst case 
degree dependency maximum number previous states current step dependent 
extreme degree unlimited nite 
extreme degree zero case temporal dependency 
distance dependency maximum number steps current step furthest previous state current step dependent intermediate steps 
extreme distance unlimited nite 
extreme distance zero 
density dependency ratio degree dependency distance dependency 
dimensions sporadic long range dependencies involve long distance low density temporal dependencies 
handled markovian non markovian meta sequencing dependencies captured segmentation points handled temporal representation higher level lower level markovian non markovian dependencies long distance low density need show lower level processes 
note possible markovian non markovian meta sequencing handle non markovian task long distance high density high degree temporal dependencies insulate lower level processes dependencies case parity problem sequentially 
markovian non markovian meta sequencing turn non markovian task markovian task special case low density long short distance dependencies disappear task properly segmented due hand localization lower level processes hand insulation states higher level process 
processes markovian low density temporal dependencies absorbed chain markovian processes 
non markovian turn non markovian tasks markovian ones typically involves temporal dependencies short distance high density high degree localized lower level processes 
separating handling coexisting global local non markovian dependencies typically involves cases temporal dependencies varied distances preferably naturally separated categories long distance short distance type handled corresponding level 
analysis diculties major diculties sss faces large search space 
simultaneous learning multiple levels involving control control individual action modules results larger search space system nd optimal near optimal con guration 
comparison structurally pre determined hierarchical rl smaller spaces deal 
cross level interactions 
adjacent levels higher level control interacts lower level control 
words learning lower level control higher level control turn higher level control need adapt learning performance lower level control 
complex dynamic interactions section 
structurally pre determined hierarchies deal problems level interactions 
interactions occur di erent modules level 
due learning performance module change ect modules 
modules adapt change learning outcomes turn ect original module 
complex dynamic interactions 
general possible training regimes model computationally simplest reduces interactions di erent components hierarchical system complex follows incremental elemental composite learning lin singh rst train system elemental tasks elemental task mapped particular module completion training elemental tasks train system composite tasks composed elemental tasks 
ect autonomous self segmentation case 
easiest setting sss needed 
simultaneous elemental composite learning singh tham train system elemental composite tasks associated task label simultaneously 
case elemental tasks learned composite tasks fact provide clues segmenting composite tasks 
segmentation case completely autonomous 
simultaneous composite learning train components system simultaneously composite tasks reinforcement information environment words system learns decompose task autonomously 
dicult setting focus sss 
sss accommodate methods require separate training di erent modules incremental training goes elemental composite tasks simultaneous training composite elemental tasks pre segmentation 
sss relies reinforcement completely autonomous self segmentation compares di erent amounts reinforcement resulting di erent ways segmentation tends choose best way basis 
experiments data choose maze domains tasks reinforcement learning demonstrating approach 
rst domain taken mccallum 
chosen simple involves multiple possible segments useful illustrating approach 
second domain taken tadepalli dietterich demonstrates reuse segments 
third domain adopted wiering schmidhuber 
larger demonstrate approach fares goal maze requiring segmentation 
number randomly chosen indicates unique observational state perceived agent 
relation size state space shared subroutines develop 
involves short range long range non markovian dependencies enables exploration interaction types 
domain show proper con gurations modules learned 
simplicity level hierarchy 
maze maze possible starting locations goal location 
agent occupies cell time step obtains local information observation concerning adjacent cells left right cell regarding opening wall 
move adjacent cell step go left go right go go 
adjacent cell wall agent remain original cell step 
see indicates starting cells 
number indicates observational state observation true state perceived agent 
domain minimum path length number steps optimal path starting cell goal cell 
paths optimal average path length measure quality set paths 
reinforcement structure applying learning simple reward reaching goal location reward punishment 
parameter settings learning modules follows value discount rate initial learning rates modules aq cq uniformly learning rates change number episodes completed initial temperatures aq cq temperatures change number episodes completed 
total number training episodes 
number steps allowed episode learning episode ends limit reached soon goal reached shown cells marked perceived agent 
di erent actions required cells order obtain shortest paths goal 
likewise cells marked perceived require di erent actions 
order remove non markovian dependencies module divide possible sequence starting cells goal number segments di erent modules segments consistent policy adopted module 
con rmed results experiments single agent atemporal representation learn task learning curve due oscillations goal goal goal di erent segmentations modules modules modules 
line indicates action sub sequence produced module learned sss 
averaging values 
adding temporal representation memory may partially remedy problem approach diculty dealing long range dependencies see earlier discussions comparable sss atemporal representation 
see maze experiments comparison sss uses temporal representation 
hand con rmed experiments sss segment sequences remove non markovian dependencies 
domain easily veri ed minimum cq pairs modules needed order remove non markovian dependencies module 
see 
nding optimal paths modules proven dicult agent able nd switching points modules exactly right sequence 
general domain modules easier segmentation done faster learning 
modules possibilities alternatives proper segmentation removes non markovian dependencies 
anova analysis number modules block training shows signi cant main ect number modules signi cant interaction number modules block training indicates number modules signi cant impact learning performance 
performance resulting systems shown conditions completely deterministic policy module stochastic policy boltzmann distribution action selection 
case indicated figures signi cant performance improvements terms percentages optimal path traversed terms average path length go incrementally modules modules 
pairwise tests successive average path lengths con rmed 
see test results 
segmentation result learning sss discerned set cq aq policies 
example set policies represent particular segmentation learned run sss shown 
variation reduce complexity domain facilitate learning limit starting location cells 
setting easier need concerned half maze see 
learning performance enhanced change 
change leads better learning performance modules 
possibilities reducing complexity start module 
learning performance enhanced change 
segmentation performance determined large extent inherent diculty domain deal 
performance modules available possibilities segmentation 
modules perc optimal path perc optimal path avg path length pairwise test ect number modules performance learning terms percentage optimal paths average path length maximum length imposed path 
state module right left cq continue module right left cq continue module right left cq continue aq state module right left cq continue module right left cq continue module right left cq continue aq state module right left cq continue module right left cq continue module right left cq continue aq state module right left cq continue module right left cq continue module right left cq continue aq state module right left cq continue module right left cq continue module right left cq continue aq state module right left cq continue module right left cq continue module right left cq continue aq learned values di erent modules result sss 
state cq aq values module listed indicates best value module state 
goal possible segmentation modules starting location allowed 
goal maze maze requiring segmentation reuse modules 
number randomly chosen indicates observational state perceived agent 
changes gradually relation change complexity domain 
experiments demonstrated proper segmentation possible sss 
learning set markovian processes atemporal representations higher lower level sss removed non markovian dependencies handled sequences learned chain markovian processes 
clear experiments guaranteed segmentation learning process succeed 
convergence optimal solutions probabilistic illustrated 
maze maze starting location marked goal location 
reaching goal location agent reach top arms 
step agent obtains local observation concerning adjacent cells regarding opening wall 
see number indicates observational state observation perceived agent 
agent move adjacent cell step go left go right go go 
adjacent cell wall agent remain original cell step 
reward reaching goal location visiting tops arms 
parameter settings modules follows value discount rate initial learning rates modules aq cq uniformly learning rates change number episodes completed initial temperatures aq cq temperatures change total number training episodes 
number steps allowed episode learning episode ends limit reached soon goal reached having reached tops arms 
domain shortest path consists steps 
minimum modules goal segmentation modules 
line indicates action sequence produced module learned sss 
modules alternate 
cq pairs needed atemporal input representation order remove nonmarkovian dependencies segmentation obtain shortest path 
con rmed results experiments single agent atemporal representation learn task 
modules exactly way segmenting sequence considering shortest path goal switching di erent module top cell arm marked switching middle cell arms marked 
see 
con rmed experiments segmentation allows repeated modules way goal 
reuse modules lead compression sequences 
modules added reuse modules reduced third module example place second module 
shown experiments domain unfortunately learning easier add modules 
domain exactly way segmentation switching top cell arm cell arms may lead optimal path advantage modules 
observed experiments slight statistically signi cant decrease performance modules added terms learning process learned product 
general modules slower learning anova analysis number modules block training shows signi cant main ect number modules signi cant interaction number modules block training indicates number modules signi cant negative detrimental impact learning 
learning terms performance resulting systems looked percentages optimal path deterministic stochastic condition average path lengths conditions 
see figures 
test shows increases average path lengths degradation performance corresponding increases modules modules statistically signi cant 
see example learned modules cq pairs 
experiments domain demonstrated point earlier regarding feasibility self segmentation 
non markovian dependencies removed set markovian processes atemporal representations 
furthermore experiments demonstrated reuse module di erent places sequence words possibility subroutines called number times 
reuse module occurs segments similar terms expected values 
reuse module improper places leads oscillations values leads lower values module 
reuse turn state module right left cq continue module right left cq continue aq state module right left cq continue module right left cq continue aq state module right left cq continue module right left cq continue aq state module right left cq continue module right left cq continue aq state module right left cq continue module right left cq continue aq resulting modules learning 
state cq aq values module listed indicates best value module state 
modules perc optimal path perc optimal path avg path length pairwise test ect number modules performance learning terms percentage optimal paths average path length maximum length 
places 
reuse modules signi cant advantages 
leads compression descriptions sequences related idea minimum description length thrun schwartz 
allows handling sequences eciently handled 
example domain handled simpler models segmentation limited linear chaining small number modules way reusing modules wiering schmidhuber 
simpler model handle domain modules chain modules formed 
approach resulted ineciency 
minimum modules required sss domain wiering schmidhuber model required modules order obtain optimal path 
increased number arms say modules required wiering schmidhuber model modules suce sss 
advantage training arms sss model applied maze arms retraining 
maze maze starting location marked goal location marked 
agent reach key marked rst order open door marked 
opening door agent reach goal 
see 
agent local information concerning adjacent cells regarding opening wall coordinates locations entities varied re training agent 
agent information regarding current location step 
agent move adjacent cell step go left go right go go 
reward reaching goal location reward punishment 
parameter settings learning modules follows value discount rate initial learning rates modules aq cq uniformly learning rates change number episodes completed initial temperatures aq cq temperatures change total number training episodes 
number steps allowed episode learning 
randomly select group cells maze possible locations sets cells possible locations respectively 
domain involves long range short range non markovian dependencies 
long range dependencies result requirement reaching sequentially order 
providing aq information requiring aq select lower level modules accomplish subtasks reduce long range dependencies ect short range dependencies aq aq ect skip intermediate steps 
shortening distance dependencies main advantages hierarchical reinforcement learning 
hand short range dependencies cq modules results lack current location information observational input relied cq modules action decisions distinguish di erent cells identical adjacent cell settings 
sum modules including cq aq deal non markovian dependencies 
maze maze involving long short range dependencies 
modules perc optimal path perc optimal path ect number modules performance learning terms percentage optimal paths 
mccallum method constructing temporal representation deal non markovian dependencies module 
method adopted appeared better alternative compared methods see section comparisons 
speci cally module method performs statistical tests di erences value distributions regard di erent state extensions consists current observational input plus features preceding steps 
test distinguishing di erent state extensions known test 
learning simpli ed experiments imposing order testing preceding steps goes backwards immediately preceding state action pair furthest away dependencies module ect short range 
results showed modules faster better learning terms average path lengths certain limit 
maximum steps allowed path 
anova analysis number modules block training showed signi cant main ect number modules signi cant interaction number modules block training indicated number modules signi cant positive impact learning 
learning terms performance resulting systems looked percentages optimal path deterministic stochastic condition 
tests showed signi cant di erences cases 
see figures 
turns learning di erent modules involved di erent history features desirable savings entails compared brute force approaches include history features fold state space 
example learning aq came rely information concerning locations select modules go information due non history past actions order correct selections modules 
example choosing module going aq chose module go 
cq modules hand came rely local information concerning adjacent cells utilized past states order disambiguate locations correct action decisions reaching respective subgoals 
separation long range short range dependencies dealt aq cq respectively resulted eciency compactness resulting policies 
shown experiments temporal representation module system acquisition temporal representation mccallum method took long time temporal dependencies long distance domain 
hand atemporal representation lower level modules segmentation segments created order remove local temporal dependencies 
correspondingly modules needed 
results shared subroutines developed sub tasks contributed compactness resulting representation 
completely separate sets modules di erent sub tasks modules shared di erent sub tasks fewer modules needed 
example episode pair locations episode module called go go di erent episodes 
sum experiments demonstrated separation long range non markovian temporal dependencies analyzed section possibility developing shared subroutines resulted eciency compactness policies 
observations discuss observations concerning sss number natural segments task vs number modules experiments indicate domain preferable number slightly modules natural segments order segmentation easier 
reuse subsequences subroutines sequence reusable subsequences developed sss segmenting sequence module may multiple times similar situations encountered multiple times 
capability useful developing compact action policy compact plan representation hierarchical planning sun sessions domains recurring structures 
subsequences subroutines shared multiple tasks modules automatically shared multiple tasks subtasks demonstrated experiments maze extra complicated processes mechanisms thrun schwartz 
sss straightforward process achieve compact representation domains shared sequential segments 
convergence experimental results clear sss guaranteed converge optimal policies 
futile look convergence proof 
empirically highly judging results sss achieves optimal policies practice 
appears algorithm advantageous temporal dependencies especially long range dependencies internal states needed disambiguation temporal dependencies long distance cases modules limited memory may 
algorithm may fail diverge oscillate modules handle existent temporal dependencies domain sucient exploration 
domains probabilistic state transitions noisy observations 
clear sss fare circumstances 
remain explored 
note experimentally compare sss precup dietterich parr russell algorithms require substantial amount priori domain knowledge comparable sss 
hand parr russell mccallum deal partial observability requiring priori knowledge segment sequences develop hierarchies subsequences comparable sss respect 
plan extraction sss enables extraction explicit hierarchical plans 
sun sessions proposed plan extraction algorithms constructed explicit plans value functions obtained reinforcement learning 
explicit plans perform open loop semi openloop execution environmental feedback rely sensing current state step rely sensing 
explicit plans useful closed loop policies situations discussed sun sessions due ability explicit sequencing open loop execution 
plan extraction leads compression policies sun sessions significant savings representation 
basically plan extraction algorithm applies beam search process focuses paths starting state goal state values learned rl collects best actions states encountered way explicit sequence 
details see sun sessions 
applying plan extraction algorithm maze extracted hierarchical plan rl algorithms learning learn closed loop policies implicit reactive plan explicit plan open loop semi open loop fashion barto way achieve goal actions selected form sequence ahead execution time run minimum feedback environment 
algorithm analyzed regard essential theoretical properties 
complete sound certain conditions see sun sessions details analysis 
plan go right call subplan call subplan call subplan call subplan call subplan call subplan 
subplan go right go go 
subplan go go go right 
hierarchical plan extraction possible domain segmentation 
note reinforcement learning subsequent plan extraction autonomous learning priori knowledge pre determined structures 
discussions hierarchical reinforcement learning 
existing models hierarchical reinforcement learning kaelbling :10.1.1.134.2462:10.1.1.134.2462
examining di erent ways doing hierarchical rl separate cases structurally pre determined domain speci hierarchies automatic building hierarchies 
rl number existing approaches assume xed domain speci hierarchy place learning starts 
mahadevan connell hand coded behavior subsumption hierarchy module particular subtask receiving separate special reward 
similarly pre wired di erential input features reward functions di erent modules ga optimize reward functions di erent modules 
hierarchical models dayan hinton parr russell dietterich learning level hierarchies involved domain speci hierarchical structures pre constructed priori knowledge 
sutton similarly included priori determined mixture models di erent levels abstraction model world di erent time scales see precup 
models sutton parr russell deal non markovian dependencies deal issue non adaptively precup 
wiering schmidhuber independently developed scheme similar sss 
applied xed structure chained lower level modules lower level modules adapted 
distinguish directions automatically building hierarchies upward downward building hierarchies 
example ring constructed hierarchies bottom lowest level nodes represent primitive actions setting nodes higher levels represent sequences actions chaining lowerlevel nodes see weiss 
model constructed chains solely sequential occurrence events 
lin pre training scheme trained system especially lower level modules elemental tasks rst trained system composite tasks consisted constituent elemental tasks 
singh developed cq model trained lower level modules stand elemental tasks applying ml gating methods learn assign components constituent elemental tasks composite tasks modules 
tham applied em gating methods similar way 
rosca ballard evolutionary programming techniques developing complex subroutines 
contrast moore atkeson created resolution hierarchies exploring state space domain speci ways splitting relevant states needed lower levels ner resolutions formed 
similarly sun peterson algorithm splits regions input space inconsistent errors built hierarchies input space regions downward 
approach automatically building hierarchies simultaneously multiple levels upward downward 
approach building hierarchies separate rewards di erent modules di erent levels mahadevan connell separate training modules di erent levels lin tham pre segmentation elemental tasks singh tham 
require structurally pre determined hierarchies priori knowledge dayan hinton dietterich parr russell :10.1.1.32.8206:10.1.1.32.8206
approach involve explicit global measures optimization measures roy kaelbling 
sss uses principle automatically forming hierarchies maximization expected reinforcement aspects learning action policies learning control policies learning control policies 
contrast uses additional auxiliary principles forming hierarchies example principle description length minimization thrun schwartz secondary error terms schmidhuber entropy measures schmidhuber 
comparing similar separately developed wiering schmidhuber general allowing levels limited single chain sss exible wait currently active lower level module achieve goal sss module learns multiple possible places 
control learns terminate module right point line comparison reinforcement received termination di erent points 
approach rely assumption module achieves speci goal possible 
course general problem dicult learn 
expect performance sss better algorithms rely domain speci knowledge 
note sss decisions regarding algorithm prior learning setting parameter values 
limitation sss segments action sequences perform state abstraction 
hierarchical planning 
hierarchical reinforcement learning clearly related idea hierarchical planning sacerdoti prominent planning literature 
hierarchical planning plan solution proposed rst solution subsequently reduced primitive plans procedural structures executed 
bacchus yang studied formal properties hierarchical decomposition operators re ned concrete operators iteratively primitive operators obtained identi ed conditions decomposition possible 
eger hayes roth proposed idea plans intention 
plans represent intention expressed constraints actions 
view planning require priori speci cation re nement operators allow agent exploit execution accordance plan knowledge acquired separately particular charac schmidhuber directly comparable deal supervised learning 
environments encountered 
contrast hierarchical organization xed reduction operators mere expression constraints 
learned stochastic reduction stochastic procedures priori speci ed exible sense pre determined steps goals routines operator reduction structured constraint speci cations 
long range dependencies 
important current issue machine learning deal long range non markovian temporal dependencies 
may priori domain speci knowledge structure models domain speci ways enable deal dependencies frasconi 
may unexpected events construct higher level model higher level handle dependencies remote schmidhuber 
may construct chain markovian processes reduce long range dependencies wiering schmidhuber 
contrast reinforcement criterion segmentation build hierarchies 
compared existing dealing long range dependencies approach relatively general non domain speci models pomdps 
non markovian dependencies created partial observability pomdps 
lin recurrent neural networks representing states compressed history traces captured hidden layers 
practical problem approach long time lags weaken trace hidden layers due limited precisions blur distinction states 
mccallum decision tree procedures split state inconsistent incorporating additional history features 
wiering schmidhuber reviewed earlier handle special cases pomdps subgoaling eliminate non markovian dependencies chain subgoals 
contrast algorithm uses combination similar extending subgoaling wiering schmidhuber temporal representation lin mccallum subsequences means simplifying learning dealing general types non markovian dependencies general handled wiering schmidhuber 
advantages sss 
summarize advantages follows sss need pre determined hierarchies priori domain speci structures help formation hierarchies domain speci built ways creating hierarchies 
sum requirement priori domain speci knowledge forming hierarchies knowledge may bene cially incorporated available 
sss uses principle forming hierarchies maximization expected reinforcement 
sss seeks somewhat proper optimal con gurations hierarchical structures correspondence temporal dependency structures domain extending wiering schmidhuber 
sss able develop subroutines potentially reused di erent points sequence wiering schmidhuber shared multiple di erent sequences thrun schwartz 
methods dealing pomdps include ring chrisman cassandra parr russell 
costs methods high 
see sondik monahan puterman 
relation classical planning hierarchical structuring sss viewed performing planning di erent levels abstraction knoblock lower level modules macro actions 

done line outlined 
example need scale algorithm deal larger state spaces function approximation schemes handle complexity values speed convergence dealt 
need look domains probabilistic state transitions noisy observations 
need systematically examine interplay long range short range temporal dependencies experiments gradually varying degree distance density dependencies systematic way 
may want test systems levels adaptively determine number levels systems learning way adaptive determination segmentation points level 
need perform detailed experimental mathematical analyses convergence properties algorithm 
bacchus yang 
downward re nement eciency hierarchical problem solving 
arti cial intelligence 

bertsekas tsitsiklis 
neuro dynamic programming 
athena scienti belmont ma 
cassandra kaelbling littman 
acting optimally partially observable stochastic domains 
proc 
th national conference arti cial intelligence 
morgan kaufmann san mateo ca 
chrisman 
reinforcement learning perceptual aliasing perceptual distinction approach 
proc 
aaai 

morgan kaufmann san mateo ca 
dayan hinton 
feudal reinforcement learning 
advances neural information processing systems 
mit press cambridge ma 
dietterich 
hierarchical reinforcement learning maxq value function decomposition 
www engr orst edu tgd cv pubs html 
elman 
finding structure time 
cognitive science 

frasconi gori soda 
recurrent neural networks prior knowledge sequence processing 
knowledge systems 

giles horne lin 
learning class large nite state machines recurrent neural network 
neural networks 

learning simple rl society mind 
technical report university cambridge computer laboratory 
kaelbling 
hierarchical learning stochastic domains preliminary results 
proc 
icml 
morgan kaufmann san francisco ca 
kaelbling littman moore 
reinforcement learning survey 
journal arti cial intelligence research 
knoblock tenenberg yang 
characterizing abstraction hierarchies planning 
proc aaai 

morgan kaufmann san mateo ca 
kuniyoshi inaba inoue 
learning watching extracting reusable task knowledge visual observation human performance 
ieee transactions robotics automation 
lin 
reinforcement learning robots neural networks 
ph thesis carnegie mellon university pittsburgh 
mahadevan connell 
automatic programming behavior robot reinforcement learning 
arti cial intelligence 

mccallum 
learning selective attention short term memory sequential tasks 
proc 
conference simulation adaptive behavior 

mit press cambridge ma 
mccallum 
reinforcement learning selective perception hidden state 
ph thesis department computer science university rochester rochester ny 

survey partially observable markov decision processes theory models algorithms 
management science 
moore atkeson 
parti game algorithm variable resolution reinforcement learning multidimensional state spaces 
machine learning 
nevill manning witten 
identifying hierarchical structure sequences linear time algorithm 
journal arti cial intelligence research 
parr russell 
approximating optimal policies partially observable stochastic domains 
proc 
ijcai 

morgan kaufmann san mateo ca 
parr russell 
reinforcement learning hierarchies machines 
advances neural information processing systems 
mit press cambridge ma 
precup sutton singh 
multi time models temporary planning 
advances neural information processing systems 
mit press cambridge ma 
eger hayes roth 
plan abstractly describe intended behavior 
proc 

duke university press durham nc 
puterman 
markov decision processes 
wiley inter science 
new york 
ring 
incremental development complex behaviors automatic construction sensory motor hierarchies 
proc 
icml 

morgan kaufmann san francisco ca 
rosca ballard 
evolution discovery hierarchical behavior 
proc 
aaai 
mit press 
cambridge ma 
roy miranda 
neural network learning theory polynomial time rbf algorithm 
manuscript 
sacerdoti 
planning hierarchy abstraction spaces 
arti cial intelligence 

schmidhuber 
learning complex extended sequences principle history compression 
neural computation 
schmidhuber 
learning unambiguous reduced sequence descriptions 
advances neural information processing systems 
singh 
learning solve markovian decision processes 
ph thesis university massachusetts amherst ma 
singh jaakkola jordan 
reinforcement learning soft state aggregation 
hanson cowan giles eds 
advances neural information processing systems 
morgan kaufmann san mateo ca 
sondik 
optimal control partially observable markov processes nite horizon discounted costs 
operations research 
sun peterson 
multi agent reinforcement learning weighting partitioning 
neural networks vol 
pp 
shortened version proceedings iconip springer verlag 
berlin 
sun sessions 
learning plans priori knowledge 
submitted publication 
shortened version proceedings ijcnn vol 
ieee press nj 
sutton 
td models modeling world mixture time scales 
proc 
icml 
morgan kaufmann san francisco ca 
tadepalli dietterich 
hierarchical explanation reinforcement learning 
proc 
international conference machine learning 

morgan kaufmann san francisco ca 
tham 
reinforcement learning multiple tasks hierarchical cmac architecture 
robotics autonomous systems 

thrun schwartz 
finding structure reinforcement learning 
neural information processing systems 
mit press cambridge ma 
watkins 
learning delayed rewards 
ph thesis cambridge university cambridge uk 
weiss 
distributed reinforcement learning 
robotics autonomous systems vol 
whitehead lin 
reinforcement learning non markov decision processes 
arti cial intelligence 


wiering schmidhuber 
hq learning 
adaptive behavior 

