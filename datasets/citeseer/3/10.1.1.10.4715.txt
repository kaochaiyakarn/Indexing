unbiased estimator variance fold cross validation bengio yves dept iro universit de montr montreal qc canada iro umontreal ca www iro umontreal ca dept iro technical report tr may st machine learning researchers perform quantitative experiments estimate generalization error compare performance different algorithms particular proposed algorithm 
order able draw statistically convincing important estimate uncertainty error error difference estimate 
studies commonly fold cross validation estimator generalization performance 
main theorem shows exists universal valid distributions unbiased estimator variance fold cross validation 
analysis accompanies result eigen decomposition covariance matrix errors different eigenvalues corresponding degrees freedom matrix components total variance 
analysis helps better understand nature problem naive estimators don take account error correlations due overlap training test sets grossly underestimate variance 
confirmed numerical experiments components variance compared difficulty learning problem number folds varied 
keywords cross validation variance estimators fold cross validation statistical comparisons algorithms error covariance estimating generalization machine learning standard measure accuracy trained models prediction error pe expected loss examples 
learning algorithms compared average performance estimates expected value prediction error epe training sets 
data distribution unknown pe epe computed 
amount data large pe estimated mean error hold test set 
usual variance estimates means independent samples computed derive error bars estimated prediction error assess statistical significance differences models 
hold technique account variance respect training set may considered inappropriate purpose algorithm comparison :10.1.1.37.3325
inefficient data forbids application small sample sizes 
situation resorts computer intensive resampling methods cross validation bootstrap estimate pe epe 
focus fold cross validation 
known crossvalidation provides unbiased estimate epe known variance may large :10.1.1.37.3325
variance estimated provide faithful confidence intervals pe epe test significance observed differences algorithms 
provides theoretical arguments showing difficulty estimation 
difficulties variance estimation addressed :10.1.1.37.3325:10.1.1.48.529
builds nadeau bengio investigated detail theoretical practical merits estimators variance cross validation 
analysis departs sampling procedure defining cross validation estimate 
considers independent training test splits focus standard fold cross validation procedure overlap test sets example original data set test example 
organized follows 
section defines measures performance algorithms estimation fold cross validation similar procedures delete jackknife 
theoretical findings summarized sections 
followed section experiments illustrating effect experimental conditions total variance decomposition components confirming underestimation variance obtained naive estimator commonly researchers 
general framework measures performance machine learning performance measure differs experimenter viewpoint 
applications interested finding best algorithm solving particular task hand specified particular training set information data generating process 
algorithm evaluation want compare learning algorithms different learning tasks care sensitivity learning algorithm choice training examples 
formally training set 
zn zi independently sampled unknown distribution learning algorithm maps data set arbitrary size function consider symmetric algorithms insensitive ordering examples training set discrepancy prediction observation measured loss functional typically quadratic loss regression misclassification loss classification 
function returned algorithm training set application evaluation goal learning usually stated minimization prediction error expected loss test examples pe expectation taken respect sampled algorithm evaluation really interested performances specific training set comparisons general basis 
context lowest level generality stated training sets size sampled performance learning algorithm measured expected performance functions returned situation epe expectation taken respect sampled independently sampled note types performances measure proposed example parameters defined predictability frameworks prequential analysis 
note notation random variables realization 
intended meaning specified clear context 
data distribution unknown pe epe computed 
estimated crucial assess uncertainty attached estimation application oriented experiments give confidence interval pe algorithm oriented experiments take account stability algorithm 
comparisons algorithms essential assess statistical significance observed differences estimate epe 
point overlooked estimating variance estimates pe epe requires caution 
hold estimates performance amount data large pe estimated mean error hold test set usual variance estimate means independent variables computed 
ideal situation independent training test sets available estimate applied compute variance epe training test examples independent test errors correlated test errors computed training set considered random variable 
illustrates crucial take correlations account 
mean variance estimators reported vs empirical variance hold estimate ideal situation independent training test sets available 
variance epe estimated independent experiments displayed dotted line 
average variance estimator ignoring correlations shows estimate highly biased large sample sizes variance estimator account correlations unbiased 
details experiment 
experiment ideal hold estimate epe 
independent training sets 
dk independent examples zi xi yi xi xi 
xid dimensional centered unit covariance gaussian variable yi xik independent centered unit variance gaussian variables 
independent test sets 
tk size sampled distribution 
learning algorithm consists fitting line ordinary squares estimate epe average quadratic loss test examples epe zi tk dk zi 
factor provides approximately 
estimates variance epe vs empirical variance epe shown bold curve experiments 
average variance estimators ignoring correlations dashed curve account correlations dotted curve displayed different training sample size estimate variance epe kn kn unbiased provided correlation test errors 
second estimate takes account correlations test errors 
looking suggests asymptotically naive estimator variance converges true variance 
shown formally advantage results long learning algorithm converges amount training data goes infinity function obtained depend particular training set 
limit correlations test errors converge 
rate convergence depend stability learning algorithm nature data distribution presence thick tails outliers slow convergence 
hold technique inefficient data forbids application real life applications small samples 
resort fold cross validation estimate pe epe 
fold cross validation estimates performance cross validation computer intensive technique available examples training test examples 
mimics training test sets repeatedly training algorithm times fraction training examples left testing purposes 
kind hold estimate performance lacks computational efficiency due repeated training meant lower variance estimate 
practice data set chunked disjoint subsets blocks size write tk th block dk training set obtained removing elements tk cross validation estimator defined average errors test block tk obtained training set deprived tk cv zi tk dk zi 
cv estimate pe epe 
question may pointless considering pe estimate epe relevant considering variance cv inform uncertainty pe epe 
hand training set enters definition cv approximation unbiased estimate pe 
general context proved suitable stability assumptions algorithm cv estimates pe accurately training error 
appealing result states cv accurate estimate pe hold testing 
statement apply pe prediction error randomized algorithm picking solutions uniformly dk hand cv explicitly defined learning algorithm function 
inner average definition cv average test loss dk estimates pe dk 
training sets 
dk clearly independent sampled outer average estimates epe 
adopt point view :10.1.1.37.3325
variance estimate epe provided hold estimate account test error dependencies due choice training set estimated single training test experiment 
situation complex additional dependencies due overlapping training sets 
dk 
describing situation detail summarizing results theoretical analysis sections detail procedures similar fold cross validation forthcoming analysis hold 
simplify analysis assume multiple precisely quadratic loss writing dk assuming xi yi zi tk yields cv xi xi weaker xi yi expectation taken respect 
yn 
note leave cross validation known fail estimate epe statistics 
failure due similarity training sets 
dk far representative samples drawn estimates fold cross validation type main variance estimates epe compare learning algorithms 
analysis applies version crossvalidation dedicated purpose want compare performances algorithms cross validation matched pairs method choice cv zi tk dk zi dk zi 
compared difference independent cross validation estimates cv avoids additional variability due train test splits 
application oriented experiments estimate pe expected error training seen section stability assumptions cv estimate pe 
alternatively may resort jackknife delete jackknife see estimate optimism bias mean error training examples estimate pe 
ideally estimate optimism average subsets size computationally intensive alternative dk zi zi 
zi dk link cross validation exhibited clearly expression jackknife estimate pe jk cv zi dk zi 
additional information jackknife estimates clues derivation reader referred 
generic notations studies variance statistics cv cv jk 
follows statistics denoted generic notation means observations ei split groups 
ei ei tk slightly abusing notation tk means zi tk dk zi cv tk ei dk zi dk zi kl zi cv zi jk note average identically distributed dependent variables 
asymptotically converges normally distributed variable completely characterized expectation variance var 
structure covariance matrix variance defined follows cov ei ej symmetry arguments permutations examples show distributions ei pairwise joint distributions ei ej identical 
result covariance matrix particular block structure possible values ij cov ei ej expression linear combination values 
lemma notation introduced section 
ei identically distributed ei 

pairs ei ej belonging test block jointly identically distributed ei ej 

pairs ei ej belonging different test blocks jointly identically distributed tk ei ej 
proof results derived immediately permutation invariance symmetry invariance respect permutations test blocks 
ei ei fk ei ej ei ej 
ei ej gk 

tk ei ej hk 
invariance respect permutations test blocks 

fk fk 
gk gk 
hk hk hk hk 
corollary covariance matrix cross validation errors 
en simple block structure depicted 
diagonal elements identical cov ei ei var ei 
diagonal entries diagonal blocks identical cov ei ej 
remaining entries identical tk cov ei ej 
corollary variance cross validation estimator linear combination moments cov ei ej structure covariance matrix 
problem estimating involve estimating covariances reduced estimating single variance parameter 
components intervene may interpreted follows fold cross validation estimate epe 
variance average taken training sets variance errors true test examples algorithm fed training sets size 
block covariance apply true test examples arises dependence test errors stemming common training set 

blocks covariance due dependence training sets share examples fact test block tk appears training sets forthcoming section structure show universal unbiased estimator 
unbiased estimator var exists consider generic estimator depends sequence cross validation errors 
en assume analytic function errors write taylor expansion ei 
show unbiased variance estimates var coefficients vanish second order coefficients lemma universal unbiased estimator var involves ei non quadratic way 
proof take expected value expressed equate var ei 
having possible values moments constant term depending moments similarly zero term ei 
third higher order coefficients 
zero quantities depending second order moments 
estimators include moments second moments expectation biased focus class estimators quadratic forms errors 
lemma expectation quadratic estimators defined linear combination terms defined follows wii tk tk wij tk wij trivial representer estimators expected value bs cs quadratic statistics invariants blocks blocks permutations described lemma tk tk tk proof result obtained exploiting corollary grouping terms equation expected values 
tk tk wii tk ae ce wij tk tk wij recognized expectation estimator defined equation 
lemma prove universally unbiased estimator var estimator var possible distributions theorem exists universally unbiased estimator var 
proof lemma prove result estimators quadratic forms expressed equation 
obtain expected value estimator equated var 
equality satisfied distributions cross validation errors satisfied admissible values 
imposes unsatisfiable constraints eigenanalysis covariance matrix way gain insight origin negative statement theorem eigenanalysis covariance decomposition performed analytically particular block structure displayed 
lemma vk binary vector indicating membership example test block eigensystem follows multiplicity eigenspace defined orthogonal basis vk multiplicity eigenspace defined orthogonal basis vk eigenvector 
proof corollary covariance matrix ee decomposed 
vk 
vk share eigenvectors eigenvalues equal zero eigenvector eigenvalue eigenspace defined orthogonal basis vk defines eigenvectors eigenvalues remaining eigenvectors eigenvalues 
lemma states vector decomposed uncorrelated parts projections subspace orthogonal vk projections subspace spanned vk orthogonal projection 
single vector example independent elements seen independent examples 
similarly projections equivalently represented respectively uncorrelated dimensional examples corresponding coordinates subspaces 
particular projection single dimensional point sample variance null resulting absence unbiased variance estimator 
projection eigenvector precisely 
unbiased estimate ar realization vector reason simple parametric assumptions gaussian maximum likelihood estimate defined 
estimated 
note problem addressed performing multiple fold splits data set 
procedure provide independent realizations possible values theorem states estimator unbiased demonstration shown bias quadratic estimator linear combination 
regarding estimation interesting see constraints restrict possible range quantities 
constraint linking mean variance ei restricted set values possible 
lemma cv cv inequalities hold shape admissible region corresponding set tighter inequalities displayed 
proof constraints result cauchy schwartz inequality provides cov var var possible values 
reasoning shows cv cv non negative covariance differences test errors training sets size test sets size variance average test error mean covariances 
variance covariance test errors affected variance average test error non negative test set size 
bound non negative 
type reasoning jk proved greater 
constraints simply rephrase eigenvalues covariance matrix non negative 
simpler looser form obtained experiments mentioned bias quadratic estimator linear combination 
admissible values provided preceding section suggest proved negligible compared section illustrates practice contribution variance due see equation order due suggests estimators take account correlations ei 
experiment true variance fold cross validation 
repeat experimental setup experiment realistic situation sample size available 
cross validation known sensitive instability algorithms addition standard setup consider outliers input xi xi 
xid dimensional mixture centered gaussian variables ti binary variable ti ti xi ti xi yi xik ti ti 
look variance fold cross validation decompose orthogonal components 
results shown 
outliers outliers bar plots contributions total variance ar cv due vs number training examples outliers contribution important small sample sizes 
large sample sizes variance considerably reduced mainly caused situations learning algorithm returns similar answers training sets 
outliers little effect contribution order ratio examples free parameters large 
difficult situations varies realization neglecting effect expected introduce bias order true variance 
interesting see quantities affected number folds decomposition imply set sign order minimize variance 
modifying affects size overlaps training sets 
dk illustrated 
fixed sample size variance contribution effects varies smoothly experiments outliers illustrate general trend variance decomposition variance components 
minimum variance reached intermediate value outliers outliers bar plots contributions vs 
special cases hold estimate epe having independent training test sets structure hold errors resemble cross validation errors know independence training test sets 
knowledge allows build course mean affected process 
unbiased variance estimate described 
seen directly proof theorem knowing removes third equation linear system 
practice restricted ordinary hold test allows estimate variance due finite test set due particular choice training set 
fold cross validation fold cross validation advocated perform hypothesis testing :10.1.1.37.3325
special case fold cross validation training blocks mutually independent overlap 
independence modify structure sense null 
block correlation stems fact training block test block vice versa 
leave cross validation leave cross validation particular case fold cross validation structure covariance matrix simplified diagonal blocks 
estimation difficulties remain particular case unbiased estimate variance 
definition lemma linear system reads admits solution 
known fold cross validation may suffer high variability responsible bad choices model selection erratic behavior estimated expected prediction error 
show estimating variance fold cross validation difficult 
problem due dependencies test errors induce absence redundant pieces information regarding average test error fold cross validation estimate 
result unbiased estimator variance fold cross validation 
experimental section shows simple cases bias incurred ignoring dependencies test errors order variance 
experiments illustrate assessment significance observed differences cross validation scores treated caution 
problem step study consists building comparing variance estimators dedicated specific structure test error dependencies 
alpaydin :10.1.1.10.4715
combined cv test comparing supervised classification learning algorithms 
neural computation 
anthony holden 
cross validation binary classification real valued functions theoretical analysis 
proceedings international conference computational learning theory pages 
blum kalai langford 
beating hold bounds progressive cross validation 
proceedings international conference computational learning theory pages 
breiman :10.1.1.37.3325
heuristics instability stabilization model selection 
annals statistics 
dawid 
prequential analysis 
kotz read banks editors encyclopedia statistical sciences update volume pages 
wiley interscience 
dietterich :10.1.1.37.3325
approximate statistical tests comparing supervised classification learning algorithms 
neural computation 
efron tibshirani 
bootstrap volume monographs statistics applied probability 
chapman hall 
hastie tibshirani 
generalized additive models volume monographs statistics applied probability 
chapman hall 
kearns ron 
algorithmic stability sanity check bounds leave cross validation 
neural computation 
kohavi 
study cross validation bootstrap accuracy estimation model selection 
proceedings fourteenth international joint conference artificial intelligence pages 
nadeau bengio 
inference generalization error 
machine learning appear 
stone 
cross choice assessment statistical predictions 
journal royal statistical society 

