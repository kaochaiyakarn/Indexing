learning coordinate cora toledo national laboratory advanced computer science col mexico cora mx nicholas jennings school electronics computer science university southampton southampton bj uk 
ecs soton ac uk examines potential impact introducing learning capabilities autonomous agents decisions run time mechanism exploit order coordinate activities 
specifically efficacy learning evaluated making decisions involved determining coordinate 
motivating hypothesis deal dynamic unpredictable environments important agents learn right situations attempt coordinate right method situations 
hypothesis evaluated empirically reinforcement algorithms grid world scenario agent predictions agents environment approximately correct agent correctly predict behaviour 
results show learning effective comes making decision selecting coordination mechanism 
effective coordination essential autonomous agents achieve goals multiagent system mas 
coordination required manage various forms dependency naturally occur agents inter linked objectives share common environment shared resources 
variety protocols structures developed address coordination problem 
range long term social laws medium term mechanisms partial global planning organizational structuring market protocols shot short term mechanisms contract net protocol :10.1.1.48.8362
coordination mechanisms different properties characteristics suited different types tasks environments 
vary degree coordination prescribed design time amount time effort require set coordination episode run time degree successful produce coordinated behaviour situation 
majority cases dimensions act forces opposing directions coordination mechanisms guaranteed succeed typically high set maintenance costs mechanisms lower set costs done author member intelligence agents multimedia group university southampton 
fail 
coordination mechanism works reasonably static environment perform poorly dynamic fast changing 
short universally best coordination mechanism 
situation believe important agents variety coordination mechanisms varying properties disposal select particular mechanism appropriate task hand 
particularly important tasks agents may choose adopt coordination mechanism highly succeed invariably correspondingly large set cost 
important tasks mechanism succeed lower set costs may appropriate 
date choice coordination mechanism situation designer typically imposes system design time application particular social law decided coordination activities handled contract net protocol 
means cases coordination mechanism employed ideally suited agents prevailing circumstances 
inflexibility means performance individual agents system may compromised fact exacerbated open dynamic nature environments agent solutions deployed 
rectify situation aim develop agents reason process coordination select mechanisms appropriate current situation 
choice coordination mechanism run time agents need coordinate 
claim fixing single coordination mechanism design time inappropriate especially dynamic open contexts scope changing modifying mechanism ensure fit prevailing circumstances 
circumvent problem achieve necessary degree flexibility coordination requires agent decisions coordinate coordination mechanism 
previous developed evaluated reasoning framework achieve 
highlighted importance difficulty making approximations behaviour agents 
especially true environment dynamic 
natural extension framework enable agents acquire knowledge run time adaptation 
agents need capable learning right decisions coordination problem 
specifically deal problem allowing agents learn right situation apply right coordination mechanism 
advances state art ways 
firstly introduces learning part agent decision making process concerned coordinate 
secondly empirically demonstrates benefits learning obtained learning beneficial decision making context 
remainder structured follows 
section details specific coordination scenario 
section formalises decision procedures agents 
sections evaluate role learning context 
section deals related section concludes presents areas 
coordination testbed testbed domain follows description contains detailed justification choice scenario various design decisions 
summary grid world number autonomous agents ai perform tasks receive units reward ri 
agent specific task sti perform tasks require agents perform called cooperative tasks cts 
task reward associated rewards cts higher sts divided coordinating agents 
agents move grid step time left right stay 
time agent single goal st ct coordination needs achieved 
arrival square containing goal agent receives associated reward 
case sts new appears randomly grid visible appropriate agent 
case cts new appears randomly grid visible agent subsequently arrives square 
agent encounters ct pursuing current goal st takes charge ct decide initiate coordination agents task coordination mechanism cm 
context agent predefined range cms disposal 
cm parameterised key attributes meta data set cost terms time steps chance success example cm may take time steps set modelled agent waiting number time steps requesting bids agents probability success agent arrive ct square reward allocated probability zero reward 
agent may decide attempting coordinate viable option case adopts null cm agent rejects adopting ct goal 
agent charge aic coordination selects cm waiting set period broadcasts request agents engage coordination 
agents respond bids composed amount reward require order participate ct time steps away ct square situated 
agent bid successful termed agent cooperation denote fact participant aic ct task 
role agent st ais denote situation agent working st broad framework highlights specific decisions see section details gives protocol agents follow time step 
agents arrive square 
ais arrives st cell goal attained receives reward updates goal 
arrives ct cell notifies aic arrived 
ct achieved rewards paid 
ais finds ct decide wants aic cm 
wait time steps broadcasting request coordination 
aic finds new ct ignores 
ais receives request coordination decides bid participate ct aic evaluates bids 
ais bid accepted adopts ct new goal 
aic respond requests coordination 
agent decides move current goal agents move simultaneously 
basic protocol followed agents 
agents receive proposal time step case reply bids proposals receive 
accept ct contract time 
agreements achieve particular ct established contracting protocol 
contract net protocol consists steps 
step aic broadcasts proposal agents 
waits bids 
second step involves selecting bids contracts ais respectively evaluation phase 
third step consists commitment terms contract time step arrive ct square 
initial presentation involves simplifying assumptions particular common knowledge deterministic environment straightforward coordination mechanisms 
framework intended flexible assumptions relaxed see appendix 
model dynamism unpredictability open agents arrive ct square time arbitrarily deemed charge agent finds ct cell randomly selects analysis 
attributes undoubtedly added list quality coordination robustness overhead limitations communication cost focus believe necessary sufficient purposes advocated 
see discussion 
features grid world elements environment change values execution time 
examples changing tasks rewards sts cts frequency tasks appear disappear grid changing number agents environment number agents needed achieve ct main consequence variations generate environment agents face difficulty estimating decisions agents 
agents take decisions factors predetermined 
clarify protocols associated role previous description scenario shows grid size specific time step agents grid cts 
ct position requires agents achieved needs agents requires agents 
specific moment shown aic negotiated agreement achieve ct :10.1.1.159.617
aiss respectively working respective specific tasks 
agents cts 
scenario agent roles 
agent decision making procedures previous developed evaluated decision making framework reasoning coordinate domain 
main focus role impact learning framework discuss details model 
concentrate decisions learning role play cm adopt bid request coordination received determine bid accept 
context agents aims maximise reward particular average reward unit time 
agent keeps track average reward termed reward rate uses rate decide charge services occasionally approximate expected rates agents able build picture 
specifically agent uses reward rate evaluate compare different actions available maintain improve rate chooses 
course decision model approximates true relative values different actions 
deciding cm select agent pursuing current goal encounters ct decide initiate coordination agents order perform 
agent determine advantage doing 
depends reward offered cms available various environmental factors effect expected demands potential coordinating agents 
model expected demands agents aic assumes randomly distributed grid current goals similarly distributed 
agents may near ct may far away likewise agents significant deviation st reach ct may able coordinate ct en route goals 
agent assesses possible cms basis long task performed reward obtain expected reward requirement agents 
case considers set time average distance away agent situated value amount time agents spend deviating path cm probability success 
assessment determines amount surplus reward agent expect expects obtain normal course operation average reward time step 
agent selects cm maximises surplus formalise decision procedure consider grid reward size sts cts coordination mechanism cm costs time steps set probability success grid world known size agent calculate expected average distance ave dist away randomly situated agent ct square average deviation ave dev agents get 
average distance direction random square point distance distance ave dist agent ave dist distance distance 
average distance ave dist agent st average distance random points grid 
averaging ave dist ave dist ave dist average deviation agent assist ct square go st compared going straight st ave dev ave dist ave dist figures agent assess average surplus reward coordinating ct tj pj 
estimate cost terms long cm take set long expects wait agents arrive 
aic usually expect receive reward units time step ave dist cost tj ave dist second aic estimate average amount reward agents require 
distinguish agent average reward refer average reward agents environment 
aic knowledge uses average reward approximation ave dev ave may globally optimal criterion deciding cm sense self interested agent point view 
pj third aic estimates expected surplus ave payoff adopting ct account probability success task ave payoff pj figures aic assess expected surplus reward coordinating ct tj pj 
ave surplus ave payoff ave deciding cms adopt agent computes expected surplus reward selects maximises value 
surplus associated cms negative agent adopts option null cm defined zero surplus 
ct ais ais st st example coordination world grid 
exemplify decision procedure consider simple scenario grid size instant time agents ais ais sts ct cms cm cm 
ais finds ct requiring agent square 
average distance agents 
average distance random squares average deviation agent 
assume st reward average reward time step agents 
expected surplus reward adopting cm cost ave bid ave payoff ave surplus cost ave bid ave payoff ave surplus circumstances ais decides attempt coordination cm aic expects obtain profit 
note case cm negative result indicates surplus 
case ais cm disposal choose null cm expected surplus zero continue st deciding bid agents receive request participate ct submit bid amount reward require compensate deviating current goal 
agent required reward determined amount time spent deviating ct square average reward time step probability success cm proposed formalise consider agent ai average reward time step ri 
agent calculates deviation number extra time steps requires reach st goes ct square 
note example ct square lies directly path st agent deviation zero 
clearly agent position submit attractive bid cost coordinating effectively zero 
means illustration consider agents depicted 
ais take time steps reach st directly steps going ct deviation time steps :10.1.1.159.617
ais take time steps reach st directly steps going ct ais deviation 
compute reward requires engaging coordination ct takes account compensation deviation possibility cm fail 
estimation bid agent participate coordination ri pj agent submits bid coordinate distance ct square 
agent selected coordinate adopts ct current goal 
st re adopted ct accomplished 
deciding ais bids accept aic received bids agents selects set maximises surplus reward new definite information received cf 
approximation section 
agent ai aic knows amount reward require time take arrive ti 
aic selection bid process calculation cost bid received 
agents required achieve ct necessary deal fact may wait ct cell remaining arrive agents travel different distances 
ways dealing situation see discussion 
simplify estimates expected reward undertaken various agents assumed aic pays additional reward time elapsed 
aic knows number time steps wait specified bid amount pay waiting time specific predefined waiting rate 
ct achieved aic received confirmation agents involved cooperation 
notifies aic arrival ct cell receives share ct reward waiting rate followed share ct reward 
decide bids accept general idea aic selects proposals cost total bids received 
considering reward requested bid waiting time cost cost bid estimates expected reward cost investment 
formally aic calculates cost subset elements form ti 
aic selects agent take longest time arrive max ti ti determine maximum time agent spend cell 
approximates cost bid reward waiting time aic pay note aiss actual values concepts discussed aic task approximation components equation ave 
cost ti ti bringing aic estimates surplus expects obtain account cost selected bids investment wait arrive 
bids selected belong subset maximizes surplus surplus pj cost tj may case bids received give positive surplus 
chosen cm expected surplus chance may agents sufficiently near provide reasonable bids 
situation aic abandons ct returns st role learning main focus give agents capability learning right decisions coordination problem 
wish endow agents capability learning right situation apply right coordination mechanism 
specifically agent decision making framework previous section particular decision procedure outlined section allows agents take decisions cm select order achieve ct procedure major respect reasoning coordination mechanisms concentrate terms evaluating role learning decided employ reinforcement learning rl technique :10.1.1.32.7692
reinforcement approach appropriate concerned agents pursuing goals obtaining rewards effectively goals accomplished 
class learning chosen online algorithm require model environment suited dynamic unpredictable scenario 
study reinforcement learning agent uses learning algorithm 
general terms agent objective learn decision policy determined state action value function 
classical model learning consists finite set states world finite set actions performed reward function agent goal consists learning policy maximises expected sum discounted rewards rt rt rt 
rt discount factor 
formally speaking discount factor determines value rewards way reward received time steps worth times worth received immediately 
approximates function takes rewards account strongly 
agent task learn optimal policy arg max 
detail assume agent performs cycle particular state selecting performing action causes agent enter new state receive immediate payoff reward 
learning algorithm estimated values clearly places learning play role 
example agent learn decision bid equation bids accept equation 
agent state action pairs called values 
experience agent updates values formula max learning rate determines rate change estimation maxa value action maximises function state problem agents select action execute 
balance decision selecting action exploited past brought positive reward action explored consequently unknown reward exploitation versus exploration 
function selects action highest value actions explored pre determined number times 
formally exploration function equates ne number times visited optimistic estimate best possible reward agent obtain state ne corresponds number times agents try particular action state pair 
summary experimental evaluation purposes agents learning algorithm values assigned value means agent trying maximize immediate reward decreasing time calculating number times value visited visits visits equation exploration function objective evaluate effect learning agents decision making cms 
compare performance agents learning algorithm rl perform learning nl 
key difference agents select cm attempt coordination step protocol specified 
remaining steps protocol rl nl agents employ decision making procedures outlined section agreements surplus equation positive set bids equation received 
detail nl agent finds ct calculates expected average surplus equation cm disposal 
simply chooses best bid 
rl exploits explores equation set cms 
rl reinforcement measure benefit having selected particular cm corresponds surplus gained achieving ct cm chosen paying 
means corresponds abstraction particular situation agents experience ct example agent role position grid agent action represents set options agent disposal set coordination mechanisms select including null cm reinforcement modelled reward obtained selecting particular cm selecting cm 
idea learning agents eventually learn policy exploring sufficient situations allows know cm choose specific situation state 
summary agents performance analysed algorithms rl agents learn select particular cm profit gained accomplishing cts particular cm 
nl agents engage learning activities 
known convergence time learning algorithm highly exploration exploitation function size look tables learning rate 
objective hand tune parameters reduce convergence time particular cases provide theoretical results areas 
fixed values parameters kept constant learning implementations accomplishing cts case considered 
agents achieve st tasks information considered reinforcement relevant agent decisions cms 
finish discussion role learning model necessary specify features environment algorithms tested 
scenarios designed scenario aiss environment submitting bid calculated equation scenario aiss calculate bids way incorporate degree random noise base line 
reason change general case face great deal uncertainty predicting value 
random element mirrors environments predictions accurate 
scenarios constitute reasonably static environment predictions dynamic predictions inherently accurate 
experimental evaluation main hypothesis seek evaluate agents coordinate effectively scenario reinforcement algorithms 
measure benefits model set experiments designed formal methodology provide information experimental variables 
order test verify hypothesis questions employ statistical inference methods particular analysis variance anova test hypotheses differences means collected 
null hypothesis equal means rejected procedure reveals experiments differences means significant accepted contrary case 
words anova tests significance observations accepting rejecting hypothesis formulated 
observations set values experimental variables result execution particular algorithm environment 
simulation variables fixed learning experiments size grid duration time units number cts grid time number agents environment st reward ct reward maximum number agents needed achieve ct coordination mechanisms considered agent cm cm cm cm cm 
experimental variables analysis total agent reward obtained st ct tasks au total agent reward obtained agents agent st role ais total agent reward obtained agents agent charge role aic total agent reward obtained agents agent cooperation role total number cts accomplished tct 
experiments described collect results experimental variables averaged simulation runs 
accept main hypothesis hypotheses rejected meaning values experimental variables particular learning algorithm produce significantly better results obtained learning ones means equal 
hypotheses tested scenario scenario au obtained performing reinforcement algorithm rl obtained agents nl algorithm 
number cts tct achieved agents means reinforcement algorithm identical agents nl 
evaluate hypothesis static environment scenario section dynamic environment scenario section 
decided evaluate fixed duration scenario time counts agents win reward time step 
reasonable compare behaviour algorithms parameters 
duration selected sufficient learning algorithms converge optimal values 
cms selected previous results indicated main ones selected agents setting 
learning select cm static environment start table summary results obtained performing anova data collected algorithms scenario 
analyse agent utility hypothesis 
rejected meaning performance algorithms significant effect au obtained 
performance nl better statistically significant amount rl nl noted winner 
clearly seen left side total reward gained nl agents higher rl agents 
agent utility au hypothesis evaluate outcome winner rejected nl rejected nl table agent au tct scenario result anova nl rl agent utility total cts total cts accomplished tct agent utility au nl rl contrasting agent performance scenario 
aic ais total cts evaluates effectiveness achieving cts 
hypothesis rejected means tct depend algorithm executed 
line au obtained agents seen cts achieved better au rejected 
important result analyse detail high number cts achieved need necessarily lead better agent performance 
scenario agents firstly need decide working cooperatively rewarding working secondly importantly need balance time invested setting cm final reward achieved 
generally speaking agents perform best ones effective making trade selecting cms time set matched reward obtained ct results obtained experiment nl agents accomplishing cts selecting correct cm maximise au 
verify argument right side table explore effectiveness agent role anova 
results show nl agents perform statistically significantly better ais roles corresponding rl agent roles rejected 
performance nl agents roles better balance cooperative individual behaviour rl agents 
expect rejected nl winner agent 
contrast significant difference reward obtained accepted 
reason aic reward obtained achieving number profitable cts accomplishing higher number rewarding cts 
terms selection cms means rl selected cms high total cts accomplished tct probability success corresponding high time set leaving time achieve cts 
nl agents choose cms take time set lower probability success 
nl performs better better balances decision go ct cm allows gain reward 
hypothesis evaluate outcome winner rejected nl accepted rejected nl table au agent role scenario result anova reason apparently poor performance rl agents results obtained fixed period included exploration exploitation phase learning agents converged optimal policy equation 
recall agents learning process need try cms exploration necessarily worth necessary step task learning 
argued allow rl agents compete nl agents know situation select cms performance improve 
verify experiment agents initiated policies learnt past experiment simulation run 
clearly distinguish phases follows define training phase covering phases exploration occurring convergence occurred acting phase rl agents exploit optimal policies convergence occurred 
agent utility au hypothesis evaluate outcome winner rejected rl rejected rl table agent au tct scenario acting phase result anova nl rl agent utility total cts total cts accomplished tct agent utility au nl rl aic ais total cts contrasting agent performance scenario acting phase 
new situation tables show results testing hypotheses anova plots data collected 
thing notice rl agents perform statistically significantly better nl ones see left side 
rls improve decision making selecting optimal policy consequently increasing au rejected 
second aspect main reason improvement total cts accomplished tct rl agents accomplish cts 
tct rl agent obtained past experiment 
third aspect analyse performance various agent roles algorithms 
contrast previous results nl aic significantly better rl aic rejected 
price paid nl agents select wrong cms non optimal ones specific situation 
nl agents accomplished cts rl agents selected cms 
regarding roles rl ais roles show agents choose cm enables accomplish sts gain reward working cooperation 
cts agents accomplishes better performance 
hypothesis evaluate outcome winner rejected rl rejected nl rejected rl table au agent role scenario acting phase result anova shown nl agents perform decision making process precisely models various actions take place environment 
compared rl agents learning converged 
question situation sufficiently static agent learning converge exploit converged policies 
perspective agents capable adapting decision making acting result occurring environment 
want rl agents perform superior fashion considering training acting phases 
order address problem follows learning explored previous assumptions longer guaranteed 
particular explore learning agents find exploit optimal policies situations fundamental actions associated cooperative activity challenging predict scenario 
learning select cm dynamic environment set experiments tested set hypotheses section 
consider case measurements taken including exploitation exploration phase training phase 
initial results summarised table 
analyzed hypotheses related au 
similarly results obtained table conclude applying rl nl produces distinctive results 
conversely table rl agents get significantly better results nl seen 
hypothesis evaluate outcome winner rejected rl rejected nl table agent au tct scenario result anova 
tct accomplished hypothesis equal means rejected 
significant impact tct achieved performing rl nl results rl nl 
relevant aspect discuss contrast previous experiments nl obtains lower au despite achieving cts 
corroborates explanation importance deciding pursue ct correct selection cm agent performance 
words major decision agent faces decide attempt coordination 
important agent gain long term reward accomplishing sts investing time agent utility au nl rl agent utility total cts total cts accomplished tct agent utility au nl rl contrasting agent performance scenario 
aic ais total cts unsuccessful cts particular frequent scenario 
right section table show reward gained achieving cts cooperative roles nl nl higher gained corresponding rl ones 
hand achieve cts hand bids higher scenario accepted agreements agents gain profit rejected 
despite fact cooperative behaviour seen decision time invested endeavours sufficient recover reward gained rl aiss rl aiss obtained total approximately total reward accomplishing sts nl aiss achieved 
regarding hypothesis evaluates aiss roles rejected rl agents winners 
additional justification results table agents decide attempt coordination invest significant amount time cm request higher bids scenario meaning profit reduced 
rl agents perform better certain invest time ct correct cm importantly worth 
time take advantage pursuing sts 
hypothesis evaluate outcome winner rejected rl rejected nl rejected nl table au agent role scenario result anova difficult see scenario nl agents accurately predict amount requested engaging ct case unpredictable environment scenario 
result agents select wrong cm may attempt coordination best thing 
optimal policy varies attempting coordination frequently static environment attempting coordination 
supported fact tct gained agent types scenario considerably lower tct mean amount accomplished scenario tct mean 
concrete nl agents predictions bid low optimistic possible cooperative agents initiate coordination situations best decision 
predictions high pessimist attempt coordination 
conclude having learning agents explore exploit cms reasonable total cts accomplished tct thing dynamic environments agents certain actions 
final experiment analyses happens various performance measurements finding optimal policies explored section acting phase 
particular examine learning agents take advantage knowing cm apply demanding environment 
agent utility au hypothesis evaluate outcome winner rejected rl rejected nl table agent au tct scenario acting phase result anova nl rl agent utility total cts total cts accomplished tct agent utility au nl rl aic ais total cts contrasting agent performance scenario acting phase 
expected similarly results scenario rl agents improve performance exploiting policies learnt 
au tct gained rl agents higher corresponding au tct consequently higher nl agents 
furthermore challenging task engaging coordination activities tct accomplished rl agents increased considerably approximately 
gives evidence claim optimal policy necessarily attempt ct time 
confirm claim table shows reward gained rl aiss increased comparison au gained role training phase 
value statistically higher ones gained rejected 
regarding cooperative roles aic clear right side nl aic winner rejected ensure superiority roles better played rl agents rejected 
hypothesis evaluate outcome winner rejected rl rejected nl rejected rl table au agent role scenario acting phase result anova better understand differences occur dealing rl agents scenario table shows hypothesis evaluation experimental variables 
order distinguish various aspects learning rl stands ones take consideration training phase rl refers agents perform solely acting total cts accomplished tct phase 
thing see rl better performance rl evaluations rejected 
hypothesis rl superior associated aic role unfortunately profitable action take scenario 
second aspect optimal solution avoid engaging cooperative action altogether spend time 
rl agents learnt decide attempt coordination cm situations means agents minimise amount time spend setting cm 
hypothesis evaluate outcome winner rejected rl rejected rl rejected rl rejected rl rejected rl table contrasting rl agent roles scenario training acting phase result anova summary dynamic unpredictable environments rl agents perform better nl agents certain invest time ct importantly worth 
rl agents time take advantage pursuing sts 
learning coordination decision helps rl agents precise model occurring environment consequently decision making improved 
turn means agents effective maximising profits 
rl agents poor performance nl agents change way reasoning environment detect adapt decision making changes agents behaviours 
aspect needs discussion apparently better performance rl agents acting phase compared achieved training phase scenario scenario 
believe observation needs taken caution number reasons 
firstly believe agents able optimal policies environment constant state flux 
experiments environment change training phase specific policies exploited consequently effective acting phase 
environment changes agents leave enter system agents alter behaviour guaranteed policies exploited optimal ones transformed environment 
secondly despite improvement shown rl agents acting learnt optimal policies important recall objective design agents operate effectively open dynamic environments 
exploration exploitation fundamental convergence time learning algorithms needs associated changes occurring environment 
believe necessary investigation fully learn learning environmental changes 
subject 
related broad strands primarily related model dealt turn 
reasoning coordination multiagent learning 
terms coordination existing assumes design time problem 
comparatively little addresses run time reasoning selection particular coordination protocols 
deal issue variety research positions investigated related flexibility introduced different aspects different levels coordination 
perspective classified introducing flexibility particular cases coordination mechanisms somewhat restricted manner 
detail durfee argued agents need flexibility coordinate different levels abstraction depending particular needs moment time 
date focused building flexibility basic planning mechanisms individual agents 
mechanisms explicitly reasoning level coordinate situation 
flexibility built cooperative problem solving agents jennings 
agents choose cooperate various conventions dictated behave particular team problem solving context 
conventions varied terms time took establish communication overhead imposed agents 
reasoning mechanism determining convention appropriate situation 
barber software engineering framework enables agents vary coordination mechanisms prevailing circumstances 
identify criteria determining particular mechanisms appropriate 
decision procedures trading criteria developed 
boutilier presents decision making framework multi agent markov decision processes reason state coordination mechanism 
concerned optimal reasoning context coordination mechanism reasoning mechanism employ particular situation 
terms learning vast literature produced years concerning learning techniques particularly learning multiagent systems 
focus mainly aspects 
agent goal learn agents environment order predict behaviour produce model 
second case learning applied learn coordinate cooperate achieve common goals specific strategies 
success lines research mainly improve cooperation coordination agents environment 
clearly important issue address concerned learning select particular coordination mechanisms 
date comparatively little concerned learning cm select context 
relevant collage systems 
objective systems improve coordination learning select coordination strategy appropriate situations 
aspects system addresses different findings complementary 
interested having agents capable learning key information necessary improve coordination specific situations 
collage agents learn choose appropriate coordination strategy particular situation 
focuses information learn collage learning situation coordination strategy 
important notice systems concerned detailed activities coordination part learning process 
agents solve particular coordination problem solve interrelations dependencies actions 
agents plan actions perform execute 
solve systems handle deep knowledge domain case coordination strategies collage 
case research aim broadly similar assumptions different deal problem alternative solutions 
framework agents endowed set decision making procedures select adequate coordination mechanisms 
dealing set mechanisms consider important agents capacity take decisions coordination dealing interactions 
leave details subsequent tasks associated protocol 
furthermore believe agents increasingly required deal dynamic issues online learning important 
collage contrast uses instance learning techniques phase recovery examples training 
consequently system defined moments phases performed gives additional problem determining phase finish 
analysed efficacy agents learning making decisions coordinate 
showed learning improve decision making agents uncertain agents actions 
improvement occurs agents learn recognise situations profitable actions selected 
showed learning ineffective agents operate static environments compete agents reasonably accurate predictions environment agents 
speaking generally believe important develop techniques enable agents coordinate flexibly dynamic unpredictable environments 
detailed aspects decision procedures specific grid world scenario believe general processes structures developed suitable reasoning coordination mechanisms general domains see examples scenario mapped variety real world problems 
particular issues exploit learning techniques allow agents take decisions experience key aspect needs broader investigation 
results viewed important step direction 
aim extend learning cover aspects agent decision framework learn decision bid request coordination section section bids accept section 
learning introduced areas learning decisions example agents learn constituent factors decisions 
intended allow agents construct models ability vary details modelling agent coordination context 
particular believed order accomplish effective learning objectives agents model level agents terminology explicitly representing knowledge 
agent decisions take consideration predictions agents refine predictions agent needs represent precise way behaviour scenario 
broader context final aspect discuss learning employed learn meta data parameters cm cm parameters modified efficiency actual execution 
aspect addressed directly related specifics different cms operate achieve coordination 
acknowledges author acknowledges funding mexico national council science technology national laboratory advanced computer science 
barber han liu 
coordinating distributed decision making reusable interaction specifications 
design applications intelligent agents third pacific rim international workshop multi agents prima pages melbourne australia august 
bourne toledo jennings 
run time selection coordination mechanisms multi agent systems 
proceedings th european conference artificial intelligence ecai pages august 
boutilier 
sequential optimality coordination multiagent systems 
proceedings sixteenth international joint conference artificial intelligence ijcai pages stockholm sweden july august 
claus boutilier 
dynamics reinforcement learning cooperative multiagent systems 
proceedings fifteenth national conference artificial intelligence aaai pages madison mi july 
cohen 
empirical methods artificial intelligence 
mit press cambridge ma 
durfee 
practically coordinating 
ai magazine spring 
durfee lesser :10.1.1.48.8362
partial global planning coordination framework distributed hypothesis formation 
ieee transactions systems man cybernetics september 
toledo 
dynamic selection coordination mechanisms 
phd thesis department electronics computer science university southampton february 
toledo bourne jennings 
reasoning commitments penalties coordination autonomous agents 
proceedings fifth international conference autonomous agents agents pages montreal quebec canada may june 
toledo jennings 
learning select coordination mechanism 
proceedings international joint conference autonomous agents multi agent systems aamas pages bologna italy july 
toledo jennings 
dynamic selection coordination mechanisms 
journal autonomous agents multi agent systems appear 
kluwer academic publishers 
fox 
organizational view distributed systems 
ieee transactions systems man cybernetics january 
galbraith 
designing complex organizations 
addison wesley publishing reading ma 
hu wellman 
online learning agents dynamic multiagent system 
proceedings second international conference autonomous agents agents pages minneapolis mn may 
jennings 
commitments conventions foundation coordination multiagent systems 
knowledge engineering review 
jennings 
agent software engineering 
artificial intelligence 
kaelbling littman moore 
reinforcement learning survey 
journal artificial intelligence research 
lesser 
reflections nature multi agent coordination implications agent architecture 
autonomous agents multi agent systems july 
malone 
modeling coordination organizations markets 
management science october 
ishii doya 
multi agent reinforcement learning approach agent internal model 
proceedings fourth international conference multi agent systems icmas pages boston ma july 
prasad lesser 
learning situation specific coordination cooperative multi agent systems 
autonomous agents multi agent systems 
rosenschein zlotkin 
rules encounter designing conventions automated negotiation computers 
mit press cambridge ma 
russell norvig 
reinforcement learning 
artificial intelligence modern approach volume learning chapter pages 
prentice hall saddle river nj 
sen hale 
learning cooperate sharing information 
proceedings twelfth national conference artificial intelligence aaai pages amherst ma july 
sen weiss 
learning multiagent systems 
weiss editor multiagent systems modern approach distributed artificial intelligence chapter pages 
mit press cambridge ma 
shoham tennenholtz 
synthesis useful social laws artificial agent societies 
proceedings tenth national conference artificial intelligence aaai pages san jose california july 
singh jaakkola littman 
convergence results policy reinforcement learning algorithms 
machine learning march 
smith davis 
frameworks cooperation distributed problem solving 
ieee transactions systems man cybernetics january 
stone veloso 
multiagent systems survey machine learning perspective 
autonomous robots june 
sugawara lesser 
learning improve coordinated actions cooperative distributed problem solving environments 
machine learning 
sutton barto 
reinforcement learning 
mit press cambridge ma 
tan 
multi agent reinforcement learning independent vs cooperative agents 
proceedings tenth international conference machine learning pages amherst ma 
vidal durfee 
agents learning agents framework analysis 
collected papers aaai workshop multiagent learning providence rhode island 
watkins dayan 
technical note learning 
machine learning 

