supervised neural gas general similarity measure barbara hammer marc lnm department mathematics computer science universit germany mail informatik uni de thomas villmann universit leipzig germany mail villmann informatik uni leipzig de august 
prototype classi cation ers intuitive sparse models excellent generalization ability 
models usually crucially depend underlying euclidian metric online variants su er problem local optima 
propose generalization learning vector quantization additional features directly integrates neighborhood cooperation ected local optima ii method combined di erentiable similarity measure metric parameters relevance factors input dimensions automatically adapted data iii obeys gradient dynamics shows robust behavior chosen objective related margin optimization 
keywords neural gas learning vector quantization generalized relevance lvq metric adaptation 
prototype classi ers kohonen learning vector quantization lvq er intuitive simple powerful learning algorithms applications 
shown standard lvq implicitly aims margin optimization generalization ability expected high dimensional data 
numerous modi cations basic lvq proposed achieve faster convergence stable behavior better adaptation optimum bayesian decision example 
drawbacks lvq variants prohibit achieve optimum results location prototypes lvq depends initialization 
algorithm gets easily stuck local optima kluwer academic publishers 
printed netherlands 
ksrng tex hammer villmann multi modal data distributions 
ii lvq crucially depends chosen metric commonly euclidian metric 
lvq fails heterogeneous data di erent relevances input dimensions 
iii lvq ers intuitive heuristic 
underlying cost function proposed highly causes instable behavior lvq case overlapping classes 
alternatives introduced take problems account order avoid problem local optima lvq kohonen proposes combine lvq neighborhood cooperation initialize lvq prototypes self organizing map som directly combine lvq som training 
approach partially solves problem local optima 
som xed neuron topology internal topology data 
simplicity dimensional lattice chosen possibly match unknown data topology 
methods identify repair topological defects costly 
addition simultaneous update neurons neighborhood provided som lvq causes strengthening inherent instabilities lvq 
kohonen advises lvq som small neighborhood size initial unsupervised som training avoid ects far possible 
alternatives avoiding local optima ered iterative schemes dynamically adapt number prototypes global position greedy scheme proposed counters utilization prototypes proposed 
number prototypes generally xed priori 
cost function available training combined global methods simulated annealing proposed 
procedure usually quite costly annealing carefully controlled points phase transitions 
ksrng tex srng follow approach proposed kohonen combination lvq neighborhood cooperation neural gas ng topology representing networks respectively 
neural gas uses data adapted topology approach su er possible topological mismatches 
approach ng lvq updates combined resulting stable robust behavior 
learning rule interpreted optimization single cost function 
ii generalizations learning schemes originally euclidian metric proposed 
variety unsupervised clustering algorithms visualization quantization data exists incorporate adaptive metrics extensions classical means algorithms gustafson geva clustering complex cluster shapes fuzzy fuzzy shell incorporation auxiliary information metric self organizing map proposed kaski 
approaches equip supervised lvq adaptive diagonal metric proposed 
particularly gradient dynamics proposed proven successful various application areas processing satellite images time series prediction 
generalize approach arbitrary di erentiable similarity measures 
parameters relevance factors metric automatically adapted data 
shown experiments alternatives weighted euclidian metric bene cial ects non gaussian data distributions 
iii approaches develop cost functions variants lvq 
rst proposals 
stable learning algorithm error formulation called generalized topology representing networks introduce topology neurons 
refer methods neural gas interested bene tial ects learning dynamics 
ksrng tex hammer villmann lvq glvq 
exact investigation equilibria speci situations show glvq yields robust behavior original lvq 
precise derivative gradient formulas generalization incorporate relevance terms 
course standard unsupervised vector quantization possesses cost function quantization error looks similar cost function underlying basic lvq variants 
complex unsupervised algorithms include neighborhood cooperation obey potential dynamic modi cation som proposed neural gas stochastic alternatives thereof 
surprising cost functions transferred learning vector quantization proposed soft competition variant lvq 
follow approach describes standard crisp assignments lvq intuitive integration cost term ng general similarity measures possible 
function robust pointed implicitly optimizes margin giving generalization 
explain cost function combined di erentiable similarity measure exible manner 
speci choices similarity measure algorithm interpreted glvq 
theoretical guarantees generalization ability standard lvq transfer cases 
preliminary results weighted euclidian metric reported demonstrate robustness method highly multimodal heterogeneous data 
rst introduce lvq glvq ng 
describe integration cost function general similarity measures 
derive formulas speci similarity measures ksrng tex srng bene cial practice 
demonstrate capability algorithm experiments 

learning vector quantization neural gas assume inputs class labels denoting nite set possible labels 
prototype classi er characterized codebook vectors prototypes neurons class labels denote fw class prototypes fw cg class prototypes assigned class classi cation realized winner takes rule stimulus vector mapped class closest prototype 
minimum kv denotes squared euclidian metric moment 
neuron called winner best matching unit 
subset input space fv called receptive eld neuron lvq aims minimizing classi cation error di erence data labeled fv cg union receptive elds prototypes labeled cr purpose lvq introduced kohonen recursively adapts winner signal 

learning rate 
explained update interpreted stochastic gradient descent cost function cost lvq lvq ksrng tex hammer villmann denotes squared euclidian distance closest prototype labeled denotes squared euclidian distance closest prototype labeled label di erent standard lvq function lvq obviously cost function highly discontinuous instabilities arise overlapping data distributions 
cost function lvq explained obtained setting sum term lvq yields identity window adaptation lvq takes place vanishes outside 
choice produces instable dynamic prototypes diverge due fact repelling forces term larger attracting forces term prevent behavior far possible window adaptation takes place chosen carefully 
lvq explicitely optimizes term related term kv kv yields hypothesis margin classi er 
generalization bounds terms hypothesis margin derived 
lvq seen classi er aims structural risk minimization training comparable svm 
sato yamada propose alternative involves hypothesis margin additional scaling factors avoiding prototype divergence discussed 
glvq sgd sgd exp denotes logistic function 
derivative yields updates 
sgd 

ksrng tex srng 
sgd 

learning rates logistic function evaluated position 

denote derivatives glvq respect respectively 
reported modi cation called generalized learning vector quantization glvq shows superior performance lvq 
stochastic gradient descent potentially multimodal function algorithm depends initialization prototypes gets easily stuck local optima 
unsupervised neural quantization schemes er intuitive possibility spread unlabeled prototypes faithfully data distribution 
neighborhood cooperation ensures initialization prototypes critical achieving global optima degree neighborhood cooperativity scaled appropriately training 
unfortunately popular self organizing map cost function discrete case modi ed versions 
addition som restricted xed lattice structure purpose easy visualization topological mismatches occur 
convenient alternative ered neural gas ng optimizes cost function cost ng wr exp ksrng tex hammer villmann denotes degree neighborhood cooperativity yields number prototypes valid rank normalization constant depending neighborhood range cardinality learning rule 
learning rate 
learning rule interpreted stochastic gradient cost function 
ng shows robust behavior adapts automatically data topology due incorporation respective rank prototypes data optimum neighborhood factor 

supervised neural gas note glvq adapt closest correct wrong prototype 
prototypes initialized outside data distribution probably adapted 
similarly prototypes capable reaching appropriate cluster center due repelling forces data di erent classes 
supervised neural gas sng de ned idea neighborhood cooperativity integrated glvq avoid dependency initialization glvq 
training point prototypes respective class adapted data point rank ng 
formulate objective sng cost function combines dynamics ng glvq wr cv 
sng cv sng glvq ksrng tex srng denotes squared euclidian distance cv denotes cardinality set prototypes labeled jw cv denotes closest prototype cv prototypes speci class adapted data point preventing neurons idle class 
ng dynamics aims spreading prototypes speci class label faithfully respective data 
simultaneous glvq dynamics sure class borders yield classi cation 
addition cost function includes terms related hypothesis margin just glvq original lvq 
note vanishing neighborhood cooperativity yields original cost function glvq 
update formulas prototypes obtained derivative 
prototypes cv adapted sgd 

cv cv 

closest wrong prototype adapted 
wr sgd 

cv cv 

learning rates logistic function evaluated position terms obtained derivative sng 

derivation formulas appendix general case underlying possibly continuous data distribution 
note original updates glvq recovered 
positive neighborhood cooperation correct prototypes adapted ksrng tex hammer villmann data point neurons outside class active 
eventually neurons spread data points respective class 
prototypes repelling function closest incorrect prototype advisable choose magnitude smaller see experiments highly multimodal classi cation tasks solved modi cation glvq 

general similarity measures sng ected initialization prototypes crucially depends euclidian metric 
noise accumulate high dimensional data dealt 
algorithm properly process heterogeneous data input dimensions subject di erent scaling 
simple powerful extension glvq generalized relevance learning vector quantization proposed euclidian metric substituted scaled metric constitute scaling relevance terms adapted automatically training 
relevance terms allow di erent problem adapted scaling input dimensions 
noisy super uous dimensions scaled small relevance terms eventually resulting pruning dimensions 
signi cant dimensions scaled large relevance terms 
adaptation takes place simultaneously adaptation prototypes minimization extended cost function glvq denotes scaled squared euclidian distance closest correct prototype data point denotes scaled ksrng tex srng squared euclidian distance closest incorrect prototype 
updates similar glvq 
addition relevance terms adapted gradient cost function respect constraints taken account normalization step 
extension cost function sng supervised relevance neural gas srng 
general substitute euclidian metric sng di erentiable similarity measure constitute parameters metric adapted training 
similarity measure refers function yields nonnegative real values need symmetric ful ll triangle inequality 
general cost function form wr cv 
sng cv denotes closest prototype belong cv measured similarity measure rank prototypes computed general similarity measure 
derivatives yields updates sgd 

cv cv prototypes cv update 
wr sgd 

cv cv closest wrong prototype learning rates logistic function evaluated position ksrng tex hammer villmann 

derivative respect computed wr sgd 
cv cv 


derivation formulas appendix 
note update formulas formulated di erentiable similarity measure metric parameters adapted automatically data driven way 
depending role parameters advisable restrict search space certain range positive normalized prevent degeneration solution 
constraints plugged algorithm explicit normalization update step 
question occurs choices similarity measure algorithm srng interpreted kernelized version sng 
constitutes common trick transfer understood possibly simple algorithms complex situations case svm example nonlinear kernel allows separate complicated data sets high dimensional space feature space provided kernel linear separator 
theoretical guarantees generalization ability preserved 
lvq srng euclidian metric interpreted margin optimization algorithm theoretical bounds generalization error margin independent data dimensionality 
lvq srng connected bounds generalization error 
refer general similarity measure srng kernel function hilbert space function ksrng tex srng interpreted scalar product high dimensional possibly nite dimensional space 
prominent application kernels machine learning context svms 
success svm various alternative machine learning tools principal independent component analysis popular 
chosen kernel xed results statistical learning theory bounds generalization error transferred directly basic version learning algorithm kernelized 
time appropriate nonlinear kernels considerably expand capacity original method yielding universal approximators case svm example 
possibly high dimensional mapping need computed explicitely computational ort reduced 
fact function constitutes kernel tested mercer theorem 
popular kernels include example polynomial kernel gaussian kernel kernels speci cally designed complex data structures strings 
case interested general similarity measures exists 
denotes metric hilbert space holds interprete cost function cost function sng possibly high dimensional hilbert space generalization ability classi er depends margin classi er 
known general class functions mercer kernels sucient condition example constitutes real valued symmetric functions conditionally positive de nite xn inequality 

holds 
example functions ksrng tex hammer villmann form kx yk arbitrary metric 
ful ll properties 
ful lls properties srng constitutes kernelized version sng generalization bounds transfer case 
note bounds generalization ability classi er transfer general similarity measure parameters adapted training feature space mapping implicitly changed training 
addition appropriate general similarities ful ll conditions interpreted original algorithm 
case example problem speci similarity measures 
experimentally srng provides generalization cases 
apart standard squared euclidian metric deal di erent similarity measures scaled squared euclidian metric adaptive relevance terms successfully tested combination glvq 
weighted quartic similarity measure 
similarity measure punishes large deviations elements squared euclidian metric small deviations tolerated 
fact particularly appropriate entries gaussian sharply clustered 
quartic similarity measure better matches test large number entries ksrng tex srng cluster 
data component wise normalized mean variance similarity measure magni es component wise distances larger variance 
analogy called locality improved kernel proposed take local correlations spatio temporal data account dna sequences computational biology similarity measure assume data points form local correlations neighbored entries relevant similarity measure example represent time window length time series 
dl computes norm measures correlation distances entries local window position data points 
typically chosen small natural number factor decreasing borders local window jjj norm denotes radius local windows 
borders range indices indices taken modulo adaptation window size borders necessary 
adaptive values 
similarity measures di erentiable similarities srng 
allows direct interpretation kernelized sng relevance terms xed 
see experiments competitive generalization ability better classi cation accuracy achieved ksrng tex hammer villmann adaptive relevance term alternatives update formulas prototypes relevance terms similarity measures result directly derivatives similarities formulas 















experiments highly multimodal artificial data rst demonstrate ability srng deal highly multimodal data adapt relevance factors automatically data structure weighted squared euclidian metric arti cially generated data sets 
data sets consist classes clusters points cluster 
centers clusters located checkerboard structure dimensional square data sets contain dimensional data points clusters sets di er respect overlap classes depicted figs 

data sets achieved copies respectively dimensional points embedded dimensions follows point embedded 
uniform noise support ksrng tex srng 
arti cial multimodal data set training classes di erent class labels indicated training set prototypes srng spread clusters clusters middle 
contain information relevant classi cation 
contain disrupted copies classi cation information respect location dimension achieved dimensions 
dimension relevant substituted 
sets randomly divided training test set size cross validation 
train srng metric prototypes class sets prototypes initialized randomly small values 
relevance factors initialized equally 
training done cycles learning rates correct prototypes incorrect prototypes relevance terms 
neighborhood range started multiplied epoch 
comparison report mean values cross validation variants srng neighborhood cooperation ksrng tex hammer villmann 
arti cial multimodal data classes di erent class labels indicated training set overlap top training set large overlap clusters bottom 
adaptation relevance terms proposed sng srng adaptation relevance factors done standard euclidian metric srng neighborhood cooperation takes place glvq proposed sato ksrng tex srng yamada neighborhood cooperation relevance adaptation simple nearest neighbor classi er nn result reports classi cation accuracy test set nearest neighbors taken training set classi cation accuracy prototypes set hand cluster centers construct data opt 
results collected table 
depicted variants lvq neighborhood cooperation capable placing prototypes clusters directly located origin 
correspondingly classi cation accuracy glvq close random guess 
sng srng spread prototypes faithfully data 
training done minimum number prototypes sucient classi cation 
sng srng usually clusters 
missing cluster accounts error 
separated data set sng srng achieve optimum classi cation accuracy 
srng capable classifying data correctly data set data embedded dimensions 
typical relevance pro le achieved importance dimension clearly pointed 
irrelevant dimensions ectively pruned relevance dimension table training test set accuracy mean values variation cases 
obtained clustering multimodal data sets 
data data data data data data opt nn glvq sng srng ksrng tex hammer villmann shared 
note training test set accuracy data set worse compared data set sng adapt relevance factors 
added noisy dimensions signi cantly reduce performance metric classi er srng account fact relevance terms 
classi cation accuracy data set bit worse owing larger overlap classes 
training test set accuracy srng close accuracy nn sets 
expected generalization error srng bad data sets contain highly overlapping data seen 
srng capable achieving comparably training error optimizing cost function due neighborhood cooperativity 
relevance pro le srng data sets similar pro le data set clearly indicating importance rst dimensions entries rst dimensions signi cant 
discrete data mushroom data set uci repository consists vectors contain symbolic attributes including binary attributes attributes di erent values 
data embedded dimension encoding symbols unary way 
addition data linearly transformed component wise zero mean unit variance 
class label edible poisonous represented respectively data set 
data high dimensional known relevant dimensions allow classi cation close 
classi cation logic formula 
example attributes related odor allow classi cation single test print color green yields accuracy 
learning task dicult standard vector quantization due ksrng tex srng high dimensionality input space 
symbolic data distribution gaussian focused 
test srng similarity measures report averaged results independent runs 
data randomly split training set data test set data 
prototypes class correspondingly start small neighborhood parameter 
learning parameters optimized runs 
srng squared euclidian metric converges epochs yields mean training set accuracy test set accuracy 
comparison metric converges epochs yields training set accuracy test set accuracy 
note data transformed unit variance metric puts bias dimensions symbols uniformly distributed 
rare potentially signi cant events boosted way 
similarity measure leads sparse accurate models 
interestingly relevance pro les emphasize dimensions extracted di erent methods rule extraction relevance pro le emphasizes dimensions odor foul odor gill color pink color green 
dimension related largest relevance term see 
relevance pro les allow deal high dimensional data similarity measure allows achieve state art performance compact models 
time series data data set experiment consists univariate time series describes monthly power consumption hours consecutive months 
data taken zip 
values adjusted 
tex hammer villmann lambda variability lambda variability 
relevance pro les srng mushroom data 
similarity measures top bottom dimensions signi cantly emphasized particularly dimension odor cases 
pected values respective season previous years relevant prediction 
addition local correlation consecutive values expected 
rst derivative time series considered account trends 
addition numbers translated zero mean 
resulting values zero mean variance 
deal classi cation absolute range divided intervals equal number elements 
real values substituted class labels respective interval index see 
task predict interval index value time step past values chosen time interval 
classi cation time windows length chosen prediction done past years 
method report results independent runs data set split training set data points corresponding ksrng tex srng power consumption 
monthly power consumption hours representative subset data starting month data linearly transformed trends di erences consecutive month predicted 
seen time series noisy seasonal features graph identi ed 
prediction task turned translation task dividing values predicted classes indicated horizontal lines 
number points classes approximately size 
rst part time series test set data points corresponding part sequence 
trained prototype classi ers prototypes class learning rates optimized respective training method 
neighborhood cooperation starts initial neighborhood size multiplied epoch 
prototypes initialized randomly small values relevance factors equal 
srng metrics radius local windows chosen local correlations degree considered local windows cut borders index range 
method trained epochs convergence observed cases 
table ii mean classi cation accuracy training test set methods reported 
addition compute mean squared error prediction obtained classi cation predicting value receptive eld constant mean value data training set receptive eld respective prototype 
note default classi cation class largest size yield ksrng tex hammer villmann classi cation accuracy 
prediction constant value yield mean squared error 
obviously classi cation accuracy prediction error best similarity measure take local correlations observed time series account 
yields better results training set simple squared weighted euclidian norm explained fact data dimensions focused small deviations data points prototypes tolerated due inherent noise process large deviations 
note classi cation accuracy prediction error test set worse compared training set 
due fact training data available time series comparably noisy 
results considerably better default classi cation prediction yield classi cation accuracy mean squared prediction error 
generalization ability metrics runs competitive yields best classi cation results test set 
relevance pro les obtained various similarity measures runs reported 
models peaks observed di erent signi cant positions immediate past right positions correspond month previous year year 
resulting relevance pro les table ii 
training test set accuracy classi cation srng various metrics mean squared error corresponding constant prediction preprocessed time series reported results mean values runs 
train test train test ksrng tex srng average average average 
relevance pro les similarity measures top middle bottom measured runs dotted lines including average 
pro les peaks immediate past season years earlier respectively clearly visible 
variance higher due redundant information windows comparably high values observed borders redundancy smaller 
reasonable 
relevance pro les vary comparably independent runs 
explained fact information integrated consecutive windows redundancy information 
local time windows borders emphasized resulting high relevance terms left side right side 
due fact information contained ksrng tex hammer villmann borders contained smaller number local time windows compared time series entries middle 

generalization learning vector quantization combines signi cant aspects model algorithm robust cost function glvq 
hand stable behavior overlapping noisy data obtained hand generalization capabilities due implicit margin achieved 
algorithm includes neighborhood cooperation reducing problem local optima initialization prototypes longer issue 
scheme experimentally proved ecient strategy 
algorithm automatically adapts metric parameters particularly suited high dimensional data 
addition problem speci similarity measure powerful models arise small number prototypes 
objectives integrated single cost function update formulas prototypes relevance terms derived case underlying possibly continuous data distribution 
addition performance method demonstrated experiments 


bauer herrmann villmann 
neural maps topographic vector quantization 
neural networks 


bauer villmann 
growing output space selforganizing feature map 
ieee transactions neural network 

bezdek 
pattern recognition fuzzy objective function algorithms 
plenum press new york 

blake merz 
uci repository machine learning databases 
irvine ca university california department information computer science 
available www ics uci edu mlearn mlrepository html ksrng tex srng 
hammer von 
relevance determination learning vector quantization 
proc 
european symposium arti cial neural networks esann pages brussels belgium 
facto publications 

neural networks research centre helsinki university technology 
bibliography self organizing map som learning vector quantization lvq 
available ira uka de bibliography neural som lvq html 
cortes vapnik 
support vector network 
machine learning 

crammer gilad bachrach tishby 
margin analysis lvq algorithm 
advances neural information processing systems appear 

dav fuzzy shell clustering application circle detection digital images 
international journal general systems 

duch 
new methodology extraction optimization application crisp fuzzy logical rules 
ieee transactions neural networks 

econ data source economic time series data university maryland available online www inform umd edu contents html 
erwin obermayer schulten 
self organizing maps ordering convergence properties energy functions 
biological cybernetics 

geva 
unsupervised optimal fuzzy clustering 
ieee transactions pattern analysis machine intelligence 

graepel burger obermayer 
self organizing maps generalizations new optimization techniques 
neurocomputing 

gustafson 
fuzzy clustering fuzzy covariance matrix 
ieee cdc pages san diego california 

hammer 
note universal approximation capability support vector machines 
neural processing letters 

hammer villmann 
learning vector quantization multimodal data 
ed arti cial neural networks icann springer pages 

hammer villmann 
batch 
verleysen ed european symposium arti cial neural networks side publications pages 

hammer villmann 
generalized relevance learning vector quantization 
neural networks 

haussler 
convolutional kernels structures 
technical report ucsc crl computer science department university california santa cruz 

heskes 
energy functions self organizing maps 
oja kaski editors kohonen maps pages 
springer 

heskes 
self organizing maps vector quantization mixture modeling 
ieee transactions neural networks 
ksrng tex hammer villmann 
jaakkola haussler 
framework detecting remote protein homologies 
journal computational biology 

juang 
discriminative learning minimum error classi cations 
ieee transactions signal processing 

kaski sinkkonen 
topography preserving latent variable model learning metrics 
yin slack editors advances self organizing maps pages springer 

kaski 
bankruptcy analysis self organizing maps learning metrics 
ieee transactions neural networks 

kohonen 
learning vector quantization 
arbib editor handbook brain theory neural networks pages 
mit press 

kohonen 
self organizing maps 
springer 

martinetz schulten 
neural gas network vector quantization application time series prediction 
ieee tnn 

martinetz schulten 
topology representing networks 
ieee transactions neural networks 

russo 
enhanced lbg algorithm 
neural networks 

pfurtscheller 
automated feature selection distinction sensitive learning vector quantization 
neurocomputing 

ritter martinetz schulten 
neural computation selforganizing maps addison wesley 

sato yamada 
generalized learning vector quantization 
tesauro touretzky leen editors advances neural information processing systems volume pages mit press 

sato yamada 
analysis convergence generalized lvq 
niklasson bod en ziemke eds 
icann pages springer 

sch olkopf 
kernel trick distances 
technical report msr tr 
microsoft research redmond wa 

sch olkopf smola 
learning kernels 
mit press 

seo obermayer 
soft learning vector quantization 
neural computation 

sonnenburg jagota 
uller 
new methods splice site recognition 
ed icann pages springer 


uence kernel consistency support vector machines 
journal machine learning research 

hammer 
generalized relevance lvq time series 
dor ner bischof hornik eds arti cial neural networks icann springer pages 

villmann der herrmann martinetz preservation self organizing feature maps exact de nition precise measurement ieee tnn 

villmann hammer 
neural maps remote sensing image analysis 
neural networks 
ksrng tex srng 
vlassis 
greedy algorithm gaussian mixture learning 
neural processing letters 
appendix remains show updates prototypes possible additional parameters chosen similarity measure derived stochastic gradient descent chosen cost function 
obviously describes general setting updates derived update formulas chosing similarity measure standard weighted euclidian metric 
underlying data distribution cost function srng wr cv cv 
sng denoting complement alternatively write function wr cv cv 
cv cv 
sng characterizes prototype closest minw 
denote heaviside function 
recall yields number prototypes closer write ksrng tex hammer villmann derivative heaviside function dirac function symmetric nonvanishing input 
denote derivative respect respectively 
derivative integrand respect cv yields summands cv cv 
cv cv 
sgd 

equals update wp cv cv cv 
sng 
cv equals wp cv wq cv 
sng cv cv cv 

cv cv cv cv coincide term vanishes due properties derivative respect cv yields summands wr cv 
cv cv 
sgd 

equals update wr wp cv cv cv 
sng 
cv equals wr wp cv wq cv cv 
sng cv 
cv ksrng tex srng 


vanishes due properties derivative respect yields summands wr cv 
cv cv 
sgd 

equals update terms wr cv sng cv 
cv 
cv wr cv sng cv 
cv 
cv rst equals wr cv wp cv 
sng cv cv cv 

vanishes 
second equals wr cv wq cv cv 
sng cv 
cv 


vanishes 
ksrng tex ksrng tex 
