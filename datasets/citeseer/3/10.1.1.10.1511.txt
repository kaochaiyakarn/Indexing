machine learning kluwer academic publishers 
manufactured netherlands 
selective sampling nearest neighbor classifiers michael lindenbaum mic cs technion ac il shaul markovitch cs technion ac il dmitry cs technion ac il computer science department technion israel institute technology haifa israel editor david aha 
existing inductive learning algorithms assumption training examples tagged 
domains tagging procedure requires significant computation resources manual labor 
cases may beneficial learner active intelligently selecting examples labeling goal reducing labeling cost 
lss lookahead algorithm selective sampling examples nearest neighbor classifiers 
algorithm looking example highest utility effect resulting classifier account 
computing expected utility example requires estimating probability possible labels 
propose random field model estimation 
lss algorithm evaluated empirically real artificial data sets performance compared selective sampling algorithms 
experiments show proposed algorithm outperforms methods terms average error rate stability 
keywords active learning selective sampling nearest neighbor random field 
existing inductive learning algorithms assume availability training set labeled examples 
domains labeling examples costly process 
consider example training classifier character recognition problem 
case character images easily available classification costly process requiring human operator 
example financial expense incurred medical test required find patient suffers certain disease 
example may chess playing program wants learn position function 
usually true score position ascertained doing deep minimax search game tree 
search computationally expensive number legal positions easily generated 
domains desirable reduce number training examples maintaining quality resulting classifier 
possible solution problem provide learning algorithm control inputs trains 
paradigm called active learning 
approach active learning membership queries angluin learner builds untagged example requests label teacher 
potential practical problem approach constructed example may lindenbaum markovitch sense particular domain illegal chess board arbitrary image character recognition domain 
alternative approach assumes potentially large set unlabeled examples available 
learner selects unlabeled example set asks teacher label 
approach called selective sampling lewis catlett freund dagan engelson 
concentrates selective sampling approach common practice 
problem selective sampling resembles problem cost sensitive learning turney tan schlimmer 
cost sensitive learning costs associated obtaining additional information case obtaining missing attributes obtaining labels unclassified points selective sampling 
related selective sampling example filtering paradigm wilson martinez smyth mckenna zhang yim yang aha kibler albert redundant labeled examples respect previously stored examples removed order reduce complexity resulting classifier improve generalization 
approach selective sampling general setting strong theoretical justification query committee inthis approach committee hypotheses consistent labeled data specified unlabeled example committee members disagree chosen seung opper sompolinsky freund ritter 
techniques field optimal experiment design fedorov applied active learning real valued functions mackay cohn ghahramani jordan 
approaches sampling process find optimum unknown function moore 
various selective sampling methods developed specific classification learning algorithms neural networks davis hwang cohn atlas lander rule induction algorithm lewis catlett hidden markov models dagan engelson 
problem active learning nearest neighbor classifiers context membership queries paradigm considered ritter 
selective sampling algorithms developed nearest neighbor classifiers 
goal fill gap develop selective sampling methodology nearest neighbor nn classification learning algorithms domains uniform labeling costs instance space 
nearest neighbor algorithm cover hart aha kibler albert non parametric classification method useful especially little information available structure class distribution fact imply parametric classifiers harder construct 
nn classifier widely rely parametric model simple implement asymptotically errs bounded rate twice bayesian error 
introduces lookahead algorithm selective sampling suitable nearest neighbor classification 
lookahead principle applied learning algorithms 
see example murthy salzberg decision tree induction 
algorithm named lss lookahead selective sampling chooses selective sampling nearest neighbor classifiers example sequence examples order maximize expected utility resulting classifier 
major components framework utility function classifiers posteriori class probability estimate unlabeled points instance space 
derive bayesian utility function propose random field model feature space classification structure 
model serves basis class probability estimation 
merit approach empirically demonstrated real artificial problems 
compared lss number known selective sampling methods adopted nearest neighbor classifiers 
lookahead selective sampling method outperformed methods terms average error rate performed stability wide range real artificial problems 
proposed algorithm large computational complexity square number training examples 
complexity justified real world problems labeling costs high comparison cost computational resources 
article organized follows 
start formalizing problem selective sampling presenting intuitive considerations selective sampling methods section 
section describe lookahead framework selective sampling 
section describe methods estimating utility classifiers 
section develop random field model feature space classification structure section show methodology implemented 
section describes experimental evaluation algorithm section concludes 

selective sampling nearest neighbor classifiers order address problem selective sampling nearest neighbor classifiers explain formalize concept selective sampling 

selective sampling process consider selective sampling paradigm 
instance space aset objects described finite collection attributes features 
teacher called expert labels instances 
supervised learning algorithm takes set labeled examples xn xn returns hypothesis 
assume unlabeled training set aset objects drawn randomly distribution xi xi xi training data set labeled examples sampling algorithm sl specified relative learning algorithm receives input returns unlabeled element contrast common non incremental learning framework algorithms receive input 
process learning selective sampling described iterative procedure iteration selective sampling procedure called obtain unlabeled example teacher called label example 
labeled example added set currently available labeled examples updated set learning lindenbaum markovitch active learner 




stopping criterion satisfied sl apply sl get example 
ask teacher label update labeled examples set 
update classifier 

return classifier 
active learning selective sampling 
active learner defined specifying stopping criterion learning algorithm selective sampling algorithm sl 
works unlabeled data teacher input 
procedure induces new classifier 
sequence repeats stopping criterion satisfied 
criterion may resource bound onthe number examples teacher willing label lower bound desired class accuracy 
adopting stopping criterion goal selective sampling algorithm produce sequence length leads best classifier measure 
pseudo code active learning system uses selective sampling shown 

nearest neighbor classification particularly interested selective sampling nearest neighbor classifier cover hart 
nearest neighbor classifier accumulates labeled examples received input 
unlabeled instance classified label nearest labeled neighbor 
variations scheme include nearest neighbor classifiers duda hart vote nearest labeled neighbors selective classifiers store utilize labeled examples selectively aha kibler albert 
set labeled examples xn similarity function nearest neighbor rule decides belongs category nearest neighbor arg min xi 
assume euclidean metric 

intuitive considerations considered example 
subsection discuss intuitive considerations selective sampling 
underlying principle uncertainty classification reduced 
possible strategy reducing uncertainty assign classification uncertainty unlabeled example select unlabeled instance associated highest uncertainty 
nearest neighbor classifier points nearest neighbors similar distances conflicting labeling 
idea develop selective sampling algorithm nearest neighbor classifier 
selective sampling nearest neighbor classifiers 
various point configurations may arise training set 
unlabeled points marked circles 
single point near border 
group points near border 
compact group points far labeled examples 
adjacent clusters far labeled examples 
careful look sampling style method exposes number intrinsic problems 
consider various point configurations shown 
configurations simpler typical ones may arise practice provide test cases analyzing behavior selective sampling algorithms 
boundary sampling obviously prefer sample point point configuration 
decision fits intuition point important proximity labeled point 
configuration shows weakness naive uncertainty sampling 
configuration boundary method sample border point 
point labeled uncertainty associated classification cluster remain high 
hand sampling point cluster yield classification high certainty points 
configuration sampling cluster preferable sampling point examples demonstrate selective sampling algorithms consider uncertainty candidate sample point effect classification remaining unlabeled points 
sampling dense regions may preferred sampling isolated point 
compact group unlabeled points surrounded instances classification illustrated configuration sense sample resulting label influences points 
sample instances row 
select points framework described section knowledge fact sample point change priorities example selection 
example configuration clusters may different classes 
allowed sample instance selecting point clusters may best strategy 
hand know able request label instance may better sampling center cluster sampling center cluster 
examples show main consideration uncertainty particular unlabeled point training set effect labeling lindenbaum markovitch 
part lookahead tree depth considering teacher response considering teacher response 
point may neighborhood 
lookahead selective sampling algorithm described section takes considerations account 

lookahead algorithm selective sampling intuition developed previous section introduce lookahead algorithm selective sampling considers sampling sequences length selects example leads best sequence illustrated 
simple way toevaluate merit sequence estimating utility selected points training data classifier regardless labels 
example may prefer sequences uniformly sample training set distribution 
problem approach take account possible responses teacher 
alternative approach views selective sampling process interaction learner teacher 
stage learner select object set unclassified instances teacher assigns possible labels selected object 
interaction represented game tree shown 
tree representation develop lookahead algorithm selective sampling 
ul function estimates merit adding labeled instances set training examples learning algorithm denote conditional class probabilities labeled set deep lookahead algorithm selective sampling respect learning algorithm selects example leads learning sequence highest expected utility 
algorithm 
algorithm specific case decision theoretic agent russell norvig 
specified maximizing expected utility pessimistic consider minimax approach 
simplified version associated lower computational cost step lookahead algorithm see 
implementation step step lookahead algorithms maximize expected utility 
actual lookahead example selection scheme relies choices utility function ul method estimating 
choices considered sections 
selective sampling nearest neighbor classifiers sk select maximal expected utility arg max utility propagation function ul maxx expected value conditional probabilities classifying 

deep lookahead algorithm 
select maximal expected utility ul equal ul 
step lookahead algorithm 

accuracy utility functions major approaches evaluating expected performance hypothesis resulting adding new example dataset 
approach estimates absolute expected accuracy hypothesis regardless current hypothesis 
approach estimates accuracy gain new hypothesis relative current 
second method different estimates accuracy current hypothesis updated data 
subsections utility functions approaches 

absolute accuracy utility function bayesian approach specify utility classifier expected accuracy 
estimating expected accuracy subtle issue known true classifier denoted target estimate accuracy hypothesis 
target known 
pac approach assumes worst case regarding target concept bayesian framework assume target function drawn set possible target functions concept class fixed distribution 
labeled data posterior distribution target functions calculated prior distribution bayes rule expected accuracy defined posterior distribution 
explicit modeling prior distribution target functions complex 
fortunately shall see calculate expected accuracy explicit knowledge actual prior lindenbaum markovitch distribution target functions simpler expressions posterior label probabilities point 
similar direct approach uses distribution targets taken freund 

consider specific target hypothesis ih binary indicator function ih bethe accuracy hypothesis relative ex ih ih dx 
recall isthe probability density function specifying instance distribution section 
probability random point drawn distribution note err 
consider set fd target functions consistent sampled data fd denotes priori defined class target functions concept class 
denote expected accuracy hypothesis produced learning algorithm labeled data expected accuracy taken relative posterior distribution fd defined bayes rule prior distribution assume prior distribution exists specify 
ex ih ex ih ih isthe probability random target function consistent equal point combining eqs 
get dx 
suppose class probabilities 
evaluation class probabilities considered section 
expected accuracy hypothesis respect instance distribution calculated eq 

finite set instances drawn probability estimated 
accuracy function definition target distribution calculated knowing explicit form distribution 
done specific distribution targets matters expected value target function point 
equation translates problem evaluating expected classifier accuracy chosen utility measure problem estimating class probabilities 
selective sampling nearest neighbor classifiers class probabilities calculated selective sampling strategy uses utility function acc 
superscript acc stands accuracy 

gain utility function introduce alternative exploratory utility function 
function prefers examples maximize expected gain classifier accuracy 
maximizes difference expected accuracy new hypothesis previous hypothesis 
expected accuracy calculated basis data implied label probabilities hypotheses 
contrast previous method essentially chooses difference accuracy estimated relative accuracy estimated relative 
suppose labeled data wish evaluate merit sampling points 
target function increase accuracy expected accuracy gain gl ex ih ex ih ex ih ih dx notation stands expected value random variable depending taken distribution target functions consistent binary implies 
integrand iszero gl dx 
expected accuracy gain non negative learning algorithm chooses best hypothesis fixed class hypotheses choose better hypothesis available 
hypothesis class fixed expected accuracy gain non negative learning algorithm produces bayesian hypothesis point posteriori probable label chosen 
shall see context model section nearest neighbor algorithm satisfies assumption 
drawn randomly wecan approximate gl gl gl 
lindenbaum markovitch shall gain gl utility function 
utility function tends sample points potential radically change current hypothesis 
may happen example new sampled point provides information expected labels reduces expected accuracy previous classifier 
proposed utility function acc conservative sense 
difference may lead faster learning domains see section 
section solved problem estimating utility function focusing determining correct class probabilities labeled data 
probabilities estimated readily construct utility function lookahead selective sampler 
section introduce realistic feature space model estimating class probabilities 

random field model feature space classification feature vectors class tend cluster feature space clusters quite complex 
close feature vectors share label 
intuitive observation rationale nearest neighbor classification approach estimate classes unlabeled instances uncertainties 
mathematically observation described assuming label point random variable random variables mutually dependent 
dependencies usually described higher dimensional space random field models 
inthe probabilistic setting estimating classification unlabeled vectors uncertainties equivalent calculating conditional class probabilities labeled data relying random field model 
assume classification feature space sample function binary valued homogeneous isotropic random field wong hajek characterized covariance function decreases distance 
similar method 
progressive image sampling 
points classifications random variables values 
homogeneity isotropy properties imply expected values equal 
properties imply covariance specified distance cov covariance function var priori class probabilities 
usually assume decreases distance 
specifying covariance uniquely determine random field distribution target functions limits considerably accords certain similarity 
example covariance substantial close points decreases distance assume labels close points expected identical 
selective sampling nearest neighbor classifiers shall describe ways calculating estimates conditional probabilities underlying random field model 
methods complement third provides theoretical justification 

calculating probabilities correlation data mean square estimation conditional mean estimation tries find value unobserved random variable observed values related random variables prior knowledge joint statistics 
having classes implies class probabilities associated feature vector uniquely specified conditional mean associated random variable 
conditional mean best estimator value squares sense papoulis 
common methods mean square error mse estimation estimating class probabilities 
choose linear estimator closed form solution described available 
binary associated unlabeled feature vector known associated feature vectors xn labeled 
linear estimator unknown label 
estimate uses known labels relies coefficients construction estimators common statistical procedure papoulis 
optimal linear estimator minimizes mse mse dimensional coefficients vector specified covariance values rij ri 
matrix dimensional vectors 
values specified random field model rij xi ri xi 
procedure straightforward easy implement 
practice reasonably close labeled points construct estimate distant points correlated contribute lack correlation implies lindenbaum markovitch corresponding coefficients zero 
estimation procedure fast 
problem linear estimation label probability range estimated values limited may lie outside interval 
may happen arbitrary specified covariance function may correspond true correlation function binary variables 
interpretation probability values clearly valid 
fortunately rarely case 
invalid values occur clipped resulting legal value gives accurate estimate mse sense binary variables 
problem linear estimators information secondorder statistics covariance matrix neglecting information higher order statistics 

direct calculation conditional probabilities conditional distribution joint distribution 
denote vector values denote probability obtain values 
find joint distribution moment data sum prob 
apriori probabilities xi unknown variables equations unique solution obtained larger 
expected second order statistics uniquely specify arbitrary joint probability function 
additional constraints introduced order obtain unique solution 
knowing number higher moments denoted gives additional constraints xi selective sampling nearest neighbor classifiers 
constraints specified eq 
lower moments 
linear system eqs 
specified moments consists equations unknown variables denoting joint probabilities uniquely solved 
intuition nearest neighbor classifier implies second order moments positive decrease distance 
making assumptions higher order moments constrains distribution basic nearest neighbor rationale may unjustified 
arbitrarily chosen moments may inconsistent binary distribution 
certain context may natural assume class probabilities equal distribution symmetric 

readily implies odd moments zero interestingly symmetry property holds 
assumption stronger just assuming holds provides additional third order moments suffice allows computation joint distribution random variables eqs 

contrast linear estimation approach relies second order statistics distribution non optimal non gaussian distributions direct method described tries calculate exact joint distribution 
requires information form higher order statistical moments guarantees estimated probabilities lie legal interval provided moments correct 
methods associated theoretical deficiency explained solved 

modified random field excursion set model mentioned deficiencies probability estimation methods described 
mse approximation conditional mean yield invalid estimate mean outside range 
direct calculation avoids problem requires set higher order moments necessarily justified nearest neighbor rationale 
major theoretical deficiency see arbitrarily specified covariance function may correspond binary random field 
guarantee exists random field binary outcomes chosen covariance function 
way solve problem specify labels indirectly additional real valued random field 
distribution hidden random field infer covariance binary field 
define gaussian random field collection random variables finite dimensional distributions variables multivariate gaussians adler 
simplicity assume random field homogeneous isotropic lindenbaum markovitch meaning covariance depends distance cov 
way finite dimensional distributions uniquely defined model random concept class set points class random excursion set adler level real valued random field rd binary class label 
value assignment joint probability may calculated numerically integrating multivariate gaussian distribution yn bn ai bi defined model xi hidden random field model corresponding observed concept illustrated 
function non negative definite finite collection xk rd covariance matrix cij xi nonnegative definite 
knowledge prior probabilities possible specify mean distribution satisfies relation hidden signal observed signal dt 

dimensional example relationship hidden field observed labels 
upper realization hidden field 
lower observed labeling function corresponding specific realization hidden field 
selective sampling nearest neighbor classifiers relation may considered condition defining mean prior probability known variance specified 
framework resembles gaussian process modeling mackay williams barber different transformation get binary nearest neighbor method rationale may satisfied specifying covariance associated hidden gaussian process decreasing function distance points 
specification induces covariance associated binary field decreasing function distance 
intuitive properties field satisfied 
show covariance functions necessarily different 
main purpose introducing excursion set specification covariance legal guarantee binary random field specified covariance 
secondary benefit choosing gaussian distribution random field specified second order statistics covariance function associated hidden higher order statistics required 
excursion set model may directly produce predictions conditional probabilities labels 
associated high computational cost 
derive covariance function binary random field guaranteeing validity 
consider situation exponential covariance function hidden real valued random field inthis case possible numerically calculate covariance function binary random field 
see resulting function close exponential 
choose exponential covariance function binary valued random field experimental implementation method see section approximation valid covariance justified theoretically 
method adopted lss algorithm 
labeled points random field model induce posterior label probabilities instance space 
bayesian decision may simply choosing label higher probability williams barber 
general nearest neighbor classification necessarily consistent optimal decision 
example prior probabilities influence bayesian decision random 
log plot covariance function binary computed numerically covariance function underlying gaussian random field 
small values plot linear indicating approximated exponential function 
lindenbaum markovitch field model nn decision 
certain configurations labeled points may give label probability larger point nearest neighbor labeled vice versa 
nn classifier preferable simple implement asymptotically errs bounded rate twice bayesian error 

lookahead selective sampling random field model previous sections describe general framework lookahead selective sampling spectrum methods probability estimation 
section instance proposed algorithm experiments described 
motivation selective sampling substantially reduce number labeled examples get reasonable classifier 
interested initial stage sampling class probabilities estimated distance samples large 
formally sampled point influences estimated probability 
long range influence non intuitive computationally expensive 
practice neglect influence closest neighbors 
choosing closest labeled points theoretical advantage bayesian decisions decision nn classifier see section 
proposed simple step lookahead selective sampling algorithm uses accuracy utility function random field model estimate conditional class probabilities 
random field model designed context nearest neighbor algorithm biased selecting informative examples nearest neighbor classifiers 
specific implementation random field model selective sampling algorithm assumptions reasonable context initial sampling stage simplify assessment class probabilities priori class probabilities equal 
influence closest nearest neighbors conditional probability point negligible 
derivations section assumptions get selective sampling nearest neighbor classifiers 
conditional probabilities class function location dimension point class point class 
points labeled class 
values equal values get direct probability calculations section eqs 
assuming symmetry property eq 

way rely second order statistics linear estimation approach 
choose exponentially decreasing covariance function function approximation covariance function computed numerically excursion set model section providing theoretical justification approach 
bayesian classification defined label consistent nearest neighbor classification 
see illustration 
assuming correct covariance function derived excursion set model guarantee valid range 
show correctness choice prove proposition 
defined eq 

proof main step prove rest similar trivial 
substituting omitting scaling parameter simplifying shown 
triangular inequality fact dij dij get 
putting get lss lookahead selective sampling algorithm depicted 
algorithm needs specification characteristic distance set depending average pair distance unlabeled training lindenbaum markovitch lss 
empty return random point 
set umax 

compute class probabilities points data eq 

compute utility approximating accuracy classifier data computed class probabilities eq 

repeat steps get 
compute class probabilities data eq 


umax umax 
return 

lss algorithm 
set scaling parameter 
parameter set experimental domains 
exact value effect algorithm substantially see section 
time complexity described algorithm straightforward implementation 
time complexity may reduced selecting random subset working managing time performance trade algorithm 

experimental evaluation implemented lss tested problems comparing performance number selective sampling methods context nearest neighbor classification 
specifically assumption limited resources teacher able label examples see section algorithms compared quality accuracy nearest neighbor classifiers produce sampling available unlabeled training set 
hypothesized algorithm outperform algorithm terms classification accuracy settings due sophisticated lookahead approach 
additional experiments conducted test effect various parameters algorithm performance 

experimental methodology lss algorithm compared selective sampling algorithms 
algorithms represent common choices literature see section 
selective sampling nearest neighbor classifiers random sampling algorithm randomly selects example 
method unsophisticated advantage yielding uniform exploration instance space 
method corresponds passive learning model 
uncertainty sampling method selects example label current classifier highest uncertainty 
common choices selective sampling 
specific uncertainty sampling method proposed nearest neighbor classifiers 
defined uncertainty example probability misclassification estimated ratio distances closest labeled neighbors different classes min min 
maximal distance sampling membership query method described ritter nearest neighbor classification 
method selects example set unlabeled points point set nearest classified neighbors label 
example selected distant closest labeled neighbor 
experiments conducted datasets 
short description datasets pima indians ionosphere image segmentation datasets natural datasets taken uci machine learning repository blake keogh merz 
classes image segmentation dataset joined create class labels corresponding sky foliage corresponding cement window path grass 
data datasets split training test sets experiments 
letters dataset dataset available uci machine learning repository 
image segmentation dataset classes letters joined create class labels corresponding corresponding 
dataset consisting instances randomly divided disjoint pairs training test sets examples runs conducted pair resulting total runs 
spirals problem artificial problem included comparison ritter 
task distinguish spirals xy plane 
code available lang witbrock basis data generation program 
artificial datasets training test sets generated independently run 
gaussians multi gaussian problems artificial datasets constructed demonstrate advantage lss algorithm domains consisting lindenbaum markovitch 
feature spaces gaussians multi gaussian problems 
bayesian decision boundaries shown 
region classification 
feature spaces gaussians multi gaussian problems consist dimensional vectors belonging classes equal priori probability 
distribution class uniform region distribution class consists symmetric gaussians gaussian problem symmetric gaussians multi gaussian problem 
feature spaces illustrated 
dataset information summarized table 
shows number attributes dataset sizes disjoint training test sets error rate best possible bayesian classifier error rate nearest neighbor classifier labeled points training set 
bayesian error available artificial datasets true probabilistic models generated natural datasets known 
examples datasets labeled training sets treated unlabeled data sets uniform labeling cost 
data sets real settings labeling cost quite significant 
example table 
dataset information includes number attributes sizes training test sets possible error bayesian error nn error test set error nearest neighbor classifier points training set 
number training set test set bayesian nn error dataset attributes size size error pima indians na ionosphere na image segmentation na letters na spirals gaussians multi gauss selective sampling nearest neighbor classifiers pima indians domain labeling example involve non trivial medical procedure testing 
example image segmentation domain labeling require human expert determine texture type 
basic quantity measured experiments average error rate classifier training points selected selective sampling algorithm 
procedure applied 
training set test sets obtained data 
natural datasets divided training test sets past usage 
artificial datasets training testing sets elements generated independently 
training set unlabeled training set selective sampling algorithm 
test set evaluation error rates resulting classifiers 

selective sampling algorithms applied training set selecting example error rate current hypothesis nearest neighbor classifier calculated test set examples put aside 

steps performed times average error rate standard deviation sampling method approximated maximum likelihood estimation assuming accuracy resulting classifiers normally distributed domain degroot 
training test sets natural datasets remain interested average performance non trivial selective sampling methods naturally randomized selection example 

performance lookahead selective sampling performance various selective sampling methods tested datasets terms average error rate standard deviation summarized tables 
learning curves ionosphere dataset gaussians problem 
results observations natural datasets pima indians ionosphere image segmentation lss algorithm superior performance measures 
better average performance stable selective sampling methods tables 
interestingly pima indians diabetes dataset learning training set lss algorithm yields better classifier nearest neighbor classifier trained available points compare table 
ionosphere dataset lss algorithm requires training set order achieve average accuracy nearest neighbor classifier training points 
letters dataset lss algorithm slightly outperforms selective sampling methods particularly hard letters dataset class consists different subclasses associated different letters 
lindenbaum markovitch table 
estimated average error rates confidence interval nearest neighbor classifiers sampling training set 
dataset lss max 
dist 
uncertainty random pima indians diabetes ionosphere image segmentation letters spirals gaussians multi gaussian statistics runs 
results show cases lss algorithm outperforms selective sampling algorithms terms average error rates resulting classifiers 
table 
estimated standard deviations confidence intervals error rates nearest neighbor classifiers sampling training set 
error rate dataset lss max 
dist 
uncertainty random pima indians diabetes ionosphere image segmentation letters spirals gaussians multi gaussian statistics runs 
results lss algorithm show stable competitors 
random uncertainty maximal distance lookahead number examples error rate random uncertainty maximal distance lookahead number examples 
learning curves selective sampling methods ionosphere dataset gaussian problem 
learning points training set gives error ionosphere dataset gaussians problem 
selective sampling nearest neighbor classifiers spirals problem non random methods performed comparably terms average error rate spirals problem table maximal distance method slightly better 
hand maximal distance method unstable see table 
gaussian problems wecan see uncertainty maximal distance selective sampling methods apparently fail detect gaussians resulting higher error rates 
variance lss algorithm lower datasets reason lss algorithm detects regions case multi gaussian problem regions classification uncertainty maximal distance methods detect occasionally creating greater variance quality resulting classifiers table 
uncertainty maximal distance selective sampling algorithms experience failure due fact consider sampling existing boundary domains exploration essential performance 
random sampling method affected problem 
running times selective sampling algorithms quite different 
naturally lss algorithm sophisticated slowest 
typical query time algorithm second quite acceptable natural setup human expert labeling examples 

effect covariance function performance lss carried additional experiments determine algorithm depends choice covariance function particular depends choice scaling parameter inthe experiments reported previous section set 
section scale parameter set twice times larger smaller original experiments 
change increases decreases actual influence range labeled points 
dependence average error rate shown table results ionosphere image segmentation datasets typical rest data summarized 
results demonstrate stability lookahead algorithm wide range values scale parameter 
addition variance error rates resulting classifiers decreases expected increasing decreases actual influence range labeled points see eq 

higher result local sampling strategy depends initial random example 
table 
estimated average error rates standard deviations classifiers training set produced lss algorithm various parameter values 
dataset scale def 
ionosphere image segm 
statistics runs 
lindenbaum markovitch table 
estimated average error rates lookahead selective sampling algorithm alternative utility functions 
pima ionosphere image letters multi utility indians segm 
spirals gauss 
gauss acc std 
gain 
effect utility function performance lss section proposed alternative exploratory utility function gain results experiment compare function standard utility function acc shown table 
datasets exploratory utility function gain yields slight improvement accuracy utility function 
notable exception noisy pima indians diabetes dataset conservative accuracy utility function advantage exploratory gain utility function tends select example changes current hypothesis 
noisy domains example turns mislabeled 

experiments step lookahead sampling intuition developed section configuration shows may beneficial consider lookahead sampling deeper example described lookahead algorithm 
conducted experiments step lookahead algorithm 
step lookahead algorithm large computational complexity tested real world domains runs standard set experiments results averaged runs 
results reported table 
see cases step lookahead barely reduces classification error resulting classifiers results achieved greater stability 

summary experimental results experiments show lookahead sampling method performs better comparable selective sampling algorithms real artificial domains 
table 
experiments step lookahead 
maximum likelihood estimates average error rates standard deviations classifiers selecting training set 
selective sampling method pima indians ionosphere image segm 
lookahead step deep lookahead steps deep selective sampling nearest neighbor classifiers 
comparing number examples percent training set needed average learning error various methods reach attained lss algorithm training sets 
pima indians domain selective sampling methods achieve accuracy obtained lss algorithm 
especially strong instance space contains region class gaussians multi gauss datasets 
case selective sampling algorithm consider examples hypothesis boundary explore large regions 
lack exploration element uncertainty maximal distance sampling methods results failure 
advantage lookahead selective sampling method seen comparing number examples needed reach pre defined accuracy level 
shows number examples needed various selective sampling algorithms reach average error level attained lookahead selective sampling training set 
pima indians domain selective sampling methods achieve accuracy due noise training set 
experimental results pima indians diabetes dataset especially interesting compared results wilson martinez explored filtering techniques reducing size stored set examples instance learning 
lss algorithm sampling training set achieves accuracy table 
ib del filtering algorithms achieve accuracy respectively table wilson martinez retaining datasets approximately size 
better performance algorithm achieved despite ib del perfect information labels available 
experiments indicate proposed selective sampling approach stable alternatives aspects stability domain variance error rates resulting classifiers lookahead selective sampling algorithm stable regard different training sets domain regard different choices example table 
stability domains results described table show second best selective sampling method changes lookahead selective sampling algorithm remains best stable compared methods majority datasets 
lindenbaum markovitch 
discussion real world domains unlabeled examples available abundance expensive label large number training 
possible solution problem investigated enable learning algorithm automatically select training examples labeled unlabeled training set 
context nearest neighbor classification paradigm viewed uses example filtering addition selective utilization filtering implemented ib algorithm aha kibler albert 
proposes lookahead framework selective sampling selects optimal example bayesian sense 
framework novel random field model instance space labeling structure major contributions research field machine learning 
random field model inspired rationale nearest neighbor classifier 
nearest neighbor classifiers little information available feature space structure 
loose minimalistic specification feature space labeling structure implied distance random field model appropriate case 
observe large changes covariance function significant effect classification performance 
algorithm number deficiencies addressed research 
consider classification point including finding labeled neighbors basic operation uncertainty maximal distance methods time complexity straightforward implementation lookahead selective sampling time complexity 
need compute class probabilities points unlabeled training set lookahead hypothesis 
higher complexity justified natural setup ready invest computational resources save time human expert role label examples 
practice maximal time example selection observed experiments couple seconds typically fraction second quite reasonable real world applications 
deficiency possible direction research lies application random field model probability estimation tool nearest neighbor classification 
mentioned section model consistent nn classifier disregard influence labeled neighbors closest current point interest 
cases neighbors account optimal strategy 
consider example labeled points distributed uniformly unit ball dimensions suppose point interest lies center ball 
higher dimensionality larger part ball weight tends outer sphere difference distances point interest closest labeled points sphere decreases dimensionality 
example supports claim order define classification point interest higher dimensions consider larger number nearest neighbors 
natural extension research application lookahead sampling models feature space classification structure classification algorithms 
extension usage techniques developed reduction selective sampling nearest neighbor classifiers stored examples instance learning algorithms 
direction looks promising light comparison results results wilson martinez see section 
issue reducing time complexity lookahead sampling investigated 
possibility define cost model amount resources time allow order select example 
cost model take account varying labeling costs providing important extension research domains labeling costs uniform instance space 
believe research significant contribution emerging field selective sampling relevance modern data mining applications growing 
acknowledgments authors neri merhav helpful discussions 
portions sixteenth national conference artificial intelligence lindenbaum markovitch 
michael lindenbaum supported part israeli ministry science 
note 
letters dataset looks disjuncts class contains letters 
sure exactly shape instance space represented particular features dataset 
adler 

geometry random fields 
john wiley sons 
aha kibler albert 

instance learning algorithms 
machine learning 
angluin 

queries concept learning 
machine learning 
blake keogh merz 

uci repository machine learning databases university california irvine 
www ics uci edu mlearn mlrepository html 
cohn atlas lander 

improving generalization active learning 
machine learning 
cohn ghahramani jordan 

active learning statistical models 
tesauro touretzky leen eds advances neural information processing systems vol 
pp 

mit press 
cover hart 

nearest neighbor pattern classification 
ieee transactions information theory 
dagan engelson 

committee sampling training probabilistic classifiers 
prieditis russell eds proceedings twelfth international conference machine learning pp 

morgan kaufmann 
davis hwang 

attentional focus training boundary region data selection 
proceedings international joint conference neural networks vol 
pp 

ieee press 
degroot 

probability statistics nd ed 
addison wesley 
duda hart 

pattern classification scene analysis 
wiley interscience 
lindenbaum markovitch lindenbaum 

farthest point strategy progressive image sampling 
ieee transactions image processing 
fedorov 

theory optimal experiments 
new york academic press 
translation 
freund seung shamir 

selective sampling query committee algorithm 
machine learning 
ritter 

active learning generalized high low game 
von der malsburg von seelen sendhoff eds proceedings international conference artificial neural networks vol 
lecture notes computer science pp 

springer verlag 
ritter 

active learning local models 
neural processing letters 
lang witbrock 

learning tell spirals apart 
touretzky hinton sejnowski eds proceedings connectionist models summer school pp 

morgan kaufmann 
lewis catlett 

heterogeneous uncertainty sampling supervised learning 
cohen hirsh eds proceedings eleventh international conference machine learning pp 

new brunswick nj rutgers university 
lindenbaum markovitch 

selective sampling nearest neighbor classifiers 
proceedings sixteenth national conference artificial intelligence pp 

orlando florida aaai press 
mackay 

gaussian processes 
neural networks machine learning 
nato asi series 
series computer system sciences 
mackay 

information objective functions active data selection 
neural computation 
moore schneider boyan lee 

memory active learning optimizing noisy continuous functions 
proceedings fifteenth international conference machine learning pp 

morgan kaufmann 
murthy salzberg 

lookahead pathology decision tree induction 
proceedings fourteenth international joint conference artificial intelligence pp 

papoulis 

probability random variables stochastic processes rd ed 
mcgraw hill 


minimisation data collection active learning 
ieee international conference neural networks proceedings vol 
pp 
vol 

ieee press 
russell norvig 

artificial intelligence modern approach 
prentice hall 
seung opper sompolinsky 

query committee 
proceedings fifth annual acm workshop computational learning theory pp 

new york acm 
smyth mckenna 

building compact competent case bases 
lecture notes artificial intelligence number lecture notes computer science pp 

springer 
tan schlimmer 

case studies cost sensitive concept acquisition 
proceedings eighth national conference artificial intelligence pp 

aaai press 
turney 

cost sensitive classification empirical evaluation hybrid genetic decision tree induction algorithm 
journal artificial intelligence research 
williams barber 

bayesian classification gaussian processes 
ieee transactions pattern analysis machine intelligence 
wilson martinez 

reduction techniques instance learning algorithms 
machine learning 
wong hajek 

stochastic processes engineering systems 
springer verlag 
zhang yim yang 

intelligent selection instances prediction functions lazy learning algorithms 
artificial intelligence review 
received july revised april accepted september final manuscript september 
