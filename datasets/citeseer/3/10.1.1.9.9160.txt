adaptive distributed query processing large scale distributed query engine supports long running queries federated data sources hard obtain statistics data sources servers resources 
addition characteristics data sources servers changing runtime 
traditional distributed query optimizer centralized adaptive techniques inadequate situation 
introduce new highly scalable distributed query processing mechanism called swap scalable adaptable query processor 
swap quickly learn adapt fluctuations selectivities operations workload servers connection speed statistics accordingly change operation order distributed query 
large scale distributed system difficult find optimal plan query 
query processor may accurate statistics participating relations stored nodes selectivities operations 
workload network bandwidth processing servers may change runtime 
problem particularly severe systems supporting continuous queries run long time 
lot works adaptive query processing address problem 
mainly focused centralized processing environments 
cases data sources geographically distributed query engine inherently distributed 
furthermore obvious number queries size data single server handle limited 
need design distributed query engine adapts changing environment runtime 
ultimate aim phd thesis build infrastructure highly adaptive distributed zhou department computer science national university singapore comp nus edu sg query processing engine features system highly distributed 
supports continuous queries ad hoc queries federated data sources 
system adaptively approach optimal processing plan little statistics 
adapt fluctuations environment mentioned 
adaptivity mechanism system support qos management user queries 
date worked new distributed query processor called swap scalable adaptable query processor 
system builds goes straightforward adaptation eddies centralized adaptive query processing mechanism reorder operations distributed query plan runtime 
reordering operations realized dynamically changing orders tuples routed processing sites fluctuations selectivities cost operations workload connection speeds servers 
result swap lead optimal plan 
swap harnesses horizontal vertical parallelism processing sites 
types parallelism eddy site providing adaptivity operations running locally 
vertical parallelism offers greater opportunity adaptivity 
particular propose new mechanism vertical parallelism learn selectivity workload connection speed processing servers accordingly adapt orders tuples routed servers 
key components scheme remote meta operator rmo virtual tuples 
rmo represents operations running remote site 
sending tuples rmo means sending tuple remote site processing 
virtual tuples sent back remote site gathering statistics operations running remote site tickets lottery routing scheme 
proposed variant stem reduce overhead dealing intermediate tuples remote site 
furthermore mechanism continuously adapt runtime changes characteristics servers mentioned 
review related section 
details design swap section 
conclude section agenda 
related due space limit reviewed related 
eddy iterator interposed operators source data 
operators continuously fetching tuples eddy may return result tuples 
routing tuples operators different orders routing scheme eddy able adaptively change order operations runtime generating query plan 
authors introduced back pressure effect lottery routing scheme enable eddy adaptively observe operator behavior cost selectivity route tuples operators order approaching optimal plan 
idea backpressure effect high cost operator consumes tuples slowly forces eddy route tuples lower cost operators 
lottery routing scheme operator assigned number tickets 
operators vie tuple operator tickets win tuple 
operator gets ticket tuple routed looses ticket returns tuple eddy 
number tickets estimate selectivity operator 
stems extend eddies splitting join operator state modules called stems 
stem created base relation addressed query 
tuples arrived built stem probe relations stems get join results 
probing stems different orders join ordering join algorithm spanning tree cyclic queries adapted 
stems provide shared data structure data table regardless number access methods 
facilitates access method adaptation 
survey adaptive query processing 
works addressed problem inaccurate unavailable statistics query optimization 
address problem inaccurate estimation workload connection speed servers fluctuations 
addition solution re optimize remaining part query plan materializing intermediate results 
means increasing overhead interrupting pipelined processing query 
db leo example direction adaptive query processing 
computes adjustments statistics process ing queries benefit adjustments optimizing subsequent queries 
flux introducing adaptivity parallel query processing 
scheme operators horizontally distributed cluster 
flux provides load balancing online repartitioning data shipping states corresponding operators 
horizontal parallelism supported 
scheme harness horizontal vertical parallelism 
clearly incorporate load balancing capabilities flux scheme 
parallelism swap swap operations distributed query plan running multiple sites continuously reordered adapt changing factors 
swap harnesses horizontal vertical parallelism 
cases eddy running processing site responsible dynamically adapting order operations running locally 
case vertical parallelism new mechanism adaptively changing order tuples routed different sites 
section schemes support types parallelism overview process query processing swap 
shall introduce preparatory phase discuss distributed query plan generated set 
preparatory phase swap adapts operations distributed query plan runtime 
prior distributed query plan generated set 
accomplished preparatory phase 
query light weight optimizer produces distributed processing graph dpg 
dpg annotated graph captures processing sites operations types parallelism employed 
optimizer may produce spanning tree cyclic query needed 
traditional distributed query optimization techniques applied 
postponed making decisions adaptive 
optimization options choosing join algorithms access methods adaptivity achieved way discussed 
algorithm launched set processing plan including incorporating remote meta operators remote output operators rmo ro represent operations running remote nodes remote access modules ra represent remote data sources 
refer interested readers details preparatory phase 
scheme horizontal parallelism scheme different sites running independently different partitions data independent subtrees complex query tree correspond la ed 
ra si ra ed la 
sn example scheme horizontal parallelism 
la denotes local access module ra denotes remote access module 
ing intra operator parallelism bushy parallelism 
cases eddy required operators running processing site 
case eddy number operators different fragments source data 
complete results obtained performing union operation output processing sites 
complete results may processed intermediate results output user final results 
second case different subtrees running different sites executed independently simultaneously 
example subtrees subtrees belonging different branches node query tree 
cases eddy running independently eddies provides adaptivity operations running site 
mechanism centralized eddy directly applied scheme 
example intra operator parallelized scheme 
example query fragments relation residing different sites 
fragments sent sites perform join operation 
eddy eddy running independently provide adaptivity operations running sites 
scheme vertical parallelism scheme query split pipelined sub queries 
sub query assigned processing site 
sites running pipelined manner 
output site input site 
tuples undergo sub queries output answers 
interesting problem output site may choice routed sites different order 
example evaluate way join residing different sites joins evaluated site site respectively route tuples site evaluate site perform join get final result route tuples site site operation ordering problem 
choice order balance workload servers minimizing cost communication system resources 
believe traditional query optimization inadequate static query plan fixes order tuples routed unable adapt inaccurate estimations changes workload servers 
scheme routing decision runtime potentially balance workload servers minimize communication cost response time 
decision choosing site output done continuously measuring workload connection speed selectivity operations candidate sites 
done distributed manner site making decision output 
scheme highly scalable sub sections introduce key components scheme provide illustrating example 
remote meta operator virtual tuples key feature remote meta operator rmo 
rmo viewed local representation operations running remote site 
responsible transmitting intermediate results local site remote sites processing collects statistical information operations running remote sites concept virtual tuples 
site needs decision choosing site candidate sites output intermediate results attach eddy corresponding remote site 
view eddy type operator regular operator continuously fetching tuples eddy returning tuples eddy 
tuples returned eddy called virtual tuples typical data tuples 
fact contain data zero data length 
operator continuously sending tuples corresponding remote sites receiving virtual tuples 
order minimize communication overhead remote sites return number virtual tuples generated rmo rmo generate virtual tuples return local eddy 
site transmit intermediate results single remote site attach remote output ro operator eddy rmo 
ro operator continuously sends intermediate results corresponding remote site receive virtual tuples remote site 
virtual tuples returned rmo gather statistics operations running remote site 
focused lottery routing scheme virtual tuples counted lottery routing scheme local eddy 
lottery routing scheme learn selectivities processing sites adaptively change decision site output intermediate results 
furthermore sites lower workload consume tuples quickly sites higher workload consume tuples slowly 
similarly sites slower connections local site consume tuples slowly 
effect back pressure limited queue size tuples routed sites lower workload faster connections 
scheme adapt fluctuations workload connection bandwidth processing sites 
decisions done distributed way sites making decisions output 
means scheme highly scalable limited number processing sites 
intermediate tuples pipelined parallelism result tuple undergone sub queries 
facilitate routing tuples attached tuple bit vector called global footprint bit corresponds sub query 
setting bit global footprint means tuple undergone corresponding sub query 
eddy tuple global footprint determine routed rmo ro operator 
vertically parallelized plan intermediate results processing site transmitted sites processing 
scheme sites receiving intermediate tuples sites treat coming virtual data sources 
sub query running single site 
example way join example stated section scheme joins running site st st intermediate joining results portion relation relation similarly queries running site avoid unnecessary overhead stem types virtual sources containing base relation 
building tuples stem performed corresponding access module 
access module knows exactly fields build tuples stem 
need add predicate stems involved join type virtual sources 
way sub queries require tuples undergo operators need st la ed ro ra ed ra ra ro rm ed la example scheme vertical parallelism 
ra denotes remote access module la denotes local access module 
maintain information different sub queries 
illustrating example illustrating example vertically parallelized case 
query example way join assume relations residing different sites 
query split joins evaluated site site respectively 
assumed source located site 
note assumptions purpose illustration scheme impose restrictions 
site eddy provides online re ordering operations executed locally 
eddy site local access module access tuples relation represent operations executing sites 
example left rmo eddy represents operations executing site continuously fetching tuples returning virtual tuples eddy 
lottery routing scheme directly applied back pressure effect eddy adaptively choose output intermediate results site selective operations lower workload faster connection speed 
site local access module access relation stems evaluation join operation remote access modules retrieve tuples relation intermediate tuples site ro operator output intermediate results 
intermediate results site site represented tuples separate virtual sources st accessed separate access modules 
separate stems sources stem tuples built stem fields base rm rs ra la relation access module 
join operations st predicates stem eddy routes result tuple ro operator detect tuple contains intermediate result tuple site needs returning virtual tuples 
eddy generate virtual tuple calling function representation object virtual source representation object accumulate number virtual tuples sent number reaches threshold sends number virtual tuples corresponding rmo 
dotted curves indicates flow virtual tuples 
similar processing performed site overview process query processing scheme 
system receives query submitted user launches preparatory phase set distributed processing plan 
query split sub queries mode degree parallelism sites execute sub queries determined 
site eddy required operators running adaptively evaluate sub query 
site needs adaptively choose site output intermediate results system attach eddy number remote meta operators candidate output sites 
simply attaches remote output operator eddy outputs results single remote site 
novel distributed query processing scheme adaptively learn selectivity workload connection speed servers 
properties change runtime system adapt behavior accordingly approach optimal plan 
proposed scheme runtime decisions distributed manner 
highly scalable 
addition proposed scheme applicable parallel query processing 
time writing implemented prototype system process evaluating system 
results reported soon 
current result step research agenda 
problems going consider complete phd thesis 
particular focus 
current scheme needs pre optimizer decision split query subqueries choose sites processing sites 
adaptively change decision adaptively merging splitting operations running different sites 
mechanisms policies merging splitting operations needed considered 
second decision adaptive try run query candidate sites time learn cost runtime 
challenge minimize duplicates minimize communication system overhead 
second situation numerous queries running system sharing computation storage network bandwidth queries essential 
current scheme remote meta operator remote output query 
sharing multiple queries reduce consumption network bandwidth system resources maintaining multiple connections 
direction enhance query engine support qos management user 
includes defining qos properties problem system adapts behavior runtime maintain qos requirements 
possible qos specifications completion time result output rate query freshness data number result tuples user penalty delaying halting query avnur hellerstein 
eddies continuously adaptive query processing 
sigmod 
hellerstein franklin chandrasekaran deshpande hildrum madden raman shah 
adaptive query processing technology evolution 
ieee data engineering bulletin 
ives florescu friedman levy weld 
adaptive query execution system data integration 
sigmod 
kabra dewitt 
efficient mid query reoptimization sub optimal query execution plans 
sigmod 
raman deshpande hellerstein 
state modules adaptive query processing 
icde 
shah hellerstein chandrasekaran franklin 
flux adaptive partitioning operator continuous query systems 
icde 
lohman 
leo db learning optimizer 
vldb pages 
zhou ooi 
tan 
swap scalable adaptable distributed query processor 
submitted publication 
