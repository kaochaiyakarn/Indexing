evaluation efficient archival storage techniques increasing volume archival data need retained long periods time motivated design low cost high efficiency storage systems 
inter file compression proposed technique improve storage efficiency exploiting high degree similarity archival data 
evaluate main inter file compression techniques data chunking delta encoding compare traditional intra file compression 
report experimental results range representative archival data sets 
years witnessed unprecedented growth volume stored digital data 
study estimated amount original digital data generated close approximately double volume data created 
increasing fraction corpus archival data immutable data retained long periods time legal archival purposes 
examples archival data include rich media audio images video documents email instant messages 
high rate archival data generation motivated number research projects look lawrence university california santa cruz jack school engineering high street santa cruz california tel cs ucsc edu christos storage systems group hewlett packard labs page mill road ms palo alto ca tel fax christos hpl hp com ways improving space efficiency disk archival storage systems 
researchers observed take advantage content overlapping common archival data improve storage efficiency :10.1.1.18.8085:10.1.1.150.1425
main techniques proposed purpose 
technique divides data object number non overlapping chunks stores unique chunks archival storage system 
chunks may fixed variable size 
second technique resemblance detection data objects uses delta encoding store deltas entire data objects 
approaches developed different contexts different goals data sets 
example variable size chunking proposed improving bandwidth consumption network file systems :10.1.1.10.8444
delta encoding data compression version control systems 
attempt compare approaches side side evaluating storage efficiency achieve applicability different archival data sets 
goals summarized follows evaluate applicability approaches different data types exhibit different degrees inter file similarities ii identify key parameters technique provide rules thumb settings different data types iii compare inter file compression techniques traditional lossless intra file techniques explore potential benefits hybrid approaches 
provide performance analysis different approaches discuss system design engineering considerations 
overview archival data nature exhibits strong inter file resemblance 
examines techniques take advantage inter file resemblance avoid storing redundant data improve storage efficiency 
techniques may combined necessary lossless intra file compression sliding window compression techniques zip variants 
systems exploit data redundancy different levels granularity developed order improve storage efficiency 
class systems detects redundant chunks data granularities range entire file emc individual fixed size disk blocks venti variable size data chunks lbfs :10.1.1.10.8444
focus variable sized chunks reported exhibit better efficiency special case fixed size blocks 
typically techniques content addressable storage cas infrastructures 
second class systems detects stores differences deltas similar files granularity bytes 
chunking data chunking involves problems 
data stream file needs divided chunks deterministic way 
consider general case variable sized chunks works type data including binary formats 
chunk boundaries defined calculating feature digital signature sliding window fixed size 
prototype rabin fingerprints computational efficiency scenario 
boundaries set value feature meets certain criteria value modulo specified integer divisor zero divisor affects average chunk size 
deterministic algorithms require knowledge files system 
chunking performed decentralized fashion clients system 
second problem uniquely identify chunks 
algorithm required computes digest variable length block data 
currently prototype reuse rabin fingerprinting code deriving chunk identifiers 
practice large data sets need algorithm guarantees low probability collisions md sha variants 
exactly chunks chunking suitable cas systems 
unique chunks stored file 
original files reconstructed constituent chunks 
system needs maintain metadata maps file identifiers list chunk identifiers 
evaluation storage efficiency take account overhead due metadata 
illustrates main parameters chunking technique 
sliding window chunk size chunk start window size rabin fingerprint size chunk id size chunking parameters developed prototype program named chc evaluate efficiency performance characteristics chunking having build complete storage system 
input chc archive tar file number files form target data set chc produces output archive includes unique chunks derived original files 
optionally compress individual chunks archive zlib compression library 
chc captures list chunk identifiers file identifier size chunk 
metadata stored output archive provides estimate storage overhead due chunking 
addition chc reconstruct original data set final chunk archive 
delta encoding delta compression compute delta encoding new file file stored system 
resemblance threshold delta calculated stored system 
key problems need addressed delta encoding 
resemblance detected efficient way 
technique proposed project 
calculates rabin fingerprints sliding window entire file window size configurable parameter 
number fingerprints produced proportional file size 
deterministic feature selection algorithm selects subset fingerprints called sketch retained compute estimate resemblance files comparing sketches approximate min wise independent permutations 
estimate computes similarity files counting number matching pairs features sketches 
shown small sketches sets features capture sufficient degrees resemblance 
new data needs stored find appropriate file system file exhibiting high degree resemblance new data 
general computationally intensive task especially expected size archival data repositories 
prototype exhaustive search stored files 
currently investigating hierarchical clustering sketches reduce search 
third problem calculate delta encoding file 
delta compression explored area prototype tool computes output zlib gzip library 
pointers files stored delta 
identifiers sha digests sketch data contribute accounted storage overhead 
prototype consists programs problems feature extraction resemblance detection delta generation 
evaluation explain experimental methodology measure efficiency describe data sets analyze storage efficiency differences performance archival storage techniques 
noted storage efficiency determining factor applicability inter file compression approach 
report storage space required percentage original uncompressed data set size 
example stored data size original represents efficiency ratio 
functionality performance approach depends settings number parameters 
expected experimental results indicate single parameter setting provides optimal results data sets 
report parameter tuning approach different data sets 
optimal parameters data set compare storage efficiency achieved approach 
required storage includes overhead due metadata needs stored 
discuss performance cost design issues applying techniques archival storage system 
data sets chose range data sets believe representative archival data 
email messages contain headers attachments show great resemblance 
source code web content typically versioned 
non textual content presentations imagery similar require lots storage space 
data logs generated high volumes contain repeated content field descriptors 
list data sets 
hp support unix logs sets different total volume linux kernel source code versions email single user mailing list archive blu hp support web site microsoft powerpoint presentations digital raster graphics california minute tiff parameter tuning case chunking expected chunk size key configuration parameter 
implicitly set setting fingerprint divisor minimum maximum allowed chunk size 
general smaller higher probability detecting common chunks files 
data high inter file similarity log files small chunk sizes result greater storage efficiency 
data case smaller chunks mean higher metadata overhead 
overhead storage space required may greater size original corpus 
shows optimal expected chunk size depends type data bit identifiers best efficiencies range bytes 
main configurable parameter case delta encoding size sketches number features resemblance detection 
experimental results consistent reported douglis sketch size features sufficient capture resemblance files 
parameter resemblance threshold efficiency chunking efficiency divisor bit chunk ids min bytes max bytes divisor size bytes list kw list list blu archive pdf powerpoint linux logs chunking efficiency divisor size number features correspond files consider sufficient resemblance exists justify calculating delta storing entire new file 
evaluation delta encoding traverse target data set file time random order 
file delta created file highest resemblance output archive long resemblance threshold corresponding feature 
entire file stored new file 
storage efficiency table shows achieved storage efficiency approaches 
additional zlib compression chunks deltas respectively 
establish baseline data set create single tar file data set compress intra file compression program gzip 
expected shown rows table inter file compression improves larger corpus sizes 
case gzip 
hp unix logs files show high similarity 
chunk compression similar data reduced original data size chunk compressed zlib similar gzip compression just original size 
impressive reductions size delta compression 
delta compression data set reduced original size combined zlib compression compressed data original size 
textual content web pages highly similar 
case hp content gzip compression efficient chunking delta 
surprisingly gzip better additional compression chunks deltas 
reason gzip dictionary efficient entire files smaller individual chunks chunk ids appear random essentially non compressible data 
context archival storage system gzip advantage effective practice discussed 
non textual data powerpoint files chunking delta especially gzip achieve better efficiency gzip 
achieved compression rates impressive log data 
raster graphics delta encoding gzip achieves modest improvement gzip 
single user email directory mailing list archive show little improvement delta 
chunking effective gzip expect reduce redundancy multiple users data 
cases inter file compression outperforms intra file compression especially individual chunks deltas internally compressed 
chunking achieves impressive efficiency large volumes similar data 
hand delta encoding better similar data 
believe due lower storage overhead required delta metadata 
typical sketch sizes bytes features bytes file size significantly smaller overhead chunk storage linear size file 
compressing set files single gzip file establish baseline measurement helps illustrate redundancy exist data set archival storage system reach levels efficiency reasons 
important files added archival system time files retrieved individually 
new file added archival store stored efficiently file incorporated existing compressed file collection new file need added existing tar gzip file 
likewise retrieving file require extracting compressed collection require additional time resources chunk file retrieval method 
experiments measured size entire corpus form tar file compressed gzip 
compressed file data set size files tar chunk chunk delta delta gzip zlib zlib hp unix logs mb hp unix logs mb linux source vers 
mb email single user mb mailing list blu mb hp web pages mb powerpoint mb digital raster graphics mb table storage efficiency comparison bit chunk ids gzip computed aggregate size compressed files sizes gzip compressed files larger 
example case hp web pages gzip efficiency original size larger shown table larger achieved delta compression zlib 
delta compression lesser extent chunking applied files intra file compression method second effective compressing large collections data additional redundancy eliminated 
performance practice space efficiency factor choose compression technique briefly discuss important systems issues computation performance 
chunking approach requires computation delta encoding 
requires hashing operations byte input file fingerprint calculation digest calculation 
contrast delta encoding requires fingerprint calculations byte sketch size 
requires calculating deltas performed efficiently linear time respect size inputs 
additional issues delta encoding include efficient file reconstruction resemblance detection large repositories 
techniques exhibit different patterns 
chunks stored basis identifiers potentially distributed hash table 
need maintaining placement metadata hashing may distributed environments 
reconstructing files may involve random contrast delta encoded objects files smaller delta files stored accessed efficiently sequential manner 
placement distributed infrastructure involved 
inter file compression emerging technique improve space efficiency archival storage systems 
provides direct comparison main techniques proposed literature chunking delta encoding compares traditional intra file compression 
general chunking delta encoding outperform gzip especially combined compression individual chunks deltas 
chunking computationally cheap easily distributed systems 
works data high similarity 
applicable applications multiple versions data version control systems log files 
hand delta encoding computationally expensive efficient similar data potentially applicable wider range data sets 
acknowledgments lawrence supported hewlett packard laboratories microsoft research supported part national science foundation ccr 
eshghi george forman hewlett packard laboratories help insight behavior file chunking 
grateful members storage systems research center university california santa cruz help preparing 
ajtai burns fagin long stockmeyer 
compactly encoding unstructured inputs differential compression 
journal acm may 
broder charikar frieze mitzenmacher 
min wise independent permutations 
journal computer systems sciences 
douglis iyengar 
application specific resemblance detection 
proceedings usenix annual technical conference san antonio texas june 
lyman varian charles jordan pal 
information 

www sims berkeley edu research projects info oct 
macdonald 
file system support delta compression 
master thesis university california berkeley 
mogul douglis feldmann krishnamurthy 
potential benefits delta encoding data compression 
proceedings conference applications technologies architectures protocols computer communication sig comm sept 
muthitacharoen chen mazi res :10.1.1.10.8444
low bandwidth network file system 
proceedings th acm symposium operating systems principles sosp pages lake louise alberta canada oct 
pratt 
feasibility data compression eliminating repeated data practical file systems 
year report 
quinlan dorward 
venti new approach archival storage 
long editor proceedings conference file storage technologies fast pages monterey california usa 
usenix 
rabin 
fingerprinting random polynomials 
technical report tr center research computing technology harvard university 
tichy 
rcs system version control 
software practice experience july 

