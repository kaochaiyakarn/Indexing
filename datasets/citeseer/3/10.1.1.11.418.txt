merl mitsubishi electric research laboratory www merl com charting manifold matthew brand tr march construct nonlinear mapping high dimensional sample space low dimensional vector space effectively recovering cartesian coordinate system manifold data sampled 
mapping preserves local geometric relations manifold pseudo invertible 
show estimate intrinsic dimensionality manifold samples decompose sample data locally linear low dimensional patches merge patches single lowdimensional coordinate system compute forward reverse mappings sample coordinate spaces 
objective functions convex solutions closed form 
may copied reproduced part commercial purpose 
permission copy part payment fee granted nonprofit educational research purposes provided partial copies include notice copying permission mitsubishi electric information technology center america acknowledgment authors individual contributions applicable portions copyright notice 
copying reproduction republishing purpose shall require license payment fee mitsubishi electric information technology center america 
rights reserved 
copyright mitsubishi electric information technology center america broadway cambridge massachusetts nips december 
appears proceedings neural information processing systems volume 
mit press march 
charting manifold matthew brand mitsubishi electric research labs broadway cambridge ma usa www merl com people brand construct nonlinear mapping high dimensional sample space low dimensional vector space effectively recovering cartesian coordinate system manifold data sampled 
mapping preserves local geometric relations manifold pseudo invertible 
show estimate intrinsic dimensionality manifold samples decompose sample data locally linear low dimensional patches merge patches single lowdimensional coordinate system compute forward reverse mappings sample coordinate spaces 
objective functions convex solutions closed form 
nonlinear dimensionality reduction nldr charting charting problem assigning low dimensional coordinate system data points high dimensional sample space 
presumed data lies near lowdimensional manifold embedded sample space exists smooth nonlinear transform manifold low dimensional vector space 
goal estimate smooth continuous mappings sample coordinate spaces 
analysis shed light intrinsic variables phenomenon example revealing perceptual configuration spaces 
goal find mapping expressed kernel mixture linear projections minimizes information loss density relative locations sample points 
constraint expressed posterior combines standard gaussian mixture model gmm likelihood function prior penalizes uncertainty due inconsistent projections mixture 
section develops special case posterior unimodal closed form yielding gmm covariances reveal patchwork overlapping locally linear subspaces cover manifold 
section shows gmm choice reduced dimension unique closed form solution minimally distorting merger subspaces dimensional coordinate space reverse mapping defining surface manifold sample space 
intrinsic dimensionality data manifold estimated growth process point point distances 
analogy differential geometry call subspaces charts merger connection section considers example problems methods knots unroll sheets visualize video data 
background topology neutral nldr algorithms divided compute mappings directly compute low dimensional embeddings 
field roots mapping algorithms demers cottrell proposed auto encoding neural networks hidden layer bottleneck effectively casting dimensionality reduction compression problem 
hastie defined principal curves nonparametric curves pass center nearby data points 
rich literature grown properly regularizing approach extending surfaces 
smola colleagues analyzed nldr problem broader framework regularized quantization methods 
advances aim embeddings gomes treat manifold completion anisotropic diffusion problem iteratively expanding points connect neighbors 
isomap algorithm represents remote distances sums trusted set distances immediate neighbors uses multidimensional scaling compute low dimensional embedding minimally distorts distances 
locally linear embedding algorithm lle represents point weighted combination trusted set nearest neighbors computes minimally distorting low dimensional barycentric embedding 
complementary strengths isomap handles holes fail data hull nonconvex vice versa lle 
offer embeddings mappings 
noted trusted set methods vulnerable noise consider subset point point relationships lowest signal noise ratio small changes trusted set induce large changes set constraints embedding making solutions unstable 
return mapping roweis colleagues proposed global coordination learning mixture locally linear projections sample coordinate space 
constructed posterior penalizes distortions mapping gave expectation maximization em training rule 
innovative variational methods highlighted difficulty hill climbing multimodal posterior 
method develop decomposition manifold locally linear neighborhoods 
bears closest relation global coordination different construction problem avoid hill climbing spiky posterior develop closed form solution 
estimating locally linear scale intrinsic dimensionality matrix sample points yn yn populating ddimensional sample space conjecture points samples manifold intrinsic dimensionality seek mapping vector space xn xn reverse mapping local relations nearby points preserved formalized 
map non catastrophic folds parallel lines manifold map continuous smooth non intersecting curves guarantees linear operations interpolation reasonable analogues smoothness means scale mapping neighborhood rd effectively linear 
consider ball radius centered data point containing data points 
count grows rd locally linear scale grow rate inflated isotropic noise smaller scales embedding curvature larger scales 
estimate look ball grows points added tracking logn logr 
noise scales noise distributed points directions equal probability 
scale curvature significant manifold longer perpendicular surface ball ball grow fast accommodate new points 
locally linear scale process peaks points distributed directions manifold local tangent space 
maximum gives estimate scale local dimensionality manifold see provided ball hasn expanded manifold boundary boundaries lower dimension scale behavior manifold space samples noise scale locally linear scale curvature scale radius log scale point count growth process manifold space radial growth process hypothesis hypothesis hypothesis points log scale point growth processes 
left locally linear scale number points ball grows noise curvature scales grows faster 
right point count growth process find intrinsic dimensionality manifold nonlinearly embedded space see 
lines slope fitted sections logr curve 
neighborhoods radius roughly points slope peaks indicating dimensionality 
data appears dominated noise points data appears manifold curvature 
ball expands cover entire data set dimensionality appears drop process begins track edges sheet 
manifold 
low dimensional manifolds sheets boundary submanifolds edges corners small relative full manifold boundary effect typically limited small rise approaches scale entire data set 
practice code simply expands ball point looks peak averaged nearby balls 
estimate globally point 
charting data charting step find soft partitioning data locally linear low dimensional neighborhoods prelude computing connection gives global lowdimensional embedding 
minimize information loss connection require data points project subspace associated neighborhood minimal loss local variance maximal agreement projections nearby points nearby neighborhoods 
criterion served maximizing likelihood function gaussian mixture model gmm density fitted data yi yi yi 
gaussian component defines local neighborhood centered axes defined eigenvectors amount data variance axis indicated eigenvalues data manifold locally linear vicinity dominant eigenvalues near zero implying associated eigenvectors constitute optimal variance preserving local coordinate system 
degree likelihood maximization naturally realize property requires gmm components shrink volume fit data tightly possible best achieved positioning components locally flat collections datapoints 
state affairs easily violated degenerate zero variance gmm components components fitted overly small locales data density manifold comparable density manifold noise scale 
consequently prior needed 
criterion implies neighboring partitions dominant axes span sim ilar subspaces disagreement large subspace angles lead inconsistent projections point uncertainty location low dimensional coordinate space 
principal insight criterion exactly cost coding location point neighborhood generated neighborhood cross entropy gaussian models defining neighborhoods dyn log log trace 
roughly speaking terms measure differences size orientation position respectively coordinate frames located means axes specified eigenvectors 
terms decline zero overlap frames maximized 
maximize consistency adjacent neighborhoods form prior exp mi ni mi measure locality 
global coordination asking dominant axes neighboring charts aligned span nearly subspace 
easier objective satisfy contains useful special case posterior yi unimodal maximized closed form associate gaussian neighborhood data point setting yi take neighborhoods priori equally probable setting pi locality measure determined local kernel 
example mi scale parameter specifying expected size neighborhood manifold sample space 
reasonable choice erf density mi contained area yi manifold expected locally linear 
uniform pi mi fixed map estimates gmm covariances mi mi note covariance dependent map estimators covariances arranged set fully constrained linear equations solved exactly mutually optimal values 
key step brings nonlocal information manifold shape local description neighborhood ensuring adjoining neighborhoods similar covariances small angles respective subspaces 
local subset data points dense direction perpendicular manifold prior encourages local chart orient parallel manifold part globally optimal solution protecting pathology noted 
equation easily adapted give reduced number charts charts centered local centroids 
connecting charts build connection set charts specified arbitrary nondegenerate gmm 
gmm gives soft partitioning dataset neighborhoods mean covariance optimal variance preserving low dimensional coordinate system neighborhood derives weighted principal component analysis exactly specified eigenvectors covariance matrix vk kv eigenvalues descending order diagonal wk id operator projecting points kth local chart local chart coordinate wk yi uk uk holds local coordinates points 
goal sew charts globally consistent low dimensional coordinate system 
chart low dimensional affine transform gk projects uk global coordinate space 
summing charts weighted average projections point yi low dimensional vector space xi yi ji yi pk pk probability chart generates point pointed point nonzero probabilities charts affine transforms charts map point place global coordinate space 
set weighted squares problem gk arg min pk yi yi gk ji 
equation generates homogeneous set equations determines solution affine transform solution methods 
temporarily anchor neighborhood origin fix indeterminacy 
adds constraint solve define indicator matrix fk identity matrix occupying kth block gk 
diagonal pk diag pk yn record point posteriors chart squared error connection sum patch anchor patch patch inconsistencies pkp gu uk uk fk setting de dg solving minimize convex gives kp ju 
remove dependence neighborhood rewriting equation argmin gu gq trace uk require gg prevent degenerate solutions equation solved rotation coordinate space setting eigenvectors associated smallest eigenvalues qq eigenvectors computed efficiently explicitly forming qq numerical efficiencies obtain zeroing vanishingly small probabilities pk yielding sparse eigenproblem 
interesting strategy numerically condition problem calculating trailing eigenvectors qq 
shown maximizes posterior gq prior favors mapping unit norm rows zero mean 
maximizes variance row spreads projected points broadly evenly coordinate space 
solutions map charts equation connection equation applied fitted mixture gaussians factors pcas density model large eigenproblems avoided connecting just small number charts cover data 
reviewers calling attention teh roweis volume shows connect set local dimensionality generalized eigenvalue problem related equation 
original data embedding xy view xyz view charting best isomap best lle regularized data linked xy view xz view random subset local charts charting projection coordinate space lle lle lle lle lle lle reconstruction back projected coordinate grid twisted curl problem 
left comparison charting isomap lle 
points randomly sampled manifold noise 
charting method recovers original space catastrophes folding albeit shear 
right manifold regularly sampled noise illustrate forward backward projections 
samples shown linked lines help visualize manifold structure 
coordinate axes random selection charts shown bold lines 
connecting subsets charts give mappings 
upper right quadrant shows various lle results 
bottom show charting solution reconstructed back projected manifold smooths noise 
connection solved equation gives forward projection point coordinate space 
numerically distinct candidates backprojection posterior mean mode exact inverse 
general may unique posterior mode exact inverse solvable closed form true 
note chart wise projection defines complementary density coordinate space id id px gk gk 
map subspace surface manifold dirac delta function mean linear function posterior mean back projection obtained integrating uncertainty chart generates gk gk denotes pseudo inverse 
general back projecting map reconstruct original points 
equation generates surface passes weighted average neighborhoods yi nonzero probability principal curve passes center local group points 
experiments synthetic examples points randomly sampled square embedded curl twist contaminated gaussian noise 
sampled manifold unrolled distortion 
addition outer curl sampled densely inner curl 
order magnitude fewer points higher noise levels possibility isometric mapping uneven sampling arguably challenging problem swiss roll curve problems featured 
left contrasts unique output charting best outputs obtained isomap lle considering neighborhood sizes points 
isomap lle show catastrophic folding change lle regularization order nondegenerate solutions 
charting data xy view data yz view local charts embedding ordinate embedding true manifold arc length knot charting 
noisy samples embedded manifold shown connected dots front side views 
subset charts shown 
solving connection gives 
removing points cut knot charting gives embedding plot true manifold arc length monotonicity modulo noise indicates correctness 
pose scale expression principal degrees freedom recovered raw jittered images images synthesized backprojection straight lines coordinate space modeling manifold facial images raw video 
row contains images synthesized back projecting axis parallel straight line coordinate space manifold image space 
blurry images correspond points manifold neighborhoods contain nearby data points 
designed isometry affine transform forward projected points disagree original points rms error lower best lle best isomap shown 
right shows problem points sampled regularly grid noise added embedding 
shows similar treatment line threaded knot contaminated gaussian noise charting 
video obtained frame video sequence courtesy roweis frey pixel images strikes variety poses expressions 
video heavily contaminated synthetic camera 
raw images image processing removed uninteresting sources variation 
took frame subsequence left right mirrored obtain points image space 
point growth process peaked just dimensions 
solved charts centered random point connection 
recovered degrees freedom recognizable pose scale expression visualized 
original data map charting flattening 
left original points mapping embedding recovered charts map 
fewer charts lead isometric mappings fold bowl shown 
conformality manifolds flattened conformally preserving local angles isometrically 
shows data finely charted connection behaves conformally isometrically 
problem suggested tenenbaum 
discussion charting breaks kernel nldr subproblems finding set locally linear neighborhoods charts adjoining neighborhoods span maximally similar subspaces computing minimal distortion merger connection charts 
solution optimal estimated scale local linearity solution optimal solution desired dimensionality problems bayesian settings 
offloading nonlinearity kernels obtain squares problems closed form solutions 
scheme attractive large eigenproblems avoided reduced set charts 
dependence trusted set methods potential source solution instability 
practice point growth estimate fairly robust data perturbations expected data density changes slowly manifold integral hausdorff dimension soft neighborhood partitioning appears charting solutions reasonably stable variations eigenvalue stability analyses may prove useful 
ultimately prefer integrate 
contrast appears virtue eigenvector methods best dimensional embedding merely linear projection best dimensional embedding unique distortion value maximizes information content embedding 
charting performs datasets signal noise ratio confounds state art methods 
reasons may nonlocal information construct system local charts global connection 
mapping preserves component local point point distances project manifold relationships perpendicular manifold discarded 
charting uses global shape information suppress noise constraints determine mapping 
acknowledgments buhmann roweis tenenbaum anonymous reviewers insightful comments suggested challenge problems 
balasubramanian schwartz 
isomap algorithm topological stability 
science january 
bregler omohundro 
nonlinear image interpolation manifold learning 
nips 
demers cottrell 
nonlinear dimensionality reduction 
nips 
gomes 
variational approach recovering manifold sample points 
eccv 
hastie stuetzle 
principal curves 
am 
statistical assoc 
hinton dayan revow 
modeling manifolds handwritten digits 
ieee trans 
neural networks 
leen 
dimensionality reduction local principal component analysis 
neural computation 
roweis saul hinton 
global coordination linear models 
nips 
roweis saul 
nonlinear dimensionality reduction locally linear embedding 
science december 
smola mika sch lkopf williamson 
regularized principal manifolds 
machine learning 
teh roweis 
automatic alignment hidden representations 
nips 
tenenbaum de silva langford 
global geometric framework nonlinear dimensionality reduction 
science december 
