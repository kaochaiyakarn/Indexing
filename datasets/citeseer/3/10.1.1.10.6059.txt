appears proceedings th ieee international conference network protocols icnp exploiting routing redundancy structured peer peer overlays ben zhao ling huang jeremy stribling anthony joseph john kubiatowicz computer science division berkeley cs berkeley edu structured peer peer overlays provide natural infrastructure resilient routing efficient fault detection precomputation backup paths 
overlays respond faults milliseconds rapidly shifting alternate routes 
adaptive mechanisms structured overlays illustrate operation context tapestry fault resilient overlay berkeley 
describe transparent protocol independent traffic redirection mechanism tunnels legacy application traffic overlays 
measurements tapestry prototype show highly responsive routing service effective circumventing range failures incurring reasonable cost maintenance bandwidth additional routing latency 

continued growth internet developers deploying new larger scale network applications file sharing instant messaging streaming multimedia voice ip voip 
applications placing increasingly heavy demands internet infrastructure requiring highly reliable delivery quick adaptation face failure 
unfortunately increasingly difficult meet criteria 
growing size complexity network lead frequent periods wide area disconnection high packet loss 
variety factors contribute including router reboots maintenance schedules bgp misconfigurations cut fibers hardware faults 
resulting loss jitter application traffic creates significant widespread deployment realtime applications voip 
magnitude loss jitter function routing protocol response time faults including time detect fault construct new path reroute traffic 
analyzed fault recovery time intra protocols large ip backbone networks 
recovery order seconds majority delay due path recalculation arises timed delay fault detection update routing entries 
exacerbated hardware features current routers 
factors reasonable expect respond route failures seconds 
wide area route convergence bgp significantly slower 
identified interactions protocol timers fundamental cause delayed convergence 
bgp disseminates reachability updates hop hop neighbors full propagation network take seconds longest alternative path source destination length typical bgp rate limiting timer 
unfortunately studies shown significant growth bgp routing tables fueled stub ases meaning delayed convergence problem grow severity time 
commonality deployed protocols network treated unstructured graph arbitrary connections implying potential dependencies peers 
local changes propagated peers network 
attempts aggregate state reduce bandwidth primary motivation timers contribute route convergence delay 
addressing schemes cidr introduce hierarchy structure namespace reduce amount routing state potentially reduce need long term timers 
problem remains inter routing driven peering agreements policy making state reduction difficult problem 
contributions address issues 
propose structured peer peer overlay networks resilient routing client daemon tunneling traffic control traffic overlay network peer proxy peer proxy legacy node legacy node register register 
tunneling traffic wide area overlay 
legacy application nodes tunnel wide area traffic overlay 
tures 
support efficient fault detection precomputation backup paths overlays rapidly switch alternate paths network failure 
second describe general redirection addressing mechanism transparently redirects ip traffic legacy applications overlay providing stable communication legacy applications face variety faults 
illustrates high level architecture 
structured overlays utilize local routing resources log resulting low control traffic fast propagation route updates 
contrast previous unstructured approaches resilient overlay networks exhibit state management problem seen bgp :10.1.1.161.2760:10.1.1.161.2760
consequently unstructured networks incur total beacon messages fault detection backup path construction preventing system scaling hundreds nodes 
adaptive routing mechanisms straightforward implement structured overlays reachable link constrained multicast 
chooses set routing paths stable second sends duplicate packets alternate paths adapt high packet loss 
maintain level routing redundancy structured networks continually refresh routing tables 
result decoupling discovery backup paths precomputation rapid adaptation failure route selection 
precomputation shields applications slower route convergence network layer 
demonstrate benefits overlay routing structured overlays combination analysis simulation experimental measurements 
deploy prototype tapestry system fault resilient overlay implements adaptive mechanisms :10.1.1.1.4310
show tapestry recover network failures milliseconds kilobytes second node beaconing traffic agile support streaming multimedia applications 
design scalable easy internet service providers isps deploy providing routing services legacy applications 
remainder follows section describes utilize structured peer peer overlay network quickly detect routing failures route precomputed backup paths 
section describes architecture fault tolerant traffic tunneling service built overlay 
evaluate design section 
discuss related section conclude section 
fault tolerant overlay routing section examine fault tolerant routing properties structured peer peer overlay networks 
give overview overlays generalized properties 
algorithms require basic key node mapping function common protocols 
motivate examples perform measurements locally designed protocol tapestry results extend 
discuss mechanisms efficient fault detection 
propose techniques routing link failures loss maintaining routing redundancy failures 

structured peer peer overlays structured peer peer overlay networks gained popularity platform construction resilient large scale distributed systems :10.1.1.140.3129:10.1.1.28.5987:10.1.1.118.8846:10.1.1.17.4065:10.1.1.16.4785:10.1.1.105.3673
structured overlays conform specific graph structure allows locate objects exchanging log messages overlay nodes 
node represents instance participant overlay nodes may hosted single physical ip host 
participating nodes assigned nodeids uniformly random large identifier space 
applicationspecific objects assigned unique identifiers called keys selected identifier space 
example pastry tapestry chord kademlia skipnet identifier space bit integers modulo chord kademlia skipnet tapestry pastry :10.1.1.28.5987:10.1.1.109.4681:10.1.1.118.8846:10.1.1.118.8846:10.1.1.1.4310:10.1.1.16.4785:10.1.1.105.3673
overlays dynamically map key unique live node called root 
overlays support routing messages key root node called routing 
deliver messages efficiently node maintains routing table consisting nodeids ip addresses nodes local node maintains overlay links 
messages forwarded overlay links nodes nodeids progressively closer key identifier space 
system defines function maps keys nodes 
example tapestry maps key live node nodeid longest prefix match node higher ac bb 
routing example tapestry 
routing path taken message node node tapestry hexadecimal digits length 
key routing overlays hop resolves digit 
nodeid value chosen digit matched exactly 
important benefit key routing node satisfying namespace constraints serve routing hop 
example tapestry pastry hop message routing key requires node nodeid begins 
property allows overlay node proactively maintain small number backup routes routing table 
detecting failed outgoing link router rapidly switch backup link providing fast failover 
background overlay networking algorithms adapt failure restoring re pairing redundancy backup links 
discuss section 
structured protocols support key routing differ performance tradeoffs 
appropriate details tapestry illustrate points discussions 
tapestry structured peer peer overlay uses prefix matching route messages keys additional hop matches key digits 
routing entry tapestry tries locate nearest node required prefix nodeid nearest neighbor search algorithm :10.1.1.1.4310
discerning factor proximity routing latency optimization routes knowledge physical network latencies overlay construction improve latency 
overhead routing overlay generally measured relative delay penalty rdp ratio overlay routing latency ip latency 
show section proximity enabled overlays tapestry provide low overhead ip 
tunneled ip traffic gains resilience faults unreasonable increases delay 

efficient fault detection internet failures durations minutes making fast response time key objective fault resilient routing mechanism 
traditionally beacons period node nodes overlay unstructured overlay tapestry pastry basic chord 
fault detection bandwidth 
unstructured overlay networks consume far maintenance bandwidth structured networks 
bandwidth measured beacons node beacon period 
response time sum fault detection time path discovery time proactively maintaining backup paths allows immediately redirect traffic failure detection eliminating path discovery time 
focus minimizing application level protocols generally depend beacons heartbeat messages detect link node failures 
bandwidth limiting factor proportional product number entries routing table heartbeat frequency 
nodes structured peer peer overlays maintain routing state grows logarithmically network size log 
compared unstructured protocols linear growth routing state overlays send beacons significantly higher frequencies consuming identical bandwidth :10.1.1.161.2760:10.1.1.161.2760
shows number heartbeats sent period unstructured overlays ron structured overlays tapestry chord 
total bandwidth consumption number beacons sent period useful capture true impact network 
queuing delay congestion happen ip router basis identical messages traversing different overlay hops place different stresses network depending number ip hops traversed 
accurate measure total bandwidth consumption tbc measured bandwidth distance product tbc msgs sec 
bytes msg 
metric reflects fact longer paths greater opportunity cost consume total resources 
crossing fewer ip hops means routes low tbc fewer chances encountering failures 
messages routing latency optimized overlays cross smaller number ip routers incur lower tbc 
quantify effect section comparing simulated tbc single struc primary route secondary route tertiary route original route path link failure rerouted path 
reachable link 
simple route selection reachable link circumvent single multiple failed links overlay path 
primary route secondary route multicast path 
constrained multicast 
examples constrained multicast showing multicast occurring different positions overlay path 
tured protocol tapestry constructed overlay hop latency information 
link quality estimation measure quality routing links nodes send periodic beacons outgoing links 
longer periodic intervals node replies aggregated acknowledgment message 
ack includes sequence numbers beacons received allowing sender gauge link quality loss rates 
backup routes need probed periodically 
conserve bandwidth send beacons primary entries beacon rate probe backup entries half rate 
derive estimated link quality current measured loss rate history past values 
avoid intermittent problems avoid route flapping introduce damping estimating loss rate ln 
ln 
instantaneous loss rate current period hysteresis factor 
explore appropriate damping factor section 

resilient routing policies having described mechanisms maintain monitor backup paths need define paths evade routing failures 
describe policies st hop nd hop rd hop secondary route tertiary route primary route 
path convergence prefix routing 
routing path prefix protocol 
note additional hop expected number nearby hop routers decreases causing paths rapidly converge 
define routes chosen failure lossy conditions 
discussions refer primary backup entries routing table backups hop nodes satisfy routing constraint away network 
reachable link selection define simple policy called reachable link selection route messages failures 
node observes link node failures near total loss connectivity outgoing route 
set latency sorted backup paths chooses route link quality defined threshold see examples 
constrained multicast simple link selection effective multiple links experiencing high loss 
propose constrained multicast message entering lossy region network duplicated copies sent multiple outgoing hops 
constrained multicast complementary triggered hop path estimated link quality higher example node monitors possible paths hop stores sorts sorted latency 
typical 
link failure estimated link qualities 
chooses link order minimum link quality 
case high loss link qualities 
path satisfies messages duplicated sent subset available paths 
shows examples constrained multicast occurring different points routing path 
discussion routing policies provide controlled tradeoff bandwidth reliability available 
naive constrained multicast exacerbate lossy links loss due congestion additional traffic sent alternate independent path 
note consequence packet loss feedback indicate congestion 
protocols provide explicit congestion feedback xcp eliminate side effect 
discuss convergence property significantly reduce bandwidth overhead imposed duplicate messages allowing selectively detect drop duplicates 
efficiency path convergence observe overhead routing away primary path controlled protocols demonstrate path convergence paths common destination intersect rate proportional distance source nodes 
shows path convergence tapestry 
property exhibited protocols consider network proximity conjunction id constraints intermediate overlay routers 
pastry tapestry protocols implement proximity routing prefix routing scheme 
additional hop expected number nodes network qualify hop router decreases linearly 
nearby nodes increasingly choose common hop zero destination name number possible routers decreases 
shows underlying network geometries protocols demonstrate path convergence properties 
path convergence message takes backup route converge back primary path hop 
minimizes impact single backup path latency 
constrained multicast convergence allows routers detect drop duplicate packets minimizing stress put network duplicate 
routers identify messages flow id monotonically increasing sequence number 
router effectively detect drop duplicates efficient finite queue sequence numbers 
self repair discussion focused routing alternate paths network links failed note self repair algorithms replenish backup paths recovering failure 
primary backup paths eventually fail leaving paths unreachable 
overlay detects path failure act replace failed route restore pre failure level path redundancy 
algorithms self repair specific overlay protocol 
general goal find additional nodes specific constraint matching certain prefix having certain position linear coordinate namespace nodes property 
general strategies possible 
node query nearby nodes minimize repair latency query nodes satisfy constraint 
strategy gives higher chance successful repair cost contacting fur normal traffic overlay traffic overlay put hash ip get hash ip put hash ip register register daemon daemon linux kernel legacy application legacy node ip network linux kernel legacy application legacy node 
proxy architecture 
architectural components involved routing messages source destination destination stores proxy id hash ip address object overlay 
source proxy retrieves proxy id overlay routes traffic 
ther away nodes 
example tapestry node query nearby nodes nodes match prefix query nodes routing table share similar nodes 

tunneling traffic structured overlays section discuss architecture tunnels legacy application traffic overlay benefit fault tolerance section tunneled traffic leverages overlay mechanisms route link node failures 

transparent tunneling shows architecture tunnels application traffic overlay nearby proxy nodes 
clients contain overlay aware daemons addressable overlay locating nearby proxies advertising mappings native ip addresses overlay proxy addresses 
new outgoing ip connection client daemon determines destination reachable overlay daemon redirects traffic nearby proxy enters overlay routes destination proxy exits destination node 
elaborate paragraphs 
proxy traffic redirection traffic redirection involves steps registering id legacy node overlay id space publishing mapping node ip address id register id daemon legacy node chooses nearby overlay node proxy service directory 
recall structured overlay ids namespace mapped specific root node 
proxy assigns id id space closest unused id inside range range defined routing protocol 
illustrates registration tunneling 
example legacy nodes registering chord proxy receive sequentially decreasing identifiers proxy id destination source proxy id 
registering proxy nodes 
legacy application nodes register nearby proxies allocated proxy ids close name proxy node 
legacy nodes address new proxy names routing overlay reach 

insures messages addressed delivered despite changes overlay membership 
assuming nodeids assigned uniformly random probability proxy node legacy clients loses new overlay node size namespace 
active proxy nodes chance proxy losing legacy node 
example bit namespace overlay nodes averaging legacy clients probability new node hijacking proxy 
step establish mapping ip address overlay id allows overlay dns name translation 
structured overlays storage layer distributed hash table dht opt utilize 
proxy stores overlay mapping hash legacy node ip proxy identifier put call dht interface storing mapping locally publish call dolr interface 
application interface endpoints client side daemon implemented set packet forwarding rules packet encapsulation process 
daemon capture packets general rules linux ip chains freebsd divert sockets 
processes ip ip encapsulation forward certain packets proxy remainder unchanged back normal network interface 
daemon responsibilities register local ip address destination overlay divert appropriate outgoing traffic overlay 
expect ip registration occur daemon starts described 
destinations reachable overlay daemon monitors outgoing traffic selects flows tunneling 
done local user transparent fashion hijacking dns requests new connection requests 
query new ip addresses determine reachable overlay cache result 
application starts connection address overlay daemon notifies proxy locates destination node proxy identifier performing get sha ip dest 
redundant proxy management overlay provides scalable way route failures network proxy may fail disconnected overlay destination nodes responsible 
outline possible solutions 
case legacy node registers small number proxy nodes sorted order policy driven metric latency available bandwidth proxy load defined node registrations bandwidth consumption 
overlay maps destination ip set destination proxy identifiers 
sender proxy caches information connection setup 
naive solution assumes overlay returns error message resent sender side proxy entry destination identifier list 
solution requires buffering sender proxy incurs roundtrip delay failure 
alternate solution embeds backup proxy identifiers inside message 
message encounters failed hop replaces destination identifier list tries route proxy 
additional routing logic implemented top proposed common call interface structured overlays 
alternate solution raid striping send data stream multiple proxies 
source proxy send sequential packets proxies identifier list bit wise xor parity block th proxy 
missing packet reconstructed remaining packets 
design provides fast transparent recovery single proxy failures cost additional proportional bandwidth 

challenges deployment consider issues arise deploying fault resilient overlays internet 
overlay nodes function application level traffic routers require low latency high bandwidth connectivity network 
expect internet service providers isps deploy overlays internal networks offer resilient traffic tunneling value added service customers 
isp chooses number location traffic overlay nodes 
adding overlay nodes interior increases number backup paths overlay resiliency placing nodes closer customers reduces likelihood failures client overlay 
issue deployed overlay limited reach isp network 
connections cross isp boundaries require cross domain solution 
possibility rdp min median th percentile internode round trip ping time ms buckets median th percentile stress savings beacon period network size overlay nodes savings hopcount savings latency 
tapestry performance 
relative delay penalty rdp packets routing tapestry network planetlab network testbed 
tapestry provides relatively low overhead compared ip wide area 

maintenance advantage proximity simulation 
proximity reduces relative bandwidth consumption tbc beacons randomized prefix routing schemes 
smaller isps merge overlays larger isps allowing fully share namespace share routing traffic 
bit namespace ensures probability namespace collisions remain statistically insignificant 
second solution set defined peering points isp overlay widearea routing similar proposed interdomain overlay 
peering points form overlay advertise local addresses objects secondary overlay 
resulting hierarchy properties similar bgp 
comparisons scope 

evaluation adaptive mechanisms explore potential adaptive fault tolerance described previous sections simulation results measurements functioning tapestry system 
tapestry comprises lines java written event driven style top seda fast non blocking version tapestry includes mechanisms section including components beacon fault detection primary backup paths reachable link selection constrained multicast duplicate packet detection :10.1.1.130.8002
section take depth look impact adaptive mechanisms proposed 
starting baseline tapestry examine impact proposed routing policies simulation experimental measurements 
test tapestry ability exploit underlying network redundancy explore tradeoff beacon rate responsiveness failures 
tapestry deployed planetlab testbed network machines sites tapestry implementation available public download oceanstore cs berkeley edu :10.1.1.109.4681
worldwide 
gain additional nodes invoke multiple virtual nodes physical node 
illustrates basic routing performance tapestry implementation measuring increase latency packet experiences tunneling overlay 
latency overhead low particularly wide area routes 
high variability short routes due combination virtual nodes competing highly loaded cpus queuing delays event handling layer 

analysis simulation evaluate potential adaptation start examining microbenchmarks simulation 
implemented network simulator stanford graph base libraries 
measurements utilize different node transit stub topologies specified construct tapestry overlay networks size nodes measure results 
proximity routing tbc mentioned previously tapestry provides proximity routing 
start illustrating advantage proximity structures reducing total bandwidth consumption tbc monitoring beacons perform comparison construct overlays different sizes proximity proximity means construct random topologies adhere basic prefix routing scheme utilize network proximity construction 
tbc saving proximity enabled overlay plotted 
see maintenance traffic proximity routing provides significant reduction resources 
section explore absolute amount maintenance traffic 
recall section tbc computed multiplying beacon bit rate distance number ip hops latency 
proportional latency overhead backup path latency source destination ms hop hop hop hop hop overlay hops converge length overlay path ip hops hop hop hop hop 
latency cost backup paths simulation 
show proportional increase routing latency tapestry routes single failure 

convergence rate simulation 
number overlay hops taken duplicated messages constrained multicast converge function path length 
proportional increase bandwidth latency source destination ms hop hop hop hop hop percentage pair wise routes percentage broken links working route route exists ip fail succeeds ip fails ip succeeds fails ip succeed 
bandwidth overhead constrained multicast simulation proportional increase bandwidth consumed single constrained multicast 

routing failures 
simulation routing behavior tapestry overlay backup routes normal ip transit stub network overlay nodes nodes randomly placed link failures 
overhead ensure resilient routing policies impose unreasonable overhead tunneled traffic simulate impact latency bandwidth consumption simulation 
measure increase latency incur route failure 
expect locally suboptimal backup routes increasing routing latency 
shows proportional increase routing latency routing single failure 
see backup routes taken closer destination rd th hop overlay path overhead incurred higher 
latency cost generally small path latency 
constrained multicast path convergence continue quantifying expected bandwidth cost constrained multicast assuming protocol path convergence 
verify tapestry routing provides path convergence 
path convergence allows limit amount bandwidth consumed duplicate messages constrained multicast 
shows duplicate messages generally converge original hop minimizing additional bandwidth 
measures additional bandwidth consumed duplicated packets assuming dropped converge paths originals 
additional bandwidth plotted ratio bandwidth consumed 
failures closer destination costly bandwidth overhead generally low 
reachability simulation experiment simulate impact random link failures structured overlay wide area 
construct node tapestry topology node transit stub network base digits prefix routing backups routing entry 
monitor connectivity pair wise paths overlay nodes incrementally inject randomly placed link errors network 
new set failures introduced measure resulting connectivity paths routed ip estimated shortest path tapestry 
assume time frame seconds tolerable delay buffered multimedia applications insufficient time ip level route convergence 
plot results probability graph showing proportion paths succeed fail protocol 
results show tapestry routing performs ideally succeeding large estimated link quality heartbeats measurement periods failure loss loss loss fail fail fail switch time ms min median th percentile probing period ms 
hysteresis tradeoff 
simulation adaptivity function incorporate hysteresis fault estimation periodic beacons 
curves show response time link failure loss event causing loss 

route switch time vs probing frequency 
measured time failure recovery plotted probing frequency 
experiment hysteresis factors shown 
majority paths connectivity maintained failures 
cases tapestry routing fails find existing path rare 

microbenchmarks deployed system microbenchmarks illustrate properties tapestry implementation deployed planetlab 
probe tapestry adaptation behavior implemented fault injection layer allows centralized controller inject network faults running nodes 
nodes instructed drop incoming network traffic message type data control message source report results 
general median values error bars representing th percentile minimum values 
th percentile values remove outlier factors garbage collection virtualization scheduling problems 
failover time microbenchmark measures correlation fail time length beacon period 
start selecting appropriate hysteresis factor link quality estimation equation 
illustrates quickly estimated values converge actual link quality different values link loss rate 
see value provides reasonable compromise response rate noise tolerance 
experiment deploy small overlay nodes including nodes berkeley washington san diego mit 
nodeids assigned traffic berkeley routes mit washington ucsd backup 
round trip distance failing link berkeley washington approximately ms 
hysteresis factors inject faults random intervals measure elapsed time detect redirect traffic fault 
plots min median th percentile values bea overhead min median th percentile position detour overlay path 
overhead fault tolerant routing 
increase latency incurred packet takes backup path 
data separated overlay hop encounter detour 
pairwise overlay paths taken planetlab nodes maximum hop count 
con period 
expected switch time scales linearly probing period small constant 
reasonable beaconing period ms response times values ms ms acceptable limits interactive applications including streaming multimedia 
messages lost traffic redirected 
redirection penalty quantify latency cost redirecting traffic backup path deploy tapestry network nodes digits base planetlab network 
probe pair wise paths select random sample source destination pairs sufficiently distinct ids require overlay hops 
path measure change latency resulting single backup path plotted hop backup path taken 
results shown 
results mirror results shown confirm single backup path little impact routing latency 
fact small number nodes planetlab overlay alternate path shorten number hops re relative overhead min median th percentile position detour overlay path bandwidth node kbytes network size nodes beacon period ms beacon period ms 
overhead constrained multicast 
total bandwidth penalty sending duplicate message loss detected hop plotted fractional increase normal routing 
data separated overlay hop encounters split 

cost monitoring 
show bandwidth fault detection function overlay network size 
individual curves represent different monitoring periods bandwidth measured kilobytes second node 
duce latency 
example nodes duke georgia tech mit route duke mit point georgia tech optimal hop keep mit backup 
failure occurs primary route message backup path route directly mit improving latency 
explains low minimum values 
constrained multicast penalty characterize cost constrained multicast start deployed network nodes planetlab 
select group paths overlay hops plot bandwidth overhead function hop duplicate message sent 
knowledge ip level routers planetlab approximate tbc metric bandwidth latency product 
shows deployed prototype performs expected duplicate messages incurring bandwidth consumption 
reports proportional increase total bandwidth consumption tbc original path compared simulation results 
beaconing overhead quantify periodic beaconing overhead tapestry implementation measuring total bandwidth beacon messages plotting size overlay network 
shows result kilobytes second sent node overlay beacon period ms 
routing entry backups ms 
bandwidth low scales logarithmically network size 
furthermore measurements consistent bandwidth estimates section 
note shows moderate large overlays respond link failures ms keeping beaconing traffic low 
importance self repair low rate transient failures handled techniques section long term stability depends crucially mechanisms repair redundancy restore locality 
continuous precomputation path discovery path redundancy slowly degrade backup routes fail time 
case tapestry means entries routing table refreshed rate keeps ahead network failures changes 
illustrate point deploy tapestry nodes lan cluster subject abrupt continuous change experiment focuses fault resilience latency impose network topology packet delay system 
measure overlay connectivity measuring rate success routing requests random pairs ids namespace 
result shown figures 
figures illustrate experiment initializes network nodes introduces massive failure event minutes manually killing kill nodes 
minutes add nodes network parallel 
minutes nodes overlay participate random churn test seconds enter leave network randomized stochastic process mean duration minutes network 
plot number nodes system average query latency success rate routing requests 
repair shows routing success rate quickly degrades massive fail event recovers 
furthermore nodes churn test slowly lose redundant paths neighbors leave network leading steady decline route connectivity 
contrast shows tapestry self repair quickly recovers massive failure join events restore routing success 
constant churn algorithms repair routes fast maintain high routing availability 
successful route requests latency ms nodes time minutes constant churn nodes massive join massive fail startup success rate median lat successful route requests latency ms nodes time minutes constant churn nodes massive join massive fail startup success rate median lat 
pair wise routing repair 
success rate tapestry routing random pairs nodes mechanisms disabled massive failure massive join constant churn conditions 

pair wise routing self repair 
success rate tapestry routing random pairs nodes enabled massive failure massive join constant churn conditions 
note relatively low routing latency shown due fact inconsistent routes led portion massive join failing resulting shorter routes lower latency successful requests 

putting experiments analysis quantify benefits proposal verify 
simulations show exploits majority routing redundancy network measurements show prototype adapt failure milliseconds reasonable dampening factor 
furthermore bandwidth required support high adaptivity low 
show addition circumventing link failures network overlay self repairs node failures order maintain high availability 

related structured peer peer overlays provide scalable routing messages objects endpoints 
design protocols led design scalable wide area applications development new protocols specialized properties :10.1.1.159.9358:10.1.1.140.3129:10.1.1.28.5987:10.1.1.118.8846:10.1.1.17.4065:10.1.1.12.1959:10.1.1.105.3673:10.1.1.101.3227
requires key routing api implemented number protocols 
resilient overlay networks allow application traffic tunneling small number overlay nodes demonstrates feasibility :10.1.1.161.2760
pair wise communication results messages limits scale tens nodes 
internet indirection infrastructure uses triggers embedded infrastructure redirect application traffic mobility resilience similar approach handling legacy applications 
redirection points trigger placement done manually 
secure overlay services sos places protected servers inside overlay domain filters tunneled application traffic edge nodes prevent denial service attacks 
detour project proposes network deployed routers direct tunneled traffic study efficiency loss rates internet routing paths :10.1.1.105.3673
detour focuses routers gather network information benefit transport protocol provide scalable methods detecting faults managing backup paths 
previous research quantified loss network connectivity wide area network 
bharat performed quantitative analysis service availability wan developed failure model parameterized failure location failure duration 
focused understanding cause consequences bgp failures 
labovitz examined latencies internet path failure fail repair resulting convergence properties bgp routing algorithms 
mahajan quantified occurence bgp misconfigurations measurement isp surveys 

show structured peer peer overlay networks provide efficient responsive fault resilient routing legacy applications 
overlays enable precomputation backup paths efficient beacons fault detection 
illustrate ideas prototype tapestry overlay network designs directly applicable protocols 
routing hop tapestry routing chooses optimal near optimal local paths soft state beacons continuously probe network detect failures maintain path redundancy 
showed simple routing policies achieve near optimal fault resilience incurring low overhead terms latency bandwidth relative fault free network 
moderate sized overlay respond link failures milliseconds kilobytes second beaconing traffic agile support streaming multimedia applications 
techniques show great promise addressing reliability responsiveness needed today global scale network applications 

chen chuah helpful insights dennis geels sean rhea allowing measurement suite 
want anonymous reviewers ta feedback suggestions 
andersen balakrishnan kaashoek morris :10.1.1.161.2760
resilient overlay networks 
proc 
sosp 
acm oct 
bu gao towsley 
routing table growth 
proc 
global internet symposium 
ieee 
chandra dahlin gao 
wan service availability 
proc 
usits 
usenix mar 
cox murray noble 
pastiche making backup cheap easy 
proc 
osdi 
acm dec 
dabek kaashoek karger morris stoica 
wide area cooperative storage cfs 
proc 
sosp 
acm oct 
dabek zhao druschel kubiatowicz stoica 
common api structured overlays 
proc 
iptps berkeley ca feb 
feamster andersen balakrishnan kaashoek 
measuring effects internet path faults reactive routing 
proc 
sigmetrics 
acm june 
gummadi gummadi gribble ratnasamy schenker stoica 
impact dht routing geometry resilience proximity 
proc 
sigcomm 
acm sep 
harvey jones saroiu theimer wolman :10.1.1.118.8846
skipnet scalable overlay network practical locality properties 
proc 
usits mar 
hildrum kubiatowicz rao zhao :10.1.1.1.4310
distributed object location dynamic network 
proc 
spaa 
acm aug 
iannaccone 
chuah bhattacharyya diot 
analysis link failures ip backbone 
proc 
internet measurement workshop marseille france nov 
acm 
kaashoek karger 
koorde simple hash table 
proc 
iptps feb 
handley 
congestion control high bandwidth delay product networks 
proceedings sigcomm pittsburgh pa august 
keromytis misra rubenstein 
sos secure overlay services 
proc 
sigcomm 
acm aug 
knuth 
stanford platform combinatorial computing 
acm press addison wesley new york 
labovitz ahuja jahanian 
delayed internet routing convergence 
proc 
sigcomm 
acm aug 
labovitz ahuja wattenhofer 
impact internet policy topology delayed routing convergence 
proc 
infocom 
ieee 
mahajan wetherall anderson 
understanding bgp misconfiguration 
proc 
sigcomm 
acm aug 
malkhi naor :10.1.1.16.4785
viceroy scalable dynamic emulation butterfly 
proc 
podc 
acm 
maymounkov mazieres :10.1.1.109.4681:10.1.1.16.4785
kademlia peer peer information system xor metric 
proc 
iptps mar 
peterson anderson culler roscoe :10.1.1.109.4681
blueprint introducing disruptive technology internet 
proc 
hotnets 
acm 
ratnasamy francis handley karp schenker 
scalable content addressable network 
proc 
sigcomm 
acm aug 
rekhter li 
architecture ip address allocation cidr 
ietf 
rfc www 
isi edu notes rfc txt 
rekhter li 
border gateway protocol bgp 
ieee micro jan 
ietf rfc 
rhea eaton geels weatherspoon zhao kubiatowicz :10.1.1.28.5987
pond oceanstore prototype 
proc 
fast san francisco apr 
usenix 
rowstron druschel :10.1.1.28.5987
pastry scalable distributed object location routing large scale peer peer systems 
proc 
middleware 
acm nov 
savage detour case informed internet routing transport :10.1.1.105.3673
ieee micro jan 
stoica morris karger kaashoek balakrishnan :10.1.1.105.3673
chord scalable peer peer lookup service internet applications 
proc 
sigcomm 
acm aug 
welsh culler brewer :10.1.1.130.8002
seda architecture conditioned scalable internet services 
proc 
sosp 
acm oct 
wieder naor 
simple fault tolerant distributed hash table 
proc 
iptps feb 
zhao duan huang joseph kubiatowicz 
landmark routing overlay networks 
proc 
iptps mar 
zhao huang kubiatowicz joseph 
exploiting routing redundancy wide area overlay 
technical report csd berkeley nov 
zhao huang rhea stribling joseph kubiatowicz 
tapestry global scale overlay rapid service deployment 
ieee sac january 

