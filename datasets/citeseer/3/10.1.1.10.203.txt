journal machine learning research submitted published xx dimensionality reduction sparse support vector machines bi bij rpi edu kristin bennett rpi edu mark embrechts rpi edu rpi edu song rpi edu department mathematical sciences department decision science engineering systems department chemistry rensselaer polytechnic institute troy ny usa editor isabelle guyon andr elisseeff describe methodology performing variable ranking selection support vector machines svms 
method constructs series sparse linear svms generate linear models generalize uses subset nonzero weighted variables linear models produce final nonlinear model 
method exploits fact linear svm kernels norm regularization inherently performs variable selection side effect minimizing capacity svm model 
distribution linear model weights provides mechanism ranking interpreting effects variables 
visualize magnitude variance weights variable 
illustrate effectiveness methodology synthetic data benchmark problems challenging regression problems drug design 
method dramatically reduce number variables outperforms svms trained attributes attributes selected correlation coefficients 
visualization resulting models useful understanding role underlying variables 
keywords variable selection dimensionality reduction support vector machines regression pattern search bootstrap aggregation model visualization 
variable selection refers problem selecting input variables predictive outcome 
appropriate variable selection enhance effectiveness domain interpretability inference model 
variable selection problems supervised unsupervised machine learning tasks including classification regression time series prediction clustering shall focus supervised regression tasks general methodology extended inference task formulated norm svm classification novelty detection campbell bennett bennett bredensteiner 
objective variable selection fold improv bi kristin bennett mark embrechts song 
bi bennett embrechts song ing prediction performance kittler enhancing understanding underlying concepts induction model 
variable selection methodology svms created address challenging problems quantitative structural activity relationships analysis 
goal analysis predict molecules 
molecule potential descriptors may highly correlated irrelevant target 
known molecules 
issues model validation challenging overfitting easy 
results svms somewhat unstable small changes training validation data model parameters may produce different sets nonzero weighted attributes 
variable selection ranking methodology exploits instability 
computational costs primary issue experiments due lack data 
method sparse svms call algorithm vs ssvm variable selection sparse svms 
variable selection search problem state search space specifying subset possible attributes task 
exhaustive evaluation variable subsets usually intractable 
genetic algorithms population learning related bayesian methods commonly search engines variable selection process yang honavar kudo 
particularly svms variable selection method introduced weston finding variables minimize bounds leave error classification 
search variable subsets efficiently performed gradient descent algorithm 
method limited separable classification problems directly applicable regression problems examined 
guyon proposed variable selection method classification recursively eliminating input variables decrease margin guyon 
generic wrapper approach sensitivity analysis applied kernel svm regression svr embrechts computationally intensive proposed approach 
variable selection methods divided lines filter wrapper methods kohavi john 
filter approach selecting variables serves preprocessing step induction 
main disadvantage filter approach totally ignores effects selected variable subset performance induction algorithm 
wrapper method searches space variable subsets estimated accuracy induction algorithm measure goodness particular variable subset 
variable selection wrapped particular induction algorithm 
methods encountered success induction tasks computationally expensive tasks large number variables 
approach vs ssvm consists largely consecutive parts variable selection nonlinear induction 
selection variables serves preprocessing step final kernel svr induction 
variable selection performed wrapping linear svms kernels sparse norm regularization 
sparse linear svms constructed identify variable subsets assess relevance computationally cheaper way compared direct wrap nonlinear svms 
variable selection linear svms final nonlinear svm inference tightly coupled employ loss function 
method similar spirit dimensionality reduction sparse support vector machines absolute shrinkage selection operator lasso method tibshirani specifically targeted svr insensitive loss function 
article organized follows 
section review sparse svms norm regularization specifically sparse svr 
section provides details vs ssvm algorithm sparse linear svms 
sections compare vs ssvm stepwise dimensionality reduction correlation coefficient ranking methods synthetic data boston housing data 
model visualization explored section reveal domain insights 
computational results real life data included section 
sparse support vector machines section investigate sparse svms 
consider regression problem finding function rn minimizes regularized risk functional boser vapnik smola yi xi loss function :10.1.1.11.2062:10.1.1.11.2062
usually insensitive loss max svr 
regularization operator called regularization parameter 
linear functions regularization operator classic svms squared norm normal vector nonlinear functions produced mapping feature space kernel function constructing linear functions feature space 
linear function feature space corresponds nonlinear function original input space 
optimal solution svms expressed support vector expansion xi 
regression function equivalently expressed kernel expansion xi ik xi classic svms quadratic programs terms 
solving typically computationally expensive solving linear programs lps 
svms transformed lps bennett breiman smola 

achieved regularizing sparse norm norm 
technique basis pursuit chen parsimonious norm approximation bradley lasso tibshirani :10.1.1.135.1907
choosing function classic svr directly apply norm coefficient vector kernel expansion regularized risk functional specified yi xi :10.1.1.11.2062
referred sparse svm optimal solution usually constructed fewer training examples xi classic svms function requires fewer kernel entries xi 
classic svr approach hyper parameters 
tube parameter difficult select know accurately function fit 
svr sch lkopf smola developed automatically adjust tube size parameter 
parameter provides upper bound fraction error examples lower bound fraction support vectors 
form svr lp rewrite uj vj uj vj 
solution uj vj equal depending sign uj vj 
bi bennett embrechts song training data xi yi xi th component xi denoted xij lp formulated variables min uj vj yi uj vj xi xj 
uj vj xi xj yi 
uj vj 

lp provides basis variable selection modeling methods 
select variables effectively employ sparse linear svr formulated lp simply replacing xi xj xij index running examples index running variables 
optimal solution construct final nonlinear model sparse nonlinear svr lp nonlinear kernel rbf kernel exp 
vs ssvm algorithm optimal solution briefly describe vs ssvm algorithm section 
vs ssvm algorithm consists essential components 
linear model sparse constructed solving linear svm lp obtain subset variables nonzero weighted linear model 
efficient search optimal hyper parameters linear svm lp pattern search 
bagging reduce variability variable selection 
method discarding significant variables comparing random variables 
nonlinear regression model created training bagging lps rbf kernels final subset variables selected 
shall explain various components detail section pseudocode steps algorithm appendix 
algorithm appendix describes final nonlinear regression modeling algorithm 
section describe filtering variables achieved visualizing bagged solutions applying rules bagged models 
sparse linear models sparse linear models constructed lp min uj vj yi uj vj xij 
uj vj xij yi 
uj vj 

solution linear svr lp 
magnitude sign component wj indicates effect th variable model 
wj 
details available website www rpi edu bij html dimensionality reduction sparse support vector machines variable contributes wj variable reduces norm inherently enforces sparseness solution 
roughly speaking vectors coordinate axes larger respect norm respect norms 
example consider vectors 
norm norm 
degree sparsity solution depends regularization parameter tube parameter lp 
pattern search hyper parameters play crucial role variable selection approach optimize pattern search approach 
optimization automatically performed validation set results applying derivative free pattern search method dennis torczon search space 
choice lp generates linear model training data 
resulting model applied validation data evaluated statistic yi yi yi mean squared error scaled variance response yi prediction yi th validation example mean actual responses 
pattern search method optimizes validation space 
range hyper parameters may problem specific prefer generic approach applicable datasets 
reasonably large range adopted produce space specifically 
pattern search algorithm embedded algorithm sub routine 
iteration pattern search algorithm starts center initially randomly chosen samples points center search space calculates objective values neighboring point finds point objective value center 
algorithm moves center new minimizer 
points center fail bring decrease objective search step determine neighboring points reduced half 
search continues search step gets sufficiently small ensuring convergence local minimizer 
full explanation pattern search svr see bennett 
variability reduction optimal weight vectors lp exhibit considerable variance due local minima pattern search small dataset size changes validation data 
different partitions data may produce different answers 
individual model considered completely reliable especially data 
bootstrap aggregation bagging procedure stable breiman :10.1.1.32.9399:10.1.1.32.9399
experiments models constructed random partitions produce distinct weight vectors 
schemes combine models 
took superset nonzero weighted variables obtained different partitions bagged subset variables 
bagging augment performance various individual models due reduced variance bagged model breiman 
problems large variance regression average usually outperforms single model 
bagging variable selection nonlinear svr modeling 
discarding significant variables sparsity linear model eliminates variables bootstrap possible bootstrap irrelevant variables included 
eliminate variables introducing random gauge variables 
intuition independent variable significance random variable barely related response may safely deleted 
vs ssvm bi bennett embrechts song algorithm augments data normally distributed random variables mean standard deviation 
random variables distributions employed 
see dreyfus thorough discussion issue 
previous empirical results embrechts 
added random variables sample correlations response magnitude 
weights random variables provide clues thresholding selection variables average weights variables bootstraps 
variables average weight greater average selected 
selection variables refined 
leave explanation scheme model visualization filtering variables section 
nonlinear svr models final set variables selected employ algorithm appendix construct nonlinear svr model 
nonlinear models constructed partitions averaged produce final model 
focus evaluating performance variable selection method optimizing predictor 
simple grid search nonlinear svr modeling select hyper parameters pattern search fold bagging 
grid search considered rbf kernels parameter equal 
parameter chosen values increment increment parameter 
computational analysis vs ssvm evaluated computational effectiveness vs ssvm synthetic data benchmark boston housing problem harrison rubinfeld 
goal examine vs ssvm improve generalization 
lps formulated training data solved cplex version ilog 
compared vs ssvm widely method stepwise regression wrapped generalized linear models glm miller 
fair comparison glm models bagged 
different glm models generated splus ripley mccullagh nelder different bootstrapped samples resulting models bagged 
bagged models performed single models bagged results 
trial half examples held test half training 
vs ssvm stepwise regression run training data 
training data divided create validation set fold bagging scheme 
final models applied hold test data order compute test procedure repeated times different training test splits data methods 
synthetic data set randomly generated solution pre specified follows independent variables response variable 
variables 
drawn independently identically distributed standard normal distribution 
th variable correlated 
th variable relates 
additional standard normally distributed variables generated dependent call dimensionality reduction sparse support vector machines table comparison selected variable subsets vs ssvm left stepwise right trials 
variables selected trials methods 
nv nv means noise variables 
times variable subsets nv nv nv nv nv nv nv nv nv nv nv nv nv times variable subsets nv nv nv nv nv nv nv nv nv nv nv nv nv nv nv nv nv nv nv nv nv nv nv nv nv nv nv nv nv nv nv nv nv nv noise variables nv nv 
calculated ex generated examples 
table left shows vs ssvm consistently selected desired variables discarded irrelevant attributes nvi 
randomly generated attributes nvi correlated response chance eliminated 
thresholding maximum weight average weight random variables added algorithm eliminate nv may remove relevant variables 
vs ssvm selected exactly fold represent function superset folds brought variables selected set 
fortunately highly correlated variables directly filtered section 
nonlinear svr better variables correlated variables beneficial noise 
results stepwise regression summarized table right 
splus glm modeling constructed gaussian family 
experimented quadratic form glm constructed function nuisance variables nv glm restricted linear models 
note appeared model stepwise regression handle linearly correlated variables better vs ssvm 
pick nonlinearly interrelated variable irrelevant variables nv missed desired variable times vs ssvm 
stepwise regression method computationally expensive problems predicted response bi bennett embrechts song scatterplot data vs ssvm synthetic observed response std predicted response scatterplot data glm synthetic observed response std synthetic data left vs ssvm result right stepwise regression result 
predicted response scatterplot data vs ssvm boston observed response std predicted response scatterplot data glm boston observed response std boston housing data left vs ssvm result right stepwise regression result 
tributes 
splus glm modeling applied problems section problems fewer sample points variables 
vs ssvm generalized better stepwise regression 
depicts observed versus predicted responses test examples trials synthetic data vs ssvm glm 
point represented vertical bar middle point representing mean length bar drawn standard deviation predictions point 
vs ssvm obtains test stepwise glm achieves 
recall proportional mean squared error smaller values better 
squared correlation coefficient observed predicted responses provided figures larger better 
summarizes results boston housing data variables selected vs ssvm stepwise regression trials 
dimensionality reduction sparse support vector machines left synthetic data right boston housing data 
dotted solid boxes contain non positive non negative weighted variables 

model visualization model visualization interpret results vs ssvm 
recall single trial vs ssvm produces different linear models different partitions bootstraps data 
final variable subset aggregate models 
examining distribution weights variable different bootstrap models yield valuable insights relationship independent response variables 
visualization techniques histograms enhance understanding role variables fayyad 
constructed variable 
consists sequence spokes 
spoke represents variable weight different bootstrap model 
bootstraps spokes 
spokes ordered corresponding bootstrap 
bootstrap normalized component wj 
line connecting spokes gives name 
relative size shape allow quickly assess relationships simple mean variance bootstraps 
left representative synthetic data 
dashed box represent variables negative weights solid box represent variables positive weights 
remaining variables weights flip signs 
stars ordered average weight variable different bootstraps 
variable positively linearly related response second reflects truth ex fitting linear functions vs ssvm may detect nonlinearly related variables correctly 
experiments detect nonlinear variables 
note part true model sign negative showing linear model focuses correcting negative values 
nv clear relation response bi bennett embrechts song weights nv flip signs different models 
strategy removing variables flipping signs refine variable selection 
variables flipping signs coincide variables correlated response 
synthetic data correlation coefficients variables response nv removing variables magnitude delete 
size shape provide information models 
variable complementary means bootstrap model selects include 
help spot complementary behavior models 
highly correlated variables filtered 
vs ssvm model visualization valuable datasets variable selection eliminate variables 
example boston housing data vs ssvm drop variables 
weights produced vs ssvm help understand role relative importance variables model 
entire boston housing data right 
drawn way synthetic data 
instance rm average number rooms dwelling positively related housing price reflects number rooms important determining housing price rooms house higher price 
indus proportion non retail business acres town appears affect housing price significantly linear modeling corresponding weights flip signs 

generalization testing datasets vs ssvm tested challenging real life data 
data created ongoing nsf funded drug design semi supervised learning project see www com 
leave cross validation performed data 
described section variables selected separately point 
exactly group examples experiments variable selection 
table summary data reduced data 
original 
st vs nd vs dataset obs 
vars 
vars 
vars vars blood brain barrier cancer hiv dimensionality reduction sparse support vector machines table experimental results full datasets reduced datasets obtained vs ssvm 
st vs ssvm means run vs ssvm full data 
full data st vs ssvm reduced data dataset std std blood brain barrier cancer hiv table summarizes datasets 
variables range greater standard deviations removed common practice commercial analytical tools chemometrics 
primitive form variable filtering rarely hurts usually improves results 
resulting numbers variables rd column table 
vs ssvm greatly reduced number attributes shown columns table improving generalization see table 
column table gives results obtained iteratively applying vs ssvm original reduced data variables weights flip signs 
table comparison vs ssvm iteratively eliminating flipped variables correlation coefficient ranking 
nd vs ssvm corr 
coef 
rank dataset std std blood brain barrier cancer hiv table gives comparison results attributes versus selected iteration vs ssvm 
compared vs ssvm correlation coefficient ranking method 
ranking method chose variables correlated response 
number chosen number variables selected vs ssvm 
variables selected algorithm construct final nonlinear model 
table presents results vs ssvm iteratively removing variables flipping signs versus results correlation coefficient ranking 
squared correlation bi bennett embrechts song table experimental results paired test 
dataset mean mean statistic value blood brain barrier cancer hiv coefficient actual predicted responses best leave best reported 
standard deviation std computed standard deviation squared errors test data scaled variance actual response 
cross referencing table table shows ranking scheme correlation coefficients failed improve generalization performance 
significance differences assessed paired test 
calculated mean error performed paired test squared errors results shown table 
absolute errors modeling full data variable reduction denoted respectively 
tables conclude vs ssvm effective reducing dimensionality problems 
phase vs ssvm significantly reduced number variables reductions achieved iteratively feeding data vs ssvm removing variables flipping signs linear models 
vs ssvm produced significantly better problems table worse generalization accuracy dramatically fewer variables 
significant improvements obtained cancer hiv data 
plots actual versus predicted responses cancer data illustrate improved generalization obtained vs ssvm 

discussion key components variable selection approach exploiting inherent selection variables done sparse linear svm lp second aggregation sparse models overcome unreliability single model third visualization analysis bagged models discover trends 
research investigated sparse svm regression algorithm sparse modeling process serve similar function 
focused model visualization visualization methods applied may informative 
instance parallel coordinate plots variable weights versus bootstraps valuable 
vs ssvm proved effective problems drug design 
number variables dramatically reduced maintaining improving generalization ability 
method outperforms svms trained attributes attributes selected correlation ranking 
chemists model visualization useful guiding modeling process interpreting effects descriptors models song 
model visualization discovered simple rule dimensionality reduction sparse support vector machines predicted response scatterplot data attributes observed response std predicted response scatterplot data vs ssvm cancer observed response std left cancer full data right cancer variable selection 
eliminating variables weights flip signs distinct individual models 
automating rule proved valuable heuristic refining variable selection 
vs ssvm general methodology suitable types problems 
demonstrated effectiveness high dimensional problems little data 
problems linear models adequately capture relationships method fail 
open research areas include theoretical underpinning approach characterization domains effective extension nonlinear interactions 
supported nsf number iis 
bennett 
combining support vector mathematical programming methods classification 
sch lkopf burges smola editors advances kernel methods support vector machines pages cambridge ma 
mit press 
bennett bredensteiner 
geometry learning 
hart meyer phillips editors geometry washington 
mathematical association america 
www rpi edu geometry ps 
boser guyon vapnik 
training algorithm optimal margin classifiers 
haussler editor proceedings th annual acm workshop computational learning theory pages pittsburgh pa july 
acm press 
bradley mangasarian rosen 
parsimonious norm approximation 
computational optimization applications 
breiman 
bagging predictors 
machine learning 
breiman 
prediction games arcing algorithms 
neural computation 
bi bennett embrechts song bennett embrechts cramer song bi 
descriptor generation selection model building quantitative structure property analysis 
editor experimental design combinatorial high throughput materials development 
wiley 
campbell bennett 
linear programming approach novelty detection 
neural information processing systems volume pages 
chen donoho saunders 
atomic decomposition basis pursuit 
technical report department statistics stanford university may 
dennis torczon 
derivative free pattern search methods multidisciplinary design problems 
fifth aiaa usaf nasa issmo symposium multidisciplinary analysis optimization pages institute aeronautics astronautics reston virginia 
embrechts bennett 
bagging neural network sensitivity analysis feature reduction problems 
proceedings inns ieee international joint conference neural networks volume pages washington 
ieee press 
fayyad grinstein 
information visualization data mining knowledge discovery 
morgan kaufmann 
guyon weston vapnik 
gene selection cancer classification support vector machines 
machine learning 
harrison rubinfeld 
hedonic prices demand clean air 
journal environ 
economics management 
ilog 
ilog cplex manual 
ilog cplex division incline village nevada 
sierra 
feature subset selection population incremental learning 
technical report 
ehu ik university basque country spain 
kittler 
feature selection extraction 
young 
fu editors handbook pattern recognition image processing 
academic press new york 
kohavi john 
wrappers feature subset selection 
artificial intelligence 
kudo sklansky 
comparison classifier specific feature selection algorithms 
spr pages 
mccullagh nelder 
generalized linear models 
chapman hall london 
miller 
subset selection regression 
monographs statistics applied probability 
london chapman hall 
dimensionality reduction sparse support vector machines bennett 
pattern search method model selection support vector regression 
proceedings siam international conference data mining philadelphia pennsylvania 
siam 
sch lkopf smola williamson bartlett 
new support vector algorithms 
neural computation 
smola 
learning kernels 
phd thesis technische universit berlin 
smola sch lkopf tsch 
linear programs automatic accuracy control regression 
proceedings icann int 
conf 
artificial neural networks berlin 
springer 
song bi bennett cramer 
prediction protein retention times exchange chromatography systems support vector machines 
journal chemical information computer science 
dreyfus 
ranking random feature variable feature selection 
journal machine learning research special issue variable feature selection 
appear issue 
tibshirani 
regression selection shrinkage lasso 
technical report statistics department stanford ca june 
vapnik 
nature statistical learning theory 
springer new york 
ripley 
modern applied statistics plus 
springer new york 
weston mukherjee chapelle pontil poggio vapnik 
feature selection svms 
neural information processing systems volume pages 
yang honavar 
feature subset selection genetic algorithm 
koza deb dorigo fogel garzon iba riolo editors genetic programming proceedings second annual conference page stanford university ca usa 
morgan kaufmann 
appendix bi bennett embrechts song algorithm variable selection algorithm 
arguments sample data return variable subset function vs ssvm add random variables rv new descriptors rv repeat randomly partition data training validation xv yv sets perform model selection parameters pattern search solve lp obtain linear model weight vector maximum number iterations combine weight vectors obtain combine 
set threshold average wn weights rv return variable subset consisting variables greater threshold algorithm induction algorithm 
arguments sample rm rm return svm regression model function ssvm repeat randomly partition data training validation xv yv sets perform model selection parameters solve lp best obtain nonlinear model maximum number bootstraps bag models return svm regression model 
