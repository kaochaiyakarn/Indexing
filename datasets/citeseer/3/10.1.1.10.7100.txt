behaviour reinforcement learning george dimitri master science artificial intelligence school informatics university edinburgh behaviour robotics successfully develop autonomous mobile robots certain point progress may require integration learning model behaviour framework 
reinforcement learning natural candidate suited problems faced autonomous agents 
previous attempts reinforcement learning behaviour mobile robots simple combinations methodologies full integrations suffered severe scaling problems appear infeasible 
furthermore implicit assumptions form basis reinforcement learning theory developed problems faced autonomous agents complex environments mind 
dissertation introduces model reinforcement learning designed specifically behaviour robots conditions faced situated agents account 
model layers distributed asynchronous reinforcement learning algorithm learned topological map standard behavioural substrate create reinforcement learning complex 
topological map creates small task relevant state space aims reinforce ment learning feasible distributed asynchronous nature model compatible behaviour design principles 
model validated experiment requires mobile robot perform puck foraging separate artificial arenas 
development dangerous beans mobile robot capable building distributed topological map environment performing reinforcement learning described results test control strategies random decision making standard reinforcement learning algorithm layered top topological map full model developed dissertation arenas 
results show model developed dissertation able learn rapidly real environment outperforms random strategy layered standard reinforcement learning algorithm 
discussion implications results suggests situated learning integration behaviour methods layered learning models merit study 
hayes agreeing supervise clearly crazy 
exceptional patience insightful comments allowed turn fragmented ideas coherent thesis keeping scope project sane focused 
george chris malcolm john hallam provided invaluable discussion insight advice research interesting 
research possible equipment cooperation mobile robot group 
particular built tether saved awful lot trouble 
owe debt gratitude irene madison jane rankin douglas arranging stay forrest hill little robot lab moved saving bacon 
george primarily responsible improbable survival past year 
churchill house year memorable 
sarah lex holt forming nodes social network 
steve mclean excellent taste music may quite possibly kept going long hours shared robot lab 
msc edinburgh funded commonwealth scholarship ref 
administered british council am deeply grateful 
alison demonstrated superb skill getting things done quickly necessary 
am deeply indebted parents marina sister just 
ii declaration declare thesis composed contained explicitly stated text submitted degree professional qualification specified 
iii george dimitri th august th may deeply loved missed iv table contents 
behaviour reinforcement learning 
research approach 
structure dissertation 
background 
behaviour robotics 
reinforcement learning 
distributed learning models 
layered learning models 

behaviour reinforcement learning 
reinforcement learning topological maps 
distributed reinforcement learning 
temporal difference methods 
monte carlo methods 
td 
reinforcement learning situated agents 
examples 
summary 
experiment puck foraging artificial arena 
overview 
evaluation 
arena testing simple puck finding 
arena hostile environment 
arena far away puck 
summary 
distributed map learning artificial arena 
environment 
physical characteristics 
arena configurations 
robot 
hardware 
software interface 
behavioural substrate 
landmark detection 
map building 
vi summary 
distributed reinforcement learning artificial arena 
implementation 
obtaining reward 
circadian events 
making choices 
place transition values 
data capture 
modifications original map building system 
results 
arena 
second arena 
third arena 
convergence 
summary 

discussion 
significance 
limitations 
implications 
situated reinforcement learning 
planning behaviour 
vii layered learning 
emergent representations 


contribution 
significance 


bibliography viii list figures graphical tabular action value representations 
rewards received transition path td td 
potential actions wall robot 
experimental arena 
second experimental arena 
third experimental arena 
arenas 
dangerous beans 
dangerous beans behavioural structure map building 
dangerous beans sensory configuration 
front sensor obstacle avoidance thresholds 
uncorrected angle corrected dead reckoning maps 
angle corrected fully corrected dead reckoning maps 
distributed topological map arena 
dangerous beans behavioural structure reinforcement learning 
sample pixel array images 
average puck reward time arena 
ix learned random routes puck arena 
average home reward time arena 
learned random routes home arena 
preferred transitions maps arena 
average puck reward time second arena 
learned puck finding behaviour second arena 
preferred puck transitions maps second arena eighth cycle average home reward time second arena 
average puck reward time third arena 
learned puck routes third arena 
preferred puck transitions maps third arena 
average home reward time third arena 
atd average action value changes time 
chapter theory intelligence account wide spectrum learning mechanisms dis played insects animals humans 
aspects autonomous agent evolved directly engineered elements behaviour require learning involve knowledge gained agent may change ways lifetime 
behaviour robotics success basis development intelligent autonomous robots way learning fits behaviour framework understood 
reinforcement learning suited kinds problems faced current generation behaviour robots 
provides goal directed learning requiring external teacher handles environments deterministic rewards require multiple steps obtain developed theoretical framework sutton barto :10.1.1.32.7692
sutton argued problem facing autonomous agent reinforcement learning problem 
researchers included reinforcement learning robots 
involved reinforcement learning robot entire sensor space suffered scaling problems mahadevan connell involve real robots attempted retrofit existing architectures potential robotic applications sutton 
dissertation introduces model reinforcement learning designed specifically chapter 
behaviour robots motivated behaviour emphasis layered distributed control argues reinforcement learning situated agents creates different set concerns emphasised reinforcement learning literature 
outlines experiment aimed evaluating model details development dangerous beans mobile robot implements model presents results experiment 
results demonstrate model capable rapid learning resulting behavioural benefits real time real robot 
behaviour reinforcement learning dissertation fusion behaviour style robot control architecture reinforcement learning 
reinforcement learning usually considered isolation conceived single process operating private central data structure 
added robot control system usually reinforcement learning module contains standard implementation reinforcement learning single behavioural module 
sense behaviour central arguments put forward dissertation motivation model represents simple combination methods true integration 
truly behaviour reinforcement learning representation control sys tem control system function representation 
behaviour architec ture provide basis reinforcement learning layered just inserted 
similarly reinforcement learning isolated rest system integrated existing distributed control system 
model map naturally neural implementation 
model dissertation result attempt integration 
research approach research approach adopted dissertation synthetic modelling pfeifer scheier 
entails development model synthesis instantiation model empirical testing analysis 
approach taken involved design experiment model developed evaluated 
chapter 
experiment required construction artificial environment development mobile robot designed operate making model 
aim experiment twofold 
construction actual robot system aimed prove model feasible terms computational requirements design effort 
second allowed empirical evaluation benefits provided model qualitative evaluation resulting behaviour 
approach inherently limited evaluates individual instantiation model particular task provided solid data led positive evaluation model established basis research 
structure dissertation chapter briefly outlines behaviour robotics reinforcement learning covers related research fields 
examines previous layered learning mod els distributed representations attempts reasoning development reinforcement learning model developed dissertation clear 
chapter introduces new reinforcement learning model discusses concerns raised reinforcement learning situated agents provides examples model applied 
chapter presents experiment examples requires mobile robot artificial arena learn find food puck explore return home rein learning distributed map arena 
chapter details construction artificial arenas development dangerous beans robot capable forming distributed map building 
chapter describes addition reinforcement learning layer robot presents results experiment 
results show model chapter feasible capable learning real time 
chapter provides discussion significance limitations implications model experiment chapter concludes summarising contribution significance dissertation outlining potential 
chapter background behaviour robotics reinforcement learning developed fields rich bodies literature documenting wide range research 
chapter presents brief overview basic ideas literature related fields paying particular attention areas relevant model experiment dissertation covers combines fields 
aims show behaviour robotics reinforcement learning natural combination previous approach fully successful merging 
section provides overview behaviour robotics principles followed brief outline reinforcement learning theory key examples ap plication robotics 
overview distributed learning models literature research layered learning models covered 
final section concludes 
behaviour robotics behaviour robotics centred idea best way study intelligence development mobile robots brooks 
prior nearly artificial intelligence research consisted focused study small aspect behaviour chapter 
background idea decomposing intelligence easier understand parts 
brooks claimed decompositions may misleading typically introspection notoriously unreliable method psychological analysis 
furthermore researchers studying individual aspects intelligence may lose sight feasibility interface constraints adhering 
behaviour robotics emphasises construction complete functional agents exist real world 
agents exist interact complex environ ments real time known situated agents 
situated agents confront issues real time control complexity world directly behave real world real time 
consequences change emphasis development different set research concerns traditionally considered important artificial intelligence 
behaviour robotics emphasises distributed parallel primarily reactive control processes emergence complex behaviour interaction pro cesses environment cheap computation construction agents layered addition complete functional behavioural levels 
point im portant facilitates incremental construction mobile robots explicitly seeks mimic evolutionary development behavioural complexity 
behaviour ap proach capable developing agents demonstrate surprisingly complex behaviour braitenberg developed significant field research major textbooks arkin pfeifer scheier murphy 
behaviour robotics starting widely accepted methodology developing mobile robots able produce working robots variety interesting problems difficulty developing systems display level intelligence insects 
partly majority research focusses hy architectures combine ideas traditional artificial intelligence behaviour robotics bryson partly thorough investigation issues behaviour representations integration learning methods behaviour systems 
brooks argued traditional approach machine learning produced learning models applicable problems faced situated agents 
research dissertation aims develop just learning model 
chapter 
background reinforcement learning reinforcement learning problem problem learning maximise numerical reward signal time environment sutton barto :10.1.1.32.7692
reward signal feedback obtained environment reinforcement learning falls unsupervised learning signal supervised learning signal indicating correct action mitchell 
specifically set states set actions reinforcement learning involves learning values state value prediction problem value state action pair control problem sutton barto :10.1.1.32.7692:10.1.1.32.7692
tasks values estimated experience reward received state state action pair interaction environment 
estimate usually achieved building table contains element desired value reinforcement learning method estimate value element 
primary solution methods employed solve reinforcement learning problems dynamic programming monte carlo estimation temporal difference methods 
dynamic programming methods priori environmental model achieve exact solu tion reinforcement learning problem requiring interaction environ ment applicable cases model available 
monte carlo methods applicable episodic tasks estimate state state action pair total reward received occurrence episode episode termination 
temporal difference methods estimate value particular state state action pair current value reward received active case state taken case state action pair value state state action pair 
methods bootstrap calculate state state action pair values values states state action pairs sutton barto :10.1.1.32.7692
important temporal difference algorithm td combines advantages monte carlo methods temporal difference methods 
comprehensive treatment wide range reinforcement learning meth ods theoretical properties excellent overview field sutton barto 
states required markov states individual state contain sufficient information determine optimal action agent knowledge agent history 
episodic tasks tasks consist finite number state transitions words guaranteed 
chapter 
background reinforcement learning attractive researchers robotics provides principled way build value driven agents agents actions guided set internal drives pfeifer scheier 
furthermore sound theoretical basis allow principled integration priori knowledge handles stochastic environments rewards take multiple steps obtain intuitively appealing 
attractive properties researchers added reinforcement learning capabilities robots 
early example development mahadevan connell robot learned push boxes reinforcement 
able learn real time required hand discretised state space statistical clustering order robot sensor space eighteen bits 
straightforward application reinforcement learning robot applications invariably leads problems 
models typically robot sensor space directly rein learning state space suffer serious performance scaling problems robot just sixteen bits sensor space states 
convergence large state space take reinforcement learning algorithm long time 
lution problem simulators long training times acceptable 
agents considered situated 
problems led researchers develop hierarchical reinforcement learning methods aim learning tractable varying levels detail mor complex statistical methods speed learning smart kaelbling 
approach function approximation method approximate value table sutton barto :10.1.1.32.7692
introduces addi tional problem selecting approximation method task hand retain theoretical guarantees known apply tabular reinforcement methods 
fundamental problem reinforcement learning methods mobile robots developed problems faced situated agents mind 
matari gives important criticism direct application reinforcement learning behaviour robotics reflects idea implicit assumptions reinforcement learning literature need reexamined context situated agents 
discussion differences concerns emphasised reinforcement learning literature posed situated agents chapter 
clear discussion reinforcement learning methods chapter 
background directly robot sensor space feasible solution problem applying reinforcement learning situated agents 
distributed learning models behaviour emphasis distributed parallel control processes implies learning model included behaviour system distributed 
section discusses distributed learning models literature 
neural network learning models inherently distributed pfeifer scheier supervised learning algorithms back propagation mitchell appropriate autonomous agents 
classes neural learn ing mechanisms useful behaviour robots hebbian learning models self organising feature maps 
hebbian learning neurons active time excitatory connection established exist existing strengthened 
method completely distributed sense central control required whatsoever provided neuron determine active 
braitenberg uses learning method exclusively develop autonomous agents display quite complex behaviour 
useful variation hebbian learning value hebbian learning excitatory connection strengthened stimulus smell food pfeifer scheier 
aimed creating connections cases significant value system stimulus important hebbian methods primarily useful associative learning tasks 
self organising feature maps sets interconnected neurons organise selves match important features input space preserving topological prop erties 
best known sofm kohonen network kohonen consists fixed size network fixed connectivity adapts match structure inherent inputs dynamic types include growing neural gas fritzke creates networks dynamic size connectivity grow required marsland adds nodes accumulated error 
reinforcement learning considered form value hebbian learning excitatory connection nodes strengthened active time 
chapter 
background primarily useful automatically learning structure input space potentially considerable importance topological maps appear ubiquitous natural systems 
major behaviour systems built distributed learning models 
involves learning activation conditions set behaviours coordi nate produce emergent walking behaviour legged robot maes brooks 
results produced algorithm impressive highly task specific useful 
second instance distributed learning behaviour robot matari brooks particular relevance dissertation 
matari brooks detail development robot called toto capable wandering office environment learning distributed topological map inspired role place cells rat hippocampus 
map independent behaviours active attempted suppress robot near landmark corresponded 
landmark behaviour maintained list landmark behaviours previously followed spread expectation increasing sensitivity 
behaviours active parallel distributed map provided constant time localisation linear time path planning spreading expectation matari 
matari considered known instance emergent data structure dissertation considered extension line research appeared 
layered learning models layered learning methodology introduced stone intended deal problems learning direct mapping input output feasible hierarchical task decomposition 
method involves machine learning layers agent control system layer learning directly affecting subsequent layers 
learning model may affect provision training examples construction input output features stone 
training examples appropriate situated learning learning model situated agent example bias kinds learning opportunities model receives 
chapter 
background layered learning method generated impressive results simulated soccer playing robots developed twice won robocup robotic soccer championship stone 
despite obvious promise layered learning applied fully situated agent 
implementations simulation stone stone veloso stone training times longer acceptable physical robots 
furthermore original stipulation layer finish learning start stone realistic situated environments research allowed learn concurrently stone 
relevant application layered learning approach kohonen networks continuous input output spaces order suitable reinforce ment learning algorithms smith 
results smith promising algorithm hampered requirement kohonen map parame ters determined experimentally network fixed dimensionality 
problem potentially solved dynamic self organising networks grow required marsland problem implies learning model feasible task specific 
kohonen networks topological maps model smith respects similar dissertation 
intended situated agents take important interaction state action spaces account explained chapter reinforcement learning potential greatly broaden behavioural scope behaviour robots current implementations fully integrated behaviour methodology suffer serious problems terms learning speed scalability 
combining layered learning parallel control distributed learning methods described chapter chapter develops reinforcement learning model attempts fully integrate behaviour robotics reinforcement learning order produce real robots capable rapid learning 
chapter behaviour reinforcement learning chapter develops model reinforcement learning autonomous agents motivated behaviour emphasis distributed control layered behavioural competencies 
model aims broaden scope behaviour systems include tasks ability learn reinforcement feasible 
model novel reasons 
embeds reinforcement learning layer distributed topological map serves state space robot sensor space directly 
second central control process single action value state value table reinforcement learning functionality distributed requiring central control spreading reinforcement table distributed map 
effect creates reinforcement learning complex emergent sense contained individual processes nodes results interaction 
distributed nature map model asynchronous updates temporal difference updates node map take place parallel time state transition state takes place 
allows convergence near convergence reinforcement complex situated agents time taken update small relative time required perform chapter 
behaviour reinforcement learning actions single transition map 
model developed follows 
section introduces idea layering reinforce ment learning topological map followed explanation reinforcement learning performed distributed fashion embedded distributed topological map 
discussion ways reinforcement learning situated agents creates different set concerns standard reinforcement learning approaches introduces asynchronous temporal difference atd learning 
followed examples model applied difficult learning problems 
final section summarises 
reinforcement learning topological maps reinforcement learning models mobile robots typically robot sensor space directly reinforcement learning state space 
results large redundant state space states markov property reactive tasks 
size state space means difficult achieve coverage reasonable amount time forcing function approximation generalisation techniques convergence state state action value table take long time 
known examples behaviour robots developing useful skills real time reinforcement learning 
model proposed intermediate layer learns topological map sensor space reinforcement learning takes place 
topological map defined graph set nodes set edges represents distinct state problem space edge ni indicates state ni topologically adjacent state respect behavioural capabilities agent 
means edge connecting nodes activation simple behavioural sequence robot control system probability move robot state ni state topological map state space reinforcement learning algorithm major advantages robot sensor space directly 
discards irrelevant sensor input results smaller task relevant state space 
state space scale addition new sensory capabilities robot task dependent chapter 
behaviour reinforcement learning sensor dependent new sensors increase robot ability distinguish states slightly richer set states introduce immediate combinatorial explosion 
topological map densely connected making value propagation state space faster 
reinforcement learning topological map tractable reinforcement learning large state space 
second map connectivity allows smaller action space actions movements nodes map raw motor commands 
actions naturally correspond behaviours behaviour robot reinforcement learning layer added top existing behaviour system greatly disturbing existing architecture requiring exclusive control robot effectors 
states topological space markov states raw pre processed sensor snapshots 
extends range reinforcement learning methods behaviour robotics tasks strictly reactive removes need generalisation similar distinct states longer similar values 
important aspect proposed model interaction assumed behavioural sub strate topological map reinforcement learning algorithm 
behavioural sub strate learning topological map feasible provides discrete actions allow movement nodes topological map 
simply topo logical map discretisation interaction topological map behavioural substrate sufficient considered grounded representation 
topological map turn reinforcement learning feasible 
strategy ex reinforcement learning level may influence way topological map develops learning topological map level continues time learning reinforcement learning level 
emphasis interaction differentiates model far previous attempts layer reinforcement learning learning models 
example smith similar model kohonen network kohonen continuous input output spaces reinforcement learning performed resulting discretisation 
model intended autonomous agent uses separate maps purposes discretisation take rela chapter 
behaviour reinforcement learning state action space account 
furthermore smith uses kohonen map number nodes map change position 
major implication reliance topological mapping level requires tractably maintainable map provides abstraction task hand grounded real world 
methods example grow required marsland automatically create update topological maps state space knowledge methods known sensor space 
real robot systems priori knowledge rel ative importance different sensor inputs relationships different sensors types sensor states important task hand crucial development topological map learning layer 
cases development layer may harder problem application reinforcement learning model developed top 
distributed reinforcement learning reinforcement learning typically applied single control process updates single state state action value table 
topological map dynamic structure behaviour principles require distributed representation parallel compu tation possible distributed structure updated processes parallel preferable 
topological maps easily built distributed fashion matari section describes reinforcement learning update equations adapted run distributed fashion distributed map 
performing reinforcement learning topological map nodes representing states edges representing actions view learning place nodes transitions graph table row node row node column action type 
illustrates graphical tabular representations simple example action value set states action types action values brackets graph 
distributed topological map node process re detecting node corresponds active transition node occurred 
allows node map maintain list chapter 
behaviour reinforcement learning transitions 
graphical tabular action value representations order add reinforcement learning topological map process augmented code perform update state state action spaces infor mation obtained current node nodes directly connected reward signal globally available 
reinforcement learning update methods intrinsically local require little modification 
sec tions consider update type turn briefly describe implemented distributed system 
update equations sutton barto 
temporal difference methods temporal difference methods easiest methods implement distributed fashion temporal update equations involve local terms 
standard step temporal difference update equation known td st rt st st st global constants st value active state time rt reward received time order implement update equation node process note active value state active immediately ceases active reward received transition 
record behaviours activated cause transition establish link nodes 
update equation state action value updates known sarsa slightly difficult case 
sarsa equation chapter 
behaviour reinforcement learning st rt st st st value action state st requires node st access value state action pair reward obtained activations 
relatively local node requires information single node way reduce information sharing required perform update terms state value state values computed state action values 
update equation st st rt st st st expected value st calculated probabilistically simply expected value action highest value state 
case equivalent learning st st monte carlo methods monte carlo estimate value state rs ns visited ns episodes rs sum returns received visit state method information relevant node neighbours distributed map provided node determine activated deactivated episode reached reward received time obtaining monte carlo state value estimate distributed fashion straightforward 
state action values obtained similarly 
td td reinforcement learning algorithm provides combination td temporal difference update equation monte carlo estimation 
performing temporal difference backup just value state current state td uses weighted average states 
parameter affects extent algorithm similarity td monte carlo methods obtained return total reward received time episode 
monte carlo methods applicable episodic tasks 
chapter 
behaviour reinforcement learning 
td methods usually implemented eligibility trace 
node eligibility trace value records extent current reward state value affect node value 
node eligibility trace decreased time increasing node corresponds active 
provided node access current reward value current state distributed implementation eligibility traces td straightforward 
primary advantage td allows faster learning steps required obtain reward sutton barto :10.1.1.32.7692
initial discovery reward state states path receive reward td state immediately reward state obtains reward step method td 
illustrated similar illustration sutton barto 
rewards received transition path td td states state zero reward 
path indicated taken td reached transition receives reward indicated thick arrow 
td transitions path obtain reward depicted reward level drops away transition indicated thinner arrows 
general td transitions finding reward state path length constructed start point reward chapter 
behaviour reinforcement learning state 
asynchronous temporal difference algorithm introduced section removes limitation 
reinforcement learning situated agents reinforcement learning strong theoretical basis developed problems facing situated agents mind 
reinforcement learning theory state action spaces emphasises asymptotic convergence optimality guarantees 
reinforcement learning situated agents learn quickly real world leads different set issues situated agents living life agre chapman 
situated agent task concern 
example soda collecting robot avoid obstacles navigate recharge batteries necessary 
reinforcement learning system part robot control system may share control reinforcement learning systems 
assume control robot actions requests taken 
implications situated agent sensors motor behaviours relevant task hand 
robot may experience transitions topological map governed reinforcement learning policy able experience anyway 
policy learning methods may appropriate 
asymptotic exploration slow 
greedy action selection methods provide asymptotic coverage state space reasonable amount time 
furthermore require occasional completely tional action agent may dangerous costly 
optimistic initial values result better behaviour smaller time frame 
alterna tively form exploration drive built agent separately 
inclusion drive low priority added advantage allowing robot explore free time 
situations large state spaces robot may satisfied suboptimal solution 
transitions take uniform time 
global parameter model reward time appropriate real environment 
chapter 
behaviour reinforcement learning performing update transition estimate time taken transition available agent experienced transition 
states lose value simply way lose value time energy expended get 
loss factored reward function transition 
similarly global parameter td longer appropriate erosion eligibility traces involves implicit assumption transitions take amount time 
rewards received respect actions states 
situations agent may receive reward moving state may receive reward presence particular state 
characteristics task taken careful consideration choosing model 
transitions take long time 
case situated agent time taken complete transition time spent state longer time required perform single update 
furthermore reinforcement learning performed distributed fashion process node principle nodes perform update parallel time take single update occur serial implementation 
simulated single processor computer happen parallel device capable parallel computation brain piece neurally inspired hardware 
implies updates may take place transitions 
learning models may required conjunction reinforcement learning 
situated learning required provide useful results quickly reinforce ment learning may take long 
fortunately reinforcement learning model provides underlying representation suited inclusion learning models modification reward function seeding initial state state action values selection motor behaviours sensory inputs 
penultimate point implies performing updates transition situated agent performing time nodes parallel 
order reliance update equations concept transition just experienced chapter 
behaviour reinforcement learning removed 
sense experiences agent obtained far provide state state action value estimates reward state values experienced 
experienced values create model state action state value estimates taken 
example node update state action values equation qt rs qt es qt st rs estimated average rewards received executing es expected state value obtained execution action st state expected state value weighted observed probability average states visited immediately state value taken value maximum action available state 
parameter set transition specific decay value 
update equation executed time 
method chronous temporal difference atd learning equation chapter develop asynchronous reinforcement learning robot 
model draws ideas reinforcement learning literature 
dynamic programming sutton barto atd learning uses model perform termed full backup uses expected value state state action pair sample backup uses sampled value :10.1.1.32.7692
dynamic programming model derived experience environment priori 
similar batch updating update rules transition experience repeatedly applied differs simply repeat previous episodes uses model environment generate value estimates performs backups nodes distributed map 
atd learning similar dyna model proposed sutton model environment built reinforcement 
differs updates occur parallel time model generate expected sample state state action values dyna easily adapted allow state action pairs state single 
ideally asynchronous updates leads convergence values reinforce ment learning complex transitions transition agent behaving best information obtained 
means situated agent model best choices possible experiences limited information obtained environment 
chapter 
behaviour reinforcement learning examples situation reinforcement learning model proposed useful case rat learning find piece food maze 
nodes topological map correspond landmarks maze connection indicating rat able go second 
reinforcement rat finding food 
potential application additional learning models associative learning modify reinforcement function locations smell cheese receive fraction reward received finding food 
experiment proposed chapter example 
application reinforcement learning development simple motor skills robot actuators 
example set motor behaviours joint angle sensors robot mechanised arm reinforcement learning model proposed learn reach touch object 
case joint angle sensors conjunction motor behaviours provide basis topological map nodes significant joint angle configurations edges indicate movement configurations possible short sequence motor behaviours 
case self organising map grow required marsland appropriate input space scaling factors create topological map 
robot receive reward touching object visual feedback provide heuristic modify reinforcement function 
task easy visual feedback possible robot learn quickly little visual feedback error aid reinforcement learning 
reinforcement learning complex motor coordination tasks changing gear car manual transmission 
requires fairly difficult sequence actions leg engage arm change gear 
map joint angle sensors arm leg set motor behaviours 
social imitation serve seed initial state values order task tractable fairly difficult learning task takes humans fair amount time effort instruction learn perform smoothly 
examples selection relevant sensors motor behaviours crucial 
example difficult robot learn touch object arm chapter 
behaviour reinforcement learning internal joint sensors entire body considered input topological map relevant difficult touch ball facing away example 
learning models may aid selection relevant sensors motor behaviours 
addition learning models may useful speeding learning may fact required learning feasible 
summary chapter model reinforcement learning autonomous agents motivated behaviour emphasis layered competencies distributed control 
model intended produce behavioural benefits real time real robot 
novel reasons 
performs reinforcement learning learned topological map directly robot sensor space 
aims learning feasible small relevant space tailored task hand 
second reinforcement learning performed distributed fashion resulting reinforcement learning complex embedded distributed topological map single state state action value table updated single control process 
allows dynamic structure potentially updated parallel parallel hardware 
order take advantage parallelism fact situated agents take longer transition perform update learning takes place time 
experiences update internal distributed model environment basis reinforcement learning reinforcement learning directly 
reinforcement learning situated agents raises different set concerns emphasised reinforcement learning literature 
asymptotic guarantees convergence limit useful real environments 
situated agents learn quickly performing best limited information obtain environment finite time 
illustrate potential applications model introduced chapter example systems reinforcement learning provide behavioural benefits outlined 
examples example rat maze forms basis experiment chapter designed test model feasibly implemented real robot provide behavioural benefits real time 
chapter experiment puck foraging artificial arena chapter presents experiment designed determine model chapter feasibly implemented mobile robot provide behavioural benefits real time 
experiment aims augment distributed map building model matari reinforcement learning model introduced chapter show produce complex goal directed path planning behaviour agent performs puck foraging artificial arena 
section provides overview experimental design followed outline evaluation methods experiment 
arena configurations designed highlight different aspects model discussed final section summarises 
overview experiment outlined intended abstraction rat maze example chapter abstraction kinds tasks commonly faced foraging animals 
models agent living static environment obstacles avoid chapter 
experiment puck foraging artificial arena landmarks purposes navigation 
agent driven internal needs need find food need explore environment need return home 
needs turn activated deactivated circadian cycle 
mobile robot placed artificial arena containing orthogonal walls henceforth referred vertical horizontal walls appear figures food pucks 
robot start prior knowledge layout arena navigate cycles 
cycle phases 
foraging robot attempt find food puck may short time possible 
soon robot food puck switches exploration phase cycle 
find food puck period time cycle length skip exploration phase move directly homing phase 

exploration robot explore areas arena relatively un explored remainder cycle length 
remainder cycle length passed robot switches homing phase 
exploration phase intended allow robot build accurate complete map environment time finding food 

homing robot return area started 
intended analogous agent return nest home sleep 
soon done robot moves cycle begins foraging 
run robot required follow walls decide action take 
robot restricted types actions turn right left go straight wall giving actions total 
robot follow wall reaches desired execute desired action 
robot may turn away wall potential actions available walls 
robot corridor may choose 
shows available actions horizontal walls corridors vertical case similar 
actions possible cases example left side corridor continues left wall right side robot may turn left 
robot determine action possible far possible chapter 
experiment puck foraging artificial arena avoid attempting illegal actions 
wall corridor potential actions wall robot state space reinforcement learning function set landmarks distributed map action space set legal actions landmark 
transition consist landmark turn taken landmark detected execution turn transition considered completed landmark detected 
order implement robot internal drives provide useful metric comparing various models drive internal reward function 
equations foraging exploration homing rewards respectively food puck view nt nave nave robot home nt number times transition just executed just taken total nave average number previous executions transitions map 
exploration reward function executed transition chapter 
experiment puck foraging artificial arena executed constant time delay set robot receive penalty near failing find puck cycle 
robot choice point aim maximise sum time value internal reinforcement function corresponding current cyclic phase 
possible try minimise time simplicity order avoid difficulties arbitrating different reinforcement learning systems single agent see discussion reward function pertaining current phase considered time 
order validate model chapter decision models experiment 
random movement robot chooses action random set cur rently legal actions 
agent builds internal distributed map arena uses determine actions legal 
model corresponds strategy typically employed behaviour systems 

synchronous reinforcement learning robot builds internal distributed map arena uses standard single step temporal difference learning algorithm 
model embodies application traditional reinforcement learning tech niques topological map state space described chapter 
asynchronous reinforcement learning robot builds internal distributed map arena uses asynchronous temporal difference atd model introduced chapter constitutes implementation fully behaviour reinforcement learning robot 
experiment facilitated direct comparison models terms performance expressed internal reward functions behavioural characteristics 
allowed evaluation model performance different types reinforcement func tions 
reward function obtaining puck cases multiple reward points puck required discovered homing reward function exactly reward point known run robot started 
exploration reward reward points differing values non stationary changed rapidly time vis particular node immediately dropped exploration reward value number chapter 
experiment puck foraging artificial arena times visited increased 
exploration phase varying length occur cycle results obtained model directly compared 
evaluation learning models proposed chapter required evaluated quantitatively qualitatively 
quantitative analysis appropriate comparing reinforcement learning models directly qualitative analysis required order assess resulting behaviour apparent complexity 
evaluate model quantitatively reward values internal drive time aver aged number runs directly compared reward functions directly performance metrics 
average change state value function map time considered transitions added reward map order examine convergence asynchronous reinforcement learning model 
order evaluate model qualitatively data set runs 
distributed map learned robot action values visualised 
way robot internal map action values examined 
recordings robot movements specific instances replayed examined view considering instances choices robot response 
internal data state robot obtain information reasons choices 
data captured runs particular model presentation analysis representative informative examples chapter order avoid repetition 
arena testing simple puck finding arena dual purpose provided testing platform development robot build distributed maps served simple problem instance reinforcement learning models compared random agent 
configuration chapter 
experiment puck foraging artificial arena arena depicted 
arenas robot starts bottom left corner facing right 
experimental arena arena configuration designed specifically contain important difficult features arena 
lower left corner corridors corner predicted useful testing corridor behaviour lower right corner box shaped section intended cause maximum sensor noise 
arena contained instances landmarks minimum length robot able detect landmark maximum length robot able detect 
arena contained section just wide considered corridor order test corridor detection 
arenas lightly shaded indicates region robot home area black circle indicates position puck 
aim experiments conducted arena verify proposed models worked able learn find food pucks explore return home quickly possible 
problem instance embodied arena deliberately friendly reinforcement learning agents 
area right containing box chapter 
experiment puck foraging artificial arena marked region near kind trap entered escaped turn marked random agent spend great deal time trap reinforcement learning agent able learn escape fairly quickly 
transition indicated transition leads puck transitions away home area 
robot return home corridor turning start trying find puck consecutive correct transitions required order find puck moving 
reinforcement learning models expected able learn find puck relatively quickly number steps goal short results learning models expected similar 
arena hostile environment second arena configuration intended provide environment relatively hostile reinforcement learning agents relatively friendly random agents depicted 
second experimental arena chapter 
experiment puck foraging artificial arena features arena difficult reinforcement learning agents 
quickest transitions home area noisy 
robot leave wall angle exactly straight turn lead walls labelled similarly turn labelled lead walls labelled noisy transitions intended test robustness reinforcement learning methods presence actions uncertain results 
addition pucks taken away fifth cycle 
reinforcement learning agents puck foraging phase seen far 
random agent puck removed 
intended test quickly reinforcement learning agents able learn go directly puck previous choice removed 
transitions labelled potentially labelled intended lead puck reward 
second arena intended simple arena random agent fairly stumble pucks home area 
arena intended useful determining reinforcement learning agents better random agent faced hostile environment evaluate differences recovery time standard asynchronous reinforcement learning models puck removed 
arena far away puck third arena designed test ability reinforcement learning agents discover puck placed transitions away complex environment able quickly find puck home 
third arena configuration depicted 
third arena complex task environment robot faced 
order successfully find puck home robot required consecutive correct transitions path paths may take time fewer transitions 
transitions puck scoring transitions reached having seen puck way get ledge expected robot path puck 
returning home may require longer path 
path lengths required intended highlight difference synchronous chapter 
experiment puck foraging artificial arena third experimental arena asynchronous reinforcement learning models agent asynchronous model expected able learn find path immediately finding puck time agent synchronous model expected take longer 
summary chapter experiment aims test reinforcement learning model introduced chapter implemented real robot provide benefits real time 
task intended abstraction rat maze example chapter robot placed artificial arena required find food puck explore return home area current activity regulated circadian cycle 
particular arena instance task determined difficulty arena configurations intended provide insight different aspect models performance 
arena intended demonstrate reinforcement chapter 
experiment puck foraging artificial arena learning models better random choice fairly benign environment second arena intended show better relatively hostile environment compare recovery times reinforcement learning models removal puck 
final arena designed allow comparison reinforcement learning models difficult environment longer sequences actions required learned 
chapter describes construction arenas development robot capable learning distributed maps 
chapter describes addition reinforcement learning models random decision models robot control system presents results experiment outlined 
chapter distributed map learning artificial arena experimental design outlined chapter requires significant preparation requires construction physical arena development behaviour robot capable distributed map building arena 
chapter describes physical characteristics arena constructed experiment hardware interface behavioural layers dangerous beans robot implement map building behaviour 
environment required environment constructed experiment resembled small maze arena objects pucks horizontal vertical walls 
shelf robot see section environment carefully engineered ensure fit robot capabilities arena order facilitate development desired behaviour pfeifer scheier 
chapter 
distributed map learning artificial arena physical characteristics arena built wooden base measuring cm cm walls constructed pieces insulation tape covered sheets white cardboard secured drawing pins 
type cardboard round sharp internal corners 
cardboard served provide smooth response infra red sensors rounded corners simplified wall behaviour 
materials chosen readily available 
white wooden cylinders lab food pucks strip black insulation tape marking easy visual detection 
arena configurations configurations introduced chapter built platform pieces requiring experiments performed com sequentially 
configurations shown 
note presence food pucks 
robot arenas section describes hardware software behavioural levels dangerous beans robot experimental runs shown 
behavioural structure control system dangerous beans depicted fig ure 
behavioural modules darker shade dependent lighter chapter 
distributed map learning artificial arena dangerous beans shade rely output rely behaviour emergent execution 
dashed arrows represent corrective relationships higher level modules provide corrective information lower level ones behaviours shown dashed boxes multiple instantiations 
solid arrows represent input output relationships 
avoid place junction landmark serial protocol khepera hardware wander irs motor map building landmark detection behavioural substrate software interface hardware dangerous beans behavioural structure map building hierarchy divided levels hardware software interface behavioural substrate landmark detection map building 
sections describe layers greater detail 
chapter 
distributed map learning artificial arena hardware dangerous beans standard issue team khepera robot serial number pixel array extension turret 
khepera diameter approximately mm infra red proximity ambient light sensors wheels incremental encoders team sa 
pixel array extension provides single line grey level intensity pixels viewing angle team sa 
infra red sensors obstacle avoidance wall pixel array puck detection 
shows sensory configuration infra red sensors numbered indicates angle view pixel array turret 
dangerous beans sensory configuration khepera board processor purposes research con serial cable kbps supported simple communication protocol allowing programs host computer interface 
approach taken recording rich data robot control state possible 
software interface software interface dangerous beans written run standard dice linux machine mobile robot lab connected dangerous beans serial cable suspended counterbalanced tether 
layer abstraction library calls developed communicated serial port covered required khepera commands thread chapter 
distributed map learning artificial arena safe 
library tested development sensor monitoring software able perform sufficiently servicing active threads tight loops 
behaviour allocated thread behaviours run asynchronously attempt scheduling kept behavioural processes parallel loosely coupled pfeifer scheier 
communication threads necessary accomplished global variables thread writing variables possibly multiple threads reading 
variables typically thread safe usually atomic control reactive continuous effect incorrect values encountered transient 
exceptions sets compound global variables required updated atomically global landmark flags 
behaviour wait flag caused behaviour pause non zero allowing behaviours suppress 
behavioural substrate behavioural substrate developed dangerous beans required produce sufficiently robust wall behaviour allow consistent reliable landmark detection map building 
behavioural layers developed handling infra red sensing averaging motor output position estimation obstacle avoidance wall map building 
considered standard behaviour substrate described 
movement sensing behaviours irs created handle interface behaviours robot sensors actuators 
irs behaviour obtained data infra red sensors posted average sensor readings global structure accessible threads 
averaging proved necessary sensors fairly noisy wall proved difficult accurate readings low activations 
motor behaviour consulted global structure contained left right motor fields behaviour issue motor commands sent sum motor com chapter 
distributed map learning artificial arena mands khepera 
behaviours active different conditions require coordination proved best simply add motor outputs transition phases order avoid brief episodes erratic behaviour 
motor checked behaviour flag include motor output waiting behaviours allowing immediate suppression effects behaviours checked wait flags 
behaviour performed dead reckoning position estimation encoder readings khepera wheels 
simple tracking equations suggested lucas dead reckoning rd ld rd ld cos xt xt rd ld sin yt rd ld right left wheel displacements respectively diameter robot 
equations approximations assumption angular change small position estimation algorithm run delay calculations allowing small adjustments point 
behaviour wrote estimated position landmark type detected position file produce dead reckoning maps type shown 
obstacle avoidance behaviours implement obstacle avoidance wander avoid 
wander behaviour simply kept dangerous beans moving forward setting left right motor outputs global speed 
behaviour implemented safe distance thresholding 
forward sensors safe distance level associated robot detected obstacle sensors reading reached level 
single global threshold front angular sensors require low different thresholds approach wall relatively directly require immediate attention lateral sensors require high thresholds gentle turns avoid breaking away wall robot trying follow 
shows threshold values front sensors 
note lower yt chapter 
distributed map learning artificial arena front sensor obstacle avoidance thresholds threshold lateral sensors wider range khepera sensors approximately mm minimum reading maximum reading kinds surfaces arena team sa extreme value minimise chances robot veer away wall due small movement sensor noise 
single forward sensor thresholds exceeded robot turned spot away incoming obstacle 
forward sensor thresholds exceeded lateral sensors checked activation indicating obstacle side robot robot turned direction 
achieved behaviour avoided robot turning walls 
lateral sensor active robot turned left default 
angular sensor thresholds exceeded robot briefly backed si turned gently away active sensor 
allowed robot turn slightly away wall potentially losing cost slightly jerky motion cor 
experiments angular sensor activation caused robot simply turn showed doing cases lose wall delayed drop sensor reading wall cause turn long 
lateral sensor thresholds exceeded robot turned gently away active sensor tracing slight curve move robot away wall quickly wall behaviours recover 
resulting emergent behaviour fact cases achieved wall explicit attempt doing attempt keep wall constant distance robot prone chapter 
distributed map learning artificial arena slightly away wall due sensor noise wandering 
wall wall implemented single behaviour active lateral sensor read suppressed forward sensors active order avoid interfering behaviour 
behaviour attempted keep sensor reading target value 
problem keeping sensor value considered steady state problem standard proportional integral differential pid control implementation obvious didate 
implementing full pid controller turned infeasible noise characteristics short range khepera infra red sensors fact khepera wheels integer control fairly low speed 
extensive testing showed simpler modified approach able best handle char khepera task requirements 
approach taken proportional attraction force small constant repulsion force 
constant repulsion force ensured khepera lose wall turning away quickly proportional attraction force acted quickly correct robot movement moving away wall 
probability robot respond strongly wall collide negligible near wall behaviour obstacle avoidance substrate lateral threshold 
control forces calculated active lateral sensors summed converted pro portion maximum possible repulsive force 
result modify direction robot half proportion target speed khepera added wheel half subtracted moving shallow curve target value equilibrium case walls side 
landmark detection landmark behaviour performed landmark detection broadcast current landmark type heading length number consecutive readings 
landmark type took values right wall left wall corridor empty space current heading chapter 
distributed map learning artificial arena radians 
behaviour dead reckoning position computed estimate angle wall supplied corrected angle back position estimator order minimise dead reckoning angular error absence compass 
landmark behaviour simple statistical approach similar matari 
set thresholded samples taken left right lateral sensor delay samples 
sample thresholded lower bound wall activation infra red sensor reading 
landmark determined corridor samples showed left activation showed right activation failing determined left right wall samples relevant side threshold 
condition met landmark type set default value free space 
new landmark detected type landmark changed estimated angle robot differed currently active landmark radians half distance expected wall angles 
landmark change landmark length count increased 
estimated angle landmark selected orthogonal directions walls expected lie selection performed current angle landmark length count average measurements far including set measurements 
averaging past certain length practice ignoring estimates required angular estimations obtained early typically significantly approach angle robot possibility just followed curved corner moving straight 
landmarks length angular correction difference tween expected angle landmark average angle obtained correct current angle estimation 
required robot required accurate angular es time dead reckoning correction maintain 
outputs position processes landmark correction landmark correction shown 
colour lines fade time indicating robot movements time progresses dots second graph indicate robot position adjustments 
adjustments difference estimated observed angle radians ignored 
chapter 
distributed map learning artificial arena uncorrected angle corrected dead reckoning maps note cases definite horizontal vertical dead reckoning error 
case position estimation correction angular displacement degrade consistently time 
examination upper right corner trace indicates position estimates differ time traced lines stay nearly parallel left definite angular degradation time 
accuracy achieved corrected angular estimates sufficient landmark discrimination case walls known horizontal vertical 
map building layer behaviours responsible map building required maintain distributed map creating new place behaviours novel landmarks linking places appeared sequentially 
addition choices robot allowed restrictive allowed matari lead encountering walls halfway nodes represent different parts land mark required merged 
place allocated place behaviour thread 
place behaviour maintained landmark descriptor consisted type angle es coordinates connectivity information corresponding landmark 
de place behaviour continuously compute probability chapter 
distributed map learning artificial arena corresponded current landmark 
place behaviour highest probability judged winner non zero probabilities time 
place behaviour maintained linked list transitions stored place active immediately type turn left right straight extra direction modifier indicate landmark turn resulted transition times combination occurred far 
sim ilar manner place behaviour kept list behaviours directly preceded 
behaviour active place behaviours point followed marked expecting 
place determined probability current landmark combination landmark type angle dead reckoning distance 
place behaviours correct type angle immediately set probabilities zero 
correct type angle calculated distance metric equation xd yd xd yd shortest distances landmark respectively 
ties zero rounded zero 
distance metric gives probabilities inversely proportionate distance reaching zero cm away 
model matari uses expectation deadlock breaker dead reckoning seldom necessary higher branching factors complex maps created dead reckoning required fairly frequently 
expectation easily modify equation give expecting nodes higher probabilities 
behaviour responsible detecting place behaviour sufficiently high probability corresponding current landmark allocating new 
simplicity behaviour determined place behaviour current best merge landmarks 
landmarks strongly active time indicating potentially account landmark checked see compatible types angles long axes overlapped 
duplicate landmarks merged 
merging involved calculating chapter 
distributed map learning artificial arena new bounds landmark merging transition lists associated places 
duplicate landmarks artifacts fact dangerous beans encountered wall half way created landmark behaviour covering half allowing new behaviour erroneously created wall encountered unexplored side 
problem occur model matari strict wall behaviour significant problem 
fortunately merging procedure adopted solved observed cases 
performs somewhat centralised role corresponds hori behavioural module coordinating vertical behavioural modules bryson roles implemented distributed system simple sion excitation relationships 
relatively difficult threaded model attempted 
addition place behaviour responsible correcting current position robot 
landmark encountered place behaviour created recorded average dead reckoning coordinate long axis stored 
landmark traversed similar average kept place behaviour longer active corrected position robot difference averages 
shows graphs run graph left produced just angular correction right produced landmark position correction 
angle corrected fully corrected dead reckoning maps chapter 
distributed map learning artificial arena clear landmark correction dead reckoning error cause robot perceived position drift 
turn cause mapping errors defeat distance measure disambiguate landmarks 
simple approach proved sufficient simplified environment experiments general environment complex approach required 
visualisation distributed mapping data produced dangerous beans test arena 
landmarks represented rectangles central circle circles 
corridors rectangles circles placed 
lines represent transitions line circle landmark circle middle indicates dangerous beans able move landmark second 
map contains landmarks edges edges distinguishable different turn types landmarks 
slightly exaggerated length landmarks artifact landmark recognition algorithm 
means landmarks may appear overlap example bottom left corner just close 
note distributed map information shown particular direction data indicated extent side outgoing line leaves landmark frequency data 
map visualised contains information required addition distributed reinforcement learning model 
summary chapter outlined properties physical arena constructed experiment outlined chapter robot designed perform distributed map building 
behavioural substrate map building navigation mechanisms robot described shown sufficiently robust provide basis distributed reinforcement learning layer 
chapter describes development layer presents results experimental arenas described chapter 
chapter 
distributed map learning artificial arena distributed topological map arena chapter distributed reinforcement learning artificial arena critical test learning model claims able improve performance existing robot system perform required real world real time 
chapter describes implementation variation reinforcement learning model described chapter presents results experiment described chapter 
results show distributed embedded asynchronous reinforce ment learning model performs significantly better random choice algorithm standard synchronous reinforcement learning algorithm capable generating complex adaptive behaviour real robot real time 
section describes additional functionality added distributed map learn ing robot chapter order implement reinforcement learning model 
section describes analyses results experiment final section draws 
chapter 
distributed reinforcement learning artificial arena implementation chapter described development dangerous beans robot capable distributed map building artificial arena 
section describes additional control structures added dangerous beans enable perform distributed reinforcement learning distributed topological map record data required provide results remain der chapter 
behavioural structure experiments shown behaviours added structure chapter shown shaded boxes 
circadian cycle internal drives circadian explore homing avoid place junction landmark irs motor serial protocol khepera hardware wander map building landmark detection behavioural substrate software interface hardware dangerous beans behavioural structure reinforcement learning part changes simply added top control structures robot required small behavioural architectural changes existing software 
final part section describe significant changes give reasons 
obtaining reward order express drives required experiment reward behaviours added dangerous beans 
behaviour exposed global variable read behaviours order obtain reward information 
behaviours updated rewards equations chapter second delay updates 
chapter 
distributed reinforcement learning artificial arena behaviour responsible determining robot facing puck receive puck reward 
puck covered black tape level pixel array showed dark band light background pixel array 
behaviour simple averaging thresholding algorithm ballard brown pixels threshold average intensity image marked dark 
dark band considered puck light interior pixels minimum size maximum size pixels edge image 
final condition required images produced pixel array dark edges 
sample pixel array images shows sample images pixel array dangerous beans arena 
top sample taken robot facing puck rest taken arbitrary positions arena 
significant dark areas samples especially near edges sample puck sight distinctive black band light background 
behaviour inhibited puck reward approximately seconds puck detected order avoid multiple rewards issued puck sighting 
behaviour checked robot position estimated threshold directions robot original location loca tion boundary considered home 
thresholds set separately arena order correspond home area 
behaviour required robot cm outside area return allocating reward 
chapter 
distributed reinforcement learning artificial arena behaviour number times transition taken taken average computed set place behaviours determined transition reward exploration reward equation chapter 
circadian events circadian behaviour responsible keeping track current cycle active phase robot 
monitored reward behaviours introduced previous section timer switched robot state foraging exploring homing modes necessary 
behaviour exposed global variable representing current phase active desire behaviours making decisions 
making choices junction behaviour extended allow place behaviours signal decisions wanted form requested turns allowing post requests global variable checked time decision 
global variable timestamped order avoid old decisions taken longer valid 
behaviour modified posted activation current drive changed active case allowed post whichever control strategy random reinforcement learning models time 
behaviour executed requested turn possible turns ignored executed presence adjoining obstacle 
turn place associated counter incremented turn successfully taken decremented 
counter reached turn banned considered decision making reinforcement learning purposes 
required occasionally attempted illegal turns caused robot bounce wall find wall resulting apparently legal transition 
junction behaviour responsible determining robot headed wall wrong direction decision reversing robot di chapter 
distributed reinforcement learning artificial arena losing contact wall 
simple turn waiting activation currently inactive lateral sensor proved sufficient turn reliably landmark broadcast successive times landmark detector 
attempted earlier turn lose wall dangerous beans wall behaviour 
place transition values robot spot puck shortly making transition guar simply particular landmark see puck reward allocated transitions places 
transition received reward obtained time robot left landmark time robot left landmark 
explained chapter place behaviour maintained list transitions originating containing turn frequency place data 
order record reward obtained transition place behaviour kept record relevant reward values soon inactive 
transition noted place led inactive transition received difference initially noted reward values reward values place led 
transition kept total reward received total number times taken number times negative reward received 
update equation asynchronous reinforcement learning model run place behaviour times turns example atd update equation chapter qt rs es qt qt st learning step parameter set qt value action turn state place time rs expected reward received action state es expected state value action state time st common value sutton barto 
due time constraints systematic evaluation effect performed 
chapter 
distributed reinforcement learning artificial arena stored values possible turns es update st calculated turn computing weighted sum values state tered turn state expected reward term rs computed action average reward obtained executions transitions turn state 
exploration reward function estimated reward computed directly equations chapter previous exploration rewards particular turn useful estimating current value 
task effectively episodic transition positive reward contribution expected value state included 
effect considering positive rewards episode prevented positive feedback loops states obtained infinite expected values 
synchronous update case value function state action pair updated immediately transition state action completed average reward reward obtained directly 
update equation synchronous case qt rt st qt qt learning step parameter set 
qt value action turn state place time rt expected reward received time st value state active time value state taken expected value maximum action taken synchronous case equivalent learning watkins dayan 
order encourage exploration actions taken state assigned initial values homing puck rewards 
initial exploration rewards set required exploration reward function chapter 
initial reward estimates immediately replaced received reward asynchronous model 
reinforcement learning models place behaviour active post decision junction behaviour action highest action value ties broken randomly 
action values available negative requested chapter 
distributed reinforcement learning artificial arena action taken random action chosen 
cases legal turns allowed landmark type far impossible considered 
random decision case legal turns simply picked random posted junction behaviour 
data capture graphs analysis chapter required data different processes running dangerous beans 
information captured run time written disk robot position estimated 
circadian events circadian behaviour 
reward levels homing explore behaviours 
place descriptor activation data place behaviour 
transition data individual transition 
average reinforcement learning value update sizes transition 
data written file timestamp processes responsible gen erating 
set scripts written awk bash convert data matlab program code examined manipulated scripts parametrised time interval specified allowing extraction subsets data 
modifications original map building system original distributed mapping system required modification order successfully perform experiments chapter 
primary difficulties encountered 
addition reinforcement learning decision making reward modules required architectural changes distributed mapping software 
changes affected internal structure control system substantially affect robot behaviour 
second added ability robot turn headed wrong direction dis functionality existing modules 
robot chapter 
distributed reinforcement learning artificial arena wall right turned slightly delayed nature landmark detec tor brief ghost wall corridor detected landmark detector right robot turned fact wall left 
furthermore reversing direction number consecutive landmarks detected longer accurate indicator length wall corridor reset half way 
problems arose added behaviour explicitly disturbed existing control software parts existing software relied emer gent behaviour control system order function correctly 
introduced subtle difficult isolate errors control system discovered exten sive testing solved suppression disturbed behaviours execution reverse turn 
random decision making agent introduced significant noise dead reckoning system 
occurred robot repeatedly double back near corners avoiding angular landmark correction sin gle landmark long take effect 
loss usefulness consecutive landmark detections landmark angular cor schemes modified supply correction values contact landmark sufficient length 
addition landmark slip place posi tion estimates wider time dead reckoning error landmarks considered merging required overlap cm 
new correction scheme provided equivalent results old reinforcement case superior results random case 
inherent difficulty accurate position estimation correction mechanisms failed cases runs occurred restarted 
cases failures occurred inaccurate angle estimates long empty spaces robot obtain corrective information primarily result angular errors 
solved addition direction sense compass mallot method landmark disambiguation dead reckoning neighbourhood characteristics dudek 
chapter 
distributed reinforcement learning artificial arena results section presents results robot control strategies developed chapter experimental arenas described chapter 
sections describe results obtained arena individually 
time average puck homing rewards time model compared discussed qualitative evaluation relevant aspects robot behaviour examples typical routes taken 
exploration component robot type behaviour considered relevant 
results arenas followed brief investigation convergence asynchronous reinforcement learning model summary primary results 
arena arena reinforcement learning models able learn find direct routes single puck relatively quickly 
shows puck reward obtained time averaged runs models error bars indicating standard error 
average puck reward atd random cycle number average puck reward time arena chapter 
distributed reinforcement learning artificial arena path puck home area relatively short expected reinforce ment learning models learned solutions quickly progressing near random results wide spread reward values indicated large error bars cycle nearly uniformly results indicated small error bars fifth cycle 
contrast random control strategy performed poorly resulting low average reward large error bars 
learned random routes puck arena left part shows route learned nearly cases reinforcement learning models puck note model turn heading homing area creating double line lower left part path 
breaks path caused landmark correction 
right sample path taken random algorithm find puck 
random algorithm move directly puck finds chance 
path quite short random agent trap right doubled back repeatedly virtually encountered puck cycle 
reinforcement learning models able return home area quicker random model indicated 
reinforcement learning models solutions quickly consistently 
asynchronous model appears able return homing area quickly cycle result active exploration strategy ability learn rapidly 
random model performed figures similar ones follow obtained superimposition dead reckoning position estimation scale drawing map 
considered reasonable path approximations 
chapter 
distributed reinforcement learning artificial arena poorly expected trap right arena 
average home reward atd random cycle number average home reward time arena learned random routes home arena shows typical routes home reinforcement models left random agent right 
note random agent gets stuck trap right time chapter 
distributed reinforcement learning artificial arena eventually wandering home reinforcement learning agents escape immediately 
preferred transitions maps arena shows robot preferred transitions puck homing phase asynchronous runs darker arrows indicating higher values 
clear maps reinforcement value complex converged entire map 
reinforcement learning models appeared explore effectively 
occasionally implementation problem caused exploration behaviour reinforcement learning models exhibit odd behaviour 
ledge bottom right arena just long included distributed map long allow robot turn moving 
transition ledge vertical wall left taken 
transition obtained high exploration value caused robot repeatedly attempt execute 
resulted robot circling bottom right corner arena cycle reached went back home area 
problem affected asynchronous model synchronous presumably exploration value propagated map quickly appear negatively impact effectiveness model 
chapter 
distributed reinforcement learning artificial arena second arena required pucks removed second arena fifth cycle trial 
reinforcement learning models puck near top arena visited runs removed random runs puck removed order results maximally comparable 
seen graph average puck reward obtained time models reinforcement learning models learned find puck relatively quickly 
choice puck removed fifth cycle reinforcement learning models experienced sharp drop performance high variation reward indicated large error bars 
asynchronous model able recover return consistently solution ninth cycle synchronous model average perform better consistently random model run 
average puck reward atd random cycle number average puck reward time second arena asynchronous model able able learn adjust value complex relatively chapter 
distributed reinforcement learning artificial arena quickly face modified environment 
despite fact expected values calculates averages rewards received residual puck reward remain transition puck sighted 
remedied average sightings similar measure appear affected performance 
learned puck finding behaviour second arena shows puck finding behaviour displayed reinforcement learning models 
left shows example puck finding path initially learned reinforcement models middle displays behaviour exhibited initially models puck taken away robots repeatedly execute transition previously led puck sighting 
asynchronous model consistently able learn take alternate puck finding route shown right synchronous model 
shows preferred puck transition maps eighth cycle chronous synchronous models 
map obtained asynchronous model left adjusted removal top puck directs movement lower puck graph note pairs walls left map appear wrong side due dead reckoning error 
map obtained syn chronous model adjusted contains regions movement directed transition puck removed 
shows reinforcement learning models able learn get back home area relatively quickly 
synchronous learning algorithm experiences drop performance increase standard error sixth cycle recovering ninth cycle asynchronous algorithm 
may indicate chapter 
distributed reinforcement learning artificial arena preferred puck transitions maps second arena eighth cycle synchronous algorithm robust asynchronous conclusively determined small number runs performed 
average home reward atd random cycle number average home reward time second arena reinforcement learning methods perform better random model chapter 
distributed reinforcement learning artificial arena second arena designed stumbling home area easy 
reinforcement learning methods able cope noisy environment underwent change agent lifetime 
error bars second arena slightly larger reflection noise inherent environment 
third arena third arena difficult arena faced robot longest path puck complex map 
due time constraints shown perform poorly runs performed random model 
addition data reinforcement learning model runs shows average puck reward time third arena 
demonstrates decisively asynchronous algorithm outperforms synchronous long path goal constructed 
asynchronous algorithm consistently learned short goal puck sixth cycle synchronous algorithm manage consistently find path 
path commonly learned asynchronous model shown left side 
fairly complex arena robot manages learn direct path puck 
representative path synchronous model shown right clearly direct path learned synchronous model 
shows preferred puck transitions asynchronous synchronous models 
map obtained asynchronous model left shows path puck propagated map clear map obtained synchronous model path puck propagated slowly transitions close puck transition having high values indicated dark arrows 
difference learning models pronounced shows average home reward obtained models time 
asynchronous model consistently finds solution quickly fourth cycle 
synchronous model runs reinforcement learning models performed arena learning runs discovered incorrect maps affected performance 
chapter 
distributed reinforcement learning artificial arena average puck reward atd cycle number average puck reward time third arena learned puck routes third arena takes longer reaching conditions seventh cycle performs 
potential explanation difference performance models models explore initially start runs home area synchronous model experience transitions near home area able chapter 
distributed reinforcement learning artificial arena average home reward preferred puck transitions maps third arena atd cycle number average home reward time third arena build path earlier case puck experience puck sighting repeatedly having built path 
revealing aspect robot type behaviour apparent repetition transitions chapter 
distributed reinforcement learning artificial arena synchronous model transition experiences required drive optimistic initial transition values 
asynchronous model repeated transitions took transition attempts determine unexplored transition illegal 
gave time explore allowed wider coverage map may contributed superior performance 
convergence final issue considered convergence 
chapter argument amount time situated agent takes transition may sufficient allow asynchronous reinforcement learning algorithm topological map converge transitions 
average change action values time average change action values time average change action values time atd average action value changes time contains samples arena showing average change ac tion value second slice time dotted vertical lines mark transition occurrences 
transitions function event points decision required reinforcement learning complex receives new data 
third graph run third arena puck discovered minutes robot built nearly com plete map arena 
graphs action values disturbed event point converge comfortably occurs 
remainder data shows similar pattern convergence conclusively proved data collected sufficiently rapidly rule possibility small changes event points 
example second graph appears spikes event point artifact sampling rate event point data spikes fact start event points 
chapter 
distributed reinforcement learning artificial arena summary clear results reinforcement learning models perform better random movement strategy task capable learning find puck return home quickly 
synchronous method learning performs roughly asynchronous method atd finding fairly short paths asynchronous model performs better long path learned recovers quickly synchronous model environment changes 
results suggest interplay exploration drive distributed map drives may subtle important effects robot performance 
data obtained runs suggests asynchronous model able converge transitions choices agent optimal knowledge 
chapter described addition reinforcement learning models random choice model dangerous beans map building robot developed chapter pre sented results obtained running models experiment chapter 
results show model developed chapter feasible brings definite behavioural benefits puck foraging task 
chapter discussion chapter provides discussion research dissertation research issues raised 
section examines significance model developed chapter experiment constructed test results obtained 
limitations model experimental design implementation considered followed discussion implications reinforcement learning situated learning models general 
final section concludes 
significance model developed dissertation significant shown behaviour style robot control architecture reinforcement learning models fully inte produce autonomous robot capable rapid learning 
integration potential widen scope behaviour systems lead synthesis robots dangerous beans learn environment order improve perfor mance 
addition reinforcement learning model developed conditions faced situated agents mind shown offer better performance model developed standard theoretical emphasis 
indicates similar change em may offer advantages developing mobile robots learning models chapter 
discussion integration layered learning models behaviour robotics promising research area 
limitations dissertation limitations 
divided limitations model chapter limitations experiment implementation chapters 
major limitations model fact relies learned topological map 
situations may possible feasibly build maintain map map may prohibitively large 
cases map large reinforcement learning layer top able learn quickly useful real time 
situations may require addition learning models hierarchical reinforcement learning methods topological mapping layers order obtain map small useful 
primary difficulty inherent model inherits limitations reinforcement learning 
reinforcement learning appropriate types problems imposes significant state property requirements 
states markov property may difficult obtain situated environments due perceptual aliasing different states perceptually indistinguishable 
cases internal state crook hayes active perception may required order disambiguate states 
experimental design implementation suffer limitations 
environ ment experiment required significant engineering robot sensory limitations 
major flaw serve remove fundamental difficulties inherent task 
level engineering required system high distributed map building dead reckoning correction particular requiring great deal time effort order function correctly 
experimental results chapter results application model single application area 
results suggest model promising may domains experiments required order confirm 
chapter 
discussion implications section considers implications research dissertation situated reinforcement learning robot learning models general 
sec tions consider implications situated reinforcement learning planning behaviour layered learning emergent representations turn section providing highly tive discussion role learning models study representation 
situated reinforcement learning results dissertation shown learning algorithm specifically geared problems faced situated agents outperform developed traditional theoretical approach mind 
situated agent employing learning algo rithm required learn real time reasonable amount computation manner provides behavioural benefits 
agents benefit able harness parallelism inherent distributed control architectures evolution ex complex methods learning strategies feasible plausible especially naive implementations methods provide behavioural benefit whatsoever 
example behaviour generated model 
corresponds closely exhibited experimental conditions survive training instances required obtain convergence model 
great deal gone making reinforcement learning single control process large state spaces feasible smart kaelbling 
results obtained dissertation suggest effort go development methods layering reinforcement learning topological map create conditions learning feasible 
promising approaches integration priori knowledge learning bias learning models bryson layered learning models stone veloso 
situated learning clearly merits study subfield machine learning right 
chapter 
discussion planning behaviour original criticisms behaviour robotics systems built able plan emphasis distributed control reactive behaviour lack syntactic representations preclude traditional planning algorithms brooks 
situated view intelligent agents consider construction execu tion plans primary activity performed intelligent agent way classical artificial intelligence agre chapman brooks generation form planning behaviour important aspect intelligent behaviour 
rein learning model viewed form plan learning sutton barto optimal choices purely local decisions state state action value table converged :10.1.1.32.7692
results chapter show danger ous beans said displaying planning behaviour finds optimal path puck back home area knowledge planner classical sense word 
research dissertation shown cases planning behaviour generated reinforcement learning methods necessarily require traditional planning 
layered learning layering reinforcement learning top topological map instance layered learning approach introduced stone 
layering learning models powerful general idea fully explored 
implications research kind feedback layered learning systems possible performance algorithm biases learning op 
machine learning research concentrated learning model isolation significant scope research kinds biases interacting learning algorithms impose 
interesting aspect type learning feasible suggests information requirement ordering learning models similar evo ordering species evolve types learning models chapter 
discussion order obtain behavioural benefits 
example dangerous beans capable dis tributed map learning addition reinforcement learning layer provided significant behavioural benefits required far engineering effort required develop map learning layer place 
may scope investigation kinds relationships learning models artificial evolution harvey 
layered learning models may interesting implications terms emergent 
interaction multiple control processes complex environment re sults complex behaviour reasonable expect interaction multiple learning models multiple control processes complex environment likewise result complex learning behaviour 
emergent representations behaviour approach artificial intelligence caused change way artificial intelligence researchers view behaviour 
behaviour considered emergent result complex interaction agent multiple control processes mor environment 
corresponding change way researchers view representation 
behaviour simply ignore oth ers maintain classical view representation result internal syntactic symbol system 
simply ignoring role representation intelligent behaviour tenable position 
real question representation exist intelligent agent form manner exists 
brooks considered argument form representation whatsoever argument central syntactic representational systems control 
fact brooks claims useful representations distributed emergent things sufficiently different traditionally considered representation called 
powerful way study representations situated agents learning 
represen tations offer agent behavioural advantage simple fact agent wishes represent learned implies types learning entitled intelligence representation 
chapter 
discussion models agent dictate representations behavioural advantages receives 
imposes dual constraint types representations agent find beneficial relevant learning model feasible able learn real time behaviour generates useful provides behavioural advantages appropriate agent level competence 
situated learning feasible task specific follows representations task specific 
way single general purpose tractable learning algorithm known representational system capable handling wide spectrum knowledge different levels detail computationally intractable 
situated intelligence expected develop way facilitate cheap computation rapid learning possible 
assertion representation arises presence learning models fol lows representations form structures generated learning models interaction 
representations emergent sense composed atomic syntactic symbols complex entities formed association task specific structures different levels de tail organised loosely hierarchical distributed structure 
complexes may identifiable symbols particular linking context 
example observing behaviour dangerous beans outside observer say robot representation map representation path puck 
dangerous beans distributed map emergent structure extent exists behaviour interaction behavioural modules environment 
similarly dangerous beans control structure path representation 
reinforcement learning com plex emergent structure consists set values cause agent certain choices certain places 
path emergent property choices results interaction distributed map reinforcement learning complex embedded environment 
claim representation path puck exists outside interaction elements simply false 
chapter 
discussion ideas expressed chapter highly speculative research pre sented dissertation despite limitations provided motivation study behaviour reinforcement learning situated learning generally raised interesting questions nature representation relationship learning 
chapter chapter concludes dissertation providing overview research contributions contained significance areas research extended 
contribution contribution dissertation threefold 
introduced model integrates behaviour style robot architecture reinforcement learning 
second detailed development mobile robot uses model learn solve difficult problem real world providing engineering contribution resulting data supports claim model capable learning real time real world 
development fully behaviour layered learning system progress bringing powerful important ideas 
significance learning model robot described dissertation displayed rapid learning intelligent behaviour challenging real world environment retaining full autonomy chapter 
integration behaviour control reinforcement learning 
significant importantly provided basis investigation behaviour reinforcement learning potential generating complex goal directed behaviour rapid learning 
furthermore research described suggests layering learning models geared specif ically problems faced situated agents coupling behaviour methods promising approach development robots exhibit complex adaptive behaviour 
research gone way making case fusion learning models behaviour control 
research dissertation means conclusive extended directions 
reinforcement learning model developed 
involve ways sensibly handle competing demands multiple reinforcement learning complexes development efficient algorithms theoretical results specific problems posed situated learning inclusion priori knowledge problem space principled way 
second model applied domains order verify applicable outside landmark navigation 
examples chapter may places start 
integration types learning reinforcement complex considered 
example social imitation seed state space large solution practical amount time associative learning methods create useful reward functions forth 
ultimate goal investigation classification known learning methods terms information requirements feasibility conditions set guidelines developing complex learning systems autonomous agents 
chapter 
behaviour systems hope moving insect level intelligence incorporate learning mechanisms levels behaviour 
dissertation argued just matter inserting learning models behaviour systems matter understanding required learning feasible real world layer learning models interaction facilitates generation complex behaviour truly integrate learning behaviour systems 
research represents small hopefully concrete step direction 
bibliography agre chapman 

plans 
maes editor new architectures autonomous agents task level decomposition emergent functionality 
mit press cambridge ma 
arkin 

behavior robotics 
mit press cambridge massachusetts 
ballard brown 

computer vision 
prentice hall 
braitenberg 

vehicles experiments synthetic psychology 
mit press cambridge massachusetts 
brooks 

planning just way avoiding figuring 
brooks editor cambrian intelligence early history new ai pages 
mit press cambridge massachusetts 
brooks 

intelligence representation 
haugeland editor mind design ii pages 
mit press cambridge massachusetts 
brooks 

role learning autonomous robots 
proceedings fourth annual workshop computational learning theory colt pages santa cruz ca 
bryson 

modularity specialized learning reexamining behavior artificial intelligence 
proceedings workshop adaptive behavior anticipatory learning systems 
springer 


topological simultaneous localization mapping slam exact localization explicit localization 
ieee transactions robotics automation 
crook hayes 

learning state confusion perceptual aliasing grid world navigation 
proceedings th british conference mobile robotics intelligent mobile robots 


learning hierarchical control structures multiple tasks changing environments 
pfeifer blumberg meyer wilson editors animals animats proceedings fifth international conference simulation adaptive behavior zurich switzerland 
bibliography dudek freedman 

local information non local way mapping graph worlds 
proceedings international joint conference artificial intelligence chambery france 


orientation behavior registered topographic maps 
maes matari meyer pollack wilson editors animals animats proceedings fourth international conference simulation adaptive behavior cape cod ma 
fritzke 

growing neural gas networks learns topologies 
tesauro touretzky leen editors advances neural information processing systems 
mit press cambridge ma usa 
harvey 

artificial evolution adaptive behaviour 
thesis school cognitive computing sciences university sussex 


action selection methods reinforcement learning 
animals animats fourth international conference simulation adaptive behaviour sab cape cod ma usa 
mit press 
team sa 
khepera vision turret user manual 
lausanne switzerland 
team sa 
khepera user manual 
lausanne switzerland 
kohonen 

self organization associative memory 
springer verlag rd edition 
lucas 

tutorial elementary trajectory model differential steering system robot wheel actuators 
rossum sourceforge net papers rossum project 
maes brooks 

learning coordinate behaviors 
proceedings american association artificial intelligence boston ma 
mahadevan connell 

automatic programming behavior robots reinforcement learning 
artificial intelligence 
marsland shapiro nehmzow 

self organising network grows required 
neural networks 
matari 

distributed model mobile robot environment learning navigation 
master thesis mit artificial intelligence laboratory 
matari 

reward functions accelerated learning 
international conference machine learning pages 
matari brooks 

learning distributed map representation navigation behaviors 
brooks editor cambrian intelligence early history new ai 
mit press cambridge massachusetts 
mitchell 

machine learning 
mcgraw hull 
bibliography mor 

dynamic action sequences reinforcement learning 
pfeifer blumberg meyer wilson editors animals animats proceedings fifth international conference simulation adaptive behavior zurich switzerland 
murphy 

ai robotics 
mit press cambridge massachusetts st edition 
pfeifer scheier 

understanding intelligence 
mit press cambridge ma 
mallot 

polarization compass robot navigation 
fifth german workshop artificial life pages 
smart kaelbling 

practical reinforcement learning continuous spaces 
proceedings seventeenth international conference machine learning pages 
smith 

applications self organising map reinforcement learning 
neural networks 
stone 

layered learning multiagent systems winning approach robotic soccer 
mit press 
stone veloso 

layered learning 
proceedings th european conference machine learning pages barcelona spain 
springer berlin 
sutton 

reinforcement learning architectures animats 
meyer wilson editors animals animats proceedings international conference simulation adaptive behavior 
sutton barto 

reinforcement learning 
mit press cambridge ma 
phillips smith 

reinforcement landmark learning 
pfeifer blumberg meyer wilson editors animals animats proceedings fifth international conference simulation adaptive behavior zurich switzerland 
watkins dayan 

learning 
machine learning 
stone 

concurrent layered learning 
proceedings second international joint conference autonomous agents multi agent systems 


