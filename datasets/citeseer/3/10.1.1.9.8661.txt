incremental thin junction trees dynamic bayesian networks frank hutter computer science dept darmstadt university technology darmstadt germany email de incremental thin junction trees general framework approximate inference static dynamic bayesian networks 
framework incrementally builds junction trees representing probability distributions dynamically changing set variables 
variables conditional probability tables introduced junction tree summed approximated splitting clusters computational efficiency 
possible applications general framework automatically generate conditionally independent clusters boyen koller bk algorithm 
theoretical boyen koller showed conditionally independent clusters strongly improves bk error bounds 
show identify conditionally independent clusters automatically theoretical results carry practice 
achieve contract anytime algorithm superior bk marginally independent clusters 
junction tree widely tools exact inference graphical models 
variety tasks simultaneously computing posterior family marginals generating probable configurations 
space time complexity exponential induced width graphical model independence graph applicable larger problems 
propose generalized framework incremental thin junction tree exact approximate inference static dynamic bayesian networks 
incrementally build thin junction tree user defined bound size bounded space time complexity 
size remains smaller times framework performs exact inference 
new variable brenda ng div 
eng 
appl 
sciences harvard university cambridge ma usa eecs harvard edu richard dearden riacs nasa ames research center moffett field ca usa dearden email arc nasa gov render larger thinned splitting large clusters smaller ones connected new separator 
new clusters may subsumed clusters removed significant size reductions possible minimal approximation error 
existing approximate junction tree algorithms build exact junction tree big approximate 
severe drawback due space time constraints may possible build exact place 
framework deals problem allowing approximations construction phase necessary ensure computational feasibility 
get large 
drawback existing approaches approximate junction trees operate fixed static bayesian networks flexible incorporate new variables 
see importance imagine additional sensor installed system simply add variable potential cpt junction tree rebuilding structure :10.1.1.30.63
easily handle dynamically changing domains dynamic bayesian networks 
sample application chose filtering dynamic bayesian networks dbns 
problem boyen koller bk algorithm prominent approach :10.1.1.119.6111
works projecting belief state time step set marginally independent clusters variables bounded expected error times 
unfortunately bounds quite loose practice chosen set clusters determines actual approximation error 
theoretical yielding tighter bounds conditionally independent sets clusters clear choose conditionally independent clusters theoretical bounds carry improved results practice 
framework automatically detect sets conditionally independent clusters subsequent bk 
set clusters induces proto junction tree size dominates bk space time complexity 
find conditionally independent sets clus ters yielding low approximation error bk way achieve contract anytime algorithm conditional bk automatic clustering short cbk ac 
compared manually determined marginally conditionally independent clusters suggested cbk ac yields tenth bk error computation time :10.1.1.119.6111:10.1.1.119.6111
automatically clusterings outperform manually determined ones 
preliminaries bayesian network pair independence graph directed acyclic graph dag node represents random variable edge defines causal relationship variable variable set parents random variable set variables directly depends family fv formed parents 
variable conditional probability distribution cpd defines probability values values parents pa 
view cdps potentials fv family variables 
semantics bayesian network specifies joint proba bility distribution variables factored form fv discrete bayesian networks random variable finite domain dv size potential product domain sizes dx variable dx 
dynamic bayesian network dbn compactly represents dynamically changing joint probability distribution set random variables pair bts bayesian network specifying prior distribution time step bts time slice bayesian network subset variables xt xt specifying evolution dynamics variables 
domain chose demonstration framework filtering problem dbns evidence time step compute belief state xt 
junction trees junction tree widely secondary structure inference graphical models 
junction tree tree structure node represents cluster variables edge connecting clusters represents separator 
cluster associated set variables vc cluster potential analogously separator set variables vs separator potential uppercase random variables lowercase variable instantiations 
bold face sets variables 
separator sk connects clusters ci cj associated set variables intersection ci cj variables vci defining characteristic junction trees running intersection property set variables clus ter ck path clusters ci cj superset vci junction tree called consistent arbitrary sets variables marginals arbitrary clusters ci cj vci coincide normalization constant ci cj vc vc junction tree said normalized iff potentials normalized 
joint system belief joint distribution variables constructing junction tree bayesian network usually done steps see overview :10.1.1.125.4164
junction tree constructed qualitatively clusters formed maximal cliques network moralized triangulated independence graph separators chosen form maximum spanning tree maximal defined terms number variables separator domains 
graphical structure determined number quantitative operations performed details see initialization means initialize entries cluster potentials separator potentials unity :10.1.1.125.4164:10.1.1.125.4164
multiplication potential variables means identify clique vc multiply division potential defined analogously multiplication 
put evidence means set entries potentials zero agree calibration performs local message passing clusters renders consistent 
normalization normalizes potential sum 
marginalizing set variables abbreviated marg means identify cluster consistent normalized junction tree vc return marginal vc inference bayesian network operations follows 
graphical structure junction tree determined outlined 
initialization family potentials fv variable multiplied 
yields joint system belief fv 
available evidence put followed calibration normalization 
yields joint system belief 
posterior marginals subsets cluster domains queried simple marginalization 
correctness approach guaranteed theorem 
theorem correct marginals 
marginalization consistent normalized junction tree subset set variables vc associated cluster yields joint system belief marginalized thin junction trees marg 
junction tree general efficient tool exact computations graphical models 
operations junction tree linear time complexity size unfortunately size grows prohibitively large 
defined sum sizes potentials 
exponential induced width underlying graphical model quickly renders junction tree approach inapplicable larger problems 
concept thin junction trees simply deals problem bounding 
soon junction trees jensen introduced approximation scheme reduces junction tree size setting lowest values potential zero 
kj introduced approximation scheme bayesian networks removed edges network moralized triangulated graph 
showed edge removal relates splitting single large clusters associated junction tree smaller clusters connected separator preserving rest junction tree structure 
resulting smaller clusters may subsumed neighbouring clusters removed substantial size reductions achieved 
thin junction trees simultaneous localization mapping slam 
interesting application specifically tailored slam domain generalize discrete dbns 
purely continuous problem slam cluster size cubic number continuous variables approximation error simple function covariances exact approximate distributions 
slam constrained variables coupled measurement 
reasons approximation easier slam 
proofs theorems companion technical report 
approach employ reduce size junction tree kj 
extend case dynamically changing probability distributions directly operating junction tree 
iteratively detect set possible splits clusters perform termination criterion reached 
definition possible splits 
set possible splits junction tree consists unordered pairs variables share exactly cluster vc 
perform split means remove unique cluster vc consistent junction tree add new clusters cu cv vcu vc vcv vc add new separator vs vc connecting cu cv 
separators neighbouring clusters move cu vd cv 
new clusters potentials cu cv new separator potential new clusters checked subsumption neighbours 
refer operation thinning junction tree 
quantify error introduced possible split kl divergence joint system beliefs represented consistent normalized junction trees split split 
kj proves important facts efficient approximations junction trees kl divergence computed locally additive splitting clusters preserves consistency 
locality kl divergence enables efficiently compute error possible split single cluster potential joint system belief require summing exponentially values 
additivity fact splitting clusters affect clusters conservation consistency enable efficient caching scheme 
best possible split split cluster cu cv yielding new junction tree order compute new set possible splits merely remove splits split cluster put new possible splits cu cv respective subsuming clusters approaches approximate junction trees achieve computational feasibility 
significant disadvantage build exact junction tree approximate problematic exact junction tree large compute place 
section introduce set operations allows incremental construction junction tree overcome problem 
incremental thin junction trees incremental thin junction trees framework exact approximate inference static dynamic bayesian networks 
building junction trees incrementally framework represent probability distributions fixed set changing set dynamic variables 
variables conditional probability tables introduced junction tree summed size kept low possible removing non maximal clusters 
cases approximations feasible split clusters computational efficiency demonstrated section 
operations build ones introduced draper incremental construction qualitative junction trees extend quantitative treatment handling cluster separator potentials :10.1.1.30.63
introduce new cliques means build new junction tree vc vc new clique vc 
initialized multiplied potentials divided introduce variable parents contained shorthand introducing clique family fv multiplying family potential 
subsumption check means check vc vd 
case say subsumed removed separator separators adjacent bend potential multiplied divided separator potential 
merge connected set clusters means delete separators connecting ci cj introduce new cluster vm vc 
neighbours checked subsumption 
sum variable means merge connected set clusters vc new cluster marginalize potential checked subsumption neighbours 
theorem states performing complex changes qualitative junction tree structure correctness posterior marginals need consider joint system belief 
show behaves new set operations 
lemma uniformly extended joint system belief 
new cliques extends joint belief junction tree uniformly new variables lemma unchanged joint system belief 
subsumption checks merging connected set clusters junction tree change joint system belief 
lemma marginalization 
summing variable junction tree joint system belief 
introduced operations thinning abbreviate perform exact inference static dynamic bayesian networks 
introduce new variables cliques conditional probability tables marginalize keeping size means subsumptions 
correctness approach guaranteed theorem 
theorem correctness 
consider initially empty set variables incrementally introduced subset variables summed 
process thinning taken place joint system belief product family potentials fv marginalized fv correct just standard junction tree approach computationally infeasible large problems 
theorem guarantees deal performing approximations 
theorem existence splits 
cluster vc possible split 
clusters maximal iterated approximations eventually lead size reduction terminate cluster variable left 
applied static dynamic bayesian networks 
algorithm potential dbn problems discuss detail section 
static models provide alternative kj method edge removal 
approach special case achieved introducing variables performing approximation 
enables informed approximation exact junction tree prohibitively large infeasible 
counter kj suggests approximating cluster potentials sampling techniques 
feasible secondary approximation technique 
bound space complexity incrementally introduce variables thin junction tree grow large 
approximate junction trees obtained variety tasks large problems exact junction trees computationally infeasible 
example simultaneous approximation posterior marginals approximation probable explanations bayesian network 
boyen koller conditionally independent clusters boyen koller bk algorithm commonly approximate inference dbns :10.1.1.119.6111
approach approximate belief state xt step product marginal distributions subsets variables xt ct ct ct ct partition xt 
intuition dbn describes system consists weakly interacting subsystems represented clusters ct ct error induced ignoring covariances variables different subsystems small due weak interactions subsystems 
accuracy bk strongly sensitive partition finding requires deep domain knowledge manually done experts 
positive side due stochasticity process individual errors introduced time step decay exponentially time expected error bounded times :10.1.1.119.6111
unfortunately error bounds quite loose boyen koller significantly improve conditionally independent clusters 
theory case developed implementation issues empirical results disregarded far 
clarity presentation detail bk algorithm conditionally independent sets clusters conditional bk short cbk 
term clustering refer marginally conditionally independent set clusters cbk 
approximate belief state cbk xt ct ct ct st st st st st separators junction tree clusters ct described propagation belief state time step dbn bts implemented efficiently conditional bk :10.1.1.119.6111:10.1.1.119.6111:10.1.1.119.6111
clustering boyen koller suggest building called proto junction tree 
easily explain construction framework 
purpose fts denote set families bts ts denote associated family potentials 
build proto junction tree clustering time slice bayesian network bts means create new empty junction tree introduce cliques vc ct ct fts 
built qualitatively initialized time slice family potentials ts multiplied 
call proto junction tree induced shows cbk filtering dbn stream observations clustering bts 
input 
clustering dbn bts continuous stream observations 
output 
filter marginals ct clusters 
compute separators 

connect clusters qualitative jt 
init proto potentials 

build proto junction tree bts 
build junction tree 
marg vc 
marg vs propagation time step 




multiply ct 
divide st 
insert new evidence 

calibrate 

marg vc 

marg vs 

cbk algorithm approximate inference dbns 
step achieved maximum algorithm see 
marginally independent clustering omit lines 
separators clustering computed 
proto junction tree constructed priors vc vs computed respectively lines 
time step temporary copy multiplied cluster potentials ct ct ct divided separator potentials st st st lines 
new evidence introduced calibrated lines 
time step cluster separator potentials ct ct ct st st st retrieved querying lines 
note straight forward extension bk algorithm marginally independent clusterings :10.1.1.119.6111
space requirement cbk linear induced proto junction tree size 
dominating factor time complexity calibration linear 
efficient inference aim find clustering induces small proto junction tree 
finer clustering choose bigger error introduced approximation equation face tradeoff time space versus approximation error 
algorithm generalizing cbk possible formulate algorithm strictly generalizes cbk propagating time 
time step introduces new variables xt puts new evidence old vari ables xt approximates remaining junction tree splitting clusters 
approximation time step algorithm equivalent cbk general powerful adaptively choose approximations conditioned observations detect react special events sparse interactions subsystems 
due space restrictions omit details companion technical report 
section employ similar methodology compute performing clusterings flexible faster cbk algorithm 
computing performing clusterings cbk boyen koller clearly state choice clustering significantly influences bk runtime approximation error :10.1.1.119.6111
suggest simply clustering reflecting subsystems complex system represented dbn hand 
approach requires domain knowledge significant trial error researcher side order determine clustering computationally feasible results fairly accurate inference 
simple algorithm automatically computes clusterings cbk perform orders magnitude better best ones determined manually boyen koller :10.1.1.119.6111
fact automatically find better clusterings hand ease cbk new dbns enable intelligent agents improve lower level inference engines automatically 
bound focus problem finding clustering low approximation error cbk induced proto junction tree size pseudo code algorithm ac automatically finds clustering subsequent cbk 
dominates cbk space time complexity yields contract anytime algorithm cbk ac combination cbk 
finding clustering ac samples training sequence evidence dbn bts builds prior belief state represented see line 
time step iteratively greedily split clusters ct st induced proto junction tree size ct smaller equal lines thinning process time step complete move time step introducing variables xt summing old variables xt introducing new evidence calibration lines 
completion number time steps quality possible split determined heuristic case choose split maximizing ratio reduction induced tree size approximation error measured kl divergence 
greedy heuristic motivated objective achieve large size reductions introducing small approximation error 
bts input 
dbn bts bound output 
clustering cbk sample training evidence init 

sample train bts 
build thin induced proto jt small 


ct st 
ct qualitative proto junction tree ct bts 
ct bound 
ct st thin 
ct qual 
proto junction tree ct bts 
move time step 

introduce new variables xt 
sum variables xt 
introduce evidence 
calibrate 
choose best performing clustering 

sample bts 
ct 
best accuracy filter marginals cbk ct bts 
finding conditionally independent cliques bk framework 
experiments parameters set respectively 
sample new sequence observations evaluate clusterings 

clustering ct best accuracy cbk ct returned line 
search clustering ac performed offline 
online filtering algorithm cbk arbitrarily long sequences time searching clustering quickly amortizes 
precomputing various clusterings different maximal proto junction tree sizes yields contract anytime algorithm online filtering conditional bk automatic clustering cbk ac 
cbk ac increase computational resources allows usage larger proto junction tree turn lets set precomputed clusters higher induced proto junction tree size resulting lower approximation error 
remains problem large dbns bts bts densely connected variables high induced width induced proto junction tree ff fully factored clustering variable cluster large :10.1.1.29.3738:10.1.1.29.3738
anytime approach reaches natural border thin fully factored clustering 
cbk ac current version completely anytime 
address thinning proto junction tree directly merely thinning junction tree inducing way achieve true anytime algorithm 
experiments section report results cbk ac task filtering bat water dbns networks boyen koller :10.1.1.119.6111
networks compare cbk ac bk best manually determined sets clusters reported error measure employ maximal error filter marginals time step :10.1.1.119.6111
dbn ran cbk ac number different bounds induced proto junction tree size 
clustering obtained water network shows induced proto junction tree size average errors cbk ac filter marginals 
compare standard bk fully factored ff clusters best manually determined marginally independent set clusters bk conditionally independent set clusters cond suggested :10.1.1.119.6111
bk just single large cluster exact yield proto junction tree size machines ran memory clustering 
table list induced proto junction tree size average error filter marginals run time algorithms water network 
induced proto junction tree size clustering dominates time complexity cbk larger development errors cbk ac filter marginals time shown 
follows pattern observed errors quite low time spikes :10.1.1.119.6111
variance marginal errors quite high clusterings bk ff 
cbk ac error spikes similar bk average yields approximation errors orders magnitude better standard bk variants comparable complexity 
bat network achieve similar results 
table shows clustering cbk ac size induced proto junction tree average error filter approach far approximate filter estimate xt exact transition function xt xt matter beneficial terms induced proto junction tree size approximate transition function 
thinning proto junction tree directly achieve approximation filter estimate transition function 
algorithm free choose promising approximations 
xavier boyen providing original water bat dbns detailed description networks pointers origin :10.1.1.119.6111
code written matlab available www de html 
standard functionality kevin murphy bayes net toolbox :10.1.1.25.1216
experiments done ghz pentium laptop mb ram running windows xp 
algorithm size error time bk ff bk bk bk cond bk exact cbk ac cbk ac cbk ac cbk ac cbk ac cbk ac cbk ac cbk ac cbk ac table size induced proto junction tree average error filter marginals wall clock time various online algorithms water network 
bk exact ran memory machines 
algorithm size error time bk ff bk bk bk exact cbk ac cbk ac cbk ac cbk ac cbk ac cbk ac cbk ac cbk ac cbk ac table table bat network 
marginals run time 
note clustering cbk ac induces proto junction tree smaller ff proto junction tree induced fully factored clustering 
possible employ standard software implementing min weight heuristic see hard problem finding minimal junction tree 
luck find clustering yields small proto junction tree heuristic 
cbk ac yields lower error bk ff 
ff necessarily places variables shared clusters approximating joint distribution product marginals done bk ff yield advantage computation time space merely introduces unnecessary error 
superiority automatically clusters demonstrated cbk ac yields lower error bk bk smaller induced proto junction tree size introduced general framework incremental thin junction trees exact approximate inference bayesian networks demonstrated filtering problem dbns 
problem errors various engines proto junction tree size bk cbk ac average error water domain maximal error time step bk ff bk bk bk cond cbk ac time step maximal error water domain maximal error time step bk ff bk bk cbk ac time step maximal error bat domain error filter estimates cbk ac bk water network averaged time steps 
bk clusterings ff bk conditionally independent clustering cond suggested :10.1.1.119.6111
bk exact induced proto junction tree size runs memory 
maximal error filter marginal time step cbk ac bk water bat domains 
number parenthesis gives size clustering induced proto junction tree 
explicitly stated conditional boyen koller algorithm cbk showed generalize cbk automatically find conditionally independent clusterings low error induced tree bounded size 
water bat dbns demonstrated empirically cbk ac shows significantly lower approximation errors standard bk equal computational complexity 
framework applied number areas 
build approximate junction trees large bayesian networks 
filtering task showed incorporate evidence choice approximation time step adapt situation hand 
enables detect special circumstances sparse interactions variables naturally 
framework applicable online inference need reduce complexity 
plan research approximation schemes estimate error splitting cluster speed new variables means draper operations incremental building qualitative junction trees :10.1.1.30.63
interesting point theoretical analysis cbk apply directly framework split clusters time steps 
conjecture theoretical properties similar needs research 
possibilities improve extend cbk ac algorithm 
discussed section contract anytime property naturally limited size induced proto junction tree ff fully factored clustering 
dbns ff prohibitively large building approximate proto junction tree achieve full contract anytime property :10.1.1.29.3738:10.1.1.29.3738
possible runtime switch clusterings computed offline 
time space constraints online inference change clustering adapted reflect 
furthermore assess performance different clusterings regular intervals online algorithm reactive scheme possible 
boyen koller :10.1.1.119.6111
tractable inference complex stochastic processes 
proc 
uai pages 
boyen koller 
exploiting architecture dynamic systems 
proc 
uai pages 
cowell dawid lauritzen spiegelhalter 
probabilistic networks expert systems 
statistics engineering information science 
springer 
draper :10.1.1.30.63
clustering thinking triangulation 
besnard hanks editors proc 
uai pages 
huang darwiche 
inference belief networks procedural guide 
international journal approximate reasoning 
hutter ng dearden 
incremental thin junction trees dynamic bayesian networks 
technical report group darmstadt university technology germany 
submitted 
preliminary version www de pdf 
jensen andersen 
approximations bayesian belief universes knowledge systems 
technical report institute electronic systems aalborg university aalborg denmark 
kj 
reduction computational complexity bayesian networks removal weak dependences 
proc 
uai pages 
lauritzen spiegelhalter 
local computations probabilities graphical structures application expert systems 
journal royal statistical society series 
murphy 
bayes net toolbox matlab 
computing science statistics 
murphy weiss 
factored frontier algorithm approximate inference dbns 
proc 
uai pages 

thin junction tree filters simultaneous localization mapping 
proc 
ijcai pages 
