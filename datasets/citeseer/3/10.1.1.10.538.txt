measures applications lexical distributional similarity julie elizabeth weeds submitted degree phil 
university sussex th september declaration declare thesis submitted different form university degree 
signature mum julie elizabeth weeds september measures applications lexical distributional similarity julie elizabeth weeds summary thesis concerned measurement application lexical distributional similarity 
words said distributionally similar appear similar contexts 
loose definition led measures proposed adopted fields geometry statistics information retrieval ir information theory 
aim investigate properties measure lexical distributional similarity 
start introducing concept lexical distributional similarity 
discuss potential applications roughly divided distributional language modelling applications semantic applications methods evaluation chapter 
look existing measures distributional similarity carry empirical comparison fifteen measures paying particular attention effects word frequency chapter 
propose new general framework distributional similarity concept lexical substitutability measure ir concepts precision recall 
framework allows investigate key factors similarity asymmetry relative influence different contexts extent words share context chapter 
consider application distributional similarity language modelling chapter predictor semantic similarity human judgements similarity spelling correction task chapter 
submitted degree phil 
university sussex th september years ago friends colleagues surprised quit city job just year start career research 
decision easy 
wanted spend life doing things enjoyed 
years decision happy 
realise lucky am able say thoroughly enjoyed experience 
course largely due people influence life studies course years 
foremost supervisor david weir guidance inspiration 
weekly meetings undoubtedly key keeping positive begun doubt 
second engineering physical sciences research council possible 
number nlp people variously sussex brighton edinburgh degree committee adam kilgarriff bill keller thoughtful input john carroll providing raw data experiments darren pearce fellow student friend latex guru support mark mclauchlan diana mccarthy john carroll rudi lutz chong ha stephen clark james curran helpful discussions 
possible support family friends 
particular mum pauline weeds supported choices 
members family friends brighton cambridge university sussex club combined ability distract probably kept sane 
bibliographic notes portions research thesis published form conference papers written david weir 
initial form occurrence retrieval framework described chapter conference empirical methods natural language processing weeds weir 
evaluation existing similarity measures distributional semantic applications second international conference corpus linguistics weeds weir 
smoothing section described weeds preliminary pseudo disambiguation experiments chapters described weeds 
contents background lexical distributional similarity 
applications 
classification applications 
language modelling 
prepositional phrase attachment ambiguity resolution 
conjunction scope identification 
word sense disambiguation 
word sense separation 
metonymy resolution 
learning countability preferences 
automatic thesaurus generation 
information retrieval 
anaphor resolution 
collocation extraction 
text simplification 
compound noun interpretation 
topic identification 
spelling correction 
clustering 
evaluation distributional similarity measures 
evaluation gold standard 
evaluation human plausibility judgements 
evaluation unseen data 
application evaluation tasks 
pseudo disambiguation tasks 
wordnet prediction task 

existing measures distributional similarity theory 
geometric measures 
correlation coefficients 
combinatorial measures 
substitutability measures 
contents mutual information measures 
empirical comparison measures 
data 
cross comparison neighbours derived different measures high low frequency neighbours 

general framework distributional similarity occurrence retrieval 
additive models 
difference weighted models 
combining precision recall 

neighbours selected occurrence retrieval models 
neighbours hope 
relationship parameter settings frequency neighbours selected 

characterisations existing measures 
norm 
norm 
cosine measure 
kendall coefficient 
dice jaccard coefficient 
jensen shannon divergence measure 
skew divergence measure 
confusion probability 
hindle measure 
lin mi measure 
mi variant jaccard coefficient 

language modelling deriving probability distributions 

smoothing algorithm 
evaluation 
results 

pseudo disambiguation experiments 
experimental set 
results 
correlation pseudo disambiguation results smoothing results contents discussion 

predicting semantic similarity correlation human synonymy judgements 
methodology 
results 
discussion 
comparison wordnet 
neighbour set comparisons 
hyponyms vs hyperonyms 
spelling correction task 
motivation 
methodology 
results 
discussion 

directions summary 
chapter 
chapter 
chapter 
chapter 
chapter 
chapter 
major contributions 
major directions 
index sets nouns experiments high frequency nouns 
low frequency nouns 
list figures vector diagram bus dog 
geometric distance metrics 
cosine measure 
positive correlation bus train 
correlation bus dog 
venn diagram similarity 
performance neighbour sets different test sets 
tendencies additive type crm select high frequency neighbours high low frequency target nouns 
tendencies additive token crm select high frequency neighbours high low frequency target nouns 
tendencies additive mi crm select high frequency neighbours high low frequency target nouns 
tendencies difference weighted type crm select high frequency neigh high low frequency target nouns 
tendencies difference weighted mi model select high frequency neigh high low frequency target nouns 
variation parameters mean similarity neighbour sets additive type crm sim 
variation parameters mean similarity neighbour sets additive type crm sim 
variation parameters mean similarity neighbour sets additive token crm dist 
variation parameters mean similarity neighbour sets additive token crm dist 
variation parameters mean similarity neighbour sets additive token crm 
variation parameters mean similarity neighbour sets difference weighted mi crm 
variation parameters mean similarity neighbour sets difference weighted mi crm 
variation parameters mean similarity neighbour sets additive mi crm simlin 
variation parameters mean similarity neighbour sets additive mi crm sim 
list figures algorithm smoothing occurrence distribution target word nearest neighbours 
distance half data set data set plotted word frequency 
results stopping condition language modelling task 
comparison error rates vote neighbour frequency weighted voting schemes 
illustrating cross validated optimal error rates measure optimised 
performance existing similarity measures respect 
performance crms respect optimal values 
performance crms respect optimal values 
performance crms respect optimal values 
illustrating cross validated optimal error rates measure optimised 
performance existing similarity measures respect 
performance crms respect optimal values 
performance crms respect optimal values 
performance crms respect optimal values 
correlation data additive type crm 
correlation data additive token crm 
correlation data additive mi crm 
correlation data difference weighted type model 
correlation data difference weighted mi model 
scatter graph wordnet measure 
scatter graph additive mi model 
scatter graph skew divergence measure dist 
illustrating correlation wordnet measure 
variation correlation wordnet respect additive type crm 
variation correlation wordnet respect additive token crm 
variation correlation wordnet respect additive mi crm 
variation correlation wordnet respect difference weighted type crm 
variation correlation wordnet respect difference weighted mi crm 
frequency bnc data depth wordnet nouns 
variation spelling correction task respect parameters scope 
list figures variation precision recall number nearest neighbours consid ered varied scope words 
variation spelling correction task respect parameters scope simlin 
variation score spelling correction task respect parameters scope additive mi crm 
variation score spelling correction task respect parameters additive mi crm scope 
list tables classification potential applications distributional similarity measures ac cording primarily distributional information se mantic information 
occurrence information bus dog 
sizes intersections feature sets 
fifteen similarity measures 
accuracy rasp 
neighbours hope associated similarity scores similarity measure 
mean cross measure correlation neighbour sets high frequency nouns 
fig ures brackets standard deviations 
mean cross measure correlation neighbour sets low frequency nouns 
fig ures brackets standard deviations 
average value high frequency target nouns low fre quency target nouns 
values indicate measure biased selecting high frequency nouns neighbours target nouns values indicate measure biased selecting low frequency nouns neighbours target nouns 
measures grouped tendency select high frequency low frequency similar frequency neighbours 
contingency table showing relationship distribution width frequency frequent nouns 
error rates pseudo disambiguation task measures true neighbours restricted neighbour sets 
figures brackets values neighbourhood size minimum error rates 
error rates pseudo disambiguation task dist dist measures true neighbours frequent neighbours 
figures brackets values minimum error rates 
table special values 
neighbours hope additive type crm 
neighbours hope additive token crm 
neighbours hope additive mi crm 
neighbours hope difference weighted type crm 
neighbours hope difference weighted token crm 
parame ter settings irrelevant model precision equal recall 
list tables neighbours hope difference weighted mi crm 
summary tendencies select high low frequency neighbours models framework 
summary observations existing measures cr frame 
mean optimal similarities crms distl figures brackets standard deviations 
mean optimal similarities crms 
figures brackets stan dard deviations 
mean optimal similarities crms sim 
figures brackets stan dard deviations 
mean optimal similarities crms dist js 
figures brackets stan dard deviations 
mean optimal similarities crms dist 
figures brackets stan dard deviations 
mean optimal similarities crms dist 
figures brackets stan dard deviations 
mean optimal similarities crms 
figures brackets stan dard deviations 
mean optimal similarities crms figures brackets standard deviations 
mean optimal similarities crms sim figures brackets standard deviations 
comparison stopping conditions baselines language modelling 
mean optimal error rates fold cross validation optimising 
figures brackets standard deviations optimal error rates folds 
summary results pseudo disambiguation task optimising number nearest neighbours 
mean optimal error rates fold cross validation optimising 
figures brackets standard deviations 
summary results pseudo disambiguation task optimising simi larity threshold 
correlations wordnet similarity measures data accord ing budanitsky hirst jarmasz szpakowicz 
comparison existing similarity measures data 
comparison crms data 
frequencies precision recall values additive mi model noun pairs data 
mean correlation distributional neighbour sets wordnet derived neigh sets 
figures brackets standard deviations 
list tables proportion pairs recall retrieval greater precision 
proportion neighbour sets size 
notation isa denote hyperonyms 
chapter thesis study measures lexical distributional similarity applications 
particular investigates characteristics measure useful different applications 
earlier research lexical distributional similarity defined fairly loosely 
words said distributionally similar appear similar contexts 
definition gives rise number questions 

context 

contexts equally important 

matter extent word occurs context 
discussed chapter number earlier researchers hindle pereira tishby lee lin lee define context terms grammatical depen dency relations :10.1.1.15.227:10.1.1.11.31
syntactic view context see distributional similarity thought terms lexical substitutability church gale hanks hindle moon distributional similarity words extent inter substituted changing plausibility sentence 
concept substitution inherently asymmetric 
possible measure appropriateness substituting word word independent measuring appropriateness substituting word word similarity defined terms inter substitutability ask inherent asymmetry substitution exploited asymmetric measure distributional similarity 
idea lexical substitutability highlights relationship distributional sim ilarity semantic similarity 
semantic similarity thought degree synonymy exists words extent inter substituted changing meaning sentence 
appear distributional similarity weaker requirement semantic similarity sentences plausible mean ing thing mean thing plausible 
words chapter 
semantic similarity logically implies distributional similarity distributional similarity logically imply semantic similarity 
hypothesis semantic similarity predicted distributional similar ity led automatic thesaurus generation hindle grefenstette lin curran moens 
results usually encouraging rarely conclusive due large number available choices distributional similarity measure selection evaluation 
order explore characteristics useful measure distributional similarity turn applications 
applications roughly divided property distributional similarity directly language modelling predictor semantic similarity automatic thesaurus generation 
hypothesis single measure distributional similarity able perform application areas due inherent asymmetry concept substitutability 
asked semantically related word dog say animal animal generally place dog say dog animal dog generally place animal 
preference direction relationship words necessarily maintained considers distributional task language modelling face sparse data 
want learn contexts animal occur look occurrences words dog know dog generally replaced animal 
want learn contexts dog occur look occurrences animal know animal occur contexts dog 
corollary expect find corpus derived distributionally similar words useful manually derived semantically similar words tasks classify distributional 
conversely manually derived semantically similar words perform better automatically generated sets similar words semantic tasks distributional similarity measure automatic generation process optimised distributional tasks 
second hypothesis sufficient know words occur partic ular context 
occurring distinct contexts far important occurring repeatedly indistinct contexts 
seen dog occur subject bark subsequent occurrences dog subject bark tell little dog words similar 
result expect observe incorporating difference ex tent word occurs shared context improve results 
say frequency information useless 
verbs tell arguments bark tells subject estimated distributional information 
hypothesise measures give weight informative contexts perform better measures give weight frequently occurring measures 
having outlined major hypotheses chapter chapter summary remainder thesis 
chapter background chapter background 
consider means words distributionally similar 
discuss different types context chapter 
orders affinities words relationship distributional semantic sim ilarity 
consider potential applications distributional similarity techniques natural language processing nlp 
large number applications tend exploit principles 
principle similar words tend occur similar contexts probability estimates formed classes clusters similar words 
second principle naturally occurring texts tend contain large numbers semantically re lated words 
discuss possible approaches evaluating similarity scores 
consider evaluation gold standard evaluation human plausibility judgements evaluation unseen data evaluation application tasks 
chapter existing measures distributional similarity chapter discuss broad range existing distributional similarity measures proposed adopted nlp 
theoretical basis compare measures terms neigh produce data 
particular examine tendencies select high low frequency words neighbours 
knowledge tendencies allow compute neighbour sets efficiently 
may give insights types tasks particular measure useful 
chapter general framework distributional similarity chapter core thesis 
chapter new general framework distributional similarity 
framework similarity viewed measure appropriateness noun distribution place inherently asymmetric 
measure appropriate word word independently appropriate word word word distribution place appropriateness viewed distribution predicts retrieves occurrences second 
way measurement distributional similarity cast occurrence retrieval cr problem measure precision recall analogy way measured document retrieval 
combine precision recall scores parameters tuned particular application 
define number different models occurrence retrieval allow explore systematically number issues distributional similarity 
explore rela tive importance different contexts type token mi models occurrence retrieval 
additive difference weighted versions model provided allow investigate considering difference extent word oc context outperforms simpler models contexts simply shared non shared 
having laid theoretical basis framework compare existing mea sures distributional similarity discussed preceding chapter 
examine cies different models parameter settings select high low frequency words 
second attempt characterise existing measures terms framework parameter settings cr characteristics 
chapter language modelling chapter consider application distributional similarity measures language modelling 
smoothing probability estimates occurrences nearest neighbours intuitively plausible way overcoming sparse data problem providing reliable probability estimates 
demonstrate chapter 
feasible thing deriving smoothed occurrence probability distributions distributional similarity measure considering similarity unseen empirical distribu tions 
compare occurrence retrieval models existing distributional similarity measures terms results pseudo disambiguation experiment 
show differences performance pseudo disambiguation experiment reflected differ ences performance original evaluation unseen empirical distributions 
chapter predicting semantic similarity chapter consider ability distribu tional similarity predict semantic similarity 
different ways 
correlate corpus derived distributional similarity judgements human synonymy judgements 
second compare neighbour sets produced distributional similarity measures neighbour sets de rived wordnet fellbaum 
third examine relationship direction asymmetric distributional similarity framework direction semantic similarity defined relation hyponymy wordnet 
fourth consider performance semantic distributional similarity measures spelling correction task 
chapter chapter number directions 
chapter background chapter background 
consider means words distributionally similar potential applications distributional similarity techniques number possible approaches evaluating similarity scores 
lexical distributional similarity fairly loose definition lexical distributional similarity words distributionally similar appear similar contexts 
definition keeping tradition shall know word keeps firth 
word described terms keeps contexts occurs measure similarity determine similarity descriptions 
definition corresponds grefenstette terms second order affinity words 
words order affinity appear context 
second order affinity exists words tend order affinities 
grefenstette defines third order affinity existing words similar second order affinities 
illustrates difference second order affinities example spelling variations tumor tumour low order affinity high second order affinity 
result surprising definition synonymy words absolute synonyms inter substituted possible contexts changing meaning 
definition dates back von leibniz said quorum philosophy applied language quine lyons church things consideration words truth referred meaning sentence 
definition see words inter substituted possible context changing meaning synonyms 
distributional similarity tests things identical substituted affecting truth 
chapter 
background inter substitution words contexts leads plausible sentences test truth conditions meaning sentence changed 
distributional equivalence maximal distributional similarity necessary condition syn appear sufficient condition synonymy 
difficult see distributional similarity sufficient condition synonymy synonyms words tend similar contexts 
distributionally similar words lin antonyms rise fall 
linguistic relation tends lead high distributional similarity hyponymy relation 
hyponymy con sidered formalisation isa relation 
car isa vehicle hyponym vehicle 
conversely super ordinate term vehicle referred hyperonym hypernym car 
strict definition hyponymy sparck jones attempt exclude relation ships factual knowledge world 
sparck jones argues relationships world knowledge linguistic nature 
noted resnik wordnet miller beckwith fellbaum gross miller uses hyponymy main organising principles considers fruit hyponym 
type hyponymy seen distributionally similar words distributional similarity usage world 
order affinities lie domain knowledge 
example occurrence words ball net sentence sports report tells world live meanings words ball net 
difference second order affinities discussed kilgarriff 
distinction looser tighter 
looser roget roget include words order affinities tighter wordnet fellbaum include words second order affinities 
wordnet organised linguistic principles synonymy hyponymy antonymy 
background place consider definition distributional similarity detail 
said words distributionally similar appear similar contexts 
raises number questions 
mean context 
second multiple occurrences word distinct context counted separately occurrences word context tell word single occurrence word context 
third contexts equally important 
consider questions turn 
context modelled number different levels 
example consider context word document occurs gram bag words side words grammatical dependency 
types context fairly similar differ mainly amount context considered 
example document considered large bag words 
example technique uses level context latent semantic indexing lsi deerwester dumais furnas landauer harshman 
lsi document database described vector words terms contains word term described vector documents occurs 
determine words terms similar respect documents plausible sentence refer sentence observed real language data 
chapter 
background occur retrieve documents relevant query contain words query 
grammatical dependency relations determining context requires notated text leads significantly different results types context 
kilgarriff note second order automatic thesaurus uses grammatical relation data determine context lin tends generate sets words occur semantic class :10.1.1.15.227
thesaurus tighter thesaurus explains lin thesaurus entries akin wordnet roget 
conversely technique lsi uses document level context provides classification akin roget 
reason bag words level context leads looser thesaurus entries grammatical dependency relations context seen considering example emma handsome clever rich comfortable home happy disposition unite best existence lived nearly years world little distress vex 
austen considering evidence start sentence see adjectives handsome clever rich occur modify emma 
model context words side word find handsome clever share context words emma rich find handsome share context words emma clever 
expect occurrences appear contexts handsome similarity diminish evidence seen emma referred handsome 
case fact remains amount second order affinity established words order affinities bag words model context 
way overcoming considering bigrams single word side context 
limited context informative 
example contexts rich tell word rich 
alternatively consider grammatical dependency relations 
knowing handsome clever rich modify emma lead certain degree similarity adjectives lead emma phrase emma similar 
words exhibit similarity things described handsome clever rich novel 
grammatical relation data model context new 
claim distribu tional hypothesis harris meaning entities meaning grammatical relations related restriction combinations entities relative entities 
distributional hypothesis subject debate 
meaning dictate behaviour behaviour determine meaning 
keil argues children learn hierarchical structure nominal concepts observing predicated level 
levin takes opposite approach shows verb participation alternations predicted semantic class 
rubenstein goodenough chapter 
background miller charles investigate link behaviour meaning correlating distributional similarity scores human synonymy judgements 
probably hindle able demonstrate plausibility distributional hypothesis 
previous similar research hirschman grishman sager limited amount grammatically annotated text processed 
robust syntactic analyzer hindle extract grammatical relation data word sample associated press news stories measure similarity mutual information hindle able show plausibility deriving semantic relatedness distribution syntactic forms 
large number subsequent researchers take similar approach grammatical relation data model context 
example pereira 
propose noun clusters distributional similarity smoothing 
large number subsequent publications lee dagan lee pereira lee lee pereira lee ex tend :10.1.1.136.3516
lee pereira example investigates different clustering techniques 
lin automatically generates thesaurus grammatical relation data 
caraballo takes similar approach generate initial clusters nouns attempts deter mine hyponymy relations clusters corpus data 
large body attempts derive levin verb classes levin distributional data 
dorr jones cluster verbs semantically alternation behaviour levin 
lapata brew syntactic frame information disambiguate verbs occur levin classes 
stevenson merlo induce semantic classification verbs alternations 
schulte im walde clusters german verbs semantically subcategorization selectional preference information derived grammatical relation data 
thirteen years debate level context 
years different levels context shown useful different applications 
example clark uses just preceding word context order automatically induce syntactic categories 
captures certain intuitions word order determiners pre nouns 
application grammatical relation data producing grammatical relation data syntactic categories clark attempting induce 
vector space models bag words level context shown ful information retrieval tasks salton wang yang expect extra associations captured level context useful topic identification 
pad lapata concept dependency path show vector space model ical relation data differentiate order affinity association relations conceptual association phrasal association second order affinity taxonomy relations synonymy hyponymy category ordination antonymy providing evidence differences types relation 
may heard debate context representation distinctions bag words level grammatical relations level fairly understood 
accordingly look learnt grammatical relation data 
concern learnt syntactically substitutable words 
chapter 
background starting point focus second questions summarised descriptions words occurrences words certain ical relations calculate similarity words multitude measures similarity adopted proposed purpose shall look detail chapter 
previous evaluating number measures 
lee uses pseudo disambiguation task evaluate measures 
lin considers measures comparison neighbour sets derived wordnet roget 
curran moens compare different measures differ ent weight functions evaluation combination macquarie bernard moby ward roget roget 
interested finding best similarity measure little done classifying measures 
classification lee considers influence information intersection supports distributions incorporated 
lee measures focus intersection occurrences words share better measures take account information occurrences outside intersection 
attempt answer questions calculation distribu tional similarity 
shared context worth regardless difference extent word appears context 
words multiple occurrences context treated distinct 
argument single occurrence words sufficient evidence say words occur 
single occurrence words animal bark may considered sufficient evidence say animal replace word dog context bark regardless number times dog bark occurred 
counter argument substitution word similar probability context may better substitution 
second contexts treated equally contexts important oth ers 
may frequently occurring contexts considered important 
example single occurrence animal bark may sufficient worry substitute animal occur context bark 
alternatively may contexts occur smaller number concepts important 
example animal may occur frequently context true majority nouns 
verbs weak resnik applied different words concepts 
verbs tell behaviour meaning animal consider giving weight similarity calculation 
questions concerned frequency information different contexts utilised measures 
chapter consider existing similarity measures utilise frequency information 
chapter general framework explore issues systematically 
possibly important issue address probably received attention literature 
vast majority existing similarity measures assume similarity symmetric property 
hypothesis similarity presence noise may want raise required number occurrences threshold value greater 
chapter 
background viewed asymmetric word similar word word similar word viewing similarity measure substitutability may appropriate substitute word word appropriate substitute word word example animal may substitute dog dog may substitute animal dog night 
animal night 
animal 
dog 
asymmetry similarity raised lee justification skew divergence measure asymmetric kullback leibler relative entropy measure 
lee explore fully asymmetry exploited 
consider symmetry existing measures chapter inherently asymmetric frame chapter 
far considered number intuitions distributional similarity cal investigated rest thesis 
turn attention question want calculate distributional similarity 
answer question motivates distributional similarity provide insights measurement evaluation 
applications potential applications distributional similarity measures 
section survey classify fifteen potential applications ranging prepositional phrase attachment ambiguity resolution information retrieval 
shall discuss classification appli cations roughly divides distributional syntactic applications semantic applications 
consider application turn discussing distributional simi larity information utilised 
shall briefly discuss clustering techniques topic relevant applications 
classification applications fifteen potential applications distributional similarity measures roughly divided categories property distributional similarity directly property distributional similarity predictor semantic similarity 
words fall syntactic domain semantic domain 
classification potential applications shown table 
application category general application characterises category 
applications specific distributional similarity information paradigm characterising application 
applications classify distributional previously tackled ing semantic class information may thought semantic applications 
example resnik generalises lexical preferences prepositional phrase attach ment problem semantic classes wordnet 
resnik gives number reasons chapter 
background distributional applications semantic applications language modelling section automatic thesaurus generation section pp attachment ambiguity resolution section information retrieval section conjunction scope identification section anaphor resolution section word sense disambiguation section collocation extraction section word sense separation section text simplification section metonymy resolution section compound noun interpretation section learning countability preferences section topic identification section spelling correction section spelling correction section table classification potential applications distributional similarity measures primarily distributional information semantic information distributional similarity classes task 
argues distributional similarity classes expensive compute symbolic labels clear semantics focus word forms senses fitted particular corpus 
tasks classify distributional counter arguments follows 
increases hardware capability mean computational power able compute distributional similarity classes large corpora british national corpus bnc 

necessary group class objects name know similar exploit information 
example knowledge cats similar dogs knowing part larger class known animals mammals pets 

applications classify distributional semantic class information predict distributional information 
accordingly obvious class need clear semantics order able 
objects class behave way distributionally modified prepositions matter mean similar things 

conflation word senses corpus data may problem distributional techniques 
sense tagged corpus data distributional techniques word senses word forms 
hand sense tagged corpus data exploit solve structural ambiguities prepositional phrase attachments 
accordingly probabilities word forms word senses may fact useful 

domain dialect specific information gained deriving distributional classes particular corpus beneficial corpus representative sentences disambiguated 
consequently tasks semantic classes previously predict distri information reason better dis similarity classes 
tasks classify semantic ones rely distributional hypothesis semantics predicted syntax 
cases expect technique uses semantic classes better uses distributional classes 
distributional hypothesis true able gain advantage distributional techniques domain dialect specific corpora 
discuss potential application turn starting distributional applications 
language modelling chapter 
background language models essential statistical natural language processing techniques 
example speech recognition likelihood current word depends signal prior probability word 
prior probability word expectation hearing word depends experience language 
may global knowledge words local contextual knowledge current context word 
statistical parsing probabilities alternative parses depend prior probabilities constituent constructions expectation seeing construction 
expectation depends global contextual knowledge construc tions 
similar techniques applied address ambiguity nearly area natural language processing 
cases language model required define prior probabilities events occurrences events 
example gram model speech recognition defines conditional probability word previous words 
problem acquire language model 
language infinite possible write possible combination words calculate probability uttered written 
problem exacerbated zipfian nature text zipf 
particular corpus small number events occur frequently large number events larger number occurrences events seen 
words max imum likelihood estimates basis corpus data estimate probabilities seen events expense unseen events 
statistical technique uses language model assigns zero probability unseen events rule correct parse interpretation utterance impossible eventually occur 
individual unseen event may small probability occurring cumulative effect unseen events greater 
example brown dellapietra desouza lai mercer report expect word triples new english text unseen training corpus english words 
consequently smoothing techniques katz laplace gale sampson chen goodman friedman singer overcome sparse data problem focus attention language modelling 
probably simplest approach smoothing na bayesian estimation known com add smoothing 
add smoothing overcomes zero probability problem assuming configuration events word triple seen training data considered 
approach tends assign probability mass unseen events add 
combined techniques simple turing estimation gale sampson estimates amount probability mass assigned unseen events number hapax events occur data 
technique allows possibility unseen events occurring allow disambiguate assigned probability 
back smoothing katz comes making independence assumption probability unseen occurrence proportional probability individual events 
ways counter intuitive 
common words seen occur evidence occur 
distributions rarer chapter 
background words seen confidence distributions subject greater change 
similarity smoothing hindle brown dagan marcus markovitch pereira dagan offers intuitively appealing alternative 
probabilities unseen occurrences estimated probabilities seen occurrences distributionally similar words 
assumption approach words seen share occurrences share occurrences 
prepositional phrase attachment ambiguity resolution prepositional phrase attachment problem discussed detail resnik 
task decide prepositional phrase attaches verb noun 
early hindle rooth lexical preference relationships task concentrated proba bility preposition verb probability preposition noun 
example mary liked house country 
mary visited house john 
sentence prepositional phrase clearly attaches noun phrase house 
judgement intuition language model tells house liked 
second sentence prepositional phrase clearly attaches verb prepositional phrase tells accompanied mary visit 
accordingly expect able determine considering probability preposition verb greater probability preposition direct object verb 
discussed hindle rooth resnik prepositional object plays part 
imagine say mary visited house gold roof 
case house gold roof mary visited 
try build statistical model basis lexical events faced extremely sparse data 
large corpus seen mention house roof normally assumed house roof explicit mention 
seen say single house roof know really visiting roof smoothing tells assume impossible 
classifiers pp attachment problem techniques maximum entropy ratnaparkhi reynar roukos decision lists collins brooks boosting abney schapire singer 
techniques overcome data sparseness problem making decisions frequently occurring features preposition absence finely tuned features 
alternative approach resnik clark weir induce probability distribu tions semantic classes lexical items 
example house type building chapter 
background seeing occurrence type building roof cottage roof may strengthen belief prepositional phrase attaches noun instance 
similarly consider semantic class verb prepositional object 
ex ample john human visit motion verb 
accumulating occurrences semantic classes reliable probability estimates formed 
techniques semantic information predict essentially distri information 
predicting words occur basis clusters semantically similar words predict words occur basis cluster words tend occur words 
nagao report state art performance pp attachment ambiguity resolution problem semantic dictionary techniques semantic information successful 
clark weir li abe wordnet techniques smoothing report performance levels 
li shows distributional tech nique outperforms wordnet technique terms accuracy terms coverage 
li reason increase accuracy due domain dependent nature corpus wall street journal reason lack coverage due size corpus sentences 
combining automatically constructed word classes wordnet achieve level performance nagao 
pan tel lin report performance unsupervised approach prepositional phrase attachment distributional similarity techniques 
significantly outperforms unsupervised techniques drawing close state art supervised techniques 
conjunction scope identification form structural ambiguity tackled similarity methods con junction scope 
resnik investigates noun form noun noun noun john business marketing major 
john athlete economics major 
kurohashi nagao note form meaning play role determining con joinability 
example plurality nouns conjoined match 
examples plurality nouns 
cases semantic similarity conjoined heads determines correct structure business marketing economics academic subjects athlete major words people 
semantic similarity form similarity resnik finds coverage increases whilst accuracy drops set noun phrases extracted wall street journal 
backing form similarity semantic similarity resnik obtains coverage accuracy 
note case distributional similarity di rectly substitute semantic similarity 
preferred structure determined considering probability noun modify third noun 
exam ple consider probability seeing business major athlete major 
chapter 
background problem language modelling problem distributionally similar words overcome sparse data problem 
approach applied form noun noun consider likelihood adjective modifying second noun eats apples ice cream 
eats apples oranges 
word sense disambiguation word sense disambiguation wsd determination meaning sense word context 
example noun plane senses characterise aeroplane tool geometric surface tree aeroplane plane circled head 
tool john knew find plane 
surface distance points plane easily calculated 
tree plane blown hurricane 
able tell sense intended essential understanding translation sentences 
word sense disambiguation algorithms lin leacock chodorow sch tze karov edelman brill kilgarriff rely ei ther implicitly explicitly yarowsky principles yarowsky sense collocation sense discourse 
example time word plane appears subject verb circle expect aeroplane sense intended 
yarowsky wsd algorithm yarowsky having labelled occurrence aeroplane sense constraining contexts head learnt occurrence 
occurrences ambiguous words constraining local contexts disam basis extremely sense intended rest discourse previous possibly subsequent occurrences word disam local context 
approaches fail global domain information decide sense 
algorithm initial input seeds 
seeds original known constraining contexts labelled 
yarowsky uses dictionary definitions supply seeds algorithm 
alternatively kilgarriff extract salient contexts corpus mutual information 
contexts displayed user identifies sense 
seeds yarowsky wsd algorithm 
alternative approach assumes particular sense word occur similar contexts word defines sense 
example karov edelman sense tag training corpus considering contexts shared target word sense words words appear dictionary definition target word 
example contexts suit shared clothes wear may tagged clothes sense suit contexts suit shared court may tagged court sense suit 
chapter 
background idea applied direction 
lin finds words occur identical local context ambiguous target word selects sense ambiguous word maximises semantic similarity wordnet word contextually similar words 
pantel lin apply similar approach word word glossing viewed word sense disambiguation task sense tags foreign words 
find contextually similar words maximise similarity set contextually similar words translations bilingual dictionary 
sch tze dispenses need external source word sense infor mation tackling related problem word sense discrimination 
schutze represents word occurrence token vector containing words occurring context sen tence 
applies hypothesis similar context vectors indicate occurrences belong sense agglomerative clustering algorithm 
way distributional similarity information opposed contextual similarity infor mation applies tokens types word sense disambiguation generalizing context probabilities classes 
due sparse data problem previously seen sense plane occur current local context subject circle 
seen word bird occur subject circle 
determine sense plane know bird aeroplane sense plane tend occur similar contexts subject fly land 
similarity inferred indirectly semantic taxonomic information determined directly distributional similarity techniques sense disambiguated data 
way word sense disambiguation problem recast language modelling problem modelling probabilities concepts words 
section consider burdensome requirement sense disambiguated training data eliminated 
word sense separation previous section discussed word sense disambiguation application similarity methods 
technique described required sense tagged training corpus limits appeal technique 
consider possibility able auto matically identify similarity word senses opposed word forms 
lin proposes technique idea neighbours sense tend closely related 
lin considers neighbour target word similar target word previous neighbour target word 
similar previous neighbour assigned sense neighbour assigned new sense 
technique simple gives promising results 
neighbours word organised sense clusters information source semantic information wsd algorithm 
example adapt idea karov edelman discussed neighbours individual senses assign particular occurrences senses 
metonymy resolution chapter 
background metonymy speech expression refer standard referent related lakoff johnson 
example name place refer sports team place england won world cup 
related words tend metonymic readings metonymy viewed highly predictable form polysemy kilgarriff 
accordingly metonymy resolution cast word sense disambiguation problem standard referent related expression viewed metonymic sense original expression 
lapata lascarides note intended paraphrase word england team usually predictable may attested corpus data due grice maxim brevity grice 
learn probable interpretation considering probabilities possible interpretations constituent parts context 
example learn enjoy book usually means enjoy reading book considering probabilities enjoy reading read book 
markert nissim nissim markert investigate exploiting larity metonymy supervised machine learning technique 
tagged data system learn won context england refers national sports team england 
nissim markert word similarity ways 
generalises england nouns semantic class 
second generalises context won dis similar contexts lost 
second case nissim markert automatically constructed thesaurus lin 
seen feasible identify semantic class noun distributional techniques 
distributionally similar nouns phrases team identify metonymic sense original word 
learning countability preferences nouns english typically classified countable uncountable example dog countable furniture uncountable sugar countable uncountable senses 
knowledge countability preferences important language analysis generation translation dictionary construction baldwin bond 
example japanese mark countability means generation component japanese english translation system decide countability noun english bond 
features baldwin bond possible classify nouns countable uncountable distribution respect determiners numerical quantifiers 
example say dog dogs dog dog 
contrast refer furniture furniture 
accordingly nouns clustered distributional similarity respect determiner nu quantifier contexts expect find distinct clusters corresponding countability classes 
uncountable nouns known mass nouns 
automatic thesaurus generation consider definition synonymy introduced section chapter 
background words absolute synonyms inter substituted possible contexts changing meaning 
philosophers linguists quine goodman cruse clark argued existence absolute synonyms generally accepted absolute synonym exist fall take new nuance meaning clark 
words completely fulfil requirements absolute syn measure extent requirements fulfilled gives rise concepts near synonymy semantic relatedness 
accordingly semantic relatedness tween words viewed extent words inter substituted changing meaning 
consequence definition semantic relatedness expect semantic related words behave similar ways 
example resnik shows selectional preferences verbs established classes semantically related nouns 
resnik hy distribution nouns verbs predicted semantics things eaten occur object word eat 
hypothesis automatic thesaurus generation opposite direction semantics nouns verbs predicted distributions things occur object word eat things eaten 
words words inter substituted expect find occurring similar contexts expect distributionally similar 
basis large body automatic thesaurus generation hindle grefenstette lin curran moens 
inherent problems evaluating automatic thesaurus extraction techniques discussed section results promising 
lin shows auto matically generated thesaurus entries derived distributional similarity measure similar wordnet derived thesaurus entries roget derived entries 
curran moens show combination dice coefficient test weight function manning sch tze produce thesaurus entries similar semantic gold standard constructed authors macquarie bernard moby ward roget roget number distributional similarity measures weight functions including lin 
lack consistent evaluation methodology pieces research difficult draw 
curran moens evaluate smaller number words lin nouns com pared nouns different semantic gold standard 
want thesaurus entries similar wordnet derived entries similar roget style entries may depend application 
earlier discussion differences wordnet roget terms second order affinities surprising distributional similarity measure best producing entries similar 
return questions evaluation section 
assume ideal distributional similarity measure determine sets semantically similar chapter 
background words 
hypothesis correct quickly easily build extend particular language dialect genre domain 
domain specific knowledge increasingly recognised important natural language understanding tasks 
example word sense reduced considering domain maynard ananiadou 
domain computing word term worm refer malicious computer program 
domain knowledge reflected thesaurus automatically generated computing specific corpus show increased similarity worm virus reduced similarity worm caterpillar 
semantic relatedness determined distributional similarity methods encompasses different semantic relations including antonymy hyponymy church 
order able rival manually constructed automatically generated ones need able distinguish different relations 
copestake vossen copestake weeds genus term hypernym extraction dictionary definitions 
structure dictionary definitions hyponymy relation implicit explicit word defined head word defining noun phrase 
example adder poisonous snake bitter type dark brown beer 
distributional similarity enhance genus term extraction process system machine readable dictionary 
approach applied corpus data 
system search large general corpus corresponding phrasal patterns adder poisonous snake words known distributionally similar 
approach taken hearst searches phrasal patterns known express specific lexical relationships 
example phrase cats dogs animals identify animal hypernym cat dog 
idea extended caraballo clustered nouns distributional similarity attempted distinguish hypernyms clusters 
lin zhao qin zhou attempt distinguish synonyms antonyms similar approach 
propose phrasal patterns indicators antonymy 
hypothesise antonyms translation words common bilingual dictionary 
information retrieval primary function search engine retrieve documents related query 
typical approach discussed strzalkowski index documents key terms simply return documents key terms exactly matching query terms 
penalises documents queries synonymous subsuming terms exactly matching terms 
query expansion xu croft technique adds conceptually similar terms phrases user query requires thesaurus source semantic information 
anaphor resolution anaphor resolution related metonymy resolution word expression refer referent related word expression 
case related word chapter 
background expression occurs antecedent text 
example synonym hyponym hyperonym may bridging anaphor bought dog man street 
new pet felt home immediately 
example know dog pet infer pet referred second sentence dog sentence 
relationship dog pet derived semantic resource wordnet distributional techniques 
techniques semantic similarity applied associative anaphora meyer dale 
associative anaphor referent explicitly stated implied bus came round corner 
driver nasty eye 
resolve anaphor know buses drivers 
relationship tween bus driver learnt expressions corpora driver bus 
order overcome sparse data problem meyer dale generalise bus vehicle wordnet noun hyponymy hierarchy 
envisage similar technique distributional similarity methods determine cluster generalise nearest neighbours consulted 
collocation extraction various definitions term collocation 
researchers kilgarriff lin define collocation habitual recurrent word combination 
collocations sense extracted considering relative word combination unigram frequencies berry nagata johnson 
specifi cally researchers mutual information measure word association purpose church hanks kilgarriff lin kilgarriff 
researchers assign term collocation specific sense lexicalised recurrent word combination multiword expression cohesive lexeme crosses word boundaries sag baldwin bond copestake flickinger 
shall discuss knowledge type collocation important dictionary construction language analysis genera tion high precision extraction corpora generally difficult focus current research mccarthy korhonen bond 
researchers mccarthy keller carroll baldwin lascarides baldwin tanaka distinguish compositional non compositional collocations 
collocation compositional meaning determined constituent parts 
expressions true collocations just frequently occurring word combinations institutionalised language 
discussed pearce alternative expression compositional semantics native speaker 
example english say strong tea powerful tea day emotional baggage emotional lin 
expression case collocation second anti collocation pearce 
compositional collocations tend pose problem natural language understanding systems meaning established parts 
problem foreign language learners chapter 
background generally generation systems need able choose alternatives 
non compositional collocations hot dog meaning established constituent parts pose problem understanding generation 
collocations need listed lexicon interest lexicographers 
pearce lin propose techniques collocation extraction idea non substitutability 
example pearce substitutes word potential collocation synonyms wordnet compares mle probabilities expression ex pected probability expression words occurred independently 
collocations occur frequently expected constituent parts anti collocations occur frequently expected constituent parts 
lin substitutes word potential col location similar words automatically generated thesaurus lin detects collocation significant difference mutual information 
tech nique views collocation occurring distributional hypothesis breaks words normally substituted substituted 
success technique demonstrates collocations important consideration similarity language modelling techniques 
baldwin 
recognises substitutability techniques identify collocations sufficient test compositionality 
discussed 
meaning compositional phrase implies meaning con 
example strong coffee implies coffee strong phrasal verb put takes direct object implication put 
hand phrasal verb carry task non compositional carried 
compositional cases collocation considered hyponym head non compositional cases 
idea baldwin 
consider hyponymy semantic distance wordnet collocation head 
alternative approach taken mccarthy 
consider distributional similarity collocation head 
hypothesis non compositional collocation different selectional preferences head 
alternatively say collocation hyponym head compositional able substitute collocation head 
text simplification aim practical simplification english text pset project carroll minnen canning devlin tait automatically simplify text aphasic readers 
aphasic readers typically problems complex syntactic constructions long sentences rare words devlin 
system lexical simplifier identifies synonyms content words wordnet oxford psycholinguistic database quinlan obtain respective frequencies original word synonyms 
word highest frequency selected simplified text 
major potential problems approach lexical simplification 
word sense ambiguity 
carroll 
note practice candidate words simplification single sense 
ambiguous words tend frequent chapter 
background system find frequent synonym 
second problem identification collocations discussed previous section generally appropriate substitute word synonyms context collocation pearce 
problems aside obvious potential application automatically generated thesaurus focuses substitutability words 
distributional similarity accurate predictor semantic similarity distributional information select appropriate substitute 
distributional similarity eliminate synonyms tend different contexts 
compound noun interpretation interpretation compound nouns necessary determine implicit relationship tween nouns 
example terrorist activities refers activities terrorists gun crime refers crime guns 
approach problem lauer compare trigram frequencies potential paraphrases 
example activities ter activities terrorists activities terrorists occurs frequently text 
approach obviously suffers data sparseness problem 
tackle applications thesaurus establish relationships probabilistically semantic classes individual lexical items 
topic identification step proposed text summarization systems extraction key concepts source text mccoy 
approach known lexical chaining morris hirst exploits property text cohesiveness texts tend contain words pertaining main topics themes text 
words related topic tend inter related identifying long chains semantically related words running text key concepts text identified 
system obviously requires source semantic similarity information automatically generated thesaurus 
spelling correction correction real word spelling errors words confused place application similarity methods 
general task choose set easily confused alternatives context 
main approaches yarowsky golding literature discuss 
approach casts problem lexical disambiguation task tests tions patterns words target word 
example context sensitive spelling cor rector hypothesize word principal intended phrase school princi ple 
analogy word sense disambiguation number different ways contextual patterns learnt 
example brill brill apply transformation learning problem 
alternatively occurrence probability estimates chapter 
background alternative derived corpus data means approach reduces language modelling task described earlier 
reason focus second approach semantic discuss 
second approach hirst st onge looks global context exploits property text cohesiveness naturally occurring coherent texts tend contain se related words morris hirst 
budanitsky hirst exploit property assuming author intended word semantically related rest text real word spelling error 
clustering applications considered potentially involve forming clusters similar words 
large body clustering algorithms manning sch tze provides general clustering algorithm chosen independently distance similarity measure 
accordingly appreciated thesis study similarity measures particular clustering technique study different clustering techniques 
purposes illustration clustering techniques nlp literature pereira hofmann puzicha sch tze jun lee pereira rooth riezler carroll clark lin pantel dhillon kumar li schulte im walde brew korhonen marx discuss nearest neighbours technique thesis 
agglomerative hierarchical clustering cutting karger pedersen tukey ini assigning object cluster 
closest similar clusters determined merged 
repeated required number clusters obtained 
nearest neighbour clustering example graph theoretic method van rijsbergen object connected nearest neighbour similar object alter natively objects similarity certain threshold 
clusters formed finding connected components 
means clustering forgy objects assigned initial clusters randomly clustering method 
dis tance object centroid cluster computed objects reassigned nearest cluster 
process iterates 
afore mentioned techniques hard clustering techniques object occur single cluster 
cluster representation object cluster representation type clustering technique implicitly assumes similar ity objects symmetric 
asymmetric measure compute similarity object centre proposed cluster 
final cluster formation assumes representation representation nearest neighbours technique lee thesis viewed soft clus tering technique object centre cluster 
object cluster nearest neighbours 
example cluster representation whilst cluster representation technique analogous cre chapter 
background ating thesaurus entry word assume similarity symmetric simple implement provides fairly transparent way evaluating similarity methods 
parameter needs considered similarity measure neighbourhood size explore optimised globally word dependent prop erty 
nearest neighbours techniques pseudo disambiguation experiments lee distance weighted averaging grishman sterling dagan lapata keller mcdonald 
smoothing technique computes distance weighted average occurrence probability estimates object nearest neighbours 
evaluation distributional similarity measures section consider standard approaches evaluation applicability area distributional similarity 
approaches evaluation gold standard evalu ation human plausibility judgements evaluation prediction unseen data application evaluation tasks 
consider actual evaluation tasks literature 
pseudo disambiguation wordnet prediction task 
variations tasks thesis 
evaluation gold standard simplest approach evaluation nlp gold standard 
simply com data answers provided technique evaluated correct answers gold standard 
example parsing parse text penn treebank marcus santorini marcinkiewicz compare trees produced gold standard trees penn treebank 
example collins charniak train statistical parsers portion penn treebank test parsing sentences held unseen portion 
field word sense disambiguation sense tagged data hand annotated team lexicographers gold standard evaluate automatic sense disambiguation techniques 
example semcor miller chodorow leacock thomas portion brown corpus hand tagged wordnet senses 
may appropriate wishes different sense inventory 
situation inexpensive way acquiring partially sense tagged corpus construct examples sense usages dictionary definitions weeds 
main issues consider considering gold standard evaluation 
readily available gold standard penn treebank 
time consuming expensive construct 
automatic sense disambiguation technique run bnc day take human lexicographer months years create equivalent gold standard 
second gold standard need automatic technique de velopment 
just gold standard 
obviously answer cases partial gold standard 
penn treebank contains correct parses sentences penn treebank human annotators annotate finite amount text chapter 
background sense tags 
idea gold standard automatic technique works data assume data don gold standard 
dangerous assumption 
results particular domain genre may generalizable domains genres 
third gold standard valid 
errors working field correct answer 
linguists argue correct parse sentence lexicographers argue sense assigned particular word context argue senses word 
automatic technique suggests different gold standard necessarily wrong 
penn treebank parse trees gold standard want produce parse trees look penn treebank parse trees 
want technique learn different 
example may want produce deeper level analysis contained penn treebank trees 
issues real problems area lexical semantics 
example pearce evaluates collocation extraction techniques gold standard constructed multi word information machine readable version new oxford dictionary english hanks 
pearce comments somewhat controversial evalu ate techniques considered standard ask standard contains collocations current contains fallen really perfect gold standard need automatically extract collocations 
gold standard evaluation distributional similarity measures problematic gold standard look word set distributionally nearest neighbours gold standard containing exact distributional similarity scores pair words 
accordingly want gold standard find team distributional similarity experts construct cheat really gold standard distributional similarity 
option overly appealing due time consuming nature fact assumes existence distributional similarity experts come back section leaves option 
discussed link semantics distributional characteristics word led semantic gold standards evaluation distributional similarity techniques lin curran moens 
aim distributional similarity techniques automatically generate perfectly reasonable thing 
leaves second issues raised unresolved 
gold standard semantic similarity need automatically generate semantic similarity data 
answer 
assuming results generalizable apply automatic techniques languages particular domains genres 
new words coming language techniques extend existing resources 
issue language change leads third issue gold standard valid 
new words coming language words change meaning 
may suggest distributional techniques study change 
casts chapter 
background doubt fairly static gold standard evaluation 
sure captures semantic link words currently 
automatic techniques tell words unrelated gold standard degree semantic similarity necessarily wrong 
possible learnt new semantic linking previously unobserved 
discussed previous section potential applications distributional similarity techniques necessarily require words semantically similar distributionally similar 
may doing badly evaluation technique distributional similarity technique flaw hypothesis links distribution semantics 
accordingly require evaluation technique evaluates distributional similarity independent semantic similarity 
evaluation human plausibility judgements evaluation human plausibility judgements related evaluation gold standard gold standard unavailable incomplete 
alternative gold standard field collocation extraction la mcdonald keller lapata johnson 
set potential collocations adjective noun pairs constructed member group humans rates plausible collocation 
judgements automatic technique correlated human judgements 
similar approach lapata las evaluate metonymy resolution algorithm 
potential paraphrases metonymic phrases generated human subjects asked rate plausibility 
example phrase finished dinner interpreted things meaning finished cooking dinner finished eating dinner 
general expect humans favour interpretation automatic technique favours interpretation rated higher 
area human plausibility judgements evaluation semantic similarity measures 
human synonymy judgements obtained rubenstein goodenough pairs words human synonymy judgements obtained miller charles subset pairs words rubenstein goodenough set 
data sets resnik jiang conrath lin budanitsky hirst jarmasz szpakowicz evaluate number different semantic similarity measures wordnet roget 
problem human plausibility judgements subjectivity judgements 
subjectivity course reduced group humans reduced discarding test instances large amount disagreement 
subjectivity priming effects completely eliminated question attempt answer fundamental problem connected human plausibility judgements evaluation distributional similarity measures 
fundamental problem appropriate evaluation technique field distributional similarity 
human judge plausible words chapter 
background distributionally similar 
able decide pairs words distributionally similar difficult task able decide collocations plausible 
plausibility collocations simple decide part psychological process lexical choice time 
asked plausible strong coffee powerful coffee simply think say deciding words appear similar contexts appear part simple psychological process 
deciding pairs words 
coffee tea coffee water plausible pair distributionally similar words involves considering different context word occurs 
hard imagine human able completely tease apart intuitive notions distributional semantic similarity 
example people rate pair dog road cat street basis dog road nouns occur object verb walk 
accordingly human plausibility judgements area distributional similarity context application distributional similarity 
discuss application tasks section 
evaluation unseen data evaluation approach comes domain language modelling 
language model assign higher probabilities sentences texts taken language strings words form part language 
language model able assign probability string actual sentence language uttered 
example sentence quoted earlier jane austen emma probably uttered jane austen wrote time greatly probable nonsensical ungrammatical utterances chomsky oft quoted linguistic examples green ideas sleep sleep ideas green chomsky 
accordingly language models evaluated ability predict set held unseen test sentences 
example data set sentences build language models evaluate language models basis probabilities assign remaining 
standard language model evaluation metrics intuition test set perplexity relative entropy test set model 
discussed cover thomas relative entropy measures inefficiency bits assuming model distribution true distribution events distribution observed test set log test set perplexity defined chen goodman rosenfeld function related measure cross entropy cross entropy ec perplexity ec ec chapter 
background logq evaluation approach applied area distributional similarity methods 
dagan 
smooth occurrence probabilities distributions distributionally nearest neighbours computed relative entropy 
test set perplexity compare similarity language model back language model 
main problems evaluation approach 
observed empirical distribution held test data population distribution 
language model model test set distribution exactly 
fact model test set distribution exactly case inefficient model different test set 
approach comparatively hard place lower bound relative entropy test set perplexity expect achieve 
second performance language model depends distributional similarity measure algorithm smooth distributions neighbours 
difficult evaluate similarity measures independent smoothing algorithm positive negative interactions certain algorithms measures 
third circularity measure relative entropy evaluate measures distributional similarity include relative entropy 
creating evaluating language model concerned similarity probability distributions 
similarity lexical occurrence distributions 
simi larity model distribution empirical distribution 
think different ways measuring give different results different ways measuring give different results 
evaluation ap proach implicitly assuming measure similarity probability distributions 
leads ask case need consider measures probability distributions lexical occurrences 
application evaluation tasks fourth evaluation approach considered application tasks 
attempting evaluate similarity results directly evaluate useful applications discussed earlier 
considered evalu ation tasks 
evaluation semantic gold standard evaluation task application automatic thesaurus generation evaluating unseen data evaluation task application language modelling 
envisage evaluation task corre sponding application mentioned section 
simply implement application see measure distributional similarity gives best results terms performance application 
consider high performance application correlated high performance 
example useful neighbours word sense disambiguation useful neighbours prepositional phrase ambiguity resolution 
unfortunately possible consider application separately thesis 
results just applications suggest semantic applications may require chapter 
background quite different word nearest neighbours applications consid ered distributional syntactic domain 
consider specific application evaluation tasks literature 
consider pseudo disambiguation evaluation technique language modelling word sense disambiguation 
consider wordnet prediction task evaluation technique automatic thesaurus generation 
pseudo disambiguation tasks pseudo disambiguation tasks standard evaluation technique gale church yarowsky sch tze pereira sch tze lee dagan golding roth rooth zohar roth lee clark weir current setting may noun neighbours decide occurrences 
pseudo disambiguation artificial task relevance application areas 
replacing occurrences particular word test suite pair set words technique choose recreate simplified version word sense disambiguation task choosing fixed num ber homonyms local context 
second language modelling wish estimate probabilities occurrences events due sparse data problem case possible occurrence seen training data 
typical approach performing pseudo disambiguation follows 
large set noun verb direct object pairs extracted parsed corpus 
portion data test data portion training data 
training data construct language model determine distributionally nearest neighbours noun 
noun verb pairs test data replaced noun verb verb triples task decide verbs take noun direct object 
performance usually measured error rate 
error rate incorrect choices ties number test instances tie results possible decide alternatives 
approaches vary details constructing training test sets way decision alternatives 
discussed 
approach pereira 
clark weir form test set randomly selecting percentage noun verb pairs original data set ensuring verb occurs sufficiently frequently 
training data formed deleting instance test noun verb pair original data 
ensure test data completely unseen 
test set completed selecting verb approximately frequency original data 
clark weir ensure occur training data 
ensures probable technique relies mle probabilities backs unigram probabilities katz perform better chance 
techniques pereira 
clark weir class cluster nouns containing target noun decide chapter 
background probable occurrence 
pereira 
find clusters distributionally similar nouns training data member target noun cluster votes verb occurs frequently training data 
clark weir semantic classes derived wordnet 
counts verb occurrences accumulated wordnet noun hyponym hierarchy appropriate level generalisation target noun determined squared tests 
different approach test set construction lee dagan lee randomly select portion occurrences test data delete instances occurrences test data occur training data 
feel method test set construction appropriate large scale evaluation distributional similarity measures completely removing occurrence types training data ensure unseen distorts distributional data measures determine similarity 
alternative approach ensures test instances unseen whilst distorting training data 
approach zohar roth differs ensure test instances unseen 
may explain achieve higher reduction error rate baseline lee 
arguments ensuring test data unseen examined chapter 
possible difference method test set construction wrong choice verb required unseen noun training data 
occurred training data strictly say verb correct choice verb construction test set 
practice lee pereira ensuring unseen training data little difference pattern results 
lee dagan 
lee differs pereira 
decision alternatives 
pereira 
form clusters distributionally similar words noun appropriate cluster vote verb preferred 
nearest neighbours noun vote preferred verb parameter optimised 
potential advantages approach discussed lee pereira 
expect nearest neighbours approach perform better noun centre cluster 
optimising parameter means fix cluster size advance 
general expect error rate initially decrease neighbours considered decisions able 
certain optimal number neighbours error rate start increase neighbours longer similar correct decision 
paradigm follow initial experiments closely aligned lee approaches 
differences ensure verb occurred training data implement voting scheme nearest neighbours vote proportional difference frequency occurrence verbs 
means neighbours occur verb approximately equal probability influence voting process neighbours occur verb significantly 
issues returned chapter 
wordnet prediction task chapter 
background wordnet prediction task lin evaluates ability distributional similar ity measure predict similarity words defined measure hyponymy relation wordnet version 
task evaluates usefulness distributional similarity measure predictor semantic similarity potential automatic saurus generation 
underlying assumption hyponymy relation wordnet gold standard semantic similarity course true 
argued distributional similarity measure closely predicts wordnet predictor semantic similarity 
consider approach taken lin detail 
necessary consider way measure distance nouns wordnet noun hierarchy 
range simple edge counting tech niques rada mili bicknell techniques consider link direction lea cock chodorow techniques factor corpus probabilities jiang conrath 
see budanitsky extensive survey wordnet similarity measures 
wordnet similarity measure lin proposal lin max max sup sup logp log log set senses noun wordnet sup set possibly indirect super classes concept wordnet 
measure implementation lin similarity theorem lin states similarity objects measured ratio amount information needed state commonality information needed fully describe 
current context objects nouns commonality concepts maximally specific common super class concepts similarity nouns defined similarity similar senses 
probability randomly selected noun refers instance concept estimated frequency concepts semcor miller sense tagged subset brown corpus 
occurrence concept word sense semcor refers instances super classes concept 
example see meat sense word chicken seen instances meat food practical point view scheme necessary ensure super class probable contains information children 
lin computes nearest neighbours similarity measure set nouns 
computes similarity overlap pair neighbour sets noun 
similarity neighbour sets thesaurus entries word calculated follows 
suppose entries constructed noun measures nearest neighbours associated similarities measure written na sa na sa nak sak nb sb nb sb similarity sets computed sim na nb sai sb chapter 
background ai score neighbour sets identical contain exactly neighbours exactly similarity scores 
opposite scale score sets share neighbours 
having computed similarity pair neighbour sets nouns lin computes mean similarity thesaurus entries standard error mean defined standard deviation data items divided square root number data items 
james curran curran moens curran compared neighbour sets produced distributional measures thesaurus entries derived wordnet macquarie bernard moby ward roget roget 
discussed earlier grammatical relation data context distributional similarity measures expect neighbour sets produced akin derived hyponymy relation wordnet roget lin kilgarriff 
argument sources noun hierarchy wordnet distributional similarity measures concept lexical substitutability return pairs words related antonymy relation 
whilst measure semantic similarity derived wordnet noun hierarchy explicitly consider antonymy relation pairs antonyms hyponyms common super class 
consequently antonyms semantically related words 
chapter considered intuitions distributional similarity 
dis cussed means words distributionally similar different orders ity exist words 
follow grammatical relation data define context 
defining context syntactically means distributional similarity thought terms lexical substitutability 
discussed large number potential applications knowing distributionally similar lexically substitutable words 
applications take advantage discussed link lexical substitutability semantic relatedness 
applications probability estimation techniques property distributional similarity directly 
evaluation major problem area research 
argue humans unable judge lexical distributional similarity independent semantic similarity case single right answer words distributionally similar 
best measure distributional similarity returns useful neighbours context particular application leads best performance application 
thesis investigates high performance application area correlated high performance 
best choice distributional similarity measure depends chapter 
background application 
chapter investigation introducing large varied collection existing distributional similarity measures 
chapter existing measures distributional similarity years broad range measures proposed adopted measuring lexical distributional similarity 
measures originate fields geometry statistics prob ability theory information theory information retrieval ir 
chapter theoretical basis fifteen existing measures 
compare measures terms neighbour sets produce data examine tendencies select high low frequency words neighbours 
theory discussion consider similarity nouns occurrences verbs direct object relation 
occurrence types noun verbs associated frequencies may form probability estimates 
noun finding neighbours referred target noun target word 
computing similarity target noun noun second noun potential neigh target noun 
target noun nearest neighbours potential neighbours highest similarity target noun 
term similarity measure encompasses measures strictly re distance divergence dissimilarity measures 
increase distance correlates decrease similarity 
distinction relevant wish weight neighbours sim ilarity 
case distance converted similarity score function proposed dagan 
sim dist ranges controls relative influence closest neighbours 
general distant neighbours non negligible similarities target word 
functions convert distance similarity possible may yield different results particular application 
reason evaluation focus ranks neighbours particular similarity scores individual word pairs 
chapter 
existing measures distributional similarity case distinction similarity distance irrelevant types measure determine nearest neighbours word 
consider measure family measures turn 
consider geometric measures minkowski distance norms cosine measure section correlation coefficients section combinatorial measures jaccard coefficient dice coefficient section substitutability measures kullback leibler di vergence measure confusion probability section mutual information mi measures hindle measure lin measure section 
geometric measures geometric measures vector geometry 
occurrence types dimensions vector space frequencies occurrence target word occurrence type calculate value component vector 
general value dimension component conditional probability occurrence type target word 
example considering similarity nouns occurrence verbs sake simplicity occurrence information table represented vector diagram 
frequencies verb noun see get total see get bus dog total table occurrence information bus dog get minkowski distance bus dog see vector diagram bus dog minkowski distance family metrics measure distance points space 
dimensions verbs values vector components get chapter 
existing measures distributional similarity bus dog see geometric distance metrics probabilities verbs noun minkowski distance nouns written positive integer 
leads familiar norm distl norm known manhattan distance taxi cab distance city block distance absolute value distance represents distance travelled points travel orthogonal directions common restriction cities 
example see value norm 
norm simple shown effective complicated similarity measures lee widely clustering kaufman rousseeuw cutting sch tze dagan 
norm computed efficiently rewritten dagan distl iff 
follows triangle inequality equality equality verbs 
widely parameter setting minkowski distance 
gives norm known euclidean distance distl example computation equivalent performing ras theorem yields crow flies distance points 
metric captures intuitions space widely kaufman rousseeuw chapter 
existing measures distributional similarity cutting sch tze schroeder noy shown effective norm 
reason discussed lee quadratic verbs shared nouns noted kaufman rousseeuw extremely sensitive effect outliers 
words large difference dimen sion strong influence score differences dimension squared 
increases lead increases effect reached defined maximal distance points single dimension cosine measure get distl maxv bus dog see cosine measure cosine measure returns cosine angle vectors see calculated dot product vectors cos angle vectors representing identical distributions zero maximum value cosine measure 
cosine value zero indicates complete lack similarity vectors words shared occurrences orthogonal 
measure popular vector similarity measure salton mcgill sch tze manning sch tze caraballo pad lapata wu zhou shown lee effective measures lexical distributional similarity measure incorporates information non shared occurrences 
vector techniques discussion far considered number dimensions semantic context space 
lin lee consider verb occurrence target nouns verb corpus possible dimension :10.1.1.15.227
possible reduce dimensionality considering say frequent occurrence types dimensions pad lapata mathematical technique singular value de composition svd deerwester sch tze 
svd see chapter 
existing measures distributional similarity malcolm moler analysis begins matrix associations pairs objects 
example pairs objects noun verb occurrences association values observed frequencies probabilities 
svd general matrix analysis technique similar eigen analysis symmetric matrices finds independent components principal axes variation data 
minor axes variation ignored leading reduced dimensionality 
technique leads efficiency improvements may particularly impor tant implementing computationally expensive clustering algorithms 
focus comparison different similarity measures general modern hard ware computational issues retaining full dimensionality significant 
potential advantage svd finding major axes variation possible away noise detect underlying features 
expect similar ity measure perform relative similarity measures independent common preprocessing data 
svd increase apparent performance similarity measures focus insignificant features noise 
order compare similarity measures full dimensionality original data 
section consider mutual information viewed way detecting significant features 
correlation coefficients random variables said positively correlated high value lead expect high value conversely low value lead expect low value 
context lexical distributional similarity correlation particular sch tze 
envisage plotting scatter graph point represents possible verb occurrence position relative axis represents probability frequency occurrence noun position relative axis represents probability frequency occurrence noun 
similar nouns bus train expect find positive correlation dissimilar nouns bus dog expect find zero negative correlation 
pearson product moment correlation coefficient known correlation coefficient pearson product moment correlation coefficient 
measures extent values variables lie side respective means 
variable represent probability distribution variable represent probability distribution correlation similarity nouns computed standard definition pearson product moment correlation coefficient arithmetic mean distribution arithmetic mean distribu tion 
coefficient lies range represents perfect negative correlation represents perfect positive correlation 
measure closely related cosine measure sec frequency occurrence verb bus frequency occurrence verb bus chapter 
existing measures distributional similarity board run get catch take frequency occurrence verb train positive correlation bus train catch get board walk take frequency occurrence verb dog correlation bus dog tion effectively finding cosine vectors values normalised subtracting respective means original values 
expect means conditional probability distributions occurrence frequency distri butions close zero large number verbs occur noun 
setting pearson product moment correlation coefficient approximated cosine measure 
explains expect see negative correlation words expect see verb occurring noun mean number times zero 
rank correlation coefficients potential problem pearson product moment correlation coefficient area assumes parametric distribution distributed normally mean 
discussed pedersen assumption normality shared common statistical tests chapter 
existing measures distributional similarity rarely valid nlp distributions tend contain large numbers rare events 
case lexical occurrence distributions see baayen thorough nature word frequency distributions 
non parametric solution correlation cient rankings spearman rank correlation coefficient kendall coefficient 
example noun rank verbs occurrence frequency 
verbs occur number times noun arithmetic mean ranks jointly occupy 
spearman rank correlation coefficient finch chater find syntactic categories simply defined product moment correlation coefficient sets rankings 
values correlated ranks formula usually simplified di difference ranks ith item number items 
formula approximation tied ranks 
formula definition run problems large number verbs tied bottom ordering 
verbs occur mean ranking verbs occur noun strong influence apparent positive correlation nouns 
kendall coefficient gibbons hatzivassiloglou mckeown lee just considers ordering put occurrence types nouns :10.1.1.136.3516
measure tests noun occurring frequently verb verb correlated noun occurring frequently 
formally pair occurrence types vi vj consider signs quantities vi vi 
signs verb pair concordance signs different pair quantity zero pair tie 
sim difference probability observing concordance probability observing 
estimated sim number observed concordances number observed set occurrence types 
coefficient advantage spearman rank correlation coefficient ties serve reduce value correlation zero 
expensive compute 
pair nouns pair verbs considered 
contrasts majority measures computed pair nouns linear time number verbs 
combinatorial measures combinatorial similarity measures counting shared non shared features 
nature measures simplest forms extent object feature considered context lexical distributional similarity possible discuss incorporate weight functions measures 
chapter 
existing measures distributional similarity consider word feature word occurred grammatical relation 
consider just noun verb direct object occurrences features noun verbs compute similarity nouns consider relative sizes regions 
sizes regions cardinality intersections specified table 
different measures combine values different ways 
discuss possibilities 
jaccard coefficient venn diagram similarity table sizes intersections feature sets jaccard coefficient salton mcgill lee curran moens known tanimoto coefficient savitch resnik defined proportion features belonging noun shared nouns ratio size intersection feature sets size union feature sets dice coefficient sim dice coefficient frakes baeza yates kay smadja mckeown hatzivassiloglou curran moens similar jaccard coefficient differing gives twice weight shared features 
computes ratio size intersection feature sets sum sizes individual feature sets shown van rijsbergen dice jaccard coefficient monotonic 
means scores computed different orderings rankings put objects 
applications little gained measures 
combinatorial measures chapter 
existing measures distributional similarity large number measures computed values table reviewed ravichandran rao 
incorporate num ber features object 
example simple matching coefficient computed russell rao coefficient ravichandran rao computed context lexical distributional similarity expect number larger values 
tend words tend words 
substitutability measures similarity nouns considered measure appropriateness substituting 
appropriateness substituting may different appropriateness substituting leads number asymmetric measures consider 
consider symmetric jensen shannon divergence measure asymmetric kullback leibler divergence measure 
referring asymmetric measures shall notation denote target noun denote potential 
kullback leibler divergence measure kullback leibler divergence measure relative entropy kullback leibler cover thomas pereira dagan clark calculate distance probability mass functions 
relative entropy distributions defined cover thomas inefficiency assuming distribution true distribution inefficiency defined information theoretic terms relative entropy measured bits log assume mathematical equivalences log 
shown cover thomas relative entropy non negative equal zero distributions identical making potentially useful distance measure 
previous examples measuring similarity nouns occurrences verbs direct object relation set measure similarity nouns inverse distance distributions log chapter 
existing measures distributional similarity informally measure average log ratio corresponding probabilities dis tribution weighted probabilities true distribution 
obvious ical definition relative entropy asymmetric 
measures cost whilst measures cost 
measure fitness substitute measure fitness substitute 
words find neighbours find nouns minimise 
intuition distance direction discussed lee expect flat uniform distribution better model sharp empirical distribution sharp model distribution flat empirical distribution 
second issue considered measuring similarity nouns relative entropy verbs 
accordingly relative entropy directly maximum likelihood estimate mle probabilities wish find nouns infinitely different 
smooth base language model probability estimates related measure directly mle probabilities 
discuss measures jensen shannon divergence measure skew divergence measure 
jensen shannon divergence measure jensen shannon divergence measure rao lin popular alternative kullback leibler divergence measure frequently word clustering nearest neighbour techniques dagan lapata dhillon schulte im walde brew 
defined dagan average kl divergence distributions average distribution dist js kullback leibler divergence measure jensen shannon di vergence measure non negative equal zero distributions identical 
contrast kullback leibler divergence measure symmetric measure measures cost average distribution place indi vidual distributions 
clear advantages relative entropy 
require smoothed probability estimates mean probability values greater zero un values zero 
second shown dagan depends verbs occur nouns 
whilst computational advantage obvious lee showed measures depend contexts occur target words perform better pseudo disambiguation experiments 
skew divergence measure skew divergence measure lee popular approximation kullback leibler divergence measure jensen shannon divergence measure frequently measure lexical distributional similarity zohar roth mccarthy schulte im walde brew pad lapata 
approximation developed unreliable mle probabilities result actual kullback leibler divergence measure equal 
defined lee chapter 
existing measures distributional similarity dist 
effect distribution smoothed distribution results non zero distribution non zero 
parameter controls extent measure approximates kullback leibler divergence measure 
close approximation close whilst avoiding problem zero probabilities associated kullback leibler divergence measure 
theoretical justification high value borne empirical evidence lee 
jensen shannon divergence measure skew divergence measure retains asymmetry kullback leibler divergence measure 
accordingly necessary consider direction computed 
defence asymmetry skew divergence measure lee comments substitutability word need symmetric 
instance fruit may best possible approximation apple distribution apple may suitable proxy distribution fruit 
kullback leibler divergence measure measures cost distribution distribution effectively calculated verbs occurring dis tribution term verb occurring distribution zero second argument model distribution needs smoothed 
words find neighbours noun expect find nouns minimise dist 
lee obtains better results pseudo disambiguation task minimising dist 
lee justifies measure direction saying distribution target word neighbours smooth distribution unreliable requires smoothing 
longer fits notion measuring proxy distribution approximates actual distribution 
conflict 
minimising dist retains meaning kl divergence whilst minimising dist appear produce best results 
shall return empirical comparison different similarity measures section 
time shall consider skew divergence measure separate measures compute sets neighbours word accordingly 
find neighbours computing dist dist dist dist intuitively difference dist dist viewed follows 
low values dist arise probabilities contexts potential neighbour similar probabilities contexts target word contexts target chapter 
existing measures distributional similarity word occurs frequently 
low values dist hand arise probabilities contexts potential neighbour similar probabilities contexts target word contexts potential neighbour occurs frequently 
confusion probability confusion probability sugawara nishimura kaneko essen steinbiss grishman sterling dagan lapata estimate probability word substituted 
intuition nouns completely confusable equally see occurring object verb token see occurring object verb token 
words knowledge verb change relative probabilities 
wish compute probability word substituted verb direct object occurrences discussed dagan 
expression rewritten bayes rule computed probability lies range represents maximal similarity represents complete lack similarity 
kullback leibler divergence measure asymmetric 
curious property word necessarily similar word 
high potential neighbour occurs frequently verbs occur target word expect 
accordingly tend higher potential neighbours high unigram probabilities 
empirical comparison consider refer reverse sion probability contrast expect high potential neighbour occurs expect verbs occur frequently target word 
accordingly higher target nouns high unigram probabilities change rankings potential neighbours target noun 
mutual information measures mutual information allows capture intuition verb occurrences informative 
example verbs get weak constraints place arguments resnik means expect see occurring nearly noun 
fact occurred nouns consideration may tell similarity nouns nouns occur object strong verb eat 
formally mutual information cover thomas chapter 
existing measures distributional similarity measure amount information random variable contains random variable 
reduction uncertainty random variable due knowledge 
example seeing verb eat reduces uncertainty object verb seeing verb informative object 
pointwise mutual information words measure relatedness degree association church hanks 
noun verb written log log intuitively measure probability verb increased knowing noun symmetric probability noun increased knowing verb 
negative values indicate probability verb decreases know noun value zero indicates verb noun occur frequently expect chance 
consider particular measures similarity mutual information 
hindle measure hindle proposed mi measure show nouns reliably clustered verb occurrences 
considering just noun verb direct object occurrences measure simplified min abs max intuitively highly informative contexts shared nouns higher similarity 
similarity increased nouns verbs occur expected 
theoretically causes problem verbs occurred noun 
verbs occurred noun expected chance contribute similarity mi events occur undefined equal 
accordingly hindle measure calculated verbs occur nouns 
consequence adopt variant proposed lin considers verbs positive mi nouns min lin mi measure chapter 
existing measures distributional similarity lin proposed measure lexical distributional similarity information theoretic similarity theorem lin similarity measured ratio amount information needed state commonality information needed fully describe 
considering just noun verb direct object occurrence data similarity tween nouns calculated lin measure simlin measure greater number grammatical relations lin showed thesaurus entries generated akin entries derived wordnet entries derived roget variants cosine measure jaccard coefficient dice coefficient 
wiebe measure successfully identify subjective adjectives 
mi approaches mutual information popular intuitive measure lexical association led incorporation variations number standard similarity measures 
example lin defines features noun contexts mi noun context positive uses combinatorial dice jaccard coefficients feature sets 
generalisation jaccard coefficient grefenstette allows weight incorporated feature 
envisage measuring cosine vectors positive mi values conditional probabilities 
curran moens explore number weights including mi cosine measure jaccard coefficient dice coefficient 
discussed resnik fung mckeown kilgarriff weeds weir wu zhou mi tends emphasise asso ciation low frequency events 
wu zhou proposes variation lin similarity measure replaces mi scores weighted mutual information wmi fung mckeown scores wmi log log pointwise mi multiplied probability occurrence give wmi score association low frequency occurrences reduced association high frequency occurrences 
empirical comparison measures section empirically compare fifteen measures introduced previous section 
table lists measures notation refer 
discuss chapter 
existing measures distributional similarity data derive neighbour sets 
look similarity rankings neighbours different neighbours nouns 
look tendencies certain measures select high low frequency neighbours discuss impact may applications 
notation description distl distl sim sim sim dist js dist dist norm section norm euclidean distance section cosine measure section pearson product moment correlation coefficient section kendall correlation coefficient section jaccard coefficient section jaccard coefficient mi select features section dice coefficient section jensen shannon divergence measure section lee skew divergence measure section lee skew divergence measure computed opposite direction section confusion probability section confusion probability computed opposite direction section hindle similarity measure positive mi section simlin data lin similarity measure section table fifteen similarity measures describe data explain rationale 
data sets nearest neighbours derived direct object data nouns extracted british national corpus bnc robust accurate statistical parser rasp briscoe carroll carroll briscoe 
briscoe carroll gives full description system 
summary rasp pipelined modular system containing modules tokenisation part speech pos punctuation tagging parsing statistical disambiguation 
particular interest parsing module parses lattice pos punctuation tags manually developed wide coverage grammar 
output system displayed set ranked trees sequence grammatical relations lexical heads 
raw data research 
rasp system designed robust parses part speech punctuation tags word tokens accuracy shown briscoe carroll competitive highly lexicalised parse selection models 
table carroll summarises reliability different relations extracted rasp 
original data set extracted parser consisted possibly indistinct lemma noun verb direct object pairs 
selected data pertaining set nouns pairs total 
set aside data noun test data eval uation tasks 
remaining consisting occurrence tokens distributed chapter 
existing measures distributional similarity distinct occurrence types calculate similarities pairs nouns measures listed table 
discuss direct object data section selected set nouns section 
direct object data look similarity nouns 
grammatical relations model context precludes finding similarities words different parts speech 
looking similarity terms substitutability expect find word part speech substitutable word part speech 
easily chosen look similarity verbs adjectives techniques 
chose look nouns basis start 
nouns tend allow sense extensions verbs adjectives pustejovsky noun hyponymy hierarchy wordnet pseudo gold standard comparison extensively previous research lin clark weir moldovan harabagiu peters guthrie wilks 
previous distributional similarity nouns single gram relation usually direct object relation lee considered multiple grammatical relations lin 
argument multiple grammatical rela tions may similarities certain pairs nouns 
may case nouns occur object verbs occur subject verbs 
just direct object data reasons 
automatically extracted data 
grammatical relations indirect object extracted parser introduce noise data 
second reason clear including relations improve neigh obtained relations included information combined 
notion words semantically similar different roles little sense distinction terms distributional similarity 
difference expect find granularity 
example classification expect obtain distribution deter miners respect nouns countability baldwin bond determiners select feature 
expect verbs selective object position subject position 
large number verbs take human possibly animate subjects argued main subject data distinguish ani mate non animate concepts expect object data able anyway 
order investigate including data non clausal subject relation descriptions nouns advantageous conducted preliminary experiment follows selected direct object noun verb pairs non clausal subject noun verb pairs data described frequent nouns frequency calculated subject object position data set considering nouns just occurring wordnet 
constructed data sets follows relations extracted approximately equal accuracy rasp table 
chapter 
existing measures distributional similarity relation precision recall score test grs dependent mod arg mod arg subj ncsubj subj dobj comp obj dobj obj iobj clausal xcomp ccomp aux conj table accuracy rasp dobj direct object relation noun verb pairs ncsubj non clausal subject relation noun verb pairs sep direct object non clausal subject noun verb pairs mixed cooccurrence verb subject object position generates count separate features data set split training test data 
training set similarity pair nouns calculated dist 
measure demonstrated effective frequent nouns pseudo disambiguation task lee 
pseudo disambiguation experiment set follows 
separate direct object dobj non clausal subject ncsubj test sets constructed paradigm described chapter 
discarded noun verb pairs test data seen training data 
resulting dobj ncsubj test sets contained pairs pairs respectively remaining noun verb pair replaced noun verb pseudo disambiguation experiments discussed detail chapter 
represents reduction test set size due discarding seen pairs 
chapter 
existing measures distributional similarity verb triple selected approximately equal frequency training data occur training data 
task nearest neighbours decide original occurrence 
order frequency weighted voting scheme 
neighbour vote casts verb occurs frequently weighted difference frequency occurs verb 
votes nearest neighbours parameter optimised 
performance measured error rate defined incorrect choices ties error test instances tie results neighbours decide alternatives 
performance nearest neighbours derived dobj ncsubj sep evaluated test sets dobj ncsubj shown 
combination neighbour set test set show variation error rate neighbourhood size expect better neighbour sets outperform poorer neighbour sets values particular low values error rate dobj neighbours ncsubj testset ncsubj neighbours ncsubj testset sep neighbours ncsubj testset ncsubj neighbours dobj testset sep neighbours dobj testset dobj neighbours dobj testset performance neighbour sets different test sets observations 
types neighbour sets better disambiguating direct object test instances non clausal subject test instances 
supports hypothesis verbs selective object position subject position 
nearest neighbours sort able disambiguate non clausal subject verbs tend occur verbs verb 

direct object derived neighbours perform non clausal subject derived neighbours ncsubj test set better dobj test set 
supports hypothesis nouns similar object occurrences similar subject cooccurrences 
chapter 
existing measures distributional similarity 
increasing amount training data including information relations lead significantly improved results information solely relation 
suggests non clausal subject data adding new information nouns difference distributional similarity grammatical relations granularity 
decided perform rest experiments just direct object relation data 
nouns explain selected nouns explain rationale 
complete set nouns subset high frequency subset low frequency nouns 
computed frequency nouns counting number times occurred direct object relation data 
counts data nouns occur frequently relation frequency occurrence direct object data reflects accurately amount data noun 
having constructed frequency list discarded nouns occurring wordnet evaluation experiments 
selected frequent nouns high frequency subset 
frequencies nouns lie range 
low frequency nouns selected nouns frequency ranks 
corresponds frequency range 
see appendix full listing nouns 
consider study nouns 
previous area considers small selection nouns frequent nouns 
example lee lee considers frequently oc nouns 
curran moens considers nouns randomly chosen wordnet cover range values certain word properties including frequency 
lin extensive study considering similarities nouns 
nouns selected frequent nouns occurring times lin parsed corpus analysis performance words different fre set nouns 
noted rosenfeld low frequency nouns stand gain distributional clustering techniques words want overcome sparse data problem 
distributional clustering technique works high frequency nouns may useful works low frequency nouns 
study set high frequency nouns set low frequency nouns order investigate techniques consideration perform significantly differently relative frequency word 
discussed detail section potential prob lem considering subset nouns may argued set subset universe nouns 
problem actual true neighbours word fall set 
words neighbour set generated complete set say nouns look different neighbour set gener ated considering similarities nouns 
number reasons restricting number nouns considered potential neighbours 
purely computational easier compute similarities similarities 
chapter 
existing measures distributional similarity second considering mid frequency nouns clear differentiation high low frequency nouns saying word frequency low frequency word frequency high frequency 
having described construction rationale data set discuss method compare distributional similarity measures 
cross comparison neighbours derived different measures possible noun noun combination set nouns data described section computed similarity nouns measures table 
sorted potential neighbours noun measure similarity order create ranked neighbour sets 
pointless majority applications consider word potential neighbour preclude possibility neighbour sets 
chance measure confusion probability measures word nearest neighbour 
table shows nearest neighbours word hope measures 
dangerous general observations single word see product moment correlation coefficient duplicates neighbours produced cosine mea sure coloured red dice coefficient duplicates neighbours produced jaccard coefficient coloured green apart duplicates huge variety neighbour sets possible evident 
distinct measures total different words suggested top neighbours hope 
words highlighted bold table suggested distinct measures feeling dream confidence desire 
order general quantitative comparison sets neighbours produced different measures technique adapted lin 
neighbour set comparison technique order compare neighbour sets size transform neighbour set neighbour rank score rank 
lin similarity scores directly require normalization scores computed different scales compared 
having performed transformation considered normalization neighbour sets word may represented ordered sets words wk 
similarity sets computed calculation lin simplifications due ranks wk wi rank scores words neighbour set 
comparison technique fact computing cosine neighbour set vectors 
consid ering words potential neighbours dimensions neighbour set space 
potential neighbours rank distance noun score zero score rank 
relevance colour groupings clear section chapter 
existing measures distributional similarity distl distl sim confidence value flavour flavour idea dream feeling feeling feeling time feeling child reason reason life desire meaning wish wish interest reason thought plenty plenty vision confidence meaning meaning people baby cause power opportunity desire desire desire man doubt authority sympathy sympathy thing flavour memory value value part dist js dist dist reason value view hem feeling feeling reason hand dissatisfaction vision idea plenty concern idea view idea scepticism doubt vision regard time concern confidence interest interest plenty fear chance way beak chance dream effect dream thought laugh money optimism faith power doubt feeling readiness sim sim simlin dream dream dream dream dream chance chance ambition spirit ambition confidence confidence confidence desire confidence commitment commitment love feeling love love love expectation opposition sense concern concern opposition ambition desire ambition ambition desire interest expectation sense sense pride fear doubt feeling feeling concern confidence suspicion expectation expectation fear memory feeling table neighbours hope associated similarity scores similarity measure earlier comments cosines correlation coefficients similar computing spearman rank order correlation coefficient neighbour sets 
want calculate correlation potential neighbours generally concerned order measure ranks dissimilar words 
lower value approximating subtracting means computing product moment correlation coefficient rank scores large number items chapter 
existing measures distributional similarity score zero find negative correlation sets 
words correlation lies represents maximum similarity sets 
achieve exactly words appear sets exactly order 
pair measures calculated similarity neighbour sets size noun set nouns 
computed mean standard deviation high frequency nouns low frequency nouns 
results tables respectively 
compactness measure represented just subscript table 
include product moment correlation coefficient dice coefficient table correlation measures cosine measure jaccard coefficient respectively nouns 
neighbour sets created randomly expect see average correlation nouns standard deviation 
accordingly majority measures overlap seen neighbour sets significantly greater expect chance significant overlap seen groups measure 
mi mea sures sim simlin 
second group distl sim js dist dist 
measures second group consider difference extent noun occurs verb 
considerable overlap sim 
groups coloured magenta cyan blue respectively tables 
overlap different measure neighbour sets general sig variation significant 
section look ways neighbours selected different measures vary frequency relative target noun 
high low frequency neighbours 
section look tendencies certain measures select high er low er fre quency words neighbours target noun 
order compare neighbour sets produced nouns considered potential neighbours high frequency nouns considered potential neighbours low frequency nouns considered potential neighbours 
possible ways analysing frequency characteristics neighbour sets 
example calculate average frequency top neighbours calculate proportion top neighbours high frequency low frequency 
neighbour set comparison technique appealing number reasons 
consistent comparing different similarity measures 
second weights importance neighbours rank set large number high frequency neighbours posi tions closest target noun considered biased large number high frequency neighbours distributed neighbour set 
third maximum score tells neighbour sets identical 
easily generalise results frequency characteristics application performance 
neighbour sets derived fre quent nouns identical derived set nouns know statistical significance level achieved exact depends measure due variation standard deviation observed 
chapter 
existing measures distributional similarity cos js cp hind lin cos js cp hind table mean cross measure correlation neighbour sets high frequency nouns 
figures brackets standard deviations need consider frequent nouns potential neighbours 
explain technique detail 
chapter 
existing measures distributional similarity cos js cp hind lin cos js cp hind table mean cross measure correlation neighbour sets low frequency nouns 
figures brackets standard deviations noun similarity measure sim sim may distance measures new sets ranked neighbours constructed set neighbour nouns restricted chapter 
existing measures distributional similarity high frequency subset nouns high sim set neighbour nouns restricted low frequency subset nouns low sim sets complete neighbour set comp sim 
new neighbour size 
neighbour set comparison technique section compare similarity neighbour sets single measure 
new sets mutually exclusive similarity zero 
comparing complete neighbour set comp sim neighbours frequency restricted frequency neighbour sets get estimate complete neighbour set high low frequency nouns 
complete neighbour set completely high frequency nouns cor relation high frequency neighbour set neighbours order correlation low frequency neighbour set 
conversely complete neighbour set completely low frequency nouns correlation high frequency neighbour set correlation low frequency neighbour set 
biases high low frequency nouns proportion high low frequency neighbours complete neighbour set expect get average correlation standard deviation high low frequency neighbour sets 
reason correlation greater shared neighbours top restricted frequency neighbour sets neighbours bottom restricted frequency neighbour sets complete neighbour set complete neighbour set iden tical restricted frequency neighbour set 
due calculate normalised score bias high frequency neighbours noun measure sim comp sim sim comp sim sim sim sim value lies range indicates neighbour set completely high frequency nouns indicates neighbour set completely low frequency nouns indicates neighbour set biases high low frequency nouns 
compute normalised score noun measure consider averages high low frequency nouns see neighbours high frequency nouns tend high low frequency nouns neighbours low frequency nouns tend high low frequency nouns 
tendencies measure select high frequency neighbours high low frequency nouns table 
seen measures possible exceptions distl dist strong tendencies select neighbours particular frequencies 
categorize mea sures relation frequency target noun tend select high er frequency neighbours nouns low er frequency neighbours noun neighbours similar frequencies nouns 
tendencies summarized table 
tendencies may surprising sight considers relationship word frequency distribution width number distinct contexts word occurs 
word observed distinct contexts observed indistinct contexts width 
frequency puts upper bound width expect find narrow words occurring frequently 
rarely case shown con chapter 
existing measures distributional similarity frequency target noun high low distl distl sim dist js dist dist sim sim simlin table average value high frequency target nouns low frequency target nouns 
values indicate measure biased selecting high frequency nouns neighbours target nouns values indicate measure biased selecting low frequency nouns neighbours target nouns 
high er frequency low er frequency similar frequency selecting measures selecting measures selecting measures sim sim distl dist dist js distl dist sim simlin table measures grouped tendency select high frequency low frequency similar fre quency neighbours table 
contingency table constructed frequent nouns nouns expect highest independence width frequency frequency excess 
null hypothesis uniformity independence frequency width expect find nouns sixteen groups 
performing test table shows observed correlation frequency width extremely significant calc 
frequent nouns widest nouns frequent nouns chapter 
existing measures distributional similarity frequency width high high med med low low total high high med med low low total table contingency table showing relationship distribution width frequency frequent nouns narrowest nouns 
accordingly similar frequency selecting measures selecting words occurred similar number distinct contexts higher frequency selecting measures selecting words occurred greater number distinct contexts lower frequency selecting measures selecting words occurred smaller number distinct contexts 
appear different distributional similarity measures implicitly tied different plausible hypotheses nature nearest neighbours word 
higher frequency selecting measures assumption best substitute noun contexts noun matter contexts 
lower frequency selecting measures assumption best substitute noun contexts noun matter contexts 
similar frequency selecting measures attempt trade extreme positions 
goals rest thesis discover position best context particular applications 
hypotheses see chapter suggest higher frequency selecting measures better semantic tasks lower frequency selecting measures better distributional tasks 
returning existing measures consideration frequency characteristics predicted definitions 
example similar frequency selecting measures consider ratio nouns share entire descriptions nouns 
way maximise ratio minimise features shared nouns 
words nouns similar number features 
measures tend focus commonality nouns commonality nouns relation noun features 
allows scope difference number features noun 
actual cause tendencies observed effect 
vast majority measures little gained considering words potential neighbours word 
part preliminary experiments performed pseudo disambiguation task described random set words frequency lying range 
different similarity measures dist dist distl simlin measure noun calculated different neighbour sets 
neighbour set restricted neighbours set frequent nouns restricted neighbours random set nouns third calculated nouns appearing twice direct object relation data extracted bnc 
chapter 
existing measures distributional similarity total nouns data neighbours taken set considered target noun true neighbours respect similarity measure 
performed pseudo disambiguation task sequence values neighbourhood size 
results minimum error rates corresponding values measure set neighbours summarized table 
dist dist distl simlin true neighbours random neighbours frequent neighbours table error rates pseudo disambiguation task measures true neighbours restricted neighbour sets 
figures brackets values neighbourhood size minimum error rates 
increase error rate observed neighbours longer taken entire set nouns set random nouns intuitively expect get best results consider noun potential neighbour 
neighbours restricted frequent noun set error rate lower random noun set 
means frequent nouns way considered random set nouns 
measures simlin increase performance true neighbours frequent neighbours 
measures dist distl decrease performance 
performance dist dist particularly interesting 
earlier chapter com mented dist retains meaning kl divergence measure whilst dist shown lee obtain best results pseudo disambiguation 
observed 
recall consider set random target nouns 
lee results hand neighbours frequent nouns calculated frequent nouns 
accordingly investigate calculated neighbours frequent nouns frequent nouns nouns dist dist 
results table 
dist dist true neighbours frequent neighbours table error rates pseudo disambiguation task dist dist measures true neighbours frequent neighbours 
figures brackets values minimum error rates 
managed achieve result lee 
clear rest results section dist performs words target words potential neighbours high frequency words 
accordingly majority experiments concentrate results dist opposed dist 
chapter 
existing measures distributional similarity chapter discussed broad range popular measures distributional similarity 
compared neighbours number produce data ex tendencies select high low frequency words neighbours 
seen jaccard coefficient dice coefficient produce identical neighbour sets cosine metric product moment correlation coefficient large variety neighbour sets produced different measures 
majority measures tend select words nearest neighbours higher frequency greater distribution width target word 
fits intuition substitute word able occur contexts target word occurs 
knowledge tendencies measures select words particular frequencies neigh explain predict behaviour application tasks 
knowledge may computational benefits 
example consider highest frequency words po neighbours measure going select neighbours set 
applications benefit distributional similarity measure particular fre quency characteristics 
example distributional similarity measure tends select higher frequency neighbours appropriate text simplification task see chapter 
frequency characteristics may negative impact particularly wish compare different types distribution 
example proposed word sense tion algorithm compare similarity neighbour descriptions target noun sense descriptions 
expect neighbour distribution wide target noun distribution measures 
wider target noun sense descriptions bias finding neighbour related widest frequent sense target noun 
chapter new general framework distributional similarity explore systematically questions distributional similarity 
substitutability similarity asymmetric 
second contexts important ones 
chapter general framework distributional similarity chapter new general framework distributional similarity 
framework directly defines similarity function require smoothing base language model allows explore systematically questions similarity raised chapter 
framework similarity words viewed measure appropriate word distribution place 
relative entropy inherently asymmetric measure appropriate word word separately appropriate word word section illustrate framework neighbours selected noun hope examine tendencies different models framework select high low frequency words neighbours 
third section describe characteristics existing distribu tional similarity measures chapter framework 
occurrence retrieval imagine formed descriptions noun terms verbs occur direct object relation corpus 
imagine lost accidentally deleted description noun happened noticed description noun similar noun 
decide description noun description noun hopefully notice 
depend validity substituting words similarity 
task set seen occurrence retrieval cr 
analogy information retrieval set documents retrieve set documents retrieve scenario set occurrences retrieve occurrences set occurrences retrieved occurrences 
continuing analogy measure done terms precision recall precision tells retrieved correct recall tells wanted retrieve retrieved 
general framework distributional similarity concept occurrence chapter 
general framework distributional similarity retrieval 
distribution word moves away identical word similar ways occurred generally distinct contexts word occurred generally distinct contexts word case call word high recall neighbour word second case call word high precision neighbour word explore merits symmetry asymmetry similarity measure varying relative importance attached precision recall 
remainder section devoted defining occurrence retrieval models crms 
models divided types 
simpler boolean concept objects sharing sharing particular feature objects nouns features verb occurrences 
type model referred additive model 
second type model incorporates difference extent noun fea ture type model referred difference weighted model 
exploring types models defined concepts precision recall allows investigate question posed chapter shared context worth regardless difference extent word appears context 
cr framework investigate second question posed chapter contexts treated equally different potential word association functions type model 
association functions decide verbs features noun determine relative importance features 
combinatorial probabilistic mi association functions allow define type token mi crms respectively 
models explore giving equal weight verb occurrence weight frequently occurring verb occurrences weight informative verb occurrences 
additive models considered intuition calculating precision recall occurrence re trieval formulate formally terms additive model 
need consider noun verb occurrences retrieved predicted conversely required description 
refer verbs features degree association noun verb possible association functions described context particular crms 
shared features noun noun referred set true positives precision retrieval features proportion features shared nouns feature weighted relative importance term association describe degree extent words occur 
chapter 
general framework distributional similarity add recall retrieval features proportion features shared nouns feature weighted relative importance add precision recall lie range equal noun exactly features 
noted recall retrieval equal precision retrieval add add 
role association function considered carefully 
decides occurrences important put description analogy document retrieval occurrences want retrieve occurrences retrieved 
weight contexts importance 
case tells retrieval process perceived relevance verb tells actual relevance verb define possible additive models occurrence retrieval type crm token crm mutual information crm 
mathematically differences models association functions determine weight features 
additive type crm model precision retrieval proportion distinct verbs verb occurrence types occur occur recall retrieval proportion distinct verbs verb occurrence types occur occur 
alternatively say precision retrieval probability verb type picked random occurring occurs recall retrieval probability verb type picked random occurring occurs 
formalise additive type crm combinatorial association function 
degree association dtype noun verb occurred dtype case noted value feature sum definitions precision recall 
definitions simplified follows add type dtype dtype add type dtype dtype simplified expressions correspond intuitions start sec tion precision proportion occurrence types occur occur chapter 
general framework distributional similarity recall proportion occurrence types occur occur 
additive token crm model contrast type model interested retrieving possibly verbs verb occurrence tokens 
precision retrieval proportion indistinct verbs verb occurrence tokens occur occur recall retrieval proportion indistinct verbs verb occurrence tokens occur occur 
alternatively say precision retrieval probability verb token picked random occurring occurs recall retrieval probability verb token picked random occurring occurs 
formalise model probabilistic association function 
degree associ ation noun verb defined token crm nouns features type crm feature weight probability occurrence 
alternatively frequency association function frequency verb noun occurred 
leads expressions precision recall probabilistic association function highlights fact values precision recall independent absolute amount data considered noun 
probabilistic association function allows easily see definitions precision recall model simplified 
basic probability theory tells follows add tok add tok simplified expressions correspond intuitions token crm start section 
precision probability verb token picked ran dom distribution occurs recall probability verb token picked random distribution occurs 
additive mutual information crm chapter 
general framework distributional similarity mutual information mi allows capture idea occurrence low probability events informative occurrence high probability events 
example occurrence dog tell dog occurs frequently different nouns 
hand dog may occurred frequently object walk walk occurs lot frequently general learn dog occurrence 
formally church hanks hindle lin degree association noun verb pointwise mutual information dmi log expression tells probability verb increased knowing noun conversely probability noun increased knowing verb purposes base log taken irrelevant measuring ratio 
mi association function means verb considered feature noun probability occurrence greater expected verbs nouns occurred independently 
informative verb occurrences add sums calculation precision recall weight 
expressions precision recall additive mi crm rewritten follows difference weighted models add mi add mi possible criticism additive models nouns features con sidered identical regardless feature occurs probability extent noun 
section define type model allows capture difference extent noun feature 
define difference function difference extent noun associated feature difference difference value zero association scores identical 
opposite require weight similarity score need convert kind shared extent similarity function 
possibility subtract difference maximum possible score 
example association function conditional probability verb noun maximum difference shared extent function difference 
undesirable property chapter 
general framework distributional similarity score lie different scales different association functions 
alternative approach analogy form distributional similarity measure defining 
basis attempt define similarity nouns respect individual feature principles define similarity nouns respect features 
order measure precision recall individual feature 
define extent function extent goes may necessarily association function 
possible extent functions discussed context particular crms 
having defined function measure precision recall individual features 
precision individual feature retrieved extent nouns go divided extent goes recall retrieval extent nouns go divided extent goes min min precision recall individual feature precision recall distribution lie range 
redefine precision recall follows dw dw precision recall individual features weights definitions precision recall distribution captures intuition retrieved verb simply right wrong right wrong certain extent 
words features shared similar extent noun considered important calculation distributional similarity 
show precision recall individual feature incorporated type token mi crms 
association functions defined earlier measure precision recall retrieval individual feature terms tokens terms mi 
sense measure type precision recall terms types individual feature types tokens equivalent 
difference weighted type crm difference weighted type crm verb occurrence type consideration weighted precision recall individual feature 
stated difference type token retrieval individual feature extent function probabilistic association function etype consequently rewrite expressions precision recall individual feature follows chapter 
general framework distributional similarity ptype min rtype min ptype ptype equal 
context precision represents extent verb retrieved correct 
context recall represents extent verb required retrieved 
substituting expressions precision recall get dw type dtype ptype dtype dw type dtype rtype dtype min min note different additive token model token effectively considered model tokens weighted equally 
model tokens treated differently type belong 
importance retrieval non retrieval single token depends proportion tokens particular type constitutes 
difference weighted token crm model verb token consideration weighted precision recall tokens type 
extent function probabilistic association function precision recall functions individual features type crm min min accordingly derive expressions precision recall follows tok min dw min min dw chapter 
general framework distributional similarity tok min min min dw tok note defined separate precision recall functions arrived expression model 
difference weighting takes account verb tokens retrieved precision retrieved incorrectly recall 
result model symmetric may place asymmetric framework 
discount possibility symmetric relationship similar words simply assume 
inclusion symmetric model derived asymmetric principles framework simply special case framework see consider combine precision recall 
allowing symmetry thoroughly explore relative merits symmetry asymmetry similarity 
difference weighted mi crm consider extent noun occurs verb terms mi emi log gives expressions precision recall individual feature pmi min rmi min difference weighted mi crm gives expressions preci sion recall retrieval occurrence distribution mi min dw mi min dw combining precision recall min min previous sections concerned defining pair numbers represent similarity words 
applications necessary compute single number order determine neighbourhood cluster membership 
classic way chapter 
general framework distributional similarity combine precision recall information retrieval compute score harmonic mean precision recall mh harmonic mean numbers lies numbers substantially closer lower numbers 
value maximised words achieve reasonably high score precision recall reasonably high 
wish assume substitute requires high precision high recall target distribution 
may situations best noun place noun retrieves correct occurrences high precision neighbour may retrieves required occurrences high recall neighbour 
factor case may play secondary role role 
retain generality investigate high precision high recall high precision high recall required high similarity computing weighted arithmetic mean harmonic mean weighted arithmetic mean precision recall mh ma sim mh ma lie range 
resulting similarity sim lie range low high 
formula combination models precision recall outlined earlier 
precision recall computed pair words model similarity depends values 
generality allows investigate empirically relative significance different terms omitted 
table summarizes special parameter settings 
special case harmonic mean precision recall score weighted arithmetic mean precision recall precision recall unweighted arithmetic mean table table special values section developed framework concept occurrence retrieval cr 
framework defined number models crms allow chapter 
general framework distributional similarity explore systematically questions similarity 
difference extent noun takes feature matter 
second features inherently salient due higher probability occurrence higher information content 
third similarity nouns necessarily symmetric relationship 
crms parameter settings provide different answers questions 
difference weighted models contrast additive models considering difference extent noun takes feature 
type token mi crms contrast relative importance attached features 
high precision neighbour necessarily high recall neighbour conversely high recall neighbour necessarily high precision neighbour constrained symmetric relationship similarity words 
section consider effects answers questions nearest neighbours noun 
neighbours selected occurrence retrieval models section illustrate neighbours obtained special parameter settings model framework 
consider relationship parameter settings tendency high low frequency neighbours selected 
neighbours hope chapter neighbours word hope selected sample set words see appendix 
neighbours hope purposes illustration attempt draw neighbour sets derived just word 
tables show top neighbours hope associated similarity scores number different parameter settings additive type model additive token model additive mi model difference weighted type model difference weighted mi model respectively 
table shows top neighbours hope associated similarity scores selected distance weighted token crm 
neighbours selected model regardless parameter settings precision equals recall 
noted parameter settings crms vast majority similarity measures consider word identical word closest neighbour word neighbour may depend application consider issue chapters 
time consider trivial decide best word place word word consider word highest similarity score nearest neighbour 
relationship parameter settings frequency neighbours selected section examine tendencies different models select neighbours particular frequencies technique described chapter section 
particular consider words identical word joint closest neighbour 
chapter 
general framework distributional similarity precision recall arith 
mean harm 
mean stir time interest dream para idea chance people feeling confidence talking idea stir commitment life value love man sense concern thing life ambition footing interest dream sense part chance feeling power power expectation table neighbours hope additive type crm precision recall arith 
mean harm 
mean para sense sense thing place place side chance chance part thing thing life feeling feeling hello time part part talking position idea faith feeling reason idea man faith reason utmost people advantage dream table neighbours hope additive token crm relationship parameter settings model frequency target noun frequencies nearest neighbours 
nature precision recall expect find high recall neighbours occur distinct contexts target noun high precision neighbours occur distinct contexts target noun 
distribution width frequency correlated expect result precision selecting lower frequency neighbours recall selecting higher frequency neighbours 
harmonic mean optimised precision recall high expect lead neighbours similar frequency target noun selected 
arithmetic mean high high precision compensate low recall vice versa 
accordingly expect neighbour sets biased high low frequency neighbours 
order technique described chapter section 
recall technique compare neighbour sets size restricted high low frequency nouns neighbour sets unrestricted frequency 
chapter 
general framework distributional similarity precision recall arith 
mean harm 
mean dream dream dream feeling ambition sense confidence interest ambition love claim confidence desire spirit sense idea expectation thought doubt self confidence man confidence suspicion para plan fear table neighbours hope additive mi crm precision recall arith 
mean harm 
mean regard state dream dream kind commitment commitment heed practice concern confidence prospect value confidence concern chance belief feeling love dinner world love ambition confidence feeling memory feeling impression power belief memory doubt society value expectation laugh industry kind chance table neighbours hope difference weighted type crm computing mean cosine correlation neighbour sets restricted frequency unrestricted frequency 
normalise dividing average correlation high frequency neighbour sets sum correlation scores 
results score range represents maximal tendency select low frequency neighbours represents maximal tendency select high frequency neighbours represents tendency direction 
question remains deviation represents significant tendency select high low frequency nouns neighbours 
standard deviations correlation scores observed lay range 
central limit theorem mean distributed normally variance distribution mean variance original distribution divided number observations 
consequently observed data maximum standard error square root variance mean 
means deviation significant level deviation statistically impossible bias high low frequency nouns chapter 
general framework distributional similarity values confidence dream feeling desire reason vision baby opportunity doubt flavour table neighbours hope difference weighted token crm 
parameter settings irrelevant model precision equal recall 
precision recall arith 
mean harm 
mean dream dream dream regard spirit ambition ambition dream desire confidence confidence ambition feeling doubt doubt confidence opposition desire love doubt ambition love desire self confidence interest spirit faith fear feeling suspicion prospect confidence interest expectation memory faith sense table neighbours hope difference weighted mi crm neighbours 
deviations observed far greater suggesting chance selection high low frequency nouns neighbours 
table summarises tendencies model select high low frequency nouns 
look model detail 
additive type crm illustrates tendencies additive type crm select high frequency neighbours different parameter settings high low frequency target nouns 
model strong tendencies high recall neighbours high frequency nouns high precision neighbours low frequency nouns 
harmonic mean selects high frequency neighbours high frequency nouns low frequency neighbours low frequency nouns 
words requiring precision recall fairly high penalising low leads neighbours similar frequency target noun 
crm tendencies additive type additive token additive mi chapter 
general framework distributional similarity high recall selects high frequency neighbours high precision selects low frequency neighbours high harmonic mean selects similar frequency neighbours selects high frequency neighbours high recall selects high frequency neighbours high precision selects low frequency neighbours high harmonic mean selects similar frequency neigh high unweighted arithmetic mean appears un biased difference weighted type selects similar frequency neighbours difference weighted token selects high frequency neighbours bias small low frequency target nouns difference weighted mi selects similar frequency neighbours table summary tendencies select high low frequency neighbours models framework additive token crm illustrates tendencies additive token crm select high frequency neighbours different parameter settings high low frequency target nouns 
model appears tendency select high frequency neighbours pa rameter settings high low frequency target nouns 
tendency strongest low values high recall seen fairly high values consider apparent bias direction 
tendency stronger high frequency target nouns low frequency target nouns suggests bias high frequency target nouns nouns higher frequency target noun 
regard appears tendency increase preference similar frequency neighbours counter high frequency preferring nature model general 
additive mi crm illustrates tendencies additive mi crm select high frequency neigh different parameter settings high low frequency target nouns 
model fairly strong tendency high recall select high frequency neigh high precision select low frequency neighbours irrespective frequency target nouns 
harmonic mean produces neighbour sets neighbours similar frequency target noun 
tendencies similar seen ad chapter 
general framework distributional similarity tendencies additive type crm select high frequency neighbours high low frequency target nouns type crm strong 
additive mi crm unweighted arithmetic mean produces neighbours sets apparent bias high low frequency neighbours 
difference weighted type crm illustrates tendencies difference weighted type crm select high frequency neighbours different parameter settings high low frequency target nouns 
model strong tendency select neighbours similar frequency target noun settings parameters little impact 
difference weighted token crm model average bias high frequency neighbours high frequency target nouns average bias high frequency neighbours low frequency target nouns 
suggests tendency selecting chapter 
general framework distributional similarity tendencies additive token crm select high frequency neighbours high low frequency target nouns neighbours higher frequency model strong models additive token crm 
difference weighted mi crm illustrates tendencies difference weighted mi model select high frequency neighbours different parameter settings high low frequency target nouns 
model strong tendency select neighbours similar frequency target noun 
settings parameters impact tendency 
high precision reduces tendency high frequency neighbours high frequency target nouns high recall reduces tendency low frequency neighbours low frequency target nouns 
chapter 
general framework distributional similarity tendencies additive mi crm select high frequency neighbours high low frequency target nouns section looked neighbours produced different models framework 
particular examined tendencies different parameter settings select high low frequency neighbours 
seen general high precision produces tendency low frequency neighbours high recall produces tendency high frequency neigh high harmonic mean produces tendency selecting neighbours similar frequency target noun 
line predictions explanation start section 
time apparently unbiased neighbour sets produced unweighted arithmetic mean additive mi crm 
possible reason mod els inherent biases strengthen weaken biases caused parameter settings 
example difference weighted type mi crms tend select neighbours similar frequency target noun additive token crm tends select high chapter 
general framework distributional similarity tendencies difference weighted type crm select high frequency neigh high low frequency target nouns frequency neighbours high low frequency target nouns 
characterisations existing measures section observations regarding existing similarity measures cr framework offered chapter 
existing measures directly derived approximated certain conditions cr 
offer mathematical proofs relationships 
measures derived cr framework consider model parameter settings closely simulate 
neighbour set comparison technique 
cases calculated mean similarities neighbour sets high frequency nouns low frequency recall take nearest neighbours measure model parameter setting compute average correlation rank scores 
chapter 
general framework distributional similarity tendencies difference weighted mi model select high frequency neigh high low frequency target nouns nouns 
need consider differences means considered statistically significant 
maximum standard deviation correlation scores observed corresponds standard error normally distributed mean 
difference means normally distributed standard deviation 
corresponds difference significant level difference significant level difference statistically impossible chance 
accordingly refer levels significance differences greater 
consider measure chapter turn 
table summarizes observations measure 
norm recall norm defined measure observations distl distl sim sim dist js dist dist simlin sim chapter 
general framework distributional similarity distance equivalent difference weighted token crm closely simulated difference weighted token crm closely simulated difference weighted token crm approximated recall additive type crm equivalent additive type crm produces identical neighbour sets additive type crm closely simulated difference weighted token crm closely simulated high recall parameter settings additive token crm closely simulated high precision parameter settings additive token crm closely simulated high recall parameter settings additive token crm closely simulated range parameter settings difference weighted mi crm produces identical neighbour sets recall difference weighted mi crm highly similar additive mi crm closely simulated setting number models including additive mi crm table summary observations existing measures cr framework distl noting algebraic equivalence min basic probability theory rewrite norm follows min min min min min chapter 
general framework distributional similarity sim dw tok words norm directly related difference weighted token crm 
constant multiplying factor required crm defines similarity range norm defines distance range distance equivalent similarity scale 
norm expect find distl exactly simulated crm quadratic differences feature values 
discussed analyse relationship norm cr framework empirically comparing neighbour sets pro duced 
table shows optimal parameter settings optimal similarities measure crms calculated neighbour set comparison technique described chapter 
target noun frequency crm high low sim 
sim 
add type add tok add mi diff type diff tok diff mi table mean optimal similarities crms distl figures brackets standard deviations 
seen table distl closely simulated crms 
closest approximation high low frequency target nouns symmetric difference weighted token crm 
relationship measure model expected model exactly simulates norm norm comes family measures norm 
seen chapter similarity neighbour sets derived norm norm great observed obviously related pairs measures norm jensen shannon divergence measure 
cosine measure obvious similarity form cosine measure crms 
table shows optimal parameter settings optimal similarities measure crms calculated neighbour set comparison technique 
seen table closely simulated crms 
clos est approximation high low frequency target nouns symmetric difference weighted token crm 
chapter 
general framework distributional similarity target noun frequency crm high low sim 
sim 
add type add tok add mi diff type diff tok diff mi table mean optimal similarities crms 
figures brackets standard deviations 
kendall coefficient recall description kendall coefficient chapter 
pair occurrence types vi vj consider signs quantities vi vi 
signs verb pair concordance signs different pair quantity zero pair tie 
sim estimated number observed concordances number observed set possible occurrence types 
consider measure terms type occurrence retrieval 
occurrence type may feature nouns noun noun 
due nature occurrence data expect large number occurrence types observed noun 
accordingly pairs verbs verb observed noun dominate expression coefficient 
case verb goes target noun tie potential neighbours 
verb goes target noun concordance depend verb occurs potential neighbour 
small number cases verbs occur target noun result concordance depend extent verb seen noun 
small minority verb pairs consequently value coefficient largely depend number shared occurrence types size set true positives 
consider recall additive type crm add type dtype dtype target noun value constant 
means differ ence recall values potential neighbours target noun depends 
consequence expect neighbour sets coefficient closely simulated parameter setting recall additive type crm 
close chapter 
general framework distributional similarity approximation depend largely verbs occur noun 
table summarises optimal parameter settings optimal mean similarities neighbour sets sim crms 
target noun frequency crm high low sim 
sim 
add type add tok add mi diff type diff tok diff mi table mean optimal similarities crms sim 
figures brackets standard deviations 
predicted high similarity neighbour sets coefficient high recall settings additive type crm 
shows similarity neighbour sets varies parameters crm 
seen optimal similarity neighbour sets close high low frequency nouns 
optimal mean similarity values high frequency nouns low frequency nouns 
small amount precision incorporated settings may approximate adjustment required verbs occur nouns 
values respectively high low frequency target nouns close optimal values difference low frequency target nouns statistically significant level 
suggests coefficient completely determined number shared occurrence types 
dice jaccard coefficient recall definition dice coefficient shown formula equivalent special case framework harmonic mean precision recall score additive type crm 
mh add type add type 
add type add type add type add type chapter 
general framework distributional similarity variation parameters mean similarity neighbour sets additive type crm sim accordingly set additive type crm dice coefficient exactly replicated 
seen jaccard coefficient dice coefficient produce identical neighbour sets goes saying neighbour sets produced additive type crm settings identical produced jaccard coefficient 
relationship framework existing measures distributional similar ity viewed empirically comparing neighbours produced jaccard coefficient dice coefficient produced additive type crm different parameter settings 
mean similarities calculated neighbour set comparison technique chapter 
general framework distributional similarity illustrated 
predicted similarity exactly high low frequency target nouns 
decreases similarity measures 
lower values high recall leads higher similarity neighbour sets high frequency nouns high precision leads higher similarity neighbour sets low frequency nouns 
variation parameters mean similarity neighbour sets additive type crm sim 
jensen shannon divergence measure having noted similarity jensen shannon divergence measure norm chapter norm difference weighted token crm chapter expect find measure similar difference weighted token crm 
chapter 
general framework distributional similarity table shows optimal mean similarities neighbour sets derived crms dist js 
usual mean similarities calculated separately high frequency target nouns low frequency target nouns 
target noun frequency crm high low sim 
sim 
add type add tok add mi diff type diff tok diff mi table mean optimal similarities crms dist js 
figures brackets standard deviations 
seen table neighbour sets derived dist js high low fre quency target nouns significantly similar derived difference weighted token crm crm 
skew divergence measure due form skew divergence measure expect crms exactly simulate 
note measure take account differences probabilities occurrences distribution log ratio expect fairly closely simulated difference weighted token crm 
expect similarity mi models dist approximation information theoretic concept relative entropy 
lastly recall skew divergence measure asymmetric 
dist measures cost distribution calculated verbs occur 
dist measures cost distribution calculated verbs occur 
expect dist high recall measure recall calculated occurrences dist high precision measure precision calculated occurrences 
table shows optimal mean similarities neighbour sets derived crms dist 
usual mean similarities calculated separately high frequency target nouns low frequency target nouns 
results table observe 
crms able simulate dist closely 
differences models fairly small 
high frequency nouns best performing models difference weighted token model additive token model 
low frequency nouns best performing models additive mi model additive token model 
notably asymmetric models get closest high levels recall high low frequency nouns 
chapter 
general framework distributional similarity target noun frequency crm high low sim 
sim 
add type add tok add mi diff type diff tok diff mi table mean optimal similarities crms dist 
figures brackets standard deviations 
illustrates variation mean similarity neighbour sets parameters additive token model 
high frequency target nouns optimal similarity significantly higher 
low frequency target nouns optimal similarity considerably higher 
appears dist high recall cr characteristics 
contrast dist reader recall equivalent dist arguments reversed 
table shows optimal mean similarities neighbour sets derived crms dist 
usual mean similarities calculated separately high frequency target nouns low frequency target nouns 
target noun frequency crm high low sim 
sim 
add type add tok add mi diff type diff tok diff mi table mean optimal similarities crms dist 
figures brackets standard deviations 
optimal performance figures simulating dist similar simulating dist 
larger differences results high low frequency target nouns 
consistently high performing model case additive token crm 
predicted reversal values optimal performance obtained 
majority crms neighbours high low frequency target nouns required high precision simulate dist 
illustrated ad token crm 
observe high frequency target nouns particular effect exact reflection seen dist 
chapter 
general framework distributional similarity variation parameters mean similarity neighbour sets additive token crm dist amount recall required neighbour sets similar 
sacrificing recall high precision leads considerable drop performance 
high precision neighbour noun occurred distinct context 
noun precision score retrieval noun occurs context 
parallel terms recall expect find noun occurs verb 
confusion probability table shows optimal mean similarities neighbour sets derived crms 
usual mean similarities calculated separately high frequency target nouns low frequency target nouns 
closely simulated crms confusion probability closely simulated additive token crm 
shows mean similarity chapter 
general framework distributional similarity variation parameters mean similarity neighbour sets additive token crm dist target noun frequency crm high low sim 
sim 
add type add tok add mi diff type diff tok diff mi table mean optimal similarities crms 
figures brackets standard deviations 
chapter 
general framework distributional similarity neighbour sets varies measure crm 
appear confusion probability high recall measure 
variation parameters mean similarity neighbour sets additive token crm table shows optimal mean similarities neighbour sets derived crms usual mean similarities calculated separately high frequency target nouns low frequency target nouns 
closely simulated crms reversed confusion probability closely simulated difference weighted mi crm 
shows mean similarity neighbour sets varies measure crm 
expected high precision measure contrast high recall measure 
relationship neighbour set similarity parameter settings observed suggests neighbour sets contain fairly mix high precision high recall neighbours 
chapter 
general framework distributional similarity target noun frequency crm high low sim 
sim 
add type add tok add mi diff type diff tok diff mi table mean optimal similarities crms figures brackets standard deviations 
variation parameters mean similarity neighbour sets difference weighted mi crm hindle measure chapter 
general framework distributional similarity recall chapter variant hindle measure considered defined min expression numerator expressions precision recall difference weighted mi crm mi min dw mi min dw min min 
note denominator expression recall depends constant similarly denominator expression precision constant 
target noun remain calculate neighbour set 
accordingly value recall potential neighbour value divided constant 
words value directly proportional recall difference weighted mi crm expect neighbour sets obtained identical 
shown empirically technique 
mean similarity neighbour sets high low frequency nouns 
precision taken account increasing similarity begins drop 
denominator expression precision depends potential neighbour target noun 
lin mi measure recall chapter lin mi measure defined simlin parallels simlin measures compute ratio shared descriptions nouns sum descriptions noun 
major difference appears mi predicted close relationship simlin harmonic mean additive mi crm 
relationship shown mh add mi add mi 
add mi add mi add mi add mi chapter 
general framework distributional similarity variation parameters mean similarity neighbour sets difference weighted mi crm follows mh simlin additive mi model condition holds framework reduces simlin 
chapter 
general framework distributional similarity necessary condition equivalence expect hold pairs words 
accordingly order investigate approximation harmonic mean simlin practice compared neighbour sets measure neighbour set comparison technique outlined earlier 
variation parameters mean similarity neighbour sets additive mi crm simlin illustrates similarity simlin additive mi crm high low frequency nouns 
average similarity neighbour rankings high frequency nouns low frequency nouns 
significantly higher similarities standard similarity measures 
optimal approximation simlin additive mi crm 
settings average similarity high frequency nouns low frequency nouns differences significant level 
suggests simlin allows compensation lack recall precision vice versa harmonic mean 
mi variant jaccard coefficient chapter 
general framework distributional similarity expect sim exactly predicted additive mi crm fundamental difference way utilises mi 
mi variant jaccard measure uses mi determine occurrences noun features mi crm uses mi determine features noun weights similarity calculation 
theoretically speaking sim appears sit halfway sim simlin 
accordingly expect best simulated setting additive type crm additive mi crm 
optimal parameter settings optimal similarities measure crms shown table 
target noun frequency crm high low sim 
sim 
add type add tok add mi diff type diff tok diff mi table mean optimal similarities crms sim figures brackets stan dard deviations 
predicted sim closely approximated additive type model additive mi model 
sim appears significantly closer crm high frequency nouns significantly closer additive type model low frequency nouns 
difference weighted type mi models parameter settings approximation sim fairly close 
cases predicted best simulations obtained higher values 
best simulation 
variation performance additive mi crm illustrated 
low frequency nouns performance difference observed statistically insignificant level 
defined framework measuring lexical distributional similarity concept substitutability 
cast problem occurrence retrieval measure precision recall analogy way measured document retrieval 
similarity defined substitutability inherently asymmetric 
cr framework require similarity viewed asymmetric settings model symmetric model difference weighted token model symmetric 
symmetric settings may thought capturing notion inter substitutability 
potential exploiting asym similarity explored chapters context applications distribu chapter 
general framework distributional similarity variation parameters mean similarity neighbour sets additive mi crm sim tional similarity 
idea asymmetry substitutability framework comparable measures relative entropy confusion probability 
consider precision recall 
framework advantages directly defines similarity measure require smoothing base language model allows explore systematically questions relative importance different features 
order defined different occurrence retrieval models crms 
models additive models difference weighted models 
allows explore effect difference feature values shared features 
additive difference weighted models defined type token mi models 
allows explore effect weighting different features degree relevance association 
rest chapter devoted analysing neighbour sets produced cr chapter 
general framework distributional similarity framework determining cr characteristics existing distributional similarity measures 
saw general high recall neighbours tend high frequency nouns high precision neighbours tend low frequency nouns high harmonic mean tends produce neighbours similar frequency original noun 
crm effect frequency neighbours produced 
particular difference weighted models tend flatter respect parameters produce neighbours similar frequency target noun 
note setting additive mi model appears produce neighbour sets particular tendencies high low frequency nouns 
regard existing distributional similarity measures seen con sidered directly derived closely approximated cr framework 
summary findings table 
knowledge cr characteristics existing measures helps understand measures 
example high recall measures sim dist tend select higher frequency nouns 
measures require recall precision dist select lower frequency neighbours high recall measures 
measures require high recall high precision sim simlin sim select neighbours similar frequency target noun 
cr framework simulates existing measures distributional similar ity 
defines space distributional similarity measures populated named measures 
exploring space discover desirable characteristics distributional similarity measures 
may turn useful measure space discovered may discover new optimal combination char 
primary goal understand different characteristics relate high performance different applications explain measure performs better 
goal mind turn applications distributional similarity 
chapter consider characteristics distributional similarity measures desirable application language modelling 
considering distributional similarity formation language modelling considering performance different models different parameter settings pseudo disambiguation task 
chapter language modelling discussed chapter section smoothing probability estimates occurrences nearest neighbour techniques intuitively plausible way providing reliable probability estimates 
chapter consider evaluation techniques application area 
section discuss nearest neighbours derive smoothed occurrence probability distributions 
evaluate probability distributions consider ing similarity unseen empirical distributions 
section compare number similarity measures including cr framework chapter area pseudo disambiguation experiments 
deriving probability distributions section consider nearest neighbours derive smoothed occurrence prob ability distributions 
consider intuitions nearest neighbour smoothing aims section 
smoothing algorithm method evaluation results 
low probability verb equally plausible nouns observed sample language data probability theory tells occurred times noun whilst reality know due discretization data occurred single time just nouns 
see data expect probabilities observed data approach theoretical population probabilities see verb occurring roughly equal number times noun 
aim smoothing provide better probability estimates sample distribution possible predict occurrences observed data seen 
smoothing algorithms achieve increase performance maximum likelihood estimation mle simply assuming impossible 
add smoothing assumes occurrences equal prior probability occurring back smoothing assumes chapter 
language modelling absence reliable empirical estimates events nouns verbs occur independently 
assumption nearest neighbour smoothing words seen similar terms occurrences share occurrences observed 
basis body nearest neighbour smoothing hindle brown dagan pereira dagan 
hypotheses aim investigate section 
nearest neighbour smoothing plausible approach smoothing 
scenario nouns share verb occurrence seen share verb occurrences 
accordingly distribution smoothed similar distribu tions similar population distribution original sample distribution 
consequently similar original random sample random sample taken population 
second hypothesis sample distributions frequently occurring nouns closer population distributions frequently occurring nouns 
verb occur occur single occurrence noun frequently occurring 
generally expect changes required distribution high frequency noun small probability adjustments allow probability mass reassigned lower frequency nouns 
changes distributions lower nouns include addition new features changes probabilities single feature significant respect distribution lower frequency noun initially features 
order investigate hypotheses smooth empirical distributions frequent nouns distributionally nearest neighbours measure distance new distribution original distribution unseen empirical distribution 
expect see derived distribution closer original distribution unseen distribution 
ideally derived distribution minimal distance empirical distributions population distribution predict equal error 
order investigate second hypothesis iterative smoothing algorithm stops certain number neighbours considered certain amount change original distribution caused 
expect see determining amount change allowed function word frequency lead improved results fixed number neighbours word 
smoothing algorithm evaluation technique rely distribu tional similarity measure determine neighbours distance original distribu tion determine distance resultant distribution empirical distributions compare distributional similarity measures approach 
example bad similarity measure base similarity estimate frequently occurring feature 
expect measure finding neighbours find derived distributions close empirical distributions 
expect distributional similarity measures considered bad lar evaluate distributional similarity measures distributional similarity measures 
con chapter 
language modelling attempt evaluate hypotheses approach pseudo disambiguation experiments compare measures 
similarity measure chosen skew divergence measure rela tive entropy established way measuring cost model distribution place empirical distribution language modelling 
may different distributional similarity measure smoothing process evaluation process 
explore 
note plausibility nearest neighbour smoothing demonstrated dagan 
experiments 
feel worth reasons 
plausibility nearest neighbour smoothing important issue motivates substantial amount distri similarity measures 
second differs dagan 
secondary hypothesis difficult show context nearest neighbour smoothing demonstrating plausibility nearest neighbour smoothing 
third experiments basis cross validating pseudo disambiguation method comparing distributional similarity measures section 
turn attention smoothing algorithm 
smoothing algorithm original data set experiments consisted noun verb direct object pairs described chapter 
selected data pertaining frequent nouns divided halves training testing 
distributional similarity calculated pair nouns training data ordered list neighbours constructed noun 
smoothed derived distribution target nouns initialised sample distribution noun vector verbs associated conditional ties observed training data 
noun current distribution ci updated distribution nearest neighbour ni calculating weighted average distribu tions 
process repeated nearest neighbour stopping condition met 
process illustrated 
discuss calculation weights possible stopping conditions 
combining probability distributions simple way combine distributions take average corresponding proba bilities distribution 
approach take consideration rank neighbour similarity target noun 
intuitively neighbour identi cal target noun learn new noun verb occurrences neighbour 
expect different population distributions give rise identical sample distributions confident sample distributions close population distributions 
neighbours similar target noun add new noun verb occurrences distribution target noun confident verbs occur target noun 
occurrences assigned lower probabilities 
neighbours suggest initialisation phase initialise current distribution noun sample occurrence distribution calculate weight get ni distribution ith nearest neighbour calculate weight ni ci ci weight ci ni weight ni weight ci weight ni stopping condition met 
output ci calculate weight ci chapter 
language modelling algorithm smoothing occurrence distribution target word nearest neighbours verb occurrence confidence verb occur target noun increases probability assigned occurrence increase consequence 
intuitions modelled assigning weights current distribution neighbour distribution calculating weighted average distributions 
weight neighbour distribution depends similarity target noun distribu chapter 
language modelling tion 
similarity function dist neighbour distribution weight calculated negative exponential function neighbour distance target noun 
target noun neighbour consideration weight distribution dist function special case function proposed dagan 
justified follows 
similarity nouns calculated average log ratio corresponding probabilities distribution 
accordingly inverse compute weight probability estimate neighbour distribution 
word distance distance function dist weight initial current distribution 
weighted average calculated new distribution current distribution iteration weight sum weights individual distributions simulating increasing confidence current distribution data seen 
stopping conditions output current distribution new distribution certain condition met 
order investigate secondary hypothesis experimented different stopping conditions 
fixed value number nearest neighbours exceeded 

combining neighbour mean distance original distribution new distribution exceeds fixed threshold 

combining neighbour mean distance original distribution new distribution exceeds threshold determined function frequency target noun 
condition number neighbours considered target noun 
number considered global parameter optimised target nouns 
second stopping condition allows number neighbours considered vary word word 
combining neighbour cause change original em distribution 
skew divergence measure determines distribution approximates measure amount change dist ci original unsmoothed distribution ci proposed new distribution 
note dif ferent considering neighbours fixed similarity distance target noun exact cut point depend changes caused nearer neighbours 
neighbour similar neighbours reinforcing changes neighbours allowed neighbour dissimilar neighbours 
third stopping condition simulate having confidence maximum hood estimates pertaining high frequency target nouns 
words distributions high frequency target nouns allowed change distributions low frequency target nouns 
intuition third stopping condition strongly supported initial statistical study 
study distance half data set training data chapter 
language modelling data set best estimate population probabilities measured noun dist 
distance plotted word frequency 
distance half data set data set plotted word fre quency ignoring anomalous point appears smooth line puts upper bound distance half data set data set 
line fitted function form threshold log log req function implement third stopping condition leaving parameter optimised 
shape function shown 
evaluation central evaluation task idea language modelling tasks seen chunk occurrence data want predict probabilities occurrences unseen chunk occurrence data 
maximum likelihood estimation predicts unseen chunk identical seen chunk 
aim smoothing algorithm changes distributions seen chunk order better predict unseen chunk 
course sample perfectly representative population estimated population probabilities ideally able predict random chunk approximately error 
accordingly measure distance potential model distributions half data set skew divergence measure 
technique akin relative entropy measure cost model distribution place empirical distribution 
skew divergence measure relative entropy means penalise heavily model distribution provide estimate occurrence empirical anomalous point corresponds word expected total data training data approximately chapter 
language modelling distribution 
approach approximately equivalent implementing hierarchical smoothing algorithm backs unigram probabilities absence similarity probability estimates evaluating relative entropy 
paradigm optimal smoothing technique provide estimated probability distri butions minimal distance seen training data distributions unseen test data distributions 
paradigm evaluate baselines 
baseline mle mle predicts population probabilities exactly sample probabilities 
accordingly perfectly predicts training data zero distance test data 
performance mle calculated distance training data test data ability training data predict test data 
baseline frequent verbs coverage test data number occurrences test data probability calculated similarity smoothing techniques undoubtedly going higher mle 
reason verb occurs neighbour stopping condition met added small probability distribution target noun 
limit neighbours considered verbs occur training data appear noun distributions 
limit reached frequent verbs added frequently higher probability tend occur greater number nouns 
accordingly consider baseline fraction occurrence added frequent verbs 
parameters optimised making generalisation add smoothing 
results probability estimation technique stopping condition parameter setting applicable computed mean similarity estimated distributions training data distributions mean similarity estimated distributions test data distributions coverage type token test data estimated distributions 
results technique summarized table 
coverage results mle show smoothing distinct occurrence types test data observed training data 
coverage tokens course lot higher occurrences types occur training data tend occur test data 
maximum likelihood techniques incorrectly conclude unseen data impossible 
frequent verbs na smoothing algorithm enables crease coverage test data 
maximum coverage achieved adding occurrences noun verb seen training data 
note maximum coverage equal allowance verbs previously unseen noun 
ignoring unseen verbs maximum occurrence type coverage unseen data set maxi mum occurrence token coverage 
maximising coverage expense decreasing similarity unseen data 
occurrence chapter 
language modelling optimum similarity similarity coverage parameter training data test data setting mean mean type token stopping condition nearest neighbours stopping condition max change fixed threshold stopping condition max change threshold freq baseline maximum likelihood estimation baseline frequent verbs table comparison stopping conditions baselines language modelling types seen test data added 
maximum similarity unseen data 
words increasing leads smoothed distributions significantly lower similarity training test data 
remains true decreased compensate increase previously unseen events assigned probability 
nearest neighbour techniques significantly outperform baselines terms similarity test data terms coverage 
suggests discriminating selection verbs added target noun distribution 
best results data set achieved third stopping condition 
note differences stopping conditions statistically significant 
variation parameter third stopping condition illustrated 
left hand graph shows variation similarity test training data right hand graph shows variation coverage 
see optimal value 
value corresponds function illustrated 
normally possible fit upper bound advance relies knowing unseen data observed 
difficult know absolute terms hope task 
compute lower bound absolute performance data set best estimate population probabilities 
data set mean similarity training data standard deviation mean similarity test data standard deviation 
lower real lower bound 
reality population distribution close half data set contain information possible occurrences seen data set 
smoothed distributions predict training data better predict test data 
suggests considerable room improvement estimating population probabilities 
differences statistically significant level chance differences occurring chance 
distance coverage average distance estimated probability distributions training data test data distributions chapter 
language modelling training data distance test data distance coverage test data estimated probability distributions type coverage token coverage results stopping condition language modelling task number possible reasons apparently poor results 
considering nouns limiting neighbours selected occurrences added distributions 
recall chapter dist tends select high frequency neighbours lower frequency potential neighbours 
ac considering potential neighbours change nearest neighbours frequent nouns 
having consider nearest neighbours computationally expensive doubt effect nearest neighbours leading improved smoothing results 
second possible reason optimal weight functions smoothing algorithm 
dagan 
explored weight functions nearest neighbours form dist control influence distant neighbours 
chapter 
language modelling experiments dagan 
show increasing value leads improved performance 
context experiments increasing result neighbours considered amount change original distribution fore undoubtedly improve coverage 
order investigate improve similarity unseen data set repeated experiments third stopping conditions value 
stopping condition able obtain optimal similarity test data 
corresponded smaller average change training data average similarity get closer test data 
stopping condition able obtain optimal similarity test data 
corresponded smaller average change training data average similarity gets minimally statistically closer test data 
results suggest value effect smoothing algorithm dagan 

considering neighbours distant original distribution increase weight current distribution iteration effectively reduces impact distant neighbours 
third possible reason wrong distributional similarity measure determine nearest neighbours target noun 
investigate section pseudo disambiguation experiments 
demonstrated plausibility similarity smoothing language modelling 
shown techniques discriminating selection verbs added target noun distribution na add type smoothing 
probabilities estimated allow different previously unseen occurrences distinguished probabilistically 
major approach smoothing allows back katz 
leads slightly counter intuitive result unseen occurrences high probability events deemed probable unseen occurrences low probability events 
compare results nearest neighbour smoothing back smoothing 
goal carry smoothing determine desirable characteristics distributional similarity measure nearest neighbour smoothing 
sufficient confident nearest neighbours approach plausible approach smoothing 
shown determining amount change allowed original unsmoothed distribution function word frequency leads small improvements performance 
main interest comparing similarity distance measures 
discussed approach really appropriate evaluation method ology relies distributional similarity measure 
section turn pseudo disambiguation experiments method evaluating different similarity measures language modelling environment 
efficiency reasons avoid issues choosing appropriate weight function follow lee nearest neighbour voting scheme smoothed probability distributions 
reason suspect chapter 
language modelling effects observed carry smoothing algorithm 
shall show section case reimplementing smoothing experiments best performing similarity measure 
pseudo disambiguation experiments discussed chapter section pseudo disambiguation tasks standard evaluation technique 
current setting may word neighbours decide occurrences occurred portion held test data 
discussed task relevance language modelling word sense disambiguation applications 
various different approaches task set discussed 
having discussed methodology results pseudo disambiguation task number different similarity measures 
experimental set section discuss set pseudo disambiguation experiments 
start ing approach 
compare approach lee discuss number statistical biases observed approach 
approach intended minimise biases 
data set experiments described chapter section 
starting point noun verb direct object pairs extracted bnc rasp parser discarded occurrence pairs noun occur wordnet consider wordnet similarity measure 
selected data pertaining set target nouns 
interested effects word frequency similarity consider high frequency target nouns low frequency target nouns 
high frequency nouns frequently occurring nouns direct object data corresponds frequency range 
low frequency nouns selected frequency ranks data corresponds frequency range 
nouns associated frequencies experiments listed appendix discarding data pertaining target nouns total amount data available occurrence tokens 
target noun available data randomly selected training data set aside test data 
training data compute similarity scores possible pairwise combinations nouns provide mle estimates noun verb occurrence probabilities evaluation task 
test data provides unseen occurrences evaluation purposes 
converted noun verb pair test data noun verb verb triple 
chosen approximately equal training data 
done constructing ranked frequency list verbs associated frequencies 
randomly select verbs frequency plus minus 
verbs frequency range test instance discarded 
method ensures systematic bias higher lower fre quency 
ensure seen test training data 
test chapter 
language modelling instances selected target noun step process whilst triples remained discarding duplicate triples randomly selecting triples remaining step 
point test instances pertaining high frequency nouns test instances pertaining low frequency nouns 
sets split disjoint subsets containing instances target noun 
subsets ways 
perform fold cross validation 
fold cross validation compute optimal parameter settings subsets error rate optimal parameter setting remaining subset 
repeated times different subset held time 
compute average optimal error rate 
compute average optimal parameter setting assume convex relationship pa rameter settings error rate 
order study relationship parameter settings error rate combine sets form development set sets form test set 
development set optimise parameters test set determine error rates optimal settings 
graphs showing relationship error rate parameter settings error rate development set shown 
having constructed test sets task take test instance nearest neighbours noun computed training data decide original occurrence 
neighbours vote equal difference frequencies occurrences casts verb occurs frequently 
votes verb summed nearest neighbours verb votes wins 
performance measured error rate 
error rate incorrect choices ties number test instances tie results neighbours decide tween alternatives 
usual approach optimise number nearest neighbours considered global parameter 
recognise may optimal global value optimal value may vary word word 
example words ith neighbour may similar may different 
unfortunately due shortage data possible optimise directly target noun 
attempt determine appropriate value considering similar neighbours target noun 
experiment different approaches 
take standard approach optimise globally 
second optimise similarity threshold determine potential neighbour dissimilar considered 
looking effect determined method differs lee test set constructed voting scheme employed 
order explain rationale method describe approach taken lee discuss number statistical biases observed approach 
discuss experimental set attempts minimise biases 
minimum number fourteen possibly indistinct occurrences target noun original test data 
chapter 
language modelling lee randomly selects portion occurrence data test data deletes instances occurrences test data occur training data 
replaces remaining noun verb pair test data noun verb verb triple selected approximately equal probability training data 
ensures approach backs unigram probabilities better chance 
test set split disjoint subsets cross validation purposes 
test instance neighbours required decide verb original occurrence construction 
order lee employs voting scheme nearest neighbours noun vote verb occurs training data 
contrast voting scheme neighbour single equal vote 
number nearest neighbours considered parameter optimised performance measured error rate consider voting scheme employed method test set construction 
voting scheme lee approach neighbour single vote give neighbour vote weighted difference frequency occurs verb 
words distinguish cases neighbour occurs verb approximately number times neighbour occurs verb significantly times 
preliminary experiment observed single vote neighbour appears introduce bias 
discuss experiment 
chapter started noun verb direct object pairs ex bnc rasp parser 
constructed training test sets frequent nouns approach lee 
neighbour sets noun constructed training data dist distl task carried single vote neighbour voting scheme described frequency weighted voting scheme 
frequency weighted voting scheme neighbour vote equal difference frequencies occurrences 
results shown 
voting schemes give similar pattern results 
vote neighbour dist marginally outperforms distl frequency weighted voting measures achieve identical optimum error rates identical value error rates frequency weighted voting significantly higher vote neighbour sight suggest better voting scheme 
coming consider error rates 
noted lee measures obtain error rate 
neighbours considered vote independent similarity similarity data effectively thrown away 
accordingly error rate depends solely distributions verbs expect approximately verbs approximately equal unigram probability 
considering just single test instance see may case vote neighbour voting scheme 
achieve majority winning votes smaller frequency difference 
case smaller number neighbours voted vote won larger error rate error rate different voting schemes freq weighted freq weighted vote vote chapter 
language modelling comparison error rates vote neighbour frequency weighted voting schemes frequency difference 
accordingly single test instance conversely may win single vote neighbour voting scheme 
large number test instances experiment vary test instance test instance expect achieve average error rate approximately 
experiments average error rate obtained single vote neighbour scheme similarly lee reports error rate 
fairly large deviation occur chance 
suggests construction test set biases evaluation correct choice 
constructing test set know approximately equal frequency training set occurs exactly occurrence test set training set 
restriction infer causes bias 
know single occurrence distribution dispersed selective distribution random verb 
words small wins win votes single vote neighbour voting scheme 
attempt remove bias changing test set construction 
order method need place restrictions 
difficult wish know probable 
approach uses frequency weighted voting remove bias overcomes considering differences frequencies neighbour occurs verb 
test set construction consider method test set construction detail 
main feature approach lee instances occurrences occur training data deleted test data 
lee justification point similarity estimation deal unseen occurrences 
prefer chapter 
language modelling approach deletes test instances training data pereira clark weir alternative distorts training data argue remaining problems considered 
guarantee inequality holds population probabilities 
ensure inequality holds sample probabilities restricting choice seen training data population probabilities perfectly predicted sample probabilities probability estimation solved problem 
single occurrence compared zero occurrences sufficient evidence conclude probable 
second joint probability small occurrence pair occurred training data unigram probability verbs disambiguated tends decrease unigram probability frequency noun increases 
deleted training occurrences test sets experiments significant negative correlation product moment correlation coefficient noun verb frequency 
frequency limits number nouns verb may occurred low frequency verb tend occurred nouns 
means verbs disambiguated occurred lower number nouns smaller number target noun potential neighbours 
third problem test set construction method biased nouns occur different verbs typically high frequency nouns 
extreme case discarding pairs occur training data may test pairs remaining selective noun 
test set constructed approach frequent nouns nouns appeared final test set 
repeated set random nouns proportion dropped 
accordingly approach evaluating target nouns particular set lead drawing incorrect nouns set 
method test set construction attempts overcome problems 
argue need discard test data basis having seen training data vice versa 
machine learning paradigm unseen data training process 
discarding seen noun verb pairs confident correct choice verb plausible incorrect choice verb 
neighbours disambiguate correctly verbs sure occur target noun different probabilities infer choices verbs truly unseen 
approach correlation noun verb frequency 
select proportion data noun test data means noun treated fairly data lost training purposes whilst allowing control number test instances noun 
accordingly able remove bias selective high frequency nouns 
possible remaining source bias 
unigram probabilities verbs estimated 
preliminary experiments concerned just frequent nouns calculated unigram probabilities verbs training data target approach taken lee pereira clark weir preliminary experi ments chapter 
chapter 
language modelling nouns ensuring approximately equal sample probability 
originally thought better calculate verb probabilities independent target nouns considered 
led biases correct choice verb 
frequency weighted voting scheme error rate vote neighbour voting scheme error rate 
suggest verb known occur frequent nouns occur frequently total verb equal unigram probability known occur frequent verbs 
poses fairly serious problem fundamentally wrong change estimates verb probabilities nouns considering potential neighbours 
change set potential neighbours instance wish consider just low frequency high frequency potential neighbours need change test set difficult comparisons 
change test set comparisons valid verb higher probability occurring set potential neighbours verb 
problem serves highlight pseudo disambiguation experiments 
experiments decided estimate verb probability direct object data available test instances particular noun remain constant potential neighbours considered 
means necessary determine baseline empirically assuming 
results section results pseudo disambiguation task crms described chapter 
compare results existing distributional similarity measures wordnet measure 
distributional similarity measures considered distl section simlin section dist section 
wordnet measure referred proposed lin discussed section 
discussed earlier purposes experiments sets unseen test instances 
set contains test instances high frequency nouns contains test instances low frequency nouns 
discussed section set split disjoint subsets fold cross validation purposes 
subsets development set test set study relationship parameters error rate 
case crms parameters optimised number nearest neighbours similarity threshold 
existing measures parameter optimised noting simlin similarity threshold distl sim distance threshold 
course feasible exhaustive search optimal parameter settings possible lie continuous scale 
optimised parameters simultaneously varied step size parameter order obtain fairly accurate estimate optimal value development set 
step sizes gave fairly smooth results interpolate intermediate results reasonable confidence 
graphs figures show points performance measured 
chapter 
language modelling having determined optimal parameter settings measure calculated perfor mance settings test set 
look results detail 
look results optimising optimising optimising number nearest neighbours considered results summarised table 
table gives average optimal error rates measure calculated fold cross validation 
results divided high frequency nouns low frequency nouns 
cross validated average optimal error rates illustrated 
baseline experiments performance obtained technique backs unigram probabilities verbs disambiguated 
construction test set approximately 
actual empirical figures high frequency noun test set low frequency noun test set 
deviation due unigram probabilities verbs exactly equal calculated training data nouns just training data target nouns 
baseline error rates different observed potential neighbours considered 
case obtain error rate high frequency noun test set low frequency nouns test set 
differences due fact correct choice verb incorrect choice verb occurred possibly times target noun training data noun considered potential neighbour 
measure noun frequency high low cross validated error cross validated error sim add type sim add tok sim add mi sim dw type sim dw tok sim dw mi distl simlin dist table mean optimal error rates fold cross validation optimising 
figures brackets standard deviations optimal error rates folds 
results table observations 
distributional similarity measures perform considerably better measure task high low frequency nouns 
majority measures performance better low frequency nouns high frequency nouns 
error rate comparison cross validated optimal error rates different measures optimising high frequency nouns low frequency nouns add add add dw dw dw lin wn base type tok mi type tok mi line measure chapter 
language modelling illustrating cross validated optimal error rates measure optimised 
best measures appear additive mi model difference weighted mi model 
followed lin mi measure 

token crms perform badly compared crms comparably norm course equivalent difference weighted token crm skew divergence measure 

additive models appear perform best high frequency nouns models appear perform best low frequency nouns 
consider effects different parameters performance 
de velopment set determine optimal parameters consider performance development set parameter varied 
table shows optimised parameter settings develop ment set error rate settings development set error rate settings test set 
shows performance varies existing similarity measures 
crms consider performance varies parameter parame ters held constant optimum values 
shows performance varies shows performance varies shows performance varies 
note interaction different parameters 
particular high value reduces effect optimal value high variation respect 
observations 
variation respect similar models measures expect 
considering neighbours increases performance neighbours allow decisions greater number cases 
increases optimal value greater number decisions wrong direction words similar target word leading decrease performance 
measure noun frequency high low chapter 
language modelling optimal devel 
test optimal devel 
test parameters error error parameters error error sim add type sim add tok sim add mi sim dw type sim dw tok sim dw mi distl simlin dist table summary results pseudo disambiguation task optimising number nearest neighbours 
variation respect differs quite lot models 
general lower value higher value lead better results 

low value indicates combination precision recall closer weighted arithmetic mean better closer unweighted harmonic mean 
differences observed respect fairly small 
suggests optimal weighted combination precision recall known unweighted harmonic mean bad choice 

best performing measures mi models 
perform best fairly high values 
indicates potential neighbour high precision retrieval informative features useful high recall retrieval 
words better sacrifice able decisions test instance small number neighbours favour having neighbours predict incorrect verb occurrences 

little difference results additive difference weighted mi models 
suggests difference extent noun feature fairly irrelevant 
know nouns occur particular verb 
difference weighted model slightly stable respect parameters 
potential neighbour high recall high precision retrieval target noun occurrences greater difference extent retrieved features shared penalised difference weighted model 

additive type crm performs better high values 
variation seen model lot greater mi models 
suggests having high recall uninformative features ones shared nouns indicator similarity 

token crms perform significantly worse models 
suggests error rate error rate performance existing similarity measures respect high frequency nouns sim wn sim sim sim lin chapter 
language modelling performance existing similarity measures respect low frequency nouns sim wn sim sim sim lin performance existing similarity measures respect type retrieval token retrieval important distributional similarity measure 

additive token model difference weighted type model buck trend perform better lower values low frequency nouns 
high recall difference weighted type model fact performs high precision mi models low frequency nouns 
crms better high values recall low frequency target nouns high levels recall high frequency target nouns 
higher recall may better low frequency nouns high precision neighbours low frequency nouns tend infrequently occurring nouns 
optimising similarity threshold set experiments optimised similarity distance threshold number nearest neighbours considered 
threshold consider neighbour greater similarity smaller distance target noun threshold 
similarity error rate error rate comparison crms respect high frequency nouns optimised add type add tok add mi dw type dw tok dw mi chapter 
language modelling comparison crms respect low frequency nouns optimised add type add tok add mi dw type dw tok dw mi performance crms respect optimal values threshold true similarity measures increasing threshold leads neighbours considered 
distance threshold distance measures increasing threshold results neighbours considered 
results summarised table 
table gives cross validated mean optimal error rates corresponding standard deviations measure 
results divided high frequency nouns low frequency nouns 
cross validated average optimal error rates illustrated figures 
results observations error rate error rate chapter 
language modelling comparison crms respect high frequency nouns optimised add type add tok add mi dw type dw tok dw mi comparison crms respect low frequency nouns optimised add type add tok add mi dw type dw tok dw mi performance crms respect optimal values 
relative terms pattern results optimised 

absolute values optimal error rates obtained higher worse optimised 
look effect different parameters performance 
table gives optimal parameter settings development set error rate development set settings error rate test set settings 
shows perfor mance varies existing similarity measures 
case split graphs similarity measures simlin distance measures distl dist 
crms consider performance varies parameter parameters error rate error rate comparison crms respect high frequency nouns optimised add type add tok add mi dw type dw tok dw mi chapter 
language modelling comparison crms respect low frequency nouns optimised add type add tok add mi dw type dw tok dw mi performance crms respect optimal values held constant optimum values 
shows performance varies shows performance varies shows performance varies 
graphs observations 
regarding crms patterns respect similar observed optimised 

striking feature graphs differences measures optimal threshold values 
value reflects average similarity target noun nearest neighbours 
example mi measures pairs words similarity greater 
reflected sharp gradient change observed point 
wordnet measure hand tends find similar measure noun frequency high low cross validated error cross validated error sim add type sim add tok sim add mi sim dw type sim dw tok sim dw mi distl simlin dist chapter 
language modelling table mean optimal error rates fold cross validation optimising 
figures brackets standard deviations 
error rate comparison cross validated optimal error rates different measures optimising high frequency nouns low frequency nouns add add add dw dw dw lin wn base type tok mi type tok mi line measure illustrating cross validated optimal error rates measure optimised pairs words similar 
accordingly optimal similarity threshold measure lot higher mi measures 
measure noun frequency high low chapter 
language modelling optimal devel 
test optimal devel 
test parameters error error parameters error error sim add comb sim add prob sim add mi sim dw comb sim dw prob sim dw mi distl simlin dist table summary results pseudo disambiguation task optimising similarity threshold correlation pseudo disambiguation results smoothing results order investigate higher performance pseudo disambiguation task really pre dicts higher performance smoothing repeated smoothing experiments section best performing measure pseudo disambiguation experiments 
discuss modifications experimental set discuss results 
experimental set took training data described section calculated similarities pairs nouns mi parameter settings 
smoothed distributions smoothing algorithm described section 
mi directly defines similarity function different neighbour weighting function required 
experiments tried weight ni sim add mi ni weight ni sim add mi ni regard stopping condition just implemented stopping conditions gave best results section 
continued dist measure distance original distribution derived distribution measure distance derived distribution unseen empirical distribution fairer comparison effect deriving nearest neighbours different similarity measure 
results high freq nouns sim wn high freq nouns sim lin low freq nouns sim wn low freq nouns sim lin similarity threshold chapter 
language modelling comparison existing similarity measures respect similarity threshold error rate performance existing similarity measures respect experiment able obtain optimal results stopping condition weight function 
settings mean distance test set distributions dropped whilst mean distance training set distribu tions dropped 
increase performance skew divergence measure statistically significantly levels 
terms coverage achieved type coverage token coverage 
considerably higher skew divergence measure achieved type coverage token coverage 
measure choice neighbour weighting function sig error rate error rate comparison crms respect high frequency nouns optimised chapter 
language modelling add type add tok add mi dw type dw tok dw mi similarity threshold comparison crms respect low frequency nouns optimised add type add tok add mi dw type dw tok dw mi similarity threshold performance crms respect optimal values skew divergence measure 
results weight function led performance levels significantly better dist 
conclude pseudo disambiguation task appear valid way predicting performance complex language modelling task 
implement smoothing careful consideration smoothing algorithm weight function required 
area research 
error rate error rate comparison crms respect high frequency nouns optimised chapter 
language modelling add type add tok add mi dw type dw tok dw mi comparison models respect low frequency nouns optimised add type add tok add mi dw type dw tok dw mi performance crms respect optimal values discussion section seen neighbours similar terms occurrences better predicting occurrences neighbours high semantic similarity target noun 
remains true low frequency target nouns fairly little occurrence data 
fact distributional similarity measures tend perform better low frequency nouns high frequency nouns 
possible explanation low frequency word may attested sense data 
contrast high frequency error rate error rate comparison models respect high frequency nouns optimised chapter 
language modelling add type add tok add mi dw type dw tok dw mi comparison crms respect low frequency nouns optimised add type add tok add mi dw type dw tok dw mi performance crms respect optimal values words attested senses 
highlights benefits thesaurus derived sample language modelled 
shown model nearest neighbours high precision re trieval informative features better models considered 
high performance obtained regardless difference extent word feature 
expected find optimising similarity threshold lead improved results chapter 
language modelling optimising number nearest neighbours considered 
borne empirical evidence 
words vary distance word considered neighbour number close neighbours 
chapter considered application distributional similarity measures language modelling 
shown previous dagan 
plausible smooth occurrence probability distributions nearest neighbour techniques 
smoothed distributions obtained substantially closer unseen empirical distributions mle distributions distributions derived add type smoothing 
considering neighbours certain condition met achieve improved results efficiency gains 
effective stopping condition considered amount change original distribution function word frequency 
considered pseudo disambiguation experiments evaluate different dis similarity measures 
saw experimental set pseudo disambiguation task delicate virtually impossible eliminate possible biases task 
developed approach intended minimise biases 
pseudo disambiguation task able establish properties distribu tional similarity measures desirable nearest neighbour smoothing techniques 
attempt answer questions outlined chapter context application 
appears contexts noun occurs frequently expected nouns contexts occurred independently salient contexts deter mining distributional similarity 
words selective verbs tell similarity arguments selective verbs nouns occur selec tive verbs frequently 
second difference extent word occurs context appear major factor 
appears sufficient know word occurred context 
measure exploit asymmetry concept substitutability performs better requires similarity symmetric 
task ing best performing models neighbours high precision retrieval target noun occurrences performed better neighbours high recall retrieval target noun occurrences 
language modelling potential application distributional similarity techniques 
chapter consider properties distributional similarity measures desirable semantic domain 
chapter predicting semantic similarity chapter consider ability distributional similarity predict semantic similarity 
comparing corpus derived distributional similarity judgements human semantic similarity judgements carrying task requires semantic similarity information 
sources human semantic similarity data 
human synonymy judge ments miller charles form standard test set semantic measures henceforth referred data 
second manually constructed noun hierarchy wordnet fellbaum 
application task consider spelling correction described budanitsky hirst 
correlation human synonymy judgements section evaluate distributional similarity measures data human synonymy judgements obtained miller charles 
methodology rubenstein goodenough established human synonymy judgements pairs nouns 
nouns everyday english pairs ranged highly synonymous car automobile semantically unrelated noon string 
subjects asked rate pairs similarity meaning scale absolute synonymy semantically unrelated 
subsequently miller charles repeated experiment consulting human judges original pairs 
selected considered highly synonymous scoring considered intermediate synonymy scoring considered low similarity scoring original experiment 
human synonymy judgements experiment shown standard data set evaluating semantic similarity measures resnik jiang conrath lin budanitsky hirst jarmasz szpakowicz 
similarity measure evaluated human synonymy judgements correlating sets similarity scores 
values pearson product moment correlation coefficient ob chapter 
predicting semantic similarity tained budanitsky hirst jarmasz szpakowicz wordnet semantic similarity measures shown table 
researchers resnik jiang conrath lin reported similar values 
discussed hirst differences occur due minor differences implementation different versions wordnet differences corpora obtain probability information 
example wordnet version research budanitsky hirst jarmasz szpakowicz wordnet version 
similarity measure resnik lin jiang conrath hirst st onge leacock chodorow table correlations wordnet similarity measures data budanitsky hirst jarmasz szpakowicz performed evaluation number distributional similarity measures 
crms proposed chapter lin distributional similarity measure simlin skew divergence measure dist norm distl 
include results lin wordnet measure show consistency implementation earlier researchers 
measure computed similarity distance pair nouns listed table 
calculate pearson product moment correlation coefficient see section set scores similarity measure set human syn judgements 
hypothesis distributional similarity predict semantic similarity expect find high positive correlation distributional similarity scores human synonymy judgements 
measures distance measures expect find high negative correlation distributional similarity scores human synonymy judgements 
results table shows similarity correlation values existing similarity measures 
table shows similarity values correlation values optimal parameter settings crm 
pairs words table ordered degree synonymy data 
column shows human synonymy judgement pair words data 
columns show distributional similarity distance score pair words 
final row shows degree correlation similarity scores column column 
figures illustrate correlation values models different parameter settings illustrate significance correlation values difference weighted token model shown results independent parameter values model 
chapter 
predicting semantic similarity scatter graph wordnet measure similarity values scatter graph additive mi model similarity values scatter graph skew divergence measure dist distance values 
noun pair simlin dist dist car automobile gem jewel journey voyage boy lad coast shore asylum magician wizard midday noon furnace stove food fruit bird cock bird crane tool implement brother monk crane implement lad brother journey car monk oracle cemetery woodland food rooster coast hill forest shore woodland monk slave coast forest lad wizard chord smile glass magician rooster voyage noon string correlation table comparison existing similarity measures data note evaluating synonymy judgements pairs nouns obvious direction particular pairing 
asymmetric measures give different results depending way pair considered 
skew divergence measure considered results noun pair considered target noun noun pair type tok best parameters mi chapter 
predicting semantic similarity type tok mi car automobile gem jewel journey voyage boy lad coast shore asylum magician wizard midday noon furnace stove food fruit bird cock bird crane tool implement brother monk crane implement lad brother journey car monk oracle cemetery woodland food rooster coast hill forest shore woodland monk slave coast forest lad wizard chord smile glass magician rooster voyage noon string correlation table comparison crms data dist dist second noun pair considered target noun dist dist 
course possible consider multiple different combinations 
example noun target noun pair second noun target noun second pair 
chapter 
predicting semantic similarity correlation data additive type crm correlation data additive token crm allow optimisation unfair measures small number pairs considered 
accordingly considered just different cases 
discuss direction better 
regard crms results direction 
noun considered target noun 
measure asymmetric symmetric respect parameter 
words similarity scores correlation obtained parameter setting noun considered target noun identical obtained second noun considered target noun 
results observations 
positive correlation distributional similarity measures human chapter 
predicting semantic similarity correlation data additive mi crm correlation data difference weighted type model synonymy judgements negative correlation distance measures human synonymy judgements 

correlation values tables see best performing measures task mi crms 
noun considered target noun perform best high values 
words get high correlation synonymy judgements second noun high precision retrieval noun cooccurrences equivalent noun having high recall retrieval second noun occurrences 

able vary parameters may give unfair advantage crms small data set smooth variation seen parameters consistently high performance high values mi crm suggests improvement existing similarity measures due parameters giving rise different random permutations possible orderings pairings 
chapter 
predicting semantic similarity correlation data difference weighted mi model discussion sim wn scatter graph sim synonymy judgements wn synonymy score scatter graph wordnet measure question remains asymmetric settings cr framework perform better symmetric settings synonymy apparently captures symmetric concept inter substitutability nouns 
answer may come considering relative frequencies nouns semantic relationship nouns pair study 
frequencies data precision recall values additive mi crm pairs shown table 
consider examples 
pairs semantic relation ship nouns hyponymy food fruit bird cock bird crane crane implement 
pairs hyperonym comes expect impact values precision recall 
example pair food sim add mi chapter 
predicting semantic similarity scatter graph sim synonymy judgements add mi synonymy score scatter graph additive mi model dist scatter graph dist synonymy judgements synonymy score scatter graph skew divergence measure dist fruit fruit hyponym food expect see high precision retrieval food expect occurrences fruit replaced hyperonym food vice versa 
explanation majority pairs ous hyponymy relation 
example pair car automobile difference apparent synonyms automobile car leads automobile having high precision re trieval add mi automobile car occurrences car high recall retrieval add mi automobile car 
answer car frequently au nouns potentially occur exactly contexts car attested contexts 
accordingly car seen high recall retrieval automobile conversely automobile seen high precision retrieval car 
chapter 
predicting semantic similarity noun noun freq freq add mi add mi freq freq car automobile gem jewel journey voyage boy lad coast shore asylum magician wizard midday noon furnace stove food fruit bird cock bird crane tool implement brother monk lad brother crane implement journey car monk oracle cemetery woodland food rooster coast hill forest shore woodland monk slave coast forest lad wizard chord smile glass magician rooster voyage noon string table frequencies precision recall values additive mi model noun pairs data frequency related pattern seen pairs study 
majority pairs freq freq 
exceptions 
noon string precision recall equal zero 
crane implement 
frequencies precision recall values pairing fairly small fairly similar 
term crane pair frequent hyperonym implement 
trend hyponym hyperonym pairs 
accordingly fact implement higher recall retrieval crane crane implement may surprising implement frequent occurrence crane 
chapter 
predicting semantic similarity similar result observed consider results skew divergence measure 
direction skew divergence measure smaller correctly predicted cases relative frequencies nouns 
fact majority noun pairs ordered direction frequent frequent may reason asymmetric measure able determine second noun general having high precision retrieval noun occurrences 
measure relies recall ordering nouns unable distinguish pairs semantically related unrelated pairs bottom list 
order test hypothesis reordered pairs ordered fre quent frequent reran correlation experiment add mi human synonymy judgements 
surprisingly optimal similarity dropped slightly 
obvious may high dis scores pairs words low synonymy judgements 
example noun voyage high recall retrieval occurrences semantically unrelated noun rooster 
little occurrence data rooster fairly commonly oc occurrences 
normally considered highly informative occurrence context rooster voyage low frequency nouns 
section considered small number word pairs 
section perform larger study rankings determined wordnet measure seen section highly correlated human synonymy judgements 
comparison wordnet section evaluate ability distributional similarity measures predict semantic sim ilarity making comparisons wordnet 
underlying assumption approach wordnet gold standard semantic similarity discussed true 
believe distributional similarity measure closely predicts semantic measure wordnet predictor semantic similarity 
seen previous section lin wordnet measure closely correlated human synonymy judgements distributional measures ta ble performs compared wordnet measures see table 
suggests lin wordnet measure basis conducting larger study distributional similarity measures ability predict semantic similarity synonymy judgements allow 
set experiments compare neighbour sets derived distributional sim ilarity measures derived wordnet measure 
technique chapter compared existing similarity measures chapter compared crms existing measures 
second set experiments con sider different types semantic similarity encompassed wordnet similarity measure 
particular examine tendencies different measures select hyponyms hyperonyms 
neighbour set comparisons chapter 
predicting semantic similarity previous chapters adaptation technique implemented lin 
compare top neighbours derived measure top neighbours derived wordnet measure 
earlier experiments see section neighbours computed high frequency target nouns low frequency target nouns combined set nouns 
measure compare neighbour sets correlation cosine coefficient rank scores neighbour sets wk wi neighbour rank score rank rank scores nouns neighbour set 
measure noun frequency high low parameters parameters distl distl sim dist js dist dist sim sim simlin sim add type sim add tok sim add mi sim dw type sim dw tok sim dw mi table mean correlation distributional neighbour sets wordnet derived neighbour sets 
figures brackets standard deviations results terms mean correlation coefficient high frequency target nouns low frequency target nouns 
table shows results existing similarity measures discussed chapter optimal parameter settings crm 
chapter expect mean overlap score chance 
difference mean scores greater statistically impossible chance dif mean correlation chapter 
predicting semantic similarity average correlation wordnet derived neighbour sets distributional similarity measure cos js cp ja lin add add add dw dw dw type tok mi type tok mi measure high frequency nouns low frequency nouns illustrating correlation wordnet measure ferences significant level probability difference occurring chance differences significant level 
correlation values measure illustrated 
results observations 
best performing crms mi ones 
additive mi crm outperforms models existing measures high low frequency nouns 
differences observed statistically significant level measures models difference weighted mi crm 

best performing existing measures simlin dist 
measures perform significantly better high frequency nouns low frequency nouns 

measures simlin sim sim perform considerably worse low frequency nouns 
consider effects parameters crms performance 
variation respect parameters model illustrated figures 
figures observations 
models high low frequency nouns correlation wordnet higher low values high values 
words neighbours wordnet measure tend high recall retrieval target noun cooccurrences 

high value leads high performance high frequency nouns poor performance low frequency nouns 
suggests wordnet derived neighbours high frequency nouns high precision retrieval target noun occurrences wordnet derived neighbours low frequency nouns 
chapter 
predicting semantic similarity variation correlation wordnet respect additive type crm explains existing measures jaccard measure lin mea sure high frequency nouns poorly low frequency 
saw earlier measures similar setting cr framework tend pick neighbours similar frequency target noun high frequency neighbours high frequency nouns low frequency neighbours low frequency nouns 
measures frequency sets task tend select high frequency neighbours high frequency nouns low frequency nouns 
having considered similarity neighbour sets different settings cr framework wordnet derived neighbours examine tendencies different parameter settings select hyponyms hyperonyms 
chapter 
predicting semantic similarity variation correlation wordnet respect additive token crm hyponyms vs hyperonyms section examine relationship direction semantic similarity direction distributional similarity experiments 
implicit direction semantic similarity defined hyponymy relation 
synonyms considered equivalent hyponyms hyperonyms moving away target word degree relatedness different directions 
experiment consider values similarity scores directions asymmetric measures 
second experiment consider neighbour sets terms hyponyms hyperonyms 
experiment experiment consider relationship precision recall val ues dist dist pairs hyponyms hyperonyms 
hypothesis direction hyponymy determined relative values precision recall 
expect hy higher precision recall retrieval target noun occurrences chapter 
predicting semantic similarity variation correlation wordnet respect additive mi crm hyponym hyperonyms higher recall precision retrieval target noun occurrences hyperonym 
similarly skew divergence measure expect hyperonym considered better model target noun noun model hyperonym dist dist hyperonym 
considering pair wise combination target nouns extracted wordnet list hyponyms list hyperonyms target noun 
direct indirect relationships included sense target noun wordnet con sidered 
excluded hyperonyms hyponyms lists conflicts senses 
example sense noun hyperonym sense noun sense noun synonym hyponym sense noun 
words noun considered hyperonym hyponym synonym noun 
chapter 
predicting semantic similarity variation correlation wordnet respect difference weighted type crm process pairs nouns hyponym 
list constructed smaller lists follows hyperonyms high frequency nouns list contains pairs high frequency noun hyperonym 
hyponyms high frequency nouns list contains pairs high frequency noun hyponym 
hyperonyms low frequency nouns list contains pairs low frequency noun hyperonym 
hyponyms low frequency nouns list contains pairs low frequency noun hyponym 
descriptions note high frequency nouns average hyperonyms hyponyms low frequency nouns average hyperonyms hyponyms 
chapter 
predicting semantic similarity variation correlation wordnet respect difference weighted mi crm fact low frequency nouns type relation may correspond low fre quency nouns having smaller number senses 
interesting fact high frequency nouns tend hyponyms hyperonyms low frequency nouns tend hyperonyms hyponyms 
correspond negative correlation frequency depth wordnet noun hierarchy 
scatter graph shows relationship frequency noun data average depth averaged senses word wordnet 
graph see low frequency nouns appear distributed hierarchy high frequency nouns tend appear middle hierarchy 
negative correlation depth frequency 
graph shows best fit regression line value pearson product moment correlation coefficient data 
list crm considered precision recall second noun retrieval noun occurrences computed proportion inequality chapter 
predicting semantic similarity frequency bnc data depth wordnet nouns true 
tie score awarded quantities equal 
hypotheses expect inequality true hy false hyponym 
results summarised table 
include difference weighted token crm model precision ways equals recall 
case skew divergence measure consider inequality dist dist 
model measure list type tok mi type mi dist hyperonyms high frequency nouns hyponyms high frequency nouns hyperonyms low frequency nouns hyponyms low frequency nouns table proportion pairs recall retrieval greater precision results observe similar pattern seen models skew divergence measure 
hypothesis appears correctly predict hyperonyms low frequency nouns hyponyms high frequency nouns 
appears incorrectly predict hyponyms low frequency nouns predicts hyperonyms high frequency nouns success rate better chance 
explain considering relative frequencies word pairs 
hyperonyms high frequency words higher frequency target word req req contrasts hyponyms high frequency words higher frequency target word 
hyperonyms low frequency words chapter 
predicting semantic similarity higher frequency target word contrasts hyponyms low fre quency words higher frequency target word 
accordingly predict hyperonym higher frequency target word hyponym lower frequency target word 
results form baseline predicting di hyponymy 
results crm predictions skew divergence measure fairly close baseline figures 
note baseline hy high frequency nouns hyperonyms low frequency nouns crms slightly better baseline badly hyperonyms high frequency nouns hyponyms low frequency nouns crms slightly badly 
contrast skew divergence measure tends slightly worse baseline 
results concur situation negative correlation frequency depth wordnet 
situation values precision recall simply predicting noun lower frequency term 
general hyponyms lower frequency hyperonym 
picking lower frequency noun hyponym tend correct 
low frequency nouns top hierarchy higher frequency hyponyms 
pairs direction relationship may incorrectly predicted 
type pair may constitute bulk hyponyms low frequency nouns list low frequency nouns lower hierarchy tend hyponyms low frequency nouns higher hierarchy 
accordingly incorrectly predict hyponyms low frequency nouns hyperonyms high frequency nouns 
attempt disentangle factors 
example look pairs words similar frequencies 
fact remains relative frequency fairly indicator direction hyponymy relation 
take weighted average results lists pairs just relative frequency words predict direction hyponymy cases 
rises difference weighted type crm 
having considered relative values precision recall consider tendencies hyperonyms hyponyms sets neighbours derived different measures 
means recall precision retrieval potential neighbour considered relation recall precision potential neighbours 
contrast experiment considered relative precision recall values single potential neighbour 
experiment hypothesis experiment noun hyperonyms greater recall retrieval occurrences hyponyms conversely hyponyms greater precision retrieval occurrences hyperonyms 
accordingly expect find noun hyperonyms neighbour sets generated high recall measures neighbour sets generated high precision measures 
similarly expect find noun hyponyms neighbour sets generated high precision measures neighbour sets generated high recall measures 
experiment lists pairs nouns experiment 
crm computed proportion neighbour sets different parameter chapter 
predicting semantic similarity settings 
consider high precision neighbour sets high recall neighbour sets high precision recall neighbour sets 
previous experiments considered neighbour sets size 
results crm shown table 
comparison results existing measures distributional similarity wordnet measure shown 
list pairs considered measure req high req low model parameters isa isa isa isa additive type additive token additive mi difference weighted type difference weighted token difference weighted mi simlin dist dist table proportion neighbour sets size 
notation isa denote hyperonyms 
wordnet measure obviously finding hyperonyms hyponyms high low frequency nouns 
fact get suggests hyponym chains wordnet long members chain nearest neighbours target noun 
wordnet measure presents upper bound desirable distributional measures find 
fact finds greater proportion hyperonyms hyponyms noun hyperonyms hyponyms 
existing distributional measures measure performs best board dist 
finds slightly hyponyms simlin significantly hyperonyms high low frequency nouns 
fact finds considerably hyperonyms hyponyms may related tendency find higher frequency nouns neighbours 
chapter 
predicting semantic similarity explain simlin considerably better finding hyperonyms high frequency nouns low frequency nouns 
previous experiment see similar pattern results crms respect 
models best finding hyperonyms low values neighbours high recall retrieval target noun occurrences hyperonyms 
true high frequency nouns low frequency nouns 
performance similar high frequency nouns low frequency nouns 
regards hyponyms results surprising 
greater proportion hyponyms high recall neighbour sets high precision neighbour sets 
greatest proportions hyponyms high low frequency nouns high recall high precision neighbours setting 
suggests high precision neighbours tend hyperonyms hyponyms 
stronger tendency high precision neighbours hyperonyms 
high precision measures find lot fewer hyperonyms hyponyms 
support idea high precision neighbours tend general nouns high recall neighbours 
fact hyponyms suggests siblings wordnet hierarchy apple pear 
contrasts high recall measures include fruit neighbour apple 
performance generally fairly poor 
best performing measures mi models find approximately noun hyperonyms noun hyponyms 
proportions considerably wordnet measure respectively 
explanation results skewed hyperonyms hyponyms obscure senses wordnet corpus 
discussion seen hyperonym necessarily higher recall retrieval hyponyms occurrences precision retrieval occurrences correspondence due correlation noun frequency depth wordnet 
seen compare precision recall scores different neighbours target noun high recall neighbours include hyperonyms hyponyms high precision neighbours include hyponyms hyperonyms 
result constant high low frequency nouns 
far chapter looked ability distributional similarity measures predict human synonymy judgements generate sets neighbours akin derived wordnet predict hyponymy relationships pairs semantically related words 
section consider performance application semantic task 
spelling correction task section examine ability distributional similarity measures predict semantic similarity spelling correction task 
adopt variant task employed budanitsky hirst evaluate wordnet measures 
task exploits property lexical cohesion texts tend contain semantically related words 
chapter 
predicting semantic similarity discuss motivation task methodology 
results number distributional similarity measures 
motivation application task allows evaluate semantic similarity independent gold standard 
task relies properties real text intuitions human judges 
property exploited texts tend cohesive contain semantically related words morris hirst 
words occurrence word occurrence semantically similar words locality church 
property exploited real word spelling error corrector described budanitsky hirst 
real word spelling error malapropism phonologically similar intended word produced writer word dairy diary confused words principle principal 
experiments spelling variations word defined words dictionary derived single insertion deletion transposition 
real word spelling errors detected na spelling correctors incorrectly word exists dictionary 
budanitsky hirst propose spelling correction method assumptions 
real word spelling error semantically related text 

frequently writer intended word semantically related nearby words 

intended word semantically unrelated nearby spelling variation related 
budanitsky hirst discuss assumption untrue human generated spelling errors 
true spelling errors created malapropism generator true experiments 
discuss methodology detail 
methodology describe methodology budanitsky hirst evaluate word net measures 
discuss modifications 
budanitsky hirst took articles wall street journal removing proper nouns words replaced word spelling variation choosing wordnet noun spelling variation 
gave corpus words malapropisms 
attempt correct spelling errors step process 
step word suspected spelling error semantically related word certain scope 
order boolean valued semantically related decision consider similarity words certain threshold set advance value separate highly related pairs words unrelated pairs determined study rubenstein goodenough 
second step suspected word detected spelling error spelling variation semantically related word scope 
budanitsky hirst vary amount search scope considered single paragraph paragraph chapter 
predicting semantic similarity side paragraphs side document 
measure scope measure precision recall errors suspicion detection steps 
constructed variant task follows bnc data set 
second replacing nth word meets certain conditions advance generated errors fly 
meant theoretically generate higher proportion spelling errors budanitsky hirst 
errors permanently affect text sure spelling errors search scope limiting number malapropisms generated 
practice generated proportion errors budanitsky hirst giving random generator prior probability generating error earlier experiments 
reason prior probability spelling error affects baseline considerably 
example imagine spelling error detector simply guesses word correctly 
precision detector equal prior probability spelling error 
recall detector increase probability guessing word spelling error increases 
recall maximum spelling detector guesses word spelling error 
maximum score case spelling error spelling error accordingly baseline score increases probability spelling error increases 
probability baseline score 
increase probability baseline score increase 
third modification consider scope terms window size side target word 
largely discard paragraph boundaries pre processing bnc 
expect negative effect performance paragraph natural unit text windows 
fourth set threshold semantic relatedness advance 
varied number nouns considered semantically related similarity threshold 
reason distributional similarity measures unable split semantically related semantically unrelated pairs value 
experimental conditions experiment consistent pseudo disambiguation experiment 
intended perform variation pseudo disambiguation experiments similarity threshold varied 
results section suggest little gained 
note experiments word may considered neighbour 
contrasts pseudo disambiguation experiments 
experiments consider word neighbour knew advance verb target word went frequently 
experiments constructed test set way prior knowledge words close physical proximity target word 
presence target word spelling variant search scope indicator intended word reason exclude semantically distributionally related word 
results show presence word indicator spelling variation intended left semantically distributionally similar words additional effect 
chapter 
predicting semantic similarity previous experiments consider similarities target nouns 
accordingly consider presence nouns set target nouns possible spelling variations set target nouns 
optimising parameters divided test instances development set test set 
development set constitutes test instances find parameter settings optimise score stage detection process 
test set constitutes remaining test instances measure precision recall score detection optimised parameter settings 
results section results spelling correction task 
hypothesis dis similarity measures predictors semantic similarity perform task semantic similarity measure 
consider parameter settings optimise harmonic mean precision recall detection process development set measure semantic relatedness 
graphs detection scope shown 
results observe optimal performance high frequency nouns low frequency nouns achieved neighbour considered target word 
performance drops rapidly semantically related words considered 
single neighbour considered detection drops high frequency nouns low frequency nouns 
nearest neighbours considered detection high frequency nouns low frequency nouns 
figures directly comparable quoted budanitsky hirst due differences methodology 
interesting note budanitsky hirst achieve optimal detection lin measure 
noun wordnet considered potential neighbour experiments expect better 
note fixed similarity threshold means generally consider case neighbour word 
shows happening terms precision recall varies scope 
reason drop value score considering semantically related words word dramatically reduces precision 
words presence semantically related word appear indicator word correctly suspicion stage spelling variation intended word detection stage 
accordingly knowledge semantically related words help task 
low scopes scope see word repeated see benefit considering word neighbours 
considering word semantic neighbours narrow scopes advantageous look repetition word wider search scope 
results semantic measure expected considered formance distributional measures 
shows score varies scope simlin 
unsurprisingly pattern results simlin 
pattern results chapter 
predicting semantic similarity variation spelling correction task respect parameters scope repeated additive mi crm previously best performing crm experiments 
model performance drop fast number neighbours considered increased neighbours fairly high precision retrieval target nouns occurrences 
shows performance varies scope 
shows performance varies scope 
optimal performance additive mi crm scope high fre quency target nouns 
slightly better simlin 
think crm better similarity measure simlin spelling correction task 
note reason high precision neighbours reducing performance high precision neighbours tend low frequency words ones occur 
consequently considering precise neighbour change result chapter 
predicting semantic similarity precision recall score high frequency nouns sim wn scope precision recall precision recall score low frequency nouns sim scope wn precision recall variation precision recall number nearest neighbours considered varied scope words considering target word 
convinced searching highly precise neighbours useful spelling correction task need see increase performance 
discussion number possible reasons results worse task 
hypothesis texts contain semantically related words wrong fit intuitions text cohesiveness 
second implemented scope terms window words paragraphs means cross natural text boundaries search semantically related words 
problem get worse performance search word 
third considering nouns potential neighbours 
earlier results tasks chapter 
predicting semantic similarity variation spelling correction task respect parameters scope simlin expect adversely affect performance extent 
fourth type semantic relatedness considered wordnet second order affinities 
semantically related words occur naturally cohesive text lexically substitutable 
nature task related words occurring text order affinities target word 
wordnet distributional similarity measures carry task attempting predict order affinities second order affinities 
results show words second order affinities related taxonomical relation strong order affinities 
results task difficult draw ability distributional similarity measures predict semantic similarity 
seen semantic measure increase performance simply searching occurrences target word spelling variations search scope 
accordingly mean different measure increases decreases drop performance 
may think smaller chapter 
predicting semantic similarity variation score spelling correction task respect parameters scope additive mi crm 
drop performance means right track finding words help correct spelling errors 
words related wordnet type linguistic relations matter 
order better task need consider larger number words possibly parts speech larger number semantic relations 
chapter examined ability distributional similarity measures predict se mantic similarity 
seen positive correlation distributional similarity values human synonymy judgements high semantic wordnet measure human synonymy judgements 
evaluated ability distributional similarity measures predict semantic similar ity defined measure wordnet noun hyponym hierarchy 
task chapter 
predicting semantic similarity variation score spelling correction task respect parameters additive mi crm scope 
establish properties distributional similarity measures desirable se mantic applications 
seen best performing distributional measures mi crms 
indicates selective verbs salient context predicting semantic similarity 
second difference extent word occurs context appear major factor 
language modelling application appears sufficient know word occurred context 
saw crms high low frequency target nouns correlation wordnet derived neighbours higher low values high values 
words neighbours wordnet measure tend high recall retrieval target noun occurrences 
high value leads high performance high frequency target nouns poor performance low frequency target nouns 
suggests wordnet derived neighbours high frequency target nouns high precision retrieval target noun occurrences wordnet derived neighbours low frequency target nouns 
explains chapter 
predicting semantic similarity measures closely simulated high value cr framework high frequency nouns low frequency nouns 
measures task tend select high frequency neighbours high frequency target nouns low frequency target nouns 
seen appears relationship precision recall cr framework direction semantic relationship pairs words 
particular high recall neighbours tend include hyperonyms hyponyms high precision neigh tend include hyponyms hyperonyms 
reason relationship precision recall hyponymy hyperonyms general tend occur greater number distinct contexts hyponyms 
strong cor relation number distinct contexts word appears total frequency 
frequent word seen possible contexts appear high recall related word hyponymy relation word 
seen pair nouns frequencies ap equal precision recall values able pick hyponym independent relative frequencies 
area study 
attempted semantic application task test measures ability predict semantic similarity independent human judgements 
related words occur ones order affinities target noun nature fact occur document 
seen set words considered semantic similarity measure unable produce increase performance obtained just considering word 
suggests second order affinities useful predicting order affinities 
consequently difficult draw task distributional similarity measures considered 
chapter directions thesis study measures lexical distributional similarity applications 
chapter summarise major major contributions finish main directions 
summary section summary chapter thesis highlight areas 
chapter chapter outlined major hypotheses 
similarity inherently asymmetric concept 
distributional similarity measure exploit useful applications symmetric measure 
second measurement sim ilarity sufficient know words occur particular context 
occurring distinct contexts important occurring indistinct contexts 
third measures give weight informative contexts perform better measures give weight frequently occurring contexts 
chapter started chapter considering means words distributionally similar 
fairly loose definition lexical distributional similarity words distributionally similar appear similar contexts 
corresponds grefenstette termed second order affinity words 
defining context terms grammatical rela tions words distributional similarity thought terms lexical substitutability church 
viewed way distributional similarity inherently asymmetric concept 
discussed fifteen potential applications knowing distributionally similar lexically substitutable words 
applications take advantage discussed link tween lexical distributional similarity semantic relatedness words mean similar things chapter 
directions similar ways 
applications probability estimation tech niques property distributional similarity directly words tend occur similar contexts predict contexts occurs contexts occurs 
final section chapter looked possible approaches evaluation area 
argued humans unable judge lexical distributional similarity independent semantic similarity case single right answer words dis similar 
best measure distributional similarity returns useful neighbours context particular application leads best performance application 
chapter chapter introduced fifteen existing measures previously proposed adopted measuring lexical distributional similarity 
compared neighbours measures produce nouns direct object data extracted bnc examined tendencies select high low frequency words neighbours 
saw jaccard coefficient dice coefficient produce identical neighbour sets cosine measure pearson product moment correlation coefficient large va neighbour sets produced different measures 
measures classified terms tendencies select words particular frequencies distribution width neighbours 
majority measures considered tend select words higher frequency target noun neighbours 
measures simlin sim tend select words similar frequency target noun neighbours measures tends select words lower frequency target noun neighbours 
discussed knowledge tendencies measures select words particu lar frequencies impact different applications 
obviously computational benefits 
example measure selects neighbours set frequently occurring words performance task improved considering words potential neighbours 
second knowledge frequency characteristics measure influence choice measure particular application 
example distribu tional similarity measure tends select higher frequency words neighbours appropriate text simplification task 
chapter chapter defined framework measuring lexical distributional similarity concept substitutability 
cast problem occurrence retrieval cr measure precision recall analogy way measured document retrieval 
similarity framework defined substitutability inherently asymmetric 
cr framework require similarity viewed asymmetric settings model symmetric model difference weighted token model symmetric 
symmetric chapter 
directions settings may thought capturing notion inter substitutability 
idea asymmetry substitutability cr comparable measures rel ative entropy confusion probability 
consider precision recall 
framework advan directly defines similarity measure require smoothing base language model allows explore systematically questions relative impor tance different features 
order defined different occurrence retrieval models crms 
models additive models difference weighted models 
allows explore effect difference feature values shared features 
additive difference weighted models defined type token mi models 
allows explore effect weighting different features degree relevance association 
remainder chapter devoted analysing neighbour sets produced cr framework determining cr characteristics existing distributional similarity measures 
saw general high recall neighbours tend high frequency nouns high precision neighbours tend low frequency nouns high harmonic mean tends produce neighbours similar frequency original noun 
crm effect frequency neighbours produced 
particular difference weighted models tend flatter respect parameters produce neighbours similar frequency target noun 
note setting additive mi model appears produce neighbour sets apparent biases high low frequency nouns 
regard existing distributional similarity measures saw considered directly derived closely approximated cr framework 
summary findings table 
knowledge cr characteristics existing measures helps understand measures 
example high recall measures sim sim tend select higher frequency nouns 
measures require recall precision sim select lower frequency neighbours high recall measures 
measures require high recall high precision sim simlin sim select neighbours similar frequency target noun 
cr framework simulates existing measures distributional similar ity 
defines space distributional similarity measures populated named measures 
exploring space aimed discover desirable characteristics dis similarity measures 
may turned useful measure space discovered 
fact discovered new optimal combinations characteristics context particular applications 
goal understand different char relate high performance different applications explain measure performs better 
chapter chapter considered application distributional similarity measures language mod 
showed previous dagan 
plausible smooth occurrence probability distributions nearest neighbour techniques 
smoothed distri chapter 
directions butions obtained substantially closer unseen empirical distributions mle dis tributions distributions derived add type smoothing 
considering neighbours certain condition met achieve improved results efficiency gains 
effective stopping condition considered amount change original distribution function word frequency 
considered pseudo disambiguation experiments evaluate different dis similarity measures 
saw experimental set pseudo disambiguation task delicate virtually impossible eliminate possible biases task 
developed approach intended minimise biases 
pseudo disambiguation task establish properties distributional similarity measures desirable nearest neighbour smoothing techniques 
appears contexts noun occurs frequently expected nouns contexts occurred independently salient contexts determining distributional similarity 
words selective verbs tell similarity arguments selective verbs nouns occur selective verbs frequently 
second difference extent word occurs context appear major factor 
appears sufficient know word occurred context 
measure exploit asymmetry concept substitutability performs better requires similarity symmetric 
task best performing models neighbours high precision retrieval target noun occurrences performed better neighbours high recall retrieval target noun occurrences 
result supports hypothesis chapter 
language modelling application task overcome sparse data fill missing information word high precision neighbours preferred 
mistakes 
contexts high precision neighbour word apparently incorrectly retrieves ones expect observe word data seen 
saw performance pseudo disambiguation task lower semantic measure wordnet noun hierarchy distributional measures 
supports hypothesis chapter semantically related words necessarily best words distributional task 
task distributional high precision distributionally similar words 
chapter saw best performing crm parameter settings determined pseudo disambiguation task additive mi crm able achieve better coverage higher similarity held test data skew diver gence measure original smoothing task 
conclude pseudo disambiguation appear valid way predicting performance complex language modelling task 
saw choice weight function smoothing algorithm significant effect performance crm 
area research 
chapter chapter examined ability distributional similarity measures predict semantic similarity 
saw positive correlation distributional similarity values chapter 
directions human synonymy judgements great semantic wordnet measure human synonymy judgements 
evaluated ability distributional similarity measures predict semantic similar ity defined wordnet measure 
task establish properties distributional similarity measures desirable semantic applications 
saw language modelling task best performing distributional measures mi crms 
indicates selective verbs salient con text predicting semantic similarity 
second difference extent word occurs context appear major factor 
language modelling application appears sufficient know word occurred context 
saw crms high low frequency target nouns cor relation wordnet derived neighbours higher low values high values 
words neighbours wordnet measure tend high recall retrieval target noun occurrences 
high value leads high performance high frequency target nouns poor performance low frequency target nouns 
suggests wordnet derived neighbours high frequency target nouns high precision retrieval target noun occurrences wordnet derived neighbours low frequency tar get nouns 
explains measures closely simulated high value cr framework high frequency nouns low frequency nouns 
best measures task tend select high frequency neighbours high frequency target nouns low frequency target nouns 
saw appears relationship precision recall cr frame direction semantic relationship pairs words 
particular high recall neighbours tend include hyperonyms hyponyms high precision neigh tend include hyponyms hyperonyms 
reason relationship precision recall hyponymy hyperonyms general tend occur greater number distinct contexts hyponyms 
strong cor relation number distinct contexts word appears total frequency 
frequent word seen possible contexts appear high recall related word hyponymy relation word 
seen pair nouns frequencies ap equal precision recall values able pick hyponym independent relative frequencies 
area study 
attempted semantic application task test measures ability predict semantic similarity independent human judgements 
related words occur ones order affinities target noun nature fact occur document 
seen set words considered semantic similarity measure unable produce increase performance obtained just considering word 
suggests second order affinities useful predicting order affinities 
consequently difficult draw task distributional similarity measures considered 
major contributions chapter 
directions main contribution development framework concept lexical 
cast problem measuring distributional similarity occurrence retrieval cr measure precision recall analogy way measured document retrieval 
cr framework allowed systematically explore various characteristics distri similarity measures 
explored merits symmetry asymmetry similarity measure varying relative importance attached precision recall 
seen distri bution word moves away identical word similar ways occurred generally distinct contexts word occurred generally distinct contexts word case call word high recall neighbour word second case call word high precision neighbour word experimental chapters shown kind neighbour preferred appears depend application hand 
high precision neighbours useful language modelling task pseudo disambiguation high recall neighbours highly correlated wordnet derived neighbour sets 
accordingly appears important consider task distributional semantic 
chapter survey potential applications classified consider primarily distributional semantic 
believe distributional similarity measures areas support hypotheses 
second explored way frequency information utilised different occurrence retrieval models crms 
type token mi crms investigated relative importance different occurrence types 
high performance mi models serves confirm hypothesis highly selective verbs important calculation distributional similarity 
additive difference weighted versions model investigated difference extent noun feature important 
shown considering difference major factor lead significantly improved results best performing mi crms 
studied tendencies measures select high low frequency nouns neighbours 
seen high recall measures tend select high er frequency nouns neighbours high precision measures tend select low er frequency nouns neighbours re high recall high precision leads neighbours similar frequency target noun 
knowledge tendencies important particular applications 
observe single high precision neighbour single high recall neighbour fore general need consider larger neighbourhood size high precision neighbours 
tendencies predictions happen consider greater number words potential neighbours 
high recall neighbours tend frequent nouns high precision neighbours tend frequent nouns 
result approximate neighbour sets computed efficiently considering possible nouns potential neighbours 
chapter 
directions important contribution occurrence retrieval better understand ing existing distributional similarity measures 
comparing existing measures cr framework analyse cr characteristics 
shown fifteen sim ilarity measures considered directly derived exactly approximated crms 
example knowing lin measure equivalent harmonic mean precision recall additive mi model explains measure badly wordnet prediction task low frequency nouns 
seen recall important preci sion wordnet prediction task nearest neighbours target noun lin measure high precision high recall 
high precision neighbours tend lower frequency words additional requirement high precision considering low frequency nouns leads drop performance 
thesis extensive study existing new measures distribu tional similarity applications 
evaluation measures performed set nouns shown performance distributional similarity techniques low frequency nouns significantly lower high frequency nouns 
suggests distributional techniques relatively little data available 
distributional domain means probability estimation techniques rare words greater confidence 
semantic domain able distributional tech niques extend existing semantic resources cover rare new words automatically generate domain genre dialect specific resources 
major directions number major directions extended 
set crms defined exhaustive 
number association functions define crm 
example chapter dis cussed potential weighted mutual information wmi 
research shown wmi outperforms mi low frequency words 
seen high perfor mance mi crms low frequency words performance wmi higher 
second evaluate crms application area discussed chapter 
see potential improvement tasks classified distributional 
particular try prepositional phrase attachment ambiguity resolution task conjunction scope identification task 
tasks previously tackled semantic classes predict ultimately distributional information 
accordingly believe possible better cr framework 
particular hypothesise high precision setting additive mi crm lead results 
semantic domain potential greatest able distinguish hyponyms hyperonyms 
useful determining compositionality collocations 
noted baldwin 
compositional collocations phrase hyponym head phrase 
accordingly predict compositional collocations phrase high precision retrieval occurrences head phrase 
perform better existing techniques mccarthy consider distributional similarity phrase head 
chapter 
directions understanding frequency selecting characteristics distributional similarity mea sures may allow greater number ways 
example previously technique compared distributions word senses word forms biased frequently occurring sense 
arithmetic mean precision recall additive mi crm appears free frequency biases observed measures settings appropriate example 
abney schapire singer 

boosting applied tagging pp attachment 
proceedings joint sigdat conference empirical methods natural language processing large corpora emnlp vlc 
austen 

emma 
penguin 
baayen 

word frequency distributions 
dordrecht kluwer academic publishers 
baldwin tanaka 

empirical model multiword expression decomposability 
proceedings acl workshop multiword expressions pp 

sapporo japan 
baldwin bond 

learning countability english nouns corpus data 
proceedings st international conference association computational linguistics acl pp 

sapporo japan 
baldwin bond 

plethora methods learning english countability 
proceedings conference empirical methods natural language processing emnlp pp 

sapporo japan 
baldwin lascarides 

statistical approach semantics constructions 
proceedings acl workshop multiword expressions pp 

sapporo japan 
savitch 

occurrence model word categorization 
proceedings rd meeting mathematics language 
bernard 
ed 

macquarie encyclopedic thesaurus 
sydney australia macquarie library 
berry 

computation collocations relevance lexical studies 
aitken bailey hamilton smith eds computer literary studies 
edinburgh new york university press 
johnson 

unsupervised learning multi word verbs 
proceedings th international conference association computational linguistics acl 
toulouse france 
bond 

determiners number english contrasted japanese exemplified machine translation 
unpublished doctoral dissertation university brisbane queensland australia 
brill 

pattern disambiguation natural language processing 
proceedings joint sigdat conference empirical methods natural language processing large corpora emnlp vlc 
briscoe carroll 

developing evaluating probabilistic lr parser partof speech punctuation labels 
th acl sigdat international workshop parsing technologies pp 

briscoe carroll 

robust accurate statistical annotation general text 
proceedings rd international conference language resources evaluation lrec pp 

las palmas canary islands 
brown dellapietra desouza lai mercer 

class gram models natural language 
computational linguistics 
budanitsky 

lexical semantic relatedness application natural language processing 
unpublished doctoral dissertation department computer science university toronto 
budanitsky hirst 

semantic distance wordnet experimental evaluation measures 
proceedings naacl workshop word net lexical resources 
pittsburgh 
caraballo 

automatic construction hypernym labelled noun hierarchy text 
proceedings th annual meeting association computational linguistics acl 
carroll 

rasp parser accuracy 
personal communication 
updated version table appears carroll carroll briscoe 

development effort probabilistic lr parsing system evaluation 
acl sigdat conference empirical methods natural language processing 
carroll minnen canning devlin tait 

practical simplification english text assist aphasic readers 
proceedings aaai workshop integrating artificial intelligence assistive technology pp 

madison wisconsin 
charniak 

maximum entropy inspired parser 
proceedings st conference north american chapter association computational linguistics naacl pp 

seattle wa 
chen goodman 

empirical study smoothing techniques language modelling tech 
rep 
tr 
cambridge massachusetts harvard university 
chomsky 

syntactic structures 

hague netherlands mouton 
church 

empirical estimates adaptation chance closer proceedings th internation conference computational linguistics coling pp 

church gale hanks hindle moon 

lexical substitutability 
atkins eds computational approaches lexicon pp 

oxford university press 
church hanks 

word association norms mutual information lexicography 
proceedings th annual conference association computational linguistics acl pp 

church hanks 

word association norms mutual information lexicography 
computational linguistics 
clark 

inducing syntactic categories context distribution clustering 
proceedings th conference natural language learning conll pp 

lisbon 
clark 

contrast pragmatic principles lexical consequences 
lehrer editors frames fields contrasts new essays semantic lexical organization 
lawrence erlbaum associates 
clark weir 

class probabilistic approach structural disambiguation 
proceedings th international conference computational linguistics coling pp 

saarbrucken germany 
clark weir 

class probability estimation semantic hierarchy 
computational linguistics 
collins 

head driven statistical models natural language processing 
unpublished doctoral dissertation computer information science university pennsylvania 
collins brooks 

prepositional phrase attachment backed model 
proceedings third workshop large corpora pp 

cambridge massachusetts 
copestake 

approach building hierarchical element lexical knowledge base machine readable dictionary 
proceedings st international workshop inheritance natural language processing pp 

tilburg 
cover thomas 

elements information theory 
new york wiley 
cruse 

lexical semantics 
cambridge university press 
curran 

ensemble methods automatic thesaurus extraction 
proceedings conference empirical methods natural language processing emnlp 
philadelphia 
curran moens 

improvements automatic thesaurus extraction 
acl siglex workshop unsupervised lexical acquisition 
philadelphia 
curran moens 

scaling context space 
proceedings th annual meeting association computational linguistics acl 
philadelphia 
cutting karger pedersen tukey 

scatter gather cluster approach browsing large document collections 
th annual international pp 

denmark 
dagan lee pereira 

similarity models word cooccurrence probabilities 
machine learning journal 
dagan marcus markovitch 

contextual word similarity estimation sparse data 
st annual meeting association computational linguistics acl pp 

deerwester dumais furnas landauer harshman 

indexing latent semantic analysis 
journal american society information science 
devlin 

simplifying natural language text readers 
unpublished doctoral dissertation university sunderland uk 
dhillon kumar 

enhanced word clustering hierarchical text classification tech 
rep nos 
tr 
austin department computer sciences university texas 
jun 

learning new compositions ones 
proceedings international conference computational natural language learning conll pp 

somerset new jersey 
dorr jones 

acquisition semantic lexicons word sense disambiguation improve precision 
proccedings siglex workshop breadth depth semantic lexicons th annual meeting association computational linguistics pp 

santa cruz ca 
essen steinbiss 

cooccurrence smoothing stochastic language modelling 
proceedings international conference acoustics speech signal processing icassp 
zohar roth 

classification approach word prediction 
proceedings st conference north american chapter association computational linguistics naacl pp 

fellbaum 
ed 

wordnet electronic lexical database 
mit press 
finch chater 

bootstrapping syntactic categories 
proceedings th annual conference cognitive science society america pp 

firth 

synopsis linguistic theory 
studies linguistic analysis pp 

oxford society 
reprinted palmer 
ed 
selected papers firth longman harlow forgy 

cluster analysis multivariate data efficiency versus interpretability classifications 
biometric society meetings 
riverside ca 
biometrics malcolm moler 

computer methods mathematical computations 
englewood cliffs new jersey prentice hall 
frakes baeza yates 
eds 

information retrieval data structures algorithms 
prentice hall 
friedman singer 

efficient bayesian parameter estimation large discrete domains 
advances neural information processing systems 
fung mckeown 

technical word term translation aid noisy parallel corpora language groups 
machine translation 
gale church yarowsky 

statistical methods word sense disambiguation 
working notes aaai symposium probabilistic approaches natural language pp 

menlo park ca aaai press 
gale sampson 

turing estimation tears 
cogs research 
gibbons 

nonparametric measures association 
quantitative applications social sciences 
park ca sage 
golding 

bayesian hybrid method context sensitive spelling correction 
proceedings rd workshop large corpora pp 

boston ma 
golding roth 

winnow approach context sensitive spelling correction 
machine learning 
goodman 

likeness meaning 
editor semantics philosophy language 
urbana illinois university illinois press 
grefenstette 

corpus derived second third order word affinities 
proceedings 
amsterdam 
grefenstette 

explorations automatic thesaurus discovery 
boston usa kluwer academic publishers 
grice 

meaning 
philosophical review 
reprinted strawson philosophical logic oxford university press grishman sterling 

smoothing automatically generated selectional constraints 
human language technology proceedings arpa workshop pp 

san francisco morgan kaufmann 
grishman sterling 

generalizing automatically generated selectional patterns 
proceedings th international conference computational linguistics coling pp 

kyoto japan 
halliday 

lexis linguistic level 
halliday robins eds memory firth 
london longman 
harris 

mathematical structures language 
new york wiley 


understanding morphology 
arnold publishers 
hatzivassiloglou mckeown 

automatic identification adjective scales clustering adjectives meaning 
proceedings st annual meeting association computational linguistics acl pp 

hearst 

automatic acquisition hyponyms large text corpora 
proceedings th international conference computational linguistics coling pp 

nantes france 
hindle 

user manual fidditch 
naval research laboratory technical memorandum hindle 

noun classification predicate argument structures 
proceedings th annual meeting association computational linguistics acl 
hindle rooth 

structural ambiguity lexical relations 
proceedings th annual meeting association computational linguistics acl pp 

berkeley ca 
hindle rooth 

structural ambiguity lexical relations 
computational linguistics 
hirschman grishman sager 

grammatically automatic word class formation 
information processing management 
hirst st onge 

lexical chains representations context detection correction malapropisms 
fellbaum pp 

mit press 
hofmann puzicha 

mixture models cooccurrence data 
proceedings international conference pattern recognition 
longer version available mit memo jarmasz szpakowicz 

roget thesaurus semantic similarity tech 
rep 
tr 
university ottawa 
jiang conrath 

semantic similarity corpus statistics lexical taxonomy 
proceedings international conference research computational linguistics 
taiwan 
karov edelman 

similarity word sense disambiguation 
computational linguistics 
katz 

estimation probabilities sparse data language model component speech recogniser 
ieee transactions acoustics speech signal processing assp 
kaufman rousseeuw 

finding groups data cluster analysis 
john wiley sons 
kay 

text translation alignment 
computational linguistics 
keil 

semantic conceptual development ontological perspective 
cambridge massachusetts harvard university press 
keil 

emergence semantic conceptual distinctions 
experimental psychology 
kilgarriff 

polysemy 
unpublished doctoral dissertation school cognitive computing sciences cogs university sussex 
kilgarriff 

words particularly characteristic text 
survey statistical approaches 
language engineering document analysis recognition pp 

kilgarriff 

wasp bench mt lexicographers workstation supporting state art lexical disambiguation 
proceedings machine translation mt summit vii 
santiago de 
kilgarriff 

word sketch extraction display significant collocations lexicography 
acl workshop collocation computation extraction analysis exploitation 
toulouse 
kilgarriff 

thesaurus 
second conference language resources evaluation lrec pp 

athens 
korhonen marx 

clustering polysemic subcategorization frame distributions semantically 
proceedings st annual meeting association computational linguistics acl pp 

sapporo japan 
kullback leibler 

information sufficiency 
ann 
math 
stat 
kurohashi nagao 

dynamic progamming method analyzing conjunctive structures japanese 
proceedings international conference computational linguistics coling 
nantes france 
lakoff johnson 

metaphors live 
chicago illinois chicago university press 
lapata brew 

subcategorization resolve verb class ambiguity 
empirical methods natural language processing 
lapata keller mcdonald 

evaluating smoothing algorithms plausibility judgements 
proceedings th annual meeting association computational linguistics acl pp 

toulouse 
lapata lascarides 

probabilistic account logical metonymy 
computational linguistics 
lapata mcdonald keller 

determinants adjective noun plausibility 
proceedings th meeting european chapter association computational linguistics eacl pp 

laplace 

philosophical essay probabilities 
springer verlag 
lauer 

designing statistical language learners experiments noun compounds 
unpublished doctoral dissertation macquarie university 
leacock chodorow 

combining local context wordnet similarity word sense identification 
fellbaum ed wordnet electronic lexical database 
mit press 
lee 

measures distributional similarity 
proceedings th annual meeting association computational linguistics acl pp 

lee 

effectiveness skew divergence statistical language analysis 
artificial intelligence statistics 
lee pereira 

distributional similarity clustering vs nearest neighbours 
proceedings th annual meeting association computational linguistics acl pp 

lee 

similarity approaches natural language processing 
unpublished doctoral dissertation harvard university cambridge massachusetts 
levin 

english verb classes alternations preliminary investigation 
chicago university press 
li 

word clustering disambiguation occurrence data 
natural language engineering 
li abe 

generalizing case frames thesaurus mdl principle 
computational linguistics 
lin 

syntactic dependency local context resolve word sense ambiguity 
proceedings acl eacl 
lin 

automatic retrieval clustering similar words 
proceedings th annual meeting association computational linguistics th international conference computational linguistics coling acl pp 

montreal 
lin 

extracting collocations text corpora 
workshop computational terminology 
montreal canada 
lin 

information theoretic definition similarity 
proceedings international conference machine learning 
madison wisconsin 
lin 

automatic identification non compositional phrases 
proceedings th annual meeting association computational linguistics acl pp 

college park usa 
lin pantel 

induction semantic classes natural language text 
proceedings acm sigkdd conference knowledge discovery data mining pp 

lin zhao qin zhou 

identifying synonyms distributionally similar words 
proceedings ijcai pp 

lin 

divergence measures shannon entropy 
ieee transactions information theory 
lyons 

semantics 
cambridge cambridge university press 
brill 

automatic rule acquisition spelling correction 
proceedings international conference machine learning icml 
manning sch tze 

foundations statistical natural language processing 
cambridge mit press 
marcus santorini marcinkiewicz 

building large annotated corpus english penn treebank 
computational linguistics 
markert nissim 

metonymy classification task 
proceedings conference empirical methods natural language processing emnlp pp 

maynard ananiadou 

term sense disambiguation domain specific thesaurus 
proceedings international conference language resources evaluation lrec 
mccarthy 

semantic preferences identify verbal participation role switching alternations 
proceedings meeting north american chapter association computational linguistics naacl 
seattle wa 
mccarthy keller carroll 

detecting continuum compositionality phrasal verbs 
proceedings acl workshop multiword expressions pp 

sapporo japan 
mccarthy korhonen bond 
eds 

proceedings acl workshop multiword expressions analysis acquisition treatment 
sapporo japan 
meyer dale 

wordnet hierarchy associative anaphora resolution 
proceedings 
miller beckwith fellbaum gross miller 

word net line lexical database 
journal lexicography 
revised available princeton university miller chodorow leacock thomas 

semantic concordance sense identification 
proceedings arpa human language technology workshop 
miller charles 

contextual correlates semantic similarity 
language cognitive processes 
moldovan harabagiu peters guthrie wilks 
eds 

wordnet lexical resources applications extensions customizations 
carnegie mellon university pittsburgh 
naacl workshop morris hirst 

lexical cohesion computed thesaural relations indicator structure text 
computational linguistics 
nissim markert 

syntactic features supervised metonymy resolution 
proceedings st annual meeting association computational linguistics acl pp 

sapporo japan 
pad lapata 

constructing semantic space models parsed corpus 
proceedings st annual meeting association computational linguistics acl pp 

sapporo japan 
pantel lin 

unsupervised approach prepositional phrase attachment contextually similar words 
proceedings th annual meeting association computational linguistics acl 
pantel lin 

word word glossing contextually similar words 
proceedings conference applied natural language processing st meeting north chapter association computational linguistics anlp naacl pp 

pearce 

synonymy collocation extraction 
proceedings naacl workshop wordnet lexical resources applications extensions customizations 
carnegie mellon university pittsburgh 
pearce 

comparative evaluation collocation extraction techniques 
proceedings third international conference language resources evaluation lrec 
hanks 
eds 

new oxford dictionary english 
oxford university press 
machine readable version pedersen 

fishing exactness 
proceedings conference south central sas users group 
texas 
pereira tishby lee 

distributional clustering similar words 
proceedings st annual meeting association computational linguistics acl 
columbus ohio 


word sense disambiguation 
unpublished master thesis engineering department cambridge university 
pustejovsky 

generative lexicon 
mit press 
quine 

empiricism 
philosophical review 
quine 

word object 
cambridge mit press 
quinlan 

oxford psycholinguistic database 
oxford university press 
rada mili bicknell 

development application metric semantic nets 
ieee transactions systems man cybernetics 
rao 

diversity measurement decomposition analysis 
indian journal statistics 
ratnaparkhi reynar roukos 

maximum entropy model prepositional phrase attachment 
proceedings arpa workshop human language technology 
ravichandran rao 

new approach fuzzy part family formation cellular manufacturing systems 
advanced manufacturing technology 
resnik 

selection information class approach lexical relationships 
unpublished doctoral dissertation university pennsylvania 
resnik 

information content evaluate semantic similarity 
proceedings th international joint conference artificial intelligence pp 

montreal 
resnik 

wordnet class probabilities 
fellbaum ed wordnet electronic database 
mit press 
roget 

thesaurus english words phrases 
london uk green rooth riezler carroll 

inducing semantically annotated lexicon em clustering 
proceedings th annual meeting association computational linguistics acl pp 

rosenfeld 

decades statistical language modeling go 
proceedings ieee vol 

rubenstein goodenough 

contextual correlates synonymy 
communications acm 
sag baldwin bond copestake flickinger 

multiword expressions pain neck nlp 
proceedings rd international conference intelligent text processing computational linguistics pp 

mexico city 
salton mcgill 

modern information retrieval 
mcgraw hill 
salton wang yang 

vector space model information retrieval 
journal american society information science 
schroeder noy 

multi agent visualisation multivariate data 
proceedings th international conference autonomous agents 
schulte im walde 

clustering verbs semantically syntactic behaviour 
proceedings th international conference computational linguistics coling pp 

schulte im walde brew 

inducing german semantic verb classes purely syntactic subcategorization information 
proceedings th annual meeting association computational linguistics acl 
sch tze 

context space 
goldman norvig charniak gale eds working notes aaai fall symposium probabilistic approaches natural language pp 

menlo park ca aaai press 
sch tze 

dimensions meaning 
proceedings conference supercomputing pp 

minneapolis mn 
sch tze 

part speech induction scratch 
proceedings st annual meeting association computational linguistics acl pp 

sch tze 

automatic word sense discrimination 
computational linguistics 
nagata 

retrieving collocations occurrences word order constraints 
proceedings th meeting association computational linguistics acl pp 

madrid spain 
mccoy 

efficiently computed lexical chains intermediate representation automatic text summarization 
computational linguistics 
smadja mckeown hatzivassiloglou 

translating collocations bilingual lexicons statistical approach 
computational linguistics 
sparck jones 

synonymy semantic classification 
unpublished doctoral dissertation university cambridge 
nagao 

corpus pp attachment ambiguity resolution semantic dictionary 
proceedings fifth workshop large corpora pp 

beijing hong kong 
stevenson merlo 

automatic verb classification distributions grammatical features 
proceedings european chapter association computational linguistics eacl 
bergen norway 
stevenson merlo 

automatic lexical acquisition statistical distributions 
proceedings th international conference computational linguistics coling 
saarbrucken germany 
strzalkowski 

developments natural language text retrieval 
proceedings text retrieval conference trec 
sugawara nishimura kaneko 

isolated word recognition hidden markov models 
proceedings international conference acoustics speech signal processing icassp 
tampa florida 


semantics 
oxford basil blackwell 
van rijsbergen 

information retrieval second ed 
butterworths 
von leibniz 

table de 
ed 
fragments ind de leibniz paris 
vossen copestake 

definition structure knowledge representation 

ed inheritance defaults lexicon 
cambridge university press 
ward 

moby thesaurus 
moby project 
weeds 

word sense disambiguation 
unpublished master thesis engineering department cambridge university 
weeds 

similarity measure 
proceedings th uk special interest group computational linguistics 
leeds 
weeds 

smoothing nearest neighbours 
proceedings th uk special interest group computational linguistics 
edinburgh 
weeds weir 

finding evaluating sets nearest neighbours 
proceedings nd international conference corpus linguistics 
lancaster uk 
weeds weir 

general framework distributional similarity 
proceedings conference empirical methods natural language processing emnlp 
sapporo japan 
wiebe 

learning subjective adjectives corpora 
aaai 
wu zhou 

synonymous collocation extraction translation information 
proceedings st annual meeting association computational linguistics acl pp 

sapporo japan 
yarowsky 

sense collocation 
proceedings arpa human language technology workshop 
princeton 
yarowsky 

comparison corpus techniques restoring accents spanish french text 
proceedings nd workshop large corpora 
kyoto japan 
yarowsky 

unsupervised word sense disambiguation supervised methods 
proceedings rd annual meeting association computational linguistics acl pp 

zipf 

human behaviour principle effort 
addison wesley 
index absolute value distance additive occurrence retrieval models additive mi crm additive token crm additive type crm skew divergence measure skew divergence measure anaphor resolution antonymy applications arithmetic mean precision recall asymmetry similarity automatic thesaurus generation decision lists dice coefficient difference weighted crms difference weighted mi crm difference weighted token crm difference weighted type crm dissimilarity measures distance measures distance threshold distance weighted averaging stopping conditions bias high frequency neighbours distribution width distributional hypothesis boosting divergence measures city block distance clustering agglomerative hierarchical hard means nearest neighbour soft clustering vs nearest neighbours occurrence retrieval high precision high recall precision occurrence retrieval high recall occurrence retrieval models crms occurrence type collocation extraction compositional collocations compound noun interpretation confusion probability conjunction scope identification correlation coefficients cosine measure countability euclidean distance evaluation wordnet application gold standard human plausibility judgements pseudo disambiguation unseen data evaluation distributional similarity measures score geometric measures grammatical dependency relation data harmonic mean precision recall hindle measure human synonymy judgements hyponymy information retrieval jaccard coefficient jensen shannon divergence measure nearest neighbours kendall coefficient kullback leibler divergence measure norm norm language modelling latent semantic indexing lsi lexical chaining lexical cohesion lexical substitutability lin mi measure lin similarity theorem lin wordnet similarity measure looser thesaurus macquarie thesaurus manhattan distance matching coefficient maximum entropy maximum likelihood estimation metonymy resolution minkowski distance moby thesaurus multiword expression analysis mutual information mi nearest neighbour nearest neighbour smoothing neighbour set comparison technique non composition collocations pearson product moment correlation coef ficient potential neighbour precision prepositional phrase attachment ambiguity res pseudo disambiguation nearest neighbour voting frequency weighted voting vote neighbour parameter optimisation test set construction rasp robust accurate statistical parser recall relative entropy roget thesaurus russell rao coefficient second order affinity similarity threshold singular value decomposition svd smoothing add algorithm bayesian evaluation nearest neighbour spearman rank correlation coefficient spelling correction symmetry similarity synonymy tanimoto coefficient target noun taxi cab distance text cohesion text simplification tighter thesaurus topic identification weighted mutual information wmi word sense disambiguation word sense separation wordnet wordnet similarity measures wordnet prediction task appendix sets nouns experiments appendix lists high low frequency sets nouns 
figures brackets associated frequencies nouns bnc data 
list order frequency 
high frequency nouns way time place hand problem head thing people part number effect life eye man child money idea information system job power interest word year question ser vice name day need door role point change attention view house face support decision position sense form chance opportunity book home cost area room woman value plan line car level case arm rate policy control action mind lot range body group business letter advantage experience government order result evidence difference relationship course account kind programme feeling friend price issue report party look game dif story responsibility amount water picture record world side step access process development detail right night care reason claim member pattern fact skill school voice benefit law authority meeting country fam ily hair light choice foot condition force week activity standard situation advice food ap proach matter note share mother material effort office risk statement quality piece sort structure agreement knowledge answer product charge hour state rule method loss market im pact demand rise message application leg finger list thought influence profit datum term girl set possibility performance back operation image function example pressure degree help deal team type goal contract contribution ball resource response glass weight property series scheme sound page model movement ability im task road meaning impression test study land history facility bit language table father copy person sign tax wife space duty contact rest son behaviour success horse proposal baby basis argument practice breath staff attack status relation increase title subject principle event seat nature feature fire theory attitude element couple love treatment requirement month strength war confidence op tion cup technique provision fund training attempt class aspect concern bill truth section un student ground variety balance version chapter death police news measure building key moment commitment colour career period heart clothes character solution patient card growth damage dog energy trouble call drink daughter act lip return cause boy project progress campaign parent machine goods fear environment appeal music intention payment visit window unit sentence opinion air style path field court club move bag implication source arrangement talk procedure sale notice mouth plant shape wall industry strategy memory hope site mistake hold concept design education lead player reputation bed stage research centre shoulder offer doctor film minute item smile analysis church pain post appearance fish society connection worker match animal income expression scene document equipment manager box size purpose potential route comment meal bottle council discussion tea collection firm threat shop tree instruc tion user production street objective investment election economy bank edge tooth protection egg presence means date target teacher pair secretary consequence competition morning capacity chal belief article technology vote significance proportion doubt husband hole gun mark shot board direction length age freedom existence sight file explanation distance customer city injury gap corner evening community coffee majority top conference affair committee army security object debt reaction garden base aid holiday stock neck brother start con trade show nose award audience train desire text noise plenty speech package extent town cut pleasure limit reader song minister peace officer sum pupil chair photograph flower run distinction priority boat defence management victory cash phone client factor track network speed links address supply mile disease department race improvement past leader cigarette appointment skin population fee permission papers lesson computer exercise trip stair faith investigation stone battle suggestion characteristic station health habit drug weapon sheet independence sister bird initiative division assessment spirit blood obligation review opposition employment budget atmosphere flow association alternative visitor turn dress identity hat vision engine notion journey surface dream cell dinner floor cover loan capital approval entry ear request pace representation emphasis criticism band round flat sex suit play art assistance shoe organisation warning debate vol ume row taste prime half wine signal framework ship recommendation reduction coat program break knife touch criterion tension description crime danger liability season burden aim awareness passage fortune credit reality judgement ticket breakfast expectation survey interview complaint lack director legislation promise reading vehicle software ring link score newspaper licence code lunch flight trend sequence wave consideration stuff foundation tradition commission secret definition map assumption output selection culture bar river driver recognition release asset distribution quantity labour reform component concentration heat conflict root relief partner cheek wish victim failure employee gift reply mass fall error fruit spot insight fun session plate trial birth prize tear examination sample candidate talent search outcome hospital crisis tendency painting mechanism drive village tool troops penalty tone inquiry total living scope bid branch pound respect efficiency jacket fight tour laugh grip finding circumstance origin interpretation leaf sympathy union shirt lady region theme possession video courage tie resistance con tent accommodation boot display crowd profile variation treaty bridge comparison temperature marriage exhibition throat thinking contents cheque background resolution star regulation involvement invitation string expenditure enemy barrier trust cat institution scale bound ary agent gaze muscle ban harm glance telephone wealth certificate communication wing qualification ex aircraft combination accident organization stake device block tape gate guidance load emo tion pool hotel proceedings front transfer symptom pension drop soul recovery representative hill trick television judgment indication island instrument magazine colleague experiment estate brain comfort kid joke president rock oil bus tongue cake milk restriction deficit height wheel owner curtain objec tion module guide revenue wind safety press pipe diet decline violence chain dimension command membership hall ambition anger shift library wood motion supplies pride knee frame preference circle plane wages sleep imagination bone strike channel sea dispute premise perspective reward mood species protest watch compensation seed formation focus winner phrase agency strain creation privilege estimate century allowance rent afternoon silence championship bomb tale guest farm shadow mix ture effectiveness constitution prospect gain camera tank incident bell clue parliament exchange es consent observation regard location guideline gas politics weekend university meat expense category suspicion receiver stick public eyebrow enthusiasm incentive deposit poem bath anxiety settlement factory excuse enquiry check struggle earth store clause bond cap beer science master conviction blow waste stress beauty opening satisfaction generation yard football weakness flavour prisoner low frequency nouns signing reactor penetration patrol newcomer stab reconstruction progression pepper patron age optimism lining gravity continent waiter terminology paw mayor lesion heed fulfil ment feasibility realm pose node facet vulnerability sterling quarry para flap economics dock differentiation beef trauma shortcoming saving readiness primacy meter manufacture daylight congestion blend blast barn token stripe shoot safeguard reinforcement rehearsal projection precision paradox contamination brigade bidding reversal recruit rag passing pang multitude leisure federation deployment aura velocity vegetation trustee tribe self esteem romance refund manipulation dwelling calendar banker aeroplane wolf spur shipment shed renewal porter mound merchant lobby hay corn bomber anomaly treat stove simplicity re reconciliation mutation misunderstanding grandchild civilian algorithm wait thickness slab self confidence reservoir nutrient monitoring liquidity classroom spice shotgun shah pointer liver groove grid fax excursion equaliser enzyme dome diversion contention commissioner bacon suspect singles severity marking grouping elite dot decrease comb carrot underground trainee surgeon rim respects receptor lap keeper instability ink inadequacy guardian gradient dart ballot arena tobacco spin quest healing happening grape garlic friction footing decay compass buzz boiler bloom bark tan overhead mechanic intruder insulation fright discrepancy deadlock abundance rocket resort referral outsider outbreak ence idiot goose fort diamond desert cough constituent cabin briefing boy appliance amusement trout taxpayer der scepticism richness purity performer lounge fridge extreme dissatisfaction detour corps backbone violin stir sole occupant metabolism interviewer honey foul feeding distributor blueprint beating wheelchair spade sickness rainbow push pigeon petal obstruction marble liaison gum geography functioning fracture despair turkey torque terrorist talking spark sailing realism plank physics opposite omission impatience haven fellowship familiarity detection classic tourism textbook telescope scholar ramp propensity parity monk microcomputer manifestation making individuality generator dependency clip alignment wheat vowel rhetoric refreshment ration pitfall migration irony helper endeavour dedication cube compatibility cane bail warranty utmost ripple repeat prohibition planner nationality imagery hatch fountain flank fig demon copper cohesion bouquet archive vault urine terrorism spectator referee pinch nickname mole mantle kidney flush enhancement dive dip administrator ad tug template spreadsheet rift replay prototype pillar permit librarian intimacy inconsistency imprint fin ferret duplication drainage transcription tally su stare selling seizure resonance psychologist pebble landmark em cotton commodity backdrop ant woodland swim skiing shop refinement promoter kindness dam cocktail burglary bracket bra wig vector tram suitability sherry pest pastry ocean insider inconvenience hide gamble fog eating conspiracy chore bump basics actress virgin sur name scanner potency parasite mob membrane mastery hp fringe abandonment voyage typewriter printing gin foliage explosive confidentiality communist ass addiction verb solvent sensibility recall pea interruption ference honesty ham generalization constant compartment chick boredom adjective vet update runway recipient paste inn glare fortress due diameter colouring bulletin blush anchor ale ace void vent twig toxin thirst stump presumption peg neutrality mist hello gloss frog excellence deduction contingent chess canon visibility third spike slate scalp radiator leverage jersey jail imposition gardening filling cone comprehension coating clump click caller symphony steward spotlight scratch rotation retailer reach pop patron mosaic moor men investigator intrusion glue dye deprivation broadcasting transcript survivor spiral schooling orbit mapping freeze footprint enforce ment differential crest consortium complement coil clone calcium vaccine travelling tractor sophistication self control rigour readership plant ing plain insistence extinction embryo ditch cruise broker blue waterfall spy shake inscription gland dawn wreath ventilation turning tuition slipper saddle remuneration persona perfection neglect napkin ger linkage likeness hollow freight fossil flute elegance dynamics deterrent cop convenience burglar buck staining pyramid pedigree packaging overtime overlap oak jar gon harness drag cock clay cannon brace bargaining trumpet supervisor subtlety substitution stigma rack playing perimeter paradigm packing necklace memo local imitation foil fireplace elevation detachment capture canopy adherence pronunciation pneumonia manpower mammal loophole lighter legion fuse formality float fingerprint exhibit compost compiler rot regression meadow mainframe irregularity ion hormone cutter cork collision captive bible berry baton witch tempo showing schema roller realization prose plague pawn ore noun mining instant hem globe gender gauge dumping fall digit depot cuff butt beak artefact arc wear vine valuable terrain spanner softness sediment screening provider programming principal photography pal km intuition intent generosity flu fingertip dump diesel cult catering cancellation bonnet annoyance walker vice uniqueness tart tackle stadium rubber razor questioning pricing pram magnet liberal leukaemia hydrogen garrison fatigue demonstrator daddy colon carbon cache breeze amp uniformity superior sitting senator notation neighbourhood loading lab ingenuity handwriting forearm fair escort donor distraction disparity climber authenticity salon resilience pellet interpreter hare grounding galaxy fill dissertation disgust bombing binding artwork 
