accurate information extraction research papers conditional random fields peng department computer science university massachusetts amherst ma cs umass edu increasing research search engines citeseer literature search hiring decisions accuracy systems paramount importance 
employs conditional random fields crfs task extracting various common fields headers citation research papers 
basic theory crfs understood best practices applying real world data requires additional exploration 
empirical exploration factors including variations gaussian exponential hyperbolic priors improved regularization classes features markov order 
standard benchmark data set achieve new state art performance reducing error average word error rate comparison previous best svm results 
accuracy compares favorably hmms 
research search engines citeseer lawrence cora mccallum give researchers tremendous power convenience research 
increasingly recruiting hiring decisions 
information quality systems significant importance 
quality critically depends information extraction component extracts meta data title author institution headers meta data component applications field search author analysis citation analysis 
andrew mccallum department computer science university massachusetts amherst ma mccallum cs umass edu previous information extraction research papers major machine learning techniques 
hidden markov models hmm seymore 
hmm learns generative model input sequence labeled sequence pairs 
enjoying wide historical success standard hmm models difficulty modeling multiple non independent features observation sequence 
second technique discriminatively trained svm classifiers han 
svm classifiers handle features 
sequence labeling problem han 
stages process classifying line independently assign label adjusting labels additional classifier examines larger windows labels 
solving information extraction problem steps looses tight interaction state transitions observations 
results research meta data extraction task conditional random field lafferty explore practical issues applying crfs information extraction general :10.1.1.120.9821
crf approach draws advantages finite state hmm discriminative svm techniques allowing arbitrary dependent features joint inference entire sequences 
crfs previously applied tasks name entity extraction mccallum li table extraction pinto shallow parsing sha pereira 
basic theory crfs understood best practices applying new real world data early exploration phase 
explore key practical issues regularization empirical study gaussian chen rosenfeld exponential goodman hyperbolic pinto priors exploration various families features including text lexicons layout proposing method beneficial zero count features incurring large memory penalties 
describe large collection experimental results traditional benchmark data sets 
dramatic improvements obtained comparison previous svm hmm results 
conditional random fields conditional random fields crfs undirected graphical models trained maximize conditional probability lafferty :10.1.1.120.9821:10.1.1.120.9821
common special case graph structure linear chain corresponds finite state machine suitable sequence labeling 
linear chain crf parameters 
defines conditional probability state label sequence yt input sequence xt zx exp kfk yt yt zx normalization constant probability state sequences sum fk yt yt feature function binary valued real valued learned weight associated feature fk 
feature functions measure aspect state transition yt yt observation sequence centered current time step example feature function value yt state title yt state author xt word appearing lexicon people names 
large positive values indicate preference event large negative values event 
model defined equ 
probable labeling sequence input arg max efficiently calculated dynamic programming viterbi algorithm 
calculating marginal probability states transitions position sequence dynamic programming inference procedure similar forward backward hidden markov models 
parameters may estimated maximum likelihood maximizing conditional probability set label sequences corresponding input sequences 
log likelihood training set consider finite state models correspondence states labels strictly necessary 
counts log scale lambda empirical distribution xi yi written log yi xi kfk yt yt log maximizing corresponds satisfying equality empirical count feature matches expected count model 
fk yt yt xi fk yt yt xi crfs share advantageous properties standard maximum entropy models including convex likelihood function guarantees learning procedure converges global maximum 
traditional maximum entropy learning algorithms gis iis pietra train crfs quasi newton gradient climber bfgs converges faster malouf sha pereira 
bfgs optimization 
experiments shall focus aspects crf deployment regularization selection different model structure feature types 
regularization crfs avoid fitting log likelihood penalized prior distribution parameters 
shows empirical distribution parameters learned likelihood including features non zero count training set 
prior distributions shape similar empirical distribution gaussian prior exponential prior hyperbolic prior shown 
provide empirical study priors 
gaussian exponential hyperbolic shapes prior distributions gaussian prior gaussian prior log likelihood penalized follows log yi xi variance 
maximizing corresponds satisfying fk yt yt xi fk yt yt xi adjusted constraint adjustments imposed priors intuitively understandable matching exact empirical feature frequencies model tuned match discounted feature frequencies 
chen rosenfeld discuss context discounting procedures common language modeling 
call term subtracted empirical counts case discounted value 
variance feature dependent 
simplicity constant variance features 
experiment alternate versions gaussian prior variance feature dependent 
gaussian priors gradually overcome increasing amounts training data right rate 
methods provide ways alter rate changing variance gaussian prior dependent feature counts 

threshold cut language modeling turing smoothing low frequency words smoothed 
apply idea smooth features frequencies lower threshold experiments standard practice language modeling 

divide count discounted value feature depend frequency training set ck fk yt yt 
discounted value ck constant features 
way increase smoothing low frequency features high frequency features 

bin divide features classes frequency 
bin features frequency training set features bin share variance 
discounted value set ck ck count features bin size ceiling function 
alternatively variance bin may set independently cross validation 
exponential prior gaussian prior penalizes square weights intention create smoothly differentiable analogue penalizing absolute value weights 
result sparse solutions features weight nearly zero provide kind soft feature selection improves generalization 
goodman proposes exponential prior specifically laplacian prior alternative gaussian prior 
prior log yi xi parameter exponential distribution 
maximizing satisfy fk yt yt xi fk yt yt xi corresponds absolute smoothing method language modeling 
set features share constant value determined absolute discounting number features occurring twice ney 
hyperbolic prior hyperbolic prior described pinto 
hyperbolic distribution log linear tails 
consequently class hyperbolic distribution important alternative class normal distributions analyzing data various scientific areas finance frequently natural language processing 
hyperbolic prior log yi xi log corresponds satisfying fk yt yt xi fi yt yt xi hyperbolic prior tested crfs mc li 
exploration feature space wise choice features vital performance machine learning solution 
feature induction mc shown provide significant improvements crfs performance 
experiments described feature induction 
focus section aspects feature space 
state transition features crfs state transitions represented features 
feature function fk yt yt equ 
general function states observations 
different state transition features defined form different markov order structures 
define different state transitions features corresponding different markov order different classes features 
higher order features model dependencies better create data sparse problem require memory training 

order inputs examined context current state 
feature functions represented yt 
separate parameters preferences state transitions 

order transitions add parameters corresponding state transitions 
feature functions yt yt yt 

second order inputs examined context current previous states 
feature function represented yt yt 

third order inputs examined context current previous states 
feature function represented yt yt yt 
supported features unsupported features prior distributions parameters common maximum entropy classifiers standard practice eliminate features zero count training data called unsupported features 
unsupported zero count features extremely useful pushing viterbi inference away certain paths assigning features negative weight 
prior allows incorporation unsupported features doing greatly increases number parameters memory requirements 
experiment models containing containing unsupported features regularization priors argue non supported features useful 
incremental support method introducing useful unsupported features exploding number parameters unsupported features 
model trained iterations supported features 
inference determines label sequences assigned high probability model 
incorrect transitions assigned high probability model selectively add model unsupported features occur transitions may help improve performance assigned negative weight training 
desired iterations procedure may performed 
local features layout features lexicon features advantages crfs maximum entropy models general easily afford arbitrary features input 
encode local spelling features layout features positions line breaks external lexicon features framework 
study features research extraction problem evaluate individual contributions give guidelines selecting features 
empirical study hidden markov models briefly describe hmm model experiments 
relax independence assumption standard hmm allow markov dependencies observations ot st ot 
vary markov orders state transition observation transitions 
experiments model second order state transitions order observation transitions performs best 
state transition probabilities emission probabilities estimated maximum likelihood estimation absolute smoothing effective previous experiments including seymore 

datasets experiment datasets research content 
consists headers research papers 
consists pre segmented citations sections research papers 
data sets standard benchmarks previous studies seymore mccallum han 
header dataset header research defined words section usually page whichever occurs 
contains fields extracted title author affiliation address note email date phone keywords web degree publication number page seymore 
header dataset contains headers 
previous research seymore mccallum han trial randomly select training remaining testing 
refer dataset dataset dataset created cora project mccallum 
contains training rest testing 
contain fields author title editor booktitle date journal volume tech institution pages location publisher note 
refer dataset performance measures give comprehensive evaluation measure performance different metrics 
addition previously word accuracy measure accuracy field measure individual fields averaged fields called macro average information retrieval literature instance accuracy measuring performance way sensitive single error part header citation 
measuring field specific performance 
word accuracy define number true positive words number false negative words number false positive words number true negative words total number words 
word accuracy calculated 
measure precision recall measure defined follows 
precision precision recall precision recall recall measuring performance 
word accuracy word accuracy percentage words predicted labels equal true labels 
word accuracy favors fields large number words 

averaged measure averaged measure computed averaging measures fields 
average measure favors labels small number words complements word accuracy 
consider word accuracy average measure evaluation 

instance accuracy instance defined single header 
instance accuracy percentage instances word correctly labeled 
experimental results report results comparing crfs hmms previously best benchmark results obtained svms han 
break results analyze various factors individually 
table shows results dataset best results bold intro page fields shown past practice seymore han 
results obtained crfs secondorder state transition features layout features supported unsupported features 
feature induction experiments dataset didn improve accuracy 
results obtained hmm model second order model transitions order observations 
results svm obtained han computing measures precision recall numbers report 
hmm crf svm acc 
instance acc 
acc 
acc 
acc 
title author affiliation address note email date phone keyword web degree average table extraction results headers table shows results dataset svm results available datasets 
analysis performance comparison table see crf performs significantly better hmms supports previous findings lafferty pinto hmm crf acc :10.1.1.120.9821
instance acc 
acc 
acc 
author booktitle date editor institution journal location note pages publisher tech title volume average table extraction results 
crfs perform significantly better approach yielding new state art performance task 
crfs increase performance nearly fields 
word accuracy improved corresponds error rate reduction 
see word accuracy misleading hmm model higher word accuracy svm performs worse svm individual fields 
interestingly hmm performs better field versus measure pushes accuracy 
better comparison comparing field measures 
comparison svm crfs improve measure error reduction 
effects regularization results different regularization methods summarized table 
setting gaussian variance features depending feature count performs better error reduction supported features error reduction supported unsupported features 
results averaged random runs average variance 
experiments gaussian prior consistently perform better 
surprisingly exponential prior hurts performance significantly 
penalizes likelihood significantly increasing cost defined negative penalized log likelihood 
hypothesized problem choice constant inappropriate 
tried varying computing absolute discounting alternatives perform worse 
results suggest gaussian prior safer prior support feat 
features method gaussian infinity gaussian variance gaussian variance gaussian variance gaussian variance gaussian cut gaussian divide count gaussian bin gaussian bin gaussian bin gaussian bin hyperbolic exponential table regularization comparisons gaussian infinity non regularized gaussian variance sets variance gaussian cut refers threshold cut method gaussian divide count refers divide count method gaussian bin refers bin method bin size equals described practice 
effects exploring feature space state transition features unsupported features 
summarize comparison different state transition models unsupported features table 
column describes different state transition models second column contains word accuracy models support features third column contains result features including unsupported features 
comparing rows see second order model performs best dramatically better firstorder transitions third order model 
order model performs significantly worse 
difference come sharing weights ignoring yt yt 
order transition feature vital 
expect third order model perform better training data available 
comparing second third columns see features including unsupported features consistently performs better ignoring 
preliminary experiments incremental support shown performance supported features ongoing 
effects layout features analyze contribution different kinds features divide features categories local features layout features external lexicon resources 
features summarized table 
support order order trans second order third order table effects unsupported features state transitions feature name description local features starts capitalized letter characters capitalized contains digit characters digits phone number zip code contains dot contains acronym initials character capitalized character punc punctuation url regular expression url email regular expression address word word layout features line start line line middle line line line external lexicon features bibtex author match word author lexicon bibtex date words jan feb notes words appeared submitted affiliation words institution labs table list features results different features shown table 
layout feature dramatically increases performance raising measure sentence accuracy 
adding lexicon features improves performance 
combing lexicon features layout features performance worse layout features 
lexicons gathered large collection bibtex files examination difficult remove noise example words author lexicon affiliations 
previous gained significant benefits dividing lexicon sections point wise information gain respect lexicon class 
error analysis table classification confusion matrix header extraction field page shown save space 
word acc 
inst 
acc 
local feature lexicon layout feature layout lexicon table results different features errors happen boundaries fields 
especially transition author affiliation keyword 
note field confused inspection labeled inconsistently training data 
errors fixed additional feature engineering example including additional specialized regular expressions email accuracy nearly perfect 
increasing amount training data expected help significantly indicated consistent nearly perfect accuracy training set 
investigates issues regularization feature spaces efficient unsupported features crfs application information extraction research papers 
regularization find gaussian prior variance depending feature frequencies performs better alternatives literature 
feature engineering key component machine learning solution especially conditionally trained models freedom choose arbitrary features plays important role regularization 
obtain new state art performance extracting standard fields research papers significant error reduction metrics 
suggest better evaluation metrics facilitate research task especially field word accuracy 
provided empirical exploration previously published priors conditionally trained loglinear models 
fundamental advances regularization crfs remains significant open research area 
acknowledgments supported part center intelligent information retrieval part sd number part national science foundation cooperative agreement number atm subcontract university atmospheric research part central intelligence agency national security agency national science foundation nsf iis 
opinions findings rec title auth 
date abs 
aff 
addr 
email deg 
note ph 
intro web title author date 
address email degree note phone intro keyword web expressed material author necessarily reflect sponsor 
chen rosenfeld 

survey smoothing techniques models 
ieee trans 
speech audio processing pp 

january 
goodman 

exponential priors maximum entropy models 
msr technical report 
han giles zha zhang fox 

automatic document meta data extraction support vector machines 
proceedings joint conference digital libraries 
lafferty mccallum pereira 

conditional random fields probabilistic models segmenting labeling sequence data 
proceedings international conference machine learning 
lawrence giles bollacker 

digital libraries autonomous citation indexing 
ieee computer 
malouf 

comparison algorithms maximum entropy parameter estimation 
proceedings sixth conference natural language learning conll mccallum 

efficiently inducing features conditional random fields 
proceedings conference uncertainty intelligence uai 
mccallum nigam rennie seymore 

automating construction internet portals machine learning 
information retrieval journal volume pages 
kluwer 

table confusion matrix mccallum li 

early results named entity recognition conditional random fields feature induction web enhanced lexicons 
proceedings seventh conference natural language learning conll 
ney essen kneser 
estimation small probabilities leaving 
ieee transactions pattern analysis machine intelligence 
pietra pietra lafferty 
inducing features random fields 
ieee transactions pattern analysis machine intelligence vol 

pinto mccallum wei croft 

table extraction conditional random fields 
th annual international acm si gir conference research development information retrieval sigir seymore mccallum rosenfeld 

learning hidden markov model structure information extraction 
proceedings aaai workshop machine learning information extraction 
sha pereira 

shallow parsing conditional random fields 
proceedings human language technology conference north american chapter association computational linguistics hlt naacl 

bibliographic attribute extraction erroneous statistical model 
proceedings joint conference digital libraries 
