formal grammar information theory 
pereira laboratories research shannon laboratory park avenue florham park nj usa pereira research att com years research models spoken written language split seemingly irreconcilable traditions formal linguistics chomsky tradition information theory shannon tradition 
harris advocated close alliance grammatical information theoretic principles analysis natural language early formal language theory provided strong link information theory linguistics 
research language computation grammatical information theoretic approaches moved far apart 
today years defensive information theoretic approach gained new strength achieved practical successes speech recognition information retrieval increasingly language analysis machine translation 
exponential increase speed storage capacity computers proximate cause engineering successes allowing automatic estimation parameters probabilistic models language counting occurrences linguistic events large bodies text speech 
argue informationtheoretic computational ideas playing increasing role scienti understanding language help bring formal linguistic information theoretic perspectives 
keywords formal linguistics information theory machine learning 
great divide years research models spoken written language split seemingly irreconcilable points view formal linguistics chomsky tradition information theory shannon tradition 
famous quote chomsky signals split 
green ideas sleep 
sleep ideas green 
fair assume sentence part sentences occurred english discourse 
statistical model sentences ruled identical grounds equally remote english 
nonsensical grammatical 
phil 
trans 
soc 
lond 
royal society pereira split harris advocated close alliance grammatical information theoretic principles analysis natural language harris 
early formal language theory provided strong link information theory linguistics 
research language computation bridges lost urge take sides personal ideological scienti today years defensive information theoretic approach led practical successes speech recognition information retrieval increasingly language analysis machine translation 
exponential increase speed storage capacity computers proximate cause successes allowing automatic estimation parameters computational models language counting occurrences linguistic events large bodies text speech 
vastly increased computer power irrelevant automatically derived models linguistic data able generalize unseen data 
argue progress design analysis models playing central role practical advances carries promise fundamentally deeper understanding information theoretic computational complexity constraints language acquisition 

harris program ascent generative linguistics early swept focus attention away distributional views language especially earlier tradition 
tradition harris developed probably best articulated proposal marriage linguistics information theory 
proposal involves main called constraints harris follows 
partial order word zero classes words called arguments word occur sentence word argument classes strong similarity argument class information word suggested harris type categorial grammar subcategorization frames linguistic formalisms 
traditional categorial grammar lambek con ates function argument relationships linear order harris factors linear order explicitly 
categorial grammar acquired technical means investigate factorizations 
clear harris partial order may formalized partial order set theoretic function types 
modern categorial grammar harris partial order constraint speci es basic con gurations corresponding elementary clauses complex clauses result applying constraint reduction elementary clauses 
likelihood word particular roughly stable likelihood occurring argument operator word cases uncertainty disagreement speakers change time current terminology interpret likelihood constraint probabilistic version selectional restrictions 
harris sharp distinction phil 
trans 
soc 
lond 
formal grammar information theory general language likelihoods llers argument positions represent tendencies technical sublanguages hard constraints argument llers correspond closely usual notion selectional restriction 
reduction consists language speci able types reduction reduced high likelihood material example zeroing repeated corresponding words reduction constraint tries account morphological processes contraction processes combine elementary clauses complex clauses relativization subordination coordination 
case harris claims high likelihood material may elided additional constraints reduction may necessary 
furthermore connections reduction transformational analyses harris chomsky suggest possibility modelling string distributions overt projection hidden generative process involving operator argument structures subject likelihood constraint transformations 
linking transformational categorial approaches syntax possibility especially intriguing cornell 
linearization relation sentences words partial order speech linear linear projection involved start harris theory left step underspeci ed 
transformational grammar seen ort ll gap speci mechanisms sentence generation tested native speaker grammaticality judgements 
linguistic events involve generation basic con gurations unordered simple clauses structure determined partial order constraint distribution follows probabilities associated likelihood constraint 
probabilities govern application reduction compression individual con gurations sets linked con gurations 
linearization yields observable aspects event 
discuss likelihood constraint stated harris current versions leaves dependences broader discourse context strongly ect likelihoods linguistic events 
discussion important feature harris constraints explicitly link linguistic structure distributional regularities involving relative frequencies di erent structural con gurations 
particular harris suggested structural distributional regularities support language acquisition small percentage possible sound sequences occurs utterances identify boundaries words relative likelihoods sentential environment 
generalization harris discussed functional role distributional regularities language proposed speci mechanisms language users take advantage phil 
trans 
soc 
lond 
pereira regularities language acquisition 
particular obvious language users acquire stable distributional information lexical grammatical information required partial order reduction linearization constraints limited evidence available linguistic environment 
question created great opening chomsky critique empiricist linguistics green ideas quote early instance 
chomsky concluded sentences equally observation sentence part thereof occurred previously abney 
observation argued statistical model frequencies word sequences assign equal zero probabilities sentences 
relies unstated assumption probabilistic model necessarily assigns zero probability unseen events 
case model probability estimates just relative frequencies observed events maximum likelihood estimator 
understand naive method badly ts training data 
problem tting tightly connected question learner generalize nite training sample 
canonical example tting polynomial observations 
nite set observed values dependent random variable distinct values independent variable seek hypothesis functional dependency set observations tted exactly polynomial high degree 
curve typically poor predictor new observation exactly matches peculiarities training sample 
avoid usually smooths data lower degree polynomial may training data exactly dependent vagaries sample 
similarly smoothing methods probability models assign probability mass unseen events jelinek mercer 
fact earliest methods due turing published chomsky attack empiricism ect statistical models language katz 
smoothing forms regularization constrain form statistical models ensure better generalization unseen data instance central theme statistical learning theory sample complexity relationship training sample size model complexity generalization ability model 
typical theoretical results area give probabilistic bounds generalization error model function model error training data sample size model complexity margin error vapnik 
qualitative terms gap test training error measure tting grows model complexity xed training sample size decreases sample size xed model complexity 
quantify trade training set accuracy generalization new data constraints model need rigorous measure model complexity 
polynomial example usual intuition complexity measured degree polynomial number tunable coe cients model intuitions harder come model classes simple parametric form 
furthermore polynomial case common sense complexity measure misleading certain approaches polynomial tting yield smaller model complexity better generalization ability vapnik 
de ni phil 
trans 
soc 
lond 
formal grammar information theory tion model complexity intimately tied learning setting instance assumes data distribution known form unknown parameters usually done statistics takes distribution free view data distributed unknown xed training test distribution valiant assumes line setting goal best possible prediction xed sequence incrementally generated environment littlestone warmuth freund schapire 
crucial idea distribution free setting model complexity measured nite model class combinatorial quantities vapnik chervonenkis vc dimension vapnik chervonenkis roughly speaking gives order polynomial upper bound number distinctions samples models class function sample size 
returning debate empiricism relationships model complexity sample size tting developed learning theory may help clarify famous argument poverty stimulus aps 
reacting empiricist especially theories chomsky argued general purpose learning abilities su cient explain children acquisition native language limited linguistic experience available learner 
particular claimed linguistic experience provide negative examples grammaticality making learner task harder 
conclude specialized innate language faculty involved 
green ideas example early instance argument asserting statistical procedures acquire model grammaticality data available learner 
aps just require restrictions model classes ensure ective generalization nite data viewpoint 
usual form aps claims learning mechanism developed speci cally language generalize limited linguistic experience 
aw argument implicitly assumes constraints learner arising particular representations learner knowledge know informational di culty learning problems characterized purely combinatorial means 
statistical learning theory gives tools compute empirically testable lower bounds sample sizes guarantee learnability model classes bounds pessimistic take account constraints model search procedure 
debate aps empirically grounded account calculations stimuli aps supporters claimed missing signi cant frequency 
aps reached extreme form chomsky principles parameters theory learnability requires set possible natural languages generated settings nite set nitely valued parameters chomsky 
extreme constraint necessary nite model classes nite vc dimension learnable information theoretic point view su cient nite classes may ciently learnable search model generalization may computation phil 
trans 
soc 
lond 
pereira ally information principle available kearns valiant 

hidden variables early empiricist theories linguistic behaviour easy targets critiques chomsky denying signi cant role internal unobservable state language user 
markov model language state information represented externally observable sequence past linguistic behaviour 
case empiricist position somewhat ed critics 
consider language user updates expectations probable responses statistics collected past experience expectations response propensities represented part user state directly available observation 
furthermore behaviour language users may give valuable information power experience encoding mechanisms 
instance language user maintains statistics pairs consecutive words bigram statistics ective anticipating reacting appropriately word user keeps statistics longer word sequences words bigram model may higher entropy 
example related nite state text compression may simplistic point view linguistics convenient test bed ideas statistical modelling constraints model structure introduces idea hidden modelling state simple form 
hidden random variables language user state statistics involving joint values represent user uncertainty interpretation best response events observed far 
uncertainty may just interpretation particular course events particular model class models best compromise tting experience far generalizing new experience 
best choice model uncertain bayesian model averaging willems combine predictions di erent candidate models language user degree belief measured past success 
model averaging way learners hedge bets particular grammars initial bets represent prior belief particular grammars updated regularizing procedure balances past experience predictive power new experience 
prior distribution grammars seen form innate knowledge implicitly biases learner better instance complex grammars 
particular nite class grammars universal prior number bits needed encode members class favours complex grammars compatible data solomono horning 
results provide way quantifying relationship prior grammars training sample size generalization power case ignored interested language acquisition aps 
advances statistical learning theory mcallester may provide new theoretical impetus research direction show prior intractable usual sense theory computation problem proven belong standard classes believed require polynomial time deterministic sequential computer instance np hard problems 
phil 
trans 
soc 
lond 
formal grammar information theory models play analogous regularizing role combinatorial complexity measure 
role hidden variables capturing uncertainty interpretation particular experience especially interesting modelling ambiguity 
example going back harris theory constraints involves covert choices language user assignment types positions partial order lexical items lexical choice selection probabilities reduction choices distributional statistics predictability linearization choices 
generally model language appeals non observables instance model assigns syntactic analyses requires hidden variables 
hidden variables representing uncertainty interpretation create factored models joint distributions far fewer parameters estimate easier learn models full joint distribution 
simple useful example may approximate conditional probability occurrence words con guration hidden class variable associations con guration study 
vocabulary size classes model uses cv parameters parameters direct model joint distribution prone tting particular vi vi aggregate bigram model saul pereira useful modelling word sequences include unseen bigrams 
model approximate probability string wn wn ny estimate probability string aggregate model trained newspaper text expectation maximization em method dempster nd green ideas sleep sleep ideas green suitably constrained statistical model simple meet chomsky particular challenge 
plausible de ned model statistical dependences hidden variables general su cient problem setting corresponding conditional probabilities observable linguistic material cases computationally intractable abe warmuth 
intractability results precluded signi cant algorithmic experimental progress carefully designed model classes learning methods em variants especially speech processing baum petrie baker 
particular learning problem easier practice interactions hidden variables tend factor observed variables 
phil 
trans 
soc 
lond 
pereira 
lexicalized models harris model dependency selection lexicalized sense postulated relationships words precursors sentences relationships structures generative grammar 
points view distributional modelling machine learning important property lexicalized models anchor analyses observable occurrences words unobservable relationships hypothetical grammatical structures probabilistic setting way state precisely lexicalization easier factor interactions hidden variables conditioning observed sentence 
lexicalized models involve hidden decisions allow ambiguous interpretations 
noted previous section hidden variable models computationally di cult learn observable variables 
alternative strategy constrain hidden variables associating sentences disambiguating information 
extreme information full analysis 
case interesting computational applications perspectives shown lexicalized probabilistic context free grammars automatically learned perform remarkable accuracy novel material charniak collins :10.1.1.5.5668
lexicalization models factor sentence generation process sequence conditionally independent events re ect linguistic distinctions head dependent argument adjunct 
models ect lexically stochastic generative grammars conditional independence assumptions generation process particular kind markovian assumption 
crucially assumptions apply hidden generative decisions observable utterance allow analysis ambiguity 
learning algorithms just discussed need full correct syntactic analysis training example realistic models human language acquisition 
possible direction reducing unrealistic amount supervision required additional observables correlated hidden variables prosodic information perceptual input associated content linguistic input siskind roy pentland 
generally may able replace direct supervision indirect correlations discuss 

power correlations poor stimulus language learner exploits acquire native language 
observed linguistic experience just string words grounded rich perceptual motor environment provide crucial clues acquisition interpretation production processes reason functional linguistic experience property lexicalized models palatable linguists structural relationships prime subject theory 
notice chomsky minimalist program chomsky lexically theories aspects chomsky ways reminiscent lexicalized theories particular lexical functional grammar bresnan hpsg pollard sag certain varieties categorial grammar cornell 
phil 
trans 
soc 
lond 
formal grammar information theory non linguistic environment 
points fundamental weakness discussed far formal grammar computational models language language taken completely autonomous process independently analysed simplistic information theory su ers problem basic measures information content signal intrinsic relative correlations signal events interest meaning signal 
particular harris likelihood reduction constraints appear ignore content carrying function utterances 
fortunately information theory provides ready tool quantifying information notion mutual information cover thomas suitable notion compression relative side variables interest de ned tishby 
enormous conceptual technical di culties building comprehensive theory grounded language processing treating language autonomous system tempting 
weaker form grounding exploited readily physical grounding grounding linguistic context 
path sentences viewed evidence sentences inference ectiveness language processor may measured accuracy deciding sentence entails answer appropriate question 
furthermore empirical evidence linguistic grounding carries information rst sight 
instance successful information retrieval systems ignore order words just frequencies words documents salton called bag words approach 
similar situations described similar ways simple statistical similarity measures word distributions documents queries ective retrieving documents relevant query 
way word senses automatically disambiguated measuring statistical similarity bag words surrounding occurrence ambiguous word bags words associated de nitions examples di erent senses word sch 
information retrieval sense disambiguation bag words techniques successful underlying coherence purposeful language syntactic semantic discourse levels 
sense discourse principle gale captures particular form coherence 
example occurrence words stocks bonds bank passage potentially indicative nancial subject matter tends disambiguate word occurrences reducing likelihood bank river bank bonds chemical bonds stocks ancient punishment device 
correlations correlations utterances physical context allow language processor learn linguistic environment little supervision yarowsky suggested new machine learning settings training blum mitchell 
lexicalized grammars bag words models represent statistical associations words certain con gurations 
kinds associations include description formal semantics natural language logical representations meanings sentences unobservable syntactic analyses equally arti cial inputs language acquisition process 
phil 
trans 
soc 
lond 
pereira represented di erent 
associations lexicalized grammars mediated hidden assignment dependency relationships pairs word occurrences utterance 
assignments potentially available leading great structural ambiguity discussed 
contrast associations bag words models statistical models instance markov models de ned impoverished unambiguous overt structures 
furthermore ective lexicalized models strong statistical independence assumptions parts underlying structure missing global coherence correlations bag words models capture 

local structure global distribution current stochastic lexicalized models lexically determined local correlations capture information relevant harris partial order likelihood constraints 
harris dependency grammar grammatical formalisms con ate linearization argument structure partial order constraint 
asserting rough stability likelihood argument operator harris implicitly assumed generative model dependents conditionally independent rest analysis head depend 
existing lexicalized models similar markovian assumptions typically extend lexical items additional features instance syntactic category charniak collins :10.1.1.5.5668
harris information theoretic arguments especially reduction refer likelihood string involves global correlations discussed section 
global correlations precisely markovian assumptions generative models leave 
markovian generative models able model potential correlations senses assigned occurrences stocks bonds di erent parts paragraph example 
problem may addressed main ways 
rst preserve markovian assumptions enrich lexical items features representing alternative global coherence states 
instance lexical items decorated sense features local correlations enforce global coherence 
features lexical items occurrence items operators arguments may disambiguate 
di culty approach introduces plethora hidden variables leading correspondingly harder learning problem 
furthermore relies careful crafting hidden variables instance choosing informative sense distinctions 
second approach adopt ideas random elds factor probabilities products exponentials indicator functions signi cant local global features events della pietra ratnaparkhi abney built incrementally greedy algorithms select informative feature step 

deciding understanding models information theoretic machine learning ideas successful variety language processing tasks speech recognition information retrieval 
common characteristic tasks phil 
trans 
soc 
lond 
formal grammar information theory sought decision nite set alternatives ranking alternatives 
example newswire lter classi es news stories topics speci ed training examples ii part speech tagger assigns tags words document iii web search engine ranks set web pages relevance natural language query iv speech recognizer decides possible transcriptions spoken utterance 
case task formalized learning mapping spoken written material choice ranking alternatives 
know earlier discussion generalization need restrict attention class mappings learned available data 
computational considerations experimental evaluation narrow mapping classes consideration 
suitable optimization procedure employed select mapping class minimizes measure error training set 
potential weakness task directed learning procedures ignore regularities relevant task 
regularities may highly informative questions 
language may redundant respect particular question task oriented learner may bene greatly redundancy discussed earlier follow language redundant respect set questions language user may need decide 
furthermore may reasonably argue task oriented learner really understand language accurately answer question intuitions understanding suggest competent language user accurately answer questions pertaining discourse processes 
instance competent language user able reliably answer questions pertaining clause discourse 
drawn question kinds learning tasks may involve understanding force attack immense challenges grounded language processing 
automatically trained machine translation brown alshawi douglas issue may task translation requires questions text accurately answered produce correct output 
easy nd reasonable questions left unanswered performing task 
single understanding task range tasks di culty measured uncertainty information theoretically entropy output absence information input 
objective learner acquire function reduce uncertainty exploiting mutual information inputs outputs tishby gorin 
tasks iv listed roughly order increasing output entropy machine translation possibly di cult 
theoretical representations postulated formal linguistics constituent structure functional dependency structures logical form understood phil 
trans 
soc 
lond 
pereira codi ed answers particular kinds questions pertaining text degrees information theoretic di culty 
instance di erent assignments arguments thematic roles lead di erent correct answers questions 
point view task learner acquire accurate procedure deciding simple sentence follows discourse traditional tasks deciding grammaticality assigning structural descriptions 
structural descriptions play important role theory proxies informational relationships external linguistic events products theory 

summary researchers information retrieval statistical pattern recognition neural networks kept developing theoretical experimental approaches problem generalization ignored formal linguistics cultural substantive reasons 
substantive reasons possibly important models proposed successful practice failed capture productive recursive nature linguistic events 
advances machine learning statistical models starting supply missing ingredients 
lexicalized statistical models informed linguistic notions phrase head argument adjunct specify complex linguistic events generated analysed sequences elementary decisions 
machine learning suggests rules elementary decisions learned examples behaviour learned decision rules generalize novel linguistic situations 
probabilities assigned complex linguistic events novel ones causal structure underlying models propagate uncertainty elementary decisions 
statistical models local structure complemented models correlations developed information retrieval speech recognition 
models proved quite successful automatically learning rank possible answers question unclear may combine lexical models uni ed account relationship linguistic structure statistical distribution 
furthermore barely touched question models may say human language acquisition 
statistical learning theory computational extensions help ask better questions rule non quantitative results coarse signi cantly narrow eld possible acquisition mechanisms 
successful advances machine learning arose theoretical analysis cortes vapnik freund schapire theory helping sharpen understanding power limitations informally designed learning algorithms :10.1.1.15.9362
remains done may seeing new version harris program computational models constrained grammatical considerations de ne broad classes possible grammars information theoretic principles specify models tted actual linguistic data 
gerald gazdar karen sp jones careful reading illuminating comments ido dagan lillian lee larry saul yves schabes yoram singer amit sing phil 
trans 
soc 
lond 
formal grammar information theory hal tishby joint research helped shape ideas yoav freund michael kearns rob schapire guidance learning theory steve abney alshawi michael collins don hindle mark johnson aravind joshi john la erty david mcallester michael hinrich sch stuart shieber ed conversations topics years 
am sure reasons disagree arguments interpretations help invaluable ort reconcile rich traditions study language derives 
abe warmuth computational complexity approximating distributions probabilistic automata 
machine learning 
abney statistical methods linguistics 
balancing act ed 
klavans resnik 
cambridge ma mit press 
abney stochastic attribute value grammars 
comp 
ling 

baker trainable grammars speech recognition 
th acoustical society america ed 
wolf klatt 
cambridge ma 
acoustical society america 
baum petrie statistical inference probablistic functions nite state markov chains 
ann 
math 
stat 

blum mitchell combining labeled unlabeled data training 
proc 
th ann 
conf 
computational learning theory 
new york acm 
bresnan 
ed 
mental representation grammatical relations 
cambridge ma mit press 
brown cocke della pietra della pietra jelinek la erty mercer statistical approach machine translation 
comp 
ling 

charniak statistical parsing context free grammar word statistics 
th natl conf 
arti cial intelligence pp 

cambridge ma aaai press mit press 
chomsky syntactic structures 
hague mouton 
chomsky review skinner verbal behavior language 
chomsky aspects theory syntax 
cambridge ma mit press 
chomsky knowledge language nature origin 
new york praeger 
chomsky minimalist program 
cambridge ma mit press 
collins head driven statistical models natural language parsing 
phd thesis university pennsylvania usa 
cornell type logical perspective minimalist derivations 
formal grammar ed 

van 
aix en provence 
cortes vapnik support vector networks 
machine learning 
cover thomas elements information theory 
wiley 
della pietra della pietra la erty inducing features random elds 
ieee trans 
pattern analysis machine intel 

dempster laird rubin maximum likelihood incomplete data em algorithm 
stat 
soc 

freund schapire decision theoretic generalization line learning application boosting 
comp 
syst 
sci 

gale church yarowsky sense discourse 
proc 
th darpa speech natural language workshop pp 

san francisco ca morgan kaufmann 
population frequencies species estimation population parameters 
biometrika 
phil 
trans 
soc 
lond 
pereira harris structural linguistics 
university chicago press 
harris string analysis sentence structure 
hague mouton 
harris language information 
new york columbia university press 
harris theory language information mathematical approach 
oxford clarendon 
horning study grammatical inference 
phd thesis stanford university usa 
jelinek mercer interpolated estimation markov source parameters sparse data 
proc 
workshop pattern recognition practice 
amsterdam north holland 
katz estimation probabilities sparse data language model component speech recognizer 
ieee trans 
acoust 
speech sig 
proc 

kearns valiant cryptographic limitations learning boolean formulae nite automata 
acm 
lambek mathematics sentence structure 
am 
math 
mon 

littlestone warmuth weighted majority algorithm 

comput 

mcallester pac bayesian model averaging 
proc 
th ann 
conf 
computational learning theory pp 

new york acm 
multimodal linguistic inference 
bull 
interest group pure appl 
logics 
type logical grammar categorial logic signs 
dordrecht kluwer 
pollard sag head driven phrase structure grammar 
university chicago press 
learnability poverty stimulus 
proc 
nd ann 
general session role learnability grammatical theory ed 
johnson pp 

berkeley ca berkeley linguistics society 
ratnaparkhi linear observed time statistical parser maximum entropy models 
nd conf 
empirical methods natural language processing emnlp ed 
cardie weischedel 
somerset nj association computational linguistics 
roy pentland learning words natural audio visual input 
int 
conf 
spoken language processing vol 
pp 

sydney australia australian speech science technology association 
salton automatic text processing transformation analysis retrieval information computer 
reading ma addison wesley 
saul pereira aggregate mixed order markov models statistical language processing 
proc 
nd conf 
empirical methods natural language processing ed 
cardie weischedel pp 

somerset nj association computational linguistics 
distributed morgan kaufmann san francisco ca 
sch ambiguity resolution language learning computational cognitive models 
stanford ca csli 
siskind computational study cross situational techniques learning word mappings 
cognition 
solomono formal theory inductive inference 

control 
derivational minimalism 
logical aspects computational linguistics ed 
retor pp 

springer 
tishby gorin algebraic learning statistical associations 
comp 
speech lang 

phil 
trans 
soc 
lond 
formal grammar information theory tishby pereira bialek extracting relevant bits information bottleneck method 
proc 
th allerton conf 
communication control computing ed 
hajek 
urbana il university illinois 
valiant theory learnable 
commun 
acm 
vapnik nature statistical learning theory 
springer 
vapnik chervonenkis uniform convergence relative frequencies events probabilities 
theory prob 
appl 

willems shtarkov context tree weighting method basic properties 
ieee trans 

theory 
yarowsky unsupervised word sense disambiguation rivaling supervised methods 
rd ann 
association computational linguistics pp 

somerset nj association computational linguistics 
discussion coleman university oxford uk 
status statistical information natural language processing systems compensate gaps theoretical knowledge model humans behaviour face uncertainty 
pereira 
uncertainty may linguistic knowledge shared interlocutors 
variation populations modelled statistical distribution 
furthermore variation drives language change speech communities 
see anthony colleagues university pennsylvania 
increasing evidence see michael spivey colleagues cornell university uncertainty explicitly represented ranging activation levels human language processing 
taylor university edinburgh uk 
tested nonsensical ungrammatical sentences smoothed language model con rm assigned appropriately di erent probabilities 
pereira 
con rmed sentences assigned positive di ering probabilities 
note added proof 
published version discusses simple experiment answering question 
ahmad university surrey uk 
harris apply model primarily specialized languages 
pereira 
harris strong distinction specialized scienti language general language arguing instance selection restrictions probably di erence degree likelihood restrictions holding 
phil 
trans 
soc 
lond 

