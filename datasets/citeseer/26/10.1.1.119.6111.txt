proceedings fourteenth annual conference uncertainty artificial intelligence uai pages madison wisconsin july tractable inference complex stochastic processes xavier boyen stanford university xb cs stanford edu monitoring control dynamic system depends crucially ability reason current status trajectory 
case stochastic system tasks typically involve belief state probability distribution state process point time 
unfortunately state spaces complex processes large making explicit representation belief state intractable 
dynamic bayesian networks dbns process represented compactly representation belief state intractable 
investigate idea maintaining compact approximation true belief state analyze conditions errors due approximations taken lifetime process accumulate answers completely irrelevant 
show error belief state contracts exponentially process evolves 
multiple approximations error process remains bounded indefinitely 
show additional structure dbn design approximation scheme improving performance significantly 
demonstrate applicability ideas context monitoring task showing orders magnitude faster inference achieved small degradation accuracy 
ability model reason stochastic processes fundamental applications 
number formal models developed describing situations type including hidden markov models kalman filters dynamic bayesian networks 
different models share underlying markov assumption fact conditionally independent past current state 
domain stochastic partially observable true state process rarely known certainty 
reasoning tasks performed belief state probability distribution state system time 
follows markov assumption belief state time completely captures information past 
particular suffices predicting probabilities trajectories system making optimal decisions actions 
daphne koller stanford university koller cs stanford edu consider example task monitoring evolving system 
belief state time summarizes evidence far generate belief state time straightforward procedure propagate current belief state state evolution model resulting distribution states time condition distribution observations obtained time getting new belief state 
effectiveness procedure depends crucially representation belief state 
certain types systems gaussian processes admit compact representation belief state effective update process kalman filtering 
cases matters simple 
consider example stochastic system represented dynamic bayesian network dbn 
dbn bayesian network allows decomposed representation state state variables compact representation probabilistic model utilizing conditional independence assumptions 
belief state distribution subset state variables time general variables time participate belief state variable value time directly affects value time included 
large dbns obvious representation belief state flat distribution state space typically infeasible particularly time critical applications 
experience bayesian networks led tacit belief structure usually synonymous easy inference 
may expect structure model support decomposed representation distribution effective inference 
unfortunately hope turns unfounded 
consider dbn assume evidence variable exist 
initial time slice variables start independent 
furthermore connections variable 
time variables fully correlated see unrolled dbn conditional independence relation holds 
observing evidence variable things worse case takes time slices state variables fully correlated 
general process decomposes completely independent subprocesses evidence ev example structured stochastic process timeslice dbn unrolled bn time slices 
belief state fully correlated early time 
factored decomposition distribution rests form conditional independence decomposition belief state possible 
phenomenon serious impediment applying probabilistic reasoning methods dynamic systems 
related problems occur types processes 
example hybrid processes involving continuous discrete variables wish represent belief state mixture gaussians 
cases number components mixture grow exponentially time making impossible represent reason exact belief state 
approach dealing problem restrict complexity belief state representation allowing inference algorithms operate effectively 
exact belief state complex compactly represented approximate belief state 
example context dbn choose represent approximate belief state factored representation 
see discussion possible belief state representations dbns 
context hybrid process choose restrict number components gaussian mixture 
idea immediately suggests new scheme monitoring stochastic process decide computationally tractable representation approximate belief state decomposes independent factors 
propagate approximate belief state time transition model condition evidence time 
general resulting belief state time fall class chosen maintain 
approximate propagate 
general strategy face risk repeated approximations cause errors build control extended periods time accumulation due repeated approximations worse spontaneous amplification due sort instability 
show problem occur mere stochasticity process serves attenuate effects errors time fast prevent accumulated error growing unboundedly 
specifically proved important knowledge new result stochastic processes certain assumptions contraction kl divergence propagation distributions stochastic transition model results constant factor reduction kl divergence 
believe result significant ap plications contexts 
result applies discrete stochastic process approximate representation belief state 
show stronger results obtained case structured processes equipped approximation scheme tailored structure process 
specifically consider processes composed weakly interacting subprocesses approximation scheme decomposes belief state product independent belief states individual processes 
assumptions contraction rate improves dramatically 
show strong connection amount interaction subprocesses quality bounds provide empirical evidence success approach practical dbns showing exploiting structure achieve orders magnitude faster inference tiny degradation accuracy 
mentioned dbns counter example intuition structured representations leads effective inference 
results show approximate inference exploit structure dbn computational gain 
basic framework focus discrete time finite state markov processes 
processes modeled explicitly hidden markov model additional structure compactly dynamic bayesian network 
discrete time markov process evolves moving state consecutive times points 
denote state time case dbn may described assignment values set state variables 
markov assumption inherent models consider asserts state system contains information independent ip past ip simplicity assume process probability transition state time state time depend obtain process described transition model denote event case hmm specified explicitly matrix dbn described compactly fragment bayesian network see section 
ip markov process typically hidden partially observable meaning state directly observable 
observe response case dbn assignment set observable random variables 
response depends stochastically exclusively state conditionally independent denote observability aspect process described observation model obtain markov assumption implies historical information needed monitor predict system evo ip lution contained available knowledge state 
knowledge summarized belief state possible states 
distinguish prior posterior belief state definition prior belief state time denoted distribution state response history including time letting denote response observed time ip posterior belief state time denoted distribution state time response history including time ip monitoring task defined task maintaining belief state time advances new responses observed 
principle procedure quite straightforward 
assume posterior belief state time observing response time new state tation models prior belief obtained stage compu distribution state time slice obtained propa gating stochastic transition model posterior obtained conditioning response observed time abstractly view function mapping define function mapping observing response time 
exact monitoring simple principle quite costly 
mentioned belief state process represented compactly dbn typically exponential number state variables impractical general feasibly store belief state far propagate various update procedures described 
interested utilizing compactly represented approximate belief states inference algorithm 
risks associated idea clear errors induced approximations may accumulate results inference completely irrelevant 
show sections stochasticity process prevents problem occurring 
simple contraction consider exact estimated belief states intuitively propagate transition model forgets information distributions forget differences closer 
see section order errors remain bounded effect needs dampen exponentially quickly 
need show reduces distance belief states constant factor 
result known norm euclidean distance distance distributions second largest eigenvalue unfortunately norm inappropriate purposes 
recall main operations involved updating belief state propagation transition model conditioning observation 
unfortunately norm behaves badly respect conditioning arbitrarily larger fact construct examples observation response cause distance grow 
search alternative distance measure try prove contraction result 
obvious candidate relative entropy kl divergence quantifies loss inefficiency incurred distribution true distribution definition space relative entropy id ln ln relative entropy variety reasons detailed ch natural measure discrepancy distribution approximation 
furthermore contrast behaves reasonably respect conditioning fact id id response time prior unfortunately simply shifted problem place 
relative entropy better behaved respect known contraction result related properties known relative entropy increases transition stochastic process id id ultimately tends zero broad class processes id ergodic 
unfortunately need stronger results wish bound accumulation error time 
main contributions proof knowledge stochastic transition contracts relative entropy geometric rate 
prove stochastic process lead contraction relative entropy distance 
useful case dbns consider somewhat general setting sets states stochastic transition necessarily 
anterior state space state space 
stochastic matrix representing random process anterior corresponding distributions induced goal measure minimal extent process causes distributions 
worst case common starting point mass distribution state mass distribution 
stochastic nature process state causes place weight posterior probability state probability mass agreement agree min mass insight natural characterization mixing properties process definition markov process stochastic transition model minimal mixing rate min min show stochastic propagation guaranteed reduce relative entropy factor proof relies lemma allows isolate probability mass distributions guaranteed mix lemma matrix admits additive contraction decomposition intuitively argued certain portion probability mass agree 
non negative ma matrix captures portion transition unifies 
conditions state unification happen probability distributions agree portion 
lemma contraction result follows easily 
essentially argument construction explicit different behavior process corresponding parts contraction decomposition suggested split process separate phases process decides contract second appropriate transition occurs 
define new intermediate state space contains new distinguished contraction state identical 
separate cases process guaranteed contract having correspond explicit transition processes behave identically remaining states behave position prove contraction theorem theorem id id 

decomposition theorem generic markov transition process stage process equivalent arrows denote stochastic transitions states 
proof fix define new phase process markovian transition new state set see 
intuitively state corre sponds state corresponds state obtained process contracts 
process transitions contraction probability state preserves state probability process behaves states duplicates aggregate havior easy show decomposed process equivalent original process case distributions 
id show id note id markovian id claim 
compound processes un id id 
results far apply discrete state markovian process 
potential impact greatest cases explicit representation belief state infeasible 
dbns allow complex processes specified compactly fall category 
results apply dbns 
unfortunately answer somewhat discouraging highly structured dbns 
extreme example imagine process composed binary variables evolving independently flipping value time slice probability variable viewed separate markov process mixing rate may expect mixing rate process processes independent hardly expect 
theorem tells computing different story transition matrix compound process gets small value definition mixing rate simply pessimistic 
unfortunately 
fallacy assumption local mixing properties automatically carry compound process 
subprocess rapidly mixing belief states variable 
belief state compound process involves dependencies variables belonging different subprocesses contraction rate bad prediction 
assume example true distribution gives probability state assignment values variables approximate distribution probability gives state opposite view state space hypercube distributions mass assignment vertices hypercube 
single step transition matrix diffuses mass distributions randomly starting points 
probability diffusion process point reaches exponentially low bits flip distributions 
approach improving results decomposed processes additional assumptions structure belief state distributions decompose 
clearly assumption unreasonable true belief state lack decomposability basis entire 
somewhat surprisingly suffices decomposability assumption approximate belief state 
show process decomposes estimated belief state decomposes way matches structure process get significantly better bounds error contraction coefficient regardless true belief state 
far error contraction goes properties true belief state crucial approximate belief state process 
fortunate feasible enforce decomposability properties approximate belief state control true belief state 
formally convenient describe results framework factored hmms section discuss applied dynamic bayesian networks 
assume system composed sev eral subprocesses subprocess state markovian evolution model 
state subprocess time written evolution model stochastic mapping states set processes time state process time 
say subprocess depends subprocess depends value model allows set response variables time depend arbitrarily states processes time interested primarily contraction properties system properties response variables irrelevant current analysis 
considering simple case subprocesses completely independent subprocess depends subprocess assume approximate belief state decomposes lines 
clearly case interesting subprocesses really independent belief state correlated place need approximate 
analysis lays foundation general case described 
theorem independent subprocesses mixing rate min distributions assume marginally independent id id set independent subprocesses contracts approximate belief state de composes lines process contraction process worse contraction individual subprocesses 
subprocess involves smaller number states transition probabilities reasonably large assuming stochastic 
analysis usually results better mixing rate 
consider general case subprocess processes interact 
assume depends subprocesses probability defines transition probability defined transition matrix anterior state spaces different 
luckily section allowed exactly possibility mixing rate defined 
mixing rate min approximate belief state respects process structure place bound mixing rate entire process 
bound depends process structure theorem 
theorem consider system consisting subprocesses assume subprocess depends subprocess influences minimum mixing rate distributions independent id id proof idea illustrate basic construction simple example generalization arbitrary structures straightforward 
consider processes assume depends basic construction follows lines proof theorem split process phases chooses contract second concludes transition way depends process contracted 
note variable plays role transitions subprocesses conditionally independent single decision contract apply context processes 
introduce separate intermediate variables decides contracts context second contracts context decomposition follows xx xy yy transitions partitioned model defined sure parts phase processes marked dotted lines induce behavior corresponding phase processes distributions 
proof theorem equivalence holds pair distributions 
construction essentially identical theorem transition distinguished contraction state taken probability specified 
get equivalence process deal slight subtlety transition behave phase contracted contraction state 
choice contraction state process longer information transition contracts probability process contracts probability order able construct contraction decomposition select satisfy assuming legitimate selection observe 
analyze actual contraction rate process analyze contraction ables initial vari variables intermediate contraction phases smaller 
analysis uses somewhat different process structure shown dashed lines show correctness partition 
processes independent design 
furthermore assumed independent conditions theorem apply contraction process minimum contraction straightforwardly contraction 
contracts loses information original state enter contraction state 
events independent probability occur product 
general subprocess contraction ratio depends subprocesses choose contraction factor influencing process assuming simplicity chosen equal resulting linear reduction contraction rate 
hand influence subprocess subprocess involves construction intermediate variable contracts independently 
total contraction product individual contractions exponential reduction 
putting contraction rate admits bound lower see interconnectivity processes costs terms contraction ratio 
cost incoming connectivity linear reduction contraction rate cost outgoing connectivity exponential 
intuitively phenomenon sense process influences value lost contraction 
theorem stated processes anterior state space identical proof applies general case 
efficient monitoring suggested main applications results task monitoring complex process 
mentioned exact monitoring task intractable requiring maintain large belief state 
proposed alternative approach maintain approximate belief state 
show contraction results allow bound cumulative error arising approximate monitoring process 
investigate particular application approach dbns 
approximate monitoring recall notation section belief states denote compactly represented approximate belief state time choice compact representation depends process 
example process composed number weakly interacting subprocesses cars freeway may reasonable represent belief state time marginal beliefs parts individual vehicles 
hybrid process said may want fixed size mixture gaussians 
approximate belief state updated process propagate transition model obtaining condition current response obtaining usually fall family compactly represented distributions chose restrict belief states 
order maintain feasibility update process approximate typically finding nearby distribution admits compact representation result new approximate belief state freeway domain example may compute new beliefs state vehicle projecting cross product individual belief states approximation resulting distribution closest terms relative entropy continuous process project back space allowable belief states approximating distribution fixed number gaussians 
analyzing error resulting type strategy distance true posterior belief state approximation intuitively error results sources old error inherited previous approximation new error derived approximating suppose approximation error increasing distance exact belief state approximation 
contraction resulting state transitions serves drive closer reducing effect old errors factor various observations move closer expectation averaged different possible responses 
expected error accumulated time behave 
formalize result need quantify error resulting approximation 
new approximate belief state approximation obviously define error approximation id relative entropy distance error measured relative definition definition say approximation incurs error relative id id theorem follows easily induction theorem stochastic process mixing rate assume approximation scheme phase incurs error relative id expectation taken possible response sequences probability ascribed process course trivial show particular approximation scheme satisfy accuracy requirement essentially definition involves true belief usually known 
nev sufficient condition requirement max ln left hand side quantity maximum log relative error caused approximation scheme time easy assess approximation step 
understand guarantees provided theorem 
bound involves relative entropy entire belief states 
certain applications may interested errors individual variables subsets variables 
fortunately bound entire distribution immediately carries projection subset variables 
furthermore note bounds relative entropy immediately imply bounds error bounds expected error error specific sequences evidence weaker 
particular error sequence evidence quite large 
fortunately contraction result holds arbitrary distributions matter far apart 
momentarily different contraction property reduce error exponentially 
ln id monitoring dbns second note consider specific application general approach 
consider process described dbn factored belief state representation certain subsets variables forced independent 
case process specified terms ordered set state ables vari probability model dbn typ ically described tbn time slice temporal bayesian network shown 
tbn associates variable conditional probability distribu tion ip parents parents contain variable time variables time total ordering 
model represents conditional distribution state time variables time set capture idea subprocess partition set disjoint partition satisfy requirement may affected subsets time slice ancestor variable parents note time slice may contain non persistent variables sensor readings may ancestor canonical variable allow depend persistent variable time slice 
clusters various correspond subprocesses theorem shall maintain approximate belief state independent prescribed 
approximate monitoring procedure dbns follows lines general procedure described section point time approximate belief state independent 
propa gate transition model condition result observations time 
compute projecting define entire distribution product factors 
case dbns accomplish update procedure quite efficiently 
generate clique tree contains clique clique contains standard clique tree propagation algorithm compute posterior distribution clique 
done distribution easily extracted appropriate clique 
savings obtained assume stationary dbn static approximation scheme 
order apply generic procedure particular problem define partition canonical variables choose partition process subprocesses 
analysis previous sections evaluate alternatives 
tradeoffs subtle subprocesses small number state variables allow efficient inference 
smaller transition matrix mixing rate better 
hand subprocesses need large edges subprocesses single time slice 
furthermore making subprocesses small increases error incurred approximation assuming independent 
specifically sets variables highly correlated splitting separate subprocesses idea 
experimental results illustrate tradeoffs 
experimental results validated algorithm context real life dbns water network monitoring biological processes water purification plant bat network monitoring freeway traffic see 
added evidence nodes water duplicate state variables xdot xdot stopped stopped slice slice evidence slice slice evidence canonical form dbns experiments bat network water network 
dotted lines indicate clusterings approximations 
added noise 
experimentation methodology follows starting initial prior uniform prior monitor process evolution approximate method fixed decomposition compare exact inference emulated algorithm trivial partition canonical state variables single cluster 
observations simulated sampling evidence variables exact distribution 
shows evolution relative entropy bat network typical run shadowed nodes evidence nodes 
network belief state consists variables roughly partitioned weakly interacting groups choose partition approximation scheme 
ultrasparc approximate monitoring took seconds time slice compared exact inference yielding fold speedup 
terms accuracy error averages remaining low time expect sparsely distributed spikes somewhat larger values peaking 
note appear grow length run predicted analysis 
practical applications emphasis selected variables computed errors beliefs variables belief states marginalized 
qualitative pattern roughly similar respectively averaged remained bounded step run experiment 
similar tests conducted water network shown decomposed belief clusters 
run length error remained bounded exception outlier averaged run 
running times sec slice approximation vs sec slice fold speedup 
investigate effect approximate belief state representation ran experiment different projections clustering introduced clustering obtained breaking clusters see clustering chosen irrespectively network topology 
theoretical tools predict perform compared 
determining factors stepwise approximation error mixing rate bounded directly linked expressiveness approximation criterion decreasing order quality 
sess mixing rates computed vector mixing rates clusters quantities topological calculate theorem 
clustering gave clustering clus tering criterion get heavily penalized higher inter connectivity clusters 
second step compare actual monitoring accuracy algorithm case 
results shown averaged different runs plotted logarithmic scale 
clustering lower curve average error error lower clustering medium curve error averages 
clearly superior clustering average error reaches 
observe errors obtained practice significantly lower predicted theoretical analysis 
qualitatively behaviour different clusterings consistent predictions theory 
particular quality clustering sensitive topology expected 
interestingly accuracy improved conditionally independent approximations 
wa ter network example belief state decomposed overlapping clusters yields sequence observations strictly speaking clusterings contain clusters subprocesses definition edges clusters time slice 
impossible find smaller clusterings bat network satisfy restriction empirical results serve illustrate main points 
exact inference respectively clusterings 
error relative entropy bat network clustering epoch time slice error relative entropy clustering clustering clustering epoch time slice experimental results relative entropy error typical run bat network comparison relative entropy error different approximate belief state representations bat network curves averaged runs 
average error just 
error remains bounded reducing maximum error factor 
approximate inference took sec slice fold speedup exact inference 
tested effect evidence quality approximation 
evidence error curve smooth converges relatively soon constant error corresponding error correct stationary distribution process approximation 
evidence error curve higher variance average typically lower indicating evidence boosts contraction opposed merely harmless 
effect evidence beneficial better clusterings 
extensions investigated effect approximate inference stochastic process 
showed stochastic nature process tends errors resulting approximation disappear rapidly process continues evolve 
analysis relies novel important theorem proving contraction result relative entropy stochastic processes 
provided refined analysis processes composed weakly interacting subprocesses 
applied idea approximate inference monitoring task 
exact inference task infeasible complex stochastic processes large number states 
process highly structured exact inference forced intractability full correlation belief state invariably occurs 
propose approach algorithm maintains approximate belief state compact representation 
belief state propagated forward time slice result approximated maintain compact representation 
analysis guarantees errors approximations accumulate 
approach provides general framework approximate inference stochastic processes 
instantiation general framework class structured processes composed weakly interacting subprocesses 
case provide refined analysis results significantly lower error bounds 
experimental results showing approach works extremely real life processes 
get order magnitude savings small processes cost low error approximation 
larger processes expect savings greater 
theoretical empirical results clearly show ability execute accurate effective inference stochastic process depends heavily structure 
result show structure stochastic process exploited inference 
fairly little approximate inference complex temporal models 
early considers simple approach domain knowledge simply eliminate variables time slice 
random sampling approach viewed maintaining approximate belief state albeit represented naively set weighted samples 
extends idea random samples time slice data order learn approximate belief state 
ideas viewed falling framework described 
contains analysis explicit connection structure process 
utilize mean field approximation context various types structured hmms 
approaches closest part dealing structured processes 
approach compound process approximated composed independent subprocesses parameters chosen way depends evidence 
approach differs ways 
applies situations subprocesses fact independent correlated due observations 
applies richer models partially decomposed systems 
second approximation scheme making subprocesses independent length approach accounts correlations subprocesses time slice 
analysis proves approximation closest possible true distribution class distributions represented independent processes analysis contrast provides explicit bounds expectation total error 
approximate inference algorithms nontemporal bayesian networks applied task 
specifically mini bucket approach somewhat related uses different form factoring decomposition course inference 
potentially relevant algorithms associated performance guarantees error approximation time 
important direction results extended relates different representations belief state process 
current analysis requires belief state representation states subprocesses completely independent 
clearly situations refined belief state representation appropriate 
example experiments show beneficial states processes approximate belief state conditionally independent third derive formal conditions advantage 
results demonstrate representations belief state useful specifically ones allow structure distribution 
show beneficial allow structure belief state representation vary time making appropriate specific properties current belief state 
general basic contraction result applies approximation scheme belief state representation intend experiment different alternatives evaluate contraction properties 
apply analysis types processes requiring compact representation belief state processes continuous state space 
priority improve analysis effect evidence 
current results show evidence hurt 
experiments show expect evidence significantly reduce error approximation 
tasks techniques applied 
example analysis applies task predicting evolution system 
applies tasks transition model depends action taken agent 
contraction analysis done phase phase basis require transition model time slice 
long transition associated action sufficiently stochastic technique predict effects plan policy 
speculative idea task constructing policies pomdps policy mapping belief states actions 
compact belief state representation turn better basis representing policy 
hope generalize contraction result case reasoning backwards time allowing apply approximate inference task computing beliefs time slice past evidence 
inference task crucial component algorithms learn probabilistic models stochastic processes data 
learning process requires iterations inference task improvement efficiency major impact 
furthermore showing influence approximations significantly affect beliefs past obtain theoretical support simple natural algorithm computes beliefs time slice fairly small window sides 
applications idea online learning clear 
gratefully acknowledge eric bauer lise getoor uri lerner software experi ments help network files tim huang providing bat network 
tom cover nir friedman leonid alex uri lerner stuart russell useful discussions comments 
research supported aro muri program integrated approach intelligent systems number daah darpa contract subcontract information extraction transport generosity powell foundation sloan foundation 
astr optimal control markov decision processes incomplete state estimation 
math 
anal 
applic 
cover thomas 
elements information theory 
wiley 
dagum horwitz 
dynamic network models forecasting 
proc 
uai 
dean kanazawa 
model reasoning persistence causation 
comp 
int 
dechter rish 
scheme approximating probabilistic inference 
proc 
uai 
forbes huang kanazawa russell 
bayesian automated taxi 
proc 
ijcai 
ghahramani jordan 
factorial hidden markov models 
proc 
nips 
jensen kj olesen pedersen 
expert system control waste water treatment pilot project 
technical report aalborg 
danish 
kalman 
new approach linear filtering prediction problems 
basic engineering 
kanazawa koller russell 
stochastic simulation algorithms dynamic probabilistic networks 
proc 
uai 
kj 
computational scheme reasoning dynamic probabilistic networks 
proc 
uai 
koller 
learning approximation stochastic processes 
proc 
ml 
lauritzen spiegelhalter 
local computations probabilities graphical structures application expert systems 
roy 
stat 
soc 
provan 
tradeoffs constructing evaluating temporal influence diagrams 
proc 
uai 
rabiner juang 
hidden markov models 
ieee acoustics speech signal processing 
saul jordan 
exploiting tractable substructure intractable networks 
proc 
nips 
smyth heckerman jordan 
probabilistic independence networks hidden markov probability models 
neural computation 
