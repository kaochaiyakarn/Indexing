algorithms non negative matrix factorization daniel lee bell laboratories lucent technologies murray hill nj sebastian seung dept brain cog 
sci 
massachusetts institute technology cambridge ma non negative matrix factorization nmf previously shown useful decomposition multivariate data 
different multiplicative algorithms nmf analyzed 
differ slightly multiplicative factor update rules 
algorithm shown minimize conventional squares error minimizes generalized kullback leibler divergence 
monotonic convergence algorithms proven auxiliary function analogous proving convergence expectation maximization algorithm 
algorithms interpreted diagonally rescaled gradient descent rescaling factor optimally chosen ensure convergence 
unsupervised learning algorithms principal components analysis vector quantization understood factorizing data matrix subject different constraints 
depending constraints utilized resulting factors shown different representational properties 
principal components analysis enforces weak orthogonality constraint resulting distributed representation uses cancellations generate variability 
hand vector quantization uses hard constraint results clustering data mutually exclusive prototypes 
previously shown nonnegativity useful constraint matrix factorization learn parts representation data 
nonnegative basis vectors learned distributed sparse combinations generate expressiveness reconstructions 
submission analyze detail numerical algorithms learning optimal nonnegative factors data 
non negative matrix factorization formally consider algorithms solving problem non negative matrix factorization nmf non negative matrix find non negative matrix factors multivariate nmf applied statistical analysis multivariate data manner 
set dimensional data vectors vectors placed columns matrix usually number examples data set 
matrix approximately factorized matrix matrix chosen smaller smaller original matrix results compressed version original data matrix 
significance approximation eq 

rewritten column column corresponding columns vector words data approximated linear combination columns weighted components 
regarded containing basis optimized linear approximation data relatively basis vectors represent data vectors approximation achieved basis vectors discover structure latent data 
submission applications nmf focuses technical aspects finding non negative matrix factorizations 
course types matrix factorizations extensively studied numerical linear algebra nonnegativity constraint previous inapplicable case 
discuss algorithms nmf iterative updates algorithms easy implement convergence properties guaranteed useful practical applications 
algorithms may possibly efficient computation time difficult implement may generalize different cost functions 
algorithms similar factors adapted previously deconvolution emission tomography astronomical images 
iteration algorithms new value multiplying current value factor depends quality approximation eq 

prove quality approximation improves monotonically application multiplicative update rules 
practice means repeated iteration update rules guaranteed converge locally optimal matrix factorization 
cost functions find approximate factorization need define cost functions quantify quality approximation 
cost function constructed measure distance matrices non negative useful measure simply square euclidean distance lower bounded zero clearly vanishes useful measure euclidean distance lower bounded zero vanishes called distance symmetric refer divergence reduces kullback leibler divergence relative entropy regarded normalized probability distributions 
consider alternative formulations nmf optimization problems problem minimize minimize problem respect subject constraints respect subject constraints convex functions convex variables 
unrealistic expect algorithm solve problems sense finding global minima 
techniques numerical optimization applied find local minima 
gradient descent simplest technique implement convergence slow 
methods conjugate gradient faster convergence vicinity local minima complicated implement gradient descent 
convergence gradient methods disadvantage sensitive choice step size inconvenient large applications 
multiplicative update rules distance multiplicative update rules compromise speed ease implementation solving problems 
theorem euclidean nonincreasing update rules euclidean distance invariant updates stationary point distance 
theorem divergence nonincreasing update rules divergence invariant updates stationary point divergence 
proofs theorems section 
note update consists multiplication factor 
particular straightforward see multiplicative factor unity perfect reconstruction necessarily fixed point update rules 
multiplicative versus additive update rules useful contrast multiplicative updates arising gradient descent 
particular simple additive update written reduces squared distance set equal small positive number equivalent conventional gradient descent 
long number sufficiently small update reduce 
diagonally rescale variables set obtain update rule theorem 
note rescaling results multiplicative factor positive component gradient denominator absolute value negative component numerator factor 
divergence diagonally rescaled gradient descent takes form reduce set small positive update obtain update rule theorem 
rescaling multiplicative rule positive component gradient denominator negative component numerator multiplicative factor 
choices small may guarantee rescaled gradient descent cause cost function decrease 
surprisingly case shown section 
proofs convergence prove theorems auxiliary function similar expectation maximization algorithm :10.1.1.11.5851
definition auxiliary function conditions satisfied 
auxiliary function useful concept lemma graphically illustrated fig 

lemma auxiliary function nonincreasing update proof note 
local minimum derivatives exist continuous small neighborhood implies iterating update eq 
obtain sequence estimates converge minimum local derivatives function objective show defining appropriate auxiliary functions update rules theorems easily follow eq 

min minimizing auxiliary function lemma 
diagonal matrix auxiliary function proof compare guarantees obvious need show eq 
find equivalent 
prove positive consider matrix just rescaling components semidefinite positive show positive semidefinite considering matrix positive eigenvector unity eigenvalue application frobenius perron theorem shows eq 
holds 
demonstrate convergence theorem proof theorem replacing eq 
eq 
results update rule eq 
auxiliary function nonincreasing update rule lemma 
writing components equation explicitly obtain reversing roles lemma similarly shown nonincreasing update rules consider auxiliary function divergence cost function lemma define auxiliary function proof straightforward verify convexity log function derive inequality holds nonnegative sum unity 
setting obtain inequality follows 
theorem follows application lemma proof theorem minimum gradient zero update rule eq 
takes form show respect determined setting auxiliary function eq 
nonincreasing update 
rewritten matrix form equivalent update rule eq 

reversing roles update rule similarly shown nonincreasing 
discussion shown application update rules eqs 
guaranteed find locally optimal solutions problems respectively 
convergence proofs rely defining appropriate auxiliary function 
currently working generalize theorems complex constraints 
update rules extremely easy implement computationally hopefully utilized wide variety applications 
acknowledge support bell laboratories 
carlos ken clarkson corinna cortes roland freund linda kaufman yann le cun sam roweis larry saul margaret wright helpful discussions 
jolliffe 
principal component analysis 
new york springer verlag 
turk pentland 
eigenfaces recognition 
cogn 
neurosci 

gersho gray rm 
vector quantization signal compression 
kluwer acad 
press 
lee dd seung hs 
unsupervised learning convex conic coding 
proceedings conference neural information processing systems 
lee dd seung hs 
learning parts objects non negative matrix factorization 
nature 
field dj 
goal sensory coding 
neural comput 

young 
sparse coding primate cortex 
handbook brain theory neural networks 
mit press cambridge ma 
press wh teukolsky sa vetterling wt flannery bp 
numerical recipes art scientific computing 
cambridge university press cambridge england 
shepp la vardi 
maximum likelihood reconstruction emission tomography 
ieee trans 
mi 
richardson wh 
bayesian iterative method image restoration 
opt 
soc 
am 

lucy lb 
iterative technique rectification observed distributions 


bouman ca sauer 
unified approach statistical tomography coordinate descent optimization 
ieee trans 
image proc 


squares formulation robust non negative factor analysis 

intell 
lab 

kivinen warmuth 
additive versus exponentiated gradient updates linear prediction 
journal information computation 
dempster ap laird nm rubin db 
maximum likelihood incomplete data em algorithm 
royal stat 
soc 

saul pereira 
aggregate mixed order markov models statistical language processing 
cardie weischedel eds 
proceedings second conference empirical methods natural language processing 
acl press 
