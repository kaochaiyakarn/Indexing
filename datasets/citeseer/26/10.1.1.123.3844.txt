proc 
ieee signal processing society workshop neural networks signal processing pp cambridge uk 
url www dcs shef ac uk miguel papers html experimental evaluation latent variable models dimensionality reduction miguel 
carreira steve renals dept computer science university sheffield sheffield dp uk carreira renals dcs shef ac uk epg data test bed dimensionality reduction methods latent variable modelling underlying lower dimension representation inferred directly data 
models mixtures investigated including factor analysis generative topographic mapping gtm 
experiments indicate nonlinear latent variable modelling reveals low dimensional structure data inaccessible investigated linear models 
latent variable modelling low dimensional generative model estimated data sample 
smooth mapping links low dimensional representation high dimensional data dimensionality reduction achieved bayes theorem 
latent variable models include factor analysis principal component analysis generative topographic mapping gtm 
apply latent variable framework data 
technique epg established relatively noninvasive conceptually simple easy tool investigation lingual activity normal pathological speech 
qualitative quantitative data patterns lingual contacts hard continuous speech may obtained epg technique studies descriptive phonetics coarticulation diagnosis treatment disordered speech 
typically subject wears artificial fit page representative typical stable phase different phonemes 
dimensional binary epg vector pictured rowwise top bottom resembling human top bottom 
upper number electrodes mounted surface detect lingual contact reading epg system 
epg signal sampled frequency hz utterance sequence raw epg data consists stream binary vectors spatial temporal structure due constraints articulatory system 
note epg signal incomplete articulatory description omitting details 
mapping phonemes epg patterns certain phonemes produce epg patterns fig 

number studies suggest tongue movements speech may appropriately modelled elementary articulatory parameters 
nguyen 
consider dimensionality reduction spatial level 
compare ability known latent variable models mixture models extract relevant structure epg data set adaptive fashion 
contrasts number widespread epg data reduction techniques priori knowledge typically form fixed linear combinations epg vector components 
ad hoc methods robust perform situations speech deviates standard impaired speakers different speech styles unusual accents 
show nonlinearity advantageous real world problem 
generative modelling latent variables latent variable modelling assumption observed high dimensional data generated underlying low dimensional process defined small number latent variables 
latent variables mapped fixed transformation dimensional data space measurement procedure noise added stochastic variation 
aim learn low dimensional generating process noise model directly learning dimensionality reducing mapping 
latent variable model specified prior latent space smooth mapping latent space data space noise model data space fig 

elements equipped parameters collectively call 
integrating joint probability density function latent space gives marginal distribution data space 
observed sample data space tn dimensional real vectors generated unknown distribution parameter estimate maximising log likelihood page psfrag replacements prior latent space dimension induced manifold data space dimension schematic latent variable model data space latent space 
parameters log tn typically em algorithm 
parameters fixed bayes theorem gives posterior distribution latent space data vector distribution probability point latent space responsible generating 
dx summarising distribution single latent space point typically mean mode results reduced dimension representative defines corresponding dimensionality reducing mapping data space latent space successful posterior distribution unimodal sharply peaked 
applying mapping representative obtain reconstructed data vector 
usual way define squared reconstruction error sample euclidean norm 
consider latent variable models em algorithms available factor analysis rubin thayer mapping linear prior latent space unit gaussian noise model diagonal gaussian 
marginal data space gaussian constrained covariance matrix 
principal component analysis pca considered particular case factor analysis isotropic gaussian noise model tipping bishop 
generative topographic mapping gtm bishop nonlinear latent variable model mapping generalised linear model prior latent space discrete uniform noise model isotropic page gaussian :10.1.1.130.110
marginal data space constrained mixture gaussians 
finite mixtures latent variable models may constructed em algorithm obtain maximum likelihood parameter estimate everitt hand 
reduced dimension representative obtained mixture component highest responsibility average component representatives weighted responsibilities 
consider mixtures multivariate bernoulli distributions component parameterised dimensional probability vector note define latent space discrete collection components 
experiments subset eur database marchal consisting bit epg frames sampled hz different utterances english speaker 
data set split training test set 
data unlabelled 
training set maximum likelihood estimates probability models factor analysis pca gtm mixtures factor analysers mixtures multivariate bernoulli distributions 
shows prototypes methods 
gives log likelihood reconstruction error curves method training test sets 
performing experiments data sets repeated epg frames removed change significantly shape curves 
factor analysis principal component analysis performed epg data followed rotation improve factor interpretability altering model 
dimensional factor loadings vector principal component vector may considered dimensionality reduction index sense projecting epg frame vector equivalent linear combination components 
resultant basis vectors shown rows fig 
th order models 
factors associated known epg data reduction indices linear combinations velar index alveolar 
note derived factors indicate adaptive structure asymmetry priori derived indices capture 
principal components similar factor loading vectors due fact data set matrix relatively isotropic multiple identity case factor analysis equivalent pca 
probabilistic pca model tipping bishop compute log likelihood curves fig 

pca outperforms factor analysis reconstruction error factor analysis outperforms pca log likelihood 
terms generative modelling factor analysis superior pca terms reconstruction error pca better linear method 
generative topographic mapping gtm factor analysis pca extract linear structure data 
factor analysis null hypothesis data sample explained factors rejected values page psfrag replacements psfrag replacements psfrag replacements psfrag replacements prototypes factors methods 
dimensional vector pictured epg frames fig 
pixel represented rectangle area proportional magnitude pixel value colour black positive values white negative ones 
row factors rotation 
row principal components rotation 
row means factor loadings mixture factor analysers 
row prototypes mixture multivariate bernoulli distributions 
significance level 
em algorithm bishop trained dimensional gtm model parameters grid dimensional latent space points scaled square grid square gaussian basis functions width equal separation basis functions centres varied :10.1.1.130.110
data point took reduced dimension representative mode posterior distribution unimodal sharply peaked data points 
log likelihood curve test set function number basis functions shows maximum indicating overfitting occurs observe reconstruction error keeps decreasing steadily past limit 
comparison methods shows gtm latent space dimensions outperforms methods log likelihood reconstruction error wide range latent space dimensions 
pca needs principal components attain reconstruction error gtm basis functions components surpass page log likelihood psfrag replacements psfrag replacements reconstruction error log likelihood pca fa mfa mfa gtm fa pca psfrag replacements mb log likelihood number factors components psfrag replacements mfa number factors components gtm log likelihood mb fa pca pca fa gtm mfa reconstruction error fa pca mfa mfa number factors components mfa gtm pca fa mb number factors components mb gtm fa pca comparison methods terms log likelihood top reconstruction error bottom left training set right test set factor analysis fa principal component analysis pca generative topographic mapping gtm mixtures factor analysers mfa mixtures multivariate bernoulli distributions mb 
note axis refers order factor analysis principal component analysis number mixture components case mixture models square root number basis functions case gtm 
log likelihood 
mixtures factor analysers factor factor analyser estimated em algorithm ghahramani hinton random starting points 
fig 
row shows loading vectors means mixture components 
effect model place factors different regions data space factors coincide factors factor analysis linear combinations means coincide typical epg patterns fig 

training set log likelihood reconstruction error mixture better factor analysis improvement test set 
log likelihood space number local maxima different log likelihood value explains appearance curves point corresponds single estimate average kind mixture number log likelihood singularities parameter space cases em failed converge proper solution 
page psfrag replacements factor factor psfrag replacements factor factor dimensional plot trajectory highlighted utterance fragment prefer kant hobbes book phonemic transcription left factor analysis latent space factors 
right gtm basis functions latent space grid points numbered 
start points marked respectively 
phonemes 
estimates 
mixtures multivariate bernoulli distributions estimated em algorithm everitt hand random starting points 
fig 
row shows prototypes pm component mixture 
note pmd value readily interpretable epg vector loading vectors positive negative values 
prototypes coincide typical epg patterns fig 

log likelihood value methods reconstruction error greater 
reason component mixture lacks latent space reconstruct data vector prototype pm 
dimensional visualisation represent graphically sequence epg frames sample utterance plotting projections twodimensional latent space joining consecutive points line 
shows representation factor analysis factors left gtm right 
linear projections ones provided factor analysis pca finding best projection sense nongaussian possible called projection pursuit 
criterion optimised factor analysis general method aim 
dimensional latent space produced gtm gives qualitatively better visualisation articulatory space dimensional linear projections 
discussion latent variable models useful relatively high dimensional information contained epg sequence way easier understand page handle 
significant advantage general probability models mixtures suitable classification clustering dimensionality reduction 
finite mixtures offer possibility fitting soft way different models different data space regions offer potential model complex data 
training slow difficult due log likelihood surface plagued singularities 
cases studied overfitting log likelihood appear number parameters model large reconstruction error presents steady decrease training test sets number parameters 
unidentified models different nontrivial combinations parameter values produce exactly distribution produce different estimates data set 
case factor analysis pca importance mixtures multivariate bernoulli distributions carreira renals may pose problems interpretation models 
gtm proven best method log likelihood reconstruction error despite limited dimensional space due computational complexity 
suggests intrinsic dimensionality epg data may substantially smaller suggested studies 
nguyen 
methods studied require knowledge latent space dimension number mixture components possible way determine optimal ones model selection 
acknowledgments supported esprit long term research project scholarship spanish ministry education science award foundation 
authors acknowledge markus svens zoubin ghahramani matlab implementations gtm mixtures factor analysers respectively alan wrench providing data 
matlab implementations principal component analysis factor analysis rotation mixtures multivariate bernoulli distributions available author url www dcs shef ac uk miguel 


latent variable models factor analysis 
charles griffin london 
bishop svens williams 

gtm generative topographic mapping 
neural computation 
carreira 
renals 

finite mixtures multivariate bernoulli distributions 
submitted neural computation 
everitt hand 

finite mixture distributions 
monographs statistics applied probability 
chapman hall london new york 
page ghahramani hinton 

em algorithm mixtures factor analyzers 
technical report crg tr university toronto 
gibbon jones 

visual display contact assessment remediation speech disorders 

disorders communication 
gibbon 

epg data reduction methods implications studies lingual coarticulation 
phonetics 
marchal 

instrumentation database cross language study coarticulation 
language speech 
nguyen marchal content 

modeling tongue contact patterns production speech 
phonetics 
rubin thayer 

em algorithms ml factor analysis 
psychometrika 
tipping bishop 

mixtures principal component analysers 
proceedings iee fifth international conference artificial neural networks 
london iee 
page 
