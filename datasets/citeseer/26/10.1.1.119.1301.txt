machine learning inductive logic programming multi agent systems kazakov daniel kudenko 
department computer science university york york yo dd united kingdom lastname cs york ac uk years multi agent systems mas received increasing attention artificial intelligence community 
research multi agent systems involves investigation autonomous rational flexible behavior entities software programs robots interaction coordination diverse areas robotics information retrieval management simulation 
designing agent systems infeasible foresee potential situations agent may encounter specify agent behavior optimally advance 
order overcome design problems agents learn adapt environment 
task optimally designing agents complex nature source uncertainty agent situated environment contains agents potentially different capabilities goals beliefs 
multi agent learning ability agents learn cooperate compete crucial domains 
overview important issues concerning application machine learning agents multi agent systems hopefully stimulate thoughts 
point selected relevant discussions goal provide exhaustive literature review 
broader review past research refer reader 
part discuss ml mas issues general viewpoint 
second part focus inductive logic programming ilp logic learning technique able exploit domain knowledge 
organized follows general overview state art research machine learning ml focused mainly learning independent reasoning process discuss integration methods complete situated agent system 
section highlight issues important ml multi agent setting agent heterogeneity awareness agents communication distribution learning tasks 
number issues arising alphabetical order application ilp agents multi agent systems example application 
finish summary outlook interesting open research questions area multi agent learning 
principles machine learning section briefly introduce research area machine learning ml classifications ml algorithms 
book mitchell defines machine learning study computer programs improve experience performance certain class tasks measured performance measure 
detail learning system aims determining description concept set concept examples provided teacher background knowledge 
concept usually interpreted subset objects observations defined larger set universe alternatively boolean valued function defined larger set 
background knowledge set concepts statements known true universe 
target concept concept learned finite number members supplied learning algorithm learning task reduced examples concept 
case learning aim concise representation concept 
examples called training examples learning 
learning incomplete set examples called inductive learning 
inductive learning assumption known inductive learning hypothesis hypothesis approximate target concept sufficiently large set training examples approximate target concept unobserved examples 
concept definition generated inductive learning evaluated set test examples 
training examples belong test set 
general categories ml algorithms black box methods white box knowledge oriented ones 
black box methods create concept representation interpretation user unclear impossible 
way algorithm able instance correctly classify unseen instances learned target concept possible explain behavior 
hand knowledge oriented algorithms create symbolic knowledge structures comprehensible 
type examples provided learning algorithm may learn positive negative examples concept birds non birds examples concepts birds flowers bees positive examples concept counter examples donald duck birds 
cases aim learning able distinguish different concepts concept complement case result description concept discrimination ones 
words learning positive examples may result compact representation target concept statements target concept true necessarily sufficient determine class membership unseen examples 
bayesian framework proven logic programs learnable arbitrarily low expected error positive examples 
upper bound expected error learner maximises bayes posterior probability learning positive examples small additive term mixture positive negative examples 
cases enumerated belong supervised learning paradigm examples annotated corresponding concept 
alternatively unsupervised learning data provided definition language represent 
additional information supplied 
language bias specifying certain order preferences possible representations data 
possible descriptions input data highest score regard bias chosen 
general ml algorithms look somewhat general description target concept mere list training examples 
exact representation concept learned depends type language description 
depends bias learning 
instance representation concept usually expected compact list examples provided 
context principle known occam razor quoted 
statement william occam ca 
non entities multiplied necessity came interpreted mean prefer simplest hypothesis fits data 
principle widely language bias ml minimal description length mdl principle 
encoding recommends choosing hypothesis minimises sum description length hypothesis description length data hypothesis 
important issue machine learning way algorithm handles misclassified examples noise data 
single example incorrectly classified exact learning concept impossible result description incomplete cover positive examples inconsistent covers negative ones 
dichotomies taxonomy ml algorithms introduced way examples learning 
batch algorithms need process learning examples examples available learning phase repeated scratch 
incremental algorithms hand able consider additional examples come 
eager learning algorithm generates hypothesis target predicate advance lazy learner decision test example 
broadly speaking distinction eager lazy learning related ability lazy learners generate local approximations target hypothesis test example eager learners commit single hypothesis training time 
traditional ml separates learning acquisition data independent processes 
brings processes closer 
active learning extends standard supervised learner requiring expensive annotated data module browsing larger unannotated dataset selecting annotation examples considered beneficial learning process 
closed loop machine learning couples ml robot carrying experiments 
way learning algorithm suggest hypothesis verify experimentally hypothesis rejected collected data give rise new round cycle 
machine learning single agents past ml research focused non agent disembodied learning algorithms account learning algorithm may embedded agent situated environment 
agent consists different interacting modules vision planning learning module just 
learning support supported wide range modules 
section discuss issues arising integrating ml algorithms agent system focusing questions learning targets respective training data agents 
applicable learning techniques 
ml system integrated agent architecture 
learning targets training data simplest model agent describes entity receives sensory input environment input internal reasoning acts environment 
model define learning target agent simply decision procedure choosing actions 
deliberately omit notion choosing optimal actions definition question optimal means depends source training data bias learner 
distinguish types data sources external teacher external teacher provides examples actions action sequences corresponding classification indicating optimality appropriateness 
model equivalent fully supervised learning 
environmental feedback agent acts receives feedback environment indicating benefit action 
feedback usually defined terms utility current state agent finds 
training model corresponds reinforcement learning 
noted necessarily states result feedback 
means environmental feedback received propagated actions potentially contributed 
actions contributed current state extent 
certainly actions contributed strongly receive recognition blame 
credit assignment problem unsolved 
common technique distribute rewards punishments actions reward punish actions higher discount factor 
see section details 
internal agent bias agent exploring environment actions looks useful patterns interesting properties environment enable agent generate concepts describing environment 
usefulness interestingness purely agent internal bias minimum description length occam razor explicit feedback agent 
assumed discovered concepts help agent perform specific goals efficiently effectively 
learning model usually denoted term unsupervised learning 
focus learning models 
relatively little unsupervised learning information see example 
section ways apply supervised learning method inductive logic programming agent context 
section discuss reinforcement learning techniques specifically focus widely agent learning algorithm learning 
reinforcement learning noted reinforcement learning rl agent adapts decision process environmental feedback resulting choice actions 
section briefly describe widely rl technique learning discuss ml algorithms rl context 
complete treatment reinforcement learning particular learning refer reader 
learning learning agent decision procedure specified policy ss maps states actions 
environmental feedback defined reward function maps states numerical rewards 
goal learning compute optimal policy ss lambda maximizes reward agent receives 
st state agent reaches time point policy ss guide actions initial state 
reward policy ss state st denoted ss st 
reward measured ways discounted cumulative reward measures cumulative rewards agent get starting state st policy ss 
rewards discounted discount parameter fl 
smaller fl agent prefers short term long term rewards 
ss st flir st finite horizon reward modification discounted cumulative reward takes finite number states account 
non discounted version ss st hx st average reward function measures average reward agent receives policy ss ss st lim hx st functions may uses learning discounted cumulative reward 
note function defines ways actions receive portion rewards 
case discounted cumulative reward actions receive proportion reward actions size proportion defined discount factor fl 
optimal policy ss lambda formally defined ss lambda argmax ss optimal policy learned associating action direct rewards received resulting state adding estimate maximal reward achieved 
estimate agent past experience 
state choosing action result estimated discounted cumulative reward qe qe fl max state resulting executing action state correct values optimal policy computed follows ss lambda argmax aq agent updates estimates function executing actions observing rewards resulting states 
table shows update algorithm 
table 
learning algorithm procedure learn discount factor state action pairs initial state repeat forever select action execute new state maximum actions learning algorithm proven converge correct values learn optimal policy condition reward values image upper bound 
convergence take iterations 
fact order guarantee optimal results possible state action pairs visited infinitely 
obviously may lead problems domains continuous actions 
problems addressed select actions main loop learning algorithm 
agent chooses seemingly optimal action get stuck local optimum 
better choose random actions higher probability early training stages lower probability stages value estimates converged true values 
storing values table may quickly lead memory space problems function approximation neural net 
approach may lead complex update mechanism 
learning efficient highly suitable learning method learning agents coupling exploration learning number disadvantages technique applicable reactive agents agents base action choice current state 
defining numerical reward function non trivial task application domains 
mentioned convergence may take long time 
learning result transparent sense explanation action preferences 
state actions ranked may yield limited insight performance agent 
ml algorithms reinforcement learning learning popular reinforcement learning technique reason learning algorithms decision tree learning inductive logic programming ilp rl context 
zeroski explore combination relational regression tree learning learning 
result expressive general function representation may applicable goals environment agent change 
approach dietterich flann combine explanation learning form learning focused speeding reasoning processes reinforcement learning 
section discuss ways combining ilp reinforcement learning 
integrating machine learning agent architecture section showed machine learning algorithms proactive 
probably latest stage development standard ml active exploration world 
specific case learning agent learning sole goal embodied agent robot 
denying ml privileged status making just ways improving agent performance tasks brings new range issues discussed 
simulated system agents assumed general case embodied agents acting real world just instance 
time constraints learning machine learning algorithms traditionally judged predictive accuracy theories learned time complexity considered secondary factor price pay low adds attractiveness algorithm 
time complexity may learning large data set infeasible value theory learned minute learned day exactly long accuracy 
situation changes machine learning framework agents 
activities perception planning control effectors compete learning resources agent 
allocation policy chosen obviously great importance 
time constraints imposed learning depend learning task 
eager learning provide theory past experience compact better generalisation power needed time learning algorithm run ideally time agent idle computational resources 
lazy learning training data past observations provide interpretation classification test example new observation come time available limited 
correct recognition danger done normal functioning agent compromised danger 
factor feasibility machine learning agents algorithm clear halting condition representative called time learning general potential improving current hypothesis running algorithm longer 
systems exhaustive search hypothesis space belong class sample genetic algorithms 
agent operate rigorous time constraints real time worst case time complexity analysis algorithm determine start learning 
time learning able implement decision procedure halt learning order meet hard deadlines cost outweighs benefits improved accuracy 
general big gap reaction times required realtime systems rts time complexity machine learning 
situation compared area speech recognition sr natural language processing nlp 
cases deal tasks inherently interrelated goal 
understanding speech improved parsing semantic analysis learning agent potentially outperform uses hard wired behavior 
different hardware application requirements acceptable nlp tasks carried line speech recognition essentially line task kept sr separate rest nlp mid shelf hardware software platforms tailor applications meet requirements areas 
expect development intersection machine learning real time systems search greater efficiency conceptual shift dealing hard time constraints frameworks allowing flexibility imprecise model computation flexible real time systems 
clear process mutual approaching ml rts 
integration ml agents easier simulations time constraints ml gradually taken account 
instance discriminate cases unlimited time learning upper bound time learning learning real time synchronisation agents actions simulated environment learning agents studied choose method inform agent current state world prompt choose action carry action change environment accordingly action possible 
consider methods step time simultaneous update environment agents prompted choose moves 
selected action environment carries actions simultaneously 
method able deal conflicting actions 
step time immediate update agent action carried immediately generation 
order agents take turns important 
agents asynchronous processes best model reality 
implementation requires hardware resources separate computers processors complex software implementation 
choice type time constraints time synchronisation individual agents actions schematically represented table 
table 
synchronisation theta time constraints unlimited upper bound real time time move round logic mas batch update conflict simulations move round york multi agent immediate update environment asynchronous multi agent progol learning recall time sensory information world updated agent able learn choose recall existing model world decide action learn outcome immediately update current model world postpone learning 
lazy learning distinction learning recall disappears new theory limited coverage built classify new example carried forward new observation theories 
case eager learning learning recall separate processes coordination discussed 
lazy learning simplest forms case reasoning requires search seen cases find closest match new analysed classified 
suitable indexing hash tables reduce look times applicability lazy learning ultimately bounded limited generalisation power fact time complexity increases number examples 
eager learning help obtain theories coverage far guaranteed lazy learning 
instance explanation learning ebl potentially able learn theory single example 
hand tasks lazy learning guarantees best results 
recall times issue eager learning compress training examples previous observations concise theory hope result faster recall 
different ways combining learning recall summarised parallel learning recall times learning recall possible course actions 
learning recall separate modes thing time agent separate modes 
active uses existing knowledge choose actions update observations collected processing 
second mode activity performed agent learning processes examples accumulated active phase incorporate model world 
examples disposed learning incremental kept agent uses batch learning starts scratch time invoked 
context agent learning incremental learning better suited lesser computational memory requirements 
cheap fly learning line computationally expensive learning amount raw information agent collects sensors considerable may lead infeasible memory requirements separate modes learning recall 
cases agent employ limited fly learning reduce strain memory 
instance reduce dimensionality object language introducing new set attributes necessary cluster examples replace cluster single representative ignore uninteresting examples altogether 
line learning applied time previous case 
tempted recall case human learning knowledge apparently updated periods process continues sleep phase crucial acquisition knowledge long term 
machine learning multi agent systems moves single agent setting environment agents acting potentially interfering acting optimally consequently learning act optimally highly complex task 
applying learning techniques multi agent systems issues taken account 
sections discuss questions arise context point relevant literature impact awareness agents behavior 
importance communication influence learning process 
learning influence team heterogeneity 
distributed learning differ multi agent learning approaches 
awareness agents multi agent system generally defined collection agents observe act environment 
important stress imply social awareness awareness agents environment knowledge behavior 
may useful increased social awareness necessary order achieve near optimal performance 
steels describes application domain task group robots collect rock samples return central storage facility 
near optimal solution defines small set simple reactive rules robots require explicit social interactions 
vidal durfee define levels social awareness follows level agents knowledge agents actions observe changes environment 
level agents recognize agents knowledge behavior 
agents model agents level agents 
level agents knowledge behavior agents past observations 
model agents include influence knowledge agent level agent models agents level agents 
definitions continued higher levels ad infinitum agent knows agent knows knows knows noted level agents able learn implicitly agents long able observe results actions agents environment 
level agents explicit social awareness incorporate agents behavior learned hypothesis behavior environment 
multi agent learning principle starts level agents 
sen studied performance learning agents level agent single stage games 
interestingly results show level agents display slowest effective learning worse level agents 
general results show diversity agents beneficial myopic agents outperform non myopic agents 
results interesting restricted specific domain need research issues order able define general principles 
communication learning eyes linguist communication learning inherently related language concepts processes 
difference admittedly limited amount implicit social interaction involved robots drop rock samples robots decision process 
robots needs knowledge actions robots treats rock samples way dropped robots 
usually knowledge skills clearly formulated conveyed means language second acquired personal experience theory learned put words 
ml terminology say learning knowledge defined context corresponds white box learning learning skills black box learning 
noted exchange parts neural networks usually associated black box learning require agents understand transmitted exchange knowledge skills 
known fact human experts reluctant formulate general rules base decisions 
hand find easier motivate decision particular case studied 
quotation architect frank lloyd wright help point expert think anymore knows 
descriptions match definitions various forms lazy learning 
learning single agent reason choose learning method quality results 
situation changes move single agent learning societies learning agents communicate 
agent cost asking information knowledge experienced agent usually lower cost involved acquiring information exploring environment purely observing actions agents 
information expressed formalism language shared agents 
words results white box learning shared agents 
consider case communication channel limited bandwidth 
lazy learning may best individual form eager learning employed agents benefit communication abilities downloading agent list personal experiences may infeasible 
team heterogeneity team agents solving task combined forces useful agents specialize different subtasks share effort efficient way 
way achieve kind heterogeneity multi agent system equip agents different sensor effector capabilities behaviors pre define roles going play team effort 
method may lead results number drawbacks 
firstly obvious specify optimal useful distribution sensors effectors behaviors 
secondly may quite expensive terms hardware terms development time design system heterogeneous agents 
alternative team learning agents homogeneous time experience diversify specialize 
balch studied conditions team agents reinforcement learning converge heterogeneity 
research distinguishes types reward functions local performance reinforcement agent receives rewards individually personally achieves task 
global performance reinforcement agents receive reward team members achieves task 
local shaped reinforcement agent receives rewards continuously gets closer accomplishing task 
balch applied different reward functions multi agent reinforcement learning domains multi robot foraging robotic soccer formation movements 
social entropy diversity measure observed results globally reinforced agents converge heterogeneous team local reinforcement leads homogeneous agents 
heterogeneity desirable multi robot foraging locally reinforced homogeneous team outperforms globally reinforced heterogeneous team 
hand robotic soccer domain results opposite heterogeneity yields better performance 
clearly results show team heterogeneity important factor multi agent performance different learning methods yield different levels heterogeneity 
interesting open research problem gain clearer detailed understanding relationship agent diversity performance learning general 
distributed learning multi agent learning research concerned question learn local hypotheses enable agents collaborate compete communicate effectively distributed learning concerned applying multi agent techniques learn global hypothesis local distributed learning agents example distributed learning consider robotic soccer domain absence global view computation opposing team strategy local observations individual players 
infeasible simply send observations super agent learns agents need generalize observations share results order compute global strategy opposing team 
currently working algorithms achieve task 
weiss distinguishes types multi agent learning course distributed learning process may lead effective coordination learning metaphor different 
multiplied learning agent learns independently agents 
may interactions concerning exchange training data outputs interference learning process takes place 
divided learning learning task distributed team agents 
division takes place functional level agents take different roles team learn separately 
interacting learning agents interact learning cooperate generating hypothesis pure exchange data 
weiss describes cooperated negotiated search solution learning task 
multi agent learning systems fall categories 
third category describes true distributed learning little research exists area 
provost hennessy describe method parallel learning learning agents receive different subsets training data learn local hypothesis generalize global hypothesis 
agent checks validity learned rule sending agent order compute accuracy dataset 
valid rules form final hypothesis 
approaches related distributed learning include weiss ace age algorithms group learning agents jointly decide actions bidding scheme jointly receive reinforcement executed actions 
inductive logic programming agents inductive logic programming ilp branch machine learning built pillars logic programming lp representation formalism training examples theories learned background knowledge form predicates extend concept language express target concept induction method learning 
logic programming object concept language ilp object language represent examples target concept concept language represent concepts theories learned order logic precisely logic programming 
means facts concepts represented predicates order logic 
ilp systems target predicate 
training examples represented ground facts object language restricted propositional logic way propositional learners id 
words training data represented single table name columns correspond name attributes target predicate see table 
ilp different propositional learners concept language 
purpose ilp uses relations expressed horn clauses 
table 
training data representation bargain cars ilp representation model mileage price model mileage price 
bmw bmw 
audi audi 
fiat uno fiat uno 
instance concept equality arguments expressed clause equal say argument greater argument writes greater propositional logic variables define relations target concept arguments examples covered explicit enumeration pairs task tedious range arguments finite impossible 
ilp systems may require modes types 
modes serve distinguish input arguments predicate instantiated query output arguments instantiated result query 
case usually indicated plus minus 
systems progol allow user indicate clauses target predicate learned argument instantiated constant marking argument hash symbol 
types specify range values argument usually defined user defined unary predicates systems provide built types nat real progol 
important predicates queried instantiated argument natural number 
generative allowing sampling type range give random natural number 
importance related type training examples available 
positive negative examples target concept provided types mark variables horn clause unified restricting way target concept search space 
hand generative type definitions required positive learning ilp system progol 
reason requirement clear closer look mechanism underlying type learning 
situations positive learning natural setting child learns speak hardly imagine parents providing examples ill formed sentences example case animal agent learning definition non action 
search target concept guided accuracy tested training examples trivial definition classifying entry positive example cover positive examples negative ones provided 
quite ingenious 
solution provided problem progol generate random examples target predicate sampling type range attribute treat examples negative 
approach proves successful combined learning bias trade theory size shorter better coverage positive fewer random examples covered better 
importance background knowledge examples equal greater predicates ilp learn simple mathematical relations identity greater 
fact concept language propositional learners easily extended include relations 
truly unique feature ilp define background set relations predicates definition target concept 
certain background predicates may necessary condition finding target concept 
hand redundant irrelevant background predicates slows learning 
instance predicate imported model search potential correlation feature price car case large import taxes predicate may prove quite useful 
example information learning represented single table adding extra attribute 
target concept simplest case imported car propositional 
show advantage ilp imagine product car mileage price indicator bargain 
define background predicate prod miles price threshold miles price threshold modes prod miles price threshold 
learn theory data table model miles price prod miles price 
allow target concept recursive appropriate define constraints car bargain buy model mileage 
expressed progol prune model miles price body body model miles price price price 
inductive learning proving clauses starting general theory lp systems explanation learning ilp systems try find general theory explain clauses 
resolution generally automated deduction ways perform induction 
generally called inverse resolution derived applying resolution step reverse way 
induction techniques general generalisation lgg 
ilp learning agents description far shows ilp symbolic knowledge oriented learning explore complex hypotheses represent concise way 
discussion ilp continues number implementation specific issues relevance ilp agents 
learning pure logic programs vs decision lists ilp algorithms divided learn theories expressed pure logic programs foil golem progol fall group include extra logical predicates 
concept language including predicate cut 
allows learning order decision lists clause ends cut 
pure logic programs clause order irrelevant 
decision list queried list searched top fashion applicable clause 
intuitive notions rule exception easily represented decision lists exception rule explicitely mention cover 
decision list example follows table 
example models behavior cat afraid dogs ones belong owner fear animals 
table 
example decision lists action cat action action cat animal stay dog animal owner owner animal owner owner cat 
action cat animal run dog animal 
action cat animal stay 
theories expressed decision lists simpler corresponding pure lp representation 
decision lists useful integrate previously learned theories newly acquired training examples 
examples misclassified existing theory added exceptions top decision list ensure correct classification 
learning replace exceptions rules possible 
foidl clog order decision list learners 
worth mentioning clog progol incremental learner 
eager ilp vs analogical prediction relative merits eager lazy learning discussed 
ilp belongs eager learning paradigm sole exception analogical prediction ap 
ap constructs separate theory order horn clause classify test example time theory provides explanation decision uncommon lazy learning 
ap suitable context rational learner provide motivation actions 
time complexity method serious obstacle larger 
single predicate vs multi predicate learning learning judged terms predictive accuracy additional training examples beneficial noise data 
agent learning concerned time complexity theory learned long takes query theory 
ilp learns logic programs recursive allow lot backtracking complexity programs easily issue 
experimental results known authors involve bottom ilp lgg 
experiments curve representing relationship number training examples average time needed prove goal reject theory learned linear 
avoid learning theories high time complexity agent content simple low coverage theory additional training examples produce theory complexity critical point 
existing ilp applied number tasks related agents 
instance zeroski combined ilp system tilde rt reinforcement learning task learning actions simple world blocks 
function learned form logical regression tree binary tree non leaf node corresponds condition expressed logic clause split branches labelled leaf nodes labelled values 
reid ryan ilp improve planning hierarchical reinforcement learning optimality traded speed hoping find solutions quickly breaking monolithic reinforcement learning task smaller ones combining solutions 
learning tree ilp learn descriptions subsets state space 
matsui proposed develop ilp agent avoids actions provably fail achieve goal 
suggested application area robocup robot football tournament world wide popularity 
ilp application domain provides agent way analyzing behavior summarizing decision tree rules implicitely 
declarative specifications software implementing agent behavior obtained compared intentions agent designers help debugging process 
progol algorithm learning action theories single agent mobile robot described lorenzo 
alonso kudenko investigate application ilp ebl complex multi agent domains conflict simulations 
learning reinforcement recomputation hypotheses 
york multi agent environment ideas discussed far studied system developed university york 
system intended general purpose java platform simulation learning natural selection community agents 
system draws parallel world nature 
provides tools specification dimensional environment range features different types terrain food water resources 
user edit large extent specification number species agents eat fear types sensors possess terrains hide walk 
single agent learning agents specified progol mind communicate dedicated progol process see 
observations collected agent sensors sent process ground logic clauses progol turn send back agent directions behavior 
provides framework single agent learning multi agent environment 
learning limited ilp implementation progol includes full scale prolog interpreter implementation various learning techniques 
learning recall carried integrated way newly learned clauses added progol database 
new multi agent progol developed available allow agents query databases 
synchronisation time constraints simplify debugging allow easier analysis agents behavior coordination agents implemented step time simultaneous update environment fashion having agents implemented separate threads 
system easily modified wait indefinitely progol responds answer selects action allocate agent limited amount time move ignored round 
default behavior agents default hard coded behavior baseline evaluation learning agents fallback plan adaptive behavior modified learning decide action required time limit 
behavior notion drives java object java object agent agent progol process progol process fig 

agents java body progol mind represent intensity agent basic needs 
drives current implementation hunger thirst fear sex drive 
step drives evaluated action taken reduce highest intensity 
snapshot ongoing experiment shown 
drives agent marked black dot shown left hand side screen 
case surrounded predators 
result prevailing drive fear action agent chosen accordingly agent attempt run away 
natural selection mentioned agents sex drive 
definition agent includes array integers combination built genetic operators crossover mutation implement agents parameters inherited features 
reproduction involves fixed cost contribution initial energy level offspring natural selection put evolutionary pressure select sets parameters improve agent performance 
outlook overview important issues application machine learning algorithms multi agent systems vice versa starting description ml algorithms moving single agent learning multi agent learning mal 
furthermore discussed application ilp logic learning technique mas 
area mal young plenty investigate 
examples interesting fig 

snapshot york multi agent environment formal models mal date developers able predict behavior learning agents depend purely observing emergent patterns 
formal models mal predict constrain behavior learning agents useful 
approach see 
complex applications mal application domains relatively simple 
interesting see mal research complex real world applications 
eventually applications encourage researchers look learning 
hope generate interest multi agent learning encourage new researchers look open issues 

alonso kudenko 
machine learning techniques adaptive logic multi agent systems 
bristol uk 

balch 
behavioral diversity multiagent cooperation 
spie workshop multiagent systems 

balch 
hierarchic social entropy information theoretic measure robot team diversity 
autonomous robots july 

bryant muggleton 
closed loop machine learning 
technical report university york department computer science york yo dd uk 

walter daelemans van den bosch zavrel 
forgetting exceptions harmful language learning 
machine learning 

dietterich flann 
explanation learning reinforcement learning 
proceedings twelfth international conference machine learning pages 

sa zeroski luc de raedt hendrik blockeel 
relational reinforcement learning 
david page editor eighth international conference ilp pages madison wisconsin usa 
springer verlag 

furukawa editor 
international conference discovery science lncs fukuoka japan 
springer verlag 

gilbert conte eds 
artificial societies computer simulation social life 
ucl press london 

gordon 
adaptive agents 
journal artificial intelligence research 

nico jacobs kurt driessens luc de raedt 
ilp systems verification validation multi agent systems 
david page editor eighth international conference ilp pages madison wisconsin usa 
springer verlag 

kaelbling littman moore 
reinforcement learning survey 
journal artificial intelligence research 

kazakov 
natural language applications machine learning 
phd thesis czech technical university prague czech republic january 

kazakov lee steve routledge 
implementing natural selection symbolic learning multi agent environment 
manuscript 

kazakov suresh manandhar 
unsupervised learning word segmentation rules genetic algorithms inductive logic programming 
machine learning 


kitano kuniyoshi noda asada matsubara osawa 
robocup challenge problem ai 
ai magazine 

klusch ed 
intelligent information agents 
springer verlag 

kudenko alonso 
logic multi agent systems conflict simulations 
oxford uk 

david lorenzo ramon 
ilp algorithm learn logic programs reasoning actions 
james cussens alan frisch editors reports ilp pages london uk 

matsui seki 
proposal inductive learning agent order logic 
james cussens alan frisch editors progress reports ilp pages london uk 

michalski ivan bratko miroslav kubat editors 
machine learning data mining methods applications 
wiley 

mitchell keller kedar 
explanation generalization unifying view 
machine learning 

tom mitchell 
machine learning 
mcgraw hill 

raymond mooney mary elaine califf 
induction order decision lists results learning past tense english verbs 
journal artificial intelligence research june 

muggleton 
inverse entailment progol 
new generation computing 

muggleton 
learning positive data 
machine learning 

muggleton bain 
analogical prediction 
proc 
th international workshop inductive logic programming ilp pages berlin 
springer verlag 

muggleton buntine 
machine invention order predicates inverting resolution 
proceedings fifth international conference machine learning pages 
morgan kaufmann san mateo ca 

muggleton feng 
efficient induction logic programs 
proceedings conference algorithmic learning theory tokyo 
ohmsha 

stephen muggleton 
personal communication 

natarajan 
imprecise approximate computations 
kluwer 

parker 
heterogeneous multi robot cooperation 
phd thesis mit department electrical engineering computer science 

plotkin 
note inductive generalization 
meltzer editors machine intelligence pages 
edinburgh university press 

provost hennessy 
scaling distributed machine learning cooperation 
proceedings thirteenth national conference artificial intelligence aaai 

quinlan 
learning logical definitions relations 
ml 

mark reid malcolm ryan 
ilp improve planning hierarchical reinforcement learning 
james cussens alan frisch editors tenth international conference ilp pages london uk 
springerverlag 

sen weiss 
learning multi agent systems 
weiss editor multiagent systems modern approach distributed ai pages 
mit press cambridge ma 

steels 
cooperation distributed agents self organization 
demazeau 
mueller editors decentralized ai proceedings european workshop modelling autonomous agents multi agent world maamaw pages 
elsevier science amsterdam 

tesauro 
temporal difference learning td gammon 
communications acm 

cynthia thompson mary elaine califf raymond mooney 
active learning natural language parsing information extraction 
proc 
sixteenth international machine learning conference bled slovenia 

url verbmobil dfki de 

vidal durfee 
agents learning agents framework analysis 
working notes aaai workshop multiagent learning pages 

watkins dayan 
learning 
machine learning 

weiss 
learning coordinate actions multiagent systems 
proceedings th international joint conference artificial intelligence ijcai pages 

weiss dillenbourg 
multi multiagent learning 
dillenbourg editor collaborative learning 
cognitive computational approaches pages 
pergamon press 
