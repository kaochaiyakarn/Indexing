conditional random fields probabilistic models segmenting labeling sequence data john lafferty lafferty cs cmu edu andrew mccallum mccallum whizbang com fernando pereira whizbang com whizbang 
labs research henry street pittsburgh pa usa school computer science carnegie mellon university pittsburgh pa usa department computer information science university pennsylvania philadelphia pa usa conditional random fields framework building probabilistic models segment label sequence data 
conditional random fields offer advantages hidden markov models stochastic grammars tasks including ability relax strong independence assumptions models 
conditional random fields avoid fundamental limitation maximum entropy markov models memms discriminative markov models directed graphical models biased states successor states 
iterative parameter estimation algorithms conditional random fields compare performance resulting models hmms memms synthetic natural language data 

need segment label sequences arises different problems scientific fields 
hidden markov models hmms stochastic grammars understood widely probabilistic models problems 
computational biology hmms stochastic grammars successfully align biological sequences find sequences homologous known evolutionary family analyze rna secondary structure durbin 
computational linguistics computer science hmms stochastic grammars applied wide variety problems text speech processing including topic segmentation part ofspeech pos tagging information extraction syntactic disambiguation manning sch tze 
hmms stochastic grammars generative models assigning joint probability paired observation label sequences parameters typically trained maxi joint likelihood training examples 
define joint probability observation label sequences generative model needs enumerate possible observation sequences typically requiring representation observations task appropriate atomic entities words nucleotides 
particular practical represent multiple interacting features long range dependencies observations inference problem models intractable 
difficulty main motivations looking conditional models alternative 
conditional model specifies probabilities possible label sequences observation sequence 
expend modeling effort observations test time fixed anyway 
furthermore conditional probability label sequence depend arbitrary features observation sequence forcing model account distribution dependencies 
chosen features may represent attributes different levels granularity observations example words characters english text aggregate properties observation sequence instance text layout 
probability transition labels may depend current observation past observations available 
contrast generative models strict independence assumptions observations instance conditional independence labels achieve tractability 
maximum entropy markov models memms conditional probabilistic sequence models attain advantages mccallum 
memms source state exponential model takes observation features input outputs distribution possible states 
exponential models trained appropriate iterative scaling method output labels associated states possible states label simplicity rest assume correspondence 
maximum entropy framework 
previously published experimental results show memms increasing recall doubling precision relative hmms faq segmentation task 
memms non generative finite state models state classifiers discriminative markov models bottou share weakness call label bias problem transitions leaving state compete transitions model 
probabilistic terms transition scores conditional probabilities possible states current state observation sequence 
state normalization transition scores implies conservation score mass bottou mass arrives state distributed possible successor states 
observation affect destination states get mass total mass pass 
causes bias states fewer outgoing transitions 
extreme case state single outgoing transition effectively ignores observation 
cases hmms viterbi decoding downgrade branch observations branch point models structures sparsely connected chains states properly handled 
markovian assumptions memms similar state conditional models insulate decisions state decisions way match actual dependencies consecutive states 
introduces conditional random fields crfs sequence modeling framework advantages memms solves label bias problem principled way 
critical difference crfs memms memm uses state exponential models conditional probabilities states current state crf single exponential model joint probability entire sequence labels observation sequence 
weights different features different states traded 
think crf finite state model unnormalized transition probabilities 
weighted finite state approaches lecun crfs assign defined probability distribution possible labelings trained maximum likelihood map estimation 
furthermore loss function convex guaranteeing convergence global optimum 
crfs generalize easily analogues stochastic context free grammars useful problems rna secondary structure prediction natural language processing 
case fully observable states discussing states label usual local maxima baum welch arise 
rib rob 
label bias example bottou 
conciseness place observation label pairs transitions states symbol represents null output label 
model describe training procedures sketch proof convergence 
give experimental results synthetic data showing crfs solve classical version label bias problem significantly crfs perform better hmms memms true data distribution higher order dependencies model case practice 
confirm results claimed advantages conditional models evaluating hmms memms crfs identical state structure part speech tagging task 

label bias problem classical probabilistic automata paz discriminative markov models bottou maximum entropy taggers ratnaparkhi memms non probabilistic sequence tagging segmentation models independently trained state classifiers punyakanok roth potential victims label bias problem 
example represents simple finite state model designed distinguish words rib rob 
suppose observation sequence time step matches transitions start state probability mass gets distributed roughly equally transitions 
observe states outgoing transition 
state seen observation training state seen observation state state choice pass mass single outgoing transition generating observation conditioning 
states single outgoing transition effectively ignore observations 
generally states low entropy state distributions take little notice observations 
returning example top path bottom path equally independently observation sequence 
words slightly common training set transitions start state slightly prefer corresponding transition word state sequence win 
behavior demonstrated experimentally section 
bottou discussed solutions label bias problem 
change state transition struc ture model 
example collapse states delay branching get discriminating observation 
operation special case determinization mohri determinization weighted finite state machines possible possible may lead combinatorial explosion 
solution mentioned start model training procedure structure 
preclude prior structural knowledge proven valuable information extraction tasks freitag mccallum 
proper solutions require models account state sequences letting transitions vote strongly depending corresponding observations 
implies score mass conserved individual transitions amplify dampen mass receive 
example transitions start state weak effect path score transitions states stronger effects amplifying damping depending actual observation proportionally higher contribution selection viterbi path 
related section discuss heuristic model classes account state sequences globally locally 
best knowledge crfs model class purely probabilistic setting guaranteed global maximum likelihood convergence 

conditional random fields follows random variable data sequences labeled random variable corresponding label sequences 
components yi assumed range finite label alphabet example range natural language sentences range part speech sentences set possible part speech tags 
random variables jointly distributed discriminative framework construct conditional model paired observation label sequences explicitly model marginal 
definition 
graph yv indexed vertices conditional random field case conditioned random variables yv obey markov property respect graph yv yw yv yw means neighbors crf random field globally conditioned observation tacitly assume graph fixed 
simplest impor weighted determinization minimization techniques shift transition weights preserving path weight mohri connection discussion deserves study 
tant example modeling sequences simple chain line 

may natural graph structure general necessary assume graphical structure graphical structure 
concerned sequences 
xn 
yn 
graph tree chain simplest example cliques edges vertices 
fundamental theorem random fields hammersley clifford joint distribution label sequence form exp fk gk data sequence label sequence set components associated vertices subgraph assume features fk gk fixed 
example boolean vertex feature gk true word xi upper case tag yi proper noun parameter estimation problem determine parameters 

training data empirical distribution 
section describe iterative scaling algorithm maximizes log likelihood objective function log log particular case construct hmm crf defining feature state pair feature state observation pair fy yu yv gy yv xv corresponding parameters play similar role logarithms usual hmm parameters 
boltzmann chain models saul jordan mackay similar form single normalization constant yield joint distribution crfs observation dependent normalization conditional distributions 
encompasses hmm models class conditional random fields expressive allows arbitrary dependencies observation yi yi yi xi xi xi yi yi yi xi xi xi yi yi yi xi xi xi 
graphical structures simple hmms left memms center chain structured case crfs right sequences 
open circle indicates variable generated model 
sequence 
addition features need specify completely state observation expect model estimated training data 
attractive property convexity loss function crfs share convexity properties general maximum entropy models 
remainder assume dependencies conditioned form chain 
simplify expressions add special start states start yn 
graphical structure shown 
chain structure conditional probability label sequence expressed concisely matrix form useful describing parameter estimation inference algorithms section 
suppose crf 
position observation sequence define matrix random variable mi mi mi exp fk ei ei gk vi vi ei edge labels yi yi vi vertex label yi 
contrast generative models conditional models crfs need enumerate possible observation sequences matrices computed directly needed training test observation sequence parameter vector 
normalization partition function start entry product matrices mn start notation conditional probability label sequence written start yn 
mi yi yi mi start 
parameter estimation crfs describe iterative scaling algorithms find parameter vector maximizes log likelihood training data 
algorithms improved iterative scaling iis algorithm della pietra 
proof technique auxiliary functions extended show convergence algorithms crfs 
iterative scaling algorithms update weights appropriately chosen 
particular iis update edge feature fk solution fk def fk ei ei fk ei ei kt total feature count def fk ei ei gk vi vi equations vertex feature updates similar form 
efficiently computing exponential sums right hand sides equations problematic global property dynamic programming sum sequences potentially varying deal algorithm algorithm uses slack feature second algorithm keeps track partial totals 
algorithm define slack feature def fk ei ei gk vi vi constant chosen observation vectors training set making feature global correspond particular edge vertex 
index 
define forward vectors base case start recurrence mi similarly backward vectors defined mi definitions update equations log log fk ei ei mi gk vi vi factors involving forward backward vectors equations meaning standard hidden markov models 
example yi marginal probability label yi observation sequence algorithm closely related algorithm darroch ratcliff mart algorithms image reconstruction 
constant algorithm quite large practice proportional length longest training observation sequence 
result algorithm may converge slowly small steps maximum iteration 
length observations number active features varies greatly algorithm obtained keeping track feature totals observation sequence separately 
def maxy 
algorithm accumulates feature expectations counters indexed 
specifically forward backward recurrences just introduced compute expectations ak feature fk bk feature gk parameter updates log log unique positive roots polynomial equations tmax ak tmax bk easily computed newton method 
single iteration algorithm algorithm roughly time space complexity known baum welch algorithm hmms 
prove convergence algorithms derive auxiliary function bound change likelihood method developed detail della pietra 

full proof somewhat detailed give idea derive auxiliary function 
simplify notation assume edge features fk parameters parameter settings 
bound change objective function auxiliary function follows log ef log ef ef ef def fk kt inequalities follow convexity log exp differentiating respect setting result zero yields equation 

experiments discuss sets experiments synthetic data highlight differences crfs memms 
experiments direct verification label bias problem discussed section 
second set experiments generate synthetic data randomly chosen hidden markov models mixture order second order model 
competing order models trained compared test data 
data second order test error rates trained models increase 
experiment corresponds common modeling practice approximating complex local long range dependencies occur natural data small order markov models 
memm error crf error memm error hmm error crf error hmm error 
plots error rates hmms crfs memms randomly generated synthetic data sets described section 
data second order error rates test models increase 
shown left plot crf typically significantly outperforms memm 
center plot shows hmm outperforms memm 
right plot open square represents data set solid circle indicates data set plot shows data second order discriminatively trained crf typically outperforms hmm 
experiments designed demonstrate advantages additional representational power crfs memms relative hmms 
results clearly indicate models parameterized exactly way crfs robust inaccurate modeling assumptions memms hmms resolve label bias problem affects performance memms 
avoid confusion different effects memms crfs experiments overlapping features observations 
set pos tagging experiments confirm advantage crfs memms 
show addition overlapping features crfs memms allows perform better hmms shown memms mccallum 

modeling label bias generate data simple hmm encodes noisy version finite state network 
state emits designated symbol probability symbols probability 
train memm crf topologies data generated hmm 
observation features simply identity observation symbols 
typical run training test samples trained convergence iterative scaling algorithm crf error memm error showing memm fails discriminate branches 
modeling mixed order sources results labels observation values results qualitatively range sizes generate data mixed order hmm state transition probabilities yi yi yi yi yi yi yi yi similarly emission probabilities xi yi xi xi yi xi xi yi 
standard order hmm 
order limit size bayes error rate resulting models conditional probability tables constrained sparse 
particular nonzero entries nonzero entries randomly generated model sample sequences length generated training testing 
randomly generated training set crf trained algorithm 
note length sequences number active features constant algorithms identical 
algorithm fairly slow converge typically approximately iterations model stabilize 
mhz pentium pc experiments iteration takes approximately seconds 
data memm trained iterative scaling require forward backward calculations efficient 
memm training converges quickly stabilizing approximately iterations 
model viterbi algorithm label test set experimental results significantly change forward backward decoding minimize symbol error rate 
results runs 
plot compares classes models point indicating error rate single test set 
increases error rates generally increase order models fail fit second order data 
compares models parameterized results models parameterized qualitatively 
shown graph crf generally outperforms memm wide margin relative error 
points small error rate memm better crf suspected result insufficient number training iterations crf 
model error oov error hmm memm crf memm crf spelling features 
word error rates pos tagging penn treebank order models trained word corpus 
oov rate 
pos tagging experiments confirm synthetic data results compared hmms memms crfs penn treebank pos tagging word input sentence labeled syntactic tags 
carried sets experiments natural language data 
trained order hmm memm crf models synthetic data experiments introducing parameters tag word pair tag tag pair training set 
results consistent observed synthetic data hmm outperforms memm consequence label bias problem crf outperforms hmm 
error rates training runs train test split shown results qualitatively similar splits data 
error rates outof vocabulary oov words observed training set reported separately 
second set experiments take advantage power conditional models adding small set orthographic features spelling begins number upper case letter contains hyphen ends suffixes ing ogy ed ly ion tion ity ies 
find expected memm crf benefit significantly features error rate reduced vocabulary error rate reduced 
usually starts training zero parameter vector corresponding uniform distribution 
datasets crf training initialization slower memm training 
fortunately optimal memm parameter vector starting point training corresponding crf 
memm trained convergence iterations 
parameters initialize training crf converged iterations 
contrast training crf uniform distribution converged iterations 

aspects crfs aspects crfs attractive applications deserve study 
section briefly mention just 
conditional random fields trained exponential loss objective function adaboost algorithm freund schapire 
typically boosting applied classification problems small fixed number classes applications boosting sequence labeling treated label separate classification problem abney 
possible apply parallel update algorithm collins 
optimize sequence exponential loss 
requires forward backward algorithm compute efficiently certain feature expectations lines algorithm feature requires separate set forward backward accumulators 
attractive aspect crfs implement efficient feature selection feature induction algorithms 
specifying advance features start feature generating rules evaluate benefit generated features automatically data 
particular feature induction algorithms della pietra 
adapted fit dynamic programming techniques conditional random fields 

related far know combine benefits conditional models global normalization random field models 
applications exponential models sequence modeling attempted build generative models rosenfeld involve hard normalization problem adopted local conditional models berger ratnaparkhi mccallum may suffer label bias 
non probabilistic local decision models widely segmentation tagging brill roth abney 
computational complexity global training models trained minimize error individual label decisions assuming neighboring labels correctly chosen 
label bias expected problem 
alternative approach discriminative modeling sequence labeling permissive generative model model local dependencies produce list candidates global discriminative model rerank candidates 
approach standard large vocabulary speech recognition schwartz austin proposed parsing collins 
methods fail correct output pruned away pass 
closest proposal gradient descent methods adjust parameters local classifiers minimize smooth loss function quadratic loss combining loss terms label 
state dependencies local done efficiently dynamic programming lecun 
methods alleviate label bias 
loss function convex may get stuck local minima 
conditional random fields offer unique combination properties discriminatively trained models sequence segmentation labeling combination arbitrary overlapping agglomerative observation features past efficient training decoding dynamic programming parameter estimation guaranteed find global optimum 
main current limitation slow convergence training algorithm relative memms hmms training fully observed data efficient 
plan investigate alternative training methods update methods collins 
refinements memm starting point experiments 
general tree structured random fields feature induction methods natural data evaluations investigated 
acknowledgments bengio bottou michael collins yann lecun alerting call label bias problem 
andrew ng sebastian thrun discussions related 
abney schapire singer 

boosting applied tagging pp attachment 
proc 
emnlp vlc 
new brunswick new jersey association computational linguistics 
berger della pietra della pietra 

maximum entropy approach natural language processing 
computational linguistics 
bottou 

une approche th de apprentissage applications la reconnaissance de la parole 
doctoral dissertation universit de paris xi 
brill 

transformation error driven learning natural language processing case study part speech tagging 
computational linguistics 
collins 

discriminative reranking natural language parsing 
proc 
icml 
stanford california 
collins schapire singer 

logistic regression adaboost bregman distances 
proc 
th colt 
darroch ratcliff 

generalized iterative scaling log linear models 
annals mathematical statistics 
della pietra della pietra lafferty 

inducing features random fields 
ieee transactions pattern analysis machine intelligence 
durbin eddy krogh mitchison 

biological sequence analysis probabilistic models proteins nucleic acids 
cambridge university press 
freitag mccallum 

information extraction hmm structures learned stochastic optimization 
proc 
aaai 
freund schapire 

decision theoretic generalization line learning application boosting 
journal computer system sciences 
hammersley clifford 

markov fields finite graphs lattices 
unpublished manuscript 
lecun bottou bengio haffner 

gradient learning applied document recognition 
proceedings ieee 
mackay 

equivalence linear boltzmann chains hidden markov models 
neural computation 
manning sch tze 

foundations statistical natural language processing 
cambridge massachusetts mit press 
mccallum freitag pereira 

maximum entropy markov models information extraction segmentation 
proc 
icml pp 

stanford california 
mohri 

finite state transducers language speech processing 
computational linguistics 
mohri 

minimization algorithms sequential transducers 
theoretical computer science 
paz 

probabilistic automata 
academic press 
punyakanok roth 

classifiers sequential inference 
nips 
forthcoming 
ratnaparkhi 

maximum entropy model part speech tagging 
proc 
emnlp 
new brunswick new jersey association computational linguistics 
rosenfeld 

sentence maximum entropy language model 
proceedings ieee workshop speech recognition understanding 
santa barbara california 
roth 

learning resolve natural language ambiguities unified approach 
proc 
th aaai pp 

menlo park california aaai press 
saul jordan 

boltzmann chains hidden markov models 
advances neural information processing systems 
mit press 
schwartz austin 

comparison approximate algorithms finding multiple best sentence hypotheses 
proc 
icassp 
minneapolis mn 
