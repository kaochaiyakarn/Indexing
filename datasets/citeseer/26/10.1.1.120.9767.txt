second order hidden markov model part speech tagging scott mary harper school electrical computer engineering purdue university west lafayette harper ecn purdue edu describes extension hidden markov model part speech tagging second order approximations tual lexical probabilities 
model creases accuracy tagger state art levels 
approximations contextual information standard statistical systems 
new methods smoothing estimated probabilities introduced address sparse data problem 
part speech tagging act assigning word sentence tag describes word sentence 
typ ically tags indicate syntactic categories noun verb occasionally include additional feature information number singular plural verb tense 
penn treebank documentation marcus defines commonly set tags 
part speech tagging important re search topic natural language processing nlp 
taggers preprocessors nlp systems making accurate performance cially important 
research done improve tagging accuracy dif ferent models methods including hidden markov models hmms kupiec char rule systems brill brill memory systems daelemans maximum entropy systems ratnaparkhi path voting con straint systems linear separator systems roth majority voting systems van halteren 
describes various modifications hmm tagger improve perfor mance accuracy comparable better best current single classifier taggers 
improvement comes second order approximations markov assump tions 
section discusses basic order hidden markov model part speech tagging extensions model handle lexicon words 
new second order hmm described section section presents experimental results 
hidden markov models hidden markov model hmm statistical construct solve classification problems inherent state sequence representation 
model visualized interlocking set states 
states connected set transition ties indicate probability traveling states 
process begins state discrete time intervals process moves new state dictated transition probabilities 
hmm exact sequence states process gener ates unknown hidden 
process enters state set output symbols emitted process 
exactly symbol emitted determined probability distri bution specific state 
output hmm sequence output symbols 
basic definitions notation rabiner el ements needed define hmm :10.1.1.131.2084
number distinct states model 
part speech tagging number tags system 
possible tag system corresponds state hmm 

number distinct output symbols alphabet hmm 
part speech tagging number words lexicon system 

state transition probabil ity distribution 
probability aij probability process move state state transition 
part speech tagging states represent tags aij probability model move tag ti tj words probability tag tj follows ti 
probability estimated data training corpus 

bj observation symbol prob ability distribution 
probability bj probability th output sym bol emitted model state part speech tagging probability word wk emitted system tag tj 
probability esti mated data training corpus 

tri initial state distribution 
ri probability model start state part speech tagging probability sentence gin tag ti 
hmm perform part speech tagging goal determine sequence tags states gen words sentence sequence output symbols 
words sen tence calculate sequence tags maximizes 
viterbi algorithm common method calculating tag sequence hmm 
algo rithm explained detail rabiner repeated 
calculating probabilities unknown words standard hmm word occur training data emit probabil ity unknown word ma trix bj wk unknown 
ing able accurately tag unknown words important frequently encountered tagging sentences applications 
area unknown words tagging deals predicting part speech informa tion word endings infor mation shown mikheev mikheev weischedel 
section highlights method devised hmms differs slightly previous approaches 
create hmm accurately tag unknown words necessary deter mine estimate probability tagger 
probabil ity word contains tag ti estimated sj suffix appropri ate term word sj necessarily morphologically significant terminology unwieldy 
new probability stored matrix cj cj word suffix ski tag tj replaces bj hmm calculations un known words 
probability esti mated collecting suffix information word training corpus 
suffixes length characters considered maximum suf fix length characters length word 
count num ber times suffix tag pair appears training corpus estimate emit prob abilities words suffixes exceptions 
estimating suffix prob abilities words length contain word information valuable classification ignored 
unknown words presumed open class words tagged open class tag ignored 
constructing suffix predictor words contain hyphens capitalized contain numeric digits separated main calculations 
estimates categories calculated separately 
ex ample unknown word capitalized probability distribution estimated capital ized words predict part speech 
capitalized words sentence classified way initial capitalization ignored 
word capitalized contain hy numeric digit general distribution 
predicting possible part speech unknown word possible matching suffixes predictions smoothed see section 
second order model part speech tagging model described section exam ple order hidden markov model 
part speech tagging called bigram tag ger 
model works reasonably part speech tagging captures limited amount contextual information available 
best statistical taggers trigram model replaces bigram transition probability aij rp ti trigram probability tj rp ti 
section describes new type tagger uses trigrams context probabilities lex ical suffix probabilities 
refer new model full second order hidden markov model 
defining new probability distributions full second order hmm uses notation similar standard order model probability distributions 
matrix con tains state transition probabilities matrix contains output symbol distributions matrix contains unknown word distributions 
rr matrix identical counterpart order model 
definitions modified enable full second order hmm contextual formation model part speech tagging 
sections assumed words sentence rp vp th tag word sentence respectively 
contextual probabilities matrix defines contextual probabil ities part speech tagger 
trigram model limiting context order approximation matrix defined follows rp tj rp tl transition matrix dimen sional probability transitioning new state depends current state previous state 
lows realistic context dependence word tags 
boundary cases special tag symbols sos 
suffix probabilities matrix defines lexical probabilities part speech tagger ma trix unknown words 
similarly trigram extension matrix approx imation lexical suffix probabilities modified include second order formation follows bij vii vp rp ti vp suffix tj rp tl equations probability model emitting word depends current state previous state 
knowledge approach tagging 
sos case 
smoothing issues full second order hmm pre cise approximation underlying probabil ities model problem arise sparseness data especially lexical esti mations 
example size ma trix wsj corpus ap possible tag tag word combinations 
attempt avoid sparse data estimation problems probability esti mates distribution smoothed 
methods smoothing discussed literature 
methods include ad method discussed gale church turing method jelinek mercer method jelinek mercer katz method katz 
methods useful smoothing gorithms variety applications 
appropriate purposes 
smoothing trigram probabilities ad turing methods limited usefulness takes account bi gram unigram probabilities 
katz smooth ing little granular effective application broad spectrum possi bilities reduced options depending number times event occurs 
smoothing function number occurances 
jelinek mercer accommodates smoothing gram probabilities differing coefficients number times gram occurs requires holding train ing data 
implemented model smooths lower order informa tion coefficients calculated number occurances trigram bigram unigram training 
method explained sections 
state transition probabilities estimate state transition probabilities want specific information 
information may available 
fixed smooth ing technique developed new method uses variable weighting 
method weight triples occur 
ka formula estimate rp tj rp tl na ka 
yoo depends numbers gl number times tk occurs number times sequence occurs number times sequence occurs total number tags appear number times tj occurs number times sequence occurs log 
log ng 
log na ka log na formulas chosen weighting element equation changes element occurs training data 
notice sum coefficients probabilities equation sum 
guarantees value returned valid probability 
value calculated tag triples values normalized creating valid probability distribution 
value smoothing technique comes clear triple question occurs infrequently 
consider calculating tag triple cd rb vb 
informa tion triple number times vb appears number times rb vb appears na number times cd rb vb appears total number tags number times rb appears number times cd rb appears values calculate cients log log ka log log values calculate probability ka ln 
jr smoothing applied probabil ity create problems tagger generalization 
smoothing allows tag triples encountered training data assigned probability 
lexical suffix probabilities lexical suffix probabilities somewhat different context probabilities 
initial experiments formula similar contextual estimates performed poorly 
poor perfor mance traced fact smoothing words incorrectly tagged tags occur word training data generalization 
alternative calculated smoothed proba bility words follows log log log number times word wk occurs tag tj number times word wk occurs tag tj preceded tag tl number times tj occurs number times sequence occurs notice method assigns probability word tag pair appear training data 
prevents tagger trying possible combination word tag increases run ning time decreases accuracy 
low accuracy original smoothing scheme emerges fact smoothing lexical probabilities far allows con textual information dominate expense lexical information 
better smooth ing approach lexical information pos created sort word class idea genotype idea radev improve estimate 
addition choosing approach smoothing matrix unknown words additional issue choosing suffix predicting part speech 
possible answers considered longest matching suffix entropy measure determine best affix average 
voting technique cij determined similar contextual smoothing different length suffixes 
length suffix word 
define sl length suffixes respectively 
length word suffixes 
suffixes length length word 
determine longest suffix matches suffix training data calculate new smoothed probability gk sk sk log log lj ark number times suffix sk oc training data 
ij sk estimate cij previous lexical smoothing 
calculating normalized 
suf fixes length weight suffix receives weight times appears 
information provided suffixes length estimating probabilities 
new viterbi algorithm modification lexical contextual probabilities step defining full second order hmm 
probabilities combined select sequence tags generated sentence 
requires modification viterbi algo rithm 
variables ra redefined shown 
new definitions take account added dependencies distributions calculate tag sequence modification viterbi algorithm shown 
run ning time algorithm nt length sentence num ber tags 
asymptotically equivalent running time standard trigram tagger maximizes probability entire tag sequence 
experiment new tagging model tested different ways 
basic experimental tech nique fold cross validation 
corpus question randomly split sections sections combined train tagger tenth testing 
results possible training testing combinations merged give accuracy mea sure 
tagger tested corpora brown corpus treebank ii cd rom marcus wall street journal corpus source 
com results taggers difficult es different researchers 
care taken comparing systems comparisons experi ments similar possible differences highlighted comparison 
compare results corpus different versions hmm tagger standard bigram hmm tagger hmm ing second order lexical probabilities hmm second order contextual probabilities standard trigram tagger full second order hmm tagger 
results cor tagger table 
expected full second order hmm highest accuracy levels 
model ing second order contextual information standard trigram model second best model second order lexical informa tion third standard bigram hmm lowest accuracies 
full second order hmm reduced number errors known words bigram tag gers raising accuracy conventional trigram tag gets accuracy increase 
similar results seen accuracies 
un known word accuracy rates increased bigrams 
full second order hmm tagger compared researcher taggers ta ble 
important note snow linear separator model roth second order viterbi algorithm variables gp max rl rp rp ti rp tj vl vp tl rtp cp arg max rl rp rp ti rp tj vl vp tl procedure 
vl vl vl unknown lma xn jp bjk vp vp known 
xn jp ai vp unknown cp arg ia xg sp 
max rt max arg max jp 
cp second order viterbi algorithm comparison brown tagger type known standard bigram second order lexical second order contextual full second order hmm corpus unknown comparison wsj corpus tagger type known unknown standard bigram second order lexical second order contextual full second order hmm error reduction second order hmm system type compared brown wsj bigram lexical trigrams contextual trigrams table comparison taggers brown wsj corpora voting constraint tagger training data con tained full lexical information unknown words training testing data cover entire wsj corpus 
full lexicon may increased accuracy model tested unknown words 
stan dard trigram tagger data weischedel 
mbt daelemans tagger type standard trigram weischedel mbt daelemans rule brill maximum entropy ratnaparkhi full second order hmm snow roth voting constraints full second order hmm known unknown open closed lexicon 
open open open open open closed closed closed table comparison full second order hmm taggers include numbers lexicon accounts inflated accuracy unknown words 
table compares accuracies taggers known words unknown words accuracy 
table contains additional pieces information 
indi cates corresponding tagger tested ing closed lexicon words ap testing data known tag ger open lexicon words known system 
second indicates hold method cross validation tagger tested entire wsj corpus reduced corpus 
cross validation tests full second order hmm run open lexicon created training data second entire wsj lexicon test set 
tests low direct comparisons sys tem 
shown table full second order hmm improved ac wsj corpus state art full wsj indicate cross performed 
mbt place numbers lexicon numbers treated unknown words 
rule maximum entropy models full wsj training testing single test set 
snow fixed subset wsj training testing cross validation 
voting constraints tagger subset wsj training testing cross validation 
testing method full wsj fixed wsj cross validation fixed full wsj fixed full wsj full wsj cross validation fixed subset wsj subset wsj cross validation full wsj cross validation levels greatest accuracy reported full wsj experiment open lexicon 
closed lexicon full second order hmm achieved accuracy highest reported wsj cor pus type experiment 
accuracy system unknown words 
accuracy achieved creating separate classifiers capitalized hy numeric digit words tests wall street journal corpus full second order hmm show accuracy rate un known words separating types words 
perfor mance bigram tagger separates classifiers 
unfortunately unknown word accu racy systems 
may due part experimental dif ferences 
noted systems hand crafted rules unknown word rules system uses statistical data 
adding additional rules system result comparable formance 
improving model unknown words major focus research 
new statistical model full second order hmm shown improve part speech tagging accuracies current models 
model second order approximations hidden markov model mikheev separates suffix probabilities different estimates fails provide data illustrating implied accuracy increase 
improves state art taggers increase asymptotic running time tra ditional trigram taggers hidden markov model 
new smoothing method explained allows second order statistics avoiding sparse data problems 
eric brill 

report progress transformation error driven learn ing 
proceedings twelfth national con ference artifical intelligence pages 
eric brill 

transformation error driven learning natural language process ing case study part speech tagging 
computational linguistics 
eugene charniak curtis hendrickson neil ja mike perkowitz 

equa tions part speech tagging 
proceedings eleventh national conference arti intelligence pages 
walter daelemans zavrel peter steven 

mbt memory part speech tagger generator 
pro ceedings fourth workshop large corpora pages 
william gale kenneth church 

wrong adding 
corpus research language 
am 


population frequencies species estimation population parameters 
biometrika 
frederick jelinek robert mercer 

interpolated estimation markov source pa rameters sparse data 
proceedings workshop pattern recognition prac tice 
katz 

estimation ties sparse data language model component speech recognizer 
ieee transactions acoustics speech signal processing 
julian kupiec 

robust part speech tagging hidden markov model 
com puter speech language 
mitchell marcus beatrice santorini mary ann marcinkiewicz 

building large annotated corpus english penn treebank 
computational linguistics 
andrei mikheev 

unsupervised learning word category guessing rules 
proceedings th annual meeting association linguistics pages 
andrei mikheev 

automatic rule tion unknown word guessing 
computa tional linguistics 
lawrence rabiner 

tutorial hidden markov models selected applica tions speech recognition 
proceeding ieee pages 
adwait ratnaparkhi 

maximum en tropy model part speech tagging 
pro ceedings conference empirical methods natural language processing pages 
dan roth dmitry 

part speech tagging network linear sep 
proceedings coling acl pages 
scott 

predicting part speech information unknown words statistical methods 
proceedings coling acl pages 
kemal 

tagging english path voting constraints 
proceed ings coling acl pages 
dragomir radev 

word class part speech disambiguation 
proceedings fourth workshop large corpora pages 
hans van halteren zavrel wal ter daelemans 

improving data driven tagging system combination 
proceedings coling cl pages 
ralph weischedel marie richard schwartz lance ramshaw jeff pal 

coping ambiguity unknown words models 
computational linguistics 
