orca language parallel programming distributed systems henri bal frans kaashoek andrew tanenbaum dept mathematics computer science vrije universiteit amsterdam netherlands orca language implementing parallel applications loosely coupled distributed systems 
languages distributed programming allows processes different machines share data 
data encapsulated data objects instances user defined data types 
implementation orca takes care physical distribution objects local memories processors 
particular implementation may replicate migrate objects order decrease access times objects increase parallelism 
gives detailed description orca language design motivates design choices 
orca intended applications programmers systems programmers 
reflected design goals provide simple easy language type secure provides clean semantics 
discusses example parallel applications orca described detail 
describes existing implementations reliable broadcasting 
performance measurements system parallel applications 
measurements show significant speedups obtained applications 
compares orca related languages systems 

communication loosely coupled distributed computing systems gets faster systems attractive running parallel applications 
amoeba system example cost sending short message sun workstations ethernet milliseconds 
slower communication multicomputers hypercubes transputer grids fast coarse grained parallel applications 
return distributed systems easy build shelf preliminary version appeared proceedings usenix serc workshop experiences building distributed multiprocessor systems ft lauderdale oct 
research supported part netherlands organization scientific research 
components interconnecting multiple workstations microprocessors local area network lan 
addition systems easily expanded far larger numbers processors shared memory multiprocessors 
research studying implementation parallel applications distributed systems 
started implementing coarse grained parallel applications top amoeba system existing sequential language extended message passing interprocess communication 
felt parallel applications message passing sequential base language disadvantages making complicated applications programmers 
developed new language distributed programming called orca 
orca intended distributed applications programming systems programming designed simple expressive efficient language clean semantics 
briefly discuss important novelties language design 
processes orca communicate shared data processors run physical shared memory 
main novelty approach way access shared data expressed 
shared physical memory distributed shared memory shared data orca accessed user defined high level operations see important implications 
supporting shared data distributed system imposes challenging implementation problems 
worked implementations orca describe 
system uses reliable broadcast protocol 
protocol integration rest system new research results 
majority languages distributed programming orca extension existing sequential language 
sequential distributed constructs especially data structures designed way integrate 
language design addresses issues dealt languages 
distributed languages simply add primitives parallelism communication sequential base language ignore problems due poor integration sequential constructs 
typical example passing pointer message usually detected may cause great havoc 
orca provides solution problem keeps semantics language clean 
time orca constructs designed semantics close conventional languages making easy programmers learn orca 
important goal design orca keep language simple possible 
interesting parallel applications exist outside area computer science language suitable general applications programmers 
orca lacks low level features useful systems programming 
addition orca reduces complexity avoiding language features aimed solely increasing efficiency especially effect achieved optimizing compiler 
language designers frequently choose adding language features adding compiler optimizations 
general prefer option 
discuss examples design principle 
principle orthogonality care design goal 
issue taken account debugging 
debugging distributed programs difficult needs help get paid considerable attention debugging 
important orca type secure language 
language design allows implementation detect errors compile time 
addition language run time system extensive error checking 
gives overview orca distributed implementation orca performance 
structured follows 
section describe orca language motivate design choices 
section example application written orca 
section discuss implementation orca reliable broadcast 
describe implement broadcast primitive top lans support unreliable broadcast 
briefly compare system implementation orca uses remote procedure call broadcasting 
section give performance measurements applications 
section compare approach related languages systems 
section 

orca orca procedural strongly typed language 
sequential statements expressions fairly conventional roughly comparable identical modula 
data structuring facilities orca substantially different modula 
orca supports records unions dynamic arrays sets bags general graphs 
pointers intentionally omitted provide security 
language lacks global variables variables simulated passing parameters 
rest section structured follows 
motivate choice shared data message passing 
look processes expressing parallelism 
subsequently describe orca communication model shared data objects 
synchronization operations shared objects discussed followed discussion hierarchically objects 
look orca data structures 

distributed shared memory languages distributed programming message passing 
choice obvious underlying hardware supports message passing 
cases message passing appropriate programming model 
message passing form communication parties interact explicitly sending receiving messages 
message passing suitable processes need communicate indirectly sharing global state information 
examples applications 
example parallel branch algorithms current best solution bound stored global variable accessed processes 
say algorithms need physical shared memory merely need logically shared data 
algorithms harder implement efficiently message passing shared data 
literature contains numerous examples distributed applications algorithms greatly benefit support shared data physical shared memory available 
applications described literature include distributed speech recognition system linear equation solving dimensional partial differential equations split merge sort computer chess distributed system services name service time service global scheduling replicated files 
difficulty providing logically shared data message passing poor match applications 
researchers worked communication models logically shared data message passing 
models programmer shared data underlying hardware provide physical shared memory 
memory model looks user shared memory implemented disjoint machines referred distributed shared memory dsm 
different forms dsm exist 
li shared virtual memory svm best known example 
simulates physical shared memory distributed system 
svm distributes pages memory space local memories 
read pages may replicated 
svm provides clean simple model unfortunately problems implementing efficiently 
existing programming languages fall dsm class 
linda supports globally shared tuple space processes access form associative addressing 
distributed systems tuple space replicated partitioned pages svm 
operations allowed tuple space low level built argue complicates programming efficient distributed implementation hard 
emerald language related dsm class provides shared name space objects location transparent invocation mechanism 
emerald replication techniques typical dsm systems 
important issue addressed orca data shared distributed processes efficient way 
languages multiprocessors shared data structures stored shared memory accessed basically way local variables simple load store instructions 
process going change part shared data structure want processes interfere locks part 
operations loads stores locks shared data structures involve little overhead access shared memory hardly expensive access local memory 
distributed system hand time needed access data depends location data 
accessing data remote processors orders magnitude expensive accessing local data 
infeasible apply multiprocessor model programming distributed systems 
operations model far low level tremendous overhead distributed systems 
key idea orca access shared data structures higher level operations 
low level instructions reading writing locking shared data programmers define composite operations manipulating shared data structures 
shared data structures model encapsulated called data objects manipulated set user defined operations 
data objects best thought instances term object shorthand notation 
note term languages systems various different meanings 
variables data types 
programmer specifies data type defining operations applied instances data objects type 
actual data contained object executable code operations hidden implementation data type 

processes parallelism orca explicit compilers currently effective generating parallelism automatically 
implicit parallelism may suitable vector machines current state art compiler technology effective distributed systems 
parallelism expressed orca explicit creation sequential processes 
processes conceptually similar procedures procedure invocations serial process invocations parallel 
initially orca program consists single process new processes created explicitly fork statement fork name actual parameters cpu number statement creates new anonymous child process 
optionally new process assigned processor 
processors numbered sequentially fork statement may contain part expression specifies processor run child process 
part absent child process created processor parent 
system move processes initiative undesirable parallel applications 
process take parameters specified definition 
kinds allowed input shared 
process may take kind data structure value input parameter 
case process gets copy actual parameter 
parent pass data objects shared parameter child 
case data object shared parent child 
parent child communicate shared object executing operations defined object type explained 
example process child declared process child id integer shared new child process created follows declare object 
create new child process passing constant value parameter object shared parameter 
fork child children pass shared objects children 
way objects get distributed descendants process created 
processes performs operation object observe effect object shared memory protected lock variable 

shared data objects data types shared data object variable data type object type 
data type definition orca consists parts specification part implementation part 
specification part defines operations applicable objects type 
simple example specification part object type encapsulating integer shown 
object specification operation value integer return current value operation assign val integer assign new value operation add val integer add val current value operation min val integer set value minimum current value val fig 

specification part object type 
implementation part contains data represent objects type code initialize data new instances type code implementing operations 
part implementation type shown 
object specification integer internal data operation value integer return return current value operation assign integer assign new value initialize objects zero fig 

implementation part object type 
operation implementation similar procedure 
operation access local variables parameters local internal data object applied 
object type defined instances objects type created declaring variables type 
object created memory local variables object allocated initialization code executed 
operations applied object 
orca syntax declare object apply operation illustrated tmp integer assign assign add increment tmp value read current value orca supports single data type mechanism encapsulating shared non shared data 
words mechanism regular sequential data types 
stronger type creating shared local objects 
object declarations object type declarations specify objects shared 
information derived usage objects objects passed shared parameter fork statement shared 
objects local treated normal variables data type 
languages different mechanisms purposes 
argus example uses clusters local data guardians shared data clusters guardians completely different 
sr provides single mechanism resources overhead operations resources far high useful sequential data types 
fact shared data accessed user defined operations important distinction model models 
shared virtual memory example simulates physical shared memory shared data accessed low level read write operations 
linda tuple space model uses fixed number built operations add read delete shared tuples 
having users define operations advantages ease programming implementation discuss shortly 
data objects logically shared processes implementation need physical shared memory 
worst case operation remote object implemented message passing 
general idea implementation take care physical distribution data objects processors 
see section way achieve goal replicate shared data objects 
replicating objects access control shared objects decentralized decreases access costs increases parallelism 
major difference say monitors centralize control shared data 

synchronization data type orca creating shared local objects 
objects shared multiple processes issue synchronization arises 
types synchronization exist mutual exclusion synchronization condition synchronization 
look turn 
mutual exclusion synchronization mutual exclusion model done implicitly executing operations objects 
conceptually operation locks entire object applied releases lock finished 
precise model guarantees serializability operation invocations operations applied simultaneously data object result executed order invocation nondeterministic 
implementation model need execute operations 
increase degree parallelism may execute multiple operations object simultaneously long effect serialized execution 
example operations read change data stored object easily executed parallel 
users define operations objects user decide pieces code executed 
example data type encapsulating integer variable may operation increment integer 
operation done 
hand integer incremented separate read write operations read current value write incremented value back increment done separate actions indivisible 
rule defining actions indivisible easy understand flexible single operations indivisible sequences operations 
model provide mutual exclusion granularity lower object level 
languages give programmers accurate control mutual exclusion synchronization 
model support indivisible operations collection objects 
operations multiple objects require distributed locking protocol complicated implement efficiently 
generality seldom needed parallel applications 
prefer keep basic model simple implement complicated actions top 
operations model apply single objects executed 
model sufficiently powerful allow users construct locks sequences different objects arbitrary actions performed 
condition synchronization second form synchronization condition synchronization allows processes wait block certain condition true 
model condition synchronization integrated operation invocations allowing operations block 
processes synchronize implicitly operations shared objects 
blocking operation consists guarded commands operation op formal parameters guard condition statements od guard condition statements od conditions boolean expressions called guards 
simplify presentation initially assume guards side effect free 
problem side effects considered discussing hierarchically objects 
operation initially blocks guards evaluates true true guard selected nondeterministically sequence statements executed 
boolean expressions may depend parameters local data operation data object 
guard fails true state object changed 
may necessary evaluate guards times 
chosen form condition synchronization highly simple fits model 
alternative approach considered rejected separate synchronization primitive independent mechanism shared objects 
illustrate difference alternatives look specific example 
consider shared queue object operations add elements tail retrieve elements head operation add item add tail operation get item get head process trying fetch element empty queue allowed continue 
words number get operations applied queue exceed number add operations 
example synchronization constraint order operations executed 
conceivable ways expressing constraints model 
processes trying execute get check status queue block queue empty 
doing get empty queue results error 

get operation blocks queue empty 
processes executing get empty queue block automatically 
cases new primitive needed blocking processes 
case primitive directly user processes second case operations objects 
approach calls extra operation queues checks queue empty 
approaches unblocking process removing head element queue done indivisible action avoid race conditions 
approach major drawback users object responsible satisfying synchronization constraints 
contrast general idea data types hide implementation details objects users 
second approach cleaner implementer object takes care synchronization hides users 
second approach condition synchronization inside operations 
model allows operations block processes block executing operations block 
important issue design synchronization mechanism provide blocking operations guaranteeing operation invocations 
operation may block point execution operations longer serialized 
solution allow operations block initially modifying object 
operation may wait certain condition true started executing block 

hierarchical objects data types useful extending language new types 
method building new types hierarchical existing data types build new ones 
internal data object objects 
note hierarchical objects derived constituent objects extending done object oriented languages 
old new objects relation inheritance relation 
nesting objects causes difficult design problem explain 
suppose existing object type oldtype specified follows object specification oldtype operation boolean operation may object type implementation type omit specification type object implementation newtype oldtype nested object operation guard 
od objects new type contain object type oldtype 
object called nested object part object 
note instances newtype single objects operations executed 
nested object invisible outside enclosing object just internal data 
implementer newtype seen user oldtype 
implementer newtype know oldtype implemented 
lack information implementation operations oldtype causes problems 
problem illustrated guard 
need know guard expressions side effects may evaluated times 
unfortunately know invocation side effects 
operation modifies side effects 
tell looking implementation operation goes idea data types 
second problem subtle 
suppose process declares object type newtype shares child processes 
processes invokes implementation object invoke nested object 
problem operation may block 
violate rule operations allowed block initially 
situation equally unattractive options 
suspend process invoking allow processes access object 
means operation longer indivisible 

block calling process allow processes access object 
implies process suspended forever process able modify 
solve problem disallowing blocking operations nested objects requires looking implementation operation see may 
cooper hamilton observed similar conflicts parallel programming data abstraction context monitors 
propose extending operation specifications information implementation operation suspends side effects 
feel elegant concessions 
specification data type reveal information implementation 
solve problems refining execution model operations 
conceptually operation executed follows 
operation repeatedly tries evaluate guards tries execute statements successful guard 
evaluating guard operation conceptually creates copy entire object including nested deeply nested objects 
copy evaluation guard execution statements 
operation commits certain alternative soon 
guard succeeds evaluates true 
corresponding statements executed invoking blocking operations nested objects 
soon guard fails statements invoke blocking operation copy entire object thrown away alternative tried 
operation commit finished executing successful guard corresponding statements invoking blocking operations nested objects 
alternatives operation fail operation process invoking blocks object modified process 
operation commits certain alternative object assigned current value copy value evaluating selected guard statements 
scheme solves problems 
operation nested object inside guard code may side effects side effects permanent guard committed 
operation nested object may block 
long guards operation fail alternative containing invocation committed 
operation effects commits certain alternative 
commitment may try alternatives effects thrown away 
operation commits alternative guards statements alternative executed blocking 
operation invocations executed 
key issue implement execution model efficiently 
quite expensive copy objects trying alternative 
nearly cases compiler able optimize away need copying objects 
object types nested objects suffer problems described 
optimizing compiler check operation guard body side effect free nonblocking 
needs access implementation code nested objects 
different global optimizations inline substitution basically need access entire source program 
mechanism test circularities nested object definitions 
solution preserves abstraction programmer point view requires global optimizations efficient 
current orca compiler performs optimizations 
approach keeps language simple relies optimization techniques achieving efficiency 

data structures procedural languages data structures graphs trees lists built dynamically allocated deallocated blocks memory linked pointers 
distributed programming approach disadvantages 
main difficulty transmit complex data structure containing pointers remote machine 
pointers implemented addresses meaningful single machine need special treatment transmitted 
important languages consider graphs class objects hard determine transmitted 
addition problems giving programmer explicit control allocation deallocation memory usually violates type security 
programmer deallocate memory leading obscure bugs 
orca problems solved graph data type 
graph orca consists zero nodes having number fields similar fields record 
graph may contain global fields store information entire graph root tree head tail list 
individual nodes graph identified values type 
variable field type initialized nil indicates name node 
example binary tree type may defined follows type node bintree type bintree graph global field root node name root tree nodes fields node data integer node names left right sons program fragment declares graph type bintree 
node graph contains data field fields identifying left right sons node 
furthermore graph global field identifying root node tree 
tree data structure created declaring variable type 
initially tree empty nodes added deleted dynamically follows bintree node add node store name delete node name construct adds new node graph returns unique name chosen run time system 
run time system automatically allocates memory new node 
sense similar standard procedure new pascal 
crucial difference primitives construct specifies data structure new block memory intended 
pascal run time system orca keep track nodes belong certain graph 
information copy graph created example passed value parameter procedure remote process 
information delete entire graph procedure declared 
global fields graph fields nodes accessed designators similar records arrays root access global field data access data field node create left son store name left son note designator field node specifies name node graph 
notation differs pascal nodes identified pointers 
notation orca may somewhat cumbersome advantage clear data structure accessed 
possible represent index graph machine address 
transmitted remote machines losing meaning 
graphs orca type secure 
certain node deleted graph fields subsequently accessed run time error occurs illustrated piece code data causes run time error run time system checks graph contains node name 
furthermore invocation returns different name re denoting different node 
node deleted graph node cause run time error 
data structuring mechanism orca properties arrays properties pointer data structures 
mechanism supports dynamic allocation memory primitive 
graphs arrays class entities orca 
design advantages easily passed remote processes assignment defined graph variables functions may return value graph type graphs automatically deallocated enclosing procedure 
feature reduces need automatic garbage collection nodes 
orca safety advantages pointers array indices 
pointers manipulated arithmetic operations array indices illegal usage detected run time 
graph type orca disadvantages compared pointers 
pointers example data structures hooked single assignment statement 
graphs difficult 
programmer anticipates join data structures built single graph 
separate graphs copied 
disadvantage run time overhead graphs 
graph represented table pointers actual nodes nodes accessed indirectly table 
cost making graphs type secure node access validated 
currently working decreasing costs global optimizations 

example object type application section give example object type definition orca parallel application uses object type 
object defines generic job queue type operations add delete jobs 
parallel programs replicated workers paradigm 
paradigm master process repeatedly generates jobs executed workers 
communication master workers takes place job queue 
application parallel branch bound discussed 

example object type specification object type shown 
formal parameter represents type elements jobs queue 
generic type object specification operation job add job tail queue operation invoked jobs added operation job boolean fetch job head queue 
operation fails queue empty invoked 
generic fig 

specification part object type definition 
different operations defined job queues 
adds new job tail queue 
operation called jobs added queue master generated jobs 
operation tries fetch job head queue 
queue empty removes job queue returns parameter job operation returns true case 
queue empty operation applied queue operation fails returns false 
conditions queue empty invoked holds operation blocks true 
implementation part shown 
objects type contain variables boolean variable done variable type queue 
type defined graph global fields identifying element queue 
element contains element queue data formal type implementation uses straightforward list manipulation 
operation interesting 
contains guards reflecting conditions described 

example parallel application orca look example application orca traveling salesman problem tsp 
salesman initial city start list cities visit 
city visited 
objective find shortest path visits cities 
problem solved parallel branch bound algorithm 
algorithm implemented orca uses manager process generate initial paths salesman starting initial city visiting part cities 
number worker processes expand initial paths nearest heuristic 
worker systematically generates paths starting initial path checks better current shortest full path 
length current best path stored data object type see 
object shared worker processes 
manager worker processes communicate shared job queue shown 
orca code master worker processes shown 
master process creates initializes shared object minimum forks worker process processor 
subsequently generates jobs calling function shown forks worker process processor 
way job generation executes parallel worker processes 
final worker process created jobs generated job generation slowed competing process processor 
worker process repeatedly fetches job job queue executes calling function tsp 
tsp function generates routes start initial route 
initial route passed parameter longer current best route tsp returns immediately partial route lead optimal solution 
route passed parameter full route visiting cities new best route value minimum updated 
possible worker processes simultaneously detect route better current best route 
value minimum updated indivisible operation min checks new value current value object 
job queue empty jobs generated operation return false workers terminate 

distributed implementation orca orca language programming distributed systems communication model shared data 
implementation language hide physical distribution hardware simulate shared data efficient way 
implementations language 
implementation described generic object implementation type queue type queue graph queue represented linear list element queue nodes element queue data data contained element done boolean set true invoked 
queue queue operation job add job tail queue add new node return name data job fill data field new node field nil nil node 
assign global data field set predecessor field fi assign global data field operation invoked indicate jobs added done true operation job boolean try fetch job queue guard nil job available remove queue nil nil fi job data assign output parameter delete node queue return true succeeded fetching job od guard done nil return false jobs done od initialization code executed object creation 
done false initialize done false generic fig 

implementation part object type definition 
worker manager job job worker worker minimum fig 

structure orca implementation tsp 
manager workers processes 
data object shared processes 
minimum data object type read written workers 
replication reliable broadcasting 
briefly discuss second implementation section 
replication data fault tolerant systems isis increase availability data presence processor failures 
orca contrast intended fault tolerant applications 
implementation replication decrease access costs shared data 
briefly stated processor keeps local copy shared data object 
copy accessed processes running processor see 
operations change object called read operations copy directly messages sent 
operations change object called write operations broadcast new values operations processors updated simultaneously 
implementation best thought layer software system shown compiled application programs run time system reliable broadcasting top layer concerned applications written orca compiled machine code orca compiler 
executable code contains calls orca run time system example create manipulate processes objects 
middle layer run time system rts 
implements primitives called upper layer 
example application performs operation shared rts ensure system behaves object placed shared memory 
achieve rts processor maintains copies shared objects updated reliable broadcasting 
bottom layer concerned implementing reliable broadcasting type array integer integer type record len integer length partial route path partial route type distances table object new instantiation type process master minimum length current best path shared object job queue shared object integer distance table distances cities minimum assign max integer initialize minimum infinity 
fork worker processor current processor fork worker minimum distance od distance main thread generates jobs jobs generated fork worker minimum distance jobs generated fork worker cpu process worker minimum shared length current best path shared job queue distance distances cities job job jobs tsp job len job path minimum distance sequential tsp od fig 

orca code master worker processes tsp 
rts worry happens broadcast message lost 
far rts concerned broadcast error free 
job bottom layer 
describe protocols algorithms layer 
section structured top discuss applications layer rts layer reliable broadcast layer 
process process cpu copy process process cpu fig 

replication data objects distributed system 
top layer orca application programs application programs translated orca compiler executable code target system 
code produced compiler contains calls rts routines manage processes shared data objects complex data structures dynamic arrays sets graphs 
discuss operation invocations compiled 
described important distinguish read write operations objects 
compiler analyses implementation code operation checks operation modifies object applied 
languages optimization difficult implement 
consider example pascal statement containing indirect assignment pointer variable hard determine data structure affected statement 
orca problem name data structure programmer 
orca equivalent pascal code look explicitly specifies name data structure modified 
orca compiler determine operations modify object data structures 
compiler stores information operation descriptor 
descriptor specifies sizes modes input output parameters operation 
orca program applies operation object compiler generates call rts primitive invoke 
routine called follows invoke object operation descriptor parameters argument identifies object operation applied 
network assume target system contain multiple types cpus 
heterogeneous implementation orca conceivable address issue 
actual implementation somewhat complicated operation may multiple guards alternatives may read 
copy wide name object 
second argument operation descriptor 
remaining arguments invoke parameters operation 
implementation primitive discussed 

middle layer orca run time system middle layer implements orca run time system 
mentioned primary job manage shared data objects 
particular implements invoke primitive described 
efficiency rts replicates objects apply operations local copies objects possible 
different design choices related replication replicate objects synchronize write operations replicated objects update invalidate copies write operation 
looked alternative strategies 
rts described uses full replication objects updates replicas applying write operations replicas implements mutual exclusion synchronization distributed update protocol 
full replication scheme chosen simplicity performance applications 
alternative rts decide dynamically store replicas 
strategy employed implementation orca 
chosen update scheme invalidation scheme reasons 
applications objects contain large amounts data bit vector 
invalidating copy object wasteful time object replicated entire value transmitted 
second cases updating copy take cpu time network bandwidth sending invalidation messages 
presence multiple copies logical data introduces called inconsistency problem 
data modified copies modified 
updating done indivisible action different processors temporarily different values logical data unacceptable 
semantics shared data objects model define simultaneous operations object conceptually serialized 
exact order executed defined 
example read operation write operation applied object simultaneously read operation may observe value write intermediate value 
processes having access object see events happen order 
rts described solves inconsistency problem distributed update protocol guarantees processes observe changes shared objects order 
way achieve lock copies object prior changing object 
unfortunately distributed locking quite expensive complicated 
update protocol locking 
key avoid locking indivisible reliable broadcast primitive properties message sent reliably source destinations 
processors simultaneously broadcast messages say destinations receive receive 
mixed forms get get excluded software protocols 
primitive implemented bottom layer system described section simply assume indivisible reliable broadcast exists 
rts uses object manager processor 
object manager lightweight process thread takes care updating local copies objects stored processor 
objects replicas stored address space shared user processes 
user processes read local copies directly intervention object managers 
write operations shared objects hand broadcast object managers system 
user process broadcasts write operation suspends message handled local 
illustrated 
invoke obj op parameters op readonly check read operation set read lock local copy obj call op code obj parameters operation locally unlock local copy obj broadcast obj op parameters managers block current process fi fig 

implementation invoke run time system primitive 
routine called user processes 
object manager maintains queue messages arrived handled 
processors receive messages order queues managers managers may ahead handling messages head queue 
object manager processor handles messages queue strict fifo order 
message may handled soon appears head queue 
handle message obj op parameters message removed queue local copy object locked operation applied local copy copy unlocked 
message sent process processor manager unblocks process see 
receive obj op parameters set write lock local copy obj call op code obj parameters apply operation local copy unlock local copy obj local process unblock fi fig 

code executed object managers handling tion messages 
write operations executed object managers order 
read operation executed concurrently write operation read may executed write 
note agreement serialization principle described 

bottom layer reliable broadcast section describe simple protocol allows group nodes unreliable broadcast network broadcast messages reliably 
protocol guarantees receivers group receive broadcast messages receivers accept messages order 
main purpose section show protocol required semantics feasible going detail protocol 
current microprocessors lans lost damaged packets processor crashes occur infrequently 
probability error zero dealt 
reason approach achieving reliable broadcast normal case highly efficient expense making error recovery complex error recovery done 
basic reliable broadcast protocol works follows 
rts wants broadcast message hands message kernel 
kernel encapsulates ordinary point point message sends special kernel called sequencer 
sequencer node contains hardware kernel 
difference flag kernel tells process messages differently 
sequencer crash protocol provides election new sequencer different node 
sequencer determines ordering broadcast messages assigning sequence number message 
sequencer receives point point message containing allocates sequence number broadcasts packet containing broadcasts issued node sequencer 
assuming packets lost easy see rtss simultaneously want broadcast reach sequencer message broadcast nodes 
broadcast completed broadcast started 
sequencer provides global ordering time 
way easily guarantee atomicity broadcasting 
modern networks highly reliable perfect protocol deal errors 
suppose node misses broadcast packet due communication failure lack buffer space packet arrived 
broadcast packet eventually arrives kernel immediately notice gap sequence numbers 
expecting got knows missed 
kernel sends special point point message sequencer asking copies missing message messages missed 
able reply requests sequencer stores old broadcast messages history buffer 
missing messages sent directly process requesting 
practical matter sequencer finite amount space history buffer store broadcast messages forever 
discover machines received broadcasts including purge broadcast messages history buffer 
protocol ways letting sequencer discover information 
thing point point message sequencer broadcast request contains header field sequence number broadcast received sender message 
way sequencer maintain table indexed node number showing node received broadcast messages 
moment sequencer compute lowest value table safely discard broadcast messages including value 
example values table sequencer knows received broadcasts deleted history buffer 
node need broadcasting sequencer date idea broadcasts received 
provide information nodes quiet certain interval just send sequencer special packet acknowledging received broadcasts 
sequencer explicitly ask information runs history space protocol described method designed implemented protocol method send messages sequencer 
kernel sender immediately broadcasts message 
receiving kernel stores message sequencer broadcasts short message 
carry sequence numbers define ordering original messages 
kernel receives right line sequence number delivers original message application 
protocols guarantee semantics different performances different circumstances 
method message sent network twice sequencer sequencer kernels 
method uses bandwidth method message appears network generates interrupts uses broadcast messages sender kernels short message sequencer kernels 
implementation orca run time system method messages generated run time system short method steals computing cycles orca application handle interrupts 
philosophy protocol described somewhat resembles described chang maxemchuk differ major aspects 
protocol messages delivered user soon special node acknowledged message 
addition fewer control messages needed normal case lost messages 
protocol highly efficient normal operation packets needed assuming message fits single packet point point packet sender sequencer broadcast packet sequencer 
comparison protocol known protocols birman joseph garcia molina 

comparison rpc protocol described implementation orca full replication objects distributed update protocol indivisible broadcasting 
compare implementation partial replication remote procedure call rpc 
updating replicas rpc complicated indivisible broadcast 
problem replicas updated consistent way 
assure consistency rpc system uses phase update protocol 
phase copies updated locked 
updates acknowledged second phase begins copies unlocked 
protocol expensive broadcasting 
time update complete depends number copies 
sense partial replication strategy replicate objects needed 
rpc system maintains statistics number read write operations issued processor object 
information decides dynamically store object keep copies 
system dynamically migrate object create delete copies 
statistics impose overhead operations general savings communication time worth overhead 
cases rpc system communication costs broadcast system 
tsp program example far efficient update global bound variable single broadcast message multiple rpcs 
rpc system efficient read write ratio object low 
case broadcast system needlessly replicate object rpc system observe behavior decide dynamically replicate object 

performance example applications section take brief look performance example orca programs 
main goal section show realistic applications speedups obtained approach 
prototype distributed implementation layered approach described previous section 
prototype runs top amoeba system extended broadcast protocol described earlier 
implementation runs distributed system containing mc cpus running mhz connected mbit ethernet 
implementation uses ethernet multicast communication broadcast message group processors 
processors ethernet connected lance chip interfaces 
performance broadcast protocol ethernet system described 
time needed multicasting short message reliably processors msec 
receivers multicast takes msec 
high performance due fact earlier implementation protocol delay msec 
difference entirely due new routing protocol group communication protocol implemented 
amoeba kernel deal different kinds networks route messages dynamically multiple networks 
protocol optimized common case lost messages 
experiments described number lost messages zero 
implementation developing parallel applications written orca 
small larger 
largest application currently parallel chess program consisting lines code 
addition tsp smaller applications include matrix multiplication prime number generation sorting 
give performance measurements sample programs running ethernet implementation 

parallel traveling salesman problem application traveling salesman problem tsp described section 
program uses shared objects job queue containing length current best path see 
clear reading current best path length done local operation communication overhead 
updating best path happens requires broadcast message 
updates best path happen infrequently important broadcast improvements immediately 
worker uses old inferior value best path investigate paths pruned new value known 
words worker search nodes necessary 
search overhead may easily dominating factor cause severe performance degradation 
performance traveling salesman program randomly generated graph cities 
implementation achieves speedup close linear 
cpus times faster cpu 

parallel pairs shortest paths problem second application describe pairs shortest paths problem asp 
problem desired find length shortest path node node graph 
parallel algorithm similar parallel version floyd algorithm 
distances nodes represented matrix 
processor computes part result matrix 
algorithm requires nontrivial amount communication synchronization processors 
performance program graph nodes 
parallel algorithm performs iterations iteration array integers sent processor processors 
spite high communication overhead implementation performance 
cpus achieves speedup 
main reasons performance broadcast messages transferring array processors 

successive overrelaxation tsp asp benefit broadcasting 
consider application needs point point message passing 
application successive overrelaxation sor iterative method solving discretized laplace equations grid 
speedup 
perfect speedup speedup orca 
number processors fig 

measured speedup orca implementation traveling salesman problem 
speedup 
perfect speedup speedup orca 
number processors fig 

measured speedup orca implementation pairs shortest paths problem 
iteration algorithm considers non boundary points grid 
point sor computes average value neighbors updates point value 
parallelized sor partitioning grid regions assigning regions different processors 
partitioning grid iteration processor needs exchange values processors 
parallel algorithm needs point point message passing 
current prototype implementation orca communication broadcasting 
message passing simulated orca having sender receiver share buffer object 
shared objects updated broadcasting processors receive update message 
sor worst case example system 
speedup 
perfect speedup speedup orca 
number processors fig 

measured speedup orca implementation successive tion 
measured speedup sor shown 
despite high communication overhead program achieves reasonable speedup 
speedup cpus 
related section compare language related languages systems 
particular look objects parallel object languages linda tuple space shared virtual memory 
objects objects object languages parallel distributed programming emerald amber alps 
objects languages typically parts 
encapsulated data 

manager process controls access data 
data accessed sending message manager process asking perform certain operation data 
objects contain process data said active 
sense parallel object languages allow processes objects share data objects semantics closer message passing shared variables 
access shared data full control manager process 
alps example operations object go manager process determines order operations executed 
way implement model store object specific processor manager process translate operations object remote procedure calls manager process 
model centralized control 
objects orca purely passive contain data manager process 
access control shared data objects distributed basically determined rules 
operations executed 

operations blocked guards false 
model implemented replicating data objects multiple processors discussed section 
read operations applied local copy message passing involved 
processes located different processors apply read operations simultaneously losing parallelism 
linda tuple space linda languages recognize disadvantages central manager processes guarding shared data 
linda supports called distributed data structures accessed simultaneously multiple processes 
contrast object languages typically serialize access shared data structures 
linda uses tuple space model implementing distributed data structures 
general distributed data structures linda built multiple tuples 
different tuples accessed independently processes manipulate different tuples data structure simultaneously 
principle multiple read operations tuple executed simultaneously 
tuples conceptually modified tuple space modifications tuple executed strictly sequentially 
idea distributed data structures appealing think support tuple space implementing data structures important disadvantages 
distributed data structures built single tuples mutual exclusion synchronization done automatically 
operations complex data structures built multiple tuples synchronized explicitly programmer 
essence tuple space supports fixed number built operations executed support building complex indivisible operations low level 
orca hand programmers define operations arbitrary complexity shared data structures operations executed mutual exclusion synchronization done automatically run time system 
means job implementation compiler run time system see operations executed parallel executed sequentially 
discussed way doing distinguishing read write operations executing reads parallel local copies advanced implementations feasible 
shared virtual memory shared virtual memory svm simulates physical shared memory distributed system 
partitions global address space fixed sized pages just virtual memory 
processor contains portion pages 
process tries access page gets page fault operating system fetch page located 
read pages may shared multiple processors 
writable pages reside single machine 
shared 
processor needs modify page invalidate copies page processors 
important differences implementation model svm 
svm partly implemented inside operating system mmu registers 
orca broadcast protocol implemented software outside operating system 
difference gives svm potential performance advantage 
model important advantages svm 
shared data objects accessed defined high level operations svm accessed low level read write instructions 
consequently choice invalidating objects write operation updating applying operation copies alternatively sending new value 
svm choice invalidating pages viable 
cases invalidating copies far efficient updating 
researchers tried solve performance problem relaxing consistency constraints memory 
weakly consistent memory models may better performance fear ruin ease programming dsm designed place 
orca intended simplify applications programming orca programmers worry consistency 
may investigate compiler able relax consistency transparently done munin system 
second important difference orca svm granularity shared data 
svm granularity page size fixed 
orca granularity object determined user 
svm single bit page modified page invalidated 
property leads wellknown problem false sharing suppose process repeatedly writes variable process repeatedly writes happen page page continuously moved resulting thrashing 
different pages thrashing occur 
svm transparent programmer control allocation variables pages 
orca problem occur separate objects treated independently 
detailed comparison shared virtual memory 

described new model language parallel programming distributed systems 
contrast models distributed programming model allows processes different machines share data 
key idea model encapsulate shared data data objects access objects user defined operations 
advantages approach programmer implementer summarized 
operations objects executed mutual exclusion synchronization done automatically simplifies programming 
condition synchronization integrated model allowing operations suspend 
mechanism suspending operations easy visible implementer operations users 
implementation model takes care physical distribution shared data processors 
particular implementation replicates shared data process directly read local copy processor 
write operation replicas updated broadcasting operation 
update strategy possible shared data accessed user defined operations 
svm example efficiently update replicas write operation logical write operation may require machine instructions modifying memory 
updating memory broadcasting machine instructions highly inefficient communication overhead instruction enormous 
defined language orca shared data objects 
design orca avoids problems distributed languages pointers global variables 
major goal design keep language simple 
particular examples simplifying language design having compiler certain optimizations 
studied distributed implementation orca 
implementation runs collection processors connected broadcast network 
looked implementations orca systems hypercubes 
implementation feasible orca language depend network topology 
port orca architectures new run time system probably new replication strategy needed language application programs changed 
approach best suited moderate grained parallel applications processes share data read frequently modified infrequently 
example tsp program uses shared object read frequently changed times 
program shows excellent performance 
applications benefit efficient broadcast protocol implementation 
usefulness broadcasting demonstrated asp program 
think orca useful language writing parallel programs distributed systems 
shown language efficient range applications 
wim van implementing orca compiler erik fred douglis arnold geels anonymous referees giving useful comments 

tanenbaum van renesse van staveren sharp mullender jansen van rossum experiences amoeba distributed operating system comm 
acm pp 
dec 

bal van renesse tanenbaum implementing distributed algorithms remote procedure calls proc 
afips nat 
computer conf chicago ill pp 
afips press june 

bal programming distributed systems silicon press summit nj 

bal tanenbaum distributed programming shared data proc 
ieee cs int 
conf 
computer languages miami fl pp 
oct 

bal kaashoek tanenbaum experience distributed programming orca proceedings ieee cs international conference computer languages new orleans la pp 
march 

li hudak memory coherence shared virtual memory systems proc 
th ann 
acm symp 
princ 
distr 
computing calgary canada pp 
aug 

ghezzi jazayeri programming language concepts john wiley new york ny 

birrell nelson implementing remote procedure calls acm trans 
comp 
syst 
pp 
feb 

bal steiner tanenbaum programming languages distributed computing systems acm computing surveys sept 

forin architectural support multilanguage parallel programming heterogenous systems proc 
nd int 
conf 
architectural support programming languages operating systems palo alto calif pp 
oct 

li ivy shared virtual memory system parallel computing proc 
int 
conf 
parallel processing vol 
ii st charles ill pp 
aug 

felten otto highly parallel chess program proc 
int 
conf 
fifth generation computer systems tokyo pp 
nov 

cheriton preliminary thoughts problem oriented shared memory decentralized approach distributed systems acm operating systems review pp 
oct 

ahuja carriero gelernter linda friends ieee computer pp 
aug 

jul levy hutchinson black fine grained mobility emerald system acm trans 
comp 
syst 
pp 
feb 

liskov distributed programming argus commun 
acm pp 
march 

andrews olsson nilsen townsend overview sr language implementation acm trans 
program 
lang 
syst 
pp 
jan 

bal evaluation sr language design report ir vrije universiteit amsterdam august 

hoare monitors operating system structuring concept commun 
acm pp 
oct 

andrews schneider concepts notations concurrent programming acm computing surveys pp 
march 

eswaran gray lorie traiger notions consistency predicate locks database system commun 
acm pp 
nov 

lucco parallel programming virtual object space sigplan notices proc 
object oriented programming systems languages applications orlando fl pp 
dec 

cooper hamilton preserving abstraction concurrent programming ieee trans 
softw 
eng 
se pp 
feb 

wirth programming language pascal acta informatica pp 


joseph birman low cost management replicated data fault tolerant distributed systems acm trans 
comp 
syst 
feb 

bal kaashoek tanenbaum jansen replication techniques speeding parallel applications distributed systems report ir vrije universiteit amsterdam netherlands oct 

chang maxemchuk reliable broadcast protocols acm trans 
comp 
syst 
pp 
aug 

birman joseph reliable communication presence failures acm trans 
comp 
syst 
pp 
feb 

garcia molina message ordering multicast environment proc 
th int 
conf 
distr 
comp 
syst newport beach ca pp 
june 

kaashoek tanenbaum group communication amoeba distributed operating system th int conf 
distributed computing systems arlington texas pp 
may 

metcalfe boggs ethernet distributed packet switching local computer networks commun 
acm pp 
july 

kaashoek tanenbaum flynn hummel bal efficient reliable broadcast protocol acm operating systems review pp 
oct 


sahni pairs shortest paths hypercube multiprocessor proc 
int 
conf 
parallel processing st charles ill pp 
aug 

chase lazowska levy amber system parallel programming network multiprocessors proc 
th acm symp 
operating system principles park az pp 
dec 

synchronization scheduling alps objects proc 
th int 
conf 
distributed computing systems san jose ca pp 
june 

kaashoek bal tanenbaum experience distributed data structure paradigm linda workshop experiences building distributed multiprocessor systems ft lauderdale fl 
oct 

farber reducing host load network load latency distributed shared memory proc 
th int 
conf 
distributed computing systems paris pp 
may 

ahamad slow memory weakening consistency enhance concurrency distributed shared memories proceedings th international conference distributed computing systems paris pp 
may 

bennet carter zwaenepoel munin distributed shared memory type specific memory coherence proceedings nd symposium principles practice parallel programming seattle wa march 

levelt kaashoek bal tanenbaum comparison paradigms distributed shared memory ir vrije universiteit amsterdam netherlands august 
