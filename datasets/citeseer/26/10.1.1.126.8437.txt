algorithmic architectural optimizations computationally efficient particle filtering student member ieee srivastava member ieee rama chellappa fellow ieee analyze computational challenges implementing particle filtering especially video sequences 
particle filtering technique filtering non linear dynamical systems driven non gaussian noise processes 
wide spread applications detection navigation tracking problems 
general particle filtering methods yield improved results difficult achieve real time performance 
analyze computational drawbacks traditional particle filtering algorithms method implementing particle filter independent metropolis hastings sampler highly amenable pipelined implementations parallelization 
analyze implementations proposed algorithm particular concentrate implementations minimum processing times 
shown design parameters fastest implementation chosen solving set convex programs 
proposed computational methodology verified cluster pcs application visual tracking 
demonstrate linear speedup algorithm methodology proposed 
index terms particle filter resampling mcmc variable design methodologies visual tracking filtering problem estimation unknown quantity usually referred state set observations corrupted noise applications broad spectrum real life problems including gps navigation tracking specific nature estimation filtering problem depends greatly state need estimate evolution state time relation state observations sources noise 
generally analytical solutions estimation possible constrained special scenarios 
example kalman filtering optimal analytic filter models linear corrupting noise processes gaussian 
non linear systems driven non gaussian processes extended kalman filter iterated extended kalman filter approximations optimal filtering scheme 
popular tool solving inference problems non linear systems particle filtering 
particle filtering applied wide variety problems tracking navigation detection chellappa center automation research electrical computer engineering department university maryland college park md 
email rama umd edu srivastava electrical computer engineering department university maryland college park md 
email glue umd edu partially supported nsf itr task order arl monitored science 
video object recognition 
generality particle filters comes sample particle approximation posterior density state vector 
allows filter handle non linearity system non gaussian nature noise processes 
resulting algorithm computationally intensive results need efficient implementations algorithm tuned specifically hardware multi processor implementations 
methods algorithmic hardware implementations particle filtering proposed literature 
authors identify resampling algorithms main computational step algorithm completely independent underlying application 
propose new resampling algorithms reduce complexity hardware implementations 
architectures efficient distributed pipelined implementations fpgas proposed 
detailed analysis basic problem addressing hardware software issues 
resampling algorithms modifications basic systematic resampling algorithm creates bottle streamlined implementation 
authors propose methodology overcome limitation basic theory alternate resampling algorithm similar monte carlo markov chain mcmc tracker interacting targets video 
number resampling schemes proposed literature 
liu chen list compare number schemes 
sufficient interest relevance called local monte carlo methods described 
motivation specifically analyzes computational challenges implementations particle filters provides general design methodology particle filtering pipelining parallelization constructs commonly hardware multi processor systems 
particle filtering involves main modules proposition weight evaluation resampling modules 
standard implementations particle filtering typically commonly known systematic resampling sr 
systematic resampling poses significant challenge pipelined implementations weights computed weight computation stage cumulative sum weights available 
means pipelined implementation start resampling weights computed 
increases latency implementations 
algorithmic implementation schemes particle filters speeding basic computations making particle filtering solutions amenable real time constraints 
demonstrate computational methodology need knowledge cumulative sum weights removed 
implies contrast traditional particle filtering implementation proposed approach suffer bottlenecks pipelining 
allows speedup filter reduce latency pipelining parallelization 
demonstrate performance implementations cluster pcs 
allows achieve speedups linear number cluster nodes 
specific contributions address computational challenges hardware multi processor implementations particle filters 
regard contributions 
algorithmic enhancements order avoid sr step propose independent metropolis hastings algorithm resampling 
show algorithmic modification amenable pipelining parallelization 
auxiliary particle filtering show problems associated proposed methodology reduced auxiliary particle filters 
allows complete freedom choice proposal density important design issue 
minimum time implementations pipeline able parallel architectures implementing proposed algorithm 
formulate set convex programs obtaining design specification fastest implementation algorithm 
prove constraint execution speed algorithm minimum resources required implementation formulated convex program 
analyze pipelining proposed implementation cluster pcs tracking vehicle video stream 
achieve speedups computation linear number cluster nodes 
rest organized follows 
traditional particle filtering algorithm section ii 
section iii mcmc sampling theory propose computational methodology section iv 
section analyzes implementations proposed methodology 
section vi demonstrate performance proposed implementations problem tracking videos cluster pcs 
ii 
particle filtering particle filtering address problem bayesian inference dynamical systems 
rd rp denote state space observation space system respectively 
xt denote state time yt noisy observation time model state sequence xt markovian random process 
assume observations yt conditionally independent state sequence 
assumptions system completely characterized xt xt state transition probability density function describing evolution system time alternatively described state transition model form xt xt nt nt noise process 
yt xt observation likelihood density describing conditional likelihood observation state 
relationship form observation model yt xt noise process independent nt 
prior state probability 
statistical descriptions models noisy observations interested making inferences state system current time 
specifically observations till time 
yt estimate posterior density function xt 
posterior aim inferences ft form ft ft xt ft xt xt dxt ft function interest 
example inference conditional mean ft xt xt 
markovian assumption state space dynamics conditional independence assumption observation model posterior probability recursively estimated bayes theorem xt yt xt xt xt xt dxt yt note unknowns terms specified computable posterior previous time step 
problem computation including integrations need analytical representation 
foregoing requirement analytic solution particle filtering approximates posterior discrete set particles samples associated weights suitably normalized 
approximation posterior density xt xt xt dirac delta function centered xt 
set st weighted particle set represents posterior density time estimated recursively st 
initial particle set obtained sampling prior density 
discuss called importance function xt xt yt easy sample function support encompasses estimation ft defined recast follows ft ft xt xt xt xt yt xt xt yt dxt ft xt xt xt xt yt dxt xt defined called importance weight wt xt xt xt yt particle filters sequentially generate st st steps importance sampling sample xt yt 
step called proposal step called proposal density 
computing importance weights compute unnormalized importance weights yt 

yt normalize weights obtain normalized weights 

inference estimation estimate inference ft ft ft sequence performed time iteration get posterior time step 
basic problem algorithm suffers time steps importance weights go zero 
weights remain zero time instants result contribute estimation ft 
practically degeneracy undesirable waste computational resource 
avoided resampling step 
resampling essentially replicates particles higher weights eliminates low weights 
done ways 
list resampling algorithms 
popular originally proposed samples particles set samples generated proposal multi nomial distribution parameters get new set particles st iteration uses new set st sequential estimation 
discuss additional sampling algorithms ii 
choice importance function crucial performance filter choice importance function xt xt yt 
ideally importance function close posterior 
choose xt xt yt yt xt xt xt obtain importance weights wt identically equal variance weights zero 
applications density function easy sample 
largely due non linearities state transition observation models 
popular choice state transition density xt xt importance function 
case importance weights wt wt yt xt choices include cleverly constructed approximations posterior density 
resampling algorithms particle filtering algorithm resampling step introduced address degeneracies resulting due importance weights getting skewed 
resampling algorithms sr technique popularly 
basic steps sr 

sample 
choice 
new particle associated weight resampled particle set st resampling scheme 
choices discussed 
particle filtering algorithms sequential importance sampling sis sr collectively called sisr algorithms 
computationally sr tricky step requires knowledge normalized weights 
resampling sr start particles generated value cumulative sum known 
basic limitation overcome proposing alternative techniques 
iii 
independent metropolis hastings algorithm section introduce monte carlo sampling techniques discuss detail metropolis hastings algorithms derivative independent metropolis hastings algorithms 
redesign basic particle filtering algorithm techniques sampling 
particle filtering special case general mcmc density sampling techniques specifically suited dynamical systems 
metropolis hastings algorithm considered general mcmc sampling 
popular samplers metropolis sampler gibbs sampler special cases algorithm 
particle filter address issue generating samples distribution functional form known upto normalizing factor difficult sample 
section hybrid sampler uses sampling methodologies adopted mcmc samplers specifically algorithm problem estimating posterior density functions 
show scheme computationally favorable systematic resampling 
metropolis hastings algorithm general theory mcmc sampling algorithm state conditions general theory fits particle filtering algorithm 
generates samples desired density say generating samples easy sample proposal distribution say produces sequence states construction markovian nature iterations 
initialize chain arbitrary value 
user specified 
generate sampling proposal function 
accept probability defined min uniform random variable mild regularity conditions shown markov chain constructed converges invariant distribution independent value chosen initialize chain 
generate monte carlo markov chain invariant distribution distribution 
initial phase chain said transient state due effects initial value chosen 
sufficient samples effect starting value diminishes ignored 
time chain transient state referred burn period 
usually dependent desired function proposal function importantly initial state 
cases estimation period difficult 
usually easier conservative guess 
heuristics estimate number burn samples say nb 
samples burn period discarded 
independent metropolis hastings algorithm independent metropolis hastings algorithm special case general proposal function set 
proposal function independent previously accepted sample chain 
mean acceptance probability proposal chain min imh algorithm strong convergence properties 
mild regularity conditions shown converge uniform rate independent value initialize chain 
study convergence properties 
sisr algorithms designed generate samples probability density function sisr suited specifically sequential nature dynamical systems 
regard key difference sisr algorithm lies fact sisr algorithm requires knowledge cumulative sum weights term 
important cumulative sum computed weights corresponding particle set known 
sr particles generated weights computed 
contrast poses bottlenecks 
section exploit property design filter suffer bottle introduced sr iv 
proposed methodology bottlenecks introduced sr technique overcome resampling 
basic issues needs resolved achieve 
generation particles importance sampling works differently algorithms 
particle filtering allows importance function defined locally particle 
mathematically ith particle time generated importance function represented xt yt parametrized poses problem application estimate posterior concept importance functions associated particle extend 
contrast algorithm requires importance function depend functionally accepted sample chain case importance function remains 
set unweighted samples 
sampled posterior density xt time approximate posterior xt xt xt dirac delta function xt 
approximate posterior time xt yt xt xt yt sampling density performed 
issue choice importance function arises 
importance function typically reflects exploits knowledge application domain clever approximation posterior 
reason reuse importance function corresponding underlying model 
keeping mind propose new importance function form xt yt xt yt note xt qualifies importance function dependence state variable 
sample xt yt need sample 
sample yt 
sampling done deterministically ease sampling uniform densities finite discrete spaces 
new importance function functionally different sisr algorithm generated particles identical 
algorithm proceeds similar 
propose particles new importance function xt 
acceptance probability takes form xt min xt xt yt xt xt xt yt choice importance function state transition model xt xt yt xt xt acceptance probability ratio likelihoods min yt yt avoid systematic resampling traditional particle filtering algorithms 
intuition generate unweighted particle set stream desired posterior 
unweighted particle set st contains particles approximating posterior time xt 
aim estimate approximation posterior time algorithm initialized containing samples prior 
main steps stated importance sampling step generate nb indices 
nb uniformly set 
nb estimate burn period number particles required 
uniform density 
importance sampling step particle set st 
time propose nb particles form set st 
nb rule yt compute importance weights particle st evaluate importance weights 
inference estimate expected value functions interest 
compute ft nb nb note samples discarded burn computation unnormalized particle set 
nb properly weighted normalized 
mcmc sampler imh sampler parse set st generate new unweighted set particles steps 
initialize chain particle proposed 

nb prob 
prob 
acceptance probability defined 
discarding nb samples burn remaining samples form st nb 
approximation xt 
compare algorithm classical sisr discussed section ii 
note sisr algorithm involves weight normalization step equation 
proposed algorithm works ratios unnormalized weights requires normalization 
allows advantages proposed methodology imh sampler works ratios importance weights 
obviates need knowledge normalized importance weights unnormalized weights 
allows imh sampler start parsing particles generated wait entire particle set generated importance weights computed 
contrast sisr resampling particles generated cumulative sum normalized weights known 
ability resample particles generated allows faster implementations 
analyzed section drawbacks proposed framework proposed framework overcomes drawbacks sisr algorithm adopting mcmc sampling strategy opposed traditional sr technique 
new framework introduce extra computations add increased complexity 
discuss drawbacks alternate formulation circumvent issue 
consider expression weight computation 
expression involves computing summations xt xt yt require ad ditional computation time 
computation terms severe bottleneck easily pipelined 
proposal density matches state transition model terms cancel 
possible circumvent problem auxiliary particle filtering paradigm 
auxiliary particle filters auxiliary particle filtering refers techniques extend state space problem include particle index 
consider new state space xt 
denotes particle index 
posterior defined yt xt xt marginalizing state gives expression xt 
assume sample joint space proposal xt yt xt yt 
unnormalized weights constructed yt xk yt resample mcmc chain expression acceptance probability remains ratio unnormalized weights 
inference step marginalize particle index state easy see marginalization identical discarding particle index information particle nature particle representation underlying density 
nutshell auxiliary variable allows completely avoid summation associated computational cost 
exist choices proposal density extended state space 
discussion 
implementation proposed methodology section approaches implementing theory section iii 
assume basic computational blocks importance sampling computation importance weight parsing particles imh algorithm available 
blocks propose implementations sequential implementation parallel implementations 
sequential implementation st fig 

sequential implementation expected inference estimation block proposal weight calc imh chain st illustrates straight forward implementation proposed algorithm 
consists blocks 
proposal block proposal block takes st particles previous time step proposes new particles particle time sampling proposal function 
algorithm amounts generating uniform number 
randomly pick particle st say 
particle obtained sampling xt yt 
assume blocks proposes particle time 
auxiliary variable framework involves sampling state associated particle index state proposal function xt yt 
weight calculator block implementation auxiliary variables 
imh chain block implementation acceptance probability calculated new particle previously accepted particle 
uniform random number generated smaller new particle retained st accepted particle chain replicated 
inference estimation block block estimates inference function equation 
computation performed parallel imh chain effect computation 
characteristics basic implementation follows 
sequential processing particles block implementation processes particle time 
process particles block needs run times 
note need generate particles represent posterior density iterate nb times nb burn period 
particles imh chain sample set st pipelining pipelining blocks processing block overlap time leading increase throughput system 
computation time estimate time required process nb particles implementation 
suppose target application proposal block generate particle tp time units 
weight computation block generates weight particle tw time units imh chain process particles td time units 
assume time required process constrained inference block ignored analysis 
setting compute total time required process particles 
implementation take tp tw td time units produce particle 
able produce particle max td tp tw time units 
total latency generating nb particles nb max td tp tw tp tw td time units 
basic sequential implementation faster replicating proposal weight computation imh chain blocks 
order exploit parallelism processing particles refinement sequential implementation 
parallel implementation single chain illustrates parallel implementation proposed algorithm 
retain single imh chain proposal weight computation blocks replicated 
having multiple imh chains introduces additional st proposal proposal proposal fig 

parallel implementation single imh chain 
buffer weight calc weight calc weight calc weight calc issues involving burn chain 
reason restrict single chain implementations 
relax restriction section 
number proposal blocks rp number weight computation blocks rw 
compute total time required process particles function rp rw latency blocks tp tw td 
choose specific values rp rw achieve smallest total processing time 
total computational time determined bottlenecks processing created due differing rates processing particles stage 
rate proposal blocks process particles rp tp weight computation blocks rw tw blocks td 
total computational time predominantly dependent rates smallest 
case rp tp rw tw td 
scenario proposal blocks smallest rate processing followed weight computation blocks 
suppose need process particles proposal blocks need rp tp time units process particles 
weight computation processing happen parallel 
quicker processing rate weight computation time set rp particles processed proposal blocks earlier particles processed weight computation blocks 
amount time required process set rp particles weight computation blocks block rw 
allowing rp rw take values real line just positive integers total time processing rp rw rp tp rp tw rw interested computing values rp rw minimize keeping mind solutions satisfy assumptions case note rp rw take positive values 
allows natural change coordinate frames form rp log rp rw log rw terms rp rw expression written rp rw qt rp rp rw rw buffer chain inference constraints minimization come assumptions ordering rates case rp tp rw log tw rw tw log rp rw naturally bounded value leads convex optimization problem inequality constraints stated min rp rw rp rw rp rp rw rw rp rw log rw log tw td tp td st rp log tw rw log note expression convex rp rw 
inequality constraint convex rp rw 
host techniques designed specifically convex optimization 
case rw tw rp tp rw tw td line reasoning identical case derive expression amount time needed process particles function rp rw 
rp rw rw tp rp tw rw note value rp greater rw impractical leading constraint rp form rp rw 
recast set equations terms rp rw defined get cost constraint equations 
min rp rw rp rw tpe rw rp rw rw rw rp log rw log tw td tw rw log tp rp rw cost function inequality constraints convex rp rw 
case rp tp td rw tw case main bottleneck proposal block followed imh chain 
accordingly total time processing particles rp rw tp tw rp transformation variables write expressions cost constraints 
min rp rw rp rw rp rp rw rw rp rw log rw log tw td tp rp log tw rw log cost inequality constraints convex rp rw 
case td min rp tp rw tw td final scenario main bottleneck imh chain 
expression total time rp rw tp tw dependent choice rp rw 
feasibility set forms solution set optimize minimum processing time 
completeness formulate convex program cost constraints 
rw rp rw tp tw tp rp log rw log td tw td rp log rw log stated case points feasible set form solution set 
depending exact location bottle neck possible upto different scenarios 
scenarios collapse identical expressions total cost leading cases discussed 
expressions cost associated constraints summarized table note case results convex cost function convex inequality constraints 
allows design algorithm determining global minima total computation time processing particles values tp tw td 
values td tw formulate convex programs associated cases illustrated table solve convex program obtain minimum times min associated values rp rw 
choose configuration gives total processing time 
algorithm allows obtain design specifications minimum processing time values tp tw td note basic computation tools optimization techniques convex programs 
convex optimization studied problem techniques solve convex programs efficiently reliably 
convex programs desirable properties respect local minima 
local minima global minima set local global minima form convex set 
note analytic solutions convex program highly dependent individual values tp tw td 
possible convex program may unique solutions 
ambiguity choice rp rw solution set resolved additional considerations resource energy constraints 
noted set solutions convex program convex 
property effectively design alternate cost functions resolve ambiguity choice rp rw 
parallel implementation multiple chains st proposal proposal proposal proposal proposal proposal buffer buffer weight calc weight calc weight calc weight calc weight calc weight calc weight calc weight calc buffer buffer imh chain imh chain fig 

parallel implementation multiple imh chains 
inference shows parallel implementation proposed algorithm multiple imh chains 
implementation basically replicates structure proposed multiple times 
implementation gives speedup proportional number imh chains 
number imh chains 
implementation generate set st particles chain need generate particles excluding required burn leading total nb particles imh chain 
time required obtaining particle set equal time required process nb particles implementation 
easily compute total time required generate st different scenarios analysis restricting total number particles imh chain nb vi 
experimental verification design methodologies proposed verified applications synthetic example originally discussed problem visual tracking testbed umiacs red blue cluster 
red cluster consists pii mhz pcs running redhat pc having ram gb 
blue cluster consists piii mhz 
mpich implementation message passing interface mpi communication threads 
chose implement multi processor cluster framework underlying theory applies hardware design clusters 
general mpi large overheads overheads common identical st case rate ordering cost constraint rp tp rw tw td rp rp rw rw rw tw min rp tp rw tw td tpe rw rp rw rw rp tp td rw tw rp tw rp td min rp tp rw tw td tp tw rp rw log tp tw tw rp log rw log rw log td rw tw rp log rp log tp rw tw log rw rp td rp tp log rp log td tw rw log rw log td tp rp log rp log td tw rw log rw log table expressions total time taken process particles bottlenecks various stages pipeline 
sisr mcmc schemes 
experimental observations remain 
mentioned earlier computation burn period hard problem 
sequential estimation proposal density general guess posterior 
cases adverse effects burn period reduced 
experiments set nb 
visual tracking implemented particle filter online tracking algorithm red cluster 
discuss finer details filter implementation 
model details summarize tracking algorithm detailing computational aspects 
typical tracking example shown 
models defining dynamical system described 
state space state xt dimensional vector defining affine deformations rectangular template 
state transition model simple random walk model gaussian noise model state transition 
xt xt nt nt observation model frame video time forms observation 
likelihood model yt xt involves comparing appearance model suitably deformed state xt observation yt 
appearance model employs mixtures gaussians mixtures model appearance 
value xt defines patch parallelogram shaped image yt 
zt yt xt patch defined xt yt 
yt xt yt zt proposal density algorithm uses proposal density state transition model 
implementation details estimate tp tw td obtained running block single pc times averaging individual runs 
tp tw ms td see main computational bottleneck particular application evaluation weights 
weight computation unusually large latency primarily involves retrieval memory 
particle compute weight need obtain template yt 
involves retrieval elements memory containing current frame 
evaluation involves evaluation mixture gaussian far complicated simple proposal blocks 
reason fixed rp rd analyzed performance architecture various values rw 
implementation proposed methodology cluster follows 
block shown assigned cluster node total rp rw rd cluster nodes employed rp performing particle proposal rw computing weights rd imh chains 
communication pcs performed mpich libraries 
holding values rp rw unity rp rd tracker tested varying number weight computation blocks 
comparison purposes implemented traditional sisr specifications proposed methodology 
main difference node performed resampling wait till particles delivered weight computation blocks starting sr algorithm 
results algorithm process frames video sequence tracking car 
shows typical tracking results 
filter run particles number cluster nodes weight computation rw varying 
rw corresponds sequential implementation rw corresponds parallel implementation single chain 
setup tried implementation sisr replacing imh chain systematic 
main difference algorithms systematic particles processed normalized weights known 
shows actual time taken seconds process frames video particles proposed algorithm sisr 
note decay exhibited time taken proposed algorithm 
fig 

frames tested set 
output tracker top time sisr particles sisr particles prop 
algo particles prop 
algo particles number cluster nodes timing speedup sisr particles sisr particles prop 
algo particles prop 
algo particles number cluster nodes speedup fig 

left actual time seconds taken process frames filter particles varying number rw 
right speedup obtained replication weight computation node 
note linear speedup obtained proposed algorithm 
shows speedup algorithm add computing nodes 
behavior translates linear increase speedup number processing nodes 
plots demonstrate proposed algorithm 
seen speedup number cluster nodes increases 
attributed increasing communication delays nodes 
standard models communication delays mpich 
processors inter processor communication dominant source delay parallelization help 
synthetic example applied design methodology implementation strategies proposed synthetic example 
problem specifications introduced 
system scalar state space state transition model defined xt xt xt cos wt wt observation model equation yt vt vt estimate times tp tw td proportion 
filtering particles formulate solve convex programs 
convex programs solved matlab optimization toolbox 
constraints active constraints satisfied equality feasible point called active minima noted give qualitative interpretation result 
case minimum achieved active constraints 
rp tp rw tw td note rate balancing condition 
corresponding minimum time min tp tw case minimum achieved boundary active constraints 
rw rp rw tw td gives minimum time min tp tw case note cost function independent rw 
minimum achieved active constraint 
giving minimum time rp tp td min tw tp case cost function constant feasible set 
minimum time min tp tw turns convex program give minimum time corresponds solution rates balanced 
interesting balanced rates intuitive appeal 
implemented filters tested red blue clusters 
filters sisr imh resampling proposal density state transition model 
third filter auxiliary particles complicated proposal density defined follows xt xt yt xt yt xk yt yt cos particular proposal density samples auxiliary state randomly mixes observation predicted state concentrate particles near posterior modes 
shows actual time computation achieved speedup parallelization filters tested clusters 
tested algorithm varying rp bottle neck initially proposal stage 
rp tp td bottleneck shifts imh sampler increase value rp produce significant gains processing time 
reflected saturation plots associated proposed algorithm imh 
contrast sisr resampling begins particles generated 
time processing scale 
auxiliary particle filtering scales linearly number processing nodes offers best speedup 
resampling method associated implementation schemes proposed allows pipeline free bottle 
implementations proposed methodologies show speedups increases linearly number processing nodes utilized 
allows parallelize algorithm achieve desired runtime rate 
contrast implementations sisr scale easily number processing nodes 
vii 
address computational challenges implementing particle filters 
provide methodology uses independent metropolis hastings sampler 
shown traditional bottleneck introduced systematic removed 
allows bottle neck free pipelined implementation 
proposed algorithm works independent underlying application 
auxiliary filter paradigm obtain alternate design suffer complexity presence arbitrary proposal function 
set convex programs compute design specifications terms resources employed stage processing achieve minimum time required process certain number particles 
validate propositions cluster pcs problem visual tracking show implementations proposed methodology achieve speedup linear number processing elements 
kalman new approach linear filtering prediction problems transactions asme journal basic engineering vol 

gordon salmon smith novel approach nonlinear nongaussian bayesian state estimation radar signal processing iee proceedings vol 
pp 

doucet freitas gordon sequential monte carlo methods practice springer verlag new york 
isard blake contour tracking stochastic propagation conditional density european conference computer vision pp 

gang chellappa structure motion sequential monte carlo methods international journal computer vision vol 
pp 

hong resampling algorithms particle filters computational complexity perspective eurasip journal applied signal processing pp 

hong resampling algorithms architectures distributed particle filters ieee transactions signal processing vol 
pp 
july 
architectures efficient implementation particle filters ph thesis dept electrical engineering state university new york stony brook august 
time sisr imh aux number processing nodes timing red cluster time sisr imh aux number processing nodes timing blue cluster speedup sisr red sisr blue imh red imh blue aux red aux blue number processing nodes speedup fig 

timing speedup synthetic example saturation occurs rp shift bottle neck proposal block imh sampler 
hong generic hardware architectures sampling resampling particle filters eurasip journal applied signal processing vol 
pp 

doucet godsill andrieu sequential monte carlo sampling methods bayesian filtering statistics computing vol 
pp 

chellappa srivastava algorithmic architectural design methodology particle filters hardware international conference computer design pp 

khan balch dellaert mcmc particle filtering tracking variable number interacting targets ieee transactions pattern analysis machine intelligence vol 

liu chen sequential monte carlo methods dynamic systems journal american statistician association vol 
pp 

robert casella monte carlo statistical methods springer verlag new york 
pitt shephard filtering simulation auxiliary particle filters journal american statistical association vol 
pp 

liu chen theoretical framework sequential importance sampling resampling sequential monte carlo methods practice doucet de freitas gordon eds 
springer verlag new york 
doucet sequential simulation methods bayesian filtering tech 
rep department engineering university cambridge 
chib greenberg understanding metropolis hastings algorithm american statistician vol 
pp 

hastings monte carlo sampling methods markov chains applications biometrika vol 
pp 

metropolis rosenbluth rosenbluth teller teller equations state calculations fast computing machines journal chemical physics vol 
pp 

geman geman stochastic relaxation gibbs distributions bayesian restoration images ieee trans 
pattern analysis machine intelligence vol 
pp 

tweedie rates convergence hastings metropolis algorithms annals statistics vol 
pp 
february 
metropolis hastings proposals estimate mean values tech 
rep department mathematical sciences norwegian university science technology 
boyd vandenberghe convex optimization cambridge university press new york ny usa 
gropp lusk skjellum high performance portable implementation mpi message passing interface standard parallel computing vol 
pp 
sept 
gropp lusk user guide mpich portable implementation mpi mathematics computer science division argonne national laboratory anl 
zhou chellappa moghaddam visual tracking recognition appearance adaptive models particle filters transactions image processing vol 
pp 
november 
received tech 
degree indian institute technology madras 
currently ph candidate department electrical computer engineering university maryland college park 
spring selected faculty fellow clarke school engineering 
participant ibm emerging leaders multimedia workshop held watson research center october 
research interests computer vision statistics geometry signal processing 
srivastava received degree indian institute technology delhi degree northwestern university evanston il ph degree university california los angeles ucla 
fall assistant professor university maryland college park 
research interests include aspects design automation including logic high level synthesis low power issues fabrication variability low power issues pertaining sensor networks computer vision 
dr srivastava received outstanding ph award computer science department ucla 
member acm ieee 
rama chellappa received 

degree university madras india 
distinction degree indian institute science bangalore 
received ph degrees electrical engineering purdue university west lafayette respectively 
professor electrical computer engineering affiliate professor computer science university maryland college park 
affiliated center automation research director institute advanced computer studies permanent member applied mathematics program chemical physics program 
named martin professor engineering 
prior joining university maryland assistant associate professor director signal image processing institute university southern california los angeles 
years published numerous book chapters peer reviewed journal conference papers 
edited authored research monographs markov random fields biometrics surveillance 
current research interests face gait analysis modeling video automatic target recognition stationary moving platforms surveillance monitoring hyperspectral processing image understanding commercial applications image processing understanding 
dr chellappa served associate editor ieee transactions 
editor chief graphical models image processing 
served editor chief ieee transactions pattern analysis machine intelligence 
served member ieee signal processing society board governors vice president awards membership 
received awards including national science foundation presidential young investigator award ibm faculty development awards excellence teaching award school engineering usc best industry related award international association pattern recognition zheng technical achievement award ieee signal processing society 
elected distinguished faculty research fellow distinguished scholar teacher university maryland 
authored received best student computer vision track international association pattern recognition 
recipient outstanding year award sundaresan office technology commercialization university maryland received clark school engineering faculty outstanding research award 
elected serve distinguished lecturer signal processing society receive service award 
golden core member ieee computer society received service award 
served general technical program chair ieee international national conferences workshops fellow international association pattern recognition 

