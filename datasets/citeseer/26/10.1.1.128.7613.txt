kernel principal component analysis bernhard scholkopf alexander smola klaus robert muller max planck institut biol 
kybernetik 
tubingen germany gmd berlin germany 
new method performing nonlinear form principal component analysis proposed 
integral operator kernel functions ciently compute principal components high dimensional feature spaces related input space nonlinear map instance space possible pixel products images 
give derivation method experimental results polynomial feature extraction pattern recognition 
principal component analysis pca basis transformation diagonalize estimate covariance matrix data xk xk xk de ned xjx new coordinates eigenvector basis orthogonal projections eigenvectors called principal components 
generalize setting nonlinear kind 
suppose rst map data nonlinearly feature space 
show arbitrarily large dimensionality certain choices perform pca done kernel functions known support vector machines boser guyon vapnik 
kernel pca assume moment data mapped feature space centered xk pca covariance matrix xj xj nd eigenvalues eigenvectors nf satisfying cv substituting note solutions lie span 
implies may consider equivalent system xk xk cv exist coe cients xi substituting de ning matrix arrive kij xi xj denotes column vector entries 
nd solutions solve eigenvalue problem nonzero eigenvalues 
clearly solutions 
shown additional solutions di erence expansion interesting 
normalize solutions belonging nonzero eigenvalues requiring corresponding vectors normalized 
virtue translates xi xj principal component extraction compute projections image test point eigenvectors xi note requires xi explicit form needed dot products 
able kernel functions computing dot products performing map aizerman braverman boser guyon vapnik choices kernel shown methods functional analysis exists map dot product space possibly nite dimension computes dot product kernels successfully support vector machines scholkopf burges vapnik include polynomial kernels radial basis functions exp kx yk sigmoid kernels tanh 
shown polynomial kernels degree correspond map feature space spanned products entries input pattern case linear pca kernel pca fig 

basic idea kernel pca nonlinear kernel function standard dot product implicitly perform pca possibly high dimensional space nonlinearly related input space 
dotted lines contour lines constant feature value 
patterns images space products pixels take account higher order statistics doing pca 
substituting kernel functions occurences obtain algorithm kernel pca fig 
compute dot product matrix cf 
eq 
kij xi xj ij solve diagonalizing normalize eigenvector expansion coe cients requiring eq 
extract principal components corresponding kernel test point computing projections eigenvectors eq 
fig 

point practice algorithm equivalent tothe form nonlinear pca obtainable explicitly mapping feature space rank dot product matrix limited sample size may able compute matrix dimensionality prohibitively high 
instance pixel input images polynomial degree yield dimensionality kernel pca deals problem automatically choosing subspace dimensionality rank providing means computing dot products vectors subspace 
way wehave toevaluate kernel functions input space dot product dimensional space 
conclude section brie mention case drop assumption xi centered note general center data compute mean set points explicit form 
go algebra xi xi xi turns matrix feature value xi weights eigenvector coefficients comparison sample input vector fig 

kernel pca feature extraction ocr task test point eigenvector 
fig 

pca kernel degrees 
points xi xi generated xi xi noise gaussian standard deviation xi rescaled xi 
sgn xi xi jj displayed contour lines constant value rst principal component 
nonlinear kernels extract features nicely increase direction main variance data linear pca best respect limited straight directions 
case call expressed terms kij shorthand ij details see scholkopf smola muller 
experiments feature extraction shows rst principal component ofa toy data set extracted polynomial kernel pca 
investigation utility kernel pca features realistic pattern recognition problem trained separating hyperplane classi er vapnik chervonenkis cortes vapnik nonlinear features extracted postal service usps handwritten digit data base kernel pca 
database contains examples dimensionality test set 
computational reasons subset training examples dot product matrix 
polynomial kernels degrees extracting rst principal components 
case linear pca best classi cation performance error attained components 
extracting number nonlinear components cases lead superior performance error 
nonlinear case performance improved larger number components note exist higher order features pixels image 
components obtained error coincides best result reported standard nonlinear support vector machines scholkopf burges vapnik 
result competitive convolutional layer neural networks reported lecun better linear classi ers operating directly image data linear support vector machine achieves scholkopf burges vapnik 
ndings con rmed object recognition task mpi chair data base details experiments see scholkopf smola muller 
add results obtained prior knowledge symmetries problem hand 
explains performance inferior virtual support vector classi ers scholkopf burges vapnik tangent distance support vector publications downloaded www mpg de people personal bs svm html 
nearest neighbour classi ers simard lecun denker 
believe adding local translation invariance generating virtual translated examples choosing suitable kernel improve results 
discussion devoted exposition new technique nonlinear principal component analysis 
develop technique kernel method far supervised learning vapnik 
clearly kernel method applied algorithm formulated terms dot products exclusively including instance means independent component analysis cf 
scholkopf smola muller 
experiments comparing utility kernel pca features pattern recognition linear classi er advantages nonlinear kernel pca rst nonlinear principal components orded better recognition rates corresponding numbers linear principal components second performance nonlinear components improved components possible linear case 
computational complexity pca grow dimensionality feature space implicitely working 
possible instance space possible th order products pixels image 
variant standard pca dot product matrix kirby sirovich diagonalize matrix number examples size representative subset comparable computational complexity need compute kernel functions dot products 
dimensionality input space smaller number examples kernel principal component extraction computationally expensive linear pca additional investment pay back results indicating pattern recognition su cient linear classi er long features extracted nonlinear 
main advantage linear pca date consists possibility reconstruct patterns principal components 
compared methods nonlinear pca autoassociative mlps bottleneck hidden layer kung principal curves hastie stuetzle kernel pca advantage nonlinear optimization involved need solve eigenvalue problem case standard pca 
danger getting trapped local minima training 
compared neural network type generalizations pca oja kernel pca advantage provides better understanding kind nonlinear features extracted principal components feature space xed priori choosing kernel function 
sense type nonlinearities looking speci ed advance speci cation wide merely selects high dimensional feature space relevant feature subspace done automatically 
respect worthwhile note sigmoid kernels sec 
fact extract features type ones extracted mlps cf 
fig 
considered nonparametric technique 
wide class admissible nonlinearities kernel pca forms framework comprising various types feature extraction systems 
number di erent kernels support vector machines polynomial gaussian sigmoid type 
led high accuracy classi ers constructed decision boundaries hyperplanes di erent feature spaces support vectors scholkopf burges vapnik 
general question best kernel problem unsolved support vector machines kernel pca 
pca feature extraction application areas including noise reduction pattern recognition regression estimation image indexing 
cases account nonlinearities bene cial kernel pca provides new tool applied little computational cost possibly substantial performance gains 

bs supported des deutschen 
supported dfg ja 
pro ted discussions blanz bottou burges solla vapnik 
bell possibility usps database 
aizerman braverman 
theoretical foundations potential function method pattern recognition learning 
automation remote control 
boser guyon vapnik 
training algorithm optimal margin classi ers 
fifth annual workshop colt pittsburgh 
acm 
cortes vapnik 
support vector networks 
machine learning 
hastie stuetzle 
principal curves 
jasa 
kirby sirovich 
application karhunen loeve procedure characterization human faces 
ieee transactions pami 
oja 
simpli ed neuron model principal component analyzer 
math 
biology 
scholkopf burges vapnik 
extracting support data task 
fayyad uthurusamy eds proceedings international conference knowledge discovery data mining menlo park ca 
aaai press 
scholkopf burges vapnik 
incorporating invariances support vector learning machines 
malsburg seelen eds icann berlin 
springer lncs vol 

scholkopf smola 
muller 
nonlinear component analysis eigenvalue problem 
technical report max planck institut fur kybernetik 
submitted neural computation 
simard lecun denker 
cient pattern recognition new transformation distance 
hanson cowan giles editors advances nips san mateo ca 
morgan kaufmann 
vapnik chervonenkis 
theory pattern recognition russian 
nauka moscow 
german translation theorie der akademie verlag berlin 
article processed latex macro package llncs style 
