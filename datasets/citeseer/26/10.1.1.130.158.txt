non stochastic multi armed bandit problem peter auer institute theoretical computer science graz university technology graz austria igi tu graz ac nicol cesa bianchi department computer science universit di milano milano italy dsi unimi november yoav freund robert schapire labs park avenue florham park nj schapire yoav research att com multi armed bandit problem gambler decide arm non identical slot machines play sequence trials maximize reward 
classical problem received attention simple model provides trade exploration trying arm find best exploitation playing arm believed give best payoff 
past solutions bandit problem relied assumptions statistics slot machines 
statistical assumptions whatsoever nature process generating payoffs slot machines 
give solution bandit problem adversary behaved stochastic process complete control payoffs 
sequence plays prove round payoff algorithm approaches best arm rate show matching lower bound best possible 
prove algorithm approaches round payoff set strategies similar rate best strategy chosen pool strategies algorithm approaches round payoff strategy rate apply results problem playing unknown repeated matrix game 
show algorithm approaches minimax payoff unknown game rate keywords adversarial bandit problem unknown matrix games ams subject classification early extended appeared proceedings th annual symposium foundations computer science pages 
multi armed bandit problem originally proposed robbins gambler choose slot machines play 
time step pulls arm machines receives reward payoff possibly zero negative 
gambler purpose maximize return sum rewards receives sequence pulls 
model arm assumed deliver rewards independently drawn fixed unknown distribution 
reward distributions differ arm arm goal find arm highest expected payoff early possible keep gambling best arm 
problem paradigmatic example trade exploration exploitation 
hand gambler plays exclusively machine thinks best exploitation may fail discover arms higher expected payoff 
hand spends time trying machines gathering statistics exploration may fail play best arm get high return 
gambler performance typically measured terms regret 
difference expected return optimal strategy pulling consistently best arm gambler expected return 
lai robbins proved gambler regret pulls small furthermore prove bound optimal sense exist strategy gambler better asymptotic performance 
formulation bandit problem allows elegant statistical treatment exploration exploitation trade may adequate model certain environments 
motivating example consider task repeatedly choosing route transmitting packets points communication network 
cast scenario bandit problem suppose fixed number possible routes transmission cost reported back sender 
costs associated route modeled stationary distribution sophisticated set statistical assumptions required 
general may difficult impossible determine right statistical assumptions domain domains may exhibit dependencies extent assumptions appropriate 
provide framework model scenarios sketched adversarial bandit problem variant bandit problem statistical assumptions generation rewards 
assume slot machine initially assigned arbitrary unknown sequence rewards time step chosen bounded real interval 
time gambler pulls arm slot machine receives corresponding reward sequence assigned slot machine 
measure gambler performance setting replace notion statistical regret worst case regret 
sequence pulls arbitrary time horizon index arm worst case regret gambler sequence pulls difference return gambler pulling arms actual gambler return returns determined initial assignment rewards 
easy see model gambler keep regret small say sublinear sequences pulls respect worst case assignment rewards arms 
problem feasible allow regret depend hardness sequence pulls measured hardness sequence roughly number times change slot machine currently played order pull arms order sequence 
trick allows effectively control worst case regret simultaneously sequences pulls expect regret bounds trivial hardness sequence compete gets close note deterministic bandit problem considered gittins varaiya 
version bandit problem different assume player compute ahead time exactly payoffs received arm problem optimization exploration exploitation 
general result efficient randomized player algorithm expected regret sequence pulls hardness sequence see theorem corollaries 
note bound holds simultaneously sequences pulls assignments rewards arms uniformly time horizon gambler willing impose upper bound hardness sequences pulls wants measure regret improved bound expected regret sequences proven see corollaries 
purpose establishing connections certain results game theory look special case worst case regret call weak regret 
time horizon call best arm arm highest return sum assigned rewards time respect initial assignment rewards 
gambler weak regret difference return best arm actual gambler return 
introduce randomized player algorithm tailored notion regret expected weak regret return best arm see theorem section 
bound holds assignments rewards arms uniformly choice time horizon complex player algorithm prove weak regret probability algorithm randomization fixed see theorems section 
implies asymptotically constant weak regret see corollary 
worst case bounds may appear weaker bounds proved statistical assumptions shown lai robbins form comparing results statistics literature important point important difference asymptotic quantification 
lai robbins assumption distribution rewards associated arm fixed total number iterations increases infinity 
contrast bounds hold finite generality model bounds applicable payoffs randomly adversarially chosen manner depend quantification order adversarial nature framework cause apparent gap 
prove point theorem show player algorithm armed bandit problem exists set reward distributions expected weak regret algorithm playing arms time steps far considered notions regret compare return gambler return sequence pulls return best arm 
notion regret probability fixed compact asymptotic notation bounds proven finite explicit constants 
explore regret best strategy set strategies available gambler 
notion strategy generalizes sequence pulls time step strategy gives recommendation form probability distribution arms arm play 
assignment rewards arms set strategies gambler call best strategy strategy yields highest return respect assignment 
regret best strategy difference return best strategy actual gambler return 
randomized player combines choices strategies vein algorithms prediction expert advice show expected regret best strategy see theorem 
note dependence number strategies logarithmic bound quite reasonable player combining large number strategies 
adversarial bandit problem closely related problem learning play unknown person finite game game played repeatedly players 
desirable property player hannan consistency similar saying bandit framework weak regret time step player converges probability 
examples hannan consistent player strategies provided authors past see survey results 
applying slight extensions theorems prove provide example simple hannan consistent player convergence rate optimal logarithmic factors 
player algorithms part algorithm freund schapire turn variant littlestone warmuth weighted majority algorithm vovk aggregating strategies :10.1.1.32.8918
setting analyzed freund schapire player scores pull reward chosen arm gains access rewards associated arms just chosen 
notation terminology adversarial bandit problem specified number possible actions action denoted integer assignment rewards infinite sequence denotes reward vectors obtained action chosen time step called trial 
assume rewards belong interval generalization results rewards arbitrary straightforward 
assume player knows number actions 
furthermore trial assume player knows rewards previously chosen actions respect view player algorithm sequence mapping set action indices previous rewards set action indices 
reward assignment return time horizon algorithm choosing actions follows write value clear context 
measure performance player algorithm worst case regret explore variants notion regret 
time horizon sequence difference actions worst case regret algorithm return time horizon obtained choosing actions regret measures player lost gained depending sign difference strategy choosing actions special case regret best single action call weak regret short defined return single globally best action time horizon write value clear context 
player algorithms randomized fixing player algorithm defines probability distribution set sequences actions 
probabilities expectations considered taken respect distribution 
follows prove kinds bounds performance randomized player bound expected regret arbitrary sequence weak regret 
form actions 
second confidence bound states high probability return time smaller globally best action 
bounds hold sequence reward assignments hold uniformly time horizon hold requiring input parameter 
upper bounds weak regret section analyze simplest player algorithm stands exponential weight algorithm exploration exploitation 
show bound algorithm real parameters initialization 
set 
draw randomly accordingly probabilities 
receive reward 
set pseudo code algorithm weak regret 
expected regret respect single best action 
sections greatly strengthen result 
algorithm described variant algorithm introduced freund schapire solving different worst case sequential allocation problem :10.1.1.32.8918
time step draws action distribution distribution mixture uniform distribution distribution assigns action probability mass exponential estimated cumulative reward action 
intuitively mixing uniform distribution done sure algorithm tries actions gets estimates rewards 
algorithm action initial rewards observes action low large rewards occur observed action selected 
drawn action sets estimated reward dividing actual gain probability action chosen compensates reward actions chosen 
choice estimated rewards guarantees expectations equal actual rewards action expectation taken respect random choice trial choices previous trials 
give main theorem bounds expected weak regret algorithm theorem holds assignment rewards understand theorem helpful consider simpler bound obtained appropriate choice parameter corollary assume algorithm run input parameter holds assignment rewards 
proof 
bound trivial expected regret theorem expected regret desired 
apply corollary necessary upper bound available tuning example time horizon known action payoff greater trial upper bound 
section give technique require prior knowledge upper bound yielding result holds uniformly rewards range rewards applying corollary gives translated rescaled range bound model rewards fall regret 
instance applicable standard loss range proof theorem 
explicit mention simple facts immediately derived definitions sequences actions drawn eq 
uses definition 
eq 
uses fact expression preceding line eq 

eq 
uses eqs 

logarithms gives summing get action combining eq 
get take expectation sides respect distribution 
expected value combining find chosen arbitrarily obtain inequality statement theorem 
additional notation 
player algorithms variants find convenient define notation quantities analysis define bounds weak regret hold uniformly time section showed yields expected regret upper expected weak regret uniformly bound return best action known advance 
bound holds uniformly easily proven guessing techniques prove corollaries section 
section describe algorithm called bound worse substantially better return best arm small compared algorithm described proceeds epochs epoch consists sequence trials 
index epochs 
epoch algorithm guesses bound return best action 
uses guess tune parameter restarting epoch 
usual denote current time step 
maintains estimate return action estimate unbiased sense estimates algorithm detects approximately actual gain action advanced happens algorithm goes epoch restarting larger bound maximal gain 
note general may differ local variable regard subroutine 
section refer total number trials 
algorithm initialization repeat 

restart choosing 
random action chosen corresponding reward 

pseudo code algorithm control weak regret uniformly time 
performance algorithm characterized theorem main result section 
theorem holds assignment rewards proof theorem divided lemmas 
bounds regret suffered epoch second bounds total number epochs 
fix arbitrarily define random variables total number epochs final value 
time steps completed epoch convenience define 
epoch consists trials note degenerate cases epochs may empty case lemma action epoch proof 
summations equal zero 
assume proof theorem trials occur epoch lemma holds trivially definition termination condition know get implies choice get statement lemma 
lemma gives implicit upper bound number epochs lemma number epochs satisfies proof 
bound holds trivially 
assume completed termination condition suppose claim lemma false 
increasing implies epoch 
function contradicting 
proof theorem 
lemmas lemma inequality lemma second inequality 
steps follow definitions simple algebra 
expectations sides gives second derivative positive convex jensen inequality note function increasing statement theorem 
hand increasing combined gives equivalent non theorem follows trivially case 
lower bounds weak regret section state lower bound expected weak regret player 
precisely choice time horizon show exists strategy assigning rewards actions expected weak regret player algorithm observe match upper bound algorithms theorem open problem close gap 
see corollary lower bound proven classical statistical bandit model crucial difference reward distribution depends number actions time horizon dependence reason lower bound contradict upper bounds form lower bound implies upper bound possible form classical bandit model 
distribution rewards fixed note lower bound considerably stronger dependence number action lower bound proven directly results 
specifically theorem number actions time horizon exists distribution assignment rewards expected weak regret algorithm expectation taken respect randomization rewards algorithm internal randomization proof appendix lower bound implies course algorithm particular choice rewards cause expected weak regret expectation respect algorithm internal randomization larger value 
bounds weak regret hold probability section showed expected weak regret algorithm section show modification achieves weak regret probability fixed uniformly bound weak regret holds probability follows easily 
modification necessary variance regret achieved algorithm large large interesting high probability bound may hold 
large variance regret comes large variance estimates payoffs fact variance close range interest ignoring dependence magnitude summing trials variance return regret large control variance modify algorithm uses estimates upper confidence bounds estimates correct expectation 
modified algorithm 
algorithm directly uses estimates choosing random algo rithm uses upper confidence bounds lemma shows appropriate upper confidence bounds 
fix time horizon follows denote denote algorithm parameters reals initialization 
set 
choose randomly distribution 
receive reward 
set pseudo code algorithm lemma proof 
fix set achieving small weak regret high probability 
step multiplied sides set step markov inequality 
denote expectation respect random choice trial conditioned past trials 
note past trials fixed random quantities note eq 
uses eq 
uses eq 
uses eq 
uses real eq 
uses choice observing get induction lemma follows lemma shows return achieved algorithm dence bounds 
lemma close upper proof 
proceed analysis algorithm set sequence actions chosen consider second inequality uses inequality uses eqs 

logarithms summing get implies yields lemma 
combining lemmas gives main result section 
theorem fixed holds assignment rewards probability proof 
assume loss generality conditions hold theorem holds trivially 
note ensures note implies choice apply lemmas 
lemma lemma probability collecting terms gives theorem 
difficult obtain algorithm need time horizon input parameter regret slightly worse proven algorithm theorem 
new algorithm called shown simply restarts doubling guess time 
careful issue choice confidence parameter minimum length runs ensure lemma holds runs algorithm parameters real initialization repeat run trials choosing theorem pseudo code algorithm theorem eq 

holds probability see theorem 
proof 
choose time horizon arbitrarily call epoch sequence trials successive restarts algorithm defined similarly define quantities sums go find numbers lemma apply theorem epoch 
loss generality assume satisfies probability random draw actions theorem get simple corollary statement sure convergence return algorithm rate convergence optimal see lower bound section 
corollary function holds assignment rewards probability 
proof 
theorem exists constant large probability implies theorem follows borel cantelli lemma 
regret best strategy pool consider setting player fixed set strategies choosing actions 
strategies select different actions different iterations 
strategies computations performed player external advice player experts general term expert borrowed cesa bianchi place restrictions generation advice 
player goal case combine advice experts way return close best expert 
formally assume player prior choosing action time provided set probability vectors interpret advice expert trial th component represents recommended probability playing action 
special case distribution concentrated single action represents deterministic recommendation 
vector rewards time expected reward expert respect chosen probability vector simply analogy define measuring expected return best strategy 
regret best strategy time horizon defined measures difference return best expert player return time results hold finite set experts 
formally regard random variable arbitrary function random sequence plays definition allows experts advice depends entire past history observed player side information may available 
point view expert meta action higher level bandit problem payoff vector defined trial immediately apply corollary obtain bound player regret relative best expert upper bound 
bound quite weak player combining experts large 
show algorithm section modified yielding regret term form bound reasonable number actions small number experts quite large exponential 
algorithm shown slightly modified version stands exponential weight algorithm exploration exploitation expert advice define vector components corresponding gains experts simplest possible expert assigns uniform weight actions round call uniform expert 
prove results need assume uniform expert included family experts 
clearly uniform expert added family experts small expense increasing 
theorem family experts includes fact slightly weaker sufficient condition uniform expert included convex hull family experts exists nonnegative numbers algorithm real parameters initialization 
get advice vectors 
set set 
draw action randomly probabilities 
receive reward 

uniform expert set set pseudo code algorithm expert advice 
holds assignment rewards 
proof 
prove theorem lines proof theorem 
logarithms summing get expert get note experts take expectations sides inequality 
note assumed uniform expert included family experts 
combining facts immediately implies statement theorem 
regret arbitrary strategies section variant algorithm prove bound expected regret sequence actions 
prove result rank sequences actions hardness 
hardness sequence defined bound regret prove grows hardness sequence measuring regret 
particular show player algorithm described expected regret regret measured sequence parameters tuned reduces denote return expected regret sequence actions 
hand actions hardness theorem follows sequence actions 
holds assignment rewards sequence actions 
corollary assume algorithm holds sequence run input parameters actions 
algorithm parameters reals initialization 
set 
draw randomly accordingly probabilities 
receive reward 
set pseudo code algorithm note statement corollary equivalently written control expected regret 
revealing algorithm able automatically trade return sequence hardness corollary assume algorithm holds sequence run input parameters actions proof theorem 
fix sequence actions 
technique follows closely proof theorem prove sequences actions drawn usual segments fix arbitrary segment logarithms sides summing action uses step piecing get partition segment furthermore get 
summing segments algorithm expectation respect random choices yields inequality statement theorem 
time horizon known apply techniques similar applied proving theorem section 
specifically introduce new algorithm runs subroutine 
suppose new run epoch started parameters set prescribed corollary set stopped actions number iterations 
clearly fixed sequence segments see proof theorem definition segment epoch expected regret epoch certainly epoch epochs finishing calculations proves 
corollary regret sequence actions 
hand runs parameters set prescribed corollary reasoning similar conclude 
corollary sequence applications game theory actions adversarial bandit problem easily related problem playing repeated games 
integer person finite game defined finite sets pure strategies set player functions function player payoff function 
note player payoff depends pure strategy chosen player pure strategies chosen players 
denote typical members respectively denote write suppose game played repeatedly time 
assume player knows payoff functions repetition round knows vector pure strategies chosen players 
pure strategy chosen player round may depend player players chose past rounds 
average regret player pure strategy rounds defined player lost average playing pure strategy rounds players kept choices fixed 
desirable property player hannan consistency defined follows 
player hannan consistent probability 
existence properties hannan consistent players investigated hannan blackwell see nice survey 
hannan consistency studied called unknown game setup assumed player knows total number players payoff function player including round player sees payoffs sees choices players resulting payoffs 
setup previously studied ba os megiddo hart mas colell 
apply results section prove player algorithm mixed strategy hannan consistent unknown game setup payoffs obtained player belong known bounded real interval 
extend results case assignment rewards chosen adaptively 
precisely view payoff received gambler trial bandit problem payoff received player th round game 
adversarial bandit framework rewards assigned arm payoff depends possibly randomized choices players turn functions realized payoffs 
bandit terminology corresponds assuming vector rewards trial chosen adversary knows gambler strategy outcome gambler random draws time leave interested reader easy lengthy task checking results including section hold additional assumption 
theorem get 
theorem player pure strategies plays unknown game setup mixed strategy payoffs holds probability note theorem rate convergence optimal logarithmic factors 
theorem corollary immediately implies result 
corollary player strategy hannan consistent unknown game setup 
pointed hannan consistency interesting consequence repeated zero sum games 
games defined matrix round row player chooses row matrix 
time column player chooses column row player gains quantity column player loses quantity 
repeated play row player goal maximize expected total gain sequence plays column player goal minimize expected total loss 
suppose round row player chooses move randomly probability distribution rows represented column vector column player similarly chooses probability vector expected payoff von neumann minimax theorem states maximum minimum taken compact set distribution vectors quantity defined equation called value zero sum game matrix words says exists mixed randomized strategy row player guarantees expected payoff regardless column player action 
payoff optimal sense column player choose mixed strategy expected payoff regardless row player action 
row player knows matrix compute strategy instance linear programming certain bring expected optimal payoff smaller round 
suppose game entirely unknown row player 
precise assume row player knows number rows matrix bound magnitude entries results section show row player play manner payoff round rapidly converge optimal maximin payoff theorem unknown game matrix value suppose row player knowing uses mixed strategy row player expected payoff round proof 
assume theorem extension general case straightforward 
maxmin strategy row player distribution vector th component row player expected payoff dividing get average round payoff gives result 
note theorem independent number columns appropriate assumptions theorem easily generalized column players infinite number strategies 
matrix large entries small known player algorithm may efficient alternative linear programming 
acknowledgments express special kurt hornik advice patience listening ideas proofs earlier draft 
yuval peres amir dembo help regarding analysis martingales 
peter auer nicol cesa bianchi gratefully acknowledge support esprit working group ep neural computational learning ii neurocolt ii alfredo ba os 
pseudo games 
annals mathematical statistics 
david blackwell 
controlled random walks 
invited address institute mathematical statistics meeting seattle washington 
nicol cesa bianchi yoav freund david haussler david helmbold robert schapire manfred warmuth 
expert advice 
journal association computing machinery may 
thomas cover joy thomas 
elements information theory 
wiley 
dean foster rakesh vohra 
randomization rule selecting forecasts 
operations research july august 
yoav freund robert schapire :10.1.1.32.8918
decision theoretic generalization line learning application boosting 
journal computer system sciences august 
yoav freund robert schapire 
adaptive game playing multiplicative weights 
games economic behavior 
drew fudenberg david levine 
consistency cautious fictitious play 
journal economic dynamics control 
gittins 
multi armed bandit allocation indices 
john wiley sons 
james hannan 
approximation bayes risk repeated play 
tucker wolfe editors contributions theory games volume iii pages 
princeton university press 
hart mas colell 
simple procedure leading correlated equilibrium 
econometrica 
hart mas colell 
general class adaptive strategies 
journal economic theory 
varaiya 
multi armed bandit problem revisited 
journal optimization theory applications october 
lai herbert robbins 
asymptotically efficient adaptive allocation rules 
advances applied mathematics 
nick littlestone manfred warmuth 
weighted majority algorithm 
information computation 
megiddo 
repeated games incomplete information played non bayesian players 
international journal game theory 

discrete parameter martingales 
north holland 
dean foster rakesh vohra 
regret line decision problem 
games economic behavior 
robbins 
aspects sequential design experiments 
bulletin american mathematical society 
vovk 
aggregating strategies 
proceedings third annual workshop computational learning theory pages 
proof theorem construct random distribution rewards follows 
play begins action chosen uniformly random action 
rewards associated action chosen independently random probability small fixed constant chosen proof 
rewards associated actions chosen independently random equal odds 
expected reward best action main part proof derivation upper bound expected gain algorithm distribution rewards 
write denote probability respect random choice rewards write denote probability conditioned action write denote probability respect uniformly random choice rewards actions including action 
analogous expectation notation 
player strategy 
random variable denoting reward received time denote sequence rewards received trial 
shorthand entire sequence rewards 
randomized playing strategy equivalent priori random choice set deterministic strategies 
adversary strategy defined oblivious actions player suffices prove upper bound expected gain deterministic strategy crucial proof simplifies notation 
formally regard algorithm fixed function step maps reward history action usual denotes return algorithm return best action 
random variable denoting number times action chosen lemma bounds difference expectations measured lemma action function defined reward sequences proof 
apply standard methods instance cover thomas 
distributions variational distance kullback liebler divergence relative entropy distributions 
denote notation conditional relative entropy shorthand relative entropy bernoulli random variables parameters cover thomas lemma states chain rule relative entropy cover thomas theorem gives conditional probability distribution conditional distribution second equality seen follows regardless past history rewards reward uniform easily computed action fixed action action conditional distribution uniform probability 
lemma follows combining 
ready prove theorem 
specifically show theorem player strategy distribution rewards described expected regret algorithm lower bounded proof 
action chosen action clearly expected payoff time expected gain algorithm apply lemma function reward sequence actions player strategy determined past rewards 
clearly fact fore combining implies expected gain best action expected gain action get regret lower bounded bound statement theorem 
small bound theorem order choosing small constant gives lower bound specifically lower bound theorem obtained theorem choosing inequality 
