empirical comparison supervised learning algorithms rich caruana caruana cs cornell edu alexandru niculescu cs cornell edu department computer science cornell university ithaca ny usa number supervised learning methods introduced decade 
unfortunately comprehensive empirical evaluation supervised learning statlog project early large scale empirical comparison supervised learning methods svms neural nets logistic regression naive bayes memory learning random forests decision trees bagged trees boosted trees boosted stumps 
examine effect calibrating models platt scaling isotonic regression performance 
important aspect study variety performance criteria evaluate learning methods 

comprehensive empirical studies comparing learning algorithms 
statlog best known study king 
statlog comprehensive performed new learning algorithms emerged bagging boosting svms random forests excellent performance 
extensive empirical evaluation modern learning methods useful 
learning algorithms domains different performance metrics appropriate domain 
example precision recall measures information retrieval medicine prefers roc area lift appropriate marketing tasks different performance metrics measure different tradeoffs predictions classifier possible learning methods perform metric suboptimal metrics 
important evaluate algorithms broad set performance metrics 
appearing proceedings rd international conference machine learning pittsburgh pa 
copyright author owner 
presents results large scale empirical comparison supervised learning algorithms performance criteria 
evaluate performance svms neural nets logistic regression naive bayes memory learning random forests decision trees bagged trees boosted trees boosted stumps eleven binary classification problems variety performance metrics accuracy score lift roc area average precision precision recall break point squared error cross entropy 
algorithm examine common variations thoroughly explore space parameters 
example compare decision tree styles neural nets sizes svms kernels performance metrics examine interpret model predictions probabilities models svms designed predict probabilities compare performance algorithm calibrating predictions platt scaling isotonic regression 
empirical results surprising 
preview prior calibration bagged trees random forests neural nets give best average performance metrics eleven test problems 
boosted trees best restrict attention metrics require probabilities 
calibration platt method boosted trees predict better probabilities methods move place 
neural nets hand calibrated hurt slightly calibration 
calibration platt method isotonic regression svms perform comparably neural nets nearly boosted trees random forests bagged trees 
boosting full decision trees dramatically outperforms boosting weaker stumps problems 
average memory learning boosted stumps single decision trees logistic regression naive bayes competitive best methods 
generalizations hold 
example boosted stumps logistic regression perform poorly average best models metrics test problems 

methodology 
learning algorithms empirical comparison supervised learning algorithms attempt explore space parameters common variations learning algorithm thoroughly computationally feasible 
section summarizes parameters learning algorithm may safely skipped readers easily bored 
svms kernels svmlight joachims linear polynomial degree radial width 
vary regularization parameter factors kernel 
ann train neural nets gradient descent backprop vary number hidden units momentum 
halt training nets different epochs validation sets select best nets 
logistic regression train regularized models varying ridge regularization parameter factors naive bayes nb weka witten frank try weka options handling continuous attributes modeling single normal modeling kernel estimation discretizing supervised discretization 
knn values ranging 
knn euclidean distance euclidean distance weighted gain ratio 
distance weighted knn locally weighted averaging 
kernel widths locally weighted averaging vary times minimum distance points train set 
random forests rf tried breiman cutler weka implementations breiman cutler yielded better results report 
forests trees 
size feature set considered split 
decision trees dt vary splitting criterion pruning options smoothing laplacian bayesian smoothing 
tree models buntine ind package buntine caruana bayes id cart cart mml 
generate trees type ls pruning laplacian smoothing bs bayesian smoothing mml laplacian smoothing 
see provost domingos description ls 
bagged trees bag dt bag trees type described 
boosted trees bst dt boost tree type 
boosting overfit consider boosted trees steps boosting 
boosted stumps bst stmp boost single level decision trees generated different splitting criteria boosted steps 
ann svm knn scale attributes mean std 
dt rf nb bag dt bst dt bst stmp don scale data 
total train different models trial problem 

performance metrics divide performance metrics groups threshold metrics ordering rank metrics probability metrics 
threshold metrics accuracy acc score fsc lift lft 
see description lift curves 
usually acc fsc fixed threshold 
lift fixed percent cases predicted positive rest negative 
thresholded metrics important close prediction threshold threshold 
ordering rank metrics depend ordering cases actual predicted values 
long ordering preserved difference predicted values fall 
metrics measure positive cases ordered negative cases viewed summary model performance possible thresholds 
rank metrics area roc curve roc average precision apr precision recall break point bep 
see provost fawcett discussion roc machine learning perspective :10.1.1.34.9927
probability metrics squared error rms cross entropy mxe interpret predicted value case conditional probability case positive class 
see caruana niculescu detailed description performance metrics 

comparing performance metrics performance metrics accuracy squared error range lift cross entropy range depends data set 
metrics lower values indicate better perfor empirical comparison supervised learning algorithms mance 
higher values better 
metrics roc area baseline rates independent data accuracy baseline rates depend data 
baseline accuracy accuracy probably performance problem bayes optimal rate achieving accuracy excellent performance 
permit averaging metrics problems performances placed comparable scales 
scaling performance problem metric baseline performance bayes optimal 
bayes optimal rate estimated real problems best observed performance proxy 
baseline model predict case percent positives data 
performances normalized baseline represents best performance 
model performs worse baseline normalized score negative 
disadvantage normalized scores recovering raw performances requires knowing performances top bottom scale new best models top scale may change 
performances define top bottom scales problem metric available www cs cornell edu caruana allow comparison results 

calibration methods learning algorithms examine designed predict probabilities 
example outputs svm normalized distances decision boundary 
naive bayes models known predict poorly calibrated probabilities unrealistic independence assumption 
number methods proposed mapping predictions posterior probabilities 
platt proposed transforming svm predictions posterior probabilities passing sigmoid 
platt method works boosted trees boosted stumps niculescu caruana 
sigmoid correct transformation learning algorithms 
zadrozny elkan general calibration method isotonic regression robertson calibrate predictions svms naive bayes boosted naive bayes decision trees 
isotonic regression general score baseline calculated predicting cases predicting yield score 
table 
description problems problem attr train size test size adult cod cov type hs letter letter medis mg slac restriction mapping function isotonic monotonically increasing 
standard algorithm isotonic regression finds piecewise constant solution linear time pair adjacent pav algorithm ayer 
calibrate models points validation set model selection 

data sets compare algorithms binary classification problems 
adult cov type letter uci repository blake merz 
cov type converted binary problem treating largest class positive rest negative 
converted letter boolean ways 
letter treats positive remaining letters negative yielding unbalanced problem 
letter uses letters positives rest negatives yielding balanced problem 
hs data set difficult class soybean positive class 
slac problem stanford linear accelerator 
medis mg medical data sets 
cod datasets 
adult cod contain nominal attributes 
anns svms transform nominal attributes boolean boolean value 
dt bag dt bst dt bst stmp rf nb model trained twice transformed attributes original ones 
see table characteristics problems 

performances metric test problem randomly select cases training rest cases large final test set 
fold cross validation empirical comparison supervised learning algorithms cases obtain trials 
trial cases train different models cases calibrate models select best parameters report performance large final test set 
run trials expensive set experiments 
fortunately trials able discern interesting differences methods 
table shows normalized score algorithm metrics 
problem metric find best parameter settings algorithm validation sets set aside crossvalidation report model normalized score final test set 
entry table averages scores trials test problems 
second column tells model predictions calibrated 
means model predictions calibrated raw model predictions 
exception svms distances separating hyperplane linearly scaled computing probability metrics 
plt iso second column indicates model predictions scaled model trained platt scaling isotonic regression respectively 
scaling methods discussed section 
table higher scores indicate better performance 
second column mean mean normalized score metrics eleven problems trials 
models table sorted mean normalized score column 
ignore column opt sel 
column discussed section 
table algorithm best performance metric boldfaced 
algorithm performance statistically distinguishable best algorithm paired tests trials ed 
entries table bold starred indicate performance significantly lower best models 
performing independent tests problematic 
differences labeled significant table probably truly significant 
considered applying stringent experiment wise value takes account number tests performed strong correlations performances different metrics calibrated uncalibrated models problematic decided keep simple 
differences table significant 
doing tests adds additional stars table 
note possible difference scores statistically significant score statistically indistinguishable averaging metrics strongest models calibrated boosted trees calibrated random forests bagged trees plt calibrated svms neural nets 
calibrated best models bagged trees random forests neural nets 
calibration poorest performing models naive bayes logistic regression decision trees 
memory methods knn remarkably unaffected calibration exhibit mediocre performance 
boosted stumps calibration mediocre performance perform nearly boosted full trees 
looking individual metrics boosted trees poor squared error cross entropy prior calibration dominate algorithms metrics calibration 
bagged trees random forests neural nets predict probabilities 
interestingly calibrating neural nets plt iso hurts calibration 
neural nets trained better adjust predictions 
boosted trees excellent performance ordering metrics area roc average precision precision recall break point 
random forests bagged trees similar performance boosted trees metrics 
neural nets svms order cases extremely 
metrics compare predictions threshold accuracy score lift best models calibrated uncalibrated random forests calibrated boosted trees bagged trees calibration 
sure iso calibration significantly improves score models nb including calibrated models neural nets bagged trees 
column opt sel mean normalized score metrics model selection done cheating looking final test sets 
means column represent best performance achieved learning method model selection done optimally 
numbers parameter optimization critical difficult algorithms 
example bagging works decision tree types requires little tuning neural nets svms require careful parameter selection 
expected mean normalized scores poorer score variance score higher variance score 
plt calibration change ordering predicted models affect metrics 
iso calibration introduce ties predicted values may affect performance ordering metrics 
empirical comparison supervised learning algorithms table 
normalized scores learning algorithm metric average eleven problems model cal acc fsc lft roc apr bep rms mxe mean opt sel bst dt plt rf plt bag dt bst dt iso rf bag dt plt rf iso bag dt iso svm plt ann svm iso ann plt ann iso bst dt knn plt knn knn iso bst stmp plt svm bst stmp iso bst stmp dt iso dt dt plt lr lr iso lr plt nb iso nb plt nb cheating column opt sel tend higher mean normalized scores selection done validation sets model selection validation sets select model best performance final test set 
comparing mean opt sel columns selection validation sets yields average decrease normalized score compared optimal selection 
expected high variance models biggest drop performance 
neural nets example relatively high variance lose substantial performance selecting models validation sets models random forests small variance lose little performance selection done validation sets 
svm variance rf anns lose rf ann 
boosted trees relatively high variance performance plt iso calibration strong remain best model selection done validation sets 

performances problem table shows normalized score algorithm test problems 
entry average performance metrics trials selection done validation sets 
free lunch theorem suggests universally best learning algorithm 
best models calibrated boosted trees random forests bagged trees perform poorly problems models poor average performance perform problems metrics 
example best models adult calibrated boosted stumps random forests bagged trees 
boosted trees perform worse 
bagged trees random forests perform mg slac 
medis best models random forests neural nets logistic regression 
models exhibit excellent performance problem naive bayes memory learning 
boosting full decision trees yields better performance empirical comparison supervised learning algorithms table 
normalized scores learning algorithm problem averaged metrics model cal adult ltr ltr medis slac hs mg cod mean bst dt plt rf plt bag dt bst dt iso rf bag dt plt rf iso bag dt iso svm plt ann svm iso ann plt ann iso bst dt knn plt knn knn iso bst stmp plt svm bst stmp iso bst stmp dt iso dt dt plt lr lr iso lr plt nb iso nb plt nb boosting stumps problems 
occasionally boosted stumps perform perform poorly average performance low 
adult boosting trees iteration boosting hurts performance tree types recovers subsequent rounds 
happens single decision trees outperform boosted counterparts 
bagged trees random forests consistently outperform single trees problems 
bagging random forests safer boosting metrics boosting yields best performance 

bootstrap analysis results depend choice problems metrics 
impact selecting problems evaluating performance metrics results 
example neural nets perform metrics problems perform poorly cod 
hadn included cod problem neural nets move places rankings 
help evaluate impact choice problems metrics performed bootstrap analysis 
randomly select bootstrap sample sampling replacement original problems 
sample problems randomly select bootstrap sample metrics original metrics sampling replacement 
bootstrap sample problems metrics rank algorithms mean performance sampled problems metrics folds 
bootstrap sampling repeated times yielding potentially different rankings learning methods 
table shows frequency method ranks st nd rd entry boosted trees column nd place tells chance boosted decision trees placed nd table results st selected problems metrics 
bootstrap analysis complements tests tables 
results suggest selected problems metrics chance boosted decision trees ranked st empirical comparison supervised learning algorithms table 
bootstrap analysis rank mean performance problems metrics model st nd rd th th th th th th th bst dt rf bag dt svm ann knn bst stmp dt nb chance seeing rank lower rd place 
random forests come st place time nd place time little chance ranking third place 
chance method boosted trees random forests bagged trees rank top chance method rank st appears clean sweep ensembles trees 
svms probably rank th neural nets probably rank th chance svms rank neural nets 
bootstrap analysis clearly shows mbl boosted level stumps plain decision trees logistic regression naive bayes competitive average top models problems metrics trained samples 

related statlog best known study king 
statlog comprehensive study performed important new learning algorithms introduced bagging boosting svms random forests 
lecun 
presents study compares learning algorithms including svms handwriting recognition problem performance criteria accuracy rejection rate computational cost 
cooper 
results study evaluates nearly dozen learning methods real medical data set accuracy roc metric 
lim 
perform empirical comparison decision trees classification methods accuracy main criterion 
bauer kohavi impressive empirical analysis ensemble methods bagging boosting 

conducts empirical comparison decision trees logistic regression 
provost domingos examine issue predicting probabilities decision trees including smoothed bagged trees 
provost fawcett discuss importance evaluating learning algorithms metrics accuracy roc 

field substantial progress decade 
learning methods boosting random forests bagging svms achieve excellent performance difficult obtain just years ago 
earlier learning methods feedforward neural nets best performance competitive newer methods particularly models calibrated training 
calibration platt method isotonic regression remarkably effective obtaining excellent performance probability metrics learning algorithms performed ordering metrics 
calibration dramatically improves performance boosted trees svms boosted stumps naive bayes provides small noticeable improvement random forests 
neural nets bagged trees memory methods logistic regression significantly improved calibration 
excellent performance metrics calibrated boosted trees best learning algorithm 
random forests close second followed uncalibrated bagged trees calibrated svms uncalibrated neural nets 
models performed poorest naive bayes logistic regression decision trees boosted stumps 
methods clearly perform better worse methods average significant variability problems metrics 
best models perform poorly models poor average empirical comparison supervised learning algorithms performance occasionally perform exceptionally 
acknowledgments provost cod data young slac data hs data zadrozny elkan isotonic regression code 
supported nsf award 
ayer brunk ewing reid silverman 

empirical distribution function sampling incomplete information 
annals mathematical statistics 
bauer kohavi 

empirical comparison voting classification algorithms bagging boosting variants 
machine learning 
blake merz 

uci repository machine learning databases 
breiman 

bagging predictors 
machine learning 
breiman 

random forests 
machine learning 
buntine caruana 

ind recursive partitioning technical report fia 
nasa ames research center 
caruana niculescu 

data mining metric space empirical analysis learning performance criteria 
knowledge discovery data mining kdd 
cooper aliferis aronis buchanan caruana fine glymour gordon meek mitchell richardson spirtes 

evaluation machine learning methods predicting pneumonia mortality 
artificial intelligence medicine 


applied data mining 
new york john wiley sons 
johnson 

support vector machine classifiers applied data 
proc 
eighth jpl airborne geoscience workshop 
joachims 

making large scale svm learning practical 
advances kernel methods 
king feng 

statlog comparison classification algorithms large realworld problems 
applied artificial intelligence 
lecun jackel bottou cortes denker drucker guyon muller sackinger simard vapnik 

comparison learning algorithms handwritten digit recognition 
international conference artificial neural networks pp 

paris ec cie 
lim loh shih 

comparison prediction accuracy complexity training time old new classification algorithms 
machine learning 
niculescu caruana 

predicting probabilities supervised learning 
proc 
nd international conference machine learning icml 
provost 

tree induction vs logistic regression learning curve analysis 
mach 
learn 
res 
platt 

probabilistic outputs support vector machines comparison regularized likelihood methods 
adv 
large margin classifiers 
provost domingos 

tree induction probability rankings 
machine learning 
provost fawcett 

analysis visualization classifier performance comparison imprecise class cost distributions 
knowledge discovery data mining pp 

robertson wright 

order restricted statistical inference 
new york john wiley sons 
schapire 

boosting approach machine learning overview 
msri workshop nonlinear estimation classification 
vapnik 

statistical learning theory 
new york john wiley sons 
witten frank 

data mining practical machine learning tools techniques 
san francisco morgan kaufmann 
second edition 
zadrozny elkan 

obtaining calibrated probability estimates decision trees naive bayesian classifiers 
icml 
zadrozny elkan 

transforming classifier scores accurate multiclass probability estimates 
kdd 
