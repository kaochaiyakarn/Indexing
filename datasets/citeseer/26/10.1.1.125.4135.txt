multi agent reinforcement learning critical survey yoav shoham rob powers computer science department stanford university stanford ca shoham powers cs stanford edu may survey ai multi agent reinforcement learning learning stochastic games 
argue exciting flawed 
fundamental flaw problem problems addressed 
tracing representative sample literature identify defined problems multi agent reinforcement learning single problem view suitable ai remarks believe progress problem 
reinforcement learning rl active research area ai years 
growing interest extending rl multiagent domain 
technical point view taken community realm markov decision problems mdps realm game theory particular stochastic markov games sgs 
body ai multi agent rl small couple dozen papers topic time writing 
contrasts literature single agent learning ai literature learning game theory cases finds hundreds thousands articles books 
despite small number discuss papers 
trace representative historical path literature 
concentrate called bellman heritage multi agent rl learning watkins dayan bellman equations bellman 
specifically discuss littman claus boutilier hu wellman bowling veloso littman greenwald course analyzing papers mention 
section trace bellman heritage summarize results obtained 
results unproblematic cases sgs common payoff aka team pure coordination sgs attempt extend general sum sgs problematic 
section trace back technical awkwardness results view misguided focus nash equilibrium ingredient learning algorithm evaluation criterion 
problem runs deeper believe basic problem addressed 
section argue distinct defined problems addressed tail bellman heritage fit 
identify interesting ai barely addressed line research 
section comments think go tackling 
bellman heritage multi agent rl section review representative sample literature 
start algorithms summarize results reported 
terminology notation 
agent stochastic game sg tuple 
set agents indexed set agent stage games usually thought games normal form see samet exception 
ai set actions pure strategies agent note assume agent strategy space games notational convenience substantive restriction 
rn ri immediate reward function agent stochastic transition function specifying probability game played game just played actions taken 
markov decision problem mdp agent sg mdp simpler structure 
minimax nash start single agent learning algorithm watkins dayan computing optimal policy mdp unknown reward transition functions max procedure directly bellman equations bellman dynamic programming procedures mdps known reward transition functions 
known certain assumptions way actions selected state time learning converges optimal value function simplest way extend multi agent sg setting just add subscript formulation learning agent pretend environment passive qi ai qi ai ri vi vi max qi ai ai ai authors tested variations algorithm sen 
approach reasons 
definition values assumes incorrectly independent actions selected agents 
second longer sensible maximum values update cure problem simply define values function agents actions qi qi ri vi left question update complex nature values 
definition player zero sum sgs littman suggests minimax learning algorithm updated minimax values littman max min 
extended general sum sgs minimax longer motivated settings 
alternative try explicitly maintain belief regarding likelihood agents policies update induced expectation values vi max ai pi qi ai 
approach spirit belief procedures game theory fictitious play brown rational learning kalai lehrer pursued claus boutilier claus boutilier 
specifically adopt belief maintenance procedures fictitious play probability action stage game assumed past empirical frequency 
procedure defined general sum game claus boutilier consider context common payoff team games 
stage game common payoff outcome agents receive payoff 
payoff general different different outcomes agents problem coordination called games pure coordination 
zero sum common payoff sgs special properties discuss section relatively straightforward understand problem learning 
situation different general sum games picture pretty 
pivotal contribution nash learning hu wellman generalization learning general sum games nash updates values nash equilibrium game defined values vi qn 
abuse notation expression represents game qi denotes payoff matrix player denotes nash payoff player 
course general nash equilibria nash payoff may unique 
nash taken apply general sum sgs interpreted nondeterministic procedure 
focus hu wellman special class sgs 
littman articulated explicitly reinterpreting nash friend foe algorithm littman 
informative view algorithms applying different special class sgs 
friend class consists sgs execution algorithm values players define game globally optimal action profile meaning payoff agent joint action payoff joint action 
foe class execution algorithm values define game saddle point 
defined number players simplicity show updated player game friend maxa foe max mina friend updates similarly regular learning foe updates minimax 
greenwald ce learning similar nash uses value correlated equilibrium update greenwald vi cei qn 
nash requires agents select unique equilibrium issue authors address explicitly suggesting possible selection mechanisms 
convergence results main criteria measure performance algorithms ability converge equilibrium self play 
littman minimax learning proven converge limit correct values zero sum game guaranteeing convergence nash equilibrium 
results standard assumptions infinite exploration conditions learning rates proofs convergence single agent learning 
claus boutilier claus boutilier conjecture independent learners belief joint action learners mentioned converge equilibrium common payoff games conditions self play decreasing exploration offer formal proof 
nash learning shown converge correct values classes games defined earlier friend games foe games 
ce learning shown converge uncorrelated nash equilibria number empirical experiments formal results 
focus equilibria 
previous section summarized developments multi agent rl editorial comments 
discuss critically 
results concerning convergence nash quite awkward 
nash attempted treat general sum sgs convergence results constrained cases bear strong similarity known cases games common payoff games 
furthermore note conditions fact quite restrictive hold games defined intermediate values execution protocol 
extremely game satisfy condition case hard verify outset 
note original single agent learning nash concentrates learning correct values case nash equilibria game 
obvious turn procedure guiding play zero sum games 
multiple optimal equilibria exist players need oracle coordinate choices order play converge nash equilibrium begs question learning coordination 
view unsatisfying aspects bellman heritage nash onwards weak awkward convergence assurances limited applicability assumption oracle manifest deeper set issues 
summarized question justifies focus nash equilibrium 
certain local debate regarding initial formulation results resolved papers bowling bowling littman littman hu wellman journal version article hu wellman 
analysis interesting generalizes conditions existence saddle point guaranteed limited zero sum games existence globally optimal nash equilibrium guaranteed limited common payoff games 
hard find natural cases conditions hold special cases 
nash appeals nash equilibrium ways 
uses execution algorithm 
second uses convergence yardstick evaluating algorithm 
troubling ways 
max min strategy employed minimax nash equilibrium strategy prescriptive force 
best equilibrium identifies conditions learning purport say prior 

manifestation lack prescriptive force existence multiple equilibria thorny problem game theory limiting focus games uniquely identified equilibrium assuming oracle merely sweeps problem rug 

magic pick unique equilibrium stage game relevant light fact playing extended sg 
concerned specific details nash descendants concerned convergence nash equilibrium evaluation criterion 
bowling veloso articulate yardstick clearly bowling veloso 
put forward criteria learning algorithm multi agent setting learning converge stationary policy terminate best response play agent property called hannan consistency game theory hannan 
particular conditions require learning terminate stationary nash equilibrium 
useful criterion weak ignores fact playing extended sg 
confront centrality nash equilibrium game theory question play central role ai 
return section briefly view answer view ce learning throwing towel admitting upfront assume correlating device agents 
don stance don see motivation focusing sample correlated equilibrium 
particular argument sample correlated equilibrium easier compute sample nash equilibrium reason think signal inherent sample equilibrium computed available agents situation 
event noted greenwald report simulations ce converge uncorrelated nash equilibria self play 
said literature learning game theory repeated games special case sgs revolves entirely question learning procedure leads nash equilibrium 
opinion gt unclear motivation doing 
comment section focus article 
defined problems multi agent learning view root difficulties field lacked clearly defined problem statement 
nash answer question 
section identify think coherent research agenda multi agent rl 
fact generously offer agendas 
identify view appropriate ai game theoretic point view 
agenda descriptive asks humans learn context learners see erev roth 
name game show experimentally certain formal model learning agrees people behavior typically laboratory experiments 
line legitimate coherent experimental psychology comment 
learning game theory adopted stance implicitly 
researchers propose various dynamics perceived plausible sense proceed investigate converge equilibria 
key concern game theory successful theory support notion nash kinds equilibrium play central role non cooperative game theory 
main limitation line research agreed objective criterion judge reasonableness dynamics 
agendas prescriptive 
ask agents people programs learn 
called distributed ai dai agenda 
problem distributed control central designer controls multiple agents design optimal policy 
assigns adaptive procedure converges optimal policy 
case role equilibrium analysis agents freedom deviate prescribed algorithm 
remaining prescriptive agendas assume learning takes place self interested agents 
understand relationship agendas worthwhile explicitly note obvious fact reinforcement learning single multi agent setting specific form acting actions conditioned runtime observations world 
question best learn specialized version general question best act 
remaining prescriptive agendas diverge interpret best 
call equilibrium agenda 
expected game theory adopt perspective studied game theory fact explicitly rejected place fudenberg kreps seen pursued outside game theory tennenholtz 
agenda described follows 
traditional view non noted game theory somewhat unusual unique having notion equilibrium associated dynamics give rise equilibrium 
cooperative game theory notion optimal strategy meaningless replaced notions best response predominantly nash equilibrium learning strategy just strategy extended game ask vector learning strategies agent forms equilibrium 
course meaningful precise game played including payoff function information structure 
particular context sgs specify aggregate payoff agent limit average sum discounted rewards 
final prescriptive agenda call ai agenda 
name viewed bit ironic part approach taken ai believe sense field 
agenda somewhat 
asks best learning strategy agent fixed class agents game 
retains design stance ai asking design optimal effective agent environment 
just happens environment characterized types agents inhabiting 
raise question parameterize space environments return section 
say ai agenda fact alien past multi agent rl ai discussion implies 
cited earlier concentrates comparing convergence rates algorithms see preliminary analysis comparing performance algorithms environments consisting learning agents hu wellman hu wellman stone littman experimental strands tied formal research agenda particular convergence analyses 
striking exception chang kaelbling chang kaelbling return section 
ai agenda quite prevailing spirit game theory 
precisely adopts optimal agent design perspective consider equilibrium concept central necessarily relevant 
essential divergence approaches lies attitude bounded rationality 
traditional game theory assumed away outset positing perfect reasoning infinite mutual modeling agents 
struggling ways gracefully back assumptions appropriate 
fair say despite notable exceptions cf rubinstein bounded rationality largely unsolved problem game theory 
contrast ai approach embraces bounded rationality starting point adds elements mutual modelling appropriate 
result fewer elegant theorems general greater degree applicability certain cases 
applies general situations complex strategy spaces particular multi agent learning settings 
said equilibrium agenda ai agenda quite different areas overlap looks closely 
discuss section order parameter ize space environments start grapple traditional game theoretic notions type spaces 
furthermore imagines learning algorithms evolve time imagine algorithms evolve equilibrium validating game theory agenda 
principle fold evolutionary element meta learning algorithm includes short term learning long term evolution theoretical construct general provide useful insight 
case trading agent competition tac serves illustrate point 
think tac setting allow application game theoretic ideas 
fact teams certainly gave thought teams behave class opponents programs engaged computation nash equilibria modelling beliefs agents part sophisticated attempts send specific signals agents 
situation sufficiently complex programs concentrated simpler tasks predicting prices different markets treating external events opposed influenced program 
reasonably argue competition team continue improve tac agent eventually agents settle equilibrium learning strategies 
believe true principle argument compelling game fairly simple played long time horizon 
tac strategy space rich convergence happen lifetime 
case provides guidance win competition 
say words ai agenda reconsider bellman heritage discussed earlier fit categorization 
minimax fits nicely ai agenda highly specialized case games 
self play common payoff sgs superficially reminiscent ai agenda probably fits better dai agenda payoff function interpreted payoff agents designer 
near tell nash descendants fit agendas discussed 
pursuing ai agenda ai agenda calls categorizing strategic environments populations agent types agent designed interact 
agent types may come distribution case hope design agent maximal expected payoff distribution case different objective called example agent maximal minimum payoff 
case need way speak agent types 
question best represent meaningful classes agents representation calculate best response 
tac wellman wurman series competitions computerized agents trade non trivial set interacting markets 
won say best response calculation note computationally hard problem 
example known general best response player sg non computable 
concentrate question parameterize space agents challenge 
objective propose specific taxonomy agent types provide guidance construction useful taxonomies different settings 
agents categorized strategy space 
space strategies complex categorization trivial 
coarse way limiting strategy space simply restrict family 
example assume agent belongs class jal learners sense claus boutilier 
principle orthogonal way restricting strategy space place computational limitations agents 
example constrain finite automata bounded number states 
kinds limitations left large space reason disciplined approaches winnowing space 
particular strategies opponent function beliefs restricting assumptions beliefs 
approach taken chang kaelbling chang kaelbling extent stone littman look limited set possible strategies beliefs 
general example assume opponent rational learner sense kalai lehrer place restrictions prior strategies 
note slippery slope asks second agent computational limitations strategy space recursively beliefs agent computational powers strategy space beliefs 
brings realm type spaces zamir interaction type spaces bounded rationality territory see gmytrasiewicz 
research done weaving different considerations coherent comprehensive agent taxonomy 
settle open problem final note regarding temptation label agent types learning weak strong respect taxonomy 
multi agent setting learning teaching inseparable 
choice informed past behavior impacts behavior 
reason neutral term multi agent adaptation apt 
doesn quite ring multi agent learning wage linguistic battle useful keep symmetric view mind thinking pursue ai agenda 
particular helps explain greater sophistication asset 
example consider infinitely repeated game chicken model pursued bounded rationality neyman papadimitriou yannakakis rubinstein 
concerned equilibrium analysis impacted limitations clear technical results obtained directly contribute ai agenda yield dare yield dare presence opponent attempts learn agent strategy play best response example fictitious play system claus boutilier best strategy agent play stationary policy agent learn yield 
watch crazy policy stone littman bully strategy stone littman oscar wilde tyranny weak 
concluding remarks reviewed previous multi agent rl argued believe clear fruitful research agenda ai multi agent learning 
critical remarks previous give impression don appreciate researchers 
truth 
best friends colleagues belong group greatly educated inspired ideas 
granted stand shoulders uncomfortable 
request colleagues decide stand shoulders refrain wearing 
bellman bellman 
dynamic programming 
princeton university press 
bowling veloso michael bowling manuela veloso 
rational convergent learning stochastic games 
proceedings seventeenth international joint conference artificial intelligence 
bowling michael bowling 
convergence problems general sum multiagent reinforcement learning 
proceedings seventeenth international conference machine learning pages 
brown brown 
iterative solution games fictitious play 
activity analysis production allocation 
john wiley sons new york 
ho chong 
sophisticated ewa learning strategic teaching repeated games 
journal economic theory 
chang kaelbling yu han chang leslie pack kaelbling 
playing believing role beliefs multi agent learning 
proceedings nips 
claus boutilier caroline claus craig boutilier 
dynamics reinforcement learning cooperative multiagent systems 
proceedings fifteenth national conference artificial intelligence pages 
erev roth ido erev alvin roth 
predicting people play games reinforcement leaning experimental games unique mixed strategy equilibria 
american economic review september 
fudenberg kreps fudenberg david kreps 
learning mixed equilibria 
games economic behavior 
gmytrasiewicz gmytrasiewicz durfee 
decision theoretic approach coordinating multiagent interactions 
proceedings twelfth international joint conference artificial intelligence pages 
greenwald amy greenwald keith hall roberto serrano 
correlated learning 
nips workshop multiagent learning 
hannan hannan 
approximation bayes risk repeated plays 
contributions theory games 
hu wellman hu wellman 
multiagent reinforcement learning theoretical framework algorithm 
proceedings fifteenth international conference machine learning pages 
hu wellman hu michael wellman 
learning agents dynamic multiagent system 
journal cognitive systems research 
hu wellman hu michael wellman 
multiagent qlearning 
journal machine learning 
samet dov samet 
learning play games extensive form valuation 
economics 
kalai lehrer ehud kalai ehud lehrer 
rational learning leads nash equilibrium 
econometrica 
littman michael littman 
generalized reinforcement learning model convergence applications 
proceedings th international conference machine learning pages 
littman michael littman 
markov games framework multiagent reinforcement learning 
proceedings th international conference machine learning pages 
littman michael littman 
friend foe learning general sum games 
proceedings eighteenth international conference machine learning 
zamir 
zamir 
formulation bayesian analysis games incomplete information 
international journal game theory 
john william 
noncomputable strategies discounted repeated games 
economic theory 
neyman abraham neyman 
bounded complexity justifies cooperation finitely repeated prisoner dilemma 
economic letters pages 
papadimitriou yannakakis papadimitriou yannakakis 
complexity bounded rationality 
stoc pages 
rubinstein ariel rubinstein 
modeling bounded rationality 
mit press 
sen sandip sen john hale 
learning coordinate sharing information 
proceedings twelfth national conference artificial intelligence pages seattle wa 
stone littman peter stone michael littman 
implicit negotiation repeated games 
john jules meyer milind tambe editors pre proceedings eighth international workshop agent theories architectures languages atal pages 
tennenholtz moshe tennenholtz 
efficient learning equilibrium 
proceedings nips 
watkins dayan watkins dayan 
technical note learning 
machine learning may 
wellman wurman michael wellman peter wurman 
trading agent competition research community 
ijcai workshop agent mediated electronic trading stockholm august 

