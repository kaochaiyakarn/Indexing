intrusion detection unlabeled data clustering leonid eskin sal stolfo department computer science columbia university new york ny flp cs columbia edu contact author eskin cs columbia edu keywords intrusion detection anomaly detection clustering unlabeled data intrusions pose serious security risk network environment 
systems hardened types intrusions intrusions successful making systems detecting intrusions critical security system 
new intrusion types detection systems unaware difficult detect 
current signature methods learning algorithms rely labeled data train generally detect new intrusions 
addition labeled training data order train misuse anomaly detection systems typically expensive 
new type clustering intrusion detection algorithm unsupervised anomaly detection trains unlabeled data order detect new intrusions 
system manually classified data necessary training 
method able detect different types intrusions maintaining low false positive rate verified kdd cup dataset 
network intrusion attack network compromises stability security information stored computers connected 
wide range activity falls definition including attempts destabilize network gain unauthorized access files privileges simply misuse software 
added security measures attacks 
goal intrusion detection build system automatically scan network activity detect intrusion attacks 
attack detected system administrator informed take corrective action 
traditionally signature automatic detection methods task 
methods extract features network data detect intrusions comparing feature values set attack signatures provided human experts 
obviously methods detect new types intrusions intrusions corresponding signature 
signature database manually revised new type attack discovered 
approaches data mining machine learning algorithms train labeled instances preclassified attack network data 
approaches generalization ability data mining methods order attempt detect new attacks 
major paradigms training data mining intrusion detection systems misuse detection anomaly detection 
misuse detection approaches instance set data labeled normal intrusion machine learning algorithm trained labeled data 
example data mining misuse detection system madam id system extracted features network connections built detection models connection records represented summary traffic network connection :10.1.1.46.2976
detection models generalized rules classify data extracted features 
approaches advantage able automatically retrain intrusion detection models different input data include new types attacks 
insert labeled instances new attacks dataset method rule sets detect 
anomaly detection approaches build models normal data attempts detect deviations normal model observed data 
anomaly detection algorithms advantage detect new types intrusions new intrusions assumption deviate normal network usage 
traditional anomaly detection algorithms require set purely normal data train model 
data contains intrusions buried training data algorithm may detect instances attacks assume normal 
labeled purely normal data readily available 
generally deal large volumes network data difficult tiresome classify manually 
obtain labeled data simulating intrusions limited set known attacks able simulate new types attacks occurring reflected training data 
manual classification limited identifying known classification time types attacks restricting detection system identifying types 
generating purely normal data difficult practice 
collect raw data network environment hard guarantee attacks time collecting data 
new type intrusion detection algorithm unsupervised anomaly detection known anomaly detection noisy data address problems 
algorithm takes inputs set unlabeled data attempts find intrusions buried data 
intrusions detected apply train misuse detection algorithm traditional anomaly detection algorithm data 
unsupervised anomaly detection algorithms assumptions data motivate general approach 
assumption number normal instances vastly number intrusions 
second assumption intrusions qualitatively different normal instances 
basic idea intrusions different normal rare appear outliers data detected 
despite inherent limitations unsupervised anomaly detection algorithms major advantage able process unlabeled data detect intrusions 
addition types algorithms useful semi automated detection helping analysts focus suspicious instances 
previous approach unsupervised anomaly detection involves building probabilistic models training data determine network data instance anomaly 
approach data modeled probabilistic model known perform kind data 
current drop requirement probabilistic model inter point distances motivate algorithm 
approach describe clusters data instances clusters simple distance metric 
clustering performed unlabeled data requiring feature vectors labels 
data clustered label anomalies instances appear small clusters 
reason method works explained assumptions data unsupervised anomaly detection 
assumption number normal instances vastly outnumber number intrusion instances 
implies normal instances form large clusters compared intrusions 
second assumption intrusions normal instances qualitatively different fall clusters 
unsupervised anomaly detection algorithms limited able detect attacks assumptions hold data case 
example algorithms able detect malicious intent authorized network uses seemingly legitimate way 
reason intrusion qualitatively different normal instances user 
algorithm may cluster instances intrusion undetectable 
example algorithm difficulty detecting syn flood dos attack 
reason attack instances intrusion occurs similar number normal instances 
algorithm may label instances attack size cluster may large typical clusters normal instances 
evaluated cluster unsupervised anomaly detection method real network data 
training testing done different subsets kdd cup data popular widely intrusion attack dataset 
various combinations subsets dataset training testing standard cross validation techniques combination yielding slightly different results 
average detection rate false positive rate 
advantages method traditional approaches data unlabeled method uses domain knowledge security results indicate approach unsupervised anomaly detection promising 
related clustering known studied problem 
studied fields including statistics machine learning databases visualization 
basic methods clustering include linkage means techniques 
means passes training data pass shifts cluster centers mean data points assigned cluster 
re assigns data points nearest prototype continues iterating manner significant changes cluster center positions occur 
means method generally produces accurate clustering linkage methods greater time complexity extremely important factor network intrusion detection due large dataset sizes 
optimizations means large datasets exist perform sufficiently fast datasets high dimensionality 
techniques clustering include clarans birch density methods dbscan ai methods self organizing maps growing networks :10.1.1.121.9220
anomaly detection widely method field computer security approaches utilize detecting intrusions 
various techniques modeling anomalous normal data developed intrusion detection 
survey techniques 
approach modeling normal sequences look ahead pairs contiguous sequences statistical method determine sequences occur frequently intrusion data opposed normal data :10.1.1.43.6197
approach prediction model obtained training decision trees normal data uses neural networks obtain model :10.1.1.134.4855
lane brodley evaluated unlabeled data anomaly detection looking user profiles comparing activity intrusion activity normal :10.1.1.158.5767
technique developed sri emerald system uses historical records normal training data 
compares distributions new data distributions obtained historical records differences distributions indicate intrusion 
problem approach historical distributions contain intrusions system may able detect similar intrusions new instances 
algorithm unsupervised anomaly detection 
algorithm mixture model explaining presence anomalies machine learning techniques estimate probability distributions mixture detect anomalies 
distance outliers similar approach 
approaches examine inter point distances instances data determine points outliers 
difference approaches problem unsupervised anomaly detection nature outliers different 
network data intrusion occurs multiple times means similar instances data 
number instances intrusion significantly smaller typical cluster normal instances 
problem related anomaly detection study outliers field statistics 
various techniques developed detecting outliers univariate multivariate structured data probability distribution 
survey outliers statistics 
methodology section describe dataset build clusters detect intrusions 
examine type data dataset features extracted intrusion types represented 
discuss data normalized standard deviation training set system able create clusters data coming different distributions 
description metric clustering algorithm follows methods labeling clusters classifying unseen instances discussed 
dataset description dataset kdd cup data contained wide variety intrusions simulated military network environment 
consisted approximately data instances vector extracted feature values connection record obtained raw network data gathered simulated intrusions 
connection sequence tcp packets ip addresses 
tcp assembled connection records bro program modified madam id :10.1.1.46.2976
connection labeled normal exactly specific kind attack 
labels assumed correct 
simulated attacks fell categories dos denial service syn flood unauthorized access remote machine password guessing unauthorized access superuser root functions buffer overflow attack probing surveillance probing vulnerabilities port scanning 
total attack types 
extracted features included basic features individual tcp connection duration protocol type number bytes transferred flag indicating normal error status connection 
features individual connection obtained domain knowledge included number file creation operations number failed login attempts root shell obtained 
number features computed second time window 
included number connections host current connection past seconds percent connections syn rej errors number connections service current connection past seconds 
total features continuous values 
normalization algorithm designed general able create clusters dataset arbitrary distribution 
problem typical data different features different scales 
causes bias features features 
example consider feature vectors set coming different distributions euclidean metric squared distance feature vectors gamma gamma gamma dominated second column 
solve problem convert data instances standard form training dataset distribution 
assumption training dataset accurately reflects range deviation feature values entire distribution 
normalize data instances fixed range choosing hard code cluster width fixed range 
training dataset average standard deviation feature vectors calculated avg vector instance std vector gamma instance gamma avg vector vector jth element feature vector 
instance feature vector training set converted follows new instance instance gamma avg vector std vector words feature value calculate standard deviations away average result new value feature 
continuous features converted symbolic ones preserved 
effect transformation instance space standardized space statistical information retrieved training set 
metric main assumptions data instances having label tend closer instances different labels metric 
finding constructing appropriate metric critical performance method 
particular choice metric dictated domain 
detecting network intrusions features data instances important greater weight differences values features greater contribution distance 
experimented weighted metrics higher weights assigned different subsets features 
standard euclidean metric equally weighted features 
reason weighted metric show increase performance significant amount 
importantly tuning metric parameters achieve maximum performance particular domain data distribution feature set undermine system generality contribute fitting 
features took discrete values issue factor metric 
metric added constant value squared distance instances discrete feature distinct values 
equivalent treating different value feature space 
clustering create clusters input data instances simple variant single linkage clustering 
effective clustering algorithm advantage working near linear time 
algorithm starts empty set clusters generates clusters single pass dataset 
new data instance retrieved normalized training set computes distance centroids clusters cluster set far 
cluster shortest distance selected distance constant cluster width instance assigned cluster 
new cluster created instance center 
formally algorithm proceeds follows assume fixed metric constant cluster width dist cluster instance distance metric defining instance defining instance cluster feature vector defines center feature space cluster 
refer defining instance centroid 

initialize set clusters empty set 

obtain data instance feature vector training set 
empty create cluster defining instance add find cluster closest instance 
words find cluster dist dist 

dist associate cluster away cluster new cluster created fc cluster defining instance 

repeat steps instances left training set 
labeling clusters hope metric instances classification close different classifications far apart 
appropriate cluster width chosen clustering obtain set clusters instances single type 
corresponds second assumption data normal intrusion instances qualitatively different 
dealing unlabeled data access labels training 
necessary find way determine clusters contain normal instances contain attacks anomalies 
assumption data normal instances constitute overwhelmingly large portion 
training dataset 
assumption highly probable clusters containing normal data larger number instances associated clusters containing anomalies 
label percentage clusters containing largest number instances associated normal 
rest clusters labeled anomalous considered contain attacks 
problem may arise approach depending sub types normal instances training set 
example may different kinds normal network activity different protocols ftp telnet www uses distinct point feature space network data instances tend cluster 
turn produce large number normal clusters type normal network 
clusters relatively small number instances associated clusters containing attack instances 
normal clusters incorrectly labeled anomalous 
prevent problem need insure percentage normal instances training set extremely large relation attacks 
type normal network adequate larger representation type sub type attack 
detection clusters created training set system ready perform detection intrusions 
instance classification proceeds follows 
convert statistical information training set clusters created 
instance conversion 

find cluster closest metric cluster cluster set dist dist 

classify label normal anomalous 
words simply find cluster closest converted give cluster classification 
system evaluation results performance measures evaluate system interested major indicators performance detection rate false positive rate 
detection rate defined number intrusion instances detected system divided total number intrusion instances test set 
false positive rate defined total number normal instances incorrectly classified intrusions divided total number normal instances 
indicators performance measure percentage intrusions system able detect incorrect classifications process 
calculate values labeled data measure performance 
filtering training dataset kdd dataset obtained simulating large number different types attacks normal activity background 
goal produce training set learning methods labeled data 
result proportion attack instances normal ones kdd training dataset large compared data expect observe practice 
second major assumption states training set represent normal network activity attacks rare data represents normal operation 
raw kdd dataset obviously satisfy condition 
trained system raw set obtained poor performance expected 
meet requirement generated training sets kdd data filtering attacks 
filtered resulting training set consisted attack normal instances 
parameter estimation main parameters values needed fixed performance measured 
cluster width doing clustering determines close instances assigned cluster 
second percentage largest clusters labeled normal detection phase 
goal set values variables performance entire domain maximized 
section report results dataset give intuitions dynamics parameters behave 
section results testing separate data sets give accurate measure performance 
single subset kdd data run series tests different values variables measuring resulting performance 
hazard training set represent narrow spectrum domain fit values variables spectrum 
subset chose representative entire kdd dataset contained instances type attack 
values cluster width maximized results set values fixed subsequent experiments different datasets 
parameters set compare best values type data 
cluster width measure indicating average radius feature width detection rate false positive rate table results tests obtain value percentage largest clusters label normal detection 
cluster width fixed tests 
width detection rate false positive rate table results tests obtain value cluster width variable 
cluster width chosen subsequent tests 
space cluster containing instances type 
particular property domain network connection records 
property network attempts measure ratio number sub types normal instances total number different sub types 
fixing values cluster width percentage largest clusters variables measuring performance single training test set results shown table 
decided value subsequent tests produced acceptable false positive rate sacrificing detection rate 
find value cluster width conducted tests training test set combination fixed value results tests shown table 
cluster width chosen width produced slightly higher detection rate false positive rate 
difference minor tests different datasets indicated width performance worse width 
shows roc receiver operating characteristic curve depicting relationship false positive detection rates fixed training test set combination 
roc curves way visualizing trade offs detection false positive rates 

roc curve false positive vs detection rate fixed training test set combination 
cross validation testing parameters specified evaluated system variant cross validation method 
cross validation standard technique obtain estimation method performance unseen data partitioned entire kdd dataset subsets containing approximately instances data 
unfortunately distribution attacks kdd dataset uneven cross validation difficult 
subsets contained instances single type 
example th th th th portions full dataset contained smurf attacks data instances th entirely neptune intrusions 
require intrusion normal sub types represented degree training dataset subsets failed meet requirement 
cross validation training subsets selected 
subsets contained mix various intrusion types conformed necessary assumptions data 
produce clustering representative intrusions 
subsets selected filtered intrusion constitute resulting dataset 
system trained filtered data cluster set resulted saved 
evaluated system performance cluster set subsets time test sets 
process repeated times different subset selected training time 
results shown table 
test sets filtered contain approximately equal number instances type attack 
necessary order meaningful measure performance example intrusions test set single type detection rate indicate system suited detecting particular type attack 
test sets contain equal percentage different types instances detection rate show system capable detecting different types intrusions 
variations clustering detection addition experiments cluster width constant indicating percent largest clusters labeled normal explored variations clustering detection methods evaluated performance single training test sets 
clustering method altered allowing multiple version passes creation assignment instances clusters 
previously pass instance cluster nearest set currently existing clusters instance assigned cluster cluster width away metric 
farther away new cluster created instance 
scheme instances appeared earlier training dataset smaller set existing clusters compare distance 
thought possibly resulted non optimal assignment instance cluster sense closest type sub type instances cluster representing set assigned cluster width closest cluster set time instance considered 
cluster non optimal choice represent different type sub type instance assigned 
prevent occurring implemented double pass method create clusters assigning instances second pass training set assign instances closest cluster complete set 
performance system change shown table 
variation changing clustering method 
performance obtained changing clustering method passes worse performance clustering pass 
second variation applied detection method choosing closest cluster instance assigning cluster classification normal anomalous chose training set test set detection rate false positive rate table performance system various training test set combinations 
represent second third tenth partitions kdd cup dataset respectively 
cluster width set largest clusters marked normal 
training test sets filtered prior 
training done total data data subset clusters 
detection rate false positive rate table results labeling majority variation detection method 
closest clusters instance assigned majority classification larger number clusters labeled anomalous instance classified correspondingly normal 
experimenting changes evaluating performance test set described concluded improve detection accuracy cases decreased detection accuracy 
analysis results cross validation show performance system depends heavily training set 
fact depends training set meets requirement representing wide variety intrusion normal sub types 
table shows training sets resulted high false positive rate compared sets 
closer examination datasets revealed contained smaller number different normal sub types sets 
resulted failure create clusters normal regions feature space data instances regions assigned incorrect clusters possibly marked anomalous 
may caused high false positive rate 
training set showed best performance test sets high detection low false positive rate 
training testing sets detection false positive rates obtained 
hand reversed situation training testing detection rate obtained similar false positive rate 
explained fact dataset different types intrusions represented set training resulted better cluster set training turn manifested increased detection rate 
actual application system expected performance greatly depends composition data shown variability detection rate different subsets 
datasets significant detection rate low false positive suggests method able detect attacks successfully 
detection vs false positive rates trade false positive detection rates inherently machine learning methods 
comparing quantities evaluate performance invariant bias distribution labels data 
especially important intrusion detection problems normal data intrusion data factor 
classical accuracy measure misleading system classifies data normal accuracy 
system false positive vs detection rate trade apparent 
percent largest clusters labeled normal decreased detection rate increased substantially larger number clusters labeled anomalous 
intrusion instances assigned clusters previously classified normal clusters labeled normal classified correctly intrusions 
time false positive rate increased normal instances assigned clusters previously labeled normal labeled anomalous classified intrusions 
clusters represented anomalous regions feature space normal instances assigned incorrectly due sub optimal metric assumption instances type sub type cluster satisfied 
negative effect mis assignment performance avoided percent largest clusters labeled normal decreased 
successfully utilize system suitable value percentage yield high detection rate keeping false positive rate tolerable low value 
assume mis assignment instances clusters occurs essentially amounts measuring property domain ratio number sub types normal instances total number different sub types 
ratio reflected number clusters representing normal regions feature space relative number clusters representing regions 
reality assumptions met mis assignment occur ratio estimated indirectly noting value percentage largest clusters labeled normal false positive detection rate combination favorable 
example choose value minimizes sum possibly weighted 
variations algorithms concluded changes clustering algorithm detection method increase performance reasons 
changing detection method perform classification majority nearest clusters labels showed improved results single training test set measure performance 
detection rate generally higher values equal keeping false positive rate relatively low 
results varied greatly apparent pattern increased 
lead suspect value produced best results related particular training test set represent value increased performance entire domain 
words number nearest clusters considered yielded best results reality dependent training test set combination portion domain represented 
value training different sets give different favorable results 
suspicion fitting single training test set confirmed tested labeling majority method training test set combinations 
results tests indicated method improve cases lowered performance 
idea changing clustering double pass method discarded immediately results variation obtained 
showed detection rate false positive rate remaining slightly higher double pass method 
possible explanation increased false positive rate double pass method instances assigned cluster average instances evenly distributed clusters 
led inability differentiate anomalous normal clusters detection phase due distribution truly normal clusters instances assigned previously 
labeled anomalous increased false positive rate 
contribution method detecting intrusions feature vectors collected network information classifications vectors 
designed system implemented method able detect large number intrusions keeping false positive rate reasonably low 
primary advantages system signature classifiers learning algorithms require labeled data training sets 
manual classification training data needs done 
second aware new types intrusions order system able detect 
required data conform assumptions 
system try automatically determine data instances fall normal class ones intrusions 
detection rate system implemented high algorithms relying labeled data system useful 
prior classification required training data knowledge needed new attacks automate process training creating new cluster sets 
practice mean periodically weeks example collecting raw data network extracting feature values training resulting set feature vectors 
help detect new unknown attacks 
addition method semi automated detection helping analysts focus portions data contain intrusions 
involves possible extensions modifications method achieve better performance better degree automation 
currently detection clusters labeled anomalous normal relative number instances contain 
possibility label clusters outliers feature space anomalous normal 
involves making assumption normal data different sub types clustered sub types intrusion data near normal region feature space 
achieve greater degree automation determine value percentage largest clusters labeled normal variable automatically standard deviation average values number instances clusters 
scheme clusters containing small fixed standard deviations lower mean number instances labeled anomalous 
advantage method current system new unknown attacks introduced network environment ratio number normal sub types total number sub types decrease 
having fixed value reflect decreased ratio cause algorithm label clusters normal really labeled anomalous 
result detection rate decrease new intrusion types introduced periodic manual updates value required 
system determines value automatically manual intervention required long periods time 
touretzky fritzke leen 
growing neural gas network learns topologies 
advances neural information processing systems 
barnett lewis 
outliers statistical data 
john wiley sons 
bock 
automatic classification 
ruprecht 
markus breunig hans peter kriegel raymond ng jorg sander 
lof identifying local outliers 
acm sigmod int 
conf 
management data pages 
denning 
intrusion detection model 
ieee transactions software engineering se 
eskin 
anomaly detection noisy data learned probability distributions 
proceedings international conference machine learning 
ester kriegel sander xu :10.1.1.121.9220
density algorithm discovering clusters large spatial databases noise 
proceedings nd international conference knowledge discovery data mining pages 
fukunaga 
statistical pattern recognition second edition 
academic press boston ma 
ghosh 
study neural networks anomaly misuse detection 
proceedings th usenix security symposium 
helman 
base system prioritizing information exploration uncertainty 
ieee transactions systems man cybernetics part systems humans 
alexander hinneburg daniel keim 
clustering methods large databases past 
alex christos faloutsos ghandeharizadeh editors sigmod proceedings acm sigmod international conference management data june philadephia pennsylvania usa 
acm press 
hofmeyr stephanie forrest somayaji :10.1.1.43.6197
intrusion detect sequences system calls 
journal computer security 
javitz valdes 
nides statistical component description justification 
technical report computer science sri international 
kdd 
kdd cup dataset kdd ics uci edu databases kddcup kddcup html 
edwin knorr raymond ng 
algorithms mining distance outliers large datasets 
proc 
th int 
conf 
large data bases vldb pages 
edwin knorr raymond ng 
finding intensional knowledge distance outliers 
vldb journal pages 
lane brodley :10.1.1.158.5767
sequence matching learning anomaly detection computer security 
aaai workshop ai approaches fraud detection risk management pages 
aaai press 
lee stolfo :10.1.1.134.4855
data mining approaches intrusion detection 
proceedings usenix security symposium 
lee stolfo mok :10.1.1.46.2976
data mining flow environments experiences intrusion detection 
proceedings conference knowledge discovery data mining kdd 
ng han 
efficient effective methods spatial data mining 
proceedings large data bases 
paxson 
bro system detecting network intruders real time 
proceedings th usenix security symposium san antonio tx 
foster provost tom fawcett ron kohavi 
case accuracy estimation comparing induction algorithms 
proceedings fifteenth international conference machine learning july 
rojas 
neural networks systematic 
springer berlin 

method discovering data groups 

christina stephanie forrest barak pearlmutter 
detecting intrusions system calls alternative data models 
ieee symposium security privacy pages 
ieee computer society 
zhang ramakrishnan livny 
birch efficient data clustering method large databases 
proc 
acm sigmod international conference management data 

