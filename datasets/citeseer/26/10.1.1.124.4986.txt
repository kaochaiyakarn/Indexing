cient support cluster web servers aron peter druschel willy zwaenepoel department computer science rice university studies mechanisms policies supporting persistent connections cluster web servers employ contentbased request distribution 
mechanisms cient content distribution requests back nodes cluster server 
trace driven simulation shows mechanisms combined extension locality aware request distribution lard policy ective yielding scalable performance requests 
implemented simpler mechanisms back forwarding 
measurements mechanism connection extended lard prototype cluster driven traces actual web servers con rm simulation results 
throughput prototype times better achieved conventional weighted round robin request distribution 
addition throughput persistent connections better 
clusters commodity workstations increasingly popular hardware platform cost ective high performance network servers 
achieving scalable server performance platforms critical delivering high performance users cost ective manner 
state art cluster web servers employ front node responsible distributing incoming requests back nodes manner transparent clients 
typically front distributes requests load back nodes balanced 
content request distribution front additionally takes account content service requested deciding back node client request assigned 
content request distribution allows integration server nodes specialized cer appear proc 
annual usenix technical conference monterey ca june 
tain types content services audio video permits partitioning server database scalability enables clever request distribution policies improve performance 
previous proposed locality distribution lard content policy achieves cache hit rates addition load balance dynamically partitioning server working set back nodes 
investigate mechanisms policies content request distribution presence persistent keepalive client connections 
persistent connections allow clients submit multiple requests server single tcp connection reducing client latency server overhead 
unfortunately persistent connections pose problems clusters content request distribution requests single connection may assigned di erent back nodes satisfy distribution policy 
describes cient mechanisms content request distribution extension lard policy presence connections 
presents simulation study mechanisms reports experimental results prototype cluster implementation 
results show persistent connections supported ciently cluster web servers content request distribution 
particular demonstrate back forwarding lard policy achieves better performance persistent connections 
rest organized follows 
section provides background information lard states problems posed clusters content request distribution 
section considers mechanisms achieving content request distribution presence persistent connections 
extended lard policy section 
section presents performance analysis request distribution mechanisms 
simulation study various mechanisms ex tended lard policy described section 
section discusses prototype implementation section reports measurement results obtained prototype 
discuss related section conclude section 
background section provides background information persistent connections content request distribution lard strategy 
state problem persistent connections pose content request distribution 
persistent connections obtaining html document typically involves requests web server fetch embedded images browsers send request separate tcp connection 
increases latency perceived client number network packets resource requirements server 
enables browsers send requests server single tcp connection 
anticipation receiving requests server keeps connection open con gurable interval typically seconds receiving request method amortizes overhead establishing tcp connection cpu network packets multiple requests allows pipelining requests 
sending multiple server responses single tcp connection short succession avoids multiple tcp slow starts increasing network utilization ective bandwidth perceived client 
rfc speci es purpose backward compatibility clients servers persistent connections explicit header 
rest connections assumed support persistence 
consider new features support persistent connections request pipelining 
content request distribution content request distribution technique employed cluster network servers front takes account service content requested deciding backend node serve agiven request 
contrast purely load schemes weighted roundrobin wrr commercial high performance cluster servers distribute incoming requests round robin fashion weighted measure load di erent back nodes 
potential advantages content request distribution increased performance hit rates back main memory caches increased secondary storage scalability due ability partition server database di erent back nodes ability back nodes specialized certain types requests audio video 
content request distribution front establish tcp connection client prior assigning connection backend node nature target client request uences assignment 
mechanism required allows chosen backend node serve request tcp connection established 
reasons performance security interoperability desirable mechanism transparent client 
discuss mechanisms purpose section 
locality aware request distribution locality aware request distribution lard speci strategy content request distribution focuses rst advantages cited improved cache hit rates backends 
lard strives improve cluster performance simultaneously achieving load balancing high cache hit rates back ends 
illustrates principle lard cluster back ends working set targets andc incoming request stream 
front directs requests back requests back 
doing increased likelihood request nds requested target cache back 
contrast round robin distribution incoming requests requests targets arrive ends 
increases likelihood cache sum sizes targets generally size working set exceeds size main memory cache individual back node 
round robin distribution cluster scale larger working sets node main memory cache entire working set 
lard ective cache size approaches sum individual node cache sizes 
adding nodes cluster accommodate increased tra discussion term target refer web document speci ed url applicable arguments get command 
due additional cpu power larger working sets due increased ective cache size 
problem persistent connections pose problem clusters employ content request distribution including lard 
problem existing scalable mechanisms content distribution operate granularity tcp connections 
multiple requests may arrive single tcp connection 
mechanism distributes load granularity tcp connection constrains feasible distribution policies requests arriving connection served single backend node 
constraint serious clusters certain requests served subset back nodes 
problem correctness back node may receive requests serve 
clusters node capable serving valid request lard policy partition working set performance loss may result back node may receive requests current share working set 
show section performance loss set performance advantages persistent connections cluster servers 
mechanisms content request distribution front performs content request distribution establish client connection decide back node serve request 
needs mechanism allows chosen back node serve request established client connection 
section discuss mechanisms 
simplest mechanisms having front redirect client browser chosen back node sending redirect locality aware request distribution response returning java applet contacts appropriate back executed browser 
mechanisms persistent connections serious drawbacks 
redirection introduces additional delay address individual back nodes exposed clients increases security risks simple outdated browsers may support redirection 
reasons consider client transparent mechanisms remainder 
relaying front simple client transparent mechanism relaying front 
depicts mechanism mechanisms discussed rest section 
front maintains persistent connections back connections back nodes 
request arrives client connection front assigns request forwards client request message appropriate back connection 
response arrives back node front forwards data client connection bu ering data necessary 
principal advantage approach simplicity transparency clients backend nodes fact allows content distribution granularity individual requests presence persistent connections 
serious disadvantage fact response data forwarded frontend 
may render front bottleneck front uses substantially powerful hardware back ends 
conceivable small clusters built front specialized layer switch ability transport connections 
aware actual implementations approach 
furthermore results section indicate front bottleneck relaying front er signi cant perfor back client front request response relaying front multiple handoff back forwarding mance advantages scalable mechanisms 
multiple tcp connection hando complex mechanism involves tcp hando protocol front back nodes 
hando protocol allows front transfer established client connection back node 
state transferred back transmits response data directly client bypassing front 
data client primarily tcp ack packets forwarded front appropriate back node cient manner 
previous designed implemented evaluated hando protocol 
single hando protocol support persistent connections requests served back node connection originally handed design hando protocol extended support allowing frontend migrate connection back nodes 
advantage multiple hando protocol allows content request distribution granularity individual requests presence persistent connections 
frontend relaying hando approach cient scalable response network tra bypasses front 
hando approach requires operating systems front back nodes customized vendor speci loadable kernel module 
design module relatively complex especially multiple hando supported 
preserve advantages persistent connections reduced server overhead reduced mechanisms request distribution client latency overhead migrating connections back nodes low tcp pipeline kept draining migration 
back request forwarding third mechanism back request forwarding combines tcp single hando protocol forwarding requests responses back nodes 
approach front hands client connections appropriate back node tcp single hando protocol 
request arrives persistent connection served back node currently handling connection connection handed back node 
front informs connection handling back node back node serve request 
backend node requests content service question directly node forwards response client client connection 
depending implementation lateral requests forwarded persistent connections back nodes network le system 
advantages back request forwarding lie fact complexity overhead multiple tcp hando avoided 
disadvantage overhead forwarding responses connection handling back node 
observation suggests back request forwarding mechanism appropriate requests result relatively small amounts response data 
results section show due relatively small average content size today web back request forwarding approach throughput delay idle load load max acceptable delay difference server throughput delay competitive 
policies section presents extension lard policy works ciently presence persistent connections back request forwarding mechanisms previous section 
front relaying mechanism tcp multiple hando mechanism allow requests distributed granularity individual requests 
place restriction request distribution policies 
particular lard policy combination mechanisms loss locality 
back forwarding mechanism hand place restrictions distribution policy mandates connection handed back node 
requests arrive persistent connection served back node policy instruct back request back node 
developed extension lard policy ciently distribute requests cluster uses back forwarding mechanism 
subsection brie presents standard lard strategy 
proceed extension 
lard strategy lard strategy yields scalable performance achieving load balancing cache locality back servers 
purpose achieving cache locality lard maintains mappings targets back nodes target considered cached associated back nodes 
achieve balance load distribution locality lard uses cost metrics cost balancing cost locality cost replacement 
intuition de nition metrics explained shows throughput delay characteristics typical back server function load measured number active connections 
load point idle de nes value node potentially underutilized 
overload de ned di erence delay back node operating load compared back node operating point idle unacceptable 
metric cost balancing captures delay servicing request queued requests 
cost locality hand re ects delay arising due presence absence target cache 
cost replacement cost re ects potential overhead caused replacement target cache 
cost metrics de ned shown 
unit cost load de ned delay experienced request cached target unloaded server 
aggregate cost sending request particular server de ned sum values returned cost metrics 
request arrives front lard policy assigns request back node yields minimum aggregate cost nodes updates mappings re ect requested target cached back node experimental results apache webserver running freebsd indicate settings overload idle iss cost 
settings simulator prototype results 
extended lard strategy basic lard strategy bases choice back node serve agiven request lard di erently pai proven strategies equivalent idle low cost high low cost balancing target server load server idle infinity load server load server cost locality target server iss cost target mapped server load server cost replacement target server iss cost target mapped server current load current assignment back nodes current partitioning working set 
extended policy works connections back forwarding mechanisms consider additional factors choice back node serve request arriving persistent connection may constrained choice back node connection handed particular policy considerations 
best choice back node handle persistent connection depends requests expected connection 

assigning request back node connection handling node causes additional forwarding overhead 
overhead weighed cost reading requested content connection handling node local disk 

requested content fetched local disk requested back node content cached connection handling node 
caching content reduces cost requests content node handling connection causes potential replication content back nodes reducing aggregate size server cache 
intuition extended lard policy follows 
regarding due structure typical web documents additional requests persistent connection normally arrive response rst request delivered client 
reason front base choice back node handle connection knowledge rst request 
lard cost metrics respect extended lard policy adds additional considerations choosing node handle request arriving handed persistent connection 
long utilization connection handling node local disk low content read disk avoiding forwarding overhead 
second choosing back forward request policy considers nodes candidates currently cache requested target 
regarding extended lard policy uses simple heuristic decide content cached connection handling node 
disk utilization connection handling node high assumed node main memory cache thrashing 
requested content locally 
disk utilization low requested content added node cache 
extended lard policy 
rst request arrives persistent connection connection handling node chosen basic lard policy described section 
subsequent request persistent connection target cached connection handling node disk utilization connection handling node low queued disk events request assigned 
cost metrics section computed connection handling node back nodes target cached 
request assigned node yields minimum aggregate cost 
purpose computing lard cost metrics single load unit assigned connec bandwidth mb apache apache average file size kb apache tion handling node active connection handles 
back forwarding mechanism fetch documents nodes node additionally assigned load units number outstanding requests batch pipelined requests duration request handling requests 
ideally front assign load remote node service time request 
front determine exactly request served estimate service time batch ofn pipelined requests 
assigns load remote node entire batch service time 
front estimates number requests batch closely spaced requests arrived connection estimates batch service time time takes batch arrives connection goes idle front assumes previous requests nished new batch requests arrives connection 
lard mappings targets back nodes updated time target fetched back node 
noted extended lard policy equivalent lard requests 
performance analysis distribution mechanisms section presents simple analysis fundamental performance tradeo multiple hando mechanism versus back idle connection detected front absence acks client 
bandwidth mb flash flash average file size kb flash mechanism request distribution presence persistent connections 
compared multiple hando mechanism back forwarding mechanism trades byte response forwarding cost hando overhead 
suggest back request forwarding appropriate requests result small amounts response data multiple hando approach win case large responses assuming factors ect performance equal 
figures show results simple analysis con rms quanti es intuition 
analysis predicts server bandwidth function average response size obtained cluster nodes multiple hando back forwarding mechanism 
analysis values hando overhead request overhead byte forwarding overhead reported apache flash web servers respectively 
expose full impact mechanisms assumptions respect request distribution policy 
assumed requests rst arriving persistent connection node connection handling node 
practical policies better results indicate upper bound impact choice request distribution mechanism actual cluster performance 
results con rm small response sizes back forwarding mechanism yields higher performance multiple hando mechanism superior large responses 
crossover point depends relative cost hando versus data forwarding lies kb kb flash 
results nearly independent average number requests received persistent connection 
average response size today web tra kb results indicate back forwarding mechanism competitive tcp multiple hando mechanism web workloads 
simulation study various request distribution policies range cluster sizes di erent request distribution mechanisms policies extended con gurable web server cluster simulator pai deal requests 
section gives overview simulator 
detailed description simulator pai 
costs basic request processing steps simulations derived performing measurements mhz pentium ii machine running freebsd widely apache web server aggressively optimized research web server called flash 
connection establishment teardown costs set cpu time request overheads transmit processing incurs bytes simulate apache flash respectively 
numbers kbyte document served main memory cache rate approximately requests sec apache flash respectively connections 
rate higher connections depends average connection 
back machines prototype implementation main memory size mb 
main memory shared os kernel server applications le cache 
account set backend cache size simulations mb 
simulator model tcp behavior data transmission 
example data transmission assumed continuous limited tcp slow start 
ect throughput results networks assumed nitely fast throughput limited disk cpu overheads 
workload simulator derived logs actual web servers 
logs contain name size requested targets client host timestamp access 
unfortunately web servers record requests arrived connection 
construct simulator working re quests heuristic 
setof requests sent client period default time web servers close idle connections successive requests considered arrived single connection 
model pipelining requests rst connection considered batch pipelined requests 
clients pipeline requests batch data server requests batch sent 
best knowledge synthetic workload generators surge specweb generate workloads representative connections 
workload generated combining logs multiple departmental web servers rice university 
trace spans month period 
logs generating workload pai 
data set trace consists targets covering gb space 
results show trace needs mb memory cover requests respectively 
simulator calculates throughput cache hit rate average cpu disk idle times back nodes statistics 
requests trace served second entire cluster calculated number requests trace divided simulated time took nish serving requests trace 
request arrival rate matched aggregate throughput server 
simulation results section simulation results comparing mechanisms policy combinations 

tcp single hando lard workload simple lard 
tcp single hando lard workload simple lard 
tcp multiple hando extended lard workload 
back forwarding extended lard workload 
ideal hando extended lard workload throughput reqs sec simple lard simple lard wrr wrr nodes cluster apache throughput mechanisms described section 
ideal hando idealized mechanism incurs overhead reassigning persistent connection back node 
useful benchmark performance results mechanism provide ceiling results obtained practical request distribution mechanism 
figures show throughput results apache flash web servers respectively running back nodes 
comparison results widely weighted round robin wrr policy included workloads 
driving simple lard workload simple lard results show throughput su ers considerably apache flash particularly small medium cluster sizes 
loss locality sets reduced server overhead persistent connections 
key result extended lard policy multiple hando mechanism back forwarding mechanism ideal mechanism ord throughput gains compared simple lard 
throughput achieved mechanism con rming mechanisms competitive web workloads 
performance lard simple lard catches extended lard schemes larger clusters 
reason follows 
su cient number back nodes aggregate cache size cluster larger working set allowing back cache targets throughput reqs sec simple lard simple lard wrr wrr nodes cluster flash throughput assigned lard policy additional targets requested connections 
eventually targets cached backend node yield high cache hit rates rst request connection subsequent requests 
result performance approaches exceed extended lard strategies large cluster sizes 
wrr obtain throughput advantages persistent connections workload remains disk bound cluster sizes unable capitalize reduced cpu overhead persistent connections 
previously reported simple lard outperforms wrr large margin cluster size increases aggregate node caches 
server node performance identical back servers disk bound policies 
results obtained flash web server predict trends web server software performance di er mainly performance loss simple lard signi cant apache 
underscores importance cient mechanism handling persistent connections cluster servers content request distribution 
throughput gains orded hypothetical ideal hando mechanism relaying front see section long bottleneck 
shown figures achieves better throughput back forwarding mechanism extended lard policy 
prototype cluster design section describes design prototype cluster 
complexity tcp mul client tcp ip conn req ack reply conn req dispatcher forward handoff tcp ip handoff req ack server handoff tcp ip client host front back tcp connection hando user level kernel tiple hando mechanism fact simulation results indicate substantial performance advantages multiple hando back request forwarding decided implement back forwarding mechanism prototype 
section gives overview various components cluster 
section describes tcp single hando protocol 
section describes tagging technique front instructs connection handling node forward request back node 
section describe back nodes fetch requests remotely nodes manner keeps server applications unchanged 
overview cluster consists front node connected back nodes high speed lan 
clients aware existence back nodes cluster ectively provides illusion single web server machine clients 
shows user level processes protocol stacks client front backends 
client application web browser unchanged runs unmodi ed standard operating system 
server process back machines unchanged shelf web server application apache zeus 
front back protocol stacks employ additional components added loadable kernel module 
front back nodes tcp single hando protocol standard tcp ip provide control session front back machine 
lard extended lard policies implemented dispatcher module front 
addition front contains forwarding module described section 
front back nodes user level startup process shown initial ize dispatcher setup control sessions front back hando protocols 
initializing cluster processes remain kernel resident provide process context dispatcher hando protocols 
disk queue lengths back nodes conveyed front control sessions mentioned 
tcp connection hando illustrates typical hando client process netscape uses tcp ip protocol connect front dispatcher module front accepts connection hands back tcp hando protocol back takes connection hando protocol server back accepts created connection server back sends replies directly client 
hando remains transparent client packets connection handling node appear coming front 
tcp packets client forwarded forwarding module connection handling back 
copy packets containing requests client sent dispatcher enable assign requests backend nodes 
request pipelining fully supported hando protocol allows clients send multiple requests waiting responses previous requests 
tcp multiple hando mechanism discussed section implemented extending design manner 
soon back server connection handling node indicates sent requisite data client hando protocol back hand back connection front hand back 
alternatively connection handed directly back informing front forward packets client appropriately 
main challenges design prevent tcp pipeline draining process hando tagging requests mentioned previous subsection forwarding module sends copy request packets dispatcher connection handed assignment subsequent requests connection back nodes connection handling node accomplished tagging level kernel req dispatcher req forward front tagged req handoff handoff tagged req req tagged req server tcp processing back tagging requests socket buffer req drop request content 
dispatcher sends requests reliably connection handling back control session hando protocol modules 
hando protocol backend receives requests places directly web server socket bu er 
tags enable web server fetch target back forwarding see section 
remains unaware presence hando protocol 
hando packets client sent forwarding module connection handling node undergo tcp processing 
hando data packets client acknowledged connection handling node 
contents request packets received discarded connection handling node see 
tagged requests received front control connection delivered server process 
fetching remote requests get back foo dispatcher get foo foo webserver document root back nfs mount webserver foo document root front back back transparent remote request fetching web server applications typically serve documents user con gurable directory refer document root 
implement remote fetching transparently web server application back node nfs mounts document root back nodes subdirectory document root directory 
tagging accomplished front dispatcher chang ing url client requests prepending name directory corresponding remote back node 
depicts situation dispatcher tags get request prepending back url order back fetch nfs 
issue concerning fetching remote les nfs client caching result caching targets multiple back nodes interfere lard cache replication 
avoid problem small modi cation freebsd disable client side caching nfs les 
prototype cluster performance section performance results obtained prototype cluster 
experimental environment experimental testbed testbed consists number client machines connected cluster server 
con 
tra clients ows front forwarded back ends 
data packets transmitted back ends clients bypass front 
front server cluster mhz intel pentium ii pc mb memory 
cluster back consists pcs type con guration front 
machines run freebsd 
loadable kernel module added os front backend nodes implements tcp single hando protocol case front forwarding module 
clients mhz intel pentium pro pcs mb memory 
clients back nodes cluster connected switched fast ethernet mbps 
front back nodes equipped network interfaces communication clients internal communication 
clients front back ends connected single port switch 
network interfaces intel pro running full duplex mode 
apache server backend nodes 
client driven program simulates multiple clients 
simulated client requests fast server cluster handle 
cluster performance results throughput reqs sec simple simple wrr wrr nodes cluster throughput apache segment rice university trace alluded section drive prototype cluster 
single back node running apache deliver req trace 
apache web server relies le caching services underlying operating system 
freebsd uses uni ed bu er cache cached les competing user processes physical memory pages 
page replacement freebsd daemon implements variant algorithm 
cache size variable depends main memory pressure user applications 
mb back ends memory demands kernel apache server processes leave mb free memory 
practice observed le cache sizes mb 
mechanism wrr policy similar simple tcp hando data back servers sent directly clients 
assignment connections back nodes purely load 
observations results 
measurements largely con rm simulation results section 
contrary simulation results wrr realizes modest performance improvements disk bound workload 
believe reduces memory demands apache server application leaves room le system cache causing better hit rates 
ect modeled simulator 
extended lard policy back forwarding mechanism ords times throughput wrr persistent connections better throughput persistent connections 
mechanism distributing requests back nodes lard policies perform worse presence persistent connections 
running extended lard back forwarding mechanism back nodes results cpu utilization frontend 
indicates front support back ends equal cpu speed 
scalability larger cluster sizes achieved employing smp front machine 
related padmanabhan mogul shown connections increase server resource requirements number network packets request ective latency perceived client 
proposed persistent connections pipelining requests adopted standard 
shows techniques dramatically improve ine ciencies 
provides cient support cluster web servers content request distribution 
heidemann describes performance problems arising interactions tcp certain situations 
proposes xes improve performance 
proposed solutions complimentary applied cluster environment 
fact proposed xes incorporated apache 
current research addresses scalability problems posed 
includes cooperative caching proxies inside network document distribution innovative techniques :10.1.1.34.4398:10.1.1.21.1584
proposal addresses complementary issue providing support cost ective scalable network servers 
network servers clusters workstations starting widely 
products available announced front nodes cluster servers 
best knowledge request distribution strategies cluster front ends variations weighted round robin take account request target content 
exception dispatch product resonate supports content request distribution 
product appear dynamic distribution policies content attempt achieve cache aggregation content request distribution 
hunt proposed tcp option designed enable content load distribution cluster server 
design roughly comparable functionality tcp single hando protocol implemented 
fox report cluster server technology inktomi search engine 
focuses reliability scalability aspects system complementary 
request distribution policy systems weighted round robin 
loosely coupled distributed servers widely deployed internet 
servers various techniques load balancing including dns round robin client re direction smart clients source forwarding hardware translation network addresses 
schemes problems related quality load balance achieved increased request latency 
detailed discussion issues goldszmidt hunt 
schemes support content request distribution 
persistent connections pose problems cluster web servers content request distribution requests appear single connection may served di erent backend nodes 
describe cient mechanisms distributing requests arriving persistent connections tcp multiple hando back request forwarding 
simulation study shows mechanisms ciently handle web workloads persistent connections 
extend locality aware request distribution lard strategy back request forwarding show yields performance results obtained simulated idealized mechanism 
proposed policies mechanisms fully transparent clients 
implemented extended lard policy back request forwarding mechanism prototype cluster 
performance results indicate extended lard strategy affords improvement throughput persistent connections 
results indicate single front cpu support back nodes equal speed 
focused studying servers serve static content 
research needed supporting request distribution mechanisms policies dynamic content 
acknowledgments erich anonymous reviewers valuable comments suggestions 
supported part nsf ccr ccr mip texas ibm partnership award 
scalable www server multicomputers 
proccedings th international parallel processing symposium apr 
apache 
www apache org 
arlitt williamson 
web server workload characterization search invariants 
proceedings acm rics conference philadelphia pa apr 
barford crovella 
generating representative web workloads network server performance evaluation 
proceedings acm sigmetrics conference madison wi july 
berners lee fielding frystyk 
rfc hypertext transfer protocol may 

dns support load balancing 
rfc apr 
chankhunthod danzig neerdaels schwartz worrell 
hierarchical internet object cache 
proceedings usenix technical conference jan 
cisco systems 
www cisco com 

chung huang 
wang 
ip techniques hosting service cluster machines 
computer networks isdn systems 
danzig hall schwartz 
case caching le objects inside internetworks 
proceedings acm sigcomm conference sept 
fielding gettys mogul nielsen berners lee 
rfc hypertext transfer protocol jan 
fox gribble chawathe brewer gauthier 
cluster scalable network services 
proceedings sixteenth acm symposium operating system principles san malo france oct 
heidemann 
performance interactions tcp implementations 
acm computer communication review april 
hunt tracey 
enabling content load distribution scalable services 
technical report ibm watson research center may 
ibm 
ibm interactive network dispatcher 
www ics raleigh ibm com ics htm 
kroeger long mogul 
exploring bounds web latency reduction caching prefetching 
proceedings usenix symposium internet technologies systems usits monterey ca dec 
malan jahanian subramanian 
push distribution substrate internet applications 
proceedings usenix symposium internet technologies systems usits monterey ca dec 
mckusick bostic karels quarterman 
design implementation bsd operating system 
addison wesley publishing 
mogul 
case persistent connection 
proceedings acm sigcomm symposium 
mogul douglis feldmann krishnamurthy 
potential bene ts delta encoding data compression 
proceedings acm sigcomm symposium cannes france sept 
nielsen gettys baird smith prud hommeaux lie lilley 
network performance ects css png 
proceedings acm sig comm symposium cannes france sept 
padmanabhan mogul 
improving latency 
proceedings second international www conference chicago il oct 
pai aron banga druschel zwaenepoel 
locality aware request distribution cluster network servers 
proceedings th acm conference architectural support programming languages operating systems san jose ca oct 
pai druschel zwaenepoel 
flash cient portable web server 
proceedings usenix technical conference monterey ca june 
pai druschel zwaenepoel 
lite uni ed bu ering caching system 
proceedings rd symposium operating systems design implementation new orleans la feb 
resonate resonate dispatch 
www com 
seltzer gwertzman 
case geographical 
proceedings workshop hot operating systems 
specweb 
www org osg web 
stevens 
tcp ip illustrated volume protocols 
addison wesley reading ma 
yoshikawa etal 
smart clients build scalable services 
proceedings usenix technical conference jan 
zeus 
www zeus uk 
