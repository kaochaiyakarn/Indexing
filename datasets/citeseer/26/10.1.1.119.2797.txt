machine learning kluwer academic publishers boston 
manufactured netherlands 
improving generalization active learning david cohn cohn psyche mit edu department brain cognitive sciences massachusetts institute technology cambridge ma les atlas electrical engineering university washington seattle wa richard ladner computer science engineering university washington seattle wa editor alex waibel 
active learning differs learning examples learning algorithm assumes control part input domain receives information 
situations active learning provably powerful learning examples giving better generalization fixed number training examples 
article consider problem learning binary concept absence noise 
describe formalism active concept learning called selective sampling show may approximately implemented neural network 
selective sampling learner receives distribution information environment queries oracle parts domain considers useful 
test implementation called domains observe significant improvement generalization 
keywords queries active learning generalization version space neural networks 

random sampling vs active learning neural network generalization problems studied respect random sampling training examples chosen random network simply passive learner 
approach generally referred learning examples 
baum haussler examine problem analytically neural networks conn tesauro provide empirical study neural network generalization learning examples 
number empirical efforts le cun 
aimed improving neural network generalization learning examples 
learning examples universally applicable paradigm 
natural learning systems simply passive form active learning examine problem domain 
active learning mean form learning learning program control inputs preliminary version article appears cohn 

cohn atlas ladner trains 
natural systems humans phenomenon exhibited high levels active examination objects low subconscious levels infant reactions speech 
broad definition active learning restrict attention simple intuitive form concept learning membership queries 
membership query learner queries point input domain oracle returns classification point 
formal learning theory directed study queries see angluin valiant queries examined respect role improving generalization behavior 
formal problems active learning provably powerful passive learning randomly examples 
simple example locating boundary unit line interval 
order achieve expected position error need draw iln random training examples 
allowed membership queries sequentially binary search possible assuming uniform distribution position error may reached queries 
imagine number algorithms employing membership queries active learning 
studying problem learning binary concepts environment 
problems learner may proceed examining information determining region uncertainty area domain believes misclassification possible 
learner asks examples exclusively region 
article discusses formalization simple approach call selective sampling 
section describe concept learning problem detail give formal definition selective sampling describing conditions necessary approach useful 
section describe sg network neural network implementation technique inspired version space search mitchell 
section contains results testing implementation different problem domains section discusses limitations selective sampling approach 
sections contain related field concluding discussion 

concept learning selective sampling arbitrary domain define concept subset points domain 
example dimensional space set points lying inside fixed rectangle plane 
classify point membership concept write je 
popular artificial neural networks concept classifiers input appropriately trained network activates designated output node threshold instance concept formally concept class set concepts usually described description language 
example class may set dimensional axis parallel rectangles see 
case neural networks concept class usually set concepts network may trained classify 
improving generalization active learning 
concept class defined set axis parallel rectangles dimensions 
positive negative examples depicted consistent concepts class 

generalization target concept training example pair consisting point usually drawn distribution point classification 
say positive example 
xj negative example 
concept consistent example concept produces classification point target 
error respect distribution probability disagree random example drawn write drawn randomly generalization problem posed follows concept class unknown target arbitrary error rate confidence examples draw classify arbitrary distribution order find concept consistent examples confidence 
problem formalized valiant studied neural networks baum haussler haussler 

region uncertainty consider concept class set sm examples classification regions domain may implicitly determined concepts consistent examples agree parts 
interested areas determined available information define region uncertainty consistent cohn atlas ladner 
region uncertainty sm set points domain concepts consistent training examples sm disagree classification sm shaded 
arbitrary distribution define size region pr sm 
ideally incremental learning procedure classify train examples monotonically non increasing 
point falls outside sm leave unchanged point inside restrict region 
probability new random point reduce uncertainty 
sm serves envelope consistent concepts disagreement concepts lie sm 
sm bounds potential error consistent hypothesis choose 
error current hypothesis basis changing current hypothesis contradicting point bound probability additional point reducing error 

selective sampling active learning consider learning sequential process drawing examples determine information successive example gives 
draw random domain probability individual sample reduce error defined decreases zero draw examples 
means efficiency learning process approaches zero eventually example draw provide information concept trying learn 
consider happens recalculate sm region uncertainty new example draw examples sm 
example reduce sm reduce uncertainty decrease efficiency draw examples 
call process selective sampling 
distribution known uniform perform selective sampling directly randomly querying points lie strictly inside sm 
frequently sample distribution target concept unknown 
case choose points domain risk assuming improving generalization active learning distribution differs greatly actual underlying problems distribution information having pay full cost drawing classifying example 
assuming drawing classified example atomic operation see valiant blumer may divide operation steps drawing unclassified example distribution second querying classification point 
cost drawing point distribution small compared cost finding point proper classification filter points drawn distribution drawing random selecting classifying training fall sm 
approach suited problems speech recognition unlabeled speech data plentiful classifying labeling speech segments laborious process 
calculating sm computationally expensive may want perform selective sampling batches 
pass draw initial batch training examples train determine initial sm 
define new distribution sample zero outside maintains relative distribution inside 
second pass drawing second batch training examples sm adding determining new smaller sm 
smaller batch size passes efficiently algorithm draw training examples see 
sm recalculated pass advantage weighed added computational cost incurred calculation 

approximations selective sampling simple concept classes set axis parallel rectangles dimensions may difficult computationally expensive represent region uncertainty exactly 
class rectangles negative examples lie corners region add complexity causing outer corners sm 
realistic complicated classes representing sm exactly easily difficult impossible task 
approximation sm may sufficient allow selective sampling practical implementations selective sampling possible number approximations process including maintaining close superset subset sm 
assume able maintain superset sm sm 
point sm superset selectively sample inside sm assured exclude part domain interest 
penalty pay efficiency may train points interest 
efficiency approach compared pure selective sampling measured ratio pr sm pr sm 
able maintain subset sm sm sampling training algorithm take additional precautions 
iteration part sm excluded sampling 
need ensure successive iterations choose subsets cover entire region uncertainty example technique discussed section 
need keep number examples iteration small prevent oversampling part domain 
cohn atlas ladner 
batch size selective sampling approaches process yields diminishing improvements added computational costs 
plots error vs training set size selective sampling different batch sizes learning axis parallel rectangle dimensions 
remainder article denote arbitrary algorithm approximation true region uncertainty sm 

neural networks selective sampling selective sampling approach holds promise improved generalization trainable classifiers 
remainder article concerned demonstrating approximation selective sampling may implemented feedforward neural network trained error backpropagation 
backpropagation algorithm rumelhart supervised neural network learning technique network training set input output pairs learns output input train neural network standard backpropagation take training example copy input nodes network calculate individual neuron outputs layer layer hidden layer proceeding output layer 
output neuron computed improving generalization active learning 
simple feedforward neural network 
node computes weighted sum inputs passes sum sigmoidal squashing function passes result output 
wj connection weight neuron neuron exp sigmoidal squashing function produces neuron outputs range 
define error output node dn 
error value propagated back network see rumelhart 
details neuron error term 
connection weights wj adjusted adding ij constant learning rate 
adjustment incrementally decreases error network example 
presenting training example turn sufficiently large network generally converge set weights network acceptably small error training example 
concept learning model target values training examples depending input instance concept learned 
patterns trained error threshold 
point need draw attention distinction neural network architecture configuration architecture neural network refers parameters network change training case network topology transfer functions 
configuration network refers network parameters change training case weights connections neurons 
network training algorithms involve changing network topology training ash consider fixed topologies train weight adjustment 
theory methods described modification equally applicable trainable classifiers 
neural network architecture single output node concept class specified set configurations network take 
configurations implements mapping input output configurations cohn atlas ladner may implement mapping 
set threshold output may say particular configuration represents concept see 
having trained training set sm say network configuration implements concept consistent training set sm 
denote concept network implements 
consider naive algorithm selective sampling neural networks examine shortcomings 
describe sg net version space paradigm mitchell overcomes difficulties 

naive neural network querying algorithm observation neural network implementation concept learner may produce real valued thresholded suggests naive algorithm defining region uncertainty 
network trained tolerances divide points domain classifications greater uncertain 
may say category corresponds region network uncertain may define sm approximation region uncertainty 
problem applying approach measures uncertainty particular configuration uncertainty configurations possible architecture 
fact part sm full region composed differences possible consistent network configurations 

thresholded output trained neural network serves classifier representing concept hopefully similar unknown target concept 
improving generalization active learning naive approach representing region uncertainty network transition area represent part domain network uncertain 
limitation exacerbated inductive bias learning algorithms including backpropagation 
backpropagation algorithm attempts classify set points tends draw sharp distinctions overly confident regions unknown 
result sm chosen method general small subset true region uncertainty 
pathological example behavior exhibited figures 
initial random sampling failed yield positive examples triangle right 
training backpropagation examples yields region uncertainty contours concentrates left half domain completely exclusion right 
final result iterations querying learning shown 
strategy related ones prone failure form regions detail target concept discovered initial stage 

version space mitchell describes learning procedure partial ordering generality concepts learned 
concept general 
concepts incomparable 
concept class set examples sm version space subset csm consistent sm 
bound concepts version space maintain subsets csm 
set specific consistent cohn atlas ladner 
pathological example naive network querying 
initial random sample failed detect second disjoint region target concept successive iterations naive querying algorithm ignored region concentrated region seen examples 
dotted line denotes true boundary unknown target concept 
concepts csm tc csm 
similarly csm csm set general concepts 
consistent concept case may active learning version space examining instances fall difference region sag sag symmetric difference operator 
instance region proves positive generalize accommodate new information proves negative modified exclude 
case version space space plausible hypotheses reduced query 
implementing active version space search entire neural network configuration represents single concept complete version space directly represented single neural network 
fact haussler pointed size sets grow exponentially size training set 
representing sets completely require keeping track manipulating exponential number network configurations 
modify version space search problem tractable impose distribution strict index ordering concepts class 
define concept general concept random point drawn pr pr 
definition generality concepts class comparable sense speak ordering represent single general concept single specific concept may concepts generality impediment 
need know concepts general case greater generality concept chosen 
improving generalization active learning maintaining concepts window version space sm subset sag 
point sm guaranteed reduce size version space 
positive invalidate leave general equally specific includes new point 
similarly new point classified negative invalidate proceeding fashion approximate step step traversal sets fixed representation size 

sg net neural network version space search algorithm interested selecting examples improve generalization behavior neural network architecture define concept class question set concepts learnable learning algorithm 
manage obtain network configurations represent concepts described simple matter implement modified version space search 
subsections describe may learn specific general concept associated network describe networks may selectively sample sm defined regions disagree 

implementing specific general network describe may learn specific concept consistent data 
case learning general concept analogous 
specific network set examples sm distribution classifies positive example points fact positive classifies negative possible rest domain 
requirement amounts choosing consistent sm minimizes pr 
network may arrived employing inductive bias 
inductive bias learning algorithm solutions 
learning algorithms inherently form inductive bias preference simple solutions complex ones tendency choose solutions absolute values parameters remain small explicitly add new inductive bias backpropagation algorithm penalizing network part domain classifies positive add bias prefers specific concepts general ones 
weight penalty carefully adjusted large outweigh training examples network converge training data 
penalty large outweigh inductive bias learning algorithm force algorithm find specific configuration consistent sm 
negative bias may implemented drawing unclassified points creating case known arbitrarily labeling negative examples 
add background examples training set 
cohn atlas ladner training large number background points addition regular training data forces network specific configuration 
creates background bias domain weighted input distribution networks error background patterns ones specific order allow network converge actual training examples spite background examples balance influence background examples training data 
network learns training example error term dj equation approach zero error term arbitrary background example may remain constant 
push random background example exerts network weights decreased match normal training examples aw background examples dominate network converge solution 
achieve balance different learning rates training examples background examples 
dynamically decrease background learning rate function network error training set 
time training example calculate new background learning rate jd ri error network constant 
train single background example value repeat 
formally algorithm follows improving generalization active learning 
initialize network random configuration 
actual training examples sm threshold 

select actual training example 

calculate output error network input network adjusting weights 
calculate new background learning rate 

draw point create background example 

calculate output error network adjusting weights modified equation 
go step 
optimally set weight update background patterns infinitesimally smaller weight update actual training patterns allowing network anneal specific configuration 
requires prohibitive amount training time 
empirically setting provides adequate bias allows convergence reasonable number iterations 
similar procedure produce general network adding positive inductive bias classifying background points drawn positive 

implementing active learning sg net represent concepts simple matter test point membership sm determining 
selective sampling may implemented follows point drawn distribution sag networks agree classification point discarded 
point true classification queried added training set 
practice merge inputs networks illustrated train 
xx important note technique somewhat robust failure modes degrade efficiency single sampling iteration causing failure learning process 
networks fail converge training data points failed converge contained sag region eligible additional sampling iteration 
cases additional examples suffice push network local minimum 
network converge training set settles solutions near specific general networks consistent data examples gleaned iteration useful 
chosen virtue lying areas networks disagreed points settle discrepancies 
may lead oversampling region cause technique fail 
cohn atlas ladner 
construction sg network equivalent original 
effects failure modes minimized keeping number examples taken iteration small 
increases efficiency learning process terms number examples classified observed tradeoff computational resources required 
time new data added training set network may completely incorporate new information 
practice large training set sizes efficient imply retrain entire network scratch new examples added 
pratt offers hope retraining may efficient information transfer strategies iterations 

experimental results experiments selective sampling run types problems solving simple boundary recognition problem dimensions learning input real valued threshold function recognizing secure region small power system 
triangle learner input network hidden layers units single output trained uniform distribution examples positive inside pair triangles negative 
task chosen intuitive visual appeal requires learning non connected concept task demands training algorithm sample selection scheme simple convex shape 
baseline case consisted networks trained randomly drawn examples training set sizes points increments examples 
test cases run architecture data selected runs sg network selective sampling iterations examples figures 
additionally runs naive querying algorithm described section run comparison 
networks trained selectively sampled data showed marked consistent improvement randomly sampled networks ones trained naive querying 
naive querying algorithm displayed erratic performance algorithms possibly due pathological nature failure modes 
improving generalization active learning 
triangle learner problem learned random examples learned examples drawn passes selective sampling 
dotted line denotes true boundary unknown target concept 

generalization error vs training set size random sampling naive querying selective sampling 
irregularity naive querying algorithm error may due intermittent failure find triangles initial random sample 
cohn atlas ladner 
real valued threshold function bit real valued threshold problem quantitative measure network performance simple higher dimensional problem 
runs selective sampling iterations examples iteration trained problem compared identical networks trained randomly sampled data 
results indicate steeper learning curve selective sampling 
plotting generalization error number training examples networks trained randomly sampled data exhibited roughly polynomial curve expected blumer 

simple linear regression error data fit coefficient determination 
networks trained selectively sampled data comparison fit indicating fit polynomial 
visually selectively sampled networks exhibited steeper drop generalization error expected active learning method 
linear regression natural logarithm errors selectively sampled networks exhibited decrease 
generalization error vs training set size random sampling selective sampling 
standard deviation error averages random case selectively sampled case 
improving generalization active learning generalization error matching error drops indicating fit exponential curve 
comparison randomly sampled networks fit achieved 
domain sg network appears provide exponential improvement generalization increasing training set size expect active learning algorithm 
suggests sg network represents approximation region uncertainty domain implements approximation selective sampling 
additional experiments run iterations indicate error decreases sampling process broken smaller frequent iterations 
observation consistent increased efficiency sampling new information incorporated earlier sampling process 

power system security analysis various load parameters electrical power system certain range system secure 
risks thermal overload 
previous research determined problem amenable neural network learning random sampling problem domain inefficient terms examples needed 
range parameters system run known distribution information readily available 
set parameters point domain analytically determine secure done solving time consuming system equations 
classification point expensive determination input distribution problem amenable solution selective sampling 
baseline case random sampling dimensions studied hwang 
comparison 
experiments ran sets networks initial random training sets data points added single iteration selective sampling 
networks trained small second iteration points total large second iteration total points 
results compared baseline cases points randomly sampled data 
estimated network errors testing randomly drawn test points 
improvement single extra iteration selective sampling yielded small set total error large set resulted improvement total 
difference significant greater confidence 

limitations selective sampling approach number limitations selective sampling approach practical mentioned previous section discussing implementations technique theoretical 
cohn atlas ladner 
practical limitations discussed earlier article exact implementation selective sampling practical relatively simple concept classes 
class complex difficult compute maintain accurate approximation sm 
case maintaining superset increased concept complexity lead cases sm effectively contains entire domain reducing efficiency selective sampling random sampling 
example section illustrates nicely bounding box suffices approximation rectangles dimensions box bounding dimensional conceivably require approximation contain input space 
case maintaining subset increased concept complexity leads extreme sm contains small subset sm 
cases oversampling regions critical problem due inductive bias training algorithm training set size may omit large regions domain 

theoretical limitations selective sampling draws power ability differentiate region uncertainty bulk domain 
cases representational complexity concept large neural network hidden units sm extend domain concept learned 
words maximum error may small due number places error may arise total uncertainty may remain large 
depending desired final error rate selective sampling may come effect longer needed 
similarly input dimension large bulk domain may uncertain simple concepts 
method avoiding problem bayesian probabilities measure degree utility querying various parts region uncertainty 
approach studied david mackay discussed briefly section 

related described article extension results published cohn 

prior related results active learning 
large body studying effects queries strict learning theory viewpoint primarily respect learning formal concepts boolean expressions finite state automata 
angluin showed minimal finite state automata polynomially learnable valiant sense examples learned polynomial number queries oracle provides counterexamples 
valiant considers various classes learnable variety forms directed learning 
eisenberg rivest puts bounds degree membership query examples help generalization underlying distribution unknown 
additionally certain smoothness constraints distribution authors describe queries may learn class initial segments unit line 
improving generalization active learning seung 
independently proposed approach selecting queries similar basing lack consensus committee learners 
freund 
showed size committee increases learners selective sampling accuracy utility estimate increases sharply 
actual implementations querying systems learning explored 
done hwang 
implements querying neural networks means inverting activation trained network determine uncertain 
approach shows promise concept learning cases relatively compact connected concepts produced impressive results power system static security problem 
susceptible pathology discussed section 
algorithm due baum lang uses queries reduce computational costs training single hidden layer neural network 
algorithm queries allow network efficiently determine connection weights input layer hidden layer 
david mackay pursues related approach data selection bayesian analysis 
assigning prior probabilities concept network configuration determine utility querying various parts 
fact point lies means consistent configurations disagree classification point 
point edge may configurations disagree querying point decrease size infinitesimally small amount 
bayesian analysis may effect determine number configurations disagree point determine parts uncertain 

article theory selective sampling described neural network implementation theory examined performance resulting system domains 
selective sampling rudimentary form active learning benefit formal grounding learning theory 
neural network implementation tested selective sampling demonstrates significant improvement passive random sampling techniques number simple problems 
paradigm suited concept learning problems relevant input distribution known cost obtaining unlabeled example input distribution small compared cost labeling example 
limitations selective sampling apparent complex problem domains approach opens door study sophisticated techniques querying learning natural intuitive means active learning 
acknowledgments supported national science foundation number ccr washington technology center ibm 
majority cohn atlas ladner done david cohn department computer science engineering university washington 
remainder done david cohn ibm watson research center yorktown heights ny 
jai choi running simulation data power system problem 
anonymous referees suggestions earlier version article 
notes 
assume inputs normalized range 

terminology judd 

inductive biases inherent backpropagation studied appears tendency fit data smallest number units possible 
atlas cohn el marks ii 

artificial neural networks power system static security assessment 
proceedings international symposium circuits systems ieee 
angluin 

learning regular sets queries counter examples 
technical report yaleu dcs tr 
dept computer science yale university new haven ct ash 

dynamic node creation backpropagation networks 
ics report 
institute cognitive science university california san diego ca 
haussler 

size net gives valid generalization 
touretzky ed advances neural information processing systems vol 

san francisco ca morgan kaufmann 
baum lang 

constructing hidden units examples queries 
lippmann 
eds advances neural information processing systems vol 

san francisco ca morgan kaufmann 
blum rivest 

training node neural network np complete 
touretzky ed advances neural information processing systems volume 
san francisco ca morgan kaufmann 
blumer ehrenfeucht haussler warmuth 

learnability vapnik chervonenkis dimension 
jacm 
cohn atlas ladner 

training connectionist networks queries selective sampling 
touretzky ed advances neural information processing systems vol 

san francisco ca morgan kaufmann 
cohn tesauro 

tight vapnik chervonenkis bounds 
neural computation 
eisenberg rivest 

sample complexity pac learning random chosen examples 
case eds acm rd annual workshop computational learning theory 
san francisco ca morgan kaufmann 


acoustic determinants infant preference speech 
infant behavior development 
freund seung shamir tishby 

information prediction query committee 
hanson eds 
advances neural information processing systems vol 

san francisco ca morgan kaufmann 
haussler 

learning conjunctive concepts structural domains 
proceedings aaai pp 

san francisco ca morgan kaufmann 
haussler 
decision theoretic generalizations pac model neural net applications 
information computation 
improving generalization active learning hwang choi oh marks 

query learning boundary search gradient computation trained multilayer perceptrons 
ijcnn 
san diego ca 
judd 

complexity loading shallow neural networks 
journal complexity 
le denker solla 

optimal brain damage 
touretzky ed advances neural information processing systems vol 

san francisco ca morgan kaufmann 
mackay 

information objective functions active data selection 
neural computation 
mitchell 

generalization search 
artificial intelligence 
pratt 

discriminability transfer neural networks 
giles 
eds advances neural information processing systems vol 

san francisco ca morgan kaufmann 
rumelhart hinton williams 

learning internal representations error propagation 
rumelhart mcclelland eds parallel distributed processing cambridge ma mit press 
seung opper sompolinsky 

query committee 
proceedings fifth annual acm workshop computational learning theory pp 

new york acm 
valiant 

theory learnable 
communications acm 
received september accepted february final manuscript december 
