journal machine learning research submitted published energy models sparse overcomplete representations yee teh cs utoronto ca max welling welling cs utoronto ca department computer science university toronto king college road toronto canada simon osindero simon gatsby ucl ac uk gatsby computational neuroscience unit university college london queen square london wc ar united kingdom geoffrey hinton hinton cs utoronto ca department computer science university toronto king college road toronto canada editors te won lee jean fran ois cardoso erkki oja shun ichi amari new way extending independent components analysis ica overcomplete representations 
contrast causal generative extensions ica maintain marginal independence sources define features deterministic linear functions inputs 
assumption results marginal dependencies features conditional independence features inputs 
assigning energies features probability distribution input states defined boltzmann distribution 
free parameters model trained contrastive divergence objective hinton :10.1.1.35.8613
number features equal number input dimensions energy model reduces noiseless ica show experimentally proposed learning algorithm able perform blind source separation speech data 
additional experiments train overcomplete energy models extract features various standard data sets containing speech natural images hand written digits faces 
keywords independent components analysis density estimation overcomplete representations sparse representations 
dominant ways understanding ica bottom filtering view top causal generative view 
information maximization approach bell sejnowski aim maximize mutual information observations non linearly transformed outputs set linear filters 
causal generative view pearlmutter parra mackay cardoso yee teh max welling simon osindero geoffrey hinton :10.1.1.48.120:10.1.1.48.120:10.1.1.56.3619
causal generative models density modelling approach teh linear components analysis energy models filtering approach information maximization different methods non gaussian linear components analysis 
hand aim build density model independent non gaussian sources linearly combined produce observations 
main point show third energy approach understanding ica combines bottom filtering view goal fitting probability density observations 
parameters energy model specify deterministic mapping observation vector feature vector feature vector determines global energy 
probability density defined normalization factor integral numerator possible observation vectors 
energy approach interesting suggests novel tractable way extending ica overcomplete multi layer models 
relationship approaches depicted 
general quite different equivalent square noiseless case number sources features equals number observations observation noise 
complete representations applied successfully wide range problems researchers argued overcomplete representations sources features observations 
apart greater model flexibility reported advantages include improved robustness presence noise simoncelli compact easily interpretable codes mallat zhang superresolution chen 
natural way extend causal generative approach overcomplete case retain assumption sources independent model generate data 
discussing energy models term feature source reasons clear discuss extensions overcomplete case 
models square causal overcomplete energy overcomplete sparse overcomplete energy models marginal distribution source vectors observation independent assumption independent assumption dependent rejecting away conditional distribution source vectors observation independent deterministic dependent explaining away independent deterministic table independence properties types models 
accept consequence observation vector creates posterior distribution multiplicity source vectors 
posterior distribution sources conditionally dependent due effect known explaining away general distribution unfortunate property computationally intractable 
natural way extend information maximization approach overcomplete representations retain simple deterministic feedforward filtering observations mutual information objective function 
manifold possible filter outputs typically consist space square case equivalence causal generative models breaks 
energy approach ica overcomplete continues proper density model retains computationally convenient property features deterministic function observation vector 
abandons marginal independence features call sources 
useful way understanding difference energy density models causal generative density models compare independence properties 
table summarizes similarities differences 
table reminds different views equivalent square case absence observations sources marginally independent 
posterior distribution source vectors conditioned observation vector collapses point absence noise sources trivially independent posterior distribution 
causal generative approach conditional independence sources seen fortuitous consequence sources observations avoiding noise observations retained overcomplete case 
energy view conditional independence features treated basic assumption remains true overcomplete case 
consider energy contributed activity feature energy model negative log probability dimensional non gaussian distribution 
combinations feature activities teh occur lower dimensional observation space maps restricted manifold feature space 
marginal dependence features overcomplete energy model understood considering illuminating infinitely inefficient way generating unbiased samples energy density model 
sample features independently prior distributions negative exponentials individual energy contributions reject cases feature activities correspond valid observation vectors 
process rejecting away creates dependencies activities different features 
applications unmixing sound sources causal generative approach clearly appropriate energy approach strong prior belief sources marginally independent 
applications real aim model probability density data discover interpretable structure data extract representation useful controlling action raw data 
applications priori reason preferring causal generative approach approach characterizes observation vector representing degree satisfies set learned features 

square ica section briefly review standard models ica 
expositions ica comon entropy linearly transformed input vectors contrast function find statistically independent directions input space 
ica algorithms ultimately reduce optimizing sort contrast function overview mention 
focus reviewing general approaches ica causal generative approach pearlmutter parra mackay cardoso information maximization approach bell sejnowski :10.1.1.48.120:10.1.1.48.120:10.1.1.56.3619
subsequent sections compare canonical approaches proposed energy approach particular explore consequences making different models overcomplete 
consider real valued input denoted dimensionality dimensional source feature vector denoted section consider special case number input dimensions equal sources features causal generative approach causal generative approach sources assumed independent distribution factorizes pi si inputs simply linear combinations sources 
assume noise inputs square invertible matrix called mixing matrix 
inverting relationship wx sparse overcomplete energy models inverse mixing matrix called filter matrix row acts linear filter inputs 
aim recover statistically independent source signals linearly mixed observations turns possible statistical properties sources non gaussian 
shall assume probability distribution sources modelled non gaussian prior distributions pi si 
relation sources inputs deterministic may view change coordinates 
deriving expression probability distribution inputs accomplished transforming expression space jacobian transformation pi si pi wt rows learning proceeds averaging log likelihood model data distribution derivatives respect gradient ascent log wi log pi si si wi th entry wi th entry matrix information maximization approach alternative neurally plausible approach ica put forward bell sejnowski 
assumed certain transformation applied inputs yi fi fi monotone squashing function sigmoid wi set linear filters 
argued maximizing mutual information outputs inputs equivalent maximizing entropy due deterministic relation lead independent components 
effect understood decomposition hi yi ym entropy hi yi individual entropies mutual information yi maximizing joint entropy involves maximizing individual entropies yi minimizing mutual information yi making yi independent 
approach best described filtering approach yi just squashed version filter outputs si wt contrast causal generative approach think generated top manner 

data distribution underlying distribution observed data sampled 
practice replace empirical distribution training set 

fact information maximization approach ica proposed followed causal generative approach 
approaches reverse order intuitive exposition 

note mutual information measured respect data distribution equivalence approaches teh square representations information maximization approach turns equivalent causal generative interpret fi cumulative distribution function pi pearlmutter parra mackay cardoso :10.1.1.48.120:10.1.1.48.120:10.1.1.56.3619
seen observing entropy written negative kl divergence change variables follows dy log dx log jacobian transformation ji yi 
basic algebra shown fact exactly equal equation fi satisfies pi cumulative function pi 
maximizing entropy equation equivalent maximizing log likelihood model 
note sources distributed pi mixed transformation maps input variables independent uniformly distributed variables range fi interval case sigmoid 
geometric interpretation helpful section 
square ica input noise previous section shown causal generative approach special case square mixing matrix noise equivalent information maximization approach 
equivalence break consider noise model inputs 
case longer deterministic relationship inputs sources 
straightforward write probabilistic model joint distribution sources inputs pi si 
unfortunately isotropic gaussian noise longer true mixing matrix optimal reconstruction sources simply equation 
typically computes maximum posteriori map value mean depending objective posterior distribution 

overcomplete generalizations ica equivalence approaches breaks sources input dimensions consider overcomplete representations data 
review overcomplete generalizations ica approaches 
causal generative approach arguably natural way extend ica framework overcomplete representations causal generative approach 
corresponding directed graphical model depicted sparse overcomplete energy models 
noiseless inputs finding probable state corresponding particular input translates optimization problem map argmax log pi si problem typically hard solved efficiently certain choices pi 
instance lewicki sejnowski argued choosing priors laplacian problem mapped standard linear program 
soften optimization problem introducing noise model inputs 
instance spherical gaussian noise model noise variance find joint probability density distribution sources inputs nx pi si 
leads maximization problem reconstruct sources inputs log pi si 
map argmax maximum likelihood learning noisy model em procedure involves averaging posterior distribution 
unfortunately inference problem intractable general approximations needed 
literature find range approximate inference techniques applied problem 
olshausen field posterior approximated delta function map value 
iteration learning data vector maximization equation needs performed 
lewicki sejnowski argued approximation significantly improved gaussian distribution map value constructed matching second derivatives locally laplace approximation 
attias girolami variational approach replaces true posterior tractable approximation adapted better approximate posterior 
mcmc sampling methods gibbs sampling may employed solve inference problem approximately olshausen 
notably different variation generative theme bayesian approach taken hyvarinen 
prior distribution possible mixing matrices introduced favors orthogonal basis vectors columns 
argue role jacobian equation precisely encourage orthogonality basis vectors reasonable assumption remove jacobian favor extra prior 
resultant expression easily extended overcomplete representations 
want stress causal generative models lead difficult inference problems 
contrast generating unbiased samples distribution relatively straightforward sample source values independently priors subsequently sample input variables conditional gaussian equation 

fact situation slightly better variational point view 
show improve bound log likelihood jointly maximizing note extra condition mixing matrix needed prevent collapsing 
sources inputs teh features inputs features inputs auxiliary vars directed graphical model corresponding causal generative approach ica 
undirected graphical model ebm 
directed graphical model representation ebm auxiliary variables clamped 
wx space space mapping information maximization approach equation 
information maximization approach section information maximization approach ica discussed simple case number inputs equal number sources noise assumed inputs 
natural question objective generalized overcomplete representations 
possibility advocated 
define parametrized nonlinear mapping inputs outputs maximize mutual information amounts maximizing entropy outputs 
note approach best classified filtering approach inputs mapped subset possible outputs image mapping forms lower dimensional manifold output space see 

showed objective translates maximizing expression entropy dx log det jacobian defined equation data distribution 

energy models interpreting ica filtering model inputs describe different way generalizing ica overcomplete representations 
energy models ebm preserve com sparse overcomplete energy models attractive property features simple deterministic functions inputs stochastic latent variables causal generative model 
consequence overcomplete setting posterior collapses point stands sharp contrast overcomplete causal models define posterior distribution sources 
fact overcomplete ebms feature values allowed values lie image mapping similar information maximization approach different causal generative approach source values allowed 
ui wi mapping feature ui parameters wi 
features assigning energy possible observation vector follows ei ui wi 
probability defined terms energy boltzmann distribution ei ui wi denotes normalization constant partition function dx 
standard ica non gaussian priors pi si implemented having number sources input dimensions ui wi ei ui log pi ui 
furthermore special case standard ica normalization term equation tractable simplifies det rows filters energy model suggests thinking ica filtering model causal generative model 
observations linearly filtered independent sources linearly mixed 
hinton teh interpreted filters linear constraints energies serving costs violating constraints 
energies corresponding heavy tailed distributions sharp peak zero means constraints frequently approximately satisfied strongly penalized grossly violated 
new approach natural include constraints input dimensions 
note marginal independence sources modelling assumption overcomplete causal models longer true features ebms general 
posterior reduces point features inputs trivially independent ui wi 
note additive form energy leads product form probability distribution called product experts poe model hinton :10.1.1.35.8613:10.1.1.35.8613
teh wi feature computed equation 
semantics probabilistic models consistent undirected graphical models depicted 
means inference ebms trivial 
hand sampling distribution difficult involves mcmc general 
precisely opposite causal generative models inference hard sampling easy 
relating ebms causal generative ica discuss proposed overcomplete ebms relate causal generative approach ica 
intuitively ebm interpreted conditional distribution obtained larger square ica model observe number variables 
relationship explains ebms features conditionally independent marginally dependent observations 
previous section argued number input dimensions matches number features ebm strictly equivalent standard ica described section 
assume features input dimensions 
consider ica model added auxiliary input dimensions denote total input space 
add additional filters new variables features denote total filter matrix 
assume new filters chosen invertible new enlarged space fully spanned 
enlarged ica model write probability distribution equation pi rows write probability density conditional distribution dx terms cancelled 
choose auxiliary variables written pi wt pi wt dx course just ebm partition function pi dx note derivation independent precise choice filters long span extra dimensions 
previous subsection seen ebm may interpreted undirected graphical model conditional independence features inputs 
discussion may conclude interpret ebm conditional distribution directed graphical model auxiliary variables clamped see 
clamping extra nodes introduce dependencies features sparse overcomplete energy models phenomenon explaining away 
words features marginally dependent unobserved 
observed input vector observed posterior distribution features collapse point trivially implying conditional independence 
relating ebms information maximization section saw information maximization approach overcomplete representations maximizes entropy equation 
fact quantity det equation normalized general opposed complete case prevents expression negative kl divergence 
define probability density det normalization constant minimizing kl divergence kl equivalent maximizing log likelihood model 
importantly consistent definition ebm choose energy log det tr log energy density model equation simple interpretation terms mapping 
mapping depicted shown coordinates define parametrization manifold 
hard show distribution transformed precisely uniform distribution manifold space normalization constant may interpreted volume manifold 
minimizing kl divergence kl interpreted mapping data manifold higher dimensional embedding space data distributed uniformly possible 
relation information maximization energy approach summarized expression kl log manifold volume 
term describes fit model data second term simply entropy uniform distribution manifold 
relative energy approach maximizing mutual information stronger preference increase volume manifold directly related entropy 
note square case manifold exactly image space volume fixed equation reduces exactly kl divergence kl 
overcomplete case experiments decide approach circumstances 

parameter estimation energy models section proposed energy models probabilistic models overcomplete representations 
discuss fit free parameters models filters efficiently data 
section address issue 
describe usual maximum likelihood method training models 
overcomplete models show maximum likelihood practical solution non trivial teh partition function 
light propose estimation method energy models called contrastive divergence hinton :10.1.1.35.8613
biased method show bias acceptably small compared gain efficiency training overcomplete models ease generalize method new intricate models 
distribution observed data model distribution equation notation apparent section 
approximate possible 
standard measure difference kullback leibler kl divergence kl log dx 
fixed minimizing kl divergence equivalent maximizing log likelihood data model energy models equation derivative kl divergence respect weight wi kl wi wi wi expectation operator distribution learning proceed derivative equation gradient descent kl divergence data distribution model distribution wi kl 
wi update rule understood lowering energy surface locations data term equation time raising energy surface locations data model predicts high probability second term equation 
eventually result energy surface low energy high probability regions data high energy low probability 
second term rhs equation obtained derivative log partition function equation respect wi square ica case log partition function exactly log second term evaluates th entry matrix model overcomplete analytic form partition function exact computation generally intractable 
second term expectation model distribution possibility markov chain monte carlo mcmc techniques approximate average samples see neal 
method inherits advantages drawbacks associated mcmc sampling 
obtained estimate consistent bias decreases zero length chains increased easily adaptable complex models 
main drawback method expensive markov chain run steps approaches equilibrium distribution hard estimate steps required 
variance mcmc estimator usually high 
reduce variance independent samples needed incurring additional computational costs 
estimating derivative accurately mcmc sampling slow unreliable due high variance 
argue unnecessary estimate derivatives averaged equilibrium distribution order train energy model data 
sparse overcomplete energy models average derivatives different distribution resulting truncating markov chain fixed number steps 
idea called contrastive divergence learning proposed hinton improve computational efficiency reduce variance expense introducing bias estimates parameters respect maximum likelihood solution 
ideas involved contrastive divergence learning 
start markov chain data distribution initialize markov chain vague distribution gaussian large variances 
reason usually vague initial distributions mode equilibrium distribution chance visited chain 
help overcome problematic feature markov chains low mixing rate chain enters mode distribution hard escape different mode 
argue starting data distribution preferable training data contains examples various modes model distribution ought 
learning modes model distribution roughly correspond modes data distribution number samples mode approximately matches number data vectors mode 
reduces variance derivative estimates 
possible danger technique certain spurious empty modes accidentally created learning may go unnoticed 
second idea contrastive divergence run markov chain iterations equilibrium 
chains started data distribution iterations consistent tendency move away data distribution provides valuable information adapt parameters model 
intuitively parameters model updated markov chain tend move away data distribution want markov chain data distribution 
combining ideas described defining distribution random variable th iteration markov chain contrastive divergence learning algorithm implemented quantity update filters wi wi wi wi pn 
relative maximum likelihood learning equations replaced equilibrium distribution markov chain initialized data distribution gives pseudo code contrastive divergence learning 
notice order compute average second term equation samples produced markov chains initialized corresponding data vectors term 
uniformly sampling initial states markov chains data vectors reduces variance 
addition filter weights wi additional parameters instance model shape energies ei similar update rules equation fit data 
standard ica correspond learning shape prior densities 

explains notation initial distribution markov chain limit distribution 
teh contrastive divergence learning energy models 
compute gradient total energy respect parameters average data cases dk 

run mcmc samplers steps starting data vector dk keeping sample sk chain 

compute gradient total energy respect parameters average samples sk 

update parameters wi data dk dk wi samples wi sk sk learning rate number samples mini batch 
ideal situation model distribution flexible perfectly model data distribution markov chain properly mixes contrastive divergence learning fixed point maximum likelihood solution hard see maximum likelihood solution markov chain change model distribution implies derivatives equation precisely cancel 
general expect contrastive divergence learning trade variance bias see williams 
apart may happen certain markov chains spurious fixed points exist contrastive divergence learning examples see mackay 
argued contrastive divergence learning sensible way fit models data shown corresponds gradient descent cost function desirable prove convergence 
show slightly weaker statement update corresponds approximate gradient descent step cost function 
define contrastive divergence cost function hinton cd kl kl :10.1.1.35.8613
note consists usual kl divergence data distribution model distribution subtracted kl divergence step distribution model distribution 
properties markov chains show step distribution closer equilibrium model distribution cd non negative cd exactly derivatives contrastive divergence cost function respect filter weights wi find gradient cd wi wi wi kl pn wi 

case finite data replace data distribution empirical distribution mixture 
case smooth model distribution able perfectly fit empirical data distribution argument fails 
fact may expect incur certain bias respect maximum likelihood solution 
sparse overcomplete energy models terms equation identical ones proposed learning algorithm equation 
term represents effect changes wi contrastive divergence effect effect markov chain parameters wi altered 
term hard compute fortunately typically small simulations hinton suggest safely ignored 
results support claim 

experiment blind source separation assess performance contrastive divergence learning algorithm compared hybrid monte carlo implementation contrastive divergence exact sampling algorithm bell sejnowski algorithm standard blind source separation problem 
model number input source dimensions energy model defined ei si log si si exp sigmoid function 
model strictly equivalent noiseless ica model sigmoidal outputs bell sejnowski 
data consisted second stereo cd recordings music sampled khz 
recording sampled factor randomly permuted time index rescaled unit variance 
resulting samples channels linearly mixed standard routine diagonal diagonal whitened presentation various learning algorithms 
compared different ways computing estimating gradient algorithm hmc hybrid monte carlo implementation contrastive divergence 
implementation uses step hybrid monte carlo simulation sample turn consists leap frog steps step sizes adapted simulation acceptance rate 
see neal detail hybrid monte carlo 
algorithm noiseless ica possible sample efficiently true equilibrium distribution causal generative view 
samples estimate second term 
fair number samples equal number data vectors mini batch 
algorithm exact compute partition function equation evaluate second term equation exactly 
precisely bell sejnowski algorithm 
parameter updates performed mini batches data vectors 
learning rate annealed iterations learning momentum factor 
note recovering sound sources input dimensions sensors possible model marginally independent 

data prepared barak pearlmutter 
webpage www bcl cs may bap demos html 

data available athttp sound media mit edu ica bench 

consisted iterations 
amari distance teh learning curves hmc exact iterations evolution amari distance various algorithms averaged runs 
note hmc converged just fast exact sampling algorithm exact algorithm exact slightly faster 
sudden changes amari distance due annealing schedule 
amari distance comparison final amari distances hmc algorithms exact final amari distances various algorithms averaged runs 
boxes lines lower quartile median upper quartile values 
whiskers show extent rest data 
outliers denoted 
plot shows deterministic method exact performs slightly better sampling methods hmc probably due variance induced sampling 
importantly shows learning brief sampling hmc performs equally learning samples equilibrium distribution 
speed convergence 
initial weights sampled gaussian standard deviation 
sparse overcomplete energy models learning monitored amari distance true unmixing matrix 
figures show results various algorithms sound separation task 
main experiment need sample equilibrium distribution order learn filters validates ideas cd learning 

experiments feature extraction examples features delivered algorithm standard datasets 
firstly demonstrate performance typical ica tasks determining overcomplete set features speech natural images 
show algorithm applied cedar cdrom dataset handwritten digits lastly feature vectors learned algorithm applied feret database human faces 
experiments described section energy function form ei ui wi log corresponds modelling data product dimensional student distributions degree hinton teh 
energy function chosen simplicity versatility describing super gaussian distributions 
algorithmic formulation allows arbitrary energy functions results may improved systematic tailoring energy function particular datasets 
speech test model extract meaningful filters speech data recordings male speakers timit database uttering sentence don ask carry rag sentences sampled khz ms segments segment corresponding samples extracted random locations 
presentation learning algorithm data centred sphered 
features trained contrastive divergence step hybrid monte carlo sampling consisting leap frog steps 
mini batches size learning rate annealed iterations 
filters initialized small random values momentum speed convergence 
show features whitened domain power spectra 
recall times filters extracted dimensions input space energy model longer equivalent causal ica model 
shows distribution power time frequency 
interesting structure khz filters localized finely tuned frequency average 
phenomenon reported abdallah plumbley 
teh 
filters overcomplete ebm 
filters row ones largest power indicating represent important features 
filters second row randomly drawn remaining filters 
corresponding power spectra 
frequency khz time ms distribution power time frequency 
envelope filter absolute value hilbert transform computed squared 
squared envelope power spectrum thresholded mapping values greater half peak value rest zero 
gaps smaller samples time samples frequency filled 
outer product templates computed weighted total power filter added diagram 
natural image patches tested algorithm standard ica task determining independent components natural images 
data set data set van hateren van der schaaf 
amari distance amari measures distance matrices permutations scalings ab maxk ab ik ab maxk ab 
data set available phys rug nl pub samples 
sparse overcomplete energy models learned filters natural images 

logarithm pixel intensities taken image patches centred whitened 
patches patch size 
trained network features contrastive divergence step hybrid monte carlo sampling consisting leap frog steps 
step size adaptive acceptance rate approximately 
wi unconstrained small weight decay wi encourage features localize 
wi initialized random vectors length initialized 
wi trained learning rate momentum factor 
result sensitive settings parameters 
random sample learned features whitened domain shown 
roughly ordered increasing spatial frequency 
hand counted total features localized spatial frequency domain 
features described gabor functions 
analyze set learned filters fitted gabor function form lewicki olshausen feature extracted parameters frequency location extent spatial frequency domains 
summarized figures show filters form nice tiling spatial frequency domains 
see figures filters learned multiple scales larger features typically lower frequency 
see emphasis horizontal vertical filters 
effect observed previous papers van hateren van der schaaf lewicki olshausen probably due 
teh spatial layout size filters described position size bars 
frequency polar plot frequency tuning orientation selectivity learned filters centre cross peak frequency orientation response describing bandwidth 
sparse overcomplete energy models learned filters cedar digits 
filters plotted whitened space clarity 
cedar digits real valued digits br set cedar cdrom 
digits available divided equally classes 
mean image entire dataset subtracted datum digits whitened zca 
network features trained manner natural image patches 
random subset learned filters shown 
easier discern structure learned filters zca whitened domain pixel space 
note superficial similarity filters natural scene experiments 
addition straight edge filters see curved filters 
interpret results set stroke detectors modelling space strokes gives rise full digit set 
feret faces full nist feret database frontal face images 
data pre processed standard manner aligning faces normalising pixel intensities cropping central oval shaped region 
additional preprocessing step centred data performed pca whitening retaining projections leading eigenvectors input dimensions 
www itl nist gov iad feret 

www cs edu index htm 
teh 
eigenfaces largest eigenvalue plotted rowwise descending eigenvalue order 
subset type ii feature vectors 
top row hand picked bottom row randomly selected 
subset type feature vectors randomly selected algorithm 
trained network features contrastive divergence hybrid monte carlo sampling sets leap frog steps 
wi unconstrained 
wi initialized uniformly vectors norm 
initialized 
learning rate wi whilst learning rate shows leading eigenvectors plotted face images shows subset filters learned algorithm 
bartlett 
kinds square ica applied lower resolution version feret database 
relation filters shown correspond type ii ica face constitutes input line type ica value pixel faces constitutes input line 
similarities notable differences features type ii results 
bartlett 
filters learn somewhat global sense pixels nonzero weight 
addition global features type ii results develop features weight concentrated localised sub regions instance focusing glasses eyes smiling furthermore global features described archetypical faces see global features appear mainly capture structure illumination face 
lastly illustrates results algorithm applied type manner pixels leading principal components inputs original pixel values 
approach leads features highly localised space 
results qualitatively similar described bartlett 

sparse overcomplete energy models show overcomplete feature sets algorithm delivers usefully employed face expression recognition system 

discussion re interpreted standard ica algorithm energy model studied extension overcomplete representations 
shown parameters ebm filter weights energy function efficiently estimated contrastive divergence learning 
number experiments standard data sets shown ebms efficiently extract useful features high dimensions 
contrary causal generative models overcomplete ica features ebm exhibit marginal dependencies 
advantage allowing dependencies model fast inference 
causal generative models assumption marginal independence leads inference needs approximated iterative data dependent scheme 
role iterations understood suppressing activity relevant features producing sparse code 
causal generative models overcomplete representations expected produce compact sparse codes fact emphasized desirable olshausen field 
surprisingly shown slow iterative process required producing sparse overcomplete representations 
suggest enriching ebms inhibitory lateral connections achieve goal suppressing relevant features order produce sparser representation 
preliminary experiments mean field approach implement lateral successful learning density models slow due iterative optimization data case 
powerful generalization ebms hierarchical non linear architecture output activities computed feed forward neural network layer may contribute total energy related see hoyer 
fit model data backpropagation compute gradients energy respect data vector hybrid monte carlo sampling weights weight updates 
algorithm applies backpropagation unsupervised setting combines contrastive divergence learning named contrastive backpropagation hinton 
contrastive backpropagation learning procedure quite flexible 
puts constraints smoothness activation functions energy functions 
procedure easily modified recurrent neural networks contain directed cycles running forward pass predetermined number steps defining energy smooth function time history activations 
backpropagation time rumelhart werbos obtain required derivatives 
data vector change forward pass recurrent network 
possible model sequential data video sequences running network forward time sequence running backward time compute derivatives required hybrid monte carlo sampling updating weights 
welling 
layer model studied second layer performed local averaging non linearly transformed activities layer 
resulted topographic 
necessary assumption energy defined boltzmann distribution normalizable 
output layer output layer input variables teh architecture hierarchical non linear energy model 
non linearities indicated sigmoidal units output layer 
energies contributed output variables layers number output variables need correspond number input variables 
ordering filters orientation location frequency changing smoothly filter 
energy approach ica stems previous products experts hinton :10.1.1.35.8613
fact model type poe energy term corresponds expert 
interesting link ebms maximum entropy models della pietra zhu 
probability distribution maximum entropy model defined boltzmann distribution equation sum energy contributions ei written ei iui ui fixed features model weights parameters model fit data 
sense proposed ebm interpreted maximum entropy model flexible learned features different energy function 
believe ebm provides flexible modelling tool trained efficiently uncover useful structure data 
acknowledgments peter dayan sam roweis zoubin ghahramani helpful discussions carl rasmussen making minimize available reviewers dave mackay helpful comments 
abdallah plumbley 
edges independent components natural images independent components natural sounds 
international conference independent component analysis blind signal separation 
sparse overcomplete energy models amari cichocki yang 
new algorithm blind signal separation 
advances neural information processing systems volume pages 
attias 
independent factor analysis 
neural computation 
bartlett movellan sejnowski 
face recognition independent component analysis 
ieee transactions neural networks 
press 
bell sejnowski 
information maximisation approach blind separation blind deconvolution 
neural computation 
cardoso 
infomax maximum likelihood blind source separation 
ieee signal processing letters 
chen donoho saunders 
atomic decomposition basis pursuit 
siam journal scientific computing 
comon 
independent component analysis new concept 
signal processing 
della pietra della pietra lafferty 
inducing features random fields 
ieee transactions pattern analysis machine intelligence 
girolami 
variational method learning overcomplete representations 
neural computation 
hinton 
training products experts minimizing contrastive divergence 
neural computation 
hinton teh 
discovering multiple constraints frequently approximately satisfied 
proceedings seventh conference uncertainty artificial intelligence pages 
hinton teh welling osindero 
contrastive backpropagation 
preparation 
hyvarinen 
estimating overcomplete independent component bases image windows 
journal mathematical imaging vision 
press 
hoyer 
layer sparse coding model learns simple complex cell receptive fields topography natural images 
vision research 
lewicki olshausen 
probabilistic framework adaptation comparison image codes 
journal optical society america optics image science vision 
lewicki sejnowski 
learning overcomplete representations 
neural computation 
mackay 
maximum likelihood covariant algorithms independent components analysis 
available electronically www inference phy cam ac uk mackay abstracts ica html 
teh mackay 
failures step learning algorithm 
available electronically www inference phy cam ac uk mackay abstracts gbm html 
mallat zhang 
matching pursuits time frequency dictionaries 
ieee transactions signal processing 
neal 
probabilistic inference markov chain monte carlo methods 
technical report crg tr university toronto department computer science 
olshausen field 
emergence simple cell receptive field properties learning sparse code natural images 
nature 
olshausen field 
sparse coding overcomplete basis set strategy employed 
vision research 
olshausen 
learning sparse codes mixture gaussians prior 
advances neural information processing systems volume pages 
pearlmutter parra 
context sensitive generalization ica 
proceedings international conference neural information processing 
rumelhart hinton williams 
learning internal representations error propagation 
rumelhart mcclelland pdp research group editors parallel distributed processing explorations microstructure cognition 
volume foundations 
mit press 
sompolinsky lee 
information maximization approach overcomplete recurrent representations 
advances neural information processing systems pages 
simoncelli freeman adelson heeger 
shiftable multi scale transforms 
ieee transactions information theory 
van hateren van der schaaf 
independent component filters natural images compared simple cells primary visual cortex 
proceedings royal society london 
welling hinton osindero 
learning sparse topographic representations products student distributions 
advances neural information processing systems 
werbos 
backpropagation time 
proceedings ieee 
williams 
analysis contrastive divergence learning gaussian boltzmann machines 
technical report edi inf rr institute adaptive neural computation university edinburgh 
zhu wu mumford 
minimax entropy principle application texture modelling 
neural computation 

