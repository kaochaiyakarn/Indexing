agglomerative information bottleneck noam slonim naftali tishby institute computer science center neural computation hebrew university jerusalem israel email cs huji ac il tishby introduce novel distributional clustering algorithm explicitly maximizes mutual information cluster data categories 
algorithm considered bottom hard version introduced information bottleneck method 
relate mutual information clusters categories bayesian classification error provides motivation obtained clusters features 
algorithm compared top soft version information bottleneck method relationship hard soft results established 
demonstrate algorithm newsgroups data set 
subset news groups achieve compression orders magnitudes loosing original mutual information 
problem self organization members set similarity conditional distributions members set introduced termed distributional clustering 
question shown special case fundamental problem features variable relevant prediction relevance variable general problem shown natural information theoretic formulation find compressed representation variable denoted mutual information high possible constraint mutual information surprisingly variational problem yields exact self consistent equations conditional distributions constrained information optimization problem called information bottleneck method 
original approach solution resulting equations analogy deterministic annealing approach clustering see :10.1.1.33.3047
top hierarchical algorithm starts single cluster undergoes cascade cluster splits determined stochastically phase transitions soft fuzzy tree clusters 
propose alternative approach information bottleneck problem greedy bottomup merging 
advantages top method 
fully deterministic yielding initially hard clusters desired number clusters 
gives higher mutual information cluster deterministic annealing algorithm considered hard zero temperature limit deterministic annealing prescribed number clusters 
furthermore bottleneck self consistent equations soften resulting hard clusters recover deterministic annealing solutions need identify cluster splits tricky 
main disadvantage method computational starts limit cluster member set information bottleneck method mutual information random variables symmetric functional joint distribution objective information bottleneck method extract compact representation variable denoted minimal loss mutual information relevance variable specifically want find possibly stochastic map minimizes lossy coding length constraint mutual information relevance variable words want find efficient representation variable close possible direct prediction predictions shown introducing multiplier positive lagrange enforce mutual information constraint problem amounts minimization lagrangian respect subject markov condition normalization 
minimization yields directly self consistent equations map normalization function 
liebler functional proved converge finite value see 
lagrange natural interpretation inverse temperature suggests deterministic annealing explore hierarchy solutions approach multiplier taken :10.1.1.33.3047
divergence emerges variational principle 
equations solved iterations variational principle eq determines shape changing annealing process mutual informations vary optimal curve analogous rate distortion function information theory follows strictly concave curve plane called information plane 
deterministic annealing fixed number clusters follows concave curve curve suboptimal certain critical value 
mutual information bayesian classification error alternative interpretation information bottleneck comes decision theory 
relevance vari ables denote classes prior probabilities members classified 
bayes decision error multiple classification problem defined error bounded see important information theoretic measure class conditional distributions called jensen shannon divergence 
measure plays important role context 
jensen shannon divergence class distributions prior defined shannon convexity entropy jensen inequality guarantees non negativity js divergence 
expressed terms kl divergence entropy expression interpretation measures far probabilities source joint see 
js divergence equals zero equal kl divergence bounded symmetric 
decision theoretic take context decision theoretic problem js divergence conditional distributions identical mutual information sample space classes important decision theoretic property js divergence provides upper lower bounds bayes classification error see representation classes obtains direct bound bayesian classification error 
entropy constraining js divergence equivalently mutual information provides decision theoretic interpretation information bottleneck method 
hard clustering limit finite cardinality representation limit eqs induces hard partition disjoint subsets 
limit member belongs subset probabilistic map smallest obtains limit values 
denote partition cardinality focus bottom agglomerative algorithm generating hard partitions case optimal say partition necessarily unique partition starting trivial partition seek sequence merges coarser coarser partitions close possible optimal 
easy verify limit eqs partition distributions simplified follows 
denote specific component cluster partition distributions easily evaluate mutual information eq 
hard partition hard clustering obtained apply reverse annealing soften clusters decreasing self consistent equations eqs 
procedure fact recover stochastic map hard partition need identify cluster splits 
demonstrate reverse deterministic annealing procedure section 
relation similar agglomerative procedure information theoretic framework analysis text categorization newsgroup corpus 
approach stems distributional clustering algorithm clustering dyadic data :10.1.1.46.2857
earlier application mutual information semantic clustering words 
agglomerative information bottleneck algorithm algorithm starts trivial partition clusters components component contains exactly element step merge components current partition single new component way locally minimizes loss mutual information partition denote new current partition merge compo nents obviously denote set components merged new component generated merge 
evaluate reduction mutual information due merge needs distributions define new partition determined follows 
probability distributions remains equal distributions new component define easy verify valid partition proper probability distributions 
notations merge define additional quantities merge prior distribution defined 
merged subset information decrease decrease mutual information prior probability information decrease decrease mutual information properties analysis due single merge due single merge simple properties quantities important algorithm 
proofs forward omit 
proposition decrease mutual information due merge merge distribution 
notice merge cost interpreted multiplication weight elements merge distance js divergence conditional distributions intuitive 
proposition decrease mutual information due merge merge distribution 
proposition implies important properties including monotonicity merge size proposition single merge components merges pairs components 
corollary corollary pairs 
optimal partition build constructed consecutive consecutive merges current minimize algorithm greedy procedure step perform best possible merge merge components partition increase current partition corollary greedy procedure check possible merging pairs components partition 
advantage merging pairs way go possible cardinalities 
information possible pairs merge 
find best possible merge evaluate reduction pair operations pair 
proposition know reduction mutual information due merge evaluated directly looking pair operations reduction factor time complexity merge 
way cutting computation time precomputing initially merge costs merge update merge costs pairs containing elements merge 
discussion input empirical probability matrix partition output initialization construct clusters calculate points corresponding couple loop find minima choose arbitrarily merge update new partition clusters update costs pointers 
couples contained 
pseudo code algorithm 
algorithm non parametric simple greedy procedure depends input empirical joint distribution 
output algorithm partitions hierarchy clustering heuristics built measure efficiency suboptimal solutions mutual information bounds bayes classification error 
quality measure obtained partition fraction mutual information captures 
curve empirically curve concave 
vs true decrease mutual information step increase decreasing point relatively high indication reached value meaningful partition clusters 
merging results substantial loss information significant reduction performance clusters features 
computational cost final part procedure low just complete merging low single cluster 
question optimality greedy procedure locally optimal step maximize cost function guarantee specific greedy procedure induces optimal partition 
general guarantee move partition optimal partition simple process merging 
reason optimal partition optimal restricted hard merges just small subset possible partitions checked assurance optimal 
situation complicated cases greedy decision unique pairs minimize choosing check potential merges resulting exponential number merges worst case 
may possible formulate conditions guarantee greedy procedure optimal obtains partition particular cardinality relevance set call dichotomy case evidence procedure may fact optimal 
details analysis 
ng hard ng ng ng ng left results agglomerative algorithm shown information plane normalized vs normalized ng dataset 
compared soft version information bottleneck reverse annealing smooth curves left 
annealing curve connected starting point dotted line 
plane hard algorithm clearly inferior soft 
right hand side agglomerative algorithm plotted vs cardinality partition subsets newsgroup dataset 
compare performance different data cardinalities normalize value forcing curves start points 
predictive information newsgroup ng similar dichotomy dataset ng better prediction possible ng expected dichotomies 
inset presents full curve normalized vs ng data comparison 
plane hard partitions superior soft ones 
application evaluate ideas algorithm apply subsets newsgroups dataset collected ken lang articles evenly distributed usenet discussion groups see 
replaced digit single character mark non alphanumeric characters 
pre processing dataset contained strings appeared times data 
dataset referred ng similarly strings appeared times constitutes ng dataset contains different strings 
evaluate dichotomy data corpus consisting discussion groups newsgroups similar topics alt atheism talk religion misc 
pre processing removing strings occur times resulting lexicon contained different strings 
refer dataset ng 
plot results algorithm data sets different planes 
normalized information vs size partition number clusters greedy procedure directly tries maximize seen strong concavity curves right 
procedure able maintain high percentage relevant mutual information original data reducing dimensionality features orders magnitude 
right hand side comparison efficiency procedure datasets 
class data consisting different strings compressed orders magnitude clusters loosing mutual information news groups decrease original mutual information 
compression orders magnitude clusters maintains similar results striking obtained contain newsgroups 
ng dataset compressed strings clusters keeping mutual information clusters keeping information 
compression efficiency obtained ng dataset 
relationship soft hard clustering demonstrated information plane normalized mutual information values vs plane soft procedure optimal direct maxi mization constraining hard partition suboptimal plane confirmed empirically provides excellent starting point reverse annealing 
results agglomerative procedure ng information plane reverse annealing different values predicted theory annealing curves merge various critical values globally optimal curve correspond rate distortion function information bottleneck problem 
reverse annealing heating procedure need identify cluster splits required original annealing cooling procedure 
seen phase diagram better recovered procedure suggesting combination agglomerative clustering reverse annealing ultimate algorithm problem 
baker mccallum 
distributional clustering words text classification acm sigir 
brown desouza mercer dellapietra lai 
class gram models natural language 
computational linguistics 
cover thomas 
elements information theory 
john wiley sons new york 
el yaniv fine tishby 
agnostic classification markovian sequences 
advances neural information processing nips 
hofmann buhmann 
pairwise data clustering deterministic annealing 
ieee transactions pami 
hofmann puzicha jordan :10.1.1.46.2857
learning dyadic data 
advances neural information processing nips appear 
lin 
divergence measures shannon entropy 
ieee transactions information theory 
rose :10.1.1.33.3047
deterministic annealing clustering compression classification regression related optimization problems 
proceedings ieee 
pereira tishby lee 
distributional clustering english words 
th annual meeting association computational linguistics columbus ohio pages 
tishby bialek pereira 
information bottleneck method extracting relevant information concurrent data 
unpublished manuscript nec research institute tr 
