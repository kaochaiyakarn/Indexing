yee teh computer science div 
university california berkeley ca eecs berkeley edu propose semiparametric model regression problems involving multiple response variables 
model set gaussian processes linearly mixed capture dependencies may exist response variables 
propose efficient approximate inference scheme semiparametric model complexity linear number training data points 
experimental results domain multi joint robot arm dynamics 
semiparametric latent factor models matthias seeger computer science div 
university california berkeley ca eecs berkeley edu interested supervised problems involving multiple responses model conditionally dependent 
statistical terminology share statistical strength multiple response variables machine learning parlance referred transfer learning demonstrate empirically sharing especially powerful data responses partially missing 
focus multivariate regression problems 
models related proposed spatial prediction name kriging example domain helps give idea want achieve technique 
accidental uranium spill spatial map uranium concentration sought covering limited area 
take soil samples locations choice measure uranium content gaussian process regression spatial prediction technique infer map 
known carbon concentration uranium concentration significantly correlated carbon concentration easier measure section indicate technique extended settings multi label classification 
michael jordan computer science div 
dept statistics university california berkeley ca jordan eecs berkeley edu lowing dense measurements 
kriging aim set joint spatial model responses aim improving prediction 
model described current goes simple kriging methods ways 
combining responses manner model uses latent random processes represent conditional dependencies responses directly 
latent processes fitted data responses model dependencies solely marginal relationships 
second nature dependencies known advance learned training data empirical bayesian techniques 
example motivating application arises computer vision interest estimate pose human image data 
case response variables joint angles human body 
known human poses highly constrained useful pose estimation algorithm take account strong dependencies joint angles 
historically problem capturing commonalities multiple responses motivations multi layer neural networks hidden units neural network envisaged nonlinear transformations adaptive basis functions shared predicting multivariate responses 
neural networks gave way kernel machines classification regression improvements flexibility analytical tractability performance core ability neural networks largely lost 
elaborate point note main paths neural networks kernel machines 
path due involved observation particular limit probability associated bayesian interpretation neural network approaches gaussian process 
purposes arguably advantageous directly gaussian process covariance function 
limit turns components response output vector independent ability model couplings components lost 
second path kernel machines optimization margins simplified problem fitting dimensional responses largely neglected problem fitting dependent multivariate responses 
problem returned research agenda architectures conditional random field links response variables graphical model formalism :10.1.1.120.9821
approach modeling dependencies response variables heads direction nonparametric crf 
spirit factor analysis view relationships components response vector reflecting linear generalized linear mixing underlying latent variables 
latent variables indexed covariate vector set indexed collections variables set stochastic processes 
specifically assume variables conditionally independently distributed gaussian process common index set 
mean response possibly nonlinear function linear combination conditionally independent gaussian processes 
model semiparametric model combines nonparametric component gaussian processes parametric component linear mixing 
refer model semiparametric latent factor model 
note factor analysis special case arising constant 
note neal limiting gaussian process special case arising 
discuss section viewed gaussian process version multiple kernel learning architecture proposed 
case simpler gaussian process models significant part challenge working computational 
challenge largely met exploiting developments literature fitting large scale gaussian process regression classification models 
particular informative vector machine ivm framework gaussian processes :10.1.1.12.8842:10.1.1.135.8784
framework subset informative likelihood terms included computation posterior yielding training algorithm scales linearly number training data points 
bayesian underpinnings ivm yields general methods setting free parameters hyperparameters important capability easily achieved context kernel methodologies 
semiparametric latent factor models section give description model nonparametric regression multiple responses 
short overview gaussian process gp regression simpler setting single responses 
gp viewed prior random real valued functions parametrized mean function covariance kernel 
random function said distributed gp finite subset 
xm covariate vectors random vector 
xm distributed gaussian mean 
xm covariance th entry xi xj 
gps zero mean covariance kernel satisfy symmetric positive definiteness spd property spd finite subset gp simply consistent way assigning multivariate gaussian distributions finite gps traditionally bayesian classification regression single response problem treated estimating random univariate function covariate space response space 
assuming parametric form random function nonparametric bayesian approach places gp prior space functions infers posterior functions training data 
returning multiple response setting covariate input space response output space 
interested predicting 
yc estimating 
model conditional distribution latent variables components yc mutually independent independent yc vc 
conditional distribution yc vc dv 
focus regression gaussian errors yc vc yc vc 
treat particular gps 
introducing set latent variables letting linearly related assume coordinates independent gp priors conditional random functions distributed gp zero mean covariance kernel kp 
setup allows gaussian linear likelihood processes mixing semiparametric latent factor model 
conditional dependencies coordinates expressed latent variables 
form assumed eq 
direct analogy factor analysis models 
note information learned single response variable yc reflected posterior latent gps 
sharing statistical strength response variables 
call model semiparametric latent factor model graphical model shown 
parameters components hyperparameters aka nuisance parameters kernel parameters variance parameters associated gaussian likelihoods yc vc 
note coordinate vc priori gp covariance kernel pkp th element matrix 
interpretation model response variable modeled gaussian process kernel adaptive conic combination base kernels 
fact model viewed gaussian process version kernel learning proposed 
model just fit kernel latent gaussian processes response variable allowing expressive sharing information response variates 
case focus current efficient represent explicitly integrate 
inference model training samples 
xn drawn model perform bayesian inference latent variables estimate parameters hyperparameters empirical bayes framework 
introducing relevant latent variables 
ui xi vi vc xi 
collect vectors ui vi dou allow incomplete observations yi entries allowed unobserved simply involves dropping likelihood terms corresponding unobserved entries 
ble indices flattened index running 

un 
un vectors linearly related kronecker product 
assume full rank pseudoinverse exists case requires different treatment 
variable distributed priori gaussian zero mean block diagonal covariance matrix diag th block th entry kp xi xj 
covariance 
posterior processes gaussian case gaussian likelihoods yc vc principle compute mean covariance functions explicitly 
prohibitively expensive fairly small values procedure scales general 
informative vector machine ivm framework computes sparse approximation full gaussian posterior means greedy forward selection active subset training sample information theoretic criteria :10.1.1.12.8842
difference processes represented dependencies empirical bayes maximization encompass large number non kernel parameters elements 
forward selection active set size consists tuples 


goal select approximate posterior yi vi close 
posterior approximation simply ignoring observations method selecting depends complete sample discussed section 
idea greedily select candidate changes posterior include incorporate likelihood term 
way measuring change information gain studied setting active learning sequential design 
qk denotes posterior approximation inclusions criterion info qi qk qi vi qk vi qi approximate posterior obtain term included iteration iteration pick maximizes info qi vi yi vi qk vi compute info current marginal qk vi known 
representation described section possible maintain marginals times score prior inclusion 
iterations active set 
ik ck determines approximate posterior eq 

representing approximate posterior representation approximate posterior eq 
satisfy properties order useful allow marginals vi maintained explicitly times allows forward selection see section small memory footprint efficiently updated new likelihood term included 
section describe properties achieved 
diag diagonal matrix bk vector collects parameters approximate posterior sense exp vti vi vi vi vi 
vid cd case regression ck bk ck note ck ordered order likelihood terms included 
convert ordering natural ordering define selection matrix rd nc ik ck 
zeros note natural ordering flatten indices runs 
covariance qk assumption represent approximate posterior terms minimize memory time requirements 
covariance qk 
sherman morrison woodbury formula obtain lower triangular cholesky factor kp mean qk obtained mean variance vi qk hi hi ai hi ai mean covariance ui extracted entries eq 
eq 
respectively th row 
forward selection carried hi computed included active set 
representation np mean hi covariance ai ui 
rows ordered inclusion iteration updated efficiently stably appending new rows columns 
iteration included likelihood term compute pm bk matrix rows new row new column new entry 
hi updated adding subtracted ai 
memory requirement dominated np matrix vector multiplications involved computing dominate update time complexity np 
computing information gain scores info requires time general subdominant np 
note costs linear number training points dominant factor large applications 
parameter hyperparameter estimation described effective procedure compute approximation posterior latent variables 
outline empirical bayes estimation parameters hyperparameters 
denote parameter hyperparameter interest denote variational parameters define approximate posterior active set indicators compute marginal probability exactly optimize variational lower bound log eq log approximate posterior eq 

double loop iterative procedure inner loop optimize eq 
respect quasi newton method keeping fixed outer loop new greedily detailed 
notice dependent 
purposes optimizing propagate derivatives respect keep fixed 
differs variational methods keep fixed optimizing 
note optimization guaranteed converge updates guaranteed increase lower bound 
practice find optimization increases behaves 
criterion gradient computation complexity conditional inference faster practice code large matrix operations 
memory requirements increased significantly overwritten 
derivation involved 
experiments section experimental results regression task modeling dynamics robot arm 
dataset created realistic simulation code provides mapping twelve covariates angles angular velocities torques joints arm responses angular accelerations joints 
preprocessed raw data fitting linear regression training set replacing responses corresponding residuals normalizing covariate response variables mean zero variance 
removal linear component regression helps clarify relative contributions nonlinear methods focus 
responses linearly mixed randomly sampled unit length vectors produce response variables 
dataset mapping twelve covariates responses known latent variables sufficient capture mapping 
dataset sizes training points testing 
report mean squared error mse average marginal log probability logp experiments 
calibrate numbers note linear regression average mse task 
compare model baseline method indep response variable simply modeled independently ivm technique models 
model joint active set size baseline individual active sets size response variate 
clear similar training set size coverage active points training indep significantly faster study attempt equalize training times 
logp log averaged test set 
models squared exponential covariance function kp exp xl length scale dimension sets variance 
kernel latent gps set variance represented scaling columns 
allow different length scales input dimension find significant impact quality prediction length scale constrained covariate dimensions relevant input dimensions tend obscure relevant ones 
note large set hyperparameters adjusted training set empirical bayesian framework validation set needed 
mean squared errors log probabilities shown table function varied 
model performs best similar accuracy achieved 
expect performance degrade larger values investigated 
rest section chose smallest value supported data 
logp table mean squared errors response variate test set average log probability training point assigned model varying compared baseline independently modeled response variables 
training set size active sets size indep active sets disjoint union size 
results shown table 
note ms errors model smaller baseline 
tested effects varying active set size results table 
see active set size significant effect prediction accuracy 
quadratic scaling observed training times largest values component gradient computation dominates component 
method models dependencies response variables test point joint predictive distribution response variables indep indep mse mse logp logp table comparing model baseline indep robot arm task 
rows correspond response variables 
time table ms test errors response active set size varied 
time gives complete training time seconds 
improve prediction model specific component addition covariates subset response variables 
table show mean squared errors attained response responses 
errors reduced significantly especially improve observe responses particular observe 
note baseline method response variable modeled independently predictive distribution factorizes knowledge responses help predicting mse logp table improved predictions model response variable model responses 
report experiment aimed improving understanding statistical strength shared response variables 
focus predicting response variate task training points 
presenting covariate response pairs simultaneously start observing response variable subset points 
subse quently covariate vectors corresponding responses subset including 
ask improve prediction response variable 
note setup similar kriging scenarios mentioned section 
table shows mean squared errors attained various values various subsets additional observed response variables 
see large improvement mean squared error 
errors smaller table 
strong dependencies response variables seen table 
note case performed worse yielding marked improvement additional training set 
occurs functional method optimizes joint functional response variables depends strongly response variables training data 
performance assessed functional improved larger goal obtain prediction particular response variable consider different functional focuses response variable interest 
table mean squared error test set varying training set sizes additional response variables discussion described model nonlinear regression problems involving multiple linked response variables 
manner reminiscent factor analysis parametric setting model response vector function linear combination set independent latent gaussian processes 
simple semiparametric approach sharing statistical strength number virtues notably flexible parametrization terms sets covariance kernels computational tractability 
efficient approximate inference strategy ivm 
primary focus prediction inferential tools provided ivm allow compute posteriors various components model particular latent factors parameters 
possible extensions model include placing automatic relevant determination ard prior columns mixing matrix letting model determine automatically 
interest consider ways mixing matrix dependent covariates 
ways combining multiple gaussian processes 
models rameters set gaussian processes endowed common prior 
hierarchical model couples gaussian processes amount sharing induces limited involves hyperparameters gaussian processes 
approach sharing involves entire processes expressive 
note considered tasks involving single regression problem multiple responses readily accommodate setting multiple related tasks single response separate training set 
noted semiparametric approach alternative parametric methodology conditional random fields crfs focus attention machine learning computer vision communities 
response variables plausibly linked simple structure example chain tree crf approach preferable approach 
hand graph chain potential intractability partition function significant drawback crf approach 
vision problems example dimensional markov random field modeling dependencies runs problem partition function 
approach couplings variables arise marginalizing latent set linearly mixed gaussian processes provides alternative implicit approach linking variables 
cases graphical models intractable approach may provide requisite tractability cost modeling flexibility 
note approach kernel approach definition need explicitly 
methods multiple responses regression proposed involve combinations outputs independent baseline method 
example method multiple linear regression 
important stress approach fundamentally different latent processes fitted jointly data 
processes represent conditional dependencies directly processes baseline method see marginal data response 
combination schemes successful response dependencies mainly unconditional may fail represent dependencies change advantage methods cheap computationally having essentially scaling independent baseline subroutine 
flexible technique computational complexity closer baseline method exists open question 
applications classification model extended classification problems problems involving non gaussian likelihoods yc vc 
basic idea gp techniques ivm extended classification single response variable case :10.1.1.12.8842
non gaussian likelihoods effectively replaced gaussians parameters determined sequential moment matching 
extension classification particular interest multiple response variable setting allows address multi label classification problems class labels assumed mutually exclusive may exhibit interesting useful interdependencies 
preliminary investigation extension considered toy example shown 
sampled dimensional covariate vectors uniformly random labeled vectors binary response variables regions shown top left panel 
label noise training responses missing random 
fit data suitably extended probit likelihoods latent gps different kernel eq 
single length scale parameter 
top left regions 
rest posterior mean function latent gps 
light colors correspond larger values 
training test set errors response variables 
remaining panels show approximate posterior mean functions latent gps 
roughly speaking gp vertical discrimination horizontal third inside vs outside separation gps distinguish inside vs outside separation lesser extent 
see model formed combinatorial code able classify response variables latent gps 
issues computational issues remain serious concern scaled larger problems 
main avenue open tackling larger datasets refrain scoring remaining points inclusions 
particular number initial inclusions selected remaining points potentially narrow candidate set stepwise excluding points worst current scores 
empirical bayes learning procedure approximates full likelihood likelihood restricted final candidate set includes active set 
large tasks elaborate caching strategies envisaged 
natural limit active set size imposed time memory scaling 
focused case current great interest explore cases particular regime variable constrained lie dimensional subspace analogy factor analysis suggests may useful constraint problems may impose overly narrow bottleneck regression mapping problems 
possible ways remove constraint consider versions operate regime 
interesting variant involves replacing eq 
components conditionally independent gp priors different kernels 
setup viewed simply particular choice generic additional independences model aid design approximate inference methods variant belief propagation 
acknowledgments supported intel microsoft research darpa support calo program 
agarwal triggs 
human pose silhouettes relevance vector regression 
proceedings ieee international conference computer vision pattern recognition 
breiman friedman 
predicting multivariate re multiple linear regression 
roy 
stat 
soc 

cressie 
statistics spatial data 
john wiley sons nd edition 
kumar hebert 
discriminative random fields discriminative framework contextual interaction classification 
proceedings ieee international conference computer vision 
lafferty pereira mccallum 
conditional random fields probabilistic models segmenting labeling sequence data 
proceedings icml 
lanckriet cristianini bartlett el ghaoui jordan 
learning kernel matrix semidefinite programming 
journal machine learning research 
lawrence seeger herbrich :10.1.1.12.8842
fast sparse gaussian process methods informative vector machine 
advances nips pages 
lawrence platt 
learning learn informative vector machine 
proceedings international conference machine learning 

hierarchical modeling gaussian processes 
communications statistics simulation computation 
neal 
priors infinite networks 
technical report crg tr department computer science university toronto 
seeger 
bayesian gaussian process models pac bayesian generalisation error bounds sparse approximations 
phd thesis university edinburgh july 
cs berkeley edu 
seeger 
teh jordan 
semiparametric latent factor models 
technical report university california berkeley 
see www cs berkeley edu 
taskar guestrin koller 
max margin markov networks 
advances nips 
appear 
vladimir vapnik 
estimation dependences empirical data 
series statistics 
springer st edition 
