natural policy gradient gatsby computational neuroscience unit queen square london uk wc ar www gatsby ucl ac uk gatsby ucl ac uk provide natural gradient method represents steepest descent direction underlying structure parameter space 
gradient methods large changes values parameters show natural gradient moving choosing greedy optimal action just better action 
greedy optimal actions chosen improvement step policy iteration approximate compatible value functions deo ned sutton :10.1.1.146.4070
show drastic performance improvements simple mdps challenging mdp tetris 
growing interest direct policy gradient methods approximate planning large markov decision problems mdps 
methods seek nd policy ss restricted class policies gradient reward 
unfortunately standard gradient descent rule 
crudely speaking rule delta ff dimensionally inconsistent left hand side units right hand side units necessarily dimensions 
covariant gradient deo ning metric underlying structure policy 
connection policy iteration showing natural gradient moving choosing greedy optimal action 
analyze performance natural gradient simple complicated mdps 
consistent amari ndings suggests plateau phenomenon severe method 
natural gradient nite mdp tuple nite set states start state nite set actions reward function theta rmax transition model 
agent decision making procedure characterized stochastic policy ss probability action state semi colon distinguish random variables parameters distribution 
assumption policy ss ergodic deo ned stationary distribution ae ss assumption average reward undiscounted reward ss ae ss ss state action value ss st gamma ss js ag value function ss ess fq ss st state action time consider dioecult case goal agent nd policy maximizes average reward restricted class smoothly parameterized policies pi fss ss represents policy ss 
exact gradient average reward see rj ae ss rss ss abuse notation ss :10.1.1.146.4070
steepest descent direction deo ned vector minimizes constraint squared length jd held small constant 
squared length deo ned respect positive deo nite matrix jd ij gij id vector notation 
steepest descent direction gamma rj 
standard gradient descent follows direction rj steepest descent assumption identity matrix hoc choice metric necessarily appropriate 
suggested amari better deo ne metric choice coordinates manifold surface coordinates parameterize 
metric deo nes natural gradient 
slightly abuse notation writing average reward technically function set distributions fss state corresponds probability manifold distribution ss point manifold coordinates 
fisher information matrix distribution ss fs ess log ss log ss clearly positive deo nite 
shown amari see fisher information matrix scale invariant metric space parameters probability distributions 
invariant sense deo nes distance points regardless choice coordinates parameterization average reward deo ned set distributions straightforward choice metric fs expectation respect stationary distribution ss 
notice fs independent parameters mdp transition model weighting stationary distribution introduces dependence parameters 
intuitively fs measures distance probability manifold corresponding state average distance 
steepest descent direction gives rj gamma rj natural gradient policy iteration compare policy improvement natural gradient policy iteration 
appropriate comparison consider case ss approximated compatible function approximator ss parameterized 
compatible function approximation vectors deo ne ss log ss ss ss log ss log ss 
minimize squared error ffl ss ae ss ss ss gamma ss function approximator compatible policy sense approximations ss 
lieu true values compute gradient equation result exact sensible choice actor critic schemes 
theorem 

minimize squared error ffl ss 
rj proof 

minimizes squared error es condition ffl implies ae ss ss ss ss 
gamma ss equivalently ae ss ss ss ss ae ss ss ss ss deo nition ss rss ss ss right hand side equal rj 
deo nition ss ae ss ss ss ss substitution leads 
rj solving 
gives 
gamma rj result follows deo nition natural gradient 
sensible actor critic frameworks ss forced natural gradient weights linear function approximator 
function approximation accurate actions large state action values feature vectors large inner product natural gradient 
greedy policy improvement greedy policy improvement step function approximator choose action state argmax ss 
section show natural gradient tends move best action just action 
rst consider policies exponential family ss exp oesa oesa feature vector 
motivation exponential family geometry geometry plane translation point tangent vector keep point manifold 
general crudely speaking probability manifold ss curved translation point tangent vector necessarily keep point manifold sphere 
consider general non exponential case 
show exponential family suoeciently large step natural gradient direction lead policy equivalent policy greedy policy improvement step 
theorem 
ss exp oesa assume rj non zero 
minimizes approximation error 
ss ss ff rj 
ss argmax ss 
proof 
previous result ss 
rj ss 
deo nition ss ss oesa gamma ess oesa 
ess oesa function follows argmax ss 
argmax rj oesa gradient step ss ff rj exp oesa ff rj oesa 
rj clear ff term rj oesa dominates ss argmax rj oesa 
sense natural gradient tends move choosing best action 
straightforward show standard non covariant gradient rule ss select better action necessarily best choose action ss 
ess ff ss exponential family demonstrate point extreme case ino nite learning rate 
return case general parameterized policy 
theorem shows natural gradient locally moving best action determined local linear approximator ss 
theorem 
assume 
minimizes approximation error update parameter ff rj 
ss ss ss ff proof 
change delta ff rj theorem delta ff 
rst order ss ss ss delta delta ss delta delta ss ff 
ff ss fff ss ff deo nition interesting note choosing greedy action general improve policy detailed studies gone understanding failure 
overhead line search guarantee improvement move greedy step improvement 
initial improvement guaranteed positive deo nite 
metrics curvatures obviously choice unique question arises better metric dioeerent setting parameter estimation fisher information converges hessian asymptotically eoecient attains cramer rao bound 
situation similar blind source separation case metric chosen underlying parameter space non singular matrices necessarily asymptotically eoecient attain second order convergence 
argued mackay strategy pull metric data independent terms hessian possible fact mackay arrives result amari blind source separation case 
previous sections argued choice appropriate understand relates hessian shown form sa ae ss ss ss rss rq ss rq ss rss unfortunately terms hessian data dependent coupled stateaction values 
clear capture information terms due rq ss dependence 
rst term relation due factor ss 
values weight curvature policy metric neglecting weighting 
similar blind source separation case metric clearly necessarily converge hessian necessarily asymptotically eoecient attain second order convergence rate 
general hessian positive deo nite curvature provides little close local maxima 
conjugate methods expected eoecient near local maximum 
experiments rst look performance natural gradient simple mdps examining performance challenging mdp tetris 
straightforward estimate online manner derivatives log ss computed anyway estimate rj 
update rule log ss st log ss st length trajectory consistent estimate rst examples concern sampling issues numerically integrate exact derivative rj 
simulations policies tend deterministic log ss prevent singular add gamma step simulations 
simulated natural policy gradient simple dimensional linear quadratic regulator dynamics ffl noise distribution ffl 
goal apply control signal keep system incurring cost step 
parameterized policy ss exp 
shows performance improvement units parameters scaled factor see gure text 
notice time obtain score orders magnitude unscaled log time cost time reward time average qi qj cost vs log time lqg time step trajectories 
policy ss exp rescaling constants shown legend 
equivalent starting distributions gamma right curves generated standard gradient method rest natural gradient 
see text 
top average reward vs time scale policy standard gradient descent sigmoidal policy parameterization ss exp exp initial conditions ss ss 
bottom average reward vs time unscaled standard gradient descent solid line natural gradient descent dashed line early window plot 
phase space plot standard gradient case solid line natural gradient case dashed line 
faster 
notice curves dioeerent rescaling identical 
invariant metric due weighting aes 
weighting ae particularly clear simple state mdp self cross transition actions rewards shown 
increasing chance self loop decreases stationary probability sigmoidal policy parameterization see gure text initial conditions corresponding ae ae self loop action probabilities initially increased gradient rule step policy improvement chooses self loop state 
standard gradient weights learning parameter ae see equation self loop action state increased faster self loop probability eoeect decreasing effective learning rate state 
leads extremely plateau average reward shown top learning state thwarted low stationary probability 
problem severe optimal policy reached ae drops low gamma initial value disastrous sampling methods 
bottom shows performance natural gradient early time window top 
time optimal policy decreased factor stationary distribution state drops 
note standard gradient increase average reward faster start sticking state phase space plot shows uneven learning dioeerent parameters heart problem 
general table lookup boltzmann policy ss exp sa straightforward show natural gradient weights components rj uniformly ae evening evening learning parameters 
game tetris provides challenging high dimensional problem 
shown greedy policy iteration methods linear function approximator exhibit drastic performance degradation providing impressive improvement see description game methods results 
upper curve replicates results 
tetris provides interesting case test gradient methods log iterations points iterations points iterations points points vs log iterations 
top curve duplicates results features simple functions heights column number holes game 
explanation performance degradation 
lower curve shows poor performance standard gradient rule 
curve right shows natural policy gradient method uses biased gradient method method gave poor performance 
obtain faster improvement higher asymptotes factor gamma added carefully controlled carefully control parameters 
due intensive computational power required simulations ran gradient smaller tetris game height demonstrate standard gradient updates right curve eventually reach performance natural gradient left curve 
guaranteed degrade policy 
consider policy compatible linear function approximator ss exp oesa oesa feature vectors 
features heights column dioeerences height adjacent columns maximum height number holes 
lower curve shows particularly poor performance standard gradient method 
attempt speed learning tried variety sophisticated methods avail conjugate methods weight decay annealing variance reduction method hessian equation shows drastic improvement natural gradient note timescale linear 
performance consistent theoretical results section showed natural gradient moving solution greedy policy improvement step 
performance somewhat slower greedy policy iteration left curve expected smaller steps 
policy degrade gradient method 
shows performance standard gradient rule right curve eventually reaches performance natural gradient scaled version game see gure text 
discussion gradient methods large policy changes compared greedy policy iteration section implies methods disparate natural gradient method moving solution policy improvement step 
overhead line search methods similar 
performance improvement guaranteed greedy policy iteration step 
interesting unfortunate note asymptotically converge hessian conjugate gradient methods sensible asymptotically 
far converge point hessian necessarily informative natural gradient eoecient demonstrated tetris 
intuition natural gradient eoecient far maximum pushing policy choosing greedy optimal actions 
region parameter space far maximum large performance changes occur 
suoeciently close maximum little performance change occurs due small gradient conjugate methods converge faster near maximum corresponding performance change negligible 
experimental necessary understand natural gradient 
acknowledgments emo peter dayan helpful discussions 
funding nsf gatsby charitable foundation 
amari 
natural gradient works eoeciently learning 
neural computation 
baxter bartlett 
direct gradient reinforcement learning 
technical report australian national university research school information sciences engineering july 
bertsekas tsitsiklis 
neuro dynamic programming 
athena 
dayan hinton 
em reinforcement learning 
neural computation 

optimizing average reward discounted reward 
colt 
press 
tsitsiklis 
actor critic algorithms 
advances neural information processing systems 
mackay 
maximum likelihood covariant algorithms independent component analysis 
technical report university cambridge 
tsitsiklis 
simulation optimization markov reward processes 
technical report massachusetts institute technology 
sutton mcallester singh mansour :10.1.1.146.4070
policy gradient methods reinforcement learning function approximation 
neural information processing systems 
xu jordan 
convergence properties em algorithm gaussian mixtures 
neural computation 
