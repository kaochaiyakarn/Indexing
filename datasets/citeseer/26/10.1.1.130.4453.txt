journal computational biology volume numbers mary ann pp 
bayesian networks analyze expression data nir friedman michal linial nachman dana pe er dna hybridization arrays simultaneously measure expression level thousands genes 
measurements provide snapshot transcription levels cell 
major challenge computational biology uncover measurements gene protein interactions key biological features cellular systems 
propose new framework discovering interactions genes multiple expression measurements 
framework builds bayesian networks representing statistical dependencies 
bayesian network graph model joint multivariate probability distributions captures properties conditional independence variables 
models attractive ability describe complex stochastic processes provide clear methodology learning noisy observations 
start showing bayesian networks describe interactions genes 
describe method recovering gene interactions microarray data tools learning bayesian networks 
demonstrate method cerevisiae cell cycle measurements spellman 

key words gene expression microarrays bayesian methods 

goal molecular biology understand regulation protein synthesis reactions external internal signals 
cells organism carry genomic data protein makeup drastically different temporally spatially due regulation 
protein synthesis regulated mechanisms different stages 
include mechanisms controlling transcription initiation rna splicing mrna transport translation initiation post translational modi cations degradation mrna protein 
main junctions regulation occurs mrna transcription 
major role machinery played proteins bind regulatory regions dna greatly affecting transcription genes regulate 
years technical breakthroughs spotting hybridization probes advances genome sequencing efforts lead development dna microarrays consist species probes cdna prede ned organization solid phase 
school computer science engineering hebrew university jerusalem israel 
institute life sciences hebrew university jerusalem israel 
center neural computation school computer science engineering hebrew university jerusalem israel 
friedman dna microarrays researchers able measure abundance thousands mrna targets simultaneously derisi lockhart wen 
classical experiments expression levels genes reported dna microarray experiments measure genes organism providing genomic viewpoint gene expression 
consequence technology facilitates new experimental approaches understanding gene expression regulation iyer spellman 
early microarray experiments examined samples mainly focused differential display tissues conditions interest 
design experiments focuses performing larger number microarray assays ranging size dozen hundreds samples 
near data sets containing thousands samples available 
experiments collect enormous amounts data clearly re ect aspects underlying biological processes 
important challenge develop methodologies statistically sound computationally tractable analyzing data sets inferring biological interactions 
analysis tools currently clustering algorithms 
algorithms attempt locate groups genes similar expression patterns set experiments alon ben dor eisen spellman :10.1.1.34.5341
analysis proven useful discovering genes regulated similar function 
ambitious goal analysis reveal structure transcriptional regulation process chen somogyi weaver 
clearly hard problem 
current data extremely noisy 
mrna expression data gives partial picture re ect key events translation protein activation 
amount samples largest experiments foreseeable provide information construct fully detailed model high statistical signi cance 
introduce new approach analyzing gene expression patterns uncovers properties transcriptional program examining statistical properties dependence conditional independence data 
base approach studied statistical tool bayesian networks pearl 
networks represent dependence structure multiple interacting quantities expression levels different genes 
approach probabilistic nature capable handling noise estimating con dence different features network 
able focus interactions signal data strong 
bayesian networks promising tool analyzing gene expression patterns 
particularly useful describing processes composed locally interacting components value component directly depends values relatively small number components 
second statistical foundations learning bayesian networks observations computational algorithms understood successfully applications 
bayesian networks provide models causal uence bayesian networks mathematically de ned strictly terms probabilities conditional independence statements connection characterization notion direct causal uence 
heckerman pearl verma spirtes :10.1.1.51.7221:10.1.1.48.9593
connection depends assumptions necessarily hold gene expression data bayesian network analysis indicative causal connections data 
remainder organized follows 
section review key concepts bayesian networks learning observations infer causality 
section describe bayesian networks applied model interactions genes discuss technical issues posed type data 
section apply approach gene expression data spellman 
analyzing statistical signi cance results biological plausibility 
section conclude discussion related approaches 

bayesian networks 
representing distributions bayesian networks consider nite set fx xn random variables variable xi may take value xi domain val xi 
capital letters variable names lowercase letters denote speci values taken variables 
sets variables denoted boldface capital letters assignments values variables sets bayesian networks fig 

example simple bayesian network structure 
network structure implies conditional independence statements 
network structure implies joint distribution product form bja cjb dja 
denoted boldface lowercase letters denote mean independent conditioned 
bayesian network representation joint probability distribution 
representation consists components 
rst component directed acyclic graph dag vertices correspond random variables xn 
second component describes conditional distribution variable parents components specify unique distribution xn 
graph represents conditional independence assumptions allow joint distribution decomposed number parameters 
graph encodes markov assumption variable xi independent parents applying chain rule probabilities properties conditional independencies joint distribution satis es decomposed product form xn ny xi pa xi set parents xi shows example graph lists markov independencies encodes product form imply 
graph speci es product form 
fully specify joint distribution need specify conditional probabilities product form 
second part bayesian network describes conditional distributions xi variable xi 
denote parameters specify distributions 
specifying conditional distributions choose representations 
focus commonly representations 
discussion suppose parents variable fu uk choice representation depends type variables dealing discrete variables 
uk takes discrete values nite set represent uk table speci es probability values joint assignment uk 
example variables binary valued table specify distributions 
general representation describe discrete conditional distribution 
loose expressiveness representation 
exibility comes price number free parameters exponential number parents 
continuous variables 
case discrete variables variable parents uk real valued representation represent possible densities 
natural choice multivariate continuous distributions gaussian distributions 
represented bayesian network linear gaussian conditional densities 
representation conditional density parents uk ai ui friedman normally distributed mean depends linearly values parents 
variance normal distribution independent parents values 
variables network linear gaussian conditional distributions joint distribution multivariate gaussian lauritzen wermuth 
hybrid networks 
network contains mixture discrete continuous variables need consider represent conditional distribution continuous variable discrete parents discrete variable continuous parents 
disallow case 
continuous variable discrete parents conditional gaussian distributions lauritzen wermuth joint assignment discrete parents represent linear gaussian distribution continuous parents 
bayesian network want answer types questions involve joint probability probability observation variables 
independencies domain independent observe 
literature contains suite algorithms answer queries ef ciently exploiting explicit representation structure jensen pearl 

equivalence classes bayesian networks bayesian network structure implies set independence assumptions addition 
ind set independence statements form independent hold distributions satisfying markov assumptions 
graph imply exactly set independencies 
example consider graphs variables graphs imply set independencies ind 
graphs equivalent ind ind 
graphs alternative ways describing set independencies 
notion equivalence crucial examine observations distribution distinguish equivalent graphs 
pearl verma show characterize equivalence classes graphs simple representation 
particular results establish equivalent graphs underlying undirected graph disagree direction arcs 
theorem pearl verma :10.1.1.51.7221
dags equivalent underlying undirected graph structures converging directed edges node edge 
equivalence class network structures uniquely represented partially directed graph pdag directed edge denotes members equivalence class contain arc undirected edge denotes members class contain arc contain arc dag pdag representation equivalence class constructed ef ciently chickering 

learning bayesian networks problem learning bayesian network stated follows 
training set fx independent instances nd network hg best matches precisely search equivalence class networks best matches theory learning networks data examined extensively decade 
brie describe high level details 
refer interested reader heckerman tutorial subject 
common approach problem introduce statistically motivated scoring function evaluates network respect training data search optimal network precise common assumptions learning networks distinguish equivalent graphs 
stronger assumptions example restricting form conditional probability distributions learn preference equivalent network 
bayesian networks score 
method deriving score bayesian considerations see cooper herskovits heckerman 
complete description 
score evaluate posterior probability graph data log log log constant independent marginal likelihood averages probability data possible parameter assignments particular choice priors determines exact bayesian score 
mild assumptions prior probabilities scoring rule asymptotically consistent suf ciently large number samples graph structures exactly capture dependencies distribution receive high probability higher score graphs see example friedman yakhini 
means suf ciently large number instances learning procedures pinpoint exact network structure correct equivalence class 
prior described heckerman geiger hybrid networks multinomial distributions conditional gaussian distributions 
prior combines earlier works priors multinomial networks buntine cooper herskovits heckerman gaussian networks geiger heckerman :10.1.1.156.9918
refer reader heckerman geiger heckerman details priors 
analysis gene expression data small number samples 
care taken choosing prior 
going details prior family priors described heckerman geiger speci ed parameters 
rst prior network re ects prior belief joint distribution variables domain second effective sample size parameter re ects strong belief prior network 
intuitively setting effective sample size equivalent having seen samples distribution de ned prior network 
experiments choose prior network random variables independent 
prior network discrete random variables uniformly distributed continuous random variables priori normal distribution 
set equivalent sample size arbitrary manner 
choice prior network ensure explicitly biasing learning procedure particular edge 
addition show results reasonably insensitive exact magnitude equivalent sample size 
assume complete data data set instance contains values variables network 
data complete prior satis es conditions speci ed heckerman geiger posterior score satis es properties 
score structure equivalent equivalent graphs guaranteed posterior score 
second score decomposable 
score rewritten sum xi pa xi contribution variable xi total network score depends values xi pa xi training instances 
local contributions variable computed closed form equation see heckerman geiger details 
prior speci ed data learning amounts nding structure maximizes score 
problem known np hard chickering 
resort heuristic search 
decomposition score crucial optimization problem 
local search procedure changes arc move ef ciently evaluate gains adding removing reversing single arc example procedure greedy hill climbing algorithm step performs local change results maximal gain reaches local maximum 
procedure necessarily nd global maximum perform practice 
examples search methods advance arc changes include beam search stochastic hill climbing simulated annealing 
friedman 
learning causal patterns recall bayesian network model dependencies multiple measurements 
interested modeling mechanism generated dependencies 
want model ow causality system interest gene transcription gene expression domain 
causal network model causal processes 
having causal interpretation facilitates predicting effect intervention domain setting value variable way manipulation affect variables 
rst glance direct connection probability distributions causality causal interpretations bayesian networks proposed pearl verma pearl :10.1.1.51.7221
causal network mathematically represented similarly bayesian network dag node represents random variable local probability model node 
causal networks stricter interpretation meaning edges parents variable immediate causes 
causal network models distribution observations effects interventions 
causes manipulating value affects value hand cause manipulating affect equivalent bayesian networks equivalent causal networks 
causal network interpreted bayesian network willing causal markov assumption values variable immediate causes independent earlier causes 
casual markov assumption holds causal network satis es markov independencies corresponding bayesian network 
example assumption natural models genetic pedigrees know genetic makeup individual parents genetic makeup ancestors informative genetic makeup 
central issue learn causal network observations 
issue received thorough treatment literature heckerman pearl verma spirtes spirtes :10.1.1.51.7221:10.1.1.48.9593
brie review relevant results needs 
detailed treatment topic refer reader pearl cooper glymour 
important distinguish observation passive measurement domain sample intervention setting values variables forces outside causal model gene knockout expression 
known interventions important tool inferring causality 
surprising causal relations inferred observations 
learn causality need assumptions 
rst assumption modeling assumption assume unknown causal structure domain satis es causal markov assumption 
assume causal networks provide reasonable model domain 
results literature require stronger version assumption causal networks provide perfect description domain independence property holds domain implied model 
second assumption latent hidden variables affect observable variables 
discuss relaxations assumption 
assumptions essentially assume possible dags domain variables true causal network 
discussed observations distinguish causal networks specify independence properties belong equivalence class see section 
best hope learn description equivalence class contains true model 
words learn pdag description equivalence class 
identify pdag uncertain true causal structure domain 
draw causal 
example directed path pdag causal ancestor networks generated pdag including true causal model 
theorem predict aspects proposed model detectable observations 
data sparse identify unique pdag model data 
situation posterior pdags represent posterior probabilities causal statements 
sense posterior probability causes sum posterior probability pdags statement holds 
see heckerman 
details bayesian approach 
situation somewhat complex combination observations results different bayesian networks interventions 
data able distinguish equivalent structures 
cooper yoo show extend bayesian approach heckerman 
learning mixed data 
possible pitfall learning causal structure presence latent variables 
situation observations depend probabilistically explained existence unobserved common cause 
consider variables distinguish hypothesis hypotheses causes causes 
careful analysis shows characterize networks latent variables result set independencies observed variables 
equivalence classes networks represented structure called partial ancestral graphs spirtes 
expected set causal allow latent variables smaller set causal allow 
causal relations recovered case 
situation complicated data identify single 
case pdags want compute posterior scores 
pdags question scoring pag consists models different number latent variables remains open question 

analyzing expression data section describe approach analyzing gene expression data bayesian network learning techniques 

outline proposed approach start modeling assumptions type expect nd 
aim understand particular system cell organism environment 
point time system state 
example state cell de ned terms concentration proteins various compartments amount external ligands bind receptors cell membrane concentration different mrna molecules cytoplasm cell biological systems consists interacting components affect consistent fashion 
consider random sampling system states probable 
likelihood state speci ed joint probability distribution cells components 
aim estimate probability distribution understand structural features 
course state system nitely complex 
resort partial view focus components 
measurements attributes components random variables represent aspect system state 
dealing mainly random variables denote mrna expression level speci genes 
consider random variables denote aspects system state phase system cell cycle process 
examples include measurements experimental conditions temporal indicators time stage sample taken background variables clinical procedure get sample exogenous cellular conditions 
aim model system joint distribution collection random variables describe system states 
model answer wide range queries system 
example expression level particular gene depend experimental condition 
dependence direct indirect 
indirect genes mediate dependency 
having model hand want learn available data answer questions system 
order learn model expression data need deal important issues arise learning gene expression domain 
involve statistical aspects interpreting results algorithmic complexity issues learning data choice local probability models 
dif culties learning expression data revolve central point contrary situations attempts learn models particular bayesian networks expression data involves transcript levels thousands genes current data sets contain dozen samples 
raises problems computational complexity statistical signi cance resulting networks 
positive side genetic regulation networks believed sparse friedman gene assumed dozen genes directly affect transcription 
bayesian networks especially suited learning sparse domains 

representing partial models learning models variables small data sets suf ciently informative signi cantly determine single model right 
different networks considered reasonable explanations data 
bayesian perspective say posterior probability models dominated single model equivalence class models 
potential approach deal problem nd networks receive high posterior score 
approach outlined madigan raftery 
unfortunately due combinatoric aspect networks set high posterior networks huge exponential number variables 
domain gene expression variables diffused posterior hope explicitly list networks plausible data 
solution follows 
attempt identify properties network interest 
example close neighbors network 
call properties features 
try estimate posterior probability features data 
precisely feature indicator function receives network structure argument returns structure associated pdag satis es feature 
posterior probability feature course exact computation posterior probability hard processing networks high posteriors 
shall see estimate posteriors nding representative networks 
feature binary attribute estimation fairly robust small set networks assuming unbiased sample posterior 
examine issue estimating posterior features brie discuss classes features involving pairs variables 
point handle pairwise features clear type analysis restricted planning examine complex features 
rst type feature markov relations markov blanket 
markov blanket minimal set variables shield rest variables model 
precisely markov blanket independent remaining variables network 
easy check relation symmetric markov blanket edge parents variable pearl 
context gene expression analysis markov relation indicates genes related joint biological interaction process 
note variables markov relation directly linked sense variable model mediates dependence 
remains possible unobserved variable protein activation intermediate interaction 
second type feature order relations ancestor networks equivalence class 
pdag contain path edges directed 
type feature involve close neighborhood captures global property 
recall assumptions discussed section learning ancestor imply cause assumptions quite strong particular assumption latent common causes necessarily hold context expression data 
view relation indication evidence causal ancestor 
estimating statistical con dence features face problem extent data support feature 
precisely want estimate posterior features de ned 
ideally sample networks posterior sampled networks estimate quantity 
unfortunately sampling posterior hard problem 
general approach problem build markov chain monte observation unique bayesian network models 
equally applies models learned gene expression data clustering models 
bayesian networks carlo mcmc sampling procedure see madigan york gilks general mcmc sampling 
clear methods scale large domains 
developments mcmc methods bayesian networks friedman koller show promise scaling choose alternative method poor man version bayesian analysis 
effective relatively simple approach estimating con dence bootstrap method efron tibshirani 
main idea bootstrap simple 
generate perturbed versions original data set learn 
way collect networks fairly reasonable models data 
networks re ect effect small perturbations data learning process 
context bootstrap follows construct data set di sampling replacement instances apply learning procedure di induce network structure gi 
feature interest calculate conf mx gi feature 
see friedman goldszmidt wyner details large scale simulation experiments method 
simulation experiments show features induced high con dence rarely false positives cases data sets small compared system learned 
bootstrap procedure appears especially robust markov order features described section 
addition simulation studies friedman koller show con dence values computed bootstrap equal bayesian posterior correlate estimates bayesian posterior features 

ef cient learning algorithms section formulated learning bayesian network structure optimization problem space directed acyclic graphs 
number graphs number variables 
consider hundreds variables deal extremely large search space 
need develop ef cient search algorithms 
facilitate ef cient learning need able focus attention search procedure relevant regions search space giving rise sparse candidate algorithm friedman nachman pe er 
main idea technique identify relatively small number candidate parents gene simple local statistics correlation 
restrict search networks candidate parents variable parents resulting smaller search space hope nd structure quickly 
possible pitfall approach early choices result overly restricted search space 
avoid problem devised iterative algorithm adapts candidate sets search 
iteration variable xi algorithm chooses set cn fy yk variables promising candidate parents xi 
search gn high scoring network pa gn xi cn 
ideally nd highest scoring network constraints heuristic search guarantee 
network guide selection better candidate sets iteration 
ensure score gn monotonically improves iteration requiring pa gn xi cn algorithm continues change candidate sets 
brie outline method choosing cn assign variable xj score relevance xi choosing variables highest score 
question measure relevance potential parent xj xi 
friedman 
examine measures relevance 
experiments successful measures simply improvement score xi add xj additional parent 
precisely calculate xi pa xi fxj xi pa xi friedman quantity measures inclusion edge xj xi improve score associated xi 
choose new candidate set contain previous parent set pa xi variables informative set parents 
refer reader friedman nachman pe er details algorithm complexity empirical results comparing performance traditional search techniques 

local probability models order specify bayesian network model need choose type local probability models learn 
current consider approaches multinomial model 
model treat variable discrete learn multinomial distribution describes probability possible state child variable state parents 
linear gaussian model 
model learn linear regression model child variable parents 
models chosen posterior ef ciently calculated closed form 
apply multinomial model need discretize gene expression values 
choose discretize values categories normal depending expression rate signi cantly lower similar greater control respectively 
control expression level gene determined experimentally methods derisi 

set average expression level gene experiments 
discretize setting threshold ratio measured expression control 
experiments choose threshold value logarithmic base scale 
values ratio control lower considered values higher considered 
models bene ts drawbacks 
hand clear discretizing measured expression levels loosing information 
linear gaussian model suffer information loss caused discretization 
hand linear gaussian model detect dependencies close linear 
particular discover combinatorial effects gene expressed genes jointly expressed expressed 
multinomial model exible capture dependencies 

application cell cycle expression patterns applied approach data spellman 

data set contains gene expression measurements mrna levels cerevisiae orfs 
experiments measure time series different cell cycle synchronization methods 
spellman 
identi ed genes expression varied different cell cycle stages 
learning data treat measurement independent sample distribution take account temporal aspect measurement 
clear cell cycle process temporal nature compensate introducing additional variable denoting cell cycle phase 
variable forced root networks learned 
presence allows model dependency expression levels current cell cycle phase 
sparse candidate algorithm fold bootstrap learning process 
performed experiments discrete multinomial distribution linear gaussian distribution 
learned features show recover intricate structure small data sets 
important note learning algorithm uses prior biological knowledge constraints 
learned networks relations solely information conveyed measurements 
results available www site www cs huji ac il labs expression 
illustrates graphical display results analysis 
note learn temporal models bayesian network includes gene expression values consecutive time points friedman :10.1.1.129.4770
raises number variables model 
currently issue 
bayesian networks fig 

example graphical display markov features 
graph shows local map gene svs 
width color edges corresponds computed con dence level 
edge directed suf ciently high con dence order genes connected edge 
local map shows cln separates svs genes 
strong connection cln genes edges connecting 
indicates high con dence genes conditionally independent expression level cln 

robustness analysis performed number tests analyze statistical signi cance robustness procedure 
tests carried smaller data set genes computational reasons 
test credibility con dence assessment created random data set randomly permuting order experiments independently gene 
gene order random composition series remained unchanged 
data set genes independent expect nd real features 
expected order markov relations random data set signi cantly lower con dence 
compare distribution con dence estimates original data set randomized set 
clearly distribution con dence estimates original data set longer heavier tail high con dence region 
linear gaussian model see random data generate feature con dence 
multinomial model expressive susceptible tting 
model see smaller gap distributions 
randomized data generate feature con dence leads believe features learned original data set con dence artifacts bootstrap estimation 
markov order friedman multinomial features confidence linear gaussian random real random real features confidence features confidence fig 

plots abundance features different con dence levels cell cycle data set solid line randomized data set dotted line 
axis denotes con dence threshold axis denotes number features con dence equal higher corresponding value 
graphs left column show markov features ones right column show order features 
top row describes features multinomial model bottom row describes features linear gaussian model 
inset graph plot tail distribution 
bayesian networks order relations markov relations fig 

comparison con dence levels obtained data sets differing number genes multinomial experiment 
relation shown point coordinate con dence genes data set coordinate con dence genes data set 
left gure shows order relation features right gure shows markov relation features 
test robustness procedure extracting dominant genes performed simulation study 
created data set sampling samples networks learned original data 
applied bootstrap procedure data set 
counted number descendents node synthetic network ordered variables count 
top dominant genes learned bootstrap experiment top real ordering 
analysis performed cerevisiae genome tested robustness analysis addition genes comparing con dence learned features gene data set smaller gene data set contains genes appearing major clusters described spellman 

compares feature con dence analysis data sets multinomial model 
see strong correlation con dence levels features data sets 
comparison linear gaussian model gives similar results 
crucial choice multinomial experiment threshold level discretization expression levels 
clear setting different threshold get different discrete expression patterns 
important test robustness sensitivity high con dence features choice threshold 
tested repeating experiments different thresholds 
comparison change threshold affects con dence features show de nite linear tendency con dence estimates features different discretization thresholds graphs shown 
obviously linear correlation gets weaker larger threshold differences 
note order relations robust changes threshold markov relations 
valid criticism discretization method penalizes genes natural range variation small xed threshold detect changes genes 
possible way avoid problem normalize expression genes data 
rescale expression level gene relative expression level mean variance genes 
note analysis methods pearson correlation compare genes ben dor 
eisen 
implicitly perform normalization 
discretize normalized data set essentially rescaling discretization factor differently gene depending variance data 
tried approach discretization levels got results comparable undesired effect normalization ampli cation measurement noise 
gene xed expression levels samples expect variance measured expression levels noise experimental conditions measurements 
normalize expression levels genes lose distinction noise true signi cant changes expression levels 
spellman data set safely assume effect grave focus genes display signi cant changes experiments 
friedman order relations markov relations fig 

comparison con dence levels multinomial experiment linear gaussian experiment 
relation shown point coordinate con dence multinomial experiment coordinate con dence linear gaussian experiment 
left gure shows order relation features right gure shows markov relation features 
original discretization method 
top markov relations highlighted method bit different interesting biologically sensible right 
order relations robust change methods discretization thresholds 
possible reason order relations depend network structure global manner remain intact local changes structure 
markov relation local easily disrupted 
graphs learned extremely sparse discretization method highlights different signals data re ected markov relations learned 
similar picture arises compare results multinomial experiment linear gaussian experiment 
case virtually correlation markov relations methods order relations show correlation 
supports assumption methods highlight different types connections genes 
consider effect choice prior learned features 
important ensure learned features simply artifacts chosen prior 
test repeated multinomial experiment different values effective sample size compared learned con dence levels learned default value 
done gene data set discretization level 
results comparisons shown 
seen con dence levels obtained value correlate obtained default setting correlation weaker 
suggests low values compared data set size making prior affect results weak 
effective sample size high prior effect noticeable 
aspect prior prior network 
experiments reported empty network uniform distribution parameters prior network 
prior noninformative keeping effect desired 
expected informative priors incorporating biological knowledge example stronger effective sample sizes obtained results biased prior beliefs 
summary results report especially order relations stable different experiments discussed previous paragraph clear analysis sensitive choice local model case multinomial model discretization method 
probably sensitive choice prior long effective sample size small compared data set size 
methods tried analysis interesting relationships data 
challenge nd alternative methods recover relationships analysis 
currently working learning networks semi parametric density models friedman nachman hoffman tresp circumvent need discretization hand allow nonlinear dependency relations 
bayesian networks order relations markov relations fig 

comparison con dence levels runs different parameter priors 
difference priors effective sample size relation shown point coordinate con dence run coordinate con dence run top row bottom row 
left gure shows order relation features right gure shows markov relation features 
runs gene set discretization threshold level 

biological analysis believe results analysis indicative biological phenomena data 
con rmed ability predict sensible relations genes known function 
examine consequences learned data 
consider turn order relations markov relations analysis 

order relations 
striking feature high con dence order relations existence dominant genes 
genes dominate order appear genes 
intuition genes indicative potential causal sources cell cycle process 
denote con dence ancestor de ne dominance score constant rewarding high con dence features threshold discard low con dence ones 
dominant genes extremely robust parameter selection discretization cutoff section local probability model 
list highest scoring dominating genes experiments appears table 
friedman table 
list dominant genes ordering relations score experiment gene orf multinomial gaussian notes mcd mitotic chromosome determinant null mutant required mismatch repair mitosis csi cell wall maintenance synthesis cln role cell cycle start null mutant exhibits arrest contains associated domain possibly nuclear rfa involved nucleotide excision repair null mutant rsr gtp binding protein ras family involved bud site selection cdc required initiation chromosomal replication null mutant rad cell cycle control checkpoint function null mutant cdc cell cycle control required exit mitosis null mutant pol required dna replication repair null mutant protein involved cellular polarization cln role cell cycle start null mutant exhibits arrest included top dominant genes experiment 
inspection list dominant genes reveals quite interesting features 
genes directly involved initiation cell cycle control 
example cln cln cdc rad functional relation established 
genes mcd rfa cdc rad cdc pol essential 
clearly key genes essential cell functions 
components complexes cdc pol 
rfa pol involved dna repair 
known dna repair associated transcription initiation dna areas active transcription repaired frequently mcgregor 
furthermore cell cycle control mechanism causes abort dna improperly replicated eisen lucchesi 
dominant genes encode nuclear proteins unknown genes potentially nuclear contains associated domain entirely nuclear proteins 
dominant genes localized cytoplasm membrane rsr 
involved processes important role cell cycle 
rsr belongs ras family proteins known initiators signal transduction cascades cell 

markov relations 
analysis markov relations multinomial experiment 
inspection top markov relations reveals functionally related 
list top scoring relations table 
involving known genes sense biologically 
orfs unknown careful searches psi blast altschul reveal rm homologies proteins functionally related gene pair :10.1.1.17.9507
example paired cts related egt cell wall maintenance protein 
unknown pairs physically adjacent chromosome presumably regulated mechanism see blumenthal special care taken pairs chromosomal location overlap complementary strands cases see artifact resulting cross hybridization 
analysis raises number biologically sensible pairs nineteen top relations 
interesting markov relations limitations clustering techniques 
high con dence markov relations nd examples conditional independence group highly correlated genes correlation explained network structure 
example involves genes cln rnr svs rad 
expression correlated spellman 
appear cluster 
network cln high con dence parent genes links see 
agrees biological knowledge cln central early cell cycle control bayesian networks table 
list top markov relations multinomial experiment con dence gene gene notes pir pir close locality chromosome close locality chromosome mcd bind dna mitosis pho pho nearly identical acid hht htb htb hta close locality chromosome cts homolog egt cell wall control involved yor yor close locality chromosome sic homolog mammalian nuclear ran protein involved nuclear function far ash part mating type switch expression uncorrelated cln svs function svs unknown nce homolog proteins suggest involved protein ste mfa mating factor receptor hhf hhf met ecm sul te cdc rad participate fragment processing clear biological relationship 
markov relations intercluster pairing genes low correlation expression 
regulatory link far ash proteins known participate mating type switch 
correlation expression patterns low spellman 
cluster different clusters 
looking list pairs markov relation con dence high relative correlation interesting pairs surface example sag mf alpha match factor induces mating process essential protein participates mating process 
match lac 
lac transport protein modi ed sequence homology 
markov relations gaussian experiment summarized table 
gaussian model focuses highly correlated genes high scoring genes tightly correlated 
checked dna sequence pairs physically adjacent genes top table signi cant overlap 
suggests correlations spurious due cross hybridization 
ignore relations highest score 
spite technical problem pairs con dence discarded biologically false 
relations robust appear multinomial experiment ste mfa cst 
interesting genes linked regulation 
include shm converts regulates gcv dip transports regulates aro 
pairs participate metabolic process cts participate cell wall regulation 
interesting high con dence examples ole faa linked fatty acid metabolism ste aga linked mating process kip msb playing role polarity establishment 

discussion new approach analyzing gene expression data builds theory algorithms learning bayesian networks 
described apply techniques gene expression data 
approach builds techniques motivated challenges posed domain novel search algorithm friedman nachman pe er approach estimating statistical con dence friedman goldszmidt wyner :10.1.1.33.1758
applied methods real expression data spellman 

prior knowledge managed extract biologically plausible analysis 
friedman table 
list top markov relations gaussian experiment con dence gene gene notes yor yor close locality chromosome cdc yor yor totally unknown cdc sph suggestion immediate link shm gcv shm gcv regulated met ecm met required convert sul de ecm sul te cdc close locality chromosome close locality chromosome rsr close locality chromosome ste mfa mating factor receptor homologs proteins close locality chromosome hta hta physically linked hhf hht cts homolog egt cell wall control involved aro dip dip transports regulates aro proteins involved cell wall regulation plasma membrane table skips additional pairs close locality 
approach quite different clustering approach alon 
ben dor 
eisen 

spellman 
attempts learn richer structure data 
methods capable discovering causal relationships interactions genes positive correlation ner intracluster structure 
currently developing hybrid approaches combine methods clustering algorithms learn models clustered genes 
biological motivation approach similar inducing genetic networks data chen somogyi weaver 
key differences models learn probabilistic semantics 
better ts stochastic nature biological processes noisy experiments 
second focus extracting features pronounced data contrast current genetic network approaches attempt nd single model explains data 
emphasize described represents preliminary step longer term project 
seen points require accurate statistical tools ef cient algorithms 
exact biological draw analysis understood 
view results described section de nitely encouraging 
currently working improving methods expression analysis expanding framework described 
promising directions extensions developing theory learning local probability models suitable type interactions appear expression data improving theory algorithms estimating con dence levels incorporating biological knowledge possible regulatory regions prior knowledge analysis improving search heuristics learning temporal models dynamic bayesian networks friedman temporal expression data developing methods discover hidden variables protein activation :10.1.1.129.4770
exciting longer term prospects line research discovering causal patterns gene expression data 
plan build extend theory learning causal relations data apply gene expression 
theory causal networks allows learning observational interventional data experiment intervenes causal mechanisms observed system 
gene expression context model knockout expressed mutants interventions 
design methods deal mixed forms data principled manner see cooper yoo direction 
addition theory provide bayesian networks tools experimental design understanding interventions deemed informative determining causal structure underlying system 
acknowledgments authors grateful gill david moises goldszmidt daphne koller pe er gavin sherlock anonymous reviewer comments drafts useful discussions relating 
help running analyzing robustness experiments 
supported part generosity michael trust israeli science foundation 
nir friedman supported alon fellowship 
maruyama 
identi cation gene regulatory networks strategic gene disruptions gene expressions 
proc 
ninth annual acm siam symposium discrete algorithms acm siam 
alon gish mack levine 
broad patterns gene expression revealed clustering analysis tumor normal colon tissues probed 
proc 
nat 
acad 
sci 
usa 
altschul thomas schaffer zhang zhang miller lipman 
gapped blast psi blast new generation protein database search programs 
nucl 
acids res 

ben dor shamir yakhini 
clustering gene expression patterns 
comp 
biol 

blumenthal 
gene clusters transcription eukaryotes 

buntine 
theory re nement bayesian networks 
proc 
seventh annual conference uncertainty ai uai 
chen skiena 
identifying gene regulatory networks experimental data 
proc 
third annual international conference computational molecular biology recomb 
chickering 
transformational characterization equivalent bayesian network structures 
proc 
eleventh conference uncertainty arti cial intelligence uai 
chickering 
learning bayesian networks np complete fisher 
lenz eds 
learning data arti cial intelligence statistics springer verlag new york 
cooper herskovits 
bayesian method induction probabilistic networks data 
machine learning 
cooper glymour eds 

computation causation discovery mit press cambridge ma 
cooper yoo 
causal discovery mixture experimental observational data 
proc 
fifteenth conference uncertainty arti cial intelligence uai 

yeast cln cln gap protein role bud formation 

derisi iyer brown 
exploring metabolic genetic control gene expression genomic scale 
science 
johnston friesen singer 
impaired rna polymerase ii activity saccharomyces cerevisiae causes cell cycle inhibition start 
mol 
gen genet 

efron tibshirani 
bootstrap chapman hall london 
eisen lucchesi 
unraveling role transcription 

eisen spellman brown botstein 
cluster analysis display genome wide expression patterns 
proc 
nat 
acad 
sci 
usa 
friedman goldszmidt wyner 
data analysis bayesian networks bootstrap approach 
proc 
fifteenth conference uncertainty arti cial intelligence uai 
friedman koller 
bayesian network structure 
proc 
sixteenth conference uncertainty arti cial intelligence uai 
friedman murphy russell 
learning structure dynamic probabilistic networks 
proc 
fourteenth conference uncertainty arti cial intelligence uai 
friedman nachman 
gaussian process networks 
proc 
sixteenth conference uncertainty arti cial intelligence uai 
friedman nachman pe er 
learning bayesian network structure massive datasets sparse candidate algorithm 
proc 
fifteenth conference uncertainty arti cial intelligence uai 
friedman friedman yakhini 
sample complexity learning bayesian networks 
proc 
twelfth conference uncertainty arti cial intelligence uai 
geiger heckerman 
learning gaussian networks 
proc 
tenth conference uncertainty arti cial intelligence uai 
gilks richardson spiegelhalter 
markov chain monte carlo methods practice crc press 

direct link sister cohesion chromosome condensation revealed analysis mcd cerevisiae 
cell 
heckerman 
tutorial learning bayesian networks jordan ed learning graphical models kluwer dordrecht netherlands 
heckerman geiger 
learning bayesian networks uni cation discrete gaussian domains 
proc 
eleventh conference uncertainty arti cial intelligence uai 
heckerman geiger chickering 
learning bayesian networks combination knowledge statistical data 
machine learning 
heckerman meek cooper 
bayesian approach causal discovery cooper glymour 
hoffman tresp 
discovering structure continuous variables bayesian networks advances neural information processing systems nips mit press cambridge ma 
iyer eisen ross schuler moore lee trent staudt hudson boguski lashkari botstein brown 
transcriptional program response human serum 
science 
jensen 
bayesian networks university college london press london 
lauritzen wermuth 
graphical models associations variables qualitative quantitative 
annals statistics 
lockhart dong byrne gallo chee want kobayashi horton brown 
dna expression monitoring hybridization high density oligonucleotide arrays 
nature biotechnology 
madigan raftery 
model selection accounting model uncertainty graphical models occam window 
am 
stat 
assoc 

madigan york 
bayesian graphical models discrete data 
inter 
stat 
rev 
mcgregor 
dna repair dna replication uv mutagenesis 


symp 
proc 

carr fuhrman wen somogyi 
cluster analysis data visualization large scale gene expression data 
pac 
symp 
biocomputing 
pearl 
probabilistic reasoning intelligent systems morgan kaufmann san francisco 
pearl 
causality models reasoning inference cambridge university press london 
pearl verma 
theory inferred causation principles knowledge representation reasoning proc 
second international conference kr 
somogyi fuhrman wuensche 
gene expression matrix extraction genetic network architectures second world congress nonlinear analysts 
eddy bateman durbin 
multiple sequence alignments hmm pro les protein domains 
nucl 
acids res 

spellman sherlock zhang iyer anders eisen brown botstein 
comprehensive identi cation cell cycle regulated genes yeast saccharomyces cerevisiae microarray hybridization 
mol 
biol 
cell 
spirtes glymour scheines 
causation prediction search springer verlag new york 
spirtes meek richardson 
algorithm causal inference presence latent variables selection bias cooper glymour 

effect dna lesions transcription elongation 

weaver stormo 
modeling regulatory networks weight matrices pac 
symp 
biocomputing 
wen carr smith barker somogyi 
large scale temporal gene expression mapping central nervous system development 
proc 
nat 
acad 
sci 
usa 
linial linial 
automated classi cation protein sequences hierarchy protein families local maps protein space 
proteins structure function genetics 
address correspondence nir friedman school computer science engineering hebrew university jerusalem israel mail nir cs huji ac 
