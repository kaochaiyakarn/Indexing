gentle tutorial em algorithm application parameter estimation gaussian mixture hidden markov models jeff bilmes bilmes cs berkeley edu international computer science institute berkeley ca computer science division department electrical engineering computer science berkeley tr april describe maximum likelihood parameter estimation problem expectation maximization em algorithm solution 
describe form em algorithm literature 
develop em parameter estimation procedure applications finding parameters mixture gaussian densities finding parameters hidden markov model hmm baum welch algorithm discrete gaussian mixture observation models 
derive update equations fairly explicit detail prove convergence properties 
try emphasize intuition mathematical rigor 
ii maximum likelihood recall definition maximum likelihood estimation problem 
density function governed set parameters set gaussians means covariances 
data set size supposedly drawn distribution assume data vectors independent identically distributed distribution resulting density samples function called likelihood parameters data just likelihood function 
likelihood thought function parameters data fixed 
maximum likelihood problem goal find find maximizes wish argmax maximize analytically easier 
depending form problem easy hard 
example simply single gaussian distribution set derivative zero solve directly mean variance data set 
problems possible find fact results standard formulas analytical expressions resort elaborate techniques 
basic em em algorithm elaborate technique 
em algorithm rw gj jj bis wu general method finding maximum likelihood estimate parameters underlying distribution data set data incomplete missing values 
main applications em algorithm 
occurs data missing values due problems limitations observation process 
second occurs optimizing likelihood function analytically intractable likelihood function simplified assuming existence values additional missing hidden parameters 
application common computational pattern recognition community 
assume data observed generated distribution 
call incomplete data 
assume complete data set exists assume specify joint density function joint density come 
arises marginal density function assumption hidden variables parameter value guesses exam ples mixture densities baum welch 
cases missing data values samples distribution assume joint relationship missing observed values 
new density function define new likelihood function called complete data likelihood 
note function fact random variable missing information unknown random presumably governed underlying distribution 
think constant random variable 
original likelihood referred incomplete data likelihood function 
em algorithm finds expected value complete data log likelihood respect unknown data observed data current parameter estimates 
define function current parameters estimates evaluate expectation new parameters optimize increase expression probably requires explanation 
key thing understand constants normal variable wish adjust random variable governed distribution right side equation re written note marginal distribution unobserved data dependent observed data current parameters space values take 
best cases marginal distribution simple analytical expression assumed parameters data 
worst cases density hard obtain 
fact density dependent analogy suppose function variables 
consider doesn effect subsequent steps extra factor constant random variable governed distribution deterministic function maximized desired 
evaluation expectation called step algorithm 
notice meaning arguments function argument corresponds parameters ultimately optimized attempt maximize likelihood 
second argument corresponds parameters evaluate expectation 
second step step em algorithm maximize expectation computed step 
find argmax steps repeated necessary 
iteration guaranteed increase loglikelihood algorithm guaranteed converge local maximum likelihood function 
rate convergence papers rw wu jx xj discuss 
recall discussion drop subscripts different density functions argument usage disambiguate different ones 
modified form step maximizing form algorithm called generalized em gem guaranteed converge 
clear exactly code algorithm 
way algorithm general form 
details steps required compute quantities dependent particular application discussed algorithm form 
find finding maximum likelihood mixture densities parameters em mixture density parameter estimation problem probably widely applications em algorithm computational pattern recognition community 
case assume probabilistic model parameters density function parameterized words assume component densities mixed coefficients mixing incomplete data log likelihood expression density data posit existence unobserved data items difficult optimize contains log sum 
consider incomplete values inform component density generated data item likelihood expression significantly simplified 
assume sample generated mixture component 
know values likelihood particular form component densities optimized variety techniques 
problem course know values assume random vector proceed 
derive expression distribution unobserved data 
guess parameters mixture density guess appropriate parameters likelihood easily compute 
addition parameters mixing prior probabilities mixture component component bayes rule compute instance unobserved data independently drawn 
look equation see case obtained desired marginal density assuming existence hidden variables making guess initial parameters distribution 
case equation takes form form looks fairly daunting greatly simplified 
note 
equation write equation maximize expression maximize term containing term containing independently related 
find expression introduce lagrange multiplier constraint solve equation summing sizes get resulting distributions possible get analytical expressions functions 
example assume dimensional gaussian component distributions mean covariance matrix derive update equations distribution need recall results matrix algebra 
trace square matrix tr equal sum diagonal elements 
trace scalar equals scalar 
tr tr tr tr tr implies tr note indicates determinant matrix ll need take derivatives function matrix respect elements matrix 
define matrix entry entry definition applies derivatives respect vector 

second shown symmetric matrix cofactor see definition inverse matrix 
shown tr diag diag log equation ignoring constant terms disappear derivatives substituting right side equation get derivative equation respect setting equal zero get easily solve obtain find note write equation derivative respect get tr diag diag tr diag diag implies gives diag setting derivative zero summarizing estimates new parameters terms old parameters follows note equations perform expectation step maximization step simultaneously 
algorithm proceeds newly derived parameters guess iteration 
learning parameters hmm em baum welch algorithm hidden markov model probabilistic model joint probability collection random variables variables continuous discrete observa tions variables hidden discrete 
hmm conditional independence assumptions random variables associated algorithms tractable 
independence assumptions hidden variable hidden variable independent previous variables observation hidden variable independent variables represent section derive em algorithm finding maximum likelihood estimate parameters hidden markov model set observed feature vectors 
algorithm known baum welch algorithm 
discrete random variable possible values assume underlying hidden markov chain defined time homogeneous independent time 
time independent stochastic transition matrix special case time described initial state distribution state say time 
particular sequence states described state time particular observation sequence described state probability particular observation vector particular time described butions represented complete collection parameters observation distri forms output distributions consider 
discrete observation assumption assume observation possible observation symbols case second form probably distribution consider mixture multivariate gaussians state describe complete set hmm parameters model basic problems associated hmms 
find forward backward procedure efficient direct evaluation 

find best state sequence explains viterbi algorithm solves problem won discuss 

find argmax baum welch called forward backward em hmms algorithm solves problem develop presently 
subsequent sections consider third problems 
second addressed rj 
efficient calculation desired quantities advantages hmms relatively efficient algorithms derived problems mentioned 
derive em algorithm directly function review efficient procedures 
recall forward procedure 
define probability seeing partial sequence state time efficiently recursively define 
backward procedure similar probability partial sequence time efficiently define 


started state define probability state time state sequence note note markovian conditional independence define things terms define probability state time state time 
expanded sum quantities time get useful values 
expression expected number times state expected number transitions away state similarly expected number transitions state state follow fact indicator random variable state time random variable move state time state jumping gun bit goal forming em algorithm estimate new parameters hmm old parameters data 
intuitively simply relative frequencies 
define update rules follows quantity expected relative frequency spent state time 
quantity expected number transitions state relative expected total number transitions away state state 
discrete distributions quantity expected number times output observations equal state relative expected total number times state 
gaussian mixtures define probability component mixture generated observation random variable indicating mixture component time state 
previous section gaussian mixtures guess update equations case observation sequences come length update equations relatively intuitive equations fact em algorithm welch hmm parameter estimation 
derive typical em notation section 
estimation formula function 
consider observed data underlying state sequence hidden unobserved 
incomplete data likelihood function complete data likelihood function function initial guessed previous estimates parameters space state sequences length particular state sequence representing quite easy function parameters wish optimize independently split terms sum optimize term individually 
term equation selecting simply repeatedly selecting values right hand side just marginal expression time adding lagrange multiplier constraint setting derivative equal zero get remainder discussion primed parameters assumed initial guessed previous param eters unprimed parameters optimized 
note assume initial distribution starts notational convenience 
basic results 
derivative summing get solving get second term equation term time looking transitions weighting corresponding probability right hand side just sum joint marginal time similar way lagrange multiplier constraint get third term equation term time looking emissions states weighting possible emission corresponding probability right hand side just sum marginal time discrete distributions lagrange multiplier constraint time observations equal contribute proba bility value get gaussian mixtures form function slightly different hidden variables include hidden state sequence variable indicating mixture component state time 
write vector indicates mixture component state time 
expand equation second terms unchanged parameters independent third term equation marginalized away sum 
equation identical equation addition sum component hidden state variables 
optimize exactly analogous way section get seen set update equations previous section 
update equations hmms multiple observation sequences similarly derived addressed rj 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
royal statist 
soc 
ser 

bis bishop 
neural networks pattern recognition 
clarendon press oxford 
gj jordan 
learning incomplete data 
technical report ai lab memo cbcl mit ai lab august 
jj jordan jacobs 
hierarchical mixtures experts em algorithm 
neural computation 
jx xu 
convergence results em approach mixtures experts architectures 
neural networks 
rj rabiner 
juang 
fundamentals speech recognition 
prentice hall signal processing series 
rw redner walker 
mixture densities maximum likelihood em algorithm 
siam review 
wu wu 
convergence properties em algorithm 
annals statistics 
xj xu jordan 
convergence properties em algorithm gaussian mixtures 
neural computation 

