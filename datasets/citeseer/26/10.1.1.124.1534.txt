tractable learning large bayes net structures sparse data anna cmu edu center automated learning discovery cmu forbes ave pittsburgh pa usa andrew moore robotics institute cmu forbes ave pittsburgh pa usa cs cmu edu statistics creating global bayes net 
addresses questions 
useful attempt learn bayesian network structure hundreds thousands nodes 
structure search proceed practically 
third question arises approach second frequent sets agrawal extremely popular area descriptive data mining turned probabilistic model 
large sparse datasets hundreds thousands records attributes appear social networks warehousing supermarket transactions web logs 
complexity structural search learning factored probabilistic models datasets unfeasible 
propose frequent sets significantly speed structural search 
previous approaches cache way sufficient statistics exploit local structure 
empirical evaluation algorithm applied massive datasets 
keywords bayesian networks graphical models statistical learning bayes net structure learning 
bayesian networks successfully applied areas pharmaceutical research decision making doctors air control marketing 
structural learning bayesian networks usually desirable costly 
provides algorithm tractable structural learning bayes nets exploring structures local level 
exploit computational efficiency frequent sets gathering statistics useful structure search assumption sparse data 
give efficient search algorithm exploit appearing proceedings international conference machine learning banff canada 
copyright authors 

learn large bayes nets 
usage bayesian networks represent expression genes activity regulators practice approximated protein activity levels motivated friedman 
suggests structure network important may provide information gene regulators 
field received increasing attention years recommender systems 
online systems amazon provide suggestions appeal user user preferences 
bayesian networks domain established 
breese 
goal recommender systems predict items user buy 
example answering analogous query bayes nets built algorithm section 
idea representing social networks people connected directed arrows explored social science domain years moreno jennings 
initially analyzed networks order nodes 
improvements data collection especially birth online communities necessary look larger networks 
example livejournal online blog community contains users personal communication 
usage graphical models domain increasingly popular due robustness noise 
studies gene expression data social networks particular suggest correlations entities local level important fact global network friedman 
computationally practical bayesian networks created algorithm natural motivation stemming important domains 
provide results sparse massive datasets showing practical training times cases superior ability model joint distribution comparison di rect extensions traditional structure search algorithms large data 
qualitatively empirically show sparse data particularly data social net characteristics modelled better going information derived pairwise occurrences 

frequent sets assume training data collection records binary categorical attributes record 
write value th attribute th record 
assume sparse data vast majority values dataset row column zero 
note assume missing values observed 
attributes represented integers 
occurrence frequency set attributes number records attributes simultaneously set 
say frequent set attributes contains exactly attributes 
threshold called support data mining literature 
sparse data support greater surprisingly easy compute frequent sets agrawal srikant 
abundance literature frequent sets collection essential part association rules algorithms agrawal agrawal srikant han kamber widely commercial data mining 
multiple frequent sets area modelling sparse datasets mannila toivonen chickering heckerman pavlov 
surprising sparseness implies occurrences items 
fact items occur expect majority counts pairwise marginals natural assume frequent sets contain essential information dataset 
chickering heckerman propose show efficient sparse representation classes machine learning algorithms including structure initialization bayes nets 
focus representational aspects frequent sets 
exploits previous research utilization frequent sets modelling sparse datasets takes new perspective 
assuming frequent sets comprise essential information data propose exploit find bayes net structures local level 
knowledge structures contained frequent sets previously order improve global model data 

algorithm description simplest idea exploiting frequent set information frequent pairs 
edges consider including bayes net source destination attributes occur support 
far fewer edges consider full possibilities number attributes 
problems idea 
problem 
method find edges negative correlations 
example attribute positive attribute positive frequent pair considered 
solution 
problem mitigated ways 
assumption sparse data necessarily evidence strong negative correlation shown appendix 
fact structure scoring metric bdeu higher positively correlated entities 
secondly attributes high marginal positive values negative correlation significant accounted stage described section 
problem 
items occur independent 
especially promiscuous attributes occur frequently occur just chance 
solution 
solution problem screen frequent pairs allowing links pool edges considered network 
significantly correlated pairs candidate edges 
greatly reduces number candidate edges 
problem 
restricting search frequent pairs significant higher order interactions 
appendix gives example easy imagine cases occurence predictive occurence dependencies statistically significant 
solution 
solved higher order frequent sets described paragraphs 
screening frequent sets 
call set edges eventually considered addition bayesian network 
suppose collection frequent sets 
screen pairs find positive pairwise correlations 
add edge variables significant correlation vari ables pair 
turn screen dependencies frequent sets size frequent set size provide new information valuable building bayes net 
possible dependencies explained interactions order 
example suppose attributes occur frequently occurence explained local bayesian network dag structure 
case way interactions explain dependencies 
case added 
fact dags contain node parents missed considering lower order interactions 
implement screening test searching possible dag structures finding best bdeu scoring structure see section parent node call way interaction 
allow pass screening test best explained local dag structure containing way interaction 
passes screening test edges highest scoring dag added 
created prioritize edges strength measured number way interactions participate 
create empty global bayesian network iterate contents allowing edge turn added improves bdeu avoids cycles 
table contains full description algorithm 

addition high mutual information links previous section pointed frequent sets bias algorithm favor interactions cause cooccurrence positive correlation 
appendix shows case sparse data positive correlations stronger negative correlations general omitting strongest correlations 
danger attributes promiscuous relatively high univariate marginal probability sparse cause significant negative correlations fortunately negative pairwise correlations detected cheaply technique meila 
mutual information attributes 
meila showed mutual information calculated efficient manner particularly dealing discrete binary data 
fact variables occurred dataset formula simplifies table 
screen bayes net structure search algorithm algorithm input max frequent set size support output bayes net collection directed edges represented source dest count dag storage 

obtain counts frequent sets size 
foreach frequent set 
find best scoring dag 
dag contains node parents 
store dag ds 
foreach 

foreach dag ds 
store edges 
order decreasing order edge counts 
foreach edge 
doesn form cycle 
improves bdeu 
add 
foreach 
return directly proportional magnitude shown equation 
full derivation available meila 
add high mutual information edges check entities occur high frequency 
reduce total number entities significantly considering ones occurred times dataset 
step statistically justified fewer occurrences mean lower possible mutual information 
table describes algorithm augments bayes net high mutual information mi edges 
table 
algorithm augments bayes net high mi edges algorithm input bayes net list attributes frequencies 
sort decreasing order frequencies 



net score old net score 
add 
try add 

edge added 


return note search space edges find edges highest mutual information 
sort entities descending order frequency 
entity number entities support consider entities occurred frequently 
edge rejected move list 
step justified entities sorted descending order frequencies mutual information lower 
edge added empirical evidence shows average pairs considered 

additional possible postprocessing 
second degree separation links cheap extra pass edge additions iterate nodes network produced previous steps attempt adding edges directly current node grandchildren 

hillclimbing standard techniques improve score hillclimbing described cooper herskovits 
technique improves score adding removing reversing arcs bayes net 
set operations edge selection procedure may differ algorithms 
usually hillclimbing performed way step existing model undergoes modification addition single edge 
order pick best edge look possibilities 
number nodes prohibits perform linear search step random hillclimbing step choose edges randomly 
specifically roll sided die probabilities addition deletion arc reversal pick edge random see performing chosen operation improves global score 

evaluation evaluation uses bdeu score described heckerman equation compare results different configurations algorithm randomized hillclimbing described section 
th variable states th parent true false case binary variables states 
datasets listed table 
holdout evaluate overfitting discussed section 

datasets algorithm tested real life datasets sizes shown table 
table 
data sets sizes datasets entities records institute drinks imdb citeseer 
institute data set records collaborations professors students collected publicly available web pages listed carnegie mellon university robotic institute web site 

drinks data set consists set records entity ingredient popular recipe internet 

imdb data set collection casts actors participated movies years extracted internet movie database 
citeseer data set publication records citeseer online library index computer science publications 
entities represented initial name single name correspond people 
non datasets available web publication 
empirical results tested algorithm variety configurations datasets listed table 
results table reported terms average bdeu score final bdeu score obtained network averaged number records dataset 
number edges resulting bayes nets reported table 
interesting note bdeu scores corresponding bayes nets obtained running described table close ones obtained random hillclimbing significantly lower number edges 
supports claim frequent itemsets contain information relevant construction highest scoring bayes net 
evident results proposed augmenting algorithms increase score 
note augmenting network highest mutual information edges total number arcs doubled highest relative improvement score compared proposed augmenting techniques 
hillclimbing improve score number edges compared 
average bdeu score support support support maximum tuple size 
bdeu scores citeseer dataset different parameterizations algorithm final score dag produced depends user defined support maximum frequent set size 
noticed citeseer imdb institute datasets lowering support increasing maximum frequent set size results higher bdeu scores 
shows score fluctuations varying maximum frequent set size fixed support citeseer dataset 

maximum frequent set size experiments tried different maximum frequent set sizes mfss 
lower bound mfss means consider pairs items structure learned solely way marginal counts 
shows obvious loss accuracy high order interactions taken account 
maximum frequent set size number frequent sets increase substantially datasets behavior changes little 
note natural upper bound maximum tuple size due sparsity datasets 
example publications citeseer database authors exactly authors 
potential number publications authors total number authors database empirical number total 
exponential drop number occurrences size tuples increases shown 
expect great improvement score bayes net increasing maximum tuple size support larger tuples 
ln number publications 
number authors citeseer 
exponential drop number publications number authors increases citeseer dataset 
support lowering support greatly increases number frequent sets considered screening 
introduces quite interactions variables low marginal counts 
model fitting contingency tables general sensitive low marginal counts zero bishop 
bdeu sensitive low counts 
despite idea keep support relatively large 
case tested support sizes smaller datasets reasonable support choices 
score model better overfit shown table 

overfitting holdout sets study overfitting 
withheld roughly third dataset case compared average likelihood node training testing datasets 
results summarized table 
networks learned score higher better learned hillclimbing testing dataset 
indicates algorithm learns better fitting models 
seen table difference average loglikelihood score training testing table 
average bdeu scores 
mfss random edges considered hillclimbing dataset rand mie mie nd mie nd citeseer imdb institute drinks table 
number links resulting nets 
mfss random edges considered hillclimbing dataset rand mie mie nd mie nd citeseer imdb institute drinks general smaller hillclimbing 
average loglikelihood testing set worse training sets indicating degree overfitting 
believe overfitting occurs due multiple hypothesis testing hundreds thousands possible parents 
correction multiple hypothesis testing problem similar corrections learning algorithms oates jensen incorporated 
table 
overfitting testing dataset train test citeseer citeseer citeseer imdb imdb imdb 
time performance experiments conducted unloaded ghz pentium iv machines gb ram 
total times required run algorithm time took random hillclimbing create bayes net adding removing reversing edges reported table 
table 
total times random hillclimbing mie create bayes net mins 
mfss dataset rand mie citeseer imdb institute drinks break total time segments corresponding major steps algorithm reported table 
biggest cost obtain frequencies time takes perform remaining operations depends number frequent sets occur frequently predefined support 
experiments shown number small fraction total number ties nodes 
interesting note random hillclimbing fast network consists small subgraphs soon subgraphs joined new edges time increases tremendously due complexity cycle detection 
example takes random hillclimbing order minutes add remove reverse edges takes hours perform operations number nodes edges relatively small increase score 
sense random graphs exactly random discussed callaway 

example application important growing application fields large bayes nets recommender systems 
related application intelligence having detected subset participants adverse event inferring 
purpose recommender service provide user suggestion products buy historical preferences 
simulated recommender query citeseer dataset 
mapping follows suppose set authors represents user preferences particular products 
learn bayes net available authorship information query network incomplete subsets authors predict selection entities authors case completes set 
answer query reported top completions highest example query subset members daphne koller group dags koller getoor pfeffer taskar 
results tables 
suggested completions fact people part collaborate closely daphne koller group details omitted space reasons moore table 
time min task 
mfss dataset task freq sets local struct search dump dag mi augment degree augment citeseer imdb institute drinks table 
completions size members dags group completion score koller friedman pfeffer getoor taskar koller pfeffer getoor tong taskar koller boyen pfeffer getoor taskar table 
completions size members dags group completion score koller grove halpern pfeffer getoor taskar koller malik weber pfeffer getoor taskar koller russell parr pfeffer getoor taskar analogy expect set relevant items predicted recommender system algorithm 
interesting note example person complete subset different table suggestions provided algorithm assumption missing people table 
observation suggests complex interactions systems built pairwise statistics 
inference took second 

related earlier area concentrated efficient representation sparse data caching way counts moore lee 
chickering heckerman meila noted computations requiring way pairwise counts sped significantly dealing sparse data caching data structures adtrees moore lee 
believe body great potential build ideas introduced papers utilizing sparse data representation low overhead efficient calculation marginals 
frequent sets learning bayes nets local scale explored pavlov 
goal answer probabilistic queries subset variables need combine local information obtain joint distribution query size estimated 
performance bayes nets learned selection variables reported worse close accuracy inferences drawn bayes net learned full dataset 
proposed integrate frequent sets local methodology modelling joint distributions 
shown mixture models obtained frequent sets maximum entropy accurate supporting claim frequent sets contain important local information modelling joint distributions 
approach speed structural search bayes nets massive datasets restrict possible parents 
full sparse candidate algorithm friedman 
original form method speed hillclimbing cost lower performance practice performance loss shown great 
motivation structural search local scale inadvertently restricts number parents 
global scale number parents bayesian network limited perceive improvement original sparse candidate algorithm 
sampling proposed techniques speed modelling massive datasets hulten domingos pelleg moore 
interesting direction orthogonal approach 
idea augmenting bayes nets high mutual information edges fact dependencies accounted frequent sets 
fast computation meila 

tractable solution bayes net structure search problem sparse datasets 
researchers frequent sets take advantage sparseness 
main new contribution perform structural search local level order produce global model 
propose techniques improve score resulting net 
key improvements augmentation edges high mutual information entities occurred dataset 
performed empirical study small large attributes datasets 
show tractable times maintaining accuracy better hill climbing tractable alternative learning structure networks size 
believe serves primary purposes 
opens new horizons modelling joint distributions massive sparse datasets 
second viewed novel way postprocess frequent sets commercial data mining 
raise question utilizing dataset properties improve computational complexity structural search 
feel immense potential exploiting properties frequency distribution obtain high accuracy models fraction time 

appendix support claim frequent sets contain essential information needed build bayes net sparse data show sparse large datasets positive correlation variables stronger negative suppose binary variables correlation coefficient 
assume dataset sparse records large 
multinomial sampling model observed correlation coefficient maximum likelihood estimate bishop 
case entities occurred times times separately dataset fact clearly violation sparseness assumption correlation significant 
case entities occurred frequency 
large conforming sparseness assumption note wehave frequency occurrence case correlation significant 
showed sparse assumption positive correlations stronger negative ones 
learning bayes net increase score screening frequent sets 
agrawal imielinski swami 

mining association rules sets items large databases 
acm sigmod pp 

agrawal srikant 

fast algorithms mining association rules 
vldb pp 

bishop fienberg holland 

discrete multivariate analysis theory practice 
mit press 
breese heckerman kadie 

empirical analysis predictive algorithms collaborative filtering 
uai 


emergent themes social network analysis results challenges opportunities 
dynamic social network modeling analysis workshop summary papers 
callaway hopcroft kleinberg newman strogatz 

randomly grown graphs really random 
physical review 
chickering heckerman 

fast learning sparse data 
uai 
cooper herskovits 

bayesian method constructing bayesian belief network databases 
uai pp 

friedman 

inferring cellular networks probabilistic graphical models 
science 
friedman nachman pe er 

learning bayes network structure massive datasets sparse candidate algorithm 
uai 
moore 

tractable structural learning large bayesian networks sparse data technical report cmu 
cmu 
han kamber 

data mining concepts techniques 
morgan kaufmann publishers 
heckerman geiger chickering 

learning bayesian combination statistical data 
machine learning 
mannila 

mixture models frequent sets combining global local methods data 
siam icdm 
hulten domingos 

mining complex models arbitrarily large databases constant time 
acm sigkdd 
mannila toivonen 

multiple uses frequent sets condensed representations 
kdd pp 

meila 

accelerated chow liu algorithm fitting tree distributions high dimensional sparse data technical report aim 
mit 
moore lee 

cached sufficient statistics efficient machine learning large datasets 
journal artificial intelligence research 
moreno jennings 

statistics social configuration 

oates jensen 

large datasets lead overly complex models explanation solution 
kdd 
pavlov mannila smyth 

independence probabilistic models query approximation binary transaction data 
ieee transactions knowledge data engineering 
pelleg moore 

tarjan red rule fast dependency tree construction 
nips 
