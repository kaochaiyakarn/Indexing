improving short text classification unlabeled background knowledge assess document similarity sarah cs rutgers edu haym hirsh hirsh cs rutgers edu computer science department rutgers university road piscataway nj usa describe method improving classification short text strings combination labeled training data plus secondary corpus unlabeled related longer documents 
show unlabeled background knowledge greatly decrease error rates particularly number examples size strings training set small 
particularly useful labeling text labor intensive job large amount information available particular problem world wide web 
approach views task information integration whirl tool combines database functionalities techniques information retrieval literature 

task classifying textual data culled sites world wide web difficult intensively studied cohen hirsh joachims nigam 
applications various machine learning techniques attempt solve problem include categorization web pages sub categories search engines classification news articles subject 
machine learning programs quinlan ripper cohen limitation learn solely previously classified data :10.1.1.50.8204
impractical extremely tedious expensive sufficient number training examples achieve high accuracy needed task 
huge proliferation data web tiny percentage realistically classified labeled programs unable exploit information achieve higher accuracy faced new unlabeled examples 
various researchers text learning mining recognized labeled examples tremendous amount unlabeled examples 
nigam 
press done expectation maximization em naive bayes classifier 
parameters naive bayes classifier set labeled examples 
learned model em probabilistically classify unlabeled documents resulting collection classified documents estimate new set parameters naive bayes 
em algorithm iterates change naive bayes parameters 
nigam number experimental results show error rates reduced significantly unlabeled examples way 
related algorithms described mccallum nigam jones 

blum mitchell training algorithm uses unlabeled data improve learning 
algorithm applies problems target concept described redundantly sufficient ways different subsets attributes describing example 
view data create predictor predictor classify unlabeled data train learner 
blum mitchell prove certain conditions unlabeled examples way sufficient pac learn concept initial weak learner 
lewis colleagues lewis gale lewis catlett unlabeled data learning focus asking labels human labeler modest subset data class membership undecided result learning data labeled far 
describes method uses corpus unlabeled data assist classification task 
preceding approaches attempt classify data require form comparable training data 
cases possible classify data classification schema labeled instances 
unlabeled corpus background knowledge learner aid decision task 
directly comparing new unlabeled example directly elements labeled training corpus unlabeled background knowledge bridge connect new example labeled examples 
labeled training example useful classifying unknown test instance exists set unlabeled background knowledge similar test example training example 
call second order approach classification data longer directly compared compared step removed intermediary 
detail look improving classification short text strings unlabeled related longer documents 
concrete example usefulness approach seen task assigning topic labels technical papers 
labeling title physics article sub specialty title containing word galaxy easily classified correctly astrophysics training articles domain 
article common topic example old white dwarfs classified correctly title words appears labeled training examples 
training set contain words old white dwarf experimental data system able correctly classify title words astrophysics utilizing corpus unlabeled abstracts field naturally available web 
second order approach system finds unlabeled abstracts similar old white dwarfs various training titles 
training titles classify old white dwarfs correctly titles quite dissimilar compared directly 
order achieve goal whirl cohen cohen conventional database system augmented special operators text comparison 
text classification program nearest neighbor approach cohen hirsh text documents specified tfidf vectors similarity text documents measured cosine similarity salton 
whirl possible pose sql queries databases text valued fields 
consider table background knowledge table test example table whirl provides framework easily specify explore second order similarity classification 
allows succinct queries specify combination training similarity background similarity new test example 
section give brief review whirl discussion classification unlabeled data 
describe distinctly different domains tested system 
domain descrip tions followed set results domains varied data sets 
conclude discussion various possible dimensions choices way take directions current research 

approach whirl text classification whirl cohen cohen information integration tool specifically designed query integrate varied textual sources web 
whirl queries search retrieve textual sources specified conditions 
assume corpus training examples labels test example assigned label 
training examples viewed table field instance hold textual data field label hold class label 
test example line table simply textual field instance 
example whirl query cohen hirsh select test instance train label train test train instance sim test instance user specified parameter query generate intermediate table containing tuples test instance train instance train label maximize similarity score test instance train instance 
traditional sql queries result set tuples ordered score highest score representing closest train instance test instance pair whirl sim operator compute similarity textual documents 
compute similarity whirl computes model text document representing document vector vector space 
representation computed passing document stemmer porter computing weights term tfidf salton weighting method 
distances vectors computed cosine metric represents statistical similarity documents 
whirl final step takes table tuples projects fields specified select statement 
note mean may training examples label multiple nearby examples training set combined single tuple final result table 
combination scores performed treating scores probability combination noisy individual scores tuples label sn final score label si 
whichever label highest score resulting projected table returned label test instance 
method bears similarities nearest neighbor method yang chute shown perform quite text classification tasks cohen hirsh 
papers value experiments 
whirl background knowledge question ask large body unlabeled data background knowledge aid classification 
pieces information need labeled may classification task hope learn word combinations examples 
background knowledge may provide corpus text contains information importance words terms tfidf values large corpus joint probability words percentage time words coexist document 
gives large context test training example new test example 
context conjunction training examples label new example 
whirl expressive language ability create conjunctive queries simply adding conditions query whirl queries text classification expanded allow background knowledge subject 
example classification physics titles discussed earlier suppose fairly small set labeled titles large set unlabeled titles papers abstracts web pages resulting search relation called background single field value 
create query classification select test instance train label train test background train instance sim background value test instance sim background value similarity comparisons query computes score whirl multiplies obtain final score tuple intermediate results table 
table projected test instance train label fields discussed 
whichever label gives highest score returned label test example 
way thinking trying connect test example directly training example tries bridge element background table 
note whirl combines scores tuples generated different matches background table 
whirl fashion essentially conducts search set items background knowledge close neighbors test ex ample provided exists training example neighbor background knowledge 
training neighbors test example defined differently background knowledge incorporated 
words test example background knowledge words background knowledge connect test example dissimilar terms word overlap direct cosine difference training examples 
final classification integrates information multiple training examples multiple bridge examples lie background text 
note approach concern class 
background item belongs 
simply text directly part decision making process 
contrast approach explicitly training set classify background items true examples add labeled set 
method allows sophisticated combination training instances background knowledge 
background instance close numerous training instances included table returned whirl query training examples close different classes 
similarly training example included table multiple times close numerous background instances 
suppose classification task consists labeling words news article topic 
test example belongs category sports instance cosine distance words test example small number training examples large 
hope large corpus unlabeled news articles articles contains words test example words training examples 
note background knowledge whirl query sense form query expansion buckley 
directly searching training examples closest test example search training examples closest background knowledge expansion test example 
standard query expansion conjunctive conditions background knowledge expansion chosen respect training example close 
means query multiple expansions maximize score conjunctive condition combined 

experiments results tested system distinct tasks taken world wide web 
case training test examples short text strings problem prevalent real world applications whirl especially designed 
problems source background knowledge varies originating site obtained labeled data unrelated sites web 
data sets technical papers common text categorization task assigning discipline sub discipline names technical papers 
created data set physics papers archive xxx lanl gov downloaded titles technical papers areas physics astrophysics condensed matter general relativity quantum cosmology month march 
background knowledge downloaded abstracts papers areas previous months january february 
total pieces knowledge background set training test set combined 
distribution classes skewed titles astrophysics condensed matter quantum cosmology 
background knowledge abstracts downloaded labels knowledge sub discipline learning program access 
news data set created obtained clarinet news 
downloaded articles sports banking headings november th ones training test sets older ones background knowledge 
total background knowledge consisted corpus articles 
background knowledge problem consisted words articles 
informal studies showed including entire articles improve accuracy substantially degraded efficiency whirl 
training data points belonged sports category belonged banking category 
sets results connection data points called words words words words corresponding test training set consisting words article respectively 
web page titles determine usefulness whirl nearest neighbor classification tool cohen hirsh data sets taken world wide web 
www edu included web page headings pages concerning cows horses cats dogs rodents birds primates 
example training example class birds wild bird center walnut creek 
titles url linked title associated web page 
labeled corpus chose half titles labels total examples 
discarded half titles labels simply kept url associated web page 
urls download words pages placed corpus background knowledge 
urls reachable ignored program created background knowledge 
total entries background knowledge database 
companies second cohen hirsh data sets consisted training set names taken hoover web site www hoovers com labeled industry names 
created background knowledge entirely different web site biz yahoo com 
downloaded web pages business category yahoo 
business hierarchy create pieces background knowledge 
yahoo 
hierarchy different number classes different way dividing companies irrelevant purposes treated solely source unlabeled background text 
piece background knowledge consisted combination web pages stored sub topic yahoo 
hierarchy 
instance table background knowledge longer text string training test examples 
results series results graphs show whirl incorporating background knowledge performs better whirl supplementary knowledge 
improvement dramatic fewer labeled examples labeled examples shorter strings 
number data sets time start section comparing core whirl method background knowledge label whirl nn traditional method ripper cohen demonstrate improvements top strong classification performance 
error rates table represent average error cross validated runs full training set 
best value problem shown bold 
whirl uses porter stemming algorithm creates tfidf representation giving data ripper 
seen table data sets whirl nn outperforms ripper improvements whirl nn report represent stronger classification performance credible state art method 
remainder results series figures report error rates baseline whirl approach whirl nn new approach label 
case report error rates vary number training examples learner 
point table 
error rates ripper vs whirl nn data set ripper whirl nn words words words words hoovers class physics class physics represents average cross validated runs 
cross validated run fifths data training set fifth test set 
holding test set steady number examples training set varied 
data set tested whirl nn whirl bg percent data 
sets results physics data 
class problem titles papers astrophysics condensed materials classes 
classes nearly number training examples 
class problem class fewer number training examples added previous problem 
clearly shows effect background knowledge text data sets 
line representing whirl bg remains horizontal fewer training examples indicating background knowledge compensated lack data 
contrast whirl nn sharply degraded fewer training examples 
helpfulness background knowledge increased fewer training examples 
third class added error rates whirl nn whirl bg went 
effect background knowledge seen 
results clarinet data set figures 
training test set figures vary terms number words example ranging words article words 
expected smaller number words training test example worse whirl nn performed 
addition background knowledge useful shorter strings test training data 
represented figures point lines intersect 
strings length background knowledge reduced error rates entire set training data 
number words training test examples increased point background knowledge helpful changed 
strings length background knowledge reduced er error rate whirl bg whirl nn percentage data 
whirl bg whirl nn class title problem error rate whirl bg whirl nn percentage data 
whirl bg whirl nn class title problem error rate whirl bg whirl nn percentage data 
whirl bg whirl nn word news error rate whirl bg whirl nn percentage data 
whirl bg whirl nn word news error rate whirl bg whirl nn percentage data 
whirl bg whirl nn word news error rate whirl bg whirl nn percentage data 
whirl bg whirl nn word news error rate whirl bg whirl nn percentage data 
whirl bg whirl nn ror rates percent data 
gives empirical evidence informative training data greater advantage having corpus background knowledge available classification 
size reduction error rate obtained running whirl bg greater fewer words example 
results domain graphed 
reductions error rate increased number training examples decreased 
domain previously discussed overlap topics background knowledge 
web page useful classifying test example belonging category dogs quite discuss cats vice versa 
training test examples caused confusion 
titles web pages pet stores animal care placed topic just easily placed different categories 
surprised see error rate decrease large percentage 
results companies data set graphed 
whirl bg outperformed whirl nn 
percent data decrease error rate substantial 
percent training examples lower difference error rate systems reduced 
results previous domains 
due fact training test examples names consisted words occured example xerox reducing number training examples reduced dictionary words training corpus substantially 
fewer words find bridges background knowledge 
error rate whirl bg whirl nn percentage data 
whirl bg whirl nn companies 
final remarks method reduce error rates text classification large body potentially uncoordinated background knowledge 
domains system applied saw substantial reductions error rates particularly set labeled examples small 
background knowledge allowed small degradation accuracy cases percent data 
number limitations address 
efficiency queries issue deal 
whirl efficient approach essentially nearest neighbor second order query condition combinatorial search space 
word combinations background knowledge crucial success method area research effect source nature information background knowledge data base 
experimental data sets background knowledge came site training test sets 
fourth data set background knowledge comes unrelated web site background knowledge web site 
ideally background knowledge culled combination different sites automatically created web search 
plan explore refinements whirl bg approach 
currently exploring disjunction whirl bg whirl nn queries 
allow correct classification test example close training examples appropriate background bridges exist 
help issue efficiency useless search pruned early 
look extensions approach 
example consider query select test instance train label train test background background train instance sim value test instance sim value value sim value type query provides different way background knowledge bridge gaps training example test example 
test training examples small bit knowledge class belong query allows small bit knowledge mapped larger pieces background knowledge similar 
investigating conjunctive clauses whirl adjust impact training test data similarity 
acknowledgments william cohen helpful comments discussions 
anonymous reviewers useful suggestions 
blum mitchell 

combining labeled unlabeled data training 
proceedings eleventh annual conference theory pp 

new york acm press 
buckley salton allan singhal 

automatic query expansion smart trec 
proceedings third text retrieval conference pp 

md nist special publication 
cohen 

fast effective rule induction 
proceedings twelfth international conference machine learning pp 

san francisco morgan kaufmann 
cohen 

integration heterogeneous databases common domains queries textual similarity 
proceedings acm sigmod international conference management data pp 

new york acm press 
cohen 

web information system reasons structured collections text 
proceedings second international acm conference autonomous agents pp 

new york acm press 
cohen hirsh 

joins generalize text categorization whirl 
proceedings fourth international conference knowledge discovery data mining pp 

menlo park california aaai press 
joachims 

text categorization support vector machines learning relevant features 
proceedings tenth european conference machine learning pp 

berlin springer 
jones mccallum nigam riloff 

bootstrapping text learning tasks 
working notes ijcai workshop text mining foundations techniques applications pp 

lewis catlett 

heterogeneous uncertainty sampling supervised learning 
proceedings eleventh international conference machine learning pp 

san francisco morgan kaufmann 
lewis gale 

sequential algorithm training text classifiers 
proceedings seventeenth annual international acm sigir conference research development information retrieval pp 

new york acm press 
mccallum nigam 

text classification bootstrapping keywords em shrinkage 
working notes acl workshop unsupervised learning natural language processing pp 
mitchell 

role unlabeled data supervised learning 
proceedings sixth international colloquium cognitive science 
nigam mccallum thrun mitchell 
press 
text classification labeled unlabeled documents em 
machine learning 
porter 

algorithm suffix stripping 
program 
quinlan 

programs machine learning san mateo morgan kaufmann 
salton 

automatic text processing 
reading ma addison 
yang chute 

example mapping method text classification retrieval 
acm transactions information systems 
