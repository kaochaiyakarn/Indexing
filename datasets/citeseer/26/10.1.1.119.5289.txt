relating data compression learnability nick littlestone manfred warmuth department computer information sciences university california santa cruz june explore learnability valued functions samples paradigm data compression 
algorithm compression small subset sample called kernel 
second algorithm predicts values function kernel algorithm acts hypothesis function learned 
second algorithm able reconstruct correct function values point original sample 
demonstrate existence suitable data compression scheme sufficient ensure learnability 
express probability hypothesis predicts function correctly random sample point function sample kernel sizes 
assumptions probability distributions sample points generated 
approach provides alternative uses vapnik chervonenkis dimension classify learnable geometric concepts 
bounds derived directly kernel size algorithms vapnik chervonenkis dimension hypothesis class 
proofs simpler introduced compression scheme provides rigorous model studying data compression connection machine learning 
learning problems learning concept subset sample domain 
consider situation points learner selected random sample domain probability distribution 
study bounds rate learning independent probability distribution approach introduced valiant 
show certain naturally arising class authors gratefully acknowledge support onr second author support faculty research committee university california santa cruz 
learning algorithms bounds depend simple characteristic algorithm size call kernel 
learning model follows denote sample domain 
learn concepts subsets samples 
concepts represented indicator function concept mapping 
learning process sequence observations particular concept class concepts 
learning corresponds finding hypothesis predicts concept small error 
hypothesis concept necessarily observations elements samples sequences denote 
call size sample 
sample zero labels determined particular class point sequence observations xm 
example sample domain euclidean plane 
class concepts collection figures set triangular regions 
aim learn particular triangle 
receive observations triangle points plane labels triangle 
arbitrary fixed probability distribution example points plane 
points sample drawn distribution labeled drawing samples learning algorithm forms hypothesis 
hypothesis evaluated distribution error hypothesis probability hypothesis disagrees random point drawn learning algorithm properties error arbitrarily small arbitrarily high probability large 
bounds independent concept trying learn underlying distribution 
bounds polynomial inverse error probabilities 
computation hypothesis computation value hypothesis point polynomial length sample 
class concepts exists algorithm fulfills called learnable 
holds class polynomially learnable 
condition formalized demanding error greater probability small uniformly concepts condition implies number required samples function grows polynomially 
necessary sufficient conditions learnability terms vapnik chervonenkis dimension vc concept class 
bounds rate learning functions vapnik chervonenkis dimension 
results non constructive sense lead specific algorithm learning 
results steps constructing learning algorithm verifying learns polynomial number sample points construct algorithm finite sample generates hypothesis consistent sample 
main theorem required hypothesis member class concepts learned 
allow hypothesis classes 
find vapnik chervonenkis dimension class hypotheses chosen 
finite dimension demonstrates learnability yields bounds speed learning 
approach motivated various examples 
number examples algorithm hypothesis specified terms fixed size subsample labeled sample 
example concept orthogonal rectangle plane determined observations 
sense sample size compressed sample fixed size 
compressed sample labels original sample reconstructed 
similarly figures polygons half spaces determined small number points 
example algorithm bl uses points determine half plane show algorithm characteristic data compression explicitly specified sufficient guarantee learnability necessary refer vapnik chervonenkis dimension 
words leave step 
require hypotheses lie particular class concepts arbitrary borel sets bounds rate learning terms size compressed subsample call kernel size 
examples learning geometric concepts examined bounds better bounds derived vapnik chervonenkis dimension 
precise general relation bounds yielded approaches known 
difficulty finding general relationship bounds reflects substantial difference approaches valuable supplements 
expect data compression algorithms described exist wide variety concept classes providing easily applied alternative approach 
basic results relating data compression learnability conditions 
proof technique amenable relaxation conditions 
presenting basic compression scheme suggest extensions 
notation sequences tuples denoted barred variables elements denoted th point xi subsequence sequence xt xt 
tk iq denotes indicator function set basic compression scheme section study learnability relation basic compression scheme 
consider data compression schemes form concept class data compression scheme kernel size consists pair mappings subsequence length 
point xi xi xi 
call kernel sample 
second condition specifies reconstructs labels sample points correctly 
say consistent usually mappings algorithms 
assume reconstruction function borel measurable 
holds example functions rn built recursively ordinary comparison arithmetic operations 
assume concepts borel measurable 
note notation simple assume kernels size sample size define kernel size concept class minimum kernel size compression schemes 
data compression scheme form basis learning algorithm 
labeled sample algorithm hypothesis concept set 
determining kernel corresponds computing hypothesis kernel encodes hypothesis 
computation value hypothesis achieved kernel input 
condition polynomial learnability algorithms polynomial length input sample size polynomial show compression scheme fixed size polynomial basic scheme appealing simplicity generality 
sample compressed kernel able reconstruct values sample 
note don require bounds length encoding kernel 
points instance reals arbitrary high precision 
compression bounded number bits discussed simpler 
theorem compression scheme kernel size error larger probability sample size proof suppose learning concept 
want find bound probability choosing sample leads hypothesis error greater 
words want bound error probability equivalently 

collection element subsequences sequence 
tk xt xt 
mark xt xi xi xi 
definition get xt 
condition guarantees roughly sets serve follows split overlap samples repeated points 
look intersection extending intersections sets eliminates explicit dependence sets gives sets probabilities easily bounded 

convenient rearrange coordinates 
permutation sends ti xm xm send xm 
note bt dp xk 
exists set theorem bt dp dp bt dp xk xi xi fori 
wx xk xk xk xk xk inner integral equals xk 
xk wx xk 
inner integral bounded bounds entire integral get size proof depends measurability sets wx xk measurability wx xk follows measurability fact compositions borel measurable functions borel measurable 
see measurability xt set measurable function measurable 
follows simple case theorem 
measurable 
theorem give explicit bounds sample size guarantee learnability 
similar bound max log log denotes vapnik chervonenkis dimension class learned 
example case learning dimensional orthogonal rectangles dimension 
kernel size straight forward compression scheme 
bounds stated theorem better roughly factor 
dimension kernel size equal 
case arbitrary dimension exists algorithm kernel size bl 
theorem compression scheme kernel size produces probability hypothesis error sample size max ln ln 
holds arbitrary 
proof follows bound previous theorem 
applying previous theorem suffices show fulfills bound 
rewritten ln ln ln holds ln ln ln ln summands expression 
inequality certainly holds summand summand easily leads bound maximum expression theorem 
similarly second summand lead second bound 
ln holds equal second bound holds larger replacing second bound inequality leads simplifies ln ln ln ln ln easily verified 
additional information section extend basic scheme allowing additional information kernel 
sample compressed element finite set kernel size set represents additional information providing 
receives element kernel input 
exactly redefined follows example wants learn unions orthogonal rectangles clearly need observations 
observation belongs rectangle 
observations subsequence original sample 
need additional information specify particular permutation points 
permuting points determine rectangle points second rectangle forth 
additional information knows locations rectangles predict accordingly 
generalize theorems previous section 
case sample compressed ln bits studied bounds contain bounds subcase 
theorem compression scheme kernel size additional information error larger probability sample size proof proof extension proof theorem 
want bound 
index set extended sequences consist element followed element subsequence 
adapt definitions tk xt xt 
xt 
xt xi xi xi ti 

rearrange coordinates 
permutation sends ti send xm 
phi rearranges xk xi xi fori 
bt dp dp describe wt xk xk clearly xk xk xk inner integral equals xk xk 
entire integral bounded bt dp way 
case compressions scheme additional information note basic compression scheme theorem theorem get bounds sample size guarantee learnability 
note bound max expression exactly bound proven case kernel empty 
theorem compression scheme kernel size additional information produces probability hypothesis error sample size max ln ln ln 
holds arbitrary 
example learning orthogonal rectangles kernel size set cardinality 
bound max ln ln ln 
bound compares favorably bounds max log log log example see hw estimate dimension 
dependence kernel size concept keep notation simple assumed kernel fixed size 
cases kernel size depend sample size concept learned 
verified easily proofs hold case 
example kernel size depends concept improved learning algorithm example kernel size depends sample size 
earlier mentioned union orthogonal rectangles represented kernel size plus finite information demonstrating learnability concept 
concept class consists arbitrary unions rectangles bounded kernel size suffice concepts class 
allowing kernel size depend concept number rectangles union find data compression scheme class 
case demonstration learnability polynomial learnability nphard find smallest number rectangles interprets sample 
get polynomial learnability polynomial approximation minimum cover 
approximation algorithm finds consistent hypothesis union log rectangles polynomial time 
kernel size log plus finite information suffices represent hypothesis 
kernel size depends sample size 
appropriate polynomial bounds sample size follow theorem 
theorem compression scheme kernel size pm additional information qm error probability qm pm pm sample size pm fixed concept class relaxing con straint relax condition asserts consistent sample 
practice hard find polynomial algorithms consistent samples 
polynomial algorithms consistent large portion samples 
question samples may missed denote number assure learnability 
point xi xi xi holds points xi 
theorem compression scheme size misses points error probability sample size proof proof theorems fact 
needed consistent points outside 
see definition proofs 
inconsistent samples 
compression scheme size inconsistent elements 
construct related compression scheme additional information size consistent points sample points 
simply applies scans sample determine samples predicted correctly 
new plus samples predict correctly 
additional information specify original size new size consists length exactly bits 
scans size removes points original 
applies 
easy see schemes predict function values 
particular error 
apply modified proof theorem scheme bound theorem follows 
note errors samples introducing complexity discussion open problems proof learnability compression schemes fixed kernel size shorter proof 
hand able show condition fixed vapnik chervonenkis dimension necessary learnability 
scheme show sufficiency necessity remains open question 
concept classes finite dimension scheme bounded kernel size bounded additional information 
compression compared compression implicit compresses sample fixed number bits encode consistent hypothesis 
contrast scheme compress fixed size subsample points arbitrarily high precision 
way gain understanding relation approaches compare bounds produced case apply 
example suppose class concepts learned subintervals interval 
suppose domain contains finite subset represented binary fractions bits possible concepts 
represent hypothesis bits single bit number give information reconstruct sample 
sufficient argument theorem guarantee learn sample size ln 
data compression scheme concept class learned kernel size 
theorem suffices take sample size ln ln 
note bound bound independent precision represent points interval 
clearly combinatorial complexity example captured fact compress point 
precision point side issue 
note vapnik chervonenkis dimension concept class example 
classifying learnability dimension avoids issue precision 
paradigm compression scheme simple extended various ways 
aim introduce basic scheme 
research relax condition required consistent sample kernel input 
practice hard find compression schemes guarantee consistency sample 
explore bounds sample missed class remain learnable 
secondly address case sample reliable 
study relation amount error speed confidence learn 
errors modeled probabilistically leads considering case learnability speed learning depends underlying probability distribution 
step doing relax requirement bounds sample size independent underlying probability distribution 
certain concept classes necessarily finite vapnik chervonenkis dimension learnable broader definition learnability 
data compression scheme applied uses extended scheme requires partial consistency sample 
require concepts remain learnable arbitrary distributions speed learning may heavily dependent distribution 
underlying distribution unknown bounds longer give practical apriori information sample size needed learn desired degree confidence 
case wish empirical tests estimate required sample size 
function value observations changed 
consider case changes probabilistically adversary determine sample changed learnability 
blumer ehrenfeucht haussler warmuth classifying learnable geometric concepts vapnik chervonenkis dimension eighteenth annual acm symposium theory computing berkeley may pp 

blumer ehrenfeucht haussler warmuth occam razor information processing letters pp 

bl blumer littlestone learning faster promised vapnik chervonenkis dimension unpublished manuscript 
hw haussler welzl epsilon nets range queries discrete computational geometry pp 

johnson approximation algorithms combinatorial problems journal computer systems sciences vol 

np complete set cover problems mit laboratory computer science unpublished manuscript 
fastest descent method covering problems russian proceedings symposium questions precision efficiency computer algorithms book pp 

rudin real complex analysis mcgraw hill series higher mathematics 
valiant theory learnable comm 
acm pp 

vc vapnik ya chervonenkis uniform convergence relative frequencies events probabilities th 
prob 
appl pp 


