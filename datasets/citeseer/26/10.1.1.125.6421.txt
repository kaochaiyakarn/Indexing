cascade correlation learning architecture scott fahlman christian lebiere august cmu cs school computer science carnegie mellon university pittsburgh pa cascade correlation new architecture supervised learning algorithm artificial neural networks 
just adjusting weights network fixed topology cascade correlation begins minimal network automatically trains adds new hidden units creating multi layer structure 
new hidden unit added network input side weights frozen 
unit permanent feature detector network available producing outputs creating complex feature detectors 
cascade correlation architecture advantages existing algorithms learns quickly network determines size topology retains structures built training set changes requires back propagation error signals connections network 
research sponsored part national science foundation contract number eet defense advanced research projects agency dod arpa order contract monitored avionics laboratory air force wright aeronautical laboratories aeronautical systems division afsc wright patterson afb oh 
views contained document authors interpreted representing official policies expressed implied defense advanced research projects agency government 

back propagation learning slow 
cascade correlation learning algorithm developed attempt overcome certain problems limitations popular back propagation backprop learning algorithm rumelhart 
important limitations slow pace backprop learns examples 
simple benchmark problems back propagation network may require thousands epochs learn desired behavior examples 
epoch defined pass entire set training examples 
attempted analyze reasons backprop learning slow identified major problems contribute slowness 
call step size problem moving target problem 
may course contributing factors identified 

step size problem step size problem occurs standard back propagation method computes partial derivative error function respect weight network 
derivatives perform gradient descent weight space reducing error step 
straightforward show take infinitesimal steps gradient vector running new training epoch recompute gradient step eventually reach local minimum error function 
experience shown situations local minimum global minimum solution problem hand 
practical learning system want take infinitesimal steps fast learning want take largest steps 
unfortunately choose step size large network reliably converge solution 
order choose reasonable step size need know just slope error function higher order derivatives curvature vicinity current point weight space 
information available standard back propagation algorithm 
number schemes proposed dealing step size problem 
form momentum rumelhart crude way summarizing slope error surface earlier points computation 
conjugate gradient methods explored context artificial neural networks number researchers watrous lapedes kramer generally results 
schemes example jacobs dynamic proposed adjust step size dynamically change gradient step 
becker lecun becker explicitly compute approximation second derivative error function step information guide speed descent 
fahlman quickprop algorithm fahlman successful algorithms handling step size problem back propagation systems 
quickprop computes values just standard backprop simple gradient descent quickprop uses second order method related newton method update weights 
learning benchmarks collected quickprop consistently performs backprop algorithms large factor 
quickprop weight update procedure depends approximations small changes weight relatively little effect error gradient observed weights second error function respect weight locally quadratic 
weight quickprop keeps copy current slope 
retains change weight update cycle 
slope computed previous training cycle weight independently slopes step define parabola jump minimum point curve 
approximations noted new point probably precisely minimum seeking 
single step iterative process algorithm 
practice complications added simplified algorithm see fahlman details 

moving target problem second source inefficiency back propagation learning call moving target problem 
briefly stated problem unit interior network trying evolve feature detector play useful role network computation greatly complicated fact units changing time 
hidden units layer net communicate directly unit sees inputs error signal propagated back network outputs 
error signal defines problem unit trying solve problem changes constantly 
situation unit moves quickly directly assume useful role see complex dance units takes long time settle 
experimenters reported backprop learning slows dramatically exponentially increase number hidden layers network 
part slowdown due attenuation dilution error signal propagates backward layers network 
believe part slowdown due moving target effect 
units interior layers net see constantly shifting picture upstream downstream units evolve impossible units move decisively solution 
common moving target problem call herd effect 
suppose separate computational sub tasks performed hidden units network 
suppose number hidden units handle tasks 
units communicate unit decide independently problems tackle 
task generates larger coherent error signal task tendency units concentrate ignore problem solved redundantly units see task remaining source error 
move problem reappears 
cases herd units eventually split deal sub tasks may long period indecision occurs 
weights backprop network random initial values prevent units behaving identically initial variability tends dissipate network trained 
way combat moving target problem allow weights units network change holding rest constant 
cascade correlation algorithm uses extreme version technique allowing hidden unit evolve time 
holding network constant time slow learning cases tested strategy allows network learn faster 
moving target effect eliminated unit frozen quickly choose useful role solution move decisively fill role 

description cascade correlation cascade correlation combines key ideas cascade architecture hidden units added network time change added 
second learning algorithm creates installs new hidden units 
new hidden unit attempt maximize magnitude correlation new unit output residual error signal trying eliminate 
cascade architecture illustrated 
begins inputs output units hidden units 
number inputs outputs dictated problem representation experimenter chosen 
input connected output unit connection adjustable weight 
bias input permanently set 
output units may just produce linear sum weighted inputs may employ non linear activation function 
experiments run far symmetric sigmoidal activation function hyperbolic tangent output range 
problems precise analog output desired binary classification linear output units best choice studied problems kind 
add hidden units network 
new hidden unit receives connection network original inputs pre existing hidden unit 
hidden unit input weights frozen time unit added net output connections trained repeatedly 
new unit adds new unit layer network incoming weights happen zero 
leads creation powerful high order feature detectors may lead deep networks high fan hidden units 
number possible strategies minimizing network depth fan new units added 
currently exploring strategies 
learning algorithm begins hidden units 
direct input output connections trained possible entire training set 
need back propagate hidden units widrow hoff delta rule perceptron learning algorithm known learning algorithms single layer networks 
simulations quickprop algorithm described earlier train output weights 
hidden units quickprop acts essentially delta rule converges faster 
point training approach asymptote 
significant error reduction occurred certain number training cycles controlled patience parameter set user run network time entire training set measure error 
satisfied network performance residual error want reduce 
attempt achieve adding new hidden unit network unit creation algorithm described 
new unit added net input weights frozen output weights trained quickprop 
cycle repeats error acceptably small give 
create new hidden unit candidate unit receives trainable input connections network external inputs pre existing hidden units 
output candidate unit connected active network 
run number passes examples training set adjusting candidate unit input weights pass 
goal adjustment maximize inputs inputs inputs initial state hidden units add hidden unit add hidden unit outputs outputs outputs cascade architecture initial state adding hidden units 
vertical lines sum incoming activation 
boxed connections frozen connections trained repeatedly 
sum output units magnitude correlation candidate unit value eo residual output error observed unit define vp ep eo network output error measured training pattern 
quantities eo values eo averaged patterns 
order maximize compute wi partial derivative respect candidate unit incoming weights wi 
manner similar derivation back propagation rule rumelhart expand differentiate formula get wi ep eo ii sign correlation candidate value output derivative pattern candidate unit activation function respect sum inputs ii input candidate unit receives unit pattern computing wi incoming connection perform gradient ascent maximize training single layer weights 
quickprop update rule faster convergence 
stops improving install new candidate unit active network freeze input weights continue cycle described 
absolute value formula candidate unit cares magnitude correlation error output sign correlation 
rule hidden unit correlates positively error unit develop negative connection weight unit attempting cancel error correlation negative output weight positive 
unit weights different outputs may mixed sign unit serve purposes developing positive correlation error output negative correlation error 
single candidate unit possible pool candidate units different set random initial weights 
receive input signals see residual error training pattern 
interact affect active network training candidate units trained parallel decide progress install candidate correlation score best 
pool candidates beneficial ways greatly reduces chance useless unit permanently installed individual candidate got stuck training parallel machine speed training parts weight space explored simultaneously 
simulations run typically small pools candidate units ensure candidates equally pool 
strictly speaking covariance true correlation formula leaves normalization terms 
early versions system true correlation version worked better situations 
training points spirals problem output pattern network trained cascade correlation 
hidden candidate units may type example sigmoid activation function 
alternatively create pool candidate units mixture nonlinear activation functions sigmoid gaussian radial activation functions compete chosen addition active network 
resulting networks mixture unit types adapted specifically problem hand may lead compact elegant solutions possible homogeneous networks 
date explored sigmoid gaussian cases extensive simulation data networks mixed unit types 
final note implementation algorithm weights output layer trained weights active network frozen 
candidate weights trained weights active network changed 
machine plenty memory possible record unit values output errors entire epoch cached values repeatedly training recomputing training case 
result tremendous speedup especially large networks 

benchmark results 
spirals problem spirals benchmark chosen primary benchmark study extremely hard problem algorithms back propagation family solve 
proposed wieland mitre net continuous valued inputs single output 
training set consists values half produce output half output 
training points arranged interlocking spirals go origin times shown 
goal develop feed forward network sigmoid units properly classifies training cases 
hidden units obviously needed single linear separator divide sets twisted way 
wieland unpublished reported modified version backprop mitre required epochs solve problem obtained solution standard backprop 
lang witbrock lang tried problem network hidden layers units 
network unusual provided shortcut connections unit received incoming connections unit earlier layer just immediately preceding layer 
architecture standard backprop able solve problem epochs backprop modified error function required epochs quickprop required epochs 
best spirals performance reported date 
lang witbrock report obtaining solution net hidden units required quickprop epochs train network 
ran problem times cascade correlation algorithm sigmoidal activation function output hidden units pool candidate units 
trials successful requiring epochs average 
number counts epochs train output weights epochs train candidate units 
number hidden units built net varied average median 
histogram number hidden units created hidden number units trials terms training epochs cascade correlation beats quickprop factor standard backprop factor building network complexity hidden units 
terms actual computation serial machine speedup greater numbers suggest reasons backprop quickprop training case requires forward backward pass connections network cascade correlation requires forward pass 
cascade correlation training epochs run network smaller final size 
strategy described possible avoid repeatedly re computing unit values parts network changing 
suppose epochs measure learning time connection crossings defined number multiply accumulate steps necessary propagate activation values forward network error values backward 
measure leaves computational steps sigmoid computations reasonably accurate measure relative computational cost accurate comparing epochs different sizes comparing runtimes different machines 
evolution hidden unit solution spirals problem 
evolution hidden unit solution spirals problem continued 
lang witbrock result backprop epochs requires connection crossings 
solution quickprop epochs network requires crossings 
average cascade correlation run pool candidate units requires crossings fold speedup quickprop fold speedup standard backprop 
smaller pool candidate units speedup serial machine greater resulting networks somewhat larger 
shows output hidden unit network built cascade correlation input scanned field 
network properly classifies training points 
see interpolates smoothly turns spiral bit farther training points farther apart 
receptive field diagram similar obtained lang witbrock backprop somewhat smoother 
figures show evolution hidden unit network new units added 
pair pictures shows output network built far just prior addition new unit receptive field unit added 
black areas indicate network unit strongly negative white areas strongly positive gray areas indicate intermediate output close zero 
units follow turns spirals smooth regular manner 
new unit builds earlier ones create receptive field wraps farther origin 
regular progression breaks units opportunistically grab chunks remaining error wrapping receptive field visible 

input parity parity popular benchmark researchers ran cascade correlation input parity problems ranging 
best results obtained sigmoid output unit hidden units output gaussian function sum weighted inputs 
confused spherical ellipsoidal gaussian units researchers 
trials value results follows cases hidden average units epochs rough comparison tesauro janssens tesauro report standard backprop takes epochs input parity 
quickprop epochs 
results obtained network hidden units cascade correlation builds compact network making effective shortcut connections 
test generalization ran trials cascade correlation input parity problem training patterns testing rest 
note nearest neighbor algorithm poorly test generally getting test cases wrong 
gaussian hidden units 
results individual trials follows 
discussion train test hidden train test cases cases units epochs errs errs believe cascade correlation algorithm offers advantages network learning algorithms currently need guess size depth connectivity pattern network advance 
reasonably small optimal net built automatically 
may possible build networks mixture nonlinear unit types 
cascade correlation learns fast 
backprop hidden units engage complex dance settle distinct useful roles cascade correlation unit sees fixed problem move decisively solve problem 
problems investigated date learning time epochs grows roughly log number hidden units ultimately needed solve problem 
cascade correlation build deep nets high order feature detectors dramatic slowdown see back propagation networks hidden layers 
cascade correlation useful incremental learning new information added trained net 
built feature detector 
available time producing outputs complex features 
training new set examples may alter network output weights quickly restored return original problem 
time train layer weights network 
rest network changing results cached 
need propagate error signals backwards network connections 
single residual error signal broadcast candidates 
weighted connections transmit signals direction eliminating troublesome difference backprop connections biological synapses 
candidate units interact pick winner 
candidate sees inputs error signals time 
limited communication architecture attractive parallel implementation 

relation principal differences cascade correlation older learning architectures dynamic creation hidden units way stack new units multiple layers fixed output layer freezing units add net way train new units hill climbing maximize candidate unit correlation residual error 
interesting discovery training unit time training network speed learning process considerably creating reasonably small net generalizes 
number researchers ash moody investigated networks add new units receptive fields single layer course learning 
single layer systems suited problems systems incapable creating higher order feature detectors combine outputs existing units 
kinds problems higher order features may advantageous 
idea building feature detectors freezing inspired part waibel modular neural networks speech waibel waibel model size structure sub network fixed experimenter learning begins 
know attempts build multi layer networks course training 
decision look models unit see pre existing units inspired extent progressively deepening threshold logic models furst jeff jackson carnegie mellon 
actively pursuing line 
gallant gallant briefly mentions progressively deepening perceptron model inverted pyramid model units frozen installed 
concentrated research effort models new hidden units generated random deliberate training process 
model lee builds multiple layer topology suit problem hand 
algorithm places new input units randomly selected locations simulated annealing search keep useful ones different approach 
acknowledgments furst paul david touretzky asking questions helped shape 
ash ash 
dynamic node creation back propagation networks technical report institute cognitive science university california san diego 
becker becker lecun 
improving convergence back propagation learning second order methods proceedings connectionist models summer school morgan kaufmann 
fahlman fahlman 
faster learning variations back propagation empirical study proceedings connectionist models summer school morgan kaufmann 

speech recognition back propagation proceedings th annual conference ieee engineering medicine biology society 
gallant gallant 
constructive algorithms network learning proceedings th annual conference cognitive science society 
jacobs jacobs 
increased rates convergence learning rate adaptation tech report coins tr university massachusetts amherst dept cis 
kramer kramer sangiovanni vincentelli 
efficient parallel learning algorithms neural networks touretzky ed advances neural information processing systems morgan kaufmann 
lang lang witbrock 
learning tell spirals apart proceedings connectionist models summer school morgan kaufmann 
lapedes lapedes farber nonlinear signal prediction system modelling los alamos national laboratory technical report la ur 
moody moody 
fast learning multi resolution hierarchies touretzky ed advances neural information processing systems morgan kaufmann 
rumelhart rumelhart hinton williams 
learning internal representations error propagation rumelhart mcclelland parallel distributed processing explorations microstructure cognition mit press 
lee 
self organizing neural nets identification problem touretzky ed advances neural information processing systems morgan kaufmann 
tesauro tesauro janssens 
scaling relations back propagation learning complex systems 
waibel waibel 
consonant recognition modular construction large phonemic time delay neural networks touretzky ed advances neural information processing systems morgan kaufmann 
watrous watrous 
learning algorithms connectionist networks applied gradient methods nonlinear optimization tech report ms cis university pennsylvania dept cis 

