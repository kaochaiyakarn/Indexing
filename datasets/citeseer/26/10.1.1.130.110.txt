gtm generative topographic mapping christopher bishop markus svens microsoft research thomson avenue cambridge cb fb microsoft com research microsoft com christopher williams institute adaptive neural computation division informatics university edinburgh forrest hill edinburgh eh ql scotland dai ed ac uk published generative topographic mapping neural computation latent variable models represent probability density data space dimensions terms smaller number latent hidden variables 
familiar example factor analysis linear transformations latent space data space 
introduce form non linear latent variable model called generative topographic mapping parameters model determined em algorithm 
gtm provides principled alternative widely self organizing map som kohonen overcomes significant limitations som 
demonstrate performance gtm algorithm toy problem simulated data flow diagnostics multi phase oil pipeline 
copyright mit press 
data sets exhibit significant correlations variables 
way capture structure model distribution data terms latent hidden variables 
familiar example approach factor analysis linear transformation latent space data space 
show latent variable framework extended allow non linear transformations remaining computationally tractable 
leads gtm generative topographic mapping algorithm constrained mixture gaussians parameters optimized em expectation maximization algorithm 
motivations provide principled alternative widely selforganizing map som algorithm kohonen set unlabelled data vectors tn dimensional data space summarized terms set vectors having spatial organization corresponding generally dimensional sheet 
algorithm achieved successes practical applications suffers significant deficiencies highlighted kohonen 
include absence cost function lack theoretical basis choosing learning rate parameter schedules neighbourhood parameters ensure topographic ordering absence general proofs convergence fact model define probability density 
problems traced heuristic origins som algorithm show gtm algorithm overcomes limitations som introducing significant disadvantages 
important application latent variable models data visualization 
models visualization regarded defining projection dimensional data space dimensional visualization space 
shall see contrast gtm model defined terms mapping latent space data space 
purposes data visualization mapping inverted bayes theorem giving rise posterior distribution latent space 
latent variables goal latent variable model find representation distribution data dimensional space td terms number latent variables xl 
achieved considering function maps points latent space corresponding points data space 
mapping governed matrix parameters consist example feed forward neural network case represent weights biases 
interested situation dimensionality latent variable space dimensionality data space wish capture fact data intrinsic dimensionality transformation maps latent variable space dimensional non euclidean manifold embedded data space illustrated schematically case 
define probability distribution latent variable space induce corresponding distribution data space 
shall refer prior distribution reasons clear shortly 
distribution space biological metaphor invoked motivating som procedure 
stressed goal neuro biological modelling development effective algorithms data analysis biological realism need considered 
assume matrix partial derivatives yk xi full column rank 
non linear function defines manifold embedded data space image latent variable space mapping confined dimensional manifold singular 
reality data approximately live lower dimensional manifold appropriate include noise model vector 
choose distribution radially symmetric gaussian centred having variance exp 
note models appropriate bernoulli binary variables sigmoid transformation multinomial mutually exclusive classes softmax normalized exponential transformation bishop combinations 
distribution space value obtained integration distribution dx 
data set tn data points determine parameter matrix inverse variance maximum likelihood 
practice convenient maximize log likelihood ln tn 
specified prior distribution functional form mapping principle determine maximizing 
integral general analytically intractable 
choose linear function choose gaussian integral convolution gaussians gaussian 
noise distribution gaussian diagonal covariance matrix obtain standard factor analysis model 
case radially symmetric gaussian model closely related principal component analysis maximum likelihood solution columns scaled principal eigenvectors 
wish extend formalism non linear functions particular develop model similar spirit som algorithm 
consider specific form order formulate latent variable model similar spirit som consider prior distribution consisting superposition delta functions located nodes regular grid latent space 
node xi mapped corresponding point xi data space forms centre corresponding gaussian distribution 
sum delta functions centred nodes regular grid latent space xi case integral performed analytically 
point xi mapped corresponding point xi data space forms centre gaussian density function illustrated 
see distribution function data space takes form xi log likelihood function ln tn xi 
particular noise model distribution corresponds constrained gaussian mixture model hinton williams revow centres gaussians xi move independently related function 
note provided mapping function smooth continuous projected points xi necessarily topographic ordering sense points xa xb close latent space map points xa xb close data space 
em algorithm choose particular parametrized form differentiable function example feed forward network sigmoidal hidden units standard techniques non linear optimization conjugate gradients quasi newton methods find weight matrix inverse variance maximize 
model consists mixture distribution suggests seek em expectation maximization algorithm dempster laird rubin bishop 
making suitable choice model see step corresponds solution set linear equations 
particular shall choose generalized linear regression model form elements consist fixed basis functions matrix 
generalized linear regression models possess universal approximation capabilities multilayer adaptive networks provided basis functions chosen appropriately 
usual limitation models number basis functions typically grow exponentially dimensionality input space bishop 
context significant problem dimensionality governed number latent variable variables typically small 
fact data visualization applications generally 
maximization regarded missing data problem identity component generated data point tn unknown 
formulate em algorithm model follows 
suppose point algorithm current weight matrix wold current inverse noise variance old 
step wold old evaluate posterior probabilities responsibilities gaussian component data point tn bayes theorem form rin wold old xi tn wold old tn xi wold old 
tn xi wold old consider expectation complete data log likelihood form rin wold old ln tn xi 
maximizing respect obtain rin wold old xi tn xi 
conveniently written matrix notation form gold new matrix elements ij xi matrix elements matrix elements rin diagonal matrix elements gii rin 
solve standard matrix inversion techniques singular value decomposition allow possible ill conditioning 
note matrix constant algorithm need evaluated start 
similarly maximizing respect obtain re estimation formula new rin wold old xi tn nd 
em algorithm alternates step corresponding evaluation posterior probabilities step solution 
jensen inequality show iteration algorithm objective function increase local maximum discussed example bishop 
typically em algorithm gives satisfactory convergence tens cycles particularly primarily interested convergence distribution achieved rapidly convergence parameters 
desired regularization term added objective function control mapping 
interpreted map maximum posteriori estimator corresponding choice prior weights case radially symmetric gaussian prior form md exp jk regularization coefficient leads modification step give gold identity matrix 
data visualization new application gtm data visualization bayes theorem invert transformation latent space data space 
particular choice prior distribution posterior distribution sum delta functions centred lattice points coefficients responsibilities rin 
coefficients provide visualization posterior responsibility map individual data points dimensional latent space 
desired visualize set data points complete posterior distribution data point may provide information convenient summarize posterior mean data point tn tn tn 
borne mind posterior distribution multi modal case posterior mean give misleading summary true distribution 
alternative approach evaluate mode distribution max arg max rin 
practice convenient plot mean mode data point significant differences indicative multi modal distribution 
choice model parameters problem density estimation finite data set fundamentally ill posed exist infinitely distributions rise observed data 
algorithm examples manifolds generated sampling prior distribution showing effect choice basis functions smoothness manifold 
basis functions gaussian width left hand plot spacing basis function centres right hand plot 
different values simply affect linear scaling embedded manifold 
density modelling requires form prior knowledge addition data set 
assumption distribution described terms reduced number latent variables part prior 
gtm algorithm prior distribution mapping functions governed prior weights example basis functions 
typically choose basis functions radially symmetric gaussians centres distributed uniform grid space common width parameter value number spacing basis functions determines smoothness manifold 
examples surfaces generated sampling prior shown 
addition basis functions necessary select latent space sample points xi 
note sample points relation number basis functions gaussian mixture centres data space relatively independent desired smoothness properties lost 
having large number sample points causes difficulty increased computational cost 
particular fitting number sample points increased number degrees freedom model controlled mapping function 
way view role latent space samples xi monte carlo approximation integral mackay bishop svens williams 
choice number location sample points xi latent space critical typically choose gaussian basis functions set case dimensional latent space sample points lie centre basis function 
note considered basis function parameters widths locations fixed gaussian prior weight matrix principle priors basis function parameters introduced treated map estimation bayesian integration 
initialize parameters gtm model initially approximates principal component analysis 
evaluate data covariance matrix obtain second principal eigenvectors determine minimizing error function xi columns eigenvectors 
represents sum squares error projections latent points data space gtm model corresponding projections obtained pca 
value initialized larger eigenvalue pca representing variance data away pca plane square half grid spacing pca projected latent points data space 
note numerical implementation care taken evaluation responsibilities involves computing exponentials distances projected latent points data points may span significant range values 
summary gtm algorithm foregoing discussion somewhat detailed underlying gtm algorithm straightforward summarized convenience 
gtm consists constrained mixture gaussians model parameters determined maximum likelihood em algorithm 
defined specifying set points xi latent space set basis functions 
adaptive parameters define constrained mixture gaussians centres xi common covariance matrix initializing training involves alternating step posterior probabilities evaluated step re estimated respectively 
evaluation log likelihood cycle monitor convergence 
experimental results results application algorithm toy problem involving data dimensions realistic problem involving dimensional data arising diagnostic measurements oil flows multi phase pipelines 
examples choose basis functions radially symmetric gaussians centres distributed uniform grid space common width parameter chosen equal twice separation neighbouring basis function centres 
results toy problem case dimensional data space dimensional latent space shown 
oil flow data second example arises problem determining fraction oil multi phase pipeline carrying mixture oil water gas bishop james 
data point consists measurements taken dual energy gamma measuring attenuation gamma beams passing pipe 
synthetically generated data models accurately attenuation processes pipe presence noise arising photon statistics 
phases pipe oil water gas belong different geometrical configurations corresponding laminar homogeneous annular flows results toy problem involving data generated dimensional curve embedded dimensions projected latent points gaussian noise distributions filled circles 
initial configuration determined principal component analysis shown left converged configuration obtained iterations em shown right 
data set consists points drawn equal probability configurations 
take latent variable space dimensional goal data visualization 
shows oil data visualized latent variable space data point plotted posterior mean vector 
point labelled multi phase configuration 
comparison shows corresponding results obtained principal component analysis 
relation self organizing map motivation gtm provide principled alternative self organizing map useful consider precise relationship gtm som 
focus attention batch versions algorithms helps relationship particularly clear 
batch version som algorithm kohonen described follows 
set vectors zi defined data space vector associated node regular lattice typically dimensional feature map analogous latent space gtm 
algorithm begins initializing vectors example setting random values setting equal random subset data points principal component analysis 
cycle algorithm proceeds follows 
data vector tn corresponding winning node identified corresponding vector zj having smallest euclidean distance zj tn tn 
vectors updated setting equal weighted averages data points zi hij tn 
ij hij neighbourhood function associated ith node 
generally chosen uni modal function feature map coordinates centred winning node example gaussian 
steps identifying winning nodes updating vectors left plot shows posterior mean projection oil flow data latent space gtm model plot right shows data set visualized principal component analysis 
plots crosses circles plus signs represent stratified annular homogeneous multi phase configurations respectively 
note non linearity gtm gives improved separation clusters 
repeated iteratively 
key ingredient algorithm width neighbourhood function hij starts relatively large value gradually reduced iteration 
kernel versus linear regression pointed cherkassky value neighbourhood function hij depends identity winning node value corresponding data vector tn 
perform partial sums groups gj data vectors assigned node re write form zi mj mean vectors group gj mj tn nj gj nj number data vectors group gj 
result analogous nadaraya watson kernel regression formula nadaraya watson kernel functions kij hij nj 
batch som algorithm replaces vectors cycle convex combination node means mj coefficients determined neighbourhood function 
note kernel coefficients satisfy kij gtm algorithm centres xi gaussian components regarded analogous vectors zi som 
evaluate xi solving step example effective kernel fij plotted function node node oil flow data set iterations em 
kernel function analogous normalized neighbourhood function som algorithm 
equation find xi xi 
define weighted means data vectors rin obtain xi fij introduced effective kernel fij fij xi xj gjj 
note effective kernel satisfies fij 
see show fij xj xi 
basis functions corresponds bias const result follows 
solution xi interpreted weighted squares regression mardia kent bibby target vectors weighting coefficients gjj 
shows example effective kernel gtm corresponding oil flow problem discussed section 
see gtm som regarded forms kernel smoothers 
key differences 
som vectors smoothed defined correspond hard assignments data points nodes corresponding vectors gtm involve soft assignments weighted posterior probabilities 
analogous distinction means clustering hard assignments fitting standard gaussian mixture model em soft assignments 
second key difference kernel function som shrink course algorithm arbitrary hand crafted manner 
gtm posterior probability distribution latent space data point forms localised bubble radius bubble shrinks automatically training shown 
responsibility bubble governs extent individual data points contribute vectors updating gaussian centres xi 
examples posterior probabilities responsibilities rin latent space points early stage left intermediate stage centre late stage right convergence gtm algorithm 
evaluated single data point training set oil flow problem discussed section plotted non linear scaling form tn highlight variation latent space 
notice responsibility bubble governs updating weight matrix updating data space vectors xi shrinks automatically learning process 
comparison gtm som significant difference gtm som algorithms gtm defines explicit probability density mixture distribution 
consequence defined objective function log likelihood convergence local maximum objective function guaranteed em algorithm dempster laird rubin 
provides direct means compare different choices model parameters compare gtm solution density model evaluating likelihood test set generative distributions respective models 
som algorithm probability density defined objective function minimized training process 
proven erwin obermayer schulten objective function exist som 
limitation som highlighted kohonen page conditions called self organization som occurs quantified practice necessary confirm empirically trained model desired spatial ordering 
contrast neighbourhood preserving nature gtm mapping automatic consequence choice continuous function 
similarly smoothness properties som determined indirectly choice neighbourhood function way changed course algorithm difficult control 
prior knowledge form map easily specified 
prior distribution gtm controlled directly properties smoothness governed explicitly basis function parameters illustrated 
consider relative computational costs gtm som algorithms 
problems involving data high dimensional spaces dominant computational cost gtm arises evaluation euclidean distances data point gaussian centre xi 
exactly calculations done som involving distances data points vectors expect iteration algorithm take approximately time 
empirical comparison computational cost gtm som obtained running algorithm oil flow data convergence defined discernible change appearance visualization map 
gtm algorithm took sec 
iterations batch som took sec 
iterations gaussian neighbourhood function 
simple top hat neighbourhood function vector updated iteration data points associated nearby vectors cpu time som algorithm reduced sec 
iterations 
potential advantage gtm practical applications arises reduction number experimental training runs needed convergence topographic ordering guaranteed 
relation algorithms algorithms published literature close links gtm 
review briefly significant 
elastic net algorithm durbin willshaw viewed gaussian mixture density model fitted penalized maximum likelihood 
penalty term encourages centres gaussians corresponding neighbouring points typically dimensional chain close data space 
differs gtm define continuous data space manifold 
training algorithm generally involves hand crafted annealing weight penalty coefficient 
similarities gtm principal curves principal surfaces hastie stuetzle leblanc tibshirani involve stage algorithm consisting projection followed smoothing generative models 
interesting note hastie stuetzle propose reducing spatial width smoothing function learning manner analogous shrinking neighbourhood function som 
modified form principal curves algorithm tibshirani introduces generative distribution mixture gaussians defined likelihood function trained em algorithm 
number gaussian components equal number data points smoothing imposed penalizing likelihood function addition derivative regularization term 
technique parametrized self organizing maps involves fitting standard som model data set finding manifold data space interpolates vectors ritter 
defines continuous manifold interpolating surface form part training algorithm basic problems som discussed section remain 
som vector quantization 
context shown re formulation vector quantization problem luttrell buhmann hnel luttrell avoid problems som procedure discussed earlier 
density network model mackay involves transforming simple distribution latent space complex distribution data space propagation non linear network 
discrete distribution latent space interpreted approximate monte carlo integration latent variables needed define data space distribution 
gtm seen particular instance framework sampling latent space regular stochastic specific form non linearity model parameters adapted em 
discussion introduced form non linear latent variable model trained efficiently em algorithm 
viewed topographic mapping algorithm key property defines probability density model 
example significance having probability density consider important practical problem dealing missing values data set components data vectors tn unobserved 
missing values missing random little rubin likelihood function obtained integrating unobserved values 
gtm model integrations performed analytically leading simple modification em algorithm 
consequence having probabilistic approach straightforward consider mixture gtm models 
case density written represents rth model set independent parameters mixing coefficients satisfying 
straightforward extend em algorithm maximize corresponding likelihood function 
gtm algorithm extended ways instance allowing independent mixing coefficients prior probabilities gaussian components estimated straightforward extension em algorithm 
independent parameters determined smooth functions latent variables normalized exponential applied generalized linear regression model case step em algorithm involve non linear optimization 
similarly inverse noise variance generalized function important property gtm existence smooth manifold data space allows local magnification factor latent data space evaluated function latent space coordinates techniques differential geometry bishop svens williams bishop svens williams 
defined likelihood function straightforward principle introduce priors model parameters discussed section bayesian techniques place maximum likelihood 
focussed batch version gtm algorithm training data update model parameters 
applications convenient consider sequential adaptation data points time 
minimizing differentiable cost function sequential algorithm obtained appealing robbins monro procedure robbins monro bishop find zero objective function gradient 
alternatively sequential form em algorithm titterington smith makov 
web site gtm provided www ncrg aston ac uk gtm includes postscript files relevant papers software implementation matlab implementation development example data sets development gtm algorithm 
supported epsrc gr neural networks visualization high dimensional data 
geoffrey hinton iain michael tipping useful discussions 
markus svens staff sans group stockholm hospitality part project 
bishop 

neural networks pattern recognition 
oxford university press 
bishop james 
analysis multiphase flows dual energy gamma neural networks 
nuclear instruments methods physics research 
bishop svens williams 
fast em algorithm latent variable density models 
touretzky mozer hasselmo eds advances neural information processing systems volume pp 

mit press 
bishop svens williams 
magnification factors gtm algorithm 
proceedings iee fifth international conference artificial neural networks cambridge pp 

bishop svens williams 
magnification factors som gtm algorithms 
proceedings workshop self organizing maps helsinki finland 
buhmann hnel 
vector quantization complexity costs 
ieee transactions information theory 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
durbin willshaw 
analogue approach travelling salesman problem 
nature 
erwin obermayer schulten 
self organizing maps ordering convergence properties energy functions 
biological cybernetics 
hastie stuetzle 
principal curves 
journal american statistical association 
hinton williams revow 
adaptive elastic models character recognition 
moody hanson lippmann eds advances neural information processing systems volume pp 

morgan kauffmann 
kohonen 

self organized formation topologically correct feature maps 
biological cybernetics 
kohonen 

self organizing maps 
berlin springer verlag 
leblanc tibshirani 
adaptive principal surfaces 
journal american statistical association 
little rubin 
statistical analysis missing data 
new york john wiley 
luttrell 

derivation class training algorithms 
ieee transactions neural networks 
luttrell 

bayesian analysis self organizing maps 
neural computation 
mackay 

bayesian neural networks density networks 
nuclear instruments methods physics research 
mardia kent bibby 
multivariate analysis 
academic press 
cherkassky 
self organization iterative kernel smoothing process 
neural computation 
nadaraya 


estimating regression 
theory probability applications 
ritter 

parametrized self organizing maps 
proceedings icann international conference artificial neural networks amsterdam pp 

springer verlag 
robbins monro 
stochastic approximation method 
annals mathematical statistics 
tibshirani 

principal curves revisited 
statistics computing 
titterington smith makov 
statistical analysis finite mixture distributions 
new york john wiley 
watson 

smooth regression analysis 
indian journal statistics 
series 

