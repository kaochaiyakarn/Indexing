similarity search high dimensions hashing gionis piotr indyk rajeev motwani department computer science stanford university stanford ca indyk cs stanford edu nearest near neighbor query problems arise large variety database applications usually context similarity searching 
late increasing interest building search index structures performing similarity search high dimensional data image databases document collections time series databases genome databases 
unfortunately known techniques solving problem fall prey curse dimensionality 
data structures scale poorly data dimensionality fact number dimensions exceeds searching trees related structures involves inspection large fraction database doing better brute force linear search 
suggested selection features choice distance metric typical applications heuristic determining approximate nearest neighbor su ce practical purposes 
examine novel scheme approximate similarity search hashing 
basic idea hash points supported navy nsf iis 
supported stanford graduate fellowship nsf nyi award ccr 
supported aro muri nsf iis nsf young investigator award ccr matching funds ibm mitsubishi schlumberger foundation shell foundation xerox 
permission copy fee part material granted provided copies distributed direct commercial advantage vldb copyright notice title publication date appear notice copying permission large data base endowment 
copy republish requires fee special permission endowment 
proceedings th vldb conference edinburgh scotland 
database ensure probability collision higher objects close far apart 
provide experimental evidence method gives signi cant improvement running time methods searching highdimensional spaces hierarchical tree decomposition 
experimental results indicate scheme scales relatively large number dimensions 
similarity search problem involves collection objects documents images characterized collection relevant features represented points high dimensional attribute space queries form points space required nd nearest similar object query 
particularly interesting studied case dimensional euclidean space 
problem major importance variety applications examples data compression databases data mining information retrieval image video databases machine learning pattern recognition statistics data analysis :10.1.1.40.9013:10.1.1.108.8490
typically features objects interest represented points distance metric measure similarity objects 
basic problem perform indexing similarity searching query objects 
number features dimensionality ranges tens thousands 
example multimedia applications ibm qbic query image content number features hundreds 
information retrieval text documents vector space representations involve thousands dimensions considered dramatic improvement dimension reduction techniques karhunen loeve transform known principal components analysis latent semantic indexing reduce dimensionality mere hundreds :10.1.1.108.8490
low dimensional case say equal solved main issue dealing large number dimensions called curse dimensionality 
despite decades intensive ort current solutions entirely satisfactory fact large theory practice provide little linear algorithm compares query point database 
particular shown empirically theoretically current indexing techniques space partitioning degrade linear search su ciently high dimensions 
situation poses serious obstacle development large scale similarity search systems 
imagine example search engine enables contentbased image retrieval world wide web 
system index signi cant fraction web number images index order tens hundreds 
clearly indexing method exhibiting linear close linear dependence data size manage data set 
premise cases necessary insist exact answer determining approximate answer refer section formal de nition 
observation underlies large body research databases including random sampling histogram estimation median approximation wavelets selectivity estimation approximate svd 
observe applications nearest neighbor search approximate answer 
example happens see relevant answers closer query point irrelevant ones fact desirable property similarity measure 
cases approximate algorithm suitable approximation factor return result exact algorithm 
situations approximate algorithm provides user time quality tradeo user decide spend time waiting exact answer satis ed quicker approximation see :10.1.1.145.538
arguments rely assumption approximate similarity search performed faster exact 
show case 
speci cally weintroduce new indexing method approximate nearest neighbor truly sublinear dependence data size high dimensional data 
space partitioning relies new method called hashing lsh 
key idea hash points hash functions ensure function probability collision higher objects close far apart 
deter mine near neighbors hashing query point retrieving elements stored buckets containing point 
provide locality sensitive hash functions simple easy implement naturally extended dynamic setting insertion deletion operations need supported 
focused euclidean spaces di erent lsh functions similarity measures dot product :10.1.1.145.538
locality sensitive hashing introduced indyk motwani purposes devising main memory algorithms nearest neighbor search particular enabled achieve worst case dn time approximate nearest neighbor query point database 
improve technique achieve signi cantly improved query time dn 
yields approximate nearest neighbor algorithm running sublinear time 
furthermore generalize algorithm analysis case external memory 
support theoretical arguments empirical evidence 
performed experiments data sets 
rst contains histograms color images histogram represented point ind dimensional space 
second contains points representing texture information blocks large aerial photographs 
tables stored disk 
compared performance algorithm performance sphere rectangle tree sr tree data structure shown comparable signi cantly cient tree decomposition indexing methods spatial data 
experiments show algorithm signi cantly faster earlier methods cases orders magnitude 
scales data size dimensionality increase 
enables new approach high performance similarity search fast retrieval approximate answer possibly followed slower accurate computation cases user satis ed approximate answer 
rest organized follows 
section introduce notation give formal de nitions similarity search problems 
section describe locality sensitive hashing show apply nearest neighbor search 
section report results experiments lsh 
related described section 
section ideas research 
preliminaries denote euclidean space lp norm length vector xd de ned jx dp jjp qjjp denotes distance points denote hamming metric space dimension space binary vectors length standard hamming metric 
dh denote hamming distance number bits di er 
nearest neighbor search problem de ned follows de nition nearest neighbor search nns set objects represented normed space preprocess ciently answer queries nding point closest query point de nition generalizes naturally case want return points 
speci cally nearest neighbors search nns wish return points database closest query point 
approximate version nns problem de ned follows de nition nearest neighbor search nns set points normed space ld est integer 
easily veri ed choosing proper parameters error induced rounding cess ciently return point arbitrarily small 
notice oper query point ation minimum interpoint distance 
distance closest point locality sensitive hashing de nition generalizes naturally nding approximate nearest neighbors 
approximate nns problem wish nd points pk distance pi query times distance ith nearest point 
algorithm section cient solutions approximate versions nns problem 
signi cant loss generality assumptions data 
distance de ned norm see comments 
coordinates points positive integers 
rst assumption restrictive usually clear advantage di erence norm similarity search 
example experiments done webseek project see chapter show comparing color histograms norms yields similar results marginally better 
data sets see section similar property 
speci cally observed nearest neighbor average query point computed norm approximate neighbor norm average value observation holds data sets 
cases queries rst set second set nearest neighbors norms exactly 
observation interesting right partially explained theorem see 
showed analytically simply applying scaling random rotation space distances induced norms equal arbitrarily small factor 
plausible real data randomly rotated di erence norm small 
data sets property hold guaranteed performing scaling random rotation algorithms norm arbitrarily small loss precision 
far second assumption concerned clearly coordinates positive properly translating origin convert coordinates integers multiplying suitably large number rounding near section locality sensitive hashing lsh 
technique originally introduced indyk motwani purposes devising main memory algorithms nns problem 
give improved version algorithm 
new algorithm respects natural earlier require hash buckets store point better running time guarantees analysis generalized case secondary memory 
largest coordinate points embed hamming cube hd cd transforming point xd binary vector unary unary xd unary denotes unary representation sequence ones followed zeroes 
fact pair points coordinates setf cg dh embedding preserves distances points 
sequel concentrate solving nns hamming space hd 
emphasize need convert data unary representation expensive large fact algorithms run time independent unary representation provides convenient framework description algorithms complicated 
de ne hash functions follows 
integer speci ed choose subsets il ji denote projection vector coordinate set compute selecting coordinate positions concatenating bits positions 
denote gj preprocessing store bucket gj total may large compress buckets resorting standard hashing 
levels hashing lsh function maps point bucket gj standard hash function maps contents buckets hash table size maximal bucket size hash table denoted algorithm analysis assume hashing chaining number points bucket exceeds new bucket size allocated linked old bucket 
implementation employ chaining relies simpler approach bucket index full new point added added index high probability 
saves overhead maintaining link structure 
number points size hash table maximum bucket size related equation memory utilization parameter ratio memory allocated index size data set 
process query search indices gl encounter points speci ed indices 
clearly number disk accesses upper bounded number indices equal pt points encountered process 
approximate nns output points pi closest general may return fewer points number points encountered remains specify choice subsets ij 
lg set ij consists elements sampled uniformly random replacement 
optimal value chosen maximize probability point close fall bucket minimize probability point far away fall bucket 
choice values deferred section 
algorithm preprocessing input set points number hash tables output hash foreach initialize hash generating random hash function gi foreach foreach store point pj bucket gi pj hash preprocessing algorithm points embedded hamming cube 
algorithm approximate nearest neighbor query input query point number appr 
nearest neighbors access hash generated preprocessing algorithm output appr 
nearest neighbors foreach gi bucket return nearest neighbors set main memory linear search approximate nearest neighbor query answering algorithm 
mainly interested complexity scheme worth pointing hash functions ciently computed data set obtained mapping dimensional hamming space 
point data set denote image mapping 
set coordinates recall need compute ji iji denote sorted order coordinates correspond ith coordinate observe projecting iji results sequence bits monotone consists number say oi ones followed ze ros 
order represent su cient compute oi task equivalent nding number elements sorted array ji smaller value ith coordinate done binary search log time constant time precomputed array ofc bits 
total time needed compute function log depending resources 
experimental section value small resort second method 
quick summarize preprocessing query answering algorithms figures 
analysis locality sensitive hashing principle method probability collision points closely related distance 
speci cally larger distance smaller collision probability 
intuition formalized follows 
bea distance function elements set letb denote set elements distance de nition functions called sensitive 
de nition probabilities considered respect random choice function 
order locality sensitive family useful satisfy inequalities 
observe hamming distance dh family projections coordinate locality sensitive 
speci cally fact hd dimensional hamming cube dh hd 
fhi hi bd bi sensitive 
generalize algorithm previous section arbitrary locality sensitive family algorithm equally applicable locality sensitive hash functions see :10.1.1.145.538
generalization simple functions dened form gi hi hi hik functions hi randomly chosen hik replacement 
choose functions gl 
case function selects bit argument resulting values gj essentially equivalent lsh algorithm solve call neighbor problem determine exists point xed distance points database distance away rst case algorithm required return point distance particular argue lsh algorithm solves problem proper choice depending show apply solution problem solve nns 
denote set points 
observe algorithm correctly solves neighbor problem properties hold exists gj gj total number blocks pointed containing points cl assume sensitive family de ne ln correctness lsh algo ln rithm follows theorem 
theorem setting log guarantees properties hold probability 
note repeating lsh algorithm times amplify probability success trial 
proof property hold probability property hold probability 
show large 
assume exists point distance proof quite similar 
set log 
probability isat denote set points expected number blocks allocated gj contain exclusively points exceed 
expected number blocks allocated gj 
markov inequality probability number exceeds 
choose probability property holds 
consider probability gj 
clearly bounded log log log setting bound probability gj gj probability gj exists probability properties hold theorem follows 
consider lsh family hamming metric dimension speci ed fact 
case show assuming ln assumption easily satis ed increasing dimensionality padding su ciently long string point representation 
fact ln ifp ln ln proof observe ln ln ln ln ln ln multiplying numerator denominator obtain ln ln ln ln order upper bound need bound note negative 
inequalities ln ln ln ln ln ln step uses assumptions ln ln ln lnn ln return nns problem 
observe reduce neighbor problem building data structures problem di erent values specifically explore equal rmax rmax smallest largest possible distance query data point respectively 
number di erent radii reduced cost increasing running time space requirement 
hand observed practice choosing value su cient produce answers quality 
explained observed distribution distances query point data set cases depend speci query point intrinsic properties data set 
assumption distribution invariance parameter vast majority queries 
experimental section adopt xed choice experiments section report results experiments locality sensitive hashing method 
performed experiments data sets 
rst contains histograms color images corel draw library histogram represented point ind dimensional space 
second contains points dimension representing texture information blocks large large aerial photographs 
describe data sets detail section 
decided randomly chosen synthetic data experiments 
data measure performance exact similarity search algorithms unsuitable evaluation approximate algorithms high data dimensionality 
main reason follows 
assume data set consisting points chosen independently random distribution 
distributions notably uniform literature assume coordinates point chosen independently 
case pair points distance sharply concentrated mean example uniform distribution unit cube expected distance standard deviation 
pairs approximately distance notion approximate nearest neighbor meaningful point approximate nearest neighbor 
implementation 
implement lsh algorithm speci ed section 
lsh functions computed described section 
denote resulting vector coordinates vk 
second level mapping functions form vk ad vk mod size hash table ak random numbers interval 
functions computed operations su ciently random purposes give low probability collision 
second level bucket directly mapped disk block 
assumed block kb data 
coordinate data sets represented byte store dimensional points block 
assume bucket size ord 
sr tree implementation katayama available web page 
allow store coordinates disk block 
performance measures 
goal experiments estimate performance measures speed sr tree lsh accuracy lsh 
speed measured number disk blocks accessed order answer query 
count disk accesses ignoring issue caching 
observe case lsh number easy predict clearly equal number indices 
number indices determines storage overhead natural parameter optimize 
error lsh measured follows 
de ne approximate nns problem ective error jqj query denotes distance query point point lsh distance closest point sum taken queries nonempty index :10.1.1.15.3125
measure small fraction queries nonempty bucket call quantity ratio 
approximate nns measure separately distance ratios closest points nearest neighbor nd closest nd nearest neighbor average ratios 
ratio de ned fraction cases points 
data sets 
rst data set consists histograms color thumbnail sized images various contents taken corel library 
histograms extracted transforming pixels images dimensional cie lab color space property space distance pair points corresponds perceptual dissimilarity colors points represent 
partitioned color space grid smaller cubes image create color histogram image counting pixels fall cubes 
dividing axis intervals obtain total cubes 
experiments assumed obtaining dimensional space 
histogram cube color corresponds dimension space representing images 
quantization performed order coordinate byte 
point representing image coordinate effectively counts number image pixels speci color 
coordinates clearly non negative normalized frequency normalized frequency point set distance distribution interpoint distance color histograms texture data set point distribution interpoint distance texture features pro les data sets 
integers assumed section 
distribution interpoint distances point sets shown 
graphs obtained computing interpoint distances random subsets points normalizing maximal value 
second data set contains feature vectors dimension representing texture information blocks large aerial photographs 
data set provided manjunath size dimensionality provides challenging problems high dimensional indexing 
features obtained gabor ltering image tiles 
gabor lter bank consists scales orientations lters total number lters 
mean standard deviation ltered output constructed feature vector 
texture features extracted large air photos 
feature extraction rst partitioned non overlapping tiles size times feature vectors computed 
query sets 
di similarity searching algorithms lack publicly available database containing typical query points 
construct query set data set 
construction follows split data set randomly disjoint parts call 
rst data set size size 
set forms database images rst points denoted query points points various veri cation purposes 
second data set chose size remaining points query set 
numbers slightly di erent scalability experiments require varying size data set 
case chose random subset required size 
experimental results section describe results experiments 
data sets consist essentially steps 
rst phase choice value number sampled bits choose data set number indices order minimize effective error 
turned optimal value essentially independent ofn value di erent values parameters 
second phase estimate uence number indices error 
measure performance lsh computing avariety data sets minimal number indices needed achieve speci ed value error 
applicable compare performance sr trees 
color histograms data set performed experiments aimed understanding behavior lsh algorithm performance relative sr tree 
mentioned started observation optimal value sampled bits essentially independent ofn approximately equal 
lack dependence explained fact smaller data sets obtained sampling large sets similar structure believe lack dependence uenced structure data 
experiments done assuming 
observation value storage overhead exert uence performance algorithm tried interval set :10.1.1.145.538
step estimated uence results error alpha number indices error vs number indices 
shown 
expected index su cient toachieve reasonably small error ective error easily exceed 
error decreases fast increases 
due fact probabilities nding empty bucket independent di erent indices probability buckets empty decays exponentially order compare performance lsh sr tree computed variety data sets minimal number indices needed achieve specied value error equal 
investigated performance algorithms varying dimension data size 
dependence data size 
performed simulations data sets sizes 
achieve better understanding scalability algorithms run experiments twice approximate nns approximate nns 
results 
notice strongly sublinear dependence exhibited lsh small matches sr tree blocks accessed requires accesses data set times larger 
time activity sr tree increases 
larger errors lsh curves nearly exhibit little dependence size data 
similar better behavior occurs approximate nns 
computed ratios fraction queries answer 
results 
parameters previous experiment 
observe say approximate nns ratios quite high small decrease disk accesses disk accesses sr tree lsh error lsh error lsh error lsh error alpha nns number database points approximate nns sr tree lsh error lsh error lsh error lsh error alpha nns number database points approximate nns number indices vs data size 
dependence dimension 
performed simulations choice limited cubes natural numbers way data created 
performed comparison approximate nns approximate nns results shown 
note lsh scales increase dimensionality change tod increases number indices 
ratio dimensions 
completes comparison lsh 
better understanding behavior lsh performed additional experiment 
presents performance lsh number nearest neighbors retrieve vary 
ratio ratio alpha nns error error number database points approximate nns alpha nns error error number database points approximate nns ratio vs data size 
texture features experiments texture feature data designed measure performance lsh algorithm large data sets note size texture le points order magnitude larger size histogram data set points 
rst step choice number sampled bits similar previous experiment skip detailed description 
just state assumed number sampled bits parameters storage overhead block size number nearest neighbors equal 
stated value equal 
varied number indices resulted error see 
shape curve similar previous experiment 
ratio roughly disk accesses disk accesses sr tree lsh error lsh error lsh error lsh error alpha nns dimensions approximate nns sr tree lsh error lsh error lsh error lsh error alpha nns dimensions approximate nns number indices vs dimension 
indices indices 
compare sr tree implemented random subsets data set sizes 
average number blocks accessed query sr tree orders magnitude larger number blocks accessed algorithm see show running times lsh ective error 
observe sr tree computes exact answers lsh provides approximation 
order perform accurate evaluation lsh decided compare modi ed sr tree algorithm produces approximate answers 
modi cation simple running sr tree data set run randomly chosen subset 
way achieve speed algorithm random sample data set smaller original set disk accesses alpha error error error number nearest neighbors number indices vs number nearest neighbors 
incurring error 
query cost versus error tradeo obtained way entire data set depicted include similar graph lsh 
observe random sampling results considerable speed sr tree algorithm keeping error relatively low 
case lsh algorithm ers considerably outperforms sr trees order magnitude faster 
previous considerable literature various versions nearest neighbor problem 
due lack space omit detailed description related reader advised read survey variety data structures nearest neighbors geometric spaces including variants trees trees structures space lling curves 
results surveyed see excellent survey 
theoretical nearest neighbor search brie surveyed 
novel scheme approximate similarity search locality sensitive hashing 
compared performance technique representative tree spatial data structures 
showed allowing small error additional storage overhead considerably improve query time 
experimental results indicate scheme scales large number dimensions data size 
additional advantage data structure running time disk accesses number disk accesses performance vs error sr tree lsh error sr tree lsh data set size number indices vs error number indices vs size 
essentially determined advance 
properties lsh suitable candidate high performance real time systems 
explore applications lsh type techniques data mining search copyrighted video data 
experience suggests lot potential improvement performance lsh algorithm 
example data structures created randomized procedure 
interesting systematic method performing task method take additional advantage structure data set 
believe investigation hybrid data structures obtained merging tree hashing approaches fruitful direction research 
arya mount narayan accounting boundary ects nearest neighbor searching 
discrete computational geometry pp 

arya mount netanyahu silverman wu :10.1.1.15.3125
optimal algorithm approximate nearest neighbor searching proceedings th annual acm siam symposium discrete algorithms pp 

bentley 
multidimensional binary search trees associative searching 
communications acm pp 

berchtold keim 
high dimensional index structures 
proceedings sigmod 
see www informatik uni halle de keim sigmod tutorial ps gz cohen :10.1.1.145.538
datar fujiwara gionis indyk motwani ullman 
yang 
finding interesting associations support pruning 
technical report computer science department stanford university 
chan 
approximate nearest neighbor queries revisited 
proceedings th annual acm symposium computational geometry pp 

cost salzberg 
weighted nearest neighbor algorithm learning symbolic features 
machine learning pp 

chaudhuri motwani narasayya 
random sampling histogram construction 
proceedings sigmod pp 

cover hart 
nearest neighbor pattern classi cation 
ieee transactions information theory pp 

ciaccia patella cost model similarity queries metric spaces proceedings pods pp 

deerwester dumais landauer furnas harshman :10.1.1.108.8490
indexing latent semantic analysis 
journal society information sciences pp 

devroye wagner 
nearest neighbor methods discrimination 
handbook statistics vol 
krishnaiah kanal eds north holland 
duda hart 
pattern classi cation scene analysis 
john wiley sons ny 
edelsbrunner 
algorithms combinatorial geometry 
springer verlag 
faloutsos barber flickner niblack petkovic equitz 
cient ective querying image content 
journal intelligent information systems pp 

faloutsos oard 
survey information retrieval filtering methods 
technical report cs tr department computer science university maryland college park 
flickner sawhney niblack ashley huang dom gorkani hafner lee petkovic steele yanker 
query image video content qbic system 
ieee computer pp 

friedman bentley finkel 
algorithm nding best matches logarithmic expected time 
acm transactions mathematical software pp 

lindenstrauss 
dimension spherical sections convex bodies 
acta math 

gersho gray 
vector quantization data compression 
kluwer 
hastie tibshirani 
discriminant adaptive nearest neighbor classi cation 
proceedings international conference knowledge discovery data mining pp 

hotelling 
analysis complex statistical variables principal components 
journal educational psychology 
pp 

indyk iyengar shivakumar 
finding video sequences internet 
technical report computer science department stanford university 
indyk motwani 
approximate nearest neighbor removing curse dimensionality 
proceedings th symposium theory computing pp 

ravi agrawal singh 
dimensionality reduction similarity searching dynamic databases 
proceedings sigmod 
karhunen 
uber methoden der 
ann 
acad 
sci 
ser 


nearest neighbor lters multivariate data 
ieee workshop nonlinear signal image processing 
katayama satoh 
sr tree index structure high dimensional nearest neighbor queries 
proc 
sigmod pp 

code available www rd ac jp katayama homepage research english html linial london rabinovich 
geometry graphs algorithmic applications 
proceedings th annual ieee symposium foundations computer science pp 

loeve 
fonctions de second 
processus brownian hermann paris 
manjunath 
dataset 
vivaldi 
ece ucsb edu manjunath research htm manjunath ma 
texture features browsing retrieval large image data 
ieee transactions pattern analysis machine intelligence special issue digital libraries pp 

manku rajagopalan lindsay 
approximate medians quantiles pass limited memory 
proceedings sig mod pp 

matias vitter wang 
wavelet histograms selectivity estimations 
proceedings sigmod pp 

motwani raghavan 
randomized algorithms 
cambridge university press 

approximate matching high dimensionality trees 
sc 
scholarly dept computer science univ maryland college park md 
pentland picard sclaro photobook tools content manipulation image databases 
proceedings spie conference storage retrieval image video databases ii 
salton mcgill 
modern information retrieval 
mcgraw hill book new york ny 
samet 
design analysis spatial data structures 
addison wesley reading ma 
smith 
integrated spatial feature image systems retrieval analysis compression 
ph thesis columbia university 
available ftp ftp ctr columbia edu ctr research advent public public thesis sellis roussopoulos faloutsos 
multidimensional access methods trees grown 
proceedings rd international conference onvery large data bases 
smeulders jain eds 
image databases multi media search 
proceedings international workshop idb mms amsterdam university press amsterdam 
smith chang 
visually searching web content 
ieee multimedia pp 

see disney ctr columbia 
edu webseek styles 
color science concepts methods quantitative data formulae 
john wiley sons new york ny 
weber schek blott 
quantitative analysis performance study similarity search methods high dimensional spaces 
proceedings th international conference onvery large data bases vldb pp 

