text classi cation string kernels lodhi cs rhul ac uk department computer science royal holloway university london egham surrey tw ex uk craig saunders craig cs rhul ac uk department computer science royal holloway university london egham surrey tw ex uk john taylor john cs rhul ac uk department computer science royal holloway university london egham surrey tw ex uk nello cristianini nello cs rhul ac uk department computer science royal holloway university london egham surrey tw ex uk chris watkins cs rhul ac uk department computer science royal holloway university london egham surrey tw ex uk editor leslie pack kaelbling propose novel approach categorizing text documents special kernel 
kernel inner product feature space generated subsequences length subsequence ordered sequence characters occurring text necessarily contiguously 
subsequences weighted decaying factor full length text emphasising occurrences close contiguous 
direct computation feature vector involve prohibitive amount computation modest values dimension feature space grows exponentially describes despite fact inner product ciently evaluated dynamic programming technique 
experimental comparisons performance kernel compared standard word feature space kernel joachims show positive results modestly sized datasets 
case contiguous subsequences considered comparison subsequences kernel di erent decay factors 
larger documents datasets introduces approximation technique shown deliver approximations ciently large datasets 
keywords kernels support vector machines string subsequence kernel approximating kernels text classi cation 
standard learning systems neural networks decision trees operate input data transformed feature vectors dn living space 
space data points separated surface clustered interpolated analysed 
resulting hypothesis applied test points vector space order predictions 
cases input data readily described explicit feature vectors example images graphs text documents 
datasets construction feature extraction module complex expensive solving entire problem 
feature extraction process requires extensive domain knowledge possible lose important information process 
extracted features play akey role ectiveness system 
kernel methods kms ective alternative explicit feature extraction 
building block kernel learning methods kms cristianini shawe taylor vapnik function known kernel function function returning inner product mapped data points higher dimensional space 
learning takes place feature space provided learning algorithm entirely rewritten data points appear inside dot products data points 
linear algorithms formulated way clustering classi cation regression 
known example kernel system support vector machine svm boser 
cristianini shawe taylor perceptron pca nearest neighbour algorithms property 
non dependence kms dimensionality feature space exibility kernel function choice di erent classi cation tasks especially text classi cation 
exploit important fact kernel functions de ned general sets watkins haussler assigning pair elements strings graphs images inner product feature space 
kernels necessary invoke mercer theorem directly shown inner products 
examine kernel method string alignment text categorization problems 
de ning inner products text documents general purpose algorithms rich class 
text clustered classi ed ranked builds preliminary results lodhi 

standard approach joachims text categorization classical text representation technique salton 
maps document high dimensional feature vector entry vector represents presence absence feature 
approach loses word order information retaining frequency terms document 
usually accompanied removal non informative words words replacing words stems losing ection information 
sparse vectors conjunction learning algorithms 
simple technique successfully supervised learning tasks support vector machines joachims 
propose radically di erent approach considers documents simply symbol sequences speci kernels 
approach domain knowledge sense considers document just long sequence capable capturing topic information 
feature space case generated set non contiguous substrings symbols described detail section 
substrings documents common similar considered higher inner product 
build advances watkins haussler demonstrate build kernels general structures sequences 
remarkable property methods map documents feature vectors explicitly representing means sequence alignment techniques 
dynamic programming technique computation kernels cient linear documents length 
empirically analyse approach experimental results set documents containing stories reuters news agency reuters dataset 
compare proposed approach classical text representation technique known bag words grams text representation technique demonstrating approach delivers state art performance categorization outperform bag words approach 
experimental analysis technique showed su ers practical limitations big text corpora 
establishes need develop approximation strategy 
furthermore text categorization tasks problems datasets huge 
important nd methods ciently compute gram matrix 
way reduce computation time provide method quickly computes approximation evaluating full kernel 
provided approximation gram matrix shown deviate signi cantly produced full string kernel various kernel methods applied large text datasets 
successfully approximate gram matrix considering subset features generated string kernel 
proposed alignment measure 
show deviation true gram matrix 
remarkably features needed order approximate full matrix computation time greatly reduced orders magnitude 
order show ectiveness method conduct experiment uses string subsequence kernel ssk full reuters dataset 

kernels support vector machines section reviews main ideas support vector machines svms kernel functions 
svms class algorithms combine principles statistical learning theory optimisation techniques idea kernel mapping 
introduced boser 
simplest version learn separating hyperplane sets points maximise margin distance plane closest point 
solution interesting statistical properties candidate valid generalisation 
main statistical properties maximal margin solution performance depend dimensionality space separation takes place 
way possible high dimensional spaces induced kernels tting 
classi cation case svms mapping data points high dimensional feature space linear learning machine nd maximal margin separation 
case kernels de ned space hyperplane feature space correspond nonlinear decision boundary input space 
case kernels de ned sets hyperplane simply corresponds dichotomy input set 
brie describe kernel function 
function calculates inner product mapped examples feature space kernel function mapping di dj di dj kernel function 
note kernel computes inner product implicitly mapping examples feature space 
mapping transforms dimensional example dimensional feature vector 
explicit extraction features feature space generally high computational cost kernel function handle problem 
mathematical foundation function established rst decade twentieth century mercer 
kernel function symmetric function di dj dj di matrix entries form kij di dj isknown kernel matrix 
matrix symmetric positive de nite matrix 
interesting note matrix main source information kms methods information learn classi er 
ways combining simple kernels obtain complex ones 
example kernel set vectors polynomial construction di dj di dj positiveinteger constant 
clearly incur small computational cost de ne new feature space 
feature space corresponding degree polynomial kernel includes products input features 
polynomial kernels create images examples feature spaces having huge numbers dimensions 
furthermore gaussian kernels de nes feature space nite number dimension di dj exp djk gaussian kernel allows algorithm learn linear classi er nite dimensional feature space 

kernel text sequences step words section describe kernel text documents 
idea compare means substrings contain substrings common similar 
important part substrings need contiguous degree contiguity substring document determines weight comparison 
example substring word card word di erent weighting 
substring dimension feature space value coordinate depends frequently compactly string embedded text 
order deal non contiguous substrings necessary introduce decay factor weight presence certain feature text see de nition details 
example 
consider simple documents words cat car bat bar 
ifwe consider obtain dimensional feature space words mapped follows cat car bat bar unnormalised kernel car cat car cat normalised version obtained follows car car cat cat car cat 
note general document contain word mapping document feature space catenation words spaces ignoring punctuation considered unique sequence 
example 
compute similarity parts famous line kant 
science organized knowledge wisdom organized life values kernel values interesting substring sizes normal sized documents direct computation relevant features impractical moderately sized texts explicit representation impossible 
turns kernel features de ned calculated cient way dynamic programming techniques 
derive kernel starting features working inner product 
case need prove satis es mercer conditions symmetry positive semi de niteness follow automatically de nition inner product 
kernel named string subsequence kernel ssk watkins haussler motivated bioinformatics applications 
maps strings feature vector indexed tuples characters 
tuple non zero entry occurs subsequence necessarily contiguously string 
weighting feature sum occurrences tuple decaying factor length occurrence 
de nition string subsequence kernel ssk nite alphabet 
string nite sequence characters including empty sequence 
strings denote jsj length string jsj st string obtained concatenating strings string substring si sj say subsequence ofs indices jsj uj sij forj juj short 
length subsequence 
denote set nite strings length set strings de ne feature spaces fn feature mapping string de ning coordinate 
features measure thenumber occurrences subsequences string weighting lengths 
inner product feature vectors strings give sum common subsequences weighted frequency occurrence lengths kn direct computation features involve jn time space number features involved 
clear features non zero components large documents 
order derive ective procedure computing kernel introduce additional function aid de ning recursive computation kernel 
jsj jtj counting length particular sequence strings just 
ne recursive computation compute kn de nition recursive computation subsequence kernel 
jsj jtj ki jsj jtj sx tj jtj kn sx kn tj notice need auxiliary function interior gaps subsequences penalised 
correctness recursion follows observing length strings increased incurring factor extra length unit 
formula sx rst term fewer character requiring single factor second jtj fewer characters 
formula second term requires addition just characters character sequence 
wished compute kn range values wewould simply perform computation largest required apply recursion kn needed stored values 
course create kernel combines di erent kn giving di erent positive weightings natural normalise introduced document length 
produce ect normalising feature vectors feature space 
create new embedding gives rise kernel kk cient computation ssk ssk measures similarity documents time proportional length sequence 
evident description recursion de nition outermost recursion sequence length length additional character sum sequence evaluated 
possible speed computation ssk 
cient recursive computation ssk reduce complexity computation rst evaluating sx tj jtj observing evaluate recursion observe sx sx sx tu sx provided occur sx tx sx observations give ano recursion computing 
evaluate kernel time 
grams language independent approach grams language independent text representation technique 
transforms documents high dimensional feature vectors feature corresponds contiguous substring 
grams adjacent characters substring alphabet number distinct grams text equal jaj shows dimensionality grams feature vector high moderate values grams document reducing dimensionality substantially 
example unique tri grams excluding words reuters dataset 
generally grams feature vector formation upper case characters converted lower case characters space assumed punctuation 
feature vectors normalised 
illustrated example 
example consider example compute tri gram quad gram feature vector 
support vector grams sup upp por ort rtg tgv gve vec ect cto tor grams supp port ctor 
represent space 
systems technique applied situations text su ers errors misspelling hu man 
choice optimal varies text corpora 
cient implementation speed described computation ssk cheap efcient techniques needed 
goal evaluate performance ssk conjunction svm di erent splits data required special properties software 
simple gradient implementation svms 
cristianini shawe taylor 
key success system form chunking 
start small subset data gradually build size training set ensuring points failed meet margin current hypothesis included chunk 
evaluation kernel function requires signi cant computational resources designed system calculate entries kernel matrix required training algorithm 
signi cantly reduce training time relatively small part kernel matrix implementation svm 
number kernel evaluations approximately equal size sample times number support vectors 
computed kernel entries saved reuse di erent splits data 
property possible train classi er number splits data incurring signi cant additional computational cost provided overlap support vectors split 
key idea save entries evaluated training test phase kernel matrix computed entries evaluate ssk new split data learn di erent category data 

experimental results section describe experiments emphasis experiments understanding ssk works practice 
objectives experiments observe uence variability tunable parameters length weight performance advantages combining di erent kernels 
order accomplish goals conducted series experiments subset documents reuters dataset 
reuters dataset reuters dataset contains stories reuters news agency 
reuters newer version corpus 
compiled david lewis publicly available www research att com lewis 
obtain training set test set exist di erent splits corpus 
modi ed apte split 
split comprises training test documents 
reuters category contain documents training set 
similarly test set category relevant documents 
experiments described section conducted full reuters dataset split 
mentioned experiments section performed subset reuters dataset 
set size subset computation ssk longer concern 
size subset reuters set documents documents training classi er evaluating performance learned classi er test set documents 
step choose categories 
earn acquisition frequent categories reuters dataset 
direct correspondence respective words categories crude corn potential candidates 
splits data sizes numbers positive examples training test sets numbers positive examples training testing set earn acquisition crude corn 
describe preprocessing stage ssk 
removed words occur list punctuation keeping spaces original places documents 
performance ssk compared performance standard word kernel wk grams kernel ngk wk linear kernel measures similarity documents indexed words df weighting scheme 
similarly ngk linear kernel return similarity score documents indexed grams 
order learn svm classi er conjunction wk preprocessed documents described 
words punctuation removed form documents 
weighted entries feature vectors variant df log tf log df weighting scheme 
tf represents term frequency df document frequency total number documents 
documents normalised document equal length 
describe preprocessing stage grams feature vectors 
consistency removed words punctuation 
document collection transformed feature vector entry feature vector represents number times corresponding substring occurs document 
note feature vectors normalised 
evaluation performance measure pr precision recall 
note measure gives equal weighting precision recall 
parameter tuned conducting preliminary experiments split data category 
note value set standard wk chosen value kernels categories 
ectiveness varying sequence length ectiveness text categorization system ssk controlled free parameters length subsequence weight decay parameter 
order understand role ssk text categorization important study performance classi er svm conjunction ssk varying note new value parameters obtain new kernel turn resultant kernel matrix contains new information 
studied ect varying parameters adopting experimental methodology described 
rst set experiments kept value parameter xed learned classi er di erent values conducted set experiments observe performance ected varying parameter empirically studied advantages combining di erent kernels 
section describes rst set experiments 
experiments value weight decay parameter set sequence length varied 
ssk compared ngk grams length varied range values 
ectiveness ssk compared ectiveness wk 
tables describe results experiments precision recall numbers shown kernels 
note results averaged runs algorithm 
results nd performance classi er varies respect varying sequence length 
ssk ective smaller moderate substrings compare larger substrings 
results show optimal size sequence length region large 
category numbers respect ssk peak sequence length 
shorter moderate non contiguous substrings able capture semantics better longer noncontiguous substrings 
practice size sequence length set set category 
tables results ngk wk 
rst focus performance svm classi er ngk compare role ngk ssk text categorization 
interesting note generalisation performance techniques comparable ngk works contiguous substrings ssk works non contiguous substrings 
results show generalisation performance svm classi er conjunction ngk higher short substrings longer substrings performance ngk worse 
classical text representation technique wk compared ssk 
worth noting performance ssk better wk category 
size dataset factor responsible degradation performance wk results show ssk ective technique perform comparably techniques irrespective size dataset 
category kernel length precision recall mean sd mean sd mean sd earn ssk ngk wk acq ssk ngk table performance precision recall svm ssk ngk wk reuters categories earn acq 
results illustrate ect variability subsequence length performance 
results averaged runs techniques 
report standard deviation 
category kernel length precision recall mean sd mean sd mean sd crude ssk ngk corn ssk ngk table performance precision recall svm ssk ngk wk reuters categories crude corn 
results illustrate ect variability subsequence length performance 
results averaged runs techniques 
report standard deviation 
ectiveness varying weight decay factor set experiments analyse ect varying generalisation performance svm learner manipulates information encoded string subsequence kernel 
ssk weights substrings proximity text 
higher values place weights non contiguous substrings vice versa 
words parameter controls interior gaps substrings 
ssk compared ngk wk 
evaluate performance techniques averaging results runs algorithm 
series experiments conducted study performance text categorization system ssk widely varying weight decay parameter 
results set experiments described tables 
average precision recall note tables show standard deviations 
value set 
di cult choice shown preceding section di erent categories obtained highest value di erent lengths 
main objective experiments described section analyse behaviour ssk varying interesting note precision peaks higher value categories corn 
corn peak achieved note category achieved maximum value increase value degrade ectiveness system substantially 
furthermore gain precision substantial categories 
gain recall obtained higher values peak obtained low value categories achieves peak slightly higher value 
note higher values substantial loss recall 
brie analyse improvement inf numbers varying numbers reach maximum value high falls minimum highest value polysemy english language 
technique ssk deals problem ssk returns high similarity score documents share non contiguous substrings 
text categorization system ssk correctly classify document share semantically di erent words 
phenomenon evident results 
compare performance ssk techniques 
note length grams set comparison 
results show ectiveness svm classi er conjunction ssk generalisation performance svm classi er conjunction ngk 
results show performance ssk better ngk cases gain performance substantial 
worth noting ssk able achieve higher values precision compared ngk 
ectiveness combining kernels preceding sections describe series experiments study choice 
observe uence combining kernels generalisation performance svm classi er 
words empirically study ect adding respective inner products di erent subsequence lengths weights 
text collection evaluation measures remain experiments 
category kernel precision recall mean sd mean sd mean sd earn ngk ssk acq ngk ssk wk table performance precision recall svm ssk ngk wk reuters categories earn acq 
results illustrate impact varying performance ssk 
results averaged runs techniques 
report standard deviation 
category kernel precision recall mean sd mean sd mean sd crude ngk ssk corn ngk ssk table performance precision recall svm ssk ngk wk reuters categories crude corn 
results illustrate impact varying performance ssk 
results averaged runs techniques 
report standard deviation 
category precision recall mean sd mean sd mean sd earn acq crude corn table performance precision recall svm combined kernels reuters categories earn acq corn crude 
ssk di erent lengths combined 
results averaged standard deviation 
combining kernels di erent lengths rst set experiments considered kernel matrix entries sum respective entries string subsequence kernels di erent lengths 
formally di dj di dj di dj string subsequence kernel matrix length length 
value weight decay parameter set set experiments 
kernels lengths combined 
results reported table 
illustration results length 
results show technique combining kernels potential improve performance system 
performance svm combined ssk better performance svm individual kernel 
evident value length category crude 
scenarios combination kernels appears gain showing kernels give similar information 
note results section averaged samples data 
combining ngk ssk experiments adding respective entries string subsequence kernel matrix grams kernel matrix 
set length kernels ssk value set 
results set experiments shown table 
combined ssk ngk observed uence combining weighted entries respective kernel matrices 
entries ngk weighted compare ssk formally unfortunately set experiments yield improvement generalisation performance svm classi er 
combining ssk di erent set experiments conducted evaluate ect combining ssk di erent 
results reported table 
length subsequence set values combined 
results showed methodology adding respective entries string subsequence kernels di erent resultant kernel matrix svm improve performance system substantially 

approximating kernels constructing gram matrix computational cost high 
may due need large number kernel evaluations large training set due high computational cost evaluating kernel 
circumstances points may 
assume training points xi yi kernel function corresponding feature space projection 
consider set vectors fsi xg 
cardinality ofs equal dimension category precision recall mean sd mean sd mean sd earn acq crude corn table performance precision recall svm combined kernels reuters categories earn acq corn crude 
ssk ngk combined 
results averaged standard deviation 
category precision recall mean sd mean sd mean sd earn acq crude corn table performance precision recall svm combined kernels reuters categories earn acq corn crude 
ssk di erent combined 
results averaged standard deviation 
ality space vectors si orthogonal si sj ij true follows fact si si si si si si si sj si si sj ij si si sj sj sj forming complete orthonormal basis cardinality set dimensionality vectors si fully orthogonal construct approximation kernel si si si propose fact conjunction cient method choosing construct approximation kernel function 
set 
ij 
carefully constructed production gram matrix closely aligned true gram matrix achieved fraction computational cost 
problem stage set ensure vectors si orthogonal 
choosing subset features choose set heuristics may include simply selecting random subset williams listing possible features selecting top frequency 
gram schmidt procedure applied kernel matrices order choose orthogonal features scholkopf smola 
approaches may include selecting data points training set close orthonormal generative model form whichever method chosen result low rank approximation gram matrix aim techniques principal components analysis latent semantic kernels 
going heuristic explicitly generating orthogonal complete set data choosing best points criteria orthogonal complete 
suppose set size having characters 
case evaluation feature vector require nn lt string kernel length document length computation approximate string kernel require nn lt compared nt required direct 
provided represent 
improvement greater evaluate kernel matrix training set size documents length requires mnn lt lm opposed nt required direct evaluation entries 
case savings nt mt choose small control size enforce inequalities section string kernel reuters data set 
discuss particular implementation method rst discuss method measuring similarity oftwo gram matrices 
similarity gram matrices order discover features needed obtain approximation true gram matrix need measure similarity gram matrices 
notion alignment 
proposed 
inner product gram matrices hk xi xj xi xj 
measure alignment matrices follows de nition alignment empirical alignment kernel kernel respect sample quantity hk hk hk ki kernel matrix sample kernel ki 
measure 
matrices fully aligned alignment measure see 
cristianini 
details kernel alignment 

approximating string kernel section show usefulness technique approximating ssk 
high computational cost ssk candidate approach 
obtaining approximation mentioned string kernel time complexity length sub sequences considering length documents involved 
datasets reuters data set contains approximately training examples test examples average length approximately characters string kernel expensive apply large text collections 
heuristic obtaining set follows 
choose substring size 
enumerate possible contiguous strings length 
choose strings length occur frequently dataset forms set notice de nition strings length orthogonal si sj ij constant conjunction string kernel degree features exactly equivalent string kernel subset gives approximation 
far quicker calculating dot product documents 
expect frequent features result approximation kernel discarding informative features 
possible frequent features may non informative possibility improving naive approach mutual information part selection process 
selecting subset string kernel approximated decide number features need conducted experiment 
generated possible grams letters space computed string kernel grams rst documents reuters dataset 
note kernel evaluations cheap documents characters 
calculated gram matrix documents features linear dot product full string kernel 
alignment matrices empirically con rming equivalent 
extracted contiguous grams rst documents computed alignment features result isvery close complete alignment 
computed alignment top features steps features steps 
order obtain comparison repeated experiment infrequent grams features selected random 
results shown gure 
seen graph small number features required generate alignment 
simply top features obtain alignment score top features gives score frequent infrequent random alignment scores gram matrix generated full string kernel frequent infrequent random selection features 

infrequent features database alignment score rapidly approaches increase number features expected random features places results 
shown small number features approximation string kernel 
full gram matrices large datasets ciently generated kernel algorithm 

experimental results order evaluate proposed conducted experiments modapte split reuters dataset 
pre processing run dataset experimentation removal common words 
ran approximation string kernel full reuters data set 
order compare results current techniques bag words approach 
software sv light joachims run experiments bag words approximation features generated 
performance proposed technique compared grams 
grams pre processing words removed 
conducted experiments 
note experiments described section sv light package 
table summarises results obtained preliminary experiments top row table indicates number features approximation 
note preliminary experiments conducted ship category number features approximation direct ect generalisation error 
category corn similar performance 
conjunction earn acq categories results stable 
suggests possibility heuristic increases number features increase alignment negligible 
earn acq ship corn table comparing di erent numbers features approximation ssk categories reuters dataset 
category wk ngk approximated ssk earn acq money fx grain crude trade interest ship wheat corn table numbers svm wk ngk ssk top reuters categories 
relationship quality alignment generalisation error investigated see correlation exists 
set experiments set number features 
result set experiments table 
numbers techniques 
table shows results comparable wk ngk 
order gauge ciency approach remember string kernel takes time 
word length characters training set documents reuters dataset number naively generate gram matrix 
approximation approach features strings length considerably faster 

novel kernel approximation text analysis 
performance string subsequence kernel empirically tested applying text categorization task 
kernel kernel learning system example clustering categorization ranking focused text categorization support vector machine 
kernel incorporate knowledge language apart removal words capture semantic information extent outperform state art systems data 
builds preliminary results obtained technique lodhi 

due extreme computational cost accessing feature space kernel trick 
sequence length features indexed strings length direct computation relevant features impractical moderately sized texts dynamic programming style computation computing kernel directly input sequences explicitly calculating feature vectors 
possible learning machine 
experiments indicate algorithm provide ective alternative standard word feature kernel previous svm applications text classi cation joachims 
able compare results obtained string kernel considered contiguous strings providing continuum varying parameter 
kernel contiguous strings combined non contiguous kernel examine extent di erent features classi cation 
addition di erent lengths strings considered comparison results obtained reuters data range di erent values 
order apply proposed approach large datasets derived fast algorithm approximates exact string kernel 
introduced method approximating kernels subset orthogonal features 
allows fast construction kernel gram matrices 
shown features arrive exactly string kernel solution approximations obtained relatively small number features 
order illustrate technique conducted extensive set experiments 
provided experimental results string kernel isknown computationally expensive 
approximation able achieve results full reuters dataset comparable produced bag words approach 
results full reuters dataset encouraging 
cases word kernel contiguous grams kernel outperformed string kernel 
led conjecture excellent results smaller datasets demonstrate kernel performing similar stemming providing semantic links words word kernel view distinct 
ect longer important datasets data learn relevance terms 
open question see ranking features positive negative examples introducing weighting scheme improve results 
approximation gram matrix produced easily kernel methods clustering principal components analysis 
relationship quality approximation generalisation error achieved algorithm needs explored 
consider fast approximation features obtain coarse gram matrix 
preliminary estimate support vectors obtained entries vectors re ned 
give rise fast form chunking large datasets 
provided fairly thorough testing string kernels text data particular considering ect varying lengths value decay parameter combinations di erent kernels 
developed approximation strategy enables apply approach large datasets 
consider extension techniques strings syllables words data text 
boser guyon vapnik 
training algorithm optimal margin classi ers 
haussler editor proceedings th annual acm workshop computational learning theory pages pittsburgh pa 
acm press 

gram document representation vector processing retrieval model 
trec 
cristianini shawe taylor 
optimizing kernel alignment 
technical report nc tr neurocolt 
cristianini shawe taylor 
support vector machines 
cambridge university press cambridge uk 
shawe taylor kandola 
kernel target alignment 
appear nips 
cristianini campbell 
kernel fast simple training procedure support vector machines 
shavlik editor proceedings international conference machine learning 
haussler 
convolution kernels discrete structures 
technical report ucsc crl university california santa cruz computer science department 
hu man acquaintance language independent document categorization grams 
trec 
joachims 
text categorization support vector learning relevant features 
european conference machine learning ecml 
joachims 
making large scale svm learning practical 
scholkopf burges smola editors advances kernel methods support vector learning 
mit press 
lodhi shawe taylor cristianini watkins 
text classi cation string kernels 
neural information processing systems nips pages 
mit press 
mercer 
functions positive negative type connection theory integral equations 
philos 
trans 
ray soc 

salton wong yang 
vector space model automatic indexing 
communications acm 
scholkopf smola 
sparse greedy matrix approximation machine learning 
international conference machine learning 
vapnik 
nature statistical learning theory 
springer verlag new york 
watkins 
dynamic alignment kernels 
technical report csd tr royal holloway university london department computer science 
williams 
nystrom method speed kernel machines 
neural information processing systems 
mit press 

