kernel matching pursuit pascal vincent bengio dept iro universit montreal qc canada iro umontreal ca technical report partement informatique recherche op universit august th matching pursuit algorithms learn function weighted sum basis functions sequentially appending functions initially empty basis approximate target function leastsquares sense 
show matching pursuit extended non squared error loss functions build kernel solutions machine learning problems keeping control sparsity solution 
derive mdl motivated generalization bounds type algorithm compare related svm support vector machine bounds 
links boosting algorithms rbf training procedures extensive experimental comparison svms classification showing comparable results typically sparser models 
renewed interest kernel methods due great part success support vector machine approach boser guyon vapnik vapnik 
kernel learning algorithms represent function learnt linear combination terms form xi xi generally input vector associated training examples symmetric positive definite kernel function 
support vector machines svms kernel learning algorithms fraction training examples solution called support vectors objective learning maximize margin decision surface case classification 
matching pursuit originally introduced signal processing community algorithm decomposes signal linear expansion waveforms selected redundant dictionary functions mallat zhang 
general greedy sparse function approximation scheme squared error loss iteratively adds new functions basis functions linear expansion 
take dictionary functions functions di form xi input part training example linear expansion essentially form support vector machine 
matching pursuit variants developed primarily signal processing wavelets community interesting links research kernel learning algorithms developed machine learning community 
connections related algorithm basis pursuit chen svms reported poggio girosi 
smola sch lkopf shows connections matching pursuit kernel pca sparse kernel feature analysis kind greedy algorithm compress design matrix svms allow handling huge data sets 
sparsity representation important issue computational efficiency resulting representation theoretical practical influence generalization performance see graepel herbrich shawe taylor floyd warmuth 
sparsity solutions svm algorithm hardly controllable solutions sparse 
research started search flexible alternative framework allow directly control sparsity terms number support vectors solution remove requirements positive definiteness representation dot product high dimensional feature space 
lead uncover connections greedy matching pursuit algorithms radial basis function training procedures boosting algorithms section 
discuss description proposed algorithm extensions thereof margin loss functions 
section give overview matching pursuit family algorithms basic version refinements thereof general framework machine learning viewpoint 
give detailed description particular implementation yields choice basis function add expansion minimizing simultaneously expansion weights choice basis function computationally efficient manner 
show section framework extended allow differentiable loss functions squared error original algorithms limited 
appropriate classification problems experiments squared loss classification problems successful results 
followed discussion margin loss functions underlining similarity traditional loss functions commonly neural networks 
section explain matching pursuit family algorithms build kernel solutions machine learning problems relates machine learning algorithms svms boosting algorithms radial basis function training procedures 
section previous theoretical minimum description length principle construct generalization error bounds proposed algorithm 
basically generalization error bounded training error plus terms grow fraction support vectors 
bounds compared bounds obtained support vector machines 
section provide experimental comparison svms different variants matching pursuit performed artificial data usps digits classification uci machine learning databases benchmarks 
main experimental result kernel matching pursuit algorithms yield generalization performance support vector machines significantly fewer support vectors 
flavors matching pursuit section describe basic matching pursuit algorithm introduced mallat zhang machine learning perspective signal processing 
successive refinements basic algorithm 
basic matching pursuit noisy observations yl target function hat points xl 
finite dictionary dm functions hilbert space interested sparse approximations expansions form fn ir gn dare chosen minimize squared norm residue rn fn shall call set gn basis number basis functions expansion 
notice typical machine learning framework noisy observations target function data points abuse notation mean yl 
article practical purposes training function associated dimensional vector represents function evaluated data points 
extensive abuse notation convenience particular notation represent dot product dimensional vectors associated functions represent norm vector associated function learnt approximation new test data dictionary functions actual functions 
finding optimal basis gn number allowed basis functions general np complete problem 
matching pursuit algorithm proceeds greedy constructive fashion starts stage recursively appends functions initially empty basis stage trying reduce norm residue rn fn fn build fn fn gn searching gn ir minimize squared norm residue rn rn gn 
gn arg min ir kgk fn input data set xl yl dictionary functions dm number basis functions desired expansion alternatively validation set decide initialize residue vector dictionary matrix dm 
xl dm xl yl performance validation set stops improving arg max nd result solution defined fn nd basic matching pursuit algorithm gn minimizes expression maximizes corresponding gn rn gn gn rn gn specified choose 
signal processing literature algorithm usually stopped reconstruction error goes predefined threshold 
machine learning problems shall error estimated independent validation set decide 
case seen primary capacity control parameter algorithm 
section show generalization error matching pursuit algorithms directly linked ratio number training examples 
pseudo code corresponding algorithm slight differences notation particular gn explanations corresponds vector detailed pseudo code 
matching pursuit backfitting basic version algorithm set basis functions obtained step suboptimal coefficients 
computationally intensive cross validation technique data scarce 
corrected step called back fitting back projection resulting algorithm known orthogonal matching pursuit omp davis mallat zhang choosing gn previously equation recompute optimal set coefficients step argmin kgk note just linear regression parameters 
step geometrical interpretation bn sub space spanned basis gn bn orthogonal complement 
pbn pb denote projection operators subspaces 
decomposed pb see 
ideally want residue rn small possible basis step wewant fn rn pb insures 
append gn expansion add orthogonal components gn contributes reducing norm residue 
pbn gn increases norm residue 
part belongs pbn compensated adjusting previous coefficients expansion back projection 
bn geometrical interpretation matching pursuit backprojection davis mallat zhang suggest maintaining additional orthogonal basis bn space facilitate back projection results computationally efficient algorithm implementation slightly modified version approach described algorithm 
rn fn matching pursuit backfitting choice function append step regardless possibility update weights find gn optimize picking dictionary function give best fit 
possible directly optimize gn arg min ir kgk shall call procedure distinguish backfitting backfitting done choice gn 
achieved efficiently backfitting 
implementation maintains representation target dictionary vectors decomposition projections bn bn span gn 
maintain step representation dictionary vector sum orthogonal components component dbn lies space bn spanned current basis expressed linear combination current basis vectors dimensional vector 
component lies bn orthogonal complement expressed original dimensional vector space coordinates 
maintain representation target decomposition current expansion fn bn plus orthogonal residue rn achieved easily considering components choose gn collinear rn procedure requires step passes dictionary searching gn updating representation basic matching pursuit requires 
detailed pseudo code algorithm 
summary variations mp regardless computational tricks orthogonality properties efficient computation versions matching pursuit differ way function append basis chosen coefficients updated step basic version find optimal gn append basis optimal keeping coefficients fixed equation 
backfitting version find optimal gn keeping coefficients fixed equation 
find optimal set coefficients new basis equation 
version find time optimal gn equation 
optimal set coefficients input data set xl yl dictionary functions dm number basis functions desired expansion alternatively validation set decide initialize residue vector dictionary matrix component db db yl db 
dm xl dm xl db initially empty gets appended additional row step ignore expressions involve db iteration performance validation set stops improving arg max db db db db component nd reduces residue ndb compensate component nd adjusting previous ndb update dictionary representation take account new basis function 
db db db db db idb db db idb db db db db result solution defined fn nd matching pursuit making orthogonality properties efficient implementations backfitting version previously described implementation algorithm algorithms computational complexity order 
extension non squared error loss gradient descent function space noticed boosting algorithms performing form gradient descent function space respect particular loss functions schapire mason :10.1.1.126.8716:10.1.1.31.2869
friedman technique adapted extend matching pursuit family algorithms optimize arbitrary differentiable loss functions doing squares fitting :10.1.1.29.9093
loss function yi fn xi computes cost predicting value fn xi true target yi alternative residue rn usual rn fn searching dictionary element append basis step 
rn direction steepest descent gradient function space evaluated data points respect rn fn yl fn fn xl fn xl gn chosen collinear gradient gn gn arg max rn gn line minimization procedure find corresponding coefficient arg min ir xi fn xi gn xi correspond basic matching pursuit notice original algorithm recovered squared error 
possible backfitting re optimizing minimize target cost conjugate gradient optimizer instance xi argmin kgk quite time consuming orthogonality property general case may desirable steps single step 
corresponding algorithm described details pseudo code previously slight differences notation particular gk explanation corresponds vector detailed pseudo code 
mention theory possible arbitrary loss functions finding optimal gk ir general case orthogonal decomposition involve solving equation turn dictionary function order choose append basis computationally prohibitive 
margin loss functions versus traditional loss functions classification seen matching pursuit family algorithms extended arbitrary loss functions discuss merits various loss functions 
particular relationship loss functions notion margin primary interest wanted build alternative svms original notion margin classification problems comes geometrically inspired hard margin linear svms smallest euclidean distance decision surface training points slightly different perspective emerged boosting community notion margin loss function 
margin quantity individual data point understood confidence measure classification function class decided sign 
margin loss function simply function margin quantity optimized 
possible formulate svm training show svm margin loss function mapping feature space svms xi xj xi xj svm solution expressed feature space iyi xi xi sv sv set support vectors solution minimizes yi xi box constraint parameter svms notation understood function gives 
margin point 
clearly sum margin loss function regularization term 
interesting compare margin loss function boosting algorithms traditional cost functions 
loss functions boosting algorithms optimize typically expressed functions schapire uses exponential margin loss function logitboost friedman hastie tibshirani uses negative binomial loglikelihood log shape similar smoothed version generalization abilities believed due margin maximization :10.1.1.31.2869
input data set xl yl dictionary functions dm number basis functions desired expansion alternatively validation set decide full backfitting update steps loss function initialize current approximation dictionary matrix dm 
fl xl dm xl performance validation set stops improving result yl fn fl arg max multiple simple line minimization argmin ir yi fi update nd multiple full backfitting ex 
gradient descent recompute argmin ir yi kd kd solution defined fn nd backfitting matching pursuit algorithm non squared loss soft margin svm loss function doom ii mason approximates theoretically motivated margin loss tanh :10.1.1.126.8716
seen left functions encourage large positive margins differ mainly penalize large negative ones 
particular tanh expected robust won penalize outliers excess 
enlightening compare traditional loss functions neural networks classification tasks express functions squared loss squared loss tanh modified target tanh tanh illustrated right 
notice squared loss tanh appears similar margin loss function doom ii slightly increases large positive margins behaves saturate unconstrained weights boosting svm algorithms impose constraints weights denoted 
loss exp adaboost log exp logitboost tanh doom ii svm margin loss squared error margin cost function squared error tanh target margin boosting svm margin loss functions left vs traditional loss functions right viewed functions margin 
interestingly born margin motivated loss functions doom ii similar traditional squared error tanh 
kernel matching pursuit links paradigms matching pursuit kernel dictionary kernel matching pursuit kmp simply idea applying matching pursuit family algorithms problems machine learning kernel dictionary kernel function ir ir ir dictionary kernel centered training points di xi 
optionally constant function included dictionary accounts bias term functional form approximation fn fn nk indexes support points 
training consider values dictionary functions training points amounts doing matching vector space dimension squared error loss complexity variations kmp basic backfitting training data candidate support points 
possible random subset training points support candidates yields 
emphasize fact dictionary gives lot additional flexibility framework possible include kind function particular restriction shape kernel positive definiteness constraint 
dictionary include single fixed kernel shape mix different kernel types choose point allowing instance algorithm choose widths gaussian support point 
similarly dictionary easily constrain algorithm kernel shape specific class prior knowledge 
dictionary incorporate non kernel functions mentioned constant function recover bias term incorporate prior knowledge 
huge data sets reduced subset dictionary speed training 
study restrict single fixed kernel resulting functional form obtained svms 
similarities differences svms functional form similar obtained support vector machine svm algorithm boser guyon vapnik main difference svms impose constraints quantity optimized svm algorithm quite different kmp greedy optimization especially squared error loss 
consequently support vectors coefficients types algorithms usually different see experimental results section 
important difference motivation research kmp capacity control achieved directly controlling sparsity solution number support vectors capacity svms controlled box constraint parameter indirect hardly controllable influence sparsity 
see graepel herbrich shawe taylor discussion merits sparsity margin ways combine 
algorithms generalized arbitrary loss functions computationally intensive imply non linear optimization step 
link radial basis functions squared error kmp gaussian kernel appears identical particular radial basis functions training algorithm called orthogonal squares rbf chen cowan ols rbf 
sch lkopf svms compared classical rbfs rbf centers chosen unsupervised means clustering svms gave better results 
knowledge experimental comparison ols rbf svms resulting functional forms alike 
empirical comparison contributions 
basically results section show ols rbf squared error kmp perform gaussian svms allowing tighter control number support vectors solution 
boosting kernels kmp basic form generalized non squared error similar boosting algorithms freund schapire friedman hastie tibshirani chosen class weak learners set kernels centered training points :10.1.1.133.1040
algorithms differ mainly loss function optimize discussed section 
bounds generalization error results vapnik minimum description length vapnik vapnik provide possible framework establishing bounds expected generalization error kmp algorithms 
simply results generalization error obtained number possible functions finite number capacity bounded log 
show essentially bound depends linearly number support vectors logarithmically total number training examples 
vapnik result theorem vapnik states expected generalization error rate binary classification training examples log log probability greater compression rate number bits transfer compressed conditional value training target classes training input points divided number bits required transmit compression training errors incorporate compressed message sending identity labels multiclass case wrongly labeled examples 
compression due representation learned training algorithm 
representation requires bits represent learned function keeping training error low 
assumes number possible functions finite obtain quantizing coefficients 
obtain compression take advantage sparse representation learned function terms support points 
obtain rough bound encode target outputs sets bits corresponding terms 
due classification errors send identity correct class training errors 
number errors le cost log bits 
case number classes nc increase number bits factor log nc similar increase numerator encode correct classes training examples 

second term required encode identity support points ln choose examples requires log bits 

third term encode quantized weights associated support point cost np bits number bits precision quantize weights chosen smallest number allows obtain discretized training set summarize kmp training errors support vectors examples probability greater choice training set log ln log log note poorly bounded log ratios apparent large log factor appears 
slightly tighter bounds obtained result vapnik vapnik learning choosing function functions probability log log 
log log quantization precision obtains log log np log log log log contrast obtain expectation bound vapnik svms expectation training sets note svms random depends training set 
note probability bounds readily converted expectation bounds 
example case mdl bound eq 
obtains expectation log ln log see role ratio log ln log keep mind poor bound 
note related compression bounds studied 
littlestone warmuth floyd warmuth graepel herbrich shawe taylor 
results graepel herbrich shawe taylor meant maximum margin classifiers draw interesting connections sparsity maximum margin 
results littlestone warmuth floyd warmuth general linked discussion apply classifiers specified subset training examples 
note case matching pursuit classifier requires support vectors weights general depend training set 
experimental results binary classification section mention kmp specification loss function means squares kmp written kmp mse kmp tanh refers kmp squared error hyperbolic tangent modified targets behaves typical loss function discussed earlier section 
specified matching pursuit algorithm train squares kmp 
train kmp tanh backfitting matching pursuit non squared loss algorithm conjugate gradient optimizer optimize experiments shows simple binary classification problem decision surface versions squared error kmp basic backfitting hard margin svm gaussian kernel 
fixed number support points backfitting versions number support points svm algorithm 
aim experiment illustrate points basic kmp iterations cycled back previously chosen support points improve weights unable separate data points 
shows backfitting versions useful improvement basic algorithm appears bad choice want sparse solutions 
backfitting kmp algorithms able find reasonable solution solution looks slightly better terms margin choose different support vectors svm necessarily close decision surface svms 
noted relevance vector machine tipping similarly produces solutions relevance vectors lie close border 
simple dot product kernel linear decision surfaces illustrates problem arise squares fit squared error penalizes large positive margins decision surface drawn cluster lower right expense misclassified points 
expected tanh loss function appears correct problem 
tried frequencies full backfitting real impact long done 
computationally intensive fashion 
left right iterations basic kmp iterations kmp backfitting iterations kmp svm 
classes 
support vectors circled 
kmp svm appear find equally reasonable solutions different support vectors 
svm chooses support vectors close decision surface 
backfitting chooses support set decision surface appears slightly worse margin 
basic kmp iterations cycled back previously chosen support points improve weights appears support vectors unable separate data points bad choice want sparse solutions 
problem squares fit leads kmp mse center misclassify points affect svms left successfully treated kmp tanh right 
postal service database main purpose experiment complement results sch lkopf obtained kmp mse mentioned equivalent orthogonal squares rbf chen cowan 
sch lkopf rbf centers chosen unsupervised means clustering referred classical rbf gradient descent optimization procedure train kernel weights 
repeated experiment kmp mse equivalent ols rbf find support centers gaussian kernel training set patterns independent test set patterns preprocessed handwritten digits 
table gives number errors obtained various algorithms tasks consisting discriminating digit versus see sch lkopf details 
validation data choose number bases support vectors kmp 
trained equal number support vectors obtained svm equal half number see sparser kmp model yield results 
seen results obtained kmp comparable obtained svms contrarily results obtained means rbfs slight loss performance half number support vectors 
table usps results number errors test set patterns number support vectors svm row uses half sv 
squared error kmp ols rbf appears perform svm 
digit class sv svm means rbf kmp sv kmp half sv benchmark datasets experiments known datasets uci machine learning databases gaussian kernels form series experiments machinery delve rasmussen system assess performance mushrooms dataset 
hyper parameters kernel box constraint parameter soft margin svm number support points kmp chosen automatically run fold cross validation 
results varying sizes training set summarized table 
reported table computed automatically delve system table results obtained mushrooms data set delve system 
kmp requires support vectors differences error rates significant 
size kmp svm value kmp svm train error error test size delve system estimations disjoint training sets size disjoint test sets size case disjoint training sets size test sets size 
wisconsin breast cancer sonar pima indians diabetes ionosphere slightly different procedure 
kernel fixed reasonable value data set procedure dataset randomly split equal sized subsets training validation testing 
svm kmp mse trained training set validation set choose optimal box constraint parameter svms early stopping decide number kmp 
trained models tested independent test set 
procedure repeated times different random splits dataset train validation test estimate confidence measures values computed resampled test nadeau bengio 
table reports average error rate measured test sets rounded average number support vectors algorithm 
seen experiments error rates obtained comparable kmp versions appear require fewer support vectors svms 
datasets contrary saw previously artificial data kmp tanh give significant improvement kmp mse 
experiments added label noise kmp tanh didn improve generalization performance table results uci datasets 
error rates significantly different values parentheses values difference svms require fewer support vectors 
dataset svm kmp mse kmp tanh svm kmp mse kmp tanh error error error wisc cancer sonar pima indians ionosphere chosen trial error svms validation set values keeping best choice advantage svms sensitive kmp 
values wisconsin breast cancer pima indians diabetes ionosphere sonar 
values tried give detailed account experiments primary intent show tanh error function advantage squared error presence label noise results inconclusive 
shown matching pursuit provides flexible framework build study alternative kernel methods extended arbitrary differentiable loss functions relates svms rbf training procedures boosting methods 
provided experimental evidence greedy constructive algorithms perform svms allowing better control sparsity solution lead solutions far fewer support vectors 
mentioned dictionary gives additional flexibility instance mix kernel shapes choose similar done weston include non kernel functions prior knowledge opens way research 
boser guyon vapnik 

algorithm optimal margin classifiers 
fifth annual workshop computational learning theory pages pittsburgh 
chen 

basis pursuit 
phd thesis department statistics stanford university 
chen cowan 

orthogonal squares learning algorithm radial basis function networks 
ieee transactions neural networks 
davis mallat zhang 

adaptive time frequency decompositions 
optical engineering 
floyd warmuth 

sample compression learnability vapnik chervonenkis dimension 
machine learning 
freund schapire 

experiments new boosting algorithm 
machine learning proceedings thirteenth international conference pages 
friedman 

greedy function approximation gradient boosting machine 
ims lecture february dept statistics stanford university 
friedman hastie tibshirani 

additive logistic regression statistical view boosting 
technical report august department statistics stanford university 
graepel herbrich shawe taylor 

generalization error bounds sparse linear classifiers 
thirteenth annual conference computational learning theory page press 
morgan kaufmann 
littlestone warmuth 

relating data compression learnability 
unpublished manuscript 
university california santa cruz 
extended version floyd warmuth 
mallat zhang 

matching pursuit time frequency dictionaries 
ieee trans 
signal proc 
mason baxter bartlett frean 

boosting algorithms gradient descent 
solla leen mller editors advances neural information processing systems volume pages 
mit press 
nadeau bengio 

inference generalization error 
solla leen mller editors advances neural information processing systems volume pages 
mit press 


orthogonal matching pursuit recursive function approximation applications wavelet decomposition 
proceedings th annual asilomar conference signals systems computers pages 
poggio girosi 

sparse representation function approximation 
neural computation 
rasmussen neal hinton van camp ghahramani tibshirani 

delve manual 
delve www cs toronto edu delve 
schapire freund bartlett lee 

boosting margin new explanation effectiveness voting methods 
annals statistics 
sch lkopf sung burges girosi niyogi poggio vapnik 

comparing support vector machines gaussian kernels radial basis function classifiers 
ieee transactions signal processing 
smola sch lkopf 

sparse greedy matrix approximation machine learning 
langley editor international conference machine learning pages san francisco 
morgan kaufmann 
tipping 

relevance vector machine 
solla leen mller editors advances neural information processing systems volume pages 
mit press 
vapnik 

nature statistical learning theory 
springer new york 
vapnik 

statistical learning theory 
wiley lecture notes economics mathematical systems volume 
weston gammerman stitson vapnik vovk watkins 

density estimation support vector machines 
sch lkopf burges smola editors advances kernel methods support vector learning pages cambridge ma 
mit press 
