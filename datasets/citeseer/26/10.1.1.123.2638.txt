general bounds mutual information parameter conditionally independent observations david haussler uc santa cruz parameter parameter space associated di erent probability distribution set parameter chosen random priori distribution conditionally independent random variables yn observed common distribution determined obtain bounds mutual information random variable giving choice parameter random variable giving sequence observations 
bound supremum mutual information choices prior distribution quantities applications density estimation computational learning theory universal coding hypothesis testing portfolio selection theory 
bounds terms metric information dimensions parameter space respect hellinger distance 
assume state nature de ned parameter parameter space allowed observe state nature directly inferences indirectly observing sequence observations yn 
observations values random variables yn conditionally independent true state nature distribution determined true state nature 
general setup fundamental statistics related disciplines including coding data compression computational learning theory supported nsf iri 
email addresses haussler cse ucsc edu supported heisenberg fellowship dfg email addresses opper physik uni de manfred opper universitat portfolio selection theory 
usually called density estimation parameter estimation statistics depending nature parameter space explore setup information theoretic point 
measure progress learning true state nature measuring able predict observation yt seeing previous observations yt 
assume discrete 
seeing previous observations produce estimate pt distribution de ned true state nature de ne loss predicting observation yt log pt yt 
logarithm base loss interpreted number bits needed encode yt making appropriate estimated distribution pt 
total loss sequentially predicting observations yn de ned pn log pt yt 
viewed number bits need encode yn adaptive estimated distributions pn 
analogous de nitions continuous estimated densities 
useful compare total loss incurred particular method obtaining estimated distributions pt total loss obtained knew true state nature true distribution encode yt 
di erence losses called regret net loss 
expected regret respect random choice yn xed state nature measures average extra number bits encode yn number bits required optimal encoding method true distribution 
called redundancy coding theory generally called risk statistics respect arbitrary loss regret functions 
risk method depends true state nature bayesian approach de ne priori distribution parameter space viewed random variable 
quantify performance method produces estimated distributions pt average risk true state nature drawn random prior distribution case posterior distribution yt jy yt unique optimal choice pt average risk redundancy optimal method bayes method risk 
simple calculation see shows bayes risk equal mutual information random variable random variable yn interpreted average amount information contained observation sequence true state nature leads standard interpretations mutual information terms density estimation coding theory 
clarke barron discuss interpretations quantity context hypothesis testing portfolio selection theory 
bayes risk obtain lower bound minimax risk minimum methods encoding predicting observations yn maximum risk true states nature quantity called information capacity minimax redundancy information theory 
denote 
key role areas mutual information investigated authors 
early showed log real valued conditional distributions smooth family densities indexed real valued parameter vectors dimension certain conditions apply 
case able estimate lower order additive terms approximation involve fisher information entropy ofthe prior 
related results clarke 
clarke barron gave detailed analysis applications risk redundancy bayes method function true state nature 
results extended mutual information minimax risk 
related lower bounds quoted obtained rissanen certain asymptotic normality assumptions 
extend obtaining bounds general settings 
approach taken relate mutual information directly certain metric properties parameter space distance states nature measured terms distance conditional distributions observation space de ne 
distance de ned relative entropy kullback leibler divergence distributions hellinger distance distributions 
distance upper bounds mutual information lower bounds 
volume relative entropy hellinger neighborhood radius true state nature de ned prior probability distance rate scales function shown key quantity determines mutual information large cases 
key theorem section examples section nal results theorems section 
section obtain new bounds mutual information minimax risk terms various notions dimension capacity parameter space include de nitions dimension metric entropy kolmogorov commonly theory empirical processes see aswell renyi information dimension associated posner entropy 
metric entropies de ned terms relative entropy hellinger distances context density estimation birge van de explicit goal applying methods empirical processes problem 
conclude section brief discussion problem obtaining bounds average instantaneous regret encoding tth observation function equivalently average additional information gained true state nature tth observation related loss measures commonly examined computational learning theory see 
applications results related problems described 
basic de nitions main result notational conventions 
variable px denotes distribution function density denoted px 
countable denotes probability continuous shorthand px 
measurable function denotes expectation random variables px denotes joint distribution iex expectation respect joint distribution conditional distribution denoted py jx jx denotes conditional density 
yjx andp yjx denote probability density ofy resp 
conditional expectation denoted iey 
similar notation condition event 
marginal distribution denoted py speci marginal probability denoted yjx marginal density denoted py speci denoted yjx 
ln ln ln 
random variables conditionally independent identically distributed common distribution denoted py random variable takes values arbitrary set 
simplicity assume takes values countable set continuous conditional densities py de ned default assumption countable results stated explicitly case comments analogous results obtained continuous functions results assumed measurable explicit mention 
yn denote atypical value upper lower bounds mutual information de ned countable ln yn similarly continuous corresponding densities denoted lower case obtaining bounds notions distance probability distributions 
probability mass functions countable set 
kullback leibler kl divergence relative entropy distance de ned dk jjq pi ln pi qi squared hellinger distance de ned dh pi qi densities analogous de nitions dk ln dx dh dx 
note dh dh dx case dh contrast kl divergence arbitrarily large 
employ notions distance distributions de ne corresponding notions distance points dk py dh py py main theorem gives bounds terms expected logarithms laplace transforms kl divergence hellinger distance 
speci cally assume conditional densities py de ned respect common sigma nite measure suitable probability space dimensional real space lebesgue measure 
theorem ln ln ln upper bound follows results mentioned give simple direct proof 
best knowledge lower bound new 
proof series lemmas calculations 
upper bound 
requires lemma previously utilized framework statistical physics 
lemma random variables real valued function ln iew ln iew proof note holder inequality real valued functions iew iew iew iew logs shows ln iew convex result follows applying jensen inequality 
lemma get chain inequalities iey nj ln yn iey nj ln yn iey nj ln yn iey nj ln ln yn yn ln iey ln yn ln dk jjp ln equality follows fact kl divergence additive product independent distributions see 
note convention ln simply removed rst equality reintroduced exponent ofthe second inequality avoiding division zero 
similar inequalities hold case continuous densities place probabilities 
establishes upper bound theorem 
turning lower bounds rst inequality follows fact establish tighter lower bound second inequality lemma new far tell 
random variables countable real de ne iew ln iew vjw vjw similarly ifv continuous appropriate ties exist iew ln iew vjw vjw note formula mutual information third equality chain substituting clear mutual information lemma random variables maximal 
proof assume countable 
show difference nonnegative 
di erence written iew ln vjw iew vjw iew vjw ln iew vjw iew vjw iew vjw ln iew vjw vjw iew vjw iew vjw ln iew vjw iew vjw iew vjw ln iew vjw rst line jensen inequality 
similar argument continuous 
lemma jensen inequality equation iey nj ln yn iey nj ln ln iey nj ln xp ln ln ny ny yt yt ln dh py py proof upper bound avoid division zero remove thatp rst line reintroduce fourth line 
similar inequalities hold case continuous densities place probabilities 
establishes lower bounds completes proof theorem 
examples illustrate theorem characterize asymptotic behavior mutual information large typical cases 
simplest case case countable sequel assume simplicity distinct conditional distributions py py densities py py di er set positive measure 
replace set equivalence classes property py py set measure zero natural way changing 
suppose countable say ig 
lnp denote entropy nite 
corollary lim proof nite clearly assume nite 
lim sup jy iey lnp conditional entropy note quantity nonnegative 
nite easily veri ed jy see lim sup case 
lower bound theorem fatou lemma lim inf lim inf ln lim inf lnp result generalizes similar result corollary removing additional conditions assumed 
case nite results renyi show di erence converges zero exponentially fast general results including corollary follow results pinsker book see 
uncountable unbounded grows 
illustrate theorem case calculate bounds simple gaussian case exact formula mutual information obtained 
gaussian distributed dimensional random vector mean spherical density yj jjy jj case hellinger kl distances easily calculated jj jj jj jj note jj jj small distances asymptotically equal 
general arbitrary large 
assume prior density mean spherical gaussian density jj jj straightforward integrations obtains simple result mutual information ln upper bound theorem reduces standard gaussian integrals gives ln dn ln weaker lower bound nice closed form tighter lower bound theorem form upper bound gaussian case gives ln ln dn upper lower bounds di er additive term ln 
laplace method estimate weaker lower bound shows close tighter lower bound asymptotically di ering upper bound additive term 
numerical evaluation case shows bounds quite 
mutual information entropy dimension section explore relationship properties metric space mutual information 
metric space distance countable distance continuous assuming py py di er set positive measure distinct 
notions entropy capacity dimension metric space equipped measure useful 
de nitions complete separable metric space measure de ned borel subsets de nition metric entropy called kolmogorov entropy partition collection ig borel subsets pairwise disjoint union diameter set diam sup 
diameter partition supremum diameters sets partition 
denote cardinality smallest nite partition diameter nite partition exists 
metric entropy de ned metric omitted understood context 
de nition posner entropy ig partition entropy de ned ash ln 
denote set countable partitions diameter posner entropy de ned inf de nition volume scaling entropy ball radius centered atx de ned fy volume scaling entropy iex ln de nition laplace transform entropy laplace transform entropy iex ln note metric entropy independent entropies depend measure independent versions de ned follows 
de nition supremum measures de ned borel subsets call posner capacity volume scaling laplace transform capacities de ned analogously 
entropies capacities corresponding notion dimension 
de nition upper lower metric dimensions de ned ln dim lim inf ln respectively 
dim value denoted called metric dimension information dimension volume scaling dimension laplace transform dimension de ned analogously substituting respectively de nition denoted dimh diml respectively 
measure independent forms dimensions de ned capacities denoted dimh diml respectively 
lemma describes relationships notions entropy capacity dimension 
lemma 




nite 


nite ln diml dimh diml dimh similarly dim dim exists 
proof part 
verify rst inequality note nonnegative random variable ln ln ln xed 
follows iex ln second inequality partition diameter denote set containing iex ln 
diam 
iex ln bx partitions diameter second inequality follows 
inequality follows fact entropy partition logarithm cardinality partition 
contrast part inequality part quite nontrivial result 
proof see inequality 
part veri ed follows 
note nonnegative random variable ln ln re re ln re ln ln ln minf ln xed letz 
fx ln br ln iex minf ln br ln iex ln br ln br ln vr ln br 
minimum cardinality partition diameter ln 
note br de nition 
fact ln increasing function fact xe wehave ln br ln ln inequality part follows 
verify part say separated distinct denote cardinality largest nite separated subset arbitrarily large separated subsets exist 
readily veri ed see 
separated subset maximal cardinality 
nite uniform distribution lnm ln 

nite distributions larger larger subsets see inequality holds 
part follows easily parts 
part implies diml dimh similarly dim dim exists 
nite iso ln 
easily veri ed parts imply diml dimh similarly dim dim exists 
part drop subscript extend chain inequalities include 
lemma problem estimating andc 
remainder assume complete separable metric space 
denote set probability measures de ned borel subsets parameter set borel subset topology generated hellinger distance recall discussion minimax risk section 
formal de nition iey nj ln yn ranges probability distributions de ned borel subsets quantity related mutual information described lemma 
lemma sup supremum prior distributions de ned borel subsets parameter space holding xed family conditional distributions py theorem nite borel subsets respect convergence measures 

lim inf lim inf ln ln dim proof follows lower bound theorem lp 
sup lp part follows lemma part part follows similarly lemma 
obtain analogous upper bounds need rst investigate relationship hellinger relative entropy distances 
useful lemma direction 
see related results 
lemma de ne ln distributions dh dk jjq dh qi pi sup qi analogous result pi holds densities infx proof case countable case densities similar 
ri qi pi 
exists dk jjq andb nite result holds trivially 
assume 
fi pi qi complement ofa 
dk jjq pi ln pi qi pi ln ri pi ln ri qi pi pi ln ri qi pi pi ri ln ri dh pi qi qi pi ri qi dk jjq dh pi ri ln ri qi pi ri qi max max sup sup ri ri ln ri ri continuously decreasing function ri ri 
establishes upper bound 
similar reasoning nonempty dk jjq dh empty min inf ri dk jjq dh inf ri lower bound follows case 
easy verify function decreasing continuous andb 
lemma shows ratios qi pi near dk jjq dh 
inf yj yj lemma positive distances di er xed constant factor 
upper bounds andc form lower bounds easily obtained case 
theorem exists particular 
inf lim sup ln lim sup ln yj yj dimh proof particular conditions existence nite follow lemma mentioned 
ab exists follows upper bound theorem bn 
part follows lemma part part follows similarly lemma 
note upper lower metric information dimensions theorems match typically case modulo assumptions stated theorems theorems give exact asymptotic characterization mutual information minimax risk showing proportional ln giving constant proportionality 
bounds instantaneous information gain de ne instantaneous information gain time ln ln countable similarly densities continuous 
denotes yn 
equality follows assumed conditional independence yt 
quantity viewed average amount information gained observing yn information gained observing yn 
easily veri ed nx total average instantaneous information gain average total information gain mutual information expected see 
instantaneous information gain interest areas including computational learning theory brie discuss results may analyze 
lemma numbers limn ln exists possibly nite 
assume sequence numbers pn bt limit exists 
proof suppose limn 
exists tj constant pn ln nj jan ln nj bc bc nx bt bt ln limn ln argument case nite similar 
follows lemma lim ln example cases described gaussian example section section limn ni limit exist 
limn ni exists limn get typical ln growth rate average instantaneous information gain particular theorem assume dimh limn ni exist nite exists lim ni dimh proof follows directly theorems lemma 
bound average instantaneous log loss nth observation adaptive method prediction bound computational learning theory applications 
new pair inequalities theorem general asymptotic estimates mutual infor mation minimax risk terms dimension metric space related quantities 
assumptions required obtaining results interesting see weakened 
treated nite dimensional case leads ln scaling law quantities 
interesting see general scaling laws established nite dimensional case theorem 
possibility theorem obtain bounds thermodynamic limit large sample size large dimension see explored 
practice important get accurate estimates moderate sample size believe bounds theorem quite close practical cases remains veri ed 
andrew barron tom cover helpful suggestions research 
amari murata 
statistical theory learning curves entropic loss 
neural computation 
barron 
strong ergodic theorem densities generalized shannon mcmillan breiman theorem 
annals probability 
barron 
exponential convergence posterior probabilities implications bayes estimators density functions 
technical report dept statistics ill urbana champaign 
barron clarke haussler 
information bounds risk bayesian predictions redundancy universal codes 
proc 
international symposium information theory 
barron cover 
bound nancial value information 
ieee trans 
information theory 
birge 
approximation dans les theorie de estimation 
zeitschrift fuer und gebiete 
birge 
estimating density hellinger distance strange facts 
probability theory related elds 
birge andp 
massart 
rates convergence minimum contrast estimators 
probability theory related fields 
clarke 
asymptotic cumulative risk bayes risk entropy loss applications 
phd thesis dept statistics university ill 
clarke barron 
information theoretic asymptotics bayes methods 
ieee transactions information theory 
clarke barron 
je prior asymptotically favorable entropy risk 
statistical planning inference 
cover thomas 
elements information theory 
wiley 
dudley 
course empirical processes 
lecture notes mathematics 

information contained sequence observations 
problems information transmission 
farmer ott 
dimension chaotic attractors 
physica 
zinn 
limit theorems empirical processes 
annals probability 

density estimation view kolmogorov ideas approximation theory 
annals statistics 
haussler 
general minimax result relative entropy 

haussler barron 
methods line prediction values 
proceedings third nec symposium computation cognition 
siam 
haussler kearns schapire 
bounds sample complexity learning information theory vc dimension 
machine learning 
haussler kearns seung tishby 
rigorous learning curve bounds statistical mechanics 
proceedings seventh annual acm workshop computational learning theory 
haussler kivinen warmuth 
tight worst case loss bounds predicting expert advice 
technical report ucsc crl university california santa cruz computer information sciences 
haussler opper 
mutual information bayes methods learning distribution 
proc 
workshop theory neural networks statistical mechanics perspective 
world scienti 
appear 

information sample parameter 
second int 
symp 
information theory pages 
dembo 
rate distortion dimension sets measures 
ieee trans 
info 
th 
kolmogorov 
entropy capacity sets functional spaces 
amer 
math 
soc 
translations ser 


asymptotic methods statistical decision theory 
springer 
littlestone warmuth 
weighted majority algorithm 
information computation 
opper haussler 
general bounds predictive errors supervised learning 
proc 
workshop theory neural networks statistical mechanics perspective 
world scienti 
appear 
pinsker 
information information stability random variables processes transl 
holden day 
pollard 
empirical processes theory applications volume nsf cbms regional conference series probability statistics 
institute math 
stat 
am 
stat 
assoc 
posner rumsey 
stochastic processes 
ann 
math 
statist 
posner rumsey 
gaussian processes 
ann 
math 
statist 
posner 
epsilon entropy data compression 
ann 
math 
statist 
posner 
epsilon entropy distributions 
editor sixth berkeley sym 
math stat 
prob volume pages 

renyi 
dimension entropy probability distributions 
acta math 
acad sci hung 
renyi 
amount information parameter sequence observations 
publ 
math 
inst 

acad 
sci 
rissanen 
stochastic complexity modeling 
annals statistics 

proof re nements inequality feynman 
math 
phys 
van 
hellinger consistency certain nonparametric maximumlikelihood estimators 
annals statistics 
yamanishi 
learning criterion stochastic rules 
machine learning 
special issue proceedings nd workshop computational learning theory 
