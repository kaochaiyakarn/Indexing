clustering ensembles categorical data daniel barbar department computer science george mason university fairfax virginia usa 
cluster ensembles offer solution challenges inherent clustering arising ill posed nature 
focus design ensembles categorical data 
approach leverages diverse input clusterings discovered random subspaces 
experimentally efficacy technique combination categorical clustering algorithm coolcat 
cluster ensembles emerged technique overcoming problems clustering algorithms 
known clustering methods may discover different patterns set data 
clustering algorithm bias resulting optimization different criteria 
furthermore ground truth clustering result validated 
cross validation technique carried tune input parameters involved clustering process 
consequence user equipped guidelines choosing proper clustering method dataset 
orthogonal issue related clustering high dimensionality 
high dimensional data pose difficult challenge clustering process 
various clustering algorithms handle data low dimensionality dimensionality data increases algorithms tend break 
cluster ensemble consists different partitions 
partitions obtained multiple applications single algorithm different initializations application different algorithms dataset 
cluster ensembles offer solution challenges inherent clustering arising ill posed nature provide robust stable solutions leveraging consensus multiple clustering results averaging emergent spurious structures arise due various biases participating algorithm tuned 
enhance quality clustering results clustering ensembles explored 
clustering ensembles categorical data received attention literature 
focus design ensembles categorical data 
techniques combination categorical clustering approach 
apply method partitions provided coolcat algorithm 
daniel barbar related briefly describe relevant literature categorical clustering cluster ensembles 
clustering categorical data attracted attention researchers 
modes algorithm extension means categorical features 
update modes clustering process authors new distance measure number mis matches points 
categorical clustering algorithm processes point time 
step point placed existing cluster rejected clusters creates new 
decision similarity function 
rock robust clustering links hierarchical clustering algorithm categorical data 
uses jaccard coefficient compute distance points 
points considered neighbors jaccard similarity exceeds certain threshold 
link points computed considering number common neighbors 
agglomerative approach construct hierarchy 
enhance quality clustering results clustering ensemble approaches explored 
cluster ensemble technique characterized components mechanism generate diverse partitions consensus function combine input partitions final clustering 
popular methodology utilizes association matrix consensus function 
graph partitioning algorithms success generate combined clustering :10.1.1.12.309
clustering ensembles categorical data received attention literature 
approach generates partition categorical attribute points cluster share value attribute 
resulting clusterings combined consensus functions 
constructs cluster ensembles data mixed numerical categorical features 
coolcat algorithm section briefly clustering algorithm coolcat 
algorithm proven effective clustering categorical data 
clusterings provided coolcat components ensembles improve grouping data 
coolcat algorithm scalable clustering algorithm discovers clusters minimal entropy categorical data 
coolcat uses categorical numerical attributes enabling mining real world datasets offered fields psychology statistics 
algorithm idea cluster containing similar points entropy smaller cluster dissimilar points 
coolcat uses entropy define criterion grouping similar objects 
clustering ensembles categorical data formally entropy measures uncertainty associated random variable 
random variable values bethe corresponding probability function defined follows log entropy multivariate vector xn defined xn logp xn xn xn minimize entropy associated clusters coolcat proceeds follows 
points xn point represented vector categorical values xi coolcat partitions points clusters entropy clustering minimized 
ck represent clustering 
entropy associated cj entropy cluster cj cj xd cj cj xd cj log xd cj coolcat uses heuristic incrementally build clusters entropy criterion 
consists main phases initialization step incremental step 
initialization phase coolcat bootstraps algorithm selecting sample points 
sample selects points maximum pairwise entropy dissimilar 
coolcat places points different clusters 
proceeds incrementally step selects point maximizes minimum pairwise entropy previously chosen points 
selected points initial seeds clusters 
incremental phase coolcat constructs clusters 
processes data batches 
data point computes entropy resulting placing point cluster assigns point cluster gives minimum entropy 
final clustering depends order points assigned clusters danger obtaining poor quality result 
circumvent problem authors coolcat propose reprocessing step 
clustering batch points fraction set reconsidered clustering size fraction input parameter 
fraction points fit corresponding clusters reassigned fitting clusters 
assess points match clusters coolcat counts number daniel barbar occurrences point attributes cluster assigned 
number occurrences converted probability value dividing cluster size 
point lowest probability cluster removed entropy criterion 
performing assessment incremental step coolcat alleviates risk imposed order input points 
coolcat requires input parameters number clusters sample size initialization step buffer size number points considered reprocessing 
coolcat effective method clustering categorical data suffers limitations inherent clustering algorithms 
solution identified coolcat depends initial clusters seeding 
greedy sequential strategy algorithm affects result reprocessing step alleviates part problem 
furthermore sparsity data high dimensional spaces severely compromise ability discovering meaningful clustering solutions 
address issues constructing ensembles clusterings resulting multiple runs coolcat random feature subspaces 
clustering ensemble techniques consider set xn points 
clustering ensemble collection clustering solutions cm 
clustering solution cl partition set cl ckl ck collection clustering solutions desired number clusters objective combine different clustering solutions compute new partition disjoint clusters 
challenge cluster ensembles design proper consensus function combines component clustering solutions improved final clustering 
ensemble techniques reduce problem defining consensus function graph partitioning problem 
approach shown results literature :10.1.1.58.6852:10.1.1.12.309
sections introduce consensus functions categorical features clustering results produced coolcat 
motivation enhance accuracy clustering coolcat construct ensemble clusterings obtained multiple runs algorithm 
accuracy diversity trade achieved obtain consensus solution superior components 
improve diversity ensemble components run coolcat operates random subspace feature space obtained random sampling fixed number attributes set ones 
diversity guaranteed providing components clustering ensembles categorical data different views projections data 
views generated randomly typically large pool attributes highly component receives different prospective data leads discovery diverse complementary structures data 
rationale approach finds justification classifier ensembles theory stochastic discrimination 
advantages random subspace method fact known context ensembles classifiers 
furthermore observe performing clustering random subspaces advantageous data redundant features discrimination power spread features case real life scenarios 
conditions fact redundant noisy features appear random subspaces 
discriminant information distributed features generate multiple meaningful cluster discrimination subspaces 
agreement assumptions theory stochastic discrimination building effective ensembles classifiers exist multiple sets features able discern training data different classes unable discern training testing data class 
categorical similarity partitioning algorithm cspa aim generate robust stable solutions consensus clustering method 
generate contributing clusterings running multiple times coolcat algorithm random subspaces 
ensemble component access random sample features drawn original dimensional feature space 
objective find consensus partition output partitions contributing clusterings improved clustering data obtained 
order derive consensus function data point xi cluster cl want define probability associated cluster cl observed xi 
probability value conform information provided component clustering ensemble 
consensus function aggregate findings clustering component utilizing probabilities 
coolcat partitions data distinct clusters 
order compute distances data points clusters represent clusters modes 
mode cluster vector frequent attribute values cluster 
particular different values attribute frequency occurrence consider data set choose attribute frequency 
ties broken randomly 
compute distance point xi cluster cl considering jaccard distance xi mode cl cluster cl defined follows dil xi cl xi cl daniel barbar xi cl represents number matching attribute values vectors xi cl number distinct attribute values vectors 
di maxl dil largest distance xi cluster 
want define probability associated cluster cl observed xi 
point xi cluster label cl assumed random variable distribution probabilities cl xi provide nonparametric estimation probabilities data clustering result 
order embed clustering result probability estimations smaller distance dil larger corresponding probability credited cl 
define cl xi follows cl xi di dil dil denominator serves normalization factor guarantee cl xi 
observe cl xi 
particular added value allows non zero probability cl xi dil 
case cl xi assumes minimum value cl xi dil 
smaller distance values dil cl xi increases proportionally difference di dil deviation dil di larger increase 
consequence corresponding cluster cl reasonable expect information provided clustering process 
equation provides nonparametric estimation posterior probability associated cluster cl construct vector pi posterior probabilities associated xi pi xi xi ck xi denotes transpose vector 
transformation xi pi maps dimensional data points xi new space relative coordinates respect cluster centroids dimension corresponds cluster 
new representation embeds information original input data clustering result 
define similarity xi xj cosine similarity corresponding probability vectors xi xj pj pi pj combine pairwise similarities similarity matrix sij xi xj 
observe general clustering may provide different number clusters different sizes boundaries 
size similarity matrix independent clustering approach providing way align different clustering results space need solve label correspondence problem 
running coolcat algorithm times different features obtain similarity matrices sm 
combined similarity matrix clustering ensembles categorical data defines consensus function guide computation consensus partition sl ij reflects average similarity xi xj pi pj contributing clusterings 
map problem finding consensus partition graph partitioning problem 
construct complete graph vertex vi identifies xi 
connecting vertices vi vj assigned weight value ij 
run metis resulting graph compute way partitioning vertices minimizes edge gives consensus clustering seek 
size resulting graph partitioning problem steps algorithm call cspa categorical similarity partitioning algorithm summarized 
input points number clusters 
produce subspaces random sampling features replacement dimensional original feature space 
run coolcat times time different sample features 

partition obtain mode cl cluster compute jaccard distance il data point xi mode cl il xi cl xi cl set maxl il compute xi il kd il set xi xi compute similarity xi xj xi pi construct matrix ij xi xj 
build consensus function 
construct complete graph vi xi 
assign ij weight value edge eij connecting vertices vi vj 
run metis spectral clustering resulting graph output resulting way partition vertices experiments apply spectral clustering compute way partitioning vertices daniel barbar categorical bipartite partitioning algorithm cbpa second approach cbpa maps problem finding consensus partition bipartite graph partitioning problem 
mapping introduced 
weight values 
extend range weight values 
graph cbpa models instances data points clusters graph edges connect instance vertex cluster vertex forming bipartite graph 
detail proceed follows construction graph 
suppose run coolcat algorithm times different random features 
instance xi clustering compute vector posterior probabilities defined equations 
vectors construct matrix 



note row vectors denotes transpose 
dimensionality km assumption clusterings produces clusters 
observe definition easily generalized case clustering may discover different number clusters 
define bipartite graph consensus partition problem maps 
consider graph constructed follows 
contains km vertices representing cluster ensemble contains vertices representing input data point 
km connecting vertices vi vj assigned weight value defined follows 
vertices vi vj represent clusters instances vertex vi represents instance xi vertex vj represents cluster corresponding entry 
vice versa note dimensionality km km written follows partition bipartite graph partitions cluster vertices instance vertices simultaneously 
partition instances output final clustering 
due special structure graph sparse graph size resulting bipartite graph partitioning problem kmn 
assuming km complexity smaller size cspa 
steps algorithm call weighted bipartite partitioning algorithm summarized 
input points number clusters 
clustering ensembles categorical data table 
characteristics data dataset points class breast cancer vote 
produce subspaces random sampling features replacement dimensional original feature space 
run coolcat times time different sample features 

partition obtain mode cl cluster 
compute jaccard distance il data point xi mode cl xi cl il xi cl set maxl il compute xi il kd il set xi xi 
construct matrix xi 
construct bipartite graph xi km cj cluster ensemble 
set vj clusters instances 
set km vj represent instance cluster 
run metis spectral clustering resulting graph output resulting way partition vertices experimental design results experiments real datasets 
characteristics datasets table 
dataset taken 
breast congressional votes uci machine learning repository 
dataset consists samples attributes 
attributes value removed selected remaining attributes experiments done research 
breast cancer data sub sampled populated class conducted previous obtain balanced data 
congressional votes dataset contains attributes consist responses treat missing values additional domain attribute value feature conducted 
evaluating quality clustering general difficult task 
class labels available datasets evaluate results computing error rate normalized mutual information nmi 
error daniel barbar rate computed confusion matrix 
nmi provides measure impartial respect number clusters 
reaches maximum value result completely matches original labels 
nmi computed average mutual information pair cluster class nmi ni log ni ni log ni jn nj log nj ni number agreement cluster class ni number data cluster nj number data class andn total number points 
analysis results dataset ran coolcat times different sets random features 
number selected features set half original dimensionality data set breast cancer vote 
clustering results coolcat input consensus clustering techniques compared 
value input coolcat ensemble algorithms actual number classes data 
figures plot error rate achieved coolcat random subspace error rates categorical clustering ensemble methods cspa metis cspa spec cbpa metis cbpa spec spec short spectral clustering 
plot error rate achieved coolcat multiple runs entire feature space 
figures show able obtain diverse clusterings random subspaces 
furthermore instable performance coolcat original space shows sensitivity initial random seeding process order data processed 
kept input parameters coolcat fixed runs sample size set reprocessing size set 
detailed results data provided tables report nmi error rate er ensembles maximum minimum average nmi error rate values input clusterings 
general ensemble techniques able filter spurious structures identified individual runs coolcat performed quite 
techniques produced error rates comparable better coolcat minimum error rate 
cspa metis provided lowest error rate methods compared data sets 
breast cancer data error rate provided cspa metis technique better best individual input clustering 
worth noticing datasets cspa metis gave error rate lower best individual input clustering entire feature space see figures 
particular breast cancer data ensemble techniques provided clustering ensembles categorical data table 
results data ens nmi ens er max er min er avg er max nmi min nmi avg nmi cspa metis cspa spec cbpa metis cbpa spec table 
ens nmi ens er max er min er avg er max nmi min nmi avg nmi cspa metis cspa spec cbpa metis cbpa spec excellent results 
dataset error rate cspa metis average input clusterings vote close average 
cbpa metis spec performed quite 
general produced error rates comparable techniques 
cbpa produced error rates average error rates input clusterings exception vote dataset 
vote data ensemble methods gave error rates close average error rate input clusterings 
case coolcat full space gave better performance 
categorical clustering ensemble techniques capable boosting performance coolcat achieve robust results 
competitive behavior previously shown coolcat improvement obtained ensemble techniques valuable achievement 
error rate subspace coolcat cspa metis cspa spec cbpa metis cbpa spec fig 

data error rates cluster ensemble methods coolcat random subspaces 
daniel barbar table 
results breast cancer data ens nmi ens er max er min er avg er max nmi min nmi avg nmi cspa metis cspa spec cbpa metis cbpa spec table 
results vote data ens nmi ens er max er min er avg er max nmi min nmi avg nmi cspa metis cspa spec cbpa metis cbpa spec error rate coolcat cspa metis cspa spec cbpa metis cbpa spec fig 

data error rates cluster ensemble methods coolcat features 
proposed techniques construct clustering ensembles categorical data 
number issues remains explored determine specific ensemble method best suited dataset achieve accurate clustering components maintaining high diversity exploiting correlations features test techniques higher dimensional categorical data 
address questions 
acknowledgments part supported nsf career award iis 
error rate clustering ensembles categorical data subspace coolcat cspa metis cspa spec cbpa metis cbpa spec fig 

data error rates cluster ensemble methods coolcat random subspaces 
error rate coolcat cspa metis cspa spec cbpa metis cbpa spec fig 

data error rates cluster ensemble methods coolcat features 
error rate subspace coolcat cspa metis cspa spec cbpa metis cbpa spec fig 

breast cancer data error rates cluster ensemble methods coolcat random subspaces 
daniel barbar error rate coolcat cspa metis cspa spec cbpa metis cbpa spec fig 

breast cancer data error rates cluster ensemble methods coolcat features 
error rate subspace coolcat cspa metis cspa spec cbpa metis cbpa spec fig 

vote data error rates cluster ensemble methods coolcat random subspaces 
error rate coolcat cspa metis cspa spec cbpa metis cbpa spec fig 

vote data error rates cluster ensemble methods coolcat features 
clustering ensembles categorical data 

cluster analysis sage publications 


weighted clustering ensembles 
proceedings siam international conference data mining 

kamel 
finding natural clusters multi clusterer combiner shared nearest neighbors 
multiple classifier systems 


barbar li couto 
coolcat entropy algorithm categorical clustering 
proceedings international conference information knowledge management 
acm press new york ny 

dhillon 
clustering documents words bipartite spectral graph partitioning 
proceedings acm sigkdd international conference knowledge discovery data mining 

fern brodley 
solving cluster ensemble problems bipartite graph partitioning 
proceedings international conference machine learning 

fred jain 
data clustering evidence accumulation 
proceedings international conference pattern recognition 

gan wu 
subspace clustering high dimensional categorical data 
sigkdd 


guha rastogi shim 
rock robust clustering algorithm categorical attributes proceedings data engineering 

xu deng 
cluster ensemble method clustering categorical data 
information fusion 

xu deng 
clustering mixed numeric categorical data cluster ensemble approach arxiv computer science prints 
ho 
random subspace method constructing decision forests 
ieee transactions pattern analysis machine intelligence 

hu 
integration cluster ensemble text summarization gene expression analysis 
proceedings ieee symposium bioinformatics 

huang 
extensions means algorithm clustering large data sets categorical values 
data min 
knowl 
discov 

karypis kumar 
multilevel way partitioning scheme irregular graphs 
technical report university minnesota department computer science army hpc research center 

kleinberg 
stochastic discrimination 
annals mathematics artificial intelligence 

kleinberg 
overtraining resistant stochastic modeling method pattern recognition 
annals statistics 

kuncheva 
diversity cluster ensembles 
proceedings international conference systems man cybernetics 

mei xin cheng han zhai 
generating semantic annotations frequent patterns context analysis 
proceedings acm sigkdd international conference knowledge discovery data mining acm press new york ny 

newman blake merz 
uci repository machine learning databases 

duin 
bagging random subspace method redundant feature spaces 
kittler poli editors second international workshop multiple classifier systems 
daniel barbar 
strehl ghosh 
cluster ensembles knowledge reuse framework combining multiple partitions 
journal machine learning research 


efficient algorithm clustering categorical data 
comput 
sci 
technol 
