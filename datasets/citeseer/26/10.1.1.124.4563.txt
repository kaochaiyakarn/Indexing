non volatile memory fast reliable file systems mary baker satoshi etienne john ousterhout margo seltzer decreasing cost non volatile ram nvram late feasible workstations include megabyte nvram enabling design higher performance reliable systems 
trace driven simulation analysis uses nvram improve performance distributed file systems non volatile file caches client workstations reduce write traffic file servers write buffers write optimized file systems reduce server disk accesses 
results show megabyte nvram diskless clients reduces amount file data written server 
increasing amount nvram shows rapidly diminishing returns particular nvram block replacement policy little difference write traffic 
closely integrating nvram volatile cache provides best total traffic reduction 
today prices volatile memory provides better performance improvement dollar nvram client caching volatile cache sizes increase nvram cheaper nvram cost effective 
server side providing half megabyte write buffer file system reduces disk accesses measured logstructured file systems lfs file system includes transaction processing workloads 

improving performance distributed file systems require improving write performance 
described supported part national science foundation ccr mip national aeronautics space administration defense advanced research projects agency contracts nag dabt 
appeared proceedings fifth international conference architectural support programming languages operating systems october 
computer science division electrical engineering computer sciences university california berkeley ca large main memory file caches effectively reduce read traffic write traffic write traffic distributed systems increasingly important 
study sprite distributed file system shows workload measured client workstation caches reduce read traffic applications reduce write traffic 
file caches clients servers continue grow satisfy read traffic proportion write traffic increase potentially bottleneck 
failure caches reduce write traffic due insufficient cache size need protect newly written dirty data machine failures 
dirty data written volatile cache memory non volatile storage guarantee permanence tradeoff reducing write traffic protecting reliability dirty data 
longer dirty data held cache absorbed cache overwritten deleted time vulnerable machine failures power outages 
reason systems unix sprite limit amount time dirty data remains cache 
eventual write back causes dirty data written server 
sprite example data written cache client file server written cache seconds percent file data transferred clients file server triggered delayed write back client caches 
forced client caches application fsync calls immediately synchronously flush file dirty data cache file server disk 
non volatile memory nvram ram battery backup offers possibility reducing write traffic 
storing dirty data nvram guarantee permanence cost transferring client cache server cache server cache disk 
nvram economically feasible general workstation environment 
today nvram times expensive megabyte dram expect megabyte cost nvram decrease 
workstations today include small amount ram battery backup time day clock 
systems include overhead batteries incremental additions amount memory included may costly 
shown table nvram available separate memory components boards redundant batteries 
incur overhead batteries failover systems component overhead boards amortized megabytes memory 
megabyte boards boards expensive depending bus megabyte boards nearly expensive times cost equivalent amount dram 
power supply ups alternative method providing non volatile memory expensive small amounts memory 
ups power support sparcstation hours costs minimum 
alternative flash eeprom write access times significantly slower ram written limited number times appropriate purposes 
consider nvram different ways improve write performance distributed systems non volatile cache client workstations write buffer file servers 
nvram combination high performance relative disks permanence relative volatile ram worthwhile small caches write buffers price dropping volatile ram 
section analyze effects addition small amount nvram cache client workstations reduce write traffic servers 
non volatile memory allow dirty data remain longer client cache loss reliability assuming possible restart crashed client quickly 
results show megabyte nvram potentially reduce file write traffic clients servers 
today prices adding nvram client caches cost effective cache includes nearly sixteen megabytes volatile memory 
results show nvram block replacement policy little difference file traffic reduction extent integration volatile cache important 
section consider non volatile write buffer file server reduce number disk accesses 
reported performance gains putting non volatile caches traditional unix file systems nfs environment 
measurements show new file systems nfs requirement synchronous operations obtain significant reduction number writes 
sprite logstructured file system lfs example file system amortizes write cost collecting large amount dirty data writing contiguously 
currently fsync requests clients force lfs write disk accumulated data 
analysis shows non volatile buffer collect large amount data writing reduce number disk write accesses file systems heavily file system 
section address difficulties incorporating nvram system designs 
data non volatile client caches considered permanent component speed number price minimum ns lithium megabyte configuration batteries megabytes sram sram ram pc bus board pc bus vme bus vme bus volatile dram table current nvram costs 
table gives current price sample non volatile memory components dallas semiconductor non volatile memory boards volatile memory component comparison 
table shows list prices lots 
column number lithium batteries gives number batteries simm board 
components extra battery case battery fails 
column minimum configuration gives minimum number megabytes necessary purchase byte wide memory bus 
nvram boards column just lists configurations different amounts memory 
boards fixed overhead cost batteries assembly plus incremental cost memory 
price megabyte column shows amortized cost configuration size indicated minimum configuration column 
data disk data reach non volatile storage server removed client cache 
require changes client cache protocols synchronous write client cache server disk 
alternative avoids latency disk access write data nvram server 
modified data may unavailable resides nvram cache crashed client 
avoid problem clients recover quickly possible move nvram component client retrieve data new location 
gathered measurements sprite distributed file system 
sprite cluster sparcstation sparcstation sun decstation decstation workstations diskless megabytes memory 
cluster number file servers traffic handled single sun file server mbytes main memory 
workstations cluster run sprite network operating system largely unix compatible 
applications running cluster standard unix applications 
addition sprite provides process migration allowing users offload jobs easily idle machines cluster 
user community measurement period included operating systems researchers architecture researchers working design simulation new subsystems group students faculty working vlsi circuit design parallel processing administrators graphics researchers 
users day day computing sprite people system occasionally 

non volatile client file caches study write events currently account third total file bytes transferred clients file server writes result sprite second delayed write back cache consistency policy cache block replacement 
file data sprite overwritten deleted half hour creation retaining file blocks non volatile client caches may significantly improve system performance reducing server write traffic 
determine effectiveness nvram purpose addressed questions long dirty data overwritten deleted 
infinite room non volatile cache percentage dirty data overwritten deleted 
written back file server due cache consistency policy 
omniscient block replacement policy nvram take get significant reduction write traffic 
block replacement policies best non volatile cache 
integrate nvram cache organization order get potential benefits 
current costs economic sense invest nvram opposed buying volatile memory 
addressed questions file system trace data simulate different models non volatile client caches varying amounts nvram different block replacement policies 

caching models considered models incorporating nonvolatile memory client file cache write aside unified compared volatile client cache model 
volatile model captures behavior sprite client caching dynamic cache sizing preference dirty blocks 
sprite caches change size relative memory needs file system virtual memory system 
simplicity assumed caches static size study give dirty blocks preference block replacement policy 
giving dirty blocks preference helps reduce write traffic expense increasing read traffic 
sprite client caches maintain separate lists kilobyte cache blocks free list contains free blocks lru list contains blocks accessed blocks 
block read written cache free block chosen 
free blocks clean block lru list chosen replacement 
files containing dirty blocks accessed selected clean block scheduled written back server block cleaner started 
block cleaner runs second intervals writes back blocks containing dirty data seconds old 
produces clean blocks subject block replacement 
sprite policy gives preference dirty blocks clean block replaced dirty block dirty block accessed volatile model 
accessed block replaced clean dirty written back server necessary 
addition data written block cleaner dirty data written server cache due sprite cache consistency policy 
sprite file servers maintain consistency client caches 
server keeps track client write file 
client opens file server recalls dirty data flushed writer cache 
clients file open simultaneously open writing server disables client caching file clients closed 
called concurrent write sharing 
concurrent write sharing read write requests file bypass client cache go directly file server 
read write requests directed single volatile cache volatile model nvram models include volatile non volatile caches illustrated 
write aside model nvram intended protect dirty data volatile cache holds copies blocks volatile memory read crash recovery 
minimizes accesses nvram accommodate nvram access times significantly slower volatile memory access times 
writing data volatile cache nvram increases traffic memory bus 
volatile cache longer uses second delayed write dirty data dirty blocks files explicitly fsync user remain nvram replaced blocks flushed back server sprite consistency mechanism 
purposes study assume data written nvram permanent data written disk 
real system possible provide method flushing data nvram server disk 
dirty data blocks written file server removed nvram may remain clean blocks volatile cache 
volatile model block replacement strictly lru respect volatile cache 
dirty block replaced written server invalidated volatile non client volatile cache server application network nvram disk client volatile cache cache cache server application network write aside model unified model nvram nvram cache models 
write aside model file data written volatile cache nvram 
nvram protect permanence dirty data volatile cache accessed system failure 
unified model volatile memory nvram combined single larger cache 
individual block may reside memory dirty blocks required kept nvram 
volatile caches 
unified model closely integrates volatile non volatile file caches 
blocks duplicated nvram volatile cache dirty blocks reside nvram clean blocks may reside volatile cache nvram 
clean block may put nvram read operation finds volatile cache full nvram free block contains accessed block 
application writes directed nvram read requests satisfied cache 
write aside policy second delayed write dirty data blocks leave nvram due consistency mechanism replaced blocks 
unified model retains strict lru semantics respect volatile cache experimented different replacement policies described section blocks nvram 
incoming write request causes block replaced nvram removed dirty block written back server access time compared block volatile cache 
block volatile cache older replaced clean copy block removed nvram 

simulations simulated client cache behavior file system trace data previously obtained sprite 
traces disk record key file system operations file opens closes seeks 
current file offset appears events making possible deduce order amount read write traffic files 
possible deduce actual number exact times write operations write operations caused application fsync requests 
traces record file truncation deletion events overwriting cause bytes die client caches 
sprite consistency protocols process migration generated trace events flush bytes client caches server 
trace data broken hour trace runs 
traces record similar workloads traces users performed long running simulations large files resulting higher file system throughput 
detailed information traces detailed information trace methodology 
simulations required passes trace data 
processed trace data convert read write delete flush invalidate operations ranges bytes 
data simulator kept track contents volatile non volatile caches client machines maintained file sizes block access modify times 
input parameters simulator specify maximum sizes volatile non volatile caches nvram model replacement policy non volatile cache 
lru replacement volatile cache simulations 
simulations counted number bytes read written applications number ages bytes overwritten deleted transferred file server 
simulations counts yielded final result measurements lifetimes cached data simulation omniscient replacement policy required third pass data 
obtain results simulated non volatile cache infinite size 
simulator produced log runs bytes overwritten deleted left remaining cache times creation deletion 
omniscient policy simulator information choose block modify time furthest 

byte lifetimes lifetime simulation determined fraction bytes die nvram write delay 
demonstrated traces written bytes die seconds bytes die hours 
traces large amounts data transferred bytes die seconds die half hour 
reality bytes die cache suggested reasons 
traces finite length assume bytes left nvram trace eventually written back server 
bytes overwritten deleted reducing fraction need written back server 
second simulation started empty caches misclassifying writes new data overwrites 
results indicate holding dirty blocks cache longer traditional seconds reduce write traffic clients servers 
lifetime simulation generated information concerning final fate file bytes non volatile cache infinite size 
cache infinite size bytes written back file server due cache block replacement 
table summarizes fate written bytes traces typical traces traces 
traces bytes written absorbed sufficiently large nvram 
exclude traces absorption possible 
table shows categories write traffic clients server concurrent writes called back bytes 
concurrent writes row gives number bytes caching disabled due concurrent write sharing category turns minuscule 
bytes called back include written back server due migration process recalled server net write traffic time minutes trace trace trace trace trace trace trace trace byte lifetimes 
net write traffic expressed percentage bytes written client caches eventually written server bytes flushed fixed write back delay cache infinite size 
axis log scale 
results shown hour traces 
traces represent workloads different traces users performed long running simulations large files resulting higher file system throughput 
traffic type megabytes traces traces overwritten leave deleted nvram total absorbed cause called back server concurrent writes traffic total server writes remaining total application writes table summary types write traffic 
table gives fate bytes written non volatile cache infinite size 
columns labeled traces give results summed traces 
columns labeled give results summed traces large amounts data transferred 
row remaining indicates bytes left cache trace interval 
column totals exceed total application writes application write entire block may cause cache block written back server 
client written file opened 
number bytes written back server due process migration percent total traffic server traffic entirely due cache consistency mechanism 
reducing write traffic require choosing cache consistency policy efficient sprite protocol block block invalidation flushing file invalidation flushing 

small nvram reduces traffic shows cases small nvram significantly reduces write traffic clients servers 
byte lifetimes derived processed trace data simulate omniscient cache manager unified nvram model flush block cache modify time furthest 
results give possible reduction write traffic function nvram cache size traces 
results optimal terms blocks necessarily terms bytes block modify time furthest may contain dirty bytes block spared replacement may contain dirty bytes overwritten deleted 
eighth megabyte nvram small comparison average megabytes volatile file cache sprite eliminates server write traffic traces 
shows increasing amount nvram results rapidly diminishing returns 
traces megabyte reduces write traffic megabytes provides reduction 
net traffic figures section includes bytes remaining nvram trace 
figures pessimistic view benefits client nvram caches 
net write traffic trace trace trace trace trace trace trace trace megabytes nvram results omniscient replacement policy 
omniscient policy replaces block latest modify time 
shows net file write traffic server omniscient policy various client nvram cache sizes 
axis log scale 
results shown traces 

replacement policies observing small non volatile cache omniscient replacement policy significantly reduces write traffic traces examined results unified nvram model realistic replacement policies lru random 
lru policy favors short lived blocks cache replacing modified accessed block 
random policy chooses random blocks replacement gauging sensitivity write traffic reduction particular replacement policy 
simulations realistic omniscient policy include effects read traffic cache replacement dirty blocks may replaced just room dirty blocks room clean blocks 
simulations block replacement policies surprised demonstrating small differences policy designed favor short lived blocks random replacement policy 
shows insignificant differences write traffic reduction replacement policies typical trace trace 
random policy behaves lru policy 
megabyte nvram omniscient policy performs better feasible replacement policies 
difference omniscient policies traces including traces 

comparison cache models contrast non volatile cache models examined effect read write traffic clients servers client memory bus traffic net write traffic omniscient random lru megabytes nvram replacement policies 
gives net file write traffic achieved replacement policies trace 
axis log scale 
frequency nvram accesses 
non volatile cache models equally absorbing write traffic unified model better read traffic effectively increasing size cache 
unified model clean blocks placed non volatile cache volatile cache depending cache contains accessed block 
write aside model dirty blocks reside non volatile cache duplicate copies volatile cache adding nvram increase size cache clean blocks 
shows effect different cache models total read write traffic clients servers 
simulations lru replacement policy volatile nonvolatile caches 
models began volatile cache 
non volatile policies simulated increasing quantities non volatile cache volatile policy simulated increasing quantity volatile cache 
unified model performs better volatile cache model addition extra megabytes cache memory write net total traffic megabytes extra memory write aside volatile unified effect cache models net total traffic 
shows percentage total file traffic clients read written file server cache models trace 
models starts megabytes volatile cache 
axis shows effect adding memory cache 
volatile model volatile memory added 
unified write aside models nvram added 
aside model performs worse 
unified model performs better write aside model reduces read traffic write traffic write aside model helps reduce write traffic 
extra megabytes cache memory unified model performs better volatile policy write aside model performs worse 
nvram closely integrated cache mechanism best effect 
amounts nvram half megabyte volatile model performs better nvram models 
volatile model pool blocks available replacement newly written data consists entire volatile cache consists nvram nvram models 
indicates closely integrated nvram model allows dirty blocks written nvram volatile cache subject second delayed write back provide superior performance models discussed 
model allow dirty data vulnerable seconds volatile cache 
differences nvram models amount traffic generate memory bus number accesses generate nvram 
blocks written nvram volatile cache write aside model blocks written nvram unified model write aside model starts twice memory traffic 
blocks flushed nvram unified model may transferred volatile cache 
megabyte nvram absorbs half bytes written applications unified model transfers original blocks volatile cache 
including traffic transferred nvram volatile cache unified model generates file cache traffic local memory bus model 
small amount data transferred cache nvram occurs application updates part file block overwriting block 
cache manager finds file block volatile cache transfers block nvram updates 
measurements show rare amounts percent write events requested applications 
unified cache model generates accesses nvram write aside model 
blocks written read nvram write aside model clean dirty blocks may read nvram unified model 
megabyte volatile memory megabyte nvram simulation results show unified model generates half times nvram accesses 
nvram access times significantly slower volatile memory access times nvram appealing 
unified model superior model terms total network traffic reduction local memory bus traffic 

non volatile versus volatile caches question considered money better spent volatile non volatile memory client caches 
examine cost effectiveness nvram versus volatile memory compare total file traffic reduction produced varying amounts volatile non volatile memory weighed benefits prices 
current costs nvram fact read traffic today majority traffic clients servers expected nvram poor showing 
cost effectiveness nvram depends amount volatile memory system 
sufficient volatile memory nvram may cost effective method reducing network write traffic today prices 
shows addition megabytes nvram top megabytes volatile cache reduces total network file traffic amount addition megabytes volatile memory 
similarly adding megabytes nvram top megabytes volatile cache produces benefit addition megabytes volatile memory 
net total traffic megabytes extra memory volatile mbytes volatile mbytes unified mbytes unified mbytes benefits additional memory 
shows percentage total file traffic clients read written file server volatile unified cache models trace 
models starts megabytes megabytes volatile cache 
axis shows effect adding memory cache 
volatile model volatile memory added 
unified model nvram added 
indicates megabytes volatile cache adding nvram right choice twice expensive volatile memory 
today prices nvram times expensive volatile memory trade favorable client caching nvram 
megabytes volatile cache nvram performs successfully larger volatile cache reduced read traffic capable reducing write traffic 
situation just nvram provides benefit additional megabytes volatile memory 
sufficient volatile memory nvram provides better price performance today prices 
non volatile caches client workstations sense systems sprite afs improve performance caching dirty data clients 
nfs file systems cache consistency mechanism requires dirty data written immediately file server removing benefit non volatile memory file caching clients 
nfs systems sense put nonvolatile memory file servers described section 
file file file file modified blocks 
segment modified blocks 
segment data block block file 
nvram file system write buffer previous section showed nvram reduce write traffic diskless clients file server section describes nvram reduce physical writes server disk system 
servers nvram file caches absorb write traffic producing reductions server disk traffic similar client server traffic 
nvram covered section consider nvram write buffer front disk reduce number disk write accesses 
nvram improve disk performance new 
traditional distributed file systems especially file servers running unix fast file system nfs environment nvram reduce disk traffic 
particularly beneficial nfs file systems nfs protocol requires synchronous write operations 
systems board caches nfs server requests non volatile memory reduce latency synchronous writes file system performance improvements reported systems board 
ibm uses megabytes nvram disk controller similar fashion 
disk writes go nonvolatile speed matching buffer reduce latency 
buffering writes allows efficient disk utilization system sort operations reduce disk head metadata file file unused blocks 
segment summary block modified blocks 
segment log structured file system 
figures show simplified example file allocation lfs 
files written file file 
file metadata block describing location file data blocks allocated file disk 
middle block file modified 
new version added log new version metadata 
file created causing blocks metadata appended log 
file blocks appended 
blocks new version file metadata appended log 
segment lfs appends summary block describes segment contents 
motion 
simulation results show disk bandwidth writing dirty data randomly disk 
writing blocks randomly requiring megabyte nvram buffered sorted utilize disk bandwidth 
consider nvram file server modern write optimized file system 
database file systems various forms logging file server improve write performance minimizing latency disk writes 
sprite log structured file system lfs extreme example logging file system file data metadata written disk log format 
file metadata operations sprite performed asynchronously synchronously 
see great improvement performance due nvram write optimized file system nfs protocol unix fast file system see improvement 
measurements lfs disk activity half megabyte non volatile write buffer sprite lfs reduce number disk write accesses file systems measured modest reduce disk write accesses heavily file system 
contrast traditional unix file systems lfs optimized writing reading 
amortizes cost writes collecting large half megabyte segments data issuing contiguous disk writes 
file system disk layout sequence segments comprise log system 
file metadata incorporated log describes location blocks file support efficient reads 
traditional file systems seek predefined disk location update metadata write different files lfs gathers dirty file data metadata single segment appends summary block describing segment contents writes unit single seek 
shows allocation files log structured file system 
log uses space disk lfs garbage collector reclaims space old segments containing data overwritten deleted compacting remaining live data smaller number new segments 
lfs able write full segments 
lfs issue disk write full segment worth data accumulated data written seek resulting lower disk bandwidth utilization 
causes partial segment writes sprite second timeout causes data written server cache disk requests user applications 
current sprite file system dirty data older seconds flushed cache seconds 
partial segments due flushing impact disk bandwidth occur file system lightly utilized 
require lfs immediately write dirty data regardless amount disk traffic forced synchronous writes applications reduce efficiency lfs 
nfs network protocol synchronous write operations lfs storage manager nfs file server result partial segment writes 
measure lfs disk activity sampled kernel counters main sprite file server half hour period weeks 
recorded number size disk writes writes result application 
lfs file systems server 
user user user user contain home directories 
processes page swap disk sprite src kernel contains sprite kernel development area various local programs installed local file system total segments total segments segments partial partial due fsync file system user local swap user user sprite src kernel user scratch table percent forced partial segments lfs file systems 
column total segments partial gives percentage segment writes partial segments including data written second delay column includes partial segments resulting 
column segments file system gives percentage segment writes received particular file system total number segment writes file systems 
total includes full partial segments 
file system 
scratch scratch disk generally storing long lived trace data 
table shows percentage partial segments due lfs file systems sprite main file server 
measurements show sprite file systems segments written lfs disk partial segments due application 
heavily file system user showed segment writes partial segments due part users executing long running data base benchmarks request database transaction 
write buffer disk accesses avoided writes remain nvram buffer segment accumulated 
file systems forced synchronous writes occur 
swap instance saw partial segments due applications write directly swap disk 
addition disk bandwidth reductions disk space cost associated partial segments 
full partial segment lfs appends block metadata 
addition lfs places byte summary block segment describe contents 
smaller average unit writing higher percentage disk space occupied metadata summary blocks 
lost disk space reclaimed lfs garbage collector runs 
table allows estimate disk space overhead partial segments 
table shows average number kilobytes file data including metadata summary information written partial segment file systems 
partial segments average kilobytes user kilobytes sprite src kernel 
user space taken metadata summary blocks partial segments third segment 
segments partial segments user overhead reach third available disk space garbage collector runs 
sprite src kernel overhead partial segment 
partial segments account total segment writes sprite src kernel overhead file system reach 
nvram eliminate partial segment writes reduce disk space overhead overhead full segments 
improve disk utilization reduce garbage collection load server cpu 
unfortunately data number disk reads unable measure ratio read accesses write accesses determine disk traffic reduction nvram possible 
examined minimizing number disk write accesses read latency may important parameter systems 
extremely large write cause potentially unacceptable latency synchronous read requests queue 
analytic results show optimal write size lfs approximately disk tracks typically kilobytes 
analytic study reports increase mean read response time due full segment writes typically 
increase response time affects reads file data cache measurements required compute effect total cost 

impact system design nvram client workstations improve write performance distributed file systems creates new system design issues 
issues arise treatment long lived data client caches clients crashing non volatile caches contain live data 
file system kbytes partial kbytes fsync partial total write traffic user local swap na user user sprite src kernel user scratch na table average number kilobytes written partial segment 
column kbytes partial gives average number kilobytes written partial segment file system 
column kbytes fsync partial gives average number kilobytes written partial segment caused fsync 
column total write traffic shows percentage bytes written file system total number bytes written file systems 
na means applicable file system 
assuring permanence dirty data remained client caches significant period time require change protocol writing client cache blocks file server 
sprite block cleaning process runs seconds writes dirty data older seconds server 
sprite assumes seconds worth dirty data lost client machine crash 
data vulnerable seconds server cache written disk 
approximately seconds applications assume data safely stored disk 
clients non volatile caches dirty data may remain indefinitely client cache transferred file server 
data client nvram cache considered permanent cache write back protocol ensure data written back server reaches permanent storage flushed client cache 
cache write protocol perform synchronous disk server wait data reaches disk flushing client nvram 
attractive alternative waiting writing synchronously provide nvram file servers write traffic significantly reduced non volatile caches clients 
client data written nvram server synchronous disk necessary client need wait replace blocks cache 
systems need guarantee data resident non volatile client caches just permanent 
clients crash holding live file data nonvolatile memory pose problem making fast recovery important 
data written non volatile caches clients unavailable system client 
systems sprite guarantee perfect distributed cache consistency clients see data 
client updates file client reads sprite file server recalls new data client send second client 
data non volatile cache client just crashed server unable satisfy read request crashed client recovers nvram accessible machine 
clients recover quickly circumstances data unavailable short period time 
fast recovery client machines may alleviate problems due client crashes important consider client may recover 
servers extended periods uncommon move disks machines 
similarly nvram viewed permanent storage possible move nvram dead machine live machine 
making blocks nvram self identifying nvram caches may moved machine machine easily disks 
functions way permitting nvram backing disk moved systems loss data 
way nvram reliable flexible current disks 
additionally files frequently shared clients dirty data remain client cache long time data flushed back server consistency action time accessed client 

decreasing cost nvram brings new opportunities operating systems designers increase system reliability performance 
cost effective may desirable include megabytes nvram workstations 
diskless client workstations megabyte nvram possible reduce client server file write traffic 
larger amounts nvram workstation applied problems fast recovery provide greater improvements performance client caching 
file caches larger absorb read traffic nvram needed absorb write traffic providing mainmemory access speeds 
simulations show behavior tightly integrated non volatile client caches robust 
changing replacement policy increasing amount nvram megabytes relatively little difference file traffic reduction 
regardless workstations include nvram default centralizing little half megabyte file system nvram file servers improve write performance write optimized file systems 
nvram reduce network traffic cpu load file server improve disk utilization 
measurements show half megabyte nvram file system lfs decreases number disk accesses usual case extreme cases 
putting nvram file server sense client workstations include allows long lived data written client non volatile caches buffered safely server nvram forced immediately disk 

people helped possible 
brian pawlowski helping obtain board responding requests 
john wilkes hp labs gave preliminary data nvram costs lots comments suggestions 
don coleman systems answered questions 
mose micro memory gave great deal information nvram pricing markets 
ken lutz helped locate memory component ups prices 
asplos referees keith bostic peter chen john hartman mike kupfer ken shirriff jim smith useful comments drafts 
john byers allen downey soumen chakrabarti vikram vij participated initial study non volatile caches sprite clients graduate operating systems course 


baker hartman kupfer shirriff ousterhout measurements distributed file system proceedings th symposium operating system principles monterey ca october 
published operating systems review october 

baker sullivan recovery box fast recovery provide high availability proceedings summer usenix conference san antonio tx june 

carson setia optimal write batch size log structured file systems proceedings usenix workshop file systems ann arbor mi may 

copeland krishnamurthy case safe ram mcc technical report number aca st february 

douglis ousterhout transparent process migration design alternatives sprite implementation software practice experience july 

bayer database cache high performance fast restart database systems acm transactions database systems december 

finlayson cheriton log files extended file service exploiting write storage proceedings th symposium operating system principles austin tx november 
published operating systems review november 

gray minute rule trading memory disc accesses byte rule trading memory cpu time proceedings acm special interest group management data san francisco ca may 

hagmann reimplementing cedar file system group commit proceedings th symposium operating system principles austin tx november 
published operating systems review november 

ibm storage control ibm storage subsystem library ga st edition september 

improving performance correctness nfs server proceedings winter usenix conference san diego ca february 

kazar anderson mason tu file system architectural overview proceedings summer usenix conference anaheim ca june 

mckusick joy leffler fabry fast file system unix acm transactions computer systems august 

menon hartung ibm disk cache proceedings compcon san francisco ca june 

moran sandberg coleman lyon breaking nfs performance barrier proceedings spring munich germany april 

ousterhout da costa harrison kunze kupfer thompson trace driven analysis unix bsd file system proceedings th symposium operating system principles orcas island wa december 
published operating systems review october 

ousterhout douglis nelson welch sprite network operating system ieee computer february 

rosenblum ousterhout design implementation log structured file system proceedings th symposium operating system principles asilomar ca october 
published operating systems review october 
available transactions computer systems february 

sandberg goldberg kleiman walsh lyon design implementation sun network filesystem proceedings summer usenix conference portland june 

seltzer chen ousterhout disk scheduling revisited proceedings winter usenix conference washington january 

thompson efficient analysis caching systems phd thesis university california berkeley october 
available technical report ucb csd 
