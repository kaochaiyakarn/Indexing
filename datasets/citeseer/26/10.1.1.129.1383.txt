simultaneous multithreading maximizing chip parallelism examines simultaneous multithreading technique permitting independent threads issue instructions superscalar multiple functional units single cycle 
models simultaneous multithreading compare alternative organizations wide superscalar fine grain multithreaded processor single chip multiple issue multiprocessing architectures 
results show single threaded superscalar fine grain multithreaded architectures limited ability utilize resources wide issue processor 
simultaneous multithreading potential achieve times throughput superscalar double fine grain multithreading 
evaluate cache configurations possible type organization evaluate tradeoffs 
show simultaneous multithreading attractive alternative single chip multiprocessors simultaneous multithreaded variety corresponding conventional multiprocessors similar execution resources 
simultaneous multithreading excellent potential increase processor utilization add substantial complexity design 
examine complexities evaluate alternative organizations design space 
examines simultaneous multithreading sm technique permits independent threads issue multiple functional units cycle 
general case binding thread functional unit completely dynamic 
objective sm substantially increase processor utilization face long memory latencies limited available parallelism thread 
simultaneous multithreading combines multiple instruction features modern superscalar processors latency hiding ability multithreaded architectures 
inherits numerous design challenges architectures achieving high register file bandwidth supporting high memory access demands meeting large forwarding requirements scheduling instructions functional units 
introduce sm models limit key aspects complex proceedings nd annual international symposium computer architecture santa margherita ligure italy june dean tullsen susan eggers henry levy department computer science engineering university washington seattle wa ity machine evaluate performance models relative superscalar fine grain multithreading show tune cache hierarchy sm processors demonstrate potential performance real estate advantages sm architectures small scale chip multiprocessors 
current microprocessors employ various techniques increase parallelism processor utilization technique limits 
example modern dec alpha powerpc mips sun ultrasparc hp pa issue instructions cycle single thread 
multiple instruction issue potential increase performance ultimately limited instruction dependencies available parallelism long latency operations single executing thread 
effects shown horizontal waste vertical waste 
multithreaded architectures hand hep tera alewife employ multiple threads fast context switch threads 
traditional multithreading hides memory functional unit latencies attacking vertical waste 
cycle architectures issue instructions thread 
technique limited amount parallelism single thread single cycle 
issue width increases ability traditional multithreading utilize processor resources decrease 
simultaneous multithreading contrast attacks horizontal vertical waste 
study evaluates potential improvement relative wide superscalar architectures conventional multithreaded architectures various simultaneous multithreading models 
place evaluation context modern superscalar processors simulate derived mhz alpha enhanced wider superscalar execution sm architectures extensions basic design 
code scheduling crucial wide generate code state art multiflow trace scheduling compiler :10.1.1.112.9778
results show limits superscalar execution traditional multithreading increase instruction throughput processors 
example show issue superscalar architecture fails sustain instructions cycle fine grain multithreaded processor capable switching contexts cycle cost utilizes wide superscalar regardless number threads 
simultaneous multithreading hand provides significant performance improvements instruction throughput limited issue bandwidth processor 
traditional means achieving parallelism con research onr nsf ccr cda nsf pyi award mip washington technology center digital equipment microsoft fellowship 
cycles issue slots full issue slot empty issue slot horizontal waste slots vertical waste slots empty issue slots defined vertical waste horizontal waste 
vertical waste introduced processor issues instructions cycle horizontal waste issue slots filled cycle 
superscalar execution opposed single issue execution introduces horizontal waste increases amount vertical waste 
multiprocessor 
chip densities increase single chip multiprocessors viable design option 
simultaneous multithreaded processor single chip multiprocessor close organizational alternatives increasing chip execution resources 
compare approaches show simultaneous multithreading potentially superior multiprocessing ability utilize processor resources 
example single simultaneous multithreaded processor functional units outperforms conventional processor multiprocessor total functional units equal issue bandwidth 
study speculated pipeline structure simultaneous multithreaded processor implementation exist 
architecture may optimistic respects number pipeline stages required instruction issue second data cache access time load delay cycles shared cache affects comparisons single chip multiprocessors 
magnitude effects discussed sections respectively 
results serve upper bound simultaneous multithreading performance constraints architecture 
real implementations may see reduced performance due various design tradeoffs intend explore implementation issues 
previous studies examined architectures exhibit simultaneous multithreading various combinations vliw superscalar multithreading features analytically simulation discuss detail section 
differs extends multiple respects methodology including accuracy detail simulations base architecture comparison workload wide issue compiler optimization scheduling technology variety sm models simulate analysis cache interactions simultaneous multithreading comparison evaluation multiprocessing simultaneous multithreading 
organized follows 
section defines detail basic machine model workloads measure simulation environment constructed 
section evaluates performance single threaded superscalar architecture provides motivation simultaneous multithreaded approach 
section presents performance range sm architectures compares superscalar architecture fine grain multithreaded processor 
section explores effect alternatives performance simultaneous multithreading 
section compares sm approach conventional multiprocessor architectures 
discuss related section summarize results section 
methodology goal evaluate architectural alternatives defined previous section wide traditional multithreaded processors simultaneous multithreaded processors small scale multiple issue multiprocessors 
developed simulation environment defines implementation simultaneous multithreaded architecture architecture straightforward extension generation wide superscalar processors running real multiprogrammed workload highly optimized execution target machine 
simulation environment simulator uses emulation instruction level simulation similar tango 
features caching partially decoded instructions fast emulated execution 
simulator models execution pipelines memory hierarchy terms hit rates bandwidths tlbs branch prediction logic wide superscalar processor 
alpha axp augmented wider superscalar execution multithreaded execution 
model deviates alpha respects support increased single stream parallelism flexible instruction issue improved branch prediction larger higher bandwidth caches 
typical simulated configuration contains functional units types integer floating point load store branch maximum issue rate instructions cycle 
assume functional units completely pipelined 
table shows instruction latencies simulations derived alpha 
instruction class latency integer multiply conditional move compare integer fp divide fp load cache hit bank conflicts load cache hit load cache hit load memory control hazard br jmp predicted control hazard br jmp mispredicted table simulated instruction latencies assume second level chip caches considerably larger alpha reasons 
multithreading puts larger strain cache subsystem second expect larger chip caches common time frame simultaneous multithreading viable 
ran simulations caches closer current processors discuss experiments appropriate show results 
caches table multi ported interleaving banks similar design sohi franklin 
instruction cache access occurs program counter crosses byte boundary instruction fetched prefetch buffer 
model lockup free caches tlbs 
tlb misses require full memory accesses execution resources 
icache dcache cache cache size kb kb kb mb assoc dm dm way dm line size banks transfer time bank cycle cycle cycles cycles table details cache hierarchy support limited dynamic execution 
dependence issued order instruction thread scheduling window instructions scheduled functional units order depending functional unit availability 
instructions scheduled due functional unit availability priority cycle 
complement straightforward issue model state art static scheduling multiflow trace scheduling compiler :10.1.1.112.9778
reduces benefits gained full dynamic execution eliminating great deal complexity don need register renaming need precise exceptions simple register scheme replicated register sets fetch decode pipes 
entry direct mapped bit branch prediction history table supports branch prediction table improves coverage branch addresses relative alpha kb cache stores prediction information branches remain cache 
conflicts table resolved 
predict return destinations entry return stack return stack hardware context 
compiler support alpha style hints computed jumps simulate effect entry jump table records jumped destination particular address 
multithreaded experiments assume support added hardware contexts 
support models simultaneous multithreaded execution discussed section 
experiments instructions scheduled strict priority order context schedule instructions available functional unit context schedule unit unutilized context experiments show instruction throughput scheme completely fair scheme virtually identical execution models relative speeds different threads change 
results priority scheme analytical advantages seen sec tion performance fair scheme extrapolated priority scheme results 
assume changes basic pipeline accommodate simultaneous multithreading 
alpha devotes full pipeline stage arrange instructions issue issue 
simultaneous multithreading requires pipeline stages instruction scheduling primary effect increase misprediction penalty 
run experiments show cycle increase misprediction penalty impact instruction throughput single threaded mode 
threads throughput tolerant misprediction delays impact 
workload workload spec benchmark suite 
gauge raw instruction throughput achievable multithreaded superscalar processors chose uniprocessor applications assigning distinct program thread 
models parallel workload achieved multiprogramming parallel processing 
way throughput results affected synchronization delays inefficient parallelization effects difficult see performance impact simultaneous multithreading 
single thread experiments benchmarks run completion default data set specified spec 
multithreaded experiments complex reduce effect benchmark difference single data point composed runs instructions length number threads number benchmarks 
runs uses different ordering benchmarks benchmark run priority position 
limit number permutations subset benchmarks equal maximum number threads 
compile program multiflow trace scheduling compiler modified produce alpha code scheduled target machine 
applications compiled different compiler options executable lowest single thread execution time target hardware experiments 
maximizing single thread parallelism compilation system avoid increases parallelism achieved simultaneous multithreading 
superscalar bottlenecks cycles gone 
section provides motivation simultaneous multithreading exposing limits wide superscalar execution identifying sources limitations bounding potential improvement possible specific latency hiding techniques 
base single hardware context machine measured issue utilization percentage issue slots filled cycle spec benchmarks 
recorded cause empty issue slot 
example instruction scheduled cycle current instruction remaining issue slots cycle issue slots idle cycles execution current instruction delayed instruction assigned cause delay 
overlapping causes cycles assigned cause delays instruction delays additive percent total issue cycles alvinn doduc eqntott espresso fpppp hydro li mdljsp nasa ora su cor swm tomcatv applications composite memory conflict long fp short fp long integer short integer load delays control hazards branch misprediction dcache icache dtlb itlb processor busy sources unused issue cycles issue superscalar processor 
processor busy represents utilized issue slots represent wasted issue slots 
tlb cache wasted cycles divided appropriately 
table specifies possible sources wasted cycles model latency hiding latency reducing techniques apply 
previous contrast quantified effects removing barriers parallelism measuring resulting increases performance :10.1.1.24.4184:10.1.1.115.6325
results shown demonstrate functional units wide superscalar processor highly underutilized 
composite results bar far right see utilization processor busy component composite bar represents average execution instructions cycle issue machine 
results indicate dominant source wasted issue bandwidth 
dominant items individual applications mdljsp swm fpppp dominant cause different case 
composite results see largest cause short fp dependences responsible issue bandwidth causes account wasted cycles 
completely eliminating factor necessarily improve performance degree graph imply causes overlap 
dominant cause wasted cycles dominant solution 
single latency tolerating technique produce dramatic increase performance programs attacks specific types latencies 
instruction scheduling targets important segments wasted issue bandwidth expect compiler achieved available gains regard 
current trends devote increasingly larger amounts chip area caches memory latencies completely eliminated achieve utilization processor 
specific latency hiding techniques limited dramatic increase parallelism needs come general latency hiding solution multithreading example 
different types multithreading potential hide sources latency different degrees 
clearer classify wasted cycles vertical source wasted issue slots possible latency hiding latency reducing technique instruction tlb data tlb rates increase tlb sizes hardware instruction prefetching hardware tlb software data prefetching faster servicing tlb misses cache larger associative faster instruction cache hierarchy hardware instruction prefetching cache larger associative faster data cache hierarchy hardware software prefetching improved instruction scheduling sophisticated dynamic execution branch misprediction improved branch prediction scheme lower branch misprediction penalty control hazard speculative execution aggressive conversion load delays level shorter load latency improved instruction scheduling dynamic scheduling cache hits short integer delay improved instruction scheduling long integer short fp long multiply long integer operation divide long floating point operation shorter fp delays latencies improved instruction scheduling memory conflict accesses memory location single cycle improved instruction scheduling table possible causes wasted issue slots latency hiding latency reducing techniques reduce number cycles wasted cause 
waste completely idle cycles horizontal waste unused issue slots non idle cycle shown previously 
measurements wasted cycles vertical waste remainder horizontal waste 
traditional multithreading fine grain fill cycles contribute vertical waste 
doing recovers fraction vertical waste inability single thread completely fill issue slots cycle traditional multithreading converts vertical waste horizontal waste eliminating 
simultaneous multithreading potential recover issue slots lost horizontal vertical waste 
section provides details effectively 
simultaneous multithreading section presents performance results simultaneous multithreaded processors 
defining machine models simultaneous multithreading spanning range hardware complexities 
show simultaneous multithreading provides significant performance improvement single thread superscalar fine grain multithreaded processors limit ambitious hardware assumptions 
machine models models reflect possible design choices combined multithreaded superscalar processor 
models differ threads issue slots functional units cycle cases basic machine wide superscalar functional units capable issuing instructions cycle core machine section 
models fine grain multithreading 
thread issues instructions cycle entire issue width processor 
hides sources vertical waste hide horizontal waste 
model feature simultaneous multithreading 
existing proposed ar similar tera processor issues operation instruction cycle 
sm full simultaneous issue 
completely flexible simultaneous multithreaded superscalar threads compete issue slots cycle 
realistic model terms hardware complexity provides insight potential simultaneous multithreading 
models represent restrictions scheme decrease hardware complexity 
sm single issue sm dual issue sm issue 
models limit number instructions thread issue active scheduling window cycle 
example sm dual issue processor thread issue maximum instructions cycle minimum threads required fill issue slots cycle 
sm limited connection 
hardware context directly connected exactly type functional unit 
example hardware supports threads integer units integer unit receive instructions exactly threads 
partitioning functional units threads dynamic models functional unit shared critical factor achieving high utilization 
choice functional units available single thread different original target machine recompiled issue type functional unit processor model 
important differences hardware implementation complexity summarized table 
notice fine grain model may necessarily represent cheapest implementation 
complexity issues inherited wide superscalar design multithreading se 
sm full simultaneous issue model inter instruction dependence checking ports register file forwarding logic scale issue bandwidth number functional units inter inst instruction register dependence forwarding scheduling model ports checking logic fus notes fine grain scheduling independent threads 
sm single issue sm dual issue sm issue sm limited forwarding fus type connection scheduling independent fus sm full simultaneous issue complex highest performance modeled scheme forwarding intact forwarding eliminated requiring threads maximum performance table comparison key hardware complexity features various models high complexity 
consider number ports needed register file single thread issue multiple instructions amount forwarding logic difficulty scheduling issued instructions functional units 
number threads 
choice functional units reasonable issue processor 
current issue processors functional units 
number ports register file logic select instructions issue issue limited connection models comparable current issue single issue dual issue 
scheduling instructions functional units complex types simultaneous multithreaded processors 
hirata design closest single issue model simulate small number configurations thread issue bandwidth increased 
implement models similar full simultaneous issue issue width architectures complexity schemes vary considerably 
performance simultaneous multithreading shows performance various models function number threads 
segments bar indicate throughput component contributed thread 
bar graphs show interesting points multithreaded design space finegrained multithreading thread cycle thread issue slots sm single issue threads cycle issue slot sm full simultaneous issue threads cycle thread potentially issue slot 
fine grain multithreaded architecture provides maximum speedup increase instruction throughput single thread execution ipc 
graph shows little advantage adding threads model 
fact threads vertical waste reduced bounds gains point 
result similar previous studies coarse grain fine grain multithreading single issue processors concluded multithreading beneficial threads 
limitations apply simultaneous multithreading ability exploit horizontal waste 
figures show advantage simultaneous multithreading models achieve maximum speedups single thread superscalar execution ranging issue rate high ipc 
speedups calculated full simultaneous issue thread result represent single thread superscalar 
sm necessary single thread able utilize entire resources processor order get maximum near maximum performance 
issue model gets nearly performance full simultaneous issue model dual issue model quite competitive reaching full simultaneous issue threads 
limited connection model approaches full simultaneous issue slowly due flexible scheduling 
models increasingly competitive full simultaneous issue ratio threads issue slots increases 
results shown see possibility trading number hardware contexts hardware complexity areas 
example wish execute instructions cycle build issue full simultaneous machine hardware contexts dual issue machine contexts limited connection machine contexts machine contexts 
tera extreme example trading pipeline complexity contexts forwarding pipelines data caches supports hardware contexts 
increases processor utilization direct result threads dynamically sharing processor resources remain idle time sharing negative effects 
see effect competition issue slots functional units full simultaneous issue model lowest priority thread threads runs speed highest priority thread 
observe impact sharing system resources caches tlbs branch prediction table full simultaneous issue highest priority thread fairly immune competition issue slots functional units degrades significantly threads added slowdown threads 
competition non execution resources plays nearly significant role performance region competition execution resources 
observed caches strained multithreaded workload single thread workload due decrease locality 
data shown pinpoints ex instructions issued cycle instructions issued cycle number threads fine grain multithreading number threads sm full simultaneous issue instructions issued cycle instructions issued cycle number threads sm single issue thread number threads models sm full simultaneous issue sm issue sm dual issue sm limited connection sm single issue fine grain lowest priority thread highest priority thread instruction throughput function number threads 
show throughput thread priority particular models shows total throughput threads machine models 
lowest segment bar contribution highest priority thread total throughput 
act areas sharing degrades performance 
sharing caches dominant effect wasted issue cycles perspective thread due cache misses grows thread threads wasted cycles due data cache misses grows 
data tlb waste increases 
section investigate cache problem 
data tlb workload increasing shared data tlb entries brings wasted cycles threads providing private tlbs entries reduces regardless number threads 
necessary extremely large caches achieve speedups shown section 
experiments significantly smaller caches shown reveal size caches affects thread thread results equally making total speedups relatively constant wide range cache sizes 
thread execution results lower hit rates thread execution relative effect changing cache size 
summary results show simultaneous multithreading performance attainable execution fine grain multithreading run wide superscalar 
seen simplified implementations sm limited thread capabilities attain high instruction throughput 
improvements come significant tuning architecture multithreaded execution fact instruction throughput various sm models somewhat hampered sharing caches tlbs 
section investigates designs resistant cache effects 
cache design simultaneous multithreaded processor measurements show performance degradation due cache sharing simultaneous multithreaded processors 
section explore cache problem 
study focuses organization level caches comparing private thread caches shared caches instructions data 
assume caches shared threads 
experiments issue model threads 
caches specified total cache size kb private shared cache size private shared 
instance private kb caches shared kb data cache 
private caches utilized fewer threads running 
exposes interesting properties multithreaded caches 
see shared caches optimize small number threads threads available cache private caches perform better large number threads 
example cache ranks models thread threads cache gives nearly opposite result 
tradeoffs instructions data 
shared data cache outperforms private data cache numbers threads compare instruction caches benefit private caches threads 
reason differing access patterns instructions data 
private caches eliminate conflicts different threads cache shared cache allows single thread issue multiple memory instructions different banks 
throughput ipc relative number threads results simulated cache configurations shown relative throughput instructions cycle cache results 
configurations appear choices 
little performance difference threads cost optimizing small number threads small making attractive option 
expect typically operate thread slots full gives best performance region worse second best performer fewer threads 
shared data cache scheme allows take advantage flexible cache partitioning private instruction caches thread sensitive presence threads 
shared data caches significant advantage data sharing environment allowing sharing lowest level data cache hierarchy special hardware cache coherence 
simultaneous multithreading versus single chip multiprocessing chip densities continue rise single chip multiprocessors provide obvious means achieving parallelism available real estate 
section compares performance simultaneous multithreading small scale single chip multiprocessing mp 
organizational level approaches extremely similar multiple register sets multiple functional units high issue bandwidth single chip 
key difference way resources partitioned scheduled multiprocessor statically partitions resources devoting fixed number functional units thread sm processor allows partitioning change cycle 
clearly scheduling complex sm processor show areas sm model requires fewer resources relative multiprocessing order achieve desired level performance 
experiments tried choose sm mp configurations reasonably equivalent cases biased favor mp 
comparisons keep equal number register sets number threads sm number mp total issue bandwidth specific functional unit configuration 
consequence item functional unit configuration optimized multiprocessor represents inefficient configuration simultaneous multithreading 
experiments kb private instruction data caches thread sm processor mp kb way set associative shared second level cache mb direct mapped third level cache 
want keep caches constant comparisons private caches natural configuration multiprocessor 
evaluate mps issues cycle processor 
evaluate sm processors issues cycle sm issue model defined section sm measurements thread limited issues cycle 
model minimizes inherent complexity differences sm mp architectures 
example sm issue processor similar single threaded processor issues cycle terms number ports register file amount inter instruction dependence checking 
experiment run version benchmarks configurations compiled issue functional unit processor closely matches mp configuration mp sm models typically favors mp 
note general tried bias tests favor mp sm results may optimistic respects amount time required schedule instructions functional units shared 
impact discussed section small 
distance load store units data cache large impact cache access time 
multiprocessor private caches private load store units minimize distances 
sm processor private caches load store units shared 
alternate configurations eliminate difference 
having load store units private unit thread associated private cache allow match mp performance fewer half total number mp functional units vs 
load store purpose test unlimited fus equal total issue bandwidth equal number register sets processors threads unlimited fus test limit sm fus unequal issue bw mp times total issue bandwidth fu utilization equal fus equal issue bw unequal reg sets common elements specific configuration test fus issue bw reg sets test fus issue bw reg sets test fus issue bw reg sets test issue bw reg sets test fus reg sets test fus reg sets test fus issue bw sm thread issue mp issue procs sm thread issue mp issue procs sm thread issue mp issue procs sm thread issue fu mp issue procs fu sm thread issue mp issue procs sm thread issue mp issue procs sm thread issue mp issue procs throughput instructions cycle results various multiprocessor vs simultaneous multithreading comparisons 
multiprocessor functional unit type processor 
cases sm processor total number fu type mp 
units threads statically share single cache combination set threads 
threads share load store unit accesses load store unit go cache allowing minimize distance cache load store unit allowing resource sharing 
shows results sm mp comparison various configurations 
tests compare performance schemes essentially unlimited number functional units fus functional unit type available issue slot 
number register sets total issue bandwidth constant experiment test thread issue sm processor issue processor mp register sets issue instructions cycle 
models ratio functional units threads issue bandwidth high configurations able utilize issue bandwidth 
simultaneous multithreading effectively 
test repeats test limits sm processor reasonable configuration functional unit configuration 
configuration outperforms multiprocessor nearly test sm configuration fewer functional units requires fewer forwarding connections 
tests mp allowed larger total issue bandwidth 
test mp processor issue instructions cycle total issue bandwidth processors sm thread issue instructions cycle threads share issue slots 
results similar despite disparity issue slots 
test thread issue sm slightly outperforms processor issue processor mp twice total issue bandwidth 
simultaneous multithreading performs tests despite handicap mp constrained respect instructions single processor issue single cycle 
test shows greater ability sm utilize fixed number functional units 
sm mp functional units issues cycle 
sm allowed contexts register sets mp limited processors register sets processor functional unit types 
simultaneous multithreading ability drive utilization fixed number functional units addition thread contexts achieves times throughput 
comparisons show simultaneous multithreading outperforms single chip multiprocessing variety configurations dynamic partitioning functional units 
important sm requires fewer resources functional units instruction issue slots achieve performance level 
example single thread issue sm processor functional units faster processor single issue mp test identical issue bandwidth requires functional units equal throughput thread issue sm mp system requires issue processors test consume functional units issue slots cycle 
advantages sm mp shown experiments performance threads results show performance maximum utilization 
advantage sm mp greater contexts processors unutilized 
idle processor leaves mp idle sm threads expand available resources 
important run parallel code degree parallelism varies time performance small number threads important target environment workload sized exact size machine threads 
case processor resources lost thread experiences latency orders magnitude larger simulated io 
granularity flexibility design configuration options richer sm units design finer granularity 
multiprocessor typically add computing units entire processors 
simultaneous multithreading benefit addition single resource functional unit register context instruction issue slot furthermore threads able share resource 
comparisons take advantage flexibility 
processor designers full advantage configurability simultaneous multithreading able construct configurations distance multiprocessing 
reasons performance complexity results shown believe component densities permit put multiple hardware contexts wide issue bandwidth single chip simultaneous multithreading represents efficient organization resources 
related built large number sources 
section note previous instruction level parallelism traditional coarse grain fine grain multithreaded architectures architectures machine multiscalar architecture multiple contexts active simultaneously simultaneous multithreading 
discuss previous studies architectures exhibit simultaneous multithreading contrast particular 
data section provides different perspective previous studies ilp remove barriers parallelism apply real ideal latency hiding techniques measure resulting performance 
smith focus effects fetch decoding dependence checking branch prediction limitations ilp butler examine limitations plus scheduling window size scheduling policy functional unit configuration lam wilson focus interaction branches ilp wall examines scheduling window size branch prediction register renaming aliasing 
previous coarse grain fine grain multithreading provides foundation simultaneous multithreading features simultaneous issuing instructions different threads cycle 
fact architectures single issue superscalar tera wide instructions 
section extended results showing fine grain multithreading runs multiple issue processor 
machine processor cluster schedules instructions execution units cycle cycle basis similar tera scheme 
simultaneous issue instructions multiple threads functional units cycle individual clusters 
franklin multiscalar architecture assigns fine grain threads processors competition execution resources processors case level task individual instruction 
hirata architecture multithreaded superscalar processor simulate performance parallel ray tracing application 
simulate caches tlbs architecture branch prediction mechanism 
show speedups high single threaded architecture threads 
yamamoto analytical model multithreaded superscalar performance simulation 
study models perfect branching perfect caches homogeneous workload threads running trace 
report increases instruction throughput threads 
dally wu describe architectures dynamically interleave operations vliw instructions individual functional units 
dally report speedups high highly parallel applications 
wu examine register file bandwidth requirements threads scheduled manner 
infinite caches show maximum speedup single thread execution parallel applications 
plot increases instruction throughput function fetch bandwidth size dispatch stack 
dispatch stack global instruction window issues fetched instructions 
system threads unlimited functional units unlimited issue bandwidth limited fetch bandwidth 
report near doubling throughput 
contrast studies multithreaded superscalar architectures heterogeneous multiprogrammed workload spec benchmarks model sources latency cache memory tlb branching real instruction latencies detail 
extend previous evaluating variety models sm execution 
look closely reasons resulting performance address shared cache issue specifically 
go comparisons single thread processors compare simultaneous multithreading relevant architectures fine grain superscalar multithreaded architectures single chip multiprocessors 
summary examined simultaneous multithreading technique allows independent threads issue instructions multiple functional units single cycle 
simultaneous multithreading combines facilities available superscalar multithreaded architectures 
models simultaneous multithreading compared wide superscalar fine grain multithreaded single chip multiple issue multiprocessing architectures 
evaluation execution driven simulation model extended dec alpha running multiprogrammed workload composed spec benchmarks compiled architecture multiflow trace scheduling compiler 
results show benefits simultaneous multithreading compared architectures 
model simultaneous multithreaded architecture properly configured achieve times instruction throughput single threaded wide superscalar issue width instructions cycle experiments 

fine grain multithreading switching new thread cycle helps close gap simultaneous multithreading architecture outperforms fine grain multithreading factor 
due inability fine grain multithreading utilize issue slots lost due horizontal waste 

simultaneous multithreaded architecture superior performance multiple issue multiprocessor total number register sets functional units 
achieving specific performance goal requires fewer hardware execution resources simultaneous multithreading 
advantage simultaneous multithreading compared approaches ability boost utilization dynamically scheduling functional units multiple threads 
sm increases hardware design flexibility simultaneous multithreaded architecture tradeoff functional units register sets issue bandwidth achieve better performance add resources fine grained manner 
simultaneous multithreading increases complexity instruction scheduling relative causes shared resource contention particularly memory subsystem 
shown simplified models simultaneous multithreading reach nearly performance general sm model complexity key areas commensurate current show properly tuning cache organization increase performance individual threads sensitive multi thread contention 
acknowledgments john donnell equator technologies digital equipment access source alpha axp version multiflow compiler 
burton smith norm jouppi reviewers helpful comments suggestions research 
agarwal 
performance tradeoffs multithreaded processors 
ieee transactions parallel distributed systems september 
agarwal lim kranz kubiatowicz 
april processor architecture multiprocessing 
th annual international symposium computer architecture pages may 
callahan cummings koblenz porterfield smith 
tera computer system 
international conference supercomputing pages june 

efficient architecture simulation techniques 
winter usenix conference pages january 
butler yeh patt scales 
single instruction steam parallelism greater 
th annual international symposium computer architecture pages may 
jr 
concurrent execution multiple instruction streams superscalar processors 
international conference parallel processing pages august 
dally carter chang lee 
machine architecture 
technical report mit concurrent vlsi architecture memo massachusetts institute technology march 
davis goldschmidt hennessy 
multiprocessor simulation tracing tango 
international conference parallel processing pages ii august 

powerpc 
hot chips vi pages august 

new cpu benchmark suites spec 
compcon spring pages 

overview axp microprocessor 
hot chips vi pages august 
franklin 
multiscalar architecture 
phd thesis university wisconsin madison 
franklin sohi 
expandable split window paradigm exploiting fine grain parallelism 
th annual international symposium computer architecture pages may 
gupta hennessy gharachorloo mowry weber 
comparative evaluation latency reducing tolerating techniques 
th annual international symposium computer architecture pages may 
halstead fujita 
multithreaded processor architecture parallel symbolic computing 
th annual international symposium computer architecture pages may 
hirata kimura nishimura 
elementary processor architecture simultaneous instruction issuing multiple threads 
th annual international symposium computer architecture pages may 
dally 
processor coupling integrating compile time runtime scheduling parallelism 
th annual international symposium computer architecture pages may 
lam wilson 
limits control flow parallelism 
th annual international symposium computer architecture pages may 
laudon gupta horowitz 
interleaving multithreading technique targeting multiprocessors workstations 
sixth international conference architectural support programming languages operating systems pages october 
lowney freudenberger lichtenstein nix :10.1.1.112.9778
multiflow trace scheduling compiler 
journal supercomputing may 

synergistic effect thread scheduling caching multithreaded computers 
compcon spring pages 
nikhil arvind 
dataflow subsume von neumann computing 
th annual international symposium computer architecture pages june 

wu 
benchmark evaluation multi threaded risc processor architecture 
international conference parallel processing pages august 
microprocessor report october 
microprocessor report october 
microprocessor report november 
saavedra barrera culler von eicken 
analysis multithreaded architectures parallel computing 
second annual acm symposium parallel algorithms architectures pages july 
smith 
architecture applications hep multiprocessor computer system 
spie real time signal processing iv pages 
smith 
study branch prediction strategies 
th annual international symposium computer architecture pages may 
sohi franklin 
high bandwidth data memory systems superscalar processors 
fourth international conference architectural support programming languages operating systems pages april 
thekkath eggers 
effectiveness multiple hardware contexts 
sixth international conference architectural support programming languages operating systems pages october 
wall 
limits instruction level parallelism 
fourth international conference architectural support programming operating systems pages april 
weber gupta 
exploring benefits multiple hardware contexts multiprocessor architecture preliminary results 
th annual international symposium computer architecture pages june 
yamamoto serrano talcott wood 
performance estimation superscalar processors 
seventh hawaii internation conferenceon system sciences pages january 
