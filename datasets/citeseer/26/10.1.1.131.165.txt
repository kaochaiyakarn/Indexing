esann proceedings european symposium artificial neural networks bruges belgium april side publi isbn 
fast fixed point algorithm independent subspace analysis hyv rinen urs ster helsinki institute information technology department computer science university helsinki finland 
independent subspace analysis isa hyvarinen hoyer extension ica 
isa components divided subspaces components different subspaces assumed independent components subspace dependencies describe fixed point algorithm isa estimation formulated analogy fastica 
particular give proof quadratic convergence algorithm simulations confirm fast convergence show method prone convergence local minima 
independent component analysis ica successfully past variety data linear model requires independent sources underlying data range limited 
motivates extension ica certain dependencies sources modeled 
isa extension inclusion pooling stage nonlinear transformation augments linear filtering 
pooling organizes filters subspaces inside dependencies allowed 
estimation similar ica follows assumption subspaces mutually independent 
performed maximizing nonlinear contrast function gradient descent quite slow inefficient 
motivates fixed point algorithm isa 
fastica algorithm method combines quick convergence simplicity usability :10.1.1.50.4731
discuss mathematical background isa framework followed convergence proof algorithm simulations showing convergence properties 
model algorithm ica method separating multivariate signal statistically independent components formulated mixing matrix 
inverting system wx identify demixing matrix wish form independence sources maximized 
isa introduce independent feature subspaces 
require independence individual sources norms projections subspaces 
model estimated maximizing independence norms 
define element ui si si urs ster supported scholarship von und foundation 
project group si elements belong th subspace compute norm 
note norm nonlinear mapping method capable modeling complicated dependency structures linear ica capture 
estimation model need maximize independence norms 
define probability distribution model log sm logz square root replaced general nonlinear contrast function 
normalizes distribution ensures unit variance computed closed form choices 

simulations function small arbitrary constant aid stability chosen 
propose new algorithm estimation isa 
estimate components iteratively update rows demixing matrix correspond feature vectors update rule formulated analogy fastica 
denotes expectation value set indices components belonging subspace 

second derivatives nonlinearity 
algorithm requires data whitened 
orthogonalize step equal decorrelation whitened space 
convergence proof show convergence new algorithm change variables update rule kth element vector esann proceedings european symposium artificial neural networks bruges belgium april side publi isbn 
sk denote subspace size consider subspace notational simplicity 
lower index indicates vector taken upper index indicates position vector 
assume near solution perturbation near diagonal vector form 
special case general converge permutation block diagonal matrix blocks size lose generality considering particular case 
follow update step analyze dynamics perturbation introduced 
going show convergence quadratic sufficient write terms linear perturbation show vanish 
notation terms higher smaller order linear terms 
evaluate expression need approximations 
expand occurrences square term follows si denotes inner product vectors th element removed 
separate linear term function taylor expansion second term isi isi expansion applied 
gives evaluating series order esann proceedings european symposium artificial neural networks bruges belgium april side publi isbn 
isi isi substitute expressions original formula update step 
split zt perturbation unperturbed term get sk ksk ks isi isi ksk ks isi isi isi isi analyze behavior values subspaces consideration outside 
subspace consideration nearly converged 
case require change individual variable goes zero algorithm determine subspace arbitrary rotation 
linear term may remain 
larger shall show quadratic convergence 
separate expand sums take expectations clearly see order individual terms 
clarity split 
sk isi sk si jg si jg isi isi isi esann proceedings european symposium artificial neural networks bruges belgium april side publi isbn 
isi sk isi isi isi js jg ksk js jg isi isi sh sk ksk js jg isi isi js jg isi isi terms cancel independent expectation unity due unit norm properties 
second term zero sk separated due independence zero mean sk 
leaves terms proportional squared terms diagonal elements 
order fall category 
shown linear terms vanish established quadratic convergence 
noted proof depend assumption spherical symmetry typically isa 
algorithm converges general dependency structures 
simulations investigate convergence properties generated mixtures supergaussian data embedded subspace structure algorithm identify sources 
data generated samples dimensional white gaussian distribution zero mean unit variance 
divided subspaces dimensionality multiplied member subspace random variable drawn uniform distribution 
serves dual purpose produces required supergaussian distribution introduces dependencies subspaces 
randomly generated mixing matrix obtain observed mixtures whitened 
seen steps required achieve convergence 
investigate algorithm converged local minimum global solution computed matrix initializing randomly observed convergence local minimum 
validated convergence properties starting optimization random point error surface close optimal solution esann proceedings european symposium artificial neural networks bruges belgium april side publi isbn 
log amari index iterations convergence algorithm reaching global minimum dim 
data reaching local minimum dim 
data simulations algorithm convergence fast algorithm usually converges minimum steps 
algorithm initialized correct solution perturbed white noise unit norm 
conditions convergence global minimum random trials 
product mixing filter matrices plotted gives block diagonal matrix global optimum 
residual log amari index corresponds residual error order mainly due assumption infinite expectations 
guaranteed general global minimum error surface 
local minimum reached indicated multiple blocks bottom leftmost position 
log amari index confirming local minimum 
esann proceedings european symposium artificial neural networks bruges belgium april side publi isbn 
known case mixtures artificially generated 
conditions get convergence global minimum starting point close 
depicted fig 
data dimensionality 
converge permuted block diagonal matrix rotations inside subspaces affect likelihood 
amari index subspaces computed adding absolute values elements blocks main diagonal 
fixed point algorithm isa analogous ones 
convergence algorithm proven quadratic 
simulations show convergence fast point problem local minima 
problem local minima probably related model specification encountered due particular algorithm 
hyv rinen :10.1.1.50.4731
fast robust fixed point algorithms independent component analysis 
ieee trans 
neural networks 
comon 
independent component analysis new concept 
signal processing 
ster hyv rinen 
complex cell pooling statistics natural images 
submitted network computation neural systems available online cs helsinki fi koster koster pdf 
cichocki amari 
adaptive blind signal image processing 
learning algorithms applications 
wiley 
hyv rinen bingham 
fast fixed point algorithm independent component analysis complex valued signals 
journal neural systems 
hyv rinen hoyer 
emergence phase shift invariant features decomposition natural images independent feature subspaces 
neural computation 

