discriminative fields modeling spatial dependencies natural images kumar martial hebert robotics institute carnegie mellon university pittsburgh pa hebert ri cmu edu discriminative random fields drf discriminative framework classification natural image regions incorporating neighborhood spatial dependencies labels observed data 
proposed model exploits local discriminative models allows relax assumption conditional independence observed data labels commonly markov random field mrf framework 
parameters drf model learned penalized maximum pseudo likelihood method 
furthermore form drf model allows map inference binary classification problems graph min cut algorithms 
performance model verified synthetic real world images 
drf model outperforms mrf model experiments 
analysis natural images important contextual information form spatial dependencies images 
probabilistic framework leads random field modeling images 
address main challenge involving modeling model arbitrarily complex dependencies observed image data labels principled manner 
literature markov random field mrf commonly model incorporate contextual information 
mrfs generally probabilistic generative framework models joint probability observed data corresponding labels 
words observed data input image yi yi data ith site set sites 
corresponding labels image sites xi mrf framework posterior labels data expressed bayes rule prior labels modeled mrf 
computational tractability observation likelihood model usually assumed factorized form yi xi 
noted researchers assumption restrictive analysis natural images 
example consider class contains man structures buildings 
data belonging class highly dependent neighbors lines edges spatially adjoining sites follow underlying organization rules random see fig 

true large number texture classes structured patterns 
efforts past model dependencies data factored approximations actual likelihood tractability 
addition simplistic forms factors preclude capturing stronger relationships observations form arbitrarily complex features desired discriminate different classes 
considering different point view classification purposes interested estimating posterior labels observations 
generative framework expends efforts model joint distribution involves implicit modeling observations 
discriminative framework models distribution directly 
noted potential advantage discriminative approach true underlying generative model may quite complex class posterior simple 
means generative approach may spend lot resources modeling generative models particularly relevant task inferring class labels 
learning class density models may harder training data limited 
discriminative random field drf model concept conditional random field crf proposed lafferty context segmentation labeling text sequences 
crfs directly model posterior distribution gibbs field 
approach allows capture arbitrary dependencies observations resorting model approximations 
model enhances crfs proposing local discriminative models capture class associations individual sites interactions neighboring sites grid lattices 
proposed model uses local discriminative models achieve site classification permitting interactions observed data label field principled manner 
research alleviates problems previous version described 
discriminative random field restate notations definition conditional random fields lafferty 
concerned binary classification xi 
observed data site yi crf definition graph indexed vertices said conditional random field conditioned random variables xi obey markov property respect graph xi xi set nodes node ni set neighbors node represents set labels nodes set 
crf random field globally conditioned observations condition positivity requiring assumed implicitly 
hammersley clifford theorem assuming pairwise clique potentials nonzero joint distribution labels observations written exp ai xi iij xi xj ni normalizing constant known partition function ai iij unary pairwise potentials respectively 
slight abuse notations rest call ai association potential iij interaction potential 
note terms explicitly depend observations association potential seen local decision term decides association site certain class ignoring neighbors 
interaction potential seen data dependent smoothing function 
simplicity rest assume random field homogeneous isotropic functional forms ai iij independent locations henceforth leave subscripts simply notations note assumption isotropy easily relaxed cost additional parameters 
association potential drf framework xi modeled local discriminative model outputs association site class xi 
generalized linear models glm extensively statistics model class posteriors observations 
site function maps observations feature vector logistic function link local class posterior modeled xi wt model parameters 
extend logistic model induce nonlinear decision boundary feature space transformed feature vector site defined hi fi 
fi 
arbitrary nonlinear functions 
element transformed vector kept accommodate bias parameter 
xi probability compactly expressed xi hi 
association potential defined xi log hi transformation sure drf yields standard logistic classifier interaction potential set zero 
note transformed feature vector site hi function set observations contrary assumption conditional independence data mrf framework allows data particular site yi get log likelihood acts association potential 
related context tree structured belief networks feng scaled likelihoods approximate actual likelihoods site required generative formulation 
scaled likelihoods obtained scaling local class posteriors learned neural network 
contrary drf model local class posterior integral part full conditional model 
parameters association interaction potential learned simultaneously drf framework 
interaction potential model interaction potential analyze interaction potential commonly mrf framework 
note mrf framework permit data interaction potential 
homogeneous isotropic ising model interaction potential xixj penalizes dissimilar pair labels cost 
form interaction prefers piecewise constant smoothing explicitly considering discontinuities data 
drf formulation interaction potential function observations similar labels pair sites observed data supports hypothesis 
words interested learning pairwise discriminative model interaction potential 
pair sites ij new feature vector ij denoting feature vector ij simplification interaction potential modeled xi xj ij model parameters 
note component ij fixed accommodate bias parameter 
form interaction potential simpler proposed parameter learning convex problem 
interesting properties interaction potential 
association potential site interaction potentials pairwise cliques pair set zero drf acts logistic classifier yields probability site pair labels observed data 
second proposed interaction potential generalization ising model 
original ising form recovered components vector bias parameter set zero 
form acts data dependent discontinuity adaptive model moderate smoothing data sites different 
data dependent smoothing especially useful absorb errors modeling association potential 
anisotropy easily included drf model interaction potentials different directional pairwise cliques different sets parameters parameter learning inference set drf parameters 
form drf model resembles posterior mrf framework assuming conditionally independent data 
mrf framework parameters class generative models yi xi parameters prior random field labels generally assumed independent learned separately 
contrast assumption learn parameters drf simultaneously 
maximum likelihood approach learn drf parameters involves evaluation partition function general np hard problem 
sampling techniques resort approximations pseudo likelihood estimate parameters 
pseudo likelihood formulation due simplicity consistency estimates large lattice limit 
pseudo likelihood approach factored approximation xi 
ising model mrfs pseudo likelihood tends overestimate interaction parameter causing map estimates field poor solutions 
experiments previous section verify observations interaction parameters 
alleviate problem take bayesian approach get maximum posteriori estimates parameters 
similar concept weight decay neural learning literature assume gaussian prior interaction parameters identity matrix 
prior parameters leads weight decay shrinkage beneficial leave exploration 
prior parameters assumed uniform 
independent training images arg max log hi ij log zi vt zi xi ni exp log hi ij ni penalized log pseudo likelihood convex respect model parameters easily maximized gradient descent 
related regarding estimation mackay suggested type ii marginal likelihood 
drf formulation integrating parameters hard problem 
choice integrate choosing non informative hyperprior 
experiments showed methods yield estimates parameters pseudo likelihood framework 
choose cross validation 
alternative ways parameter estimation include contrastive divergence saddle point approximations resembling perceptron learning rules 
currently exploring possibilities 
problem inference find optimal label configuration image optimality defined respect cost function 
current map estimate solution inference problem 
ising mrf model binary classification problems exact map solution computed mincut max flow algorithms provided 
drf model map estimates obtained algorithms 
algorithms allow negative interaction sites data dependent smoothing clique set vt ij max ij yielding approximate map estimate 
equivalent switching smoothing image discontinuities 
experiments discussion comparison mrf framework learned assuming conditionally independent likelihood homogeneous isotropic ising interaction model 
mrf posterior exp log si yi xi ni xixj interaction parameter si yi single site feature vector ith site si yi note si yi take account influence data neighborhood ith site 
order neighborhood nearest neighbors label interaction experiments 
synthetic images aim experiments obtain correct labels corrupted binary images 
base images pixels experiments top row fig 

compare drf mrf results different noise models 
noise model images generated base image 
pixel considered image site feature vector si yi simply chosen scalar representing intensity ith site 
experiments synthetic data neighborhood data interaction si yi observe gains due discriminative models association interaction potentials 
linear discriminant implemented association potential hi fi pairwise data vector ij obtained absolute difference si yi sj yj 
mrf model class conditional density si yi xi modeled gaussian 
noisy data left base image fig training noisy images rest base images testing 
experiments conducted noise model 
interaction parameters drf mrf set zero 
reduces drf model logistic classifier mrf maximum likelihood ml classifier 
ii parameters drf mrf learned pseudo likelihood approach penalty 
iii drf parameters learned penalized pseudo likelihood best mrf chosen cross validation 
map estimates labels obtained graph cuts models 
noise model image pixel corrupted independent gaussian noise standard deviation 
drf parameter learning chosen 
pixelwise classification error noise model top row table 
form noise likelihood model mrf mrf table pixelwise classification errors synthetic test images 
gaussian noise mrf drf give similar error bimodal noise drf performs better 
note label interaction data interaction tests see text 
noise ml logistic mrf pl drf pl mrf drf gaussian bimodal results synthetic images 
top row original images second row images corrupted bimodal noise third row mrf results fourth row drf results 
expected give results 
drf model marginally better mrf case 
note drf mrf results worse parameters learned penalizing pseudo likelihood shown table suffix pl 
map inference yields images parameters 
drf model affected parameters learned simultaneously mrfs 
second noise model pixel corrupted independent mixture gaussian noise 
class mixture gaussians equal mixing weights yielding bimodal class noise 
mixture model parameters mean std classes chosen inspired 
classification results shown bottom row table 
interesting point note drf yields lower error mrf logistic classifier higher error ml classifier test data 
typical noisy version base images performance different techniques compared fig 

table detection rates dr false positives fp test set containing images sites 
fp logistic classifier kept drf dr comparison 
superscript indicates neighborhood data interaction 
mrf logistic drf logistic drf dr fp image real world images proposed drf model applied task detecting man structures natural scenes 
aim label image site structured 
training test set contained images respectively size pixels corel image database 
nonoverlapping pixels block called image site 
image site dim single site feature vector si yi dim multiscale feature vector computed orientation magnitude features described 
note incorporates data interaction neighboring sites 
association potentials transformed feature vector hi computed site quadratic transforms vector 
pairwise data vector ij obtained concatenating vectors 
drf parameter learning chosen 
mrf class conditional density modeled mixture gaussians 
single gaussian class yielded poor results 
typical images test set detection results mrf drf models fig 

blocks detected structured shown enclosed artificial boundary 
drf results show higher detections lower false positives 
quantitative evaluation compared detection rates number false positives image different techniques 
comparison detection rates experiments decision threshold logistic classifier fixed yields false positive rate drf 
set experiments conducted single site features methods 
neighborhood data interaction logistic classifier drf si yi 
comparative results methods table mrf logistic drf 
detection rates mrf drf higher logistic classifier due label interaction 
higher detection rate lower false positives drf comparison mrf indicate gains due discriminative models association interaction potentials drf 
experiment take advantage power drf framework data interaction allowed logistic classifier drf logistic drf table 
drf detection rate increases substantially false positives decrease indicating importance allowing data interaction addition label interaction 
discriminative random fields provide principled approach combining local discriminative classifiers allow arbitrary overlapping features adaptive data dependent smoothing label field 
currently exploring alternative ways parameter learning contrastive divergence saddle point approximations 
aspects drf model general kernel mappings increase classification accuracy 
need method induce sparseness avoid overfitting 
addition intend extend model accommodate multiclass classification problems 
acknowledgments john lafferty jonas august immensely helpful discussions 
example structure detection results 
left column mrf results 
right column drf results 
drf higher detection rate lower false positives 
li 
markov random field modeling image analysis 
springer verlag tokyo 
feng williams 
combining belief networks neural networks scene segmentation 
ieee trans 
pattern anal 
machine intelli 
cheng bouman 
multiscale bayesian segmentation trainable context model 
ieee trans 
image processing 
wilson li 
class discrete multiresolution random fields application image segmentation 
ieee trans 
pattern anal 
machine intelli 
rubinstein hastie 
discriminative vs informative learning 
proc 
third int 
conf 
knowledge discovery data mining pages 
lafferty mccallum pereira 
conditional random fields probabilistic models segmenting labeling sequence data 
proc 
int 
conf 
machine learning 
kumar hebert 
discriminative random fields discriminative framework contextual interaction classification 
ieee int 
conf 
computer vision 
mccullagh nelder 
generalised linear models 
chapman hall london 
greig 
exact maximum posteriori estimation binary images 
journal royal statis 
soc 
mackay 
bayesian non linear modelling energy prediction competition 
maximum entropy bayesian methods pages 
williams 
bayesian regularization pruning laplacian prior 
neural computation 
figueiredo 
adaptive sparseness jeffreys prior 
advances neural information processing systems nips 
hinton 
training product experts minimizing contrastive divergence 
neural computation 
collins 
discriminative training methods hidden markov models theory experiments perceptron algorithms 
proc 
conference empirical methods natural language processing emnlp 
kolmogorov zabih 
energy functions minimized graph cuts 
proc 
european conf 
computer vision 
kumar hebert 
man structure detection natural images causal multiscale random field 
proc 
ieee int 
conf 
comp 
vision pattern recog june 
