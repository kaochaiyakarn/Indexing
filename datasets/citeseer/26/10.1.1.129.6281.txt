ieee transactions neural networks vol 
march comparison methods multiclass support vector machines support vector machines svms originally designed binary classification 
effectively extend multiclass classification ongoing research issue 
methods proposed typically construct multiclass classifier combining binary classifiers 
authors proposed methods consider classes 
computationally expensive solve multiclass problems comparisons methods large scale problems seriously conducted 
especially methods solving multiclass svm step larger optimization problem required experiments limited small data sets 
give decomposition implementations methods 
compare performance methods binary classifications directed acyclic graph svm dagsvm 
experiments indicate dag methods suitable practical methods 
results show large problems methods considering data general need fewer support vectors 
index terms decomposition methods multiclass classification support vector machines svms 
support vector machines svms originally designed binary classification 
effectively extend multiclass classification ongoing research issue 
currently types approaches multiclass svm 
constructing combining binary classifiers directly considering data optimization formulation 
comparisons cover methods 
formulation solve multiclass svm problems step variables proportional number classes 
multiclass svm methods binary classifiers constructed larger optimization problem needed 
general computationally expensive solve multiclass problem binary problem number data 
experiments limited small data sets 
give decomposition implementation methods 
compare performance methods binary classification dagsvm 
manuscript received april revised august 
supported part national science council taiwan nsc 
authors department computer science information engineering national taiwan university taipei taiwan mail cjlin csie ntu edu tw 
publisher item identifier 
chih wei hsu chih jen lin ieee note pointed primal forms proposed equivalent 
methods mentioned implementations multiclass svm 
example 
due limit space conduct experiments 
earlier comparison methods 
section ii review directed acyclic graph svm dagsvm methods solving binary classifications 
section iii give brief method considers classes show decomposition method proposed applied 
method considers variables crammer singer discussed section iv 
numerical experiments section show dag methods suitable practical methods 
results show large problems method proposed considering variables generally needs fewer support vectors 
discussions section vi 
ii 
dagsvm methods earliest implementation svm multiclass classification probably method example 
constructs svm models number classes 
th svm trained examples th class positive labels examples negative labels 
training data class th svm solves problem training data mapped higher dimensional space function penalty parameter 
minimizing means maximize margin groups data 
data linear separable penalty term reduce number training errors 
basic concept svm search balance regularization term training errors 
ieee transactions neural networks vol 
march solving decision functions say class largest value decision function class practically solve dual problem number variables number data 
variable quadratic programming problems solved 
major method called method 
introduced strategy svm 
method constructs classifiers trained data classes 
training data th th classes solve binary classification problem different methods doing testing classifiers constructed 
tests decide voting strategy suggested says th class vote th class added 
th increased 
predict class largest vote 
voting approach described called max wins strategy 
case classes identical votes thought may strategy simply select smaller index 
practically solve dual number variables number data classes 
average class data points solve quadratic programming problems variables 
third algorithm discussed directed acyclic graph svm dagsvm proposed 
training phase method solving binary svms 
testing phase uses rooted binary directed acyclic graph internal nodes leaves 
node binary svm th th classes 
test sample starting root node binary decision function evaluated 
moves left right depending output value 
go path reaching leaf node indicates predicted class 
advantage dag analysis generalization established 
similar theoretical results methods 
addition testing time method 
implemented methods modifying svm software libsvm 
iii 
method considering data decomposition implementation approach multiclass problems solving single optimization problem proposed 
idea similar approach 
constructs class rules th function separates training vectors class vectors 
decision functions obtained solving problem 
formulation follows decision function method 
binary svm easier solve dual problem 
dual formulation decision function explain equivalent formulations hsu lin comparison methods multiclass support vector machines constraints 
optimal solution adding affect optimal solution set 
implies optimal solution set 
similar arguments prove relation 
equivalence formulations discussed 
note variables zero 
say variables 
unfortunately considered impractical formula due huge amount variables 
small problems tested 
rest section discuss possible implementation solving larger problems 
remember solving binary svm main difficulty density kernel matrix general zero 
currently decomposition method major method solve binary support vector machines 
iterative process iteration index set variables separated sets working set 
iteration variables corresponding fixed subproblem variables corresponding minimized 
size selection contents important issues designing decomposition method 
example sequential minimal optimization smo platt considers variables iteration 
iteration decomposition method sub problem variables analytically solved 
optimization software needed 
working set selections binary case may dual problem 
linear constraint dual linear constraints 
think system linear equations variables 
note size working set number variables change iteration 
may determined linear system equations variables solution decomposition algorithm moved 
general select variables working set iteration 
easy task bounded constraints considered systematic way ensures selection leads decrease objective function 
unfortunately far effective ways doing 
working consider problem adding objective function dual problem linear constraints removed decision function idea bounded formulations binary classification proposed 
class problems detailed numerical study showing bounded formulation achieve similar accuracy standard svm software proposed 
linear constraints hope problem selecting working set decomposition method easier 
course clear accuracy affected terms added objective function 
see results experiment section 
decomposition method applied 
rewrite general form vector matrix 
iteration subproblem follows permutation matrix size working set 
note variables select working set 
fixed ieee transactions neural networks vol 
march assume solution current iteration size working set 
working set selection number free variables calculate select indexes largest elements free select smallest elements main idea working set selection feasible direction method optimal solution vector defined selecting smallest elements selecting components best steepest descent direction 
early implementations decomposition methods 
pointed selection working set may lead slow convergence difficult problems 
theoretical experimental analyzes authors proposed include largest elements corresponding free 
size working set pick indexes smallest elements largest corresponding free 
major difficulty implementation lies calculating elements mainly accessed 
complicated form essential avoid possible computational overheads 
importantly form symmetric indexes increases difficulty writing explicit form quadratic formulation written symmetric form reformulate 
explained 
note nonzero vector positive semidefinite convex optimization problem 
direct calculation 
easy see third terms obtained 
second term discuss explicit form particular show column consider vector form addition assume position quadratic terms compose groups 
clearly part forms symmetric matrix follows 
includes elements implies th column row indexes corresponding contributed part 
represent 
hsu lin comparison methods multiclass support vector machines third term clearly forms symmetric matrix 
similarly say th column row indexes corresponding include complicated second part symmetric form 
symmetric property quadratic term th column elements corresponding respectively 
summary follows th column obtained obtain column vector initialize th column zero vector 
elements corresponding add third part elements corresponding second part minus elements corresponding add part elements corresponding minus second part dense fully dense matrix 
number nonzero elements practical implementations compute cache elements reduction cached matrix improve training time algorithm 
different implementations procedure 
situation may vary different computational environment get details 
implemented method extension libsvm experiments section addition software available www csie ntu edu tw cjlin 
iv 
method crammer singer crammer singer proposed approach multiclass problems solving single optimization problem 
give decomposition implementation 
basically solves primal problem decision function main difference uses slack variables gap decision planes maximum numbers considered addition contain coefficients note explicitly write constraints exactly dual problem write dual objective function written identity matrix kronecker product 
positive semidefinite hessian dual objective function positive semidefinite 
way explain convex optimization problem 
ieee transactions neural networks vol 
march decision function main difference linear constraints equations interesting come karush kuhn tucker kkt condition different primal variables 
due unconstrained variables unconstrained variables addition simpler equations involves exactly variables 
sense say contains independent equations 
advantage remove linear constraints easier directly conduct working set selections decomposition method 
authors proposed choose variables associated working set 
elements working set selection index discussed 
subproblem addition vector elements zero th component main reason setting simple problem 
algorithm proposed simple iterative approach 
method 
addition easier systematically calculate complicated derivations previous section avoided 
note gradient dual objective function 
vector calculated follows information gradient iterations essential keep gradient updated 
done new obtained operations needed 
discuss selection working set stopping condition decomposition method 
kkt condition requires equivalent rewrite iterations select working set words component groups variables select largest violation kkt condition 
binary svm choosing indexes violate kkt condition common strategy choose group 
variables selected satisfy kkt condition subproblem solving guarantee strict decrease objective function dual problem 
stopping criterion stopping tolerance 
convergence decomposition method proved 
addition shows limit hsu lin comparison methods multiclass support vector machines goes zero number iterations goes infinity 
finite number iterations decomposition method stops satisfied 
implementation part publicly available users 
numerical experiments data implementation section experimental results problems statlog collection uci repository machine learning databases 
uci repository choose datasets iris wine glass vowel 
problems tested 
statlog collection choose multiclass datasets vehicle segment dna satimage letter shuttle 
note problem dna scale training data 
test data adjusted accordingly 
problem dna scale binary attributes 
give problem statistics table problems test sets available 
note problems glass satimage missing class 
original application class data set examples class 
column give best test rate listed statlog homepage 
note best rates obtained different learning methods 
important criterion evaluating performance methods accuracy rate 
unfair parameter set compare methods 
practically method people find best parameters performing model selection 
conducted training data test data assumed unknown 
best parameter set constructing model testing 
note details conduct model selection discussed section 
reduce search space parameter sets train datasets rbf kernel addition methods solving binary svms dag model consider binary problems 
note table problem statistics issue arise methods model corresponds optimization problem 
similar stopping criteria methods 
problem optimization algorithm kkt violation precise dual problem approaches general form similar derivation stopping criterion method crammer singer approach simpler vector unfortunately stopping criteria nearly fully comparable due different size dual problems approaches 
elaborate issue section vi 
note problems letter shuttle relaxed tolerance method crammer singer takes training time 
information stopping criteria decomposition method 
computational experiments section done pentium iii mb ram gcc compiler 
optimization problem binary svms approaches allocate mb memory cache storing kernel elements 
element stored cache double precision 
size cache single precision number elements stored cache doubled 
implementations double precision 
implementing approaches decomposition method shrinking technique reducing ieee transactions neural networks vol 
march training time 
precise variables bounds shrinking technique reduces size working problem considering free variables 
implemented shrinking technique methods 
methods binary classifiers details section 
methods implementation sophisticated 
disadvantage methods 
consider optimization problem problem complicated practical implementations 
results discussions problem estimate generalized accuracy different kernel parameters cost parameters problem try combinations 
criteria estimate generalized accuracy 
datasets dna satimage letter shuttle training testing sets available pair validation performance measured training training set testing training set 
train training set pair achieves best validation rate predict test set 
resulting accuracy rate column table ii 
note accuracy validation stage apply test data report highest rate 
smaller datasets test data may available simply conduct fold cross validation training data report best cross validation rate 
table ii presents result comparing methods 
optimal parameters corresponding accuracy rates 
note column means method crammer singer 
seen optimal parameters various ranges different problems essential test parameter sets 
observe accuracy similar 
statistically better 
comparing earlier results listed statlog see table ii rbf kernel best rates bold faced column table accuracy obtained svm competitive better 
example problems dna shuttle approach obtains better accuracy satimage letter 
problems accuracy close table report training time testing time number unique support vectors table iii 
note results solving optimal model 
small problems testing time conduct cross validation 
say unique support vectors training data may correspond different nonzero dual variables 
example approaches training data may support vector different binary classifiers 
methods variables data may associate different nonzero dual variables 
report number training data corresponds nonzero dual variable 
explain main factor affects testing time 
note number support vectors problems integers 
average fold cross validation 
training time dag methods best 
fact methods training procedure 
train classifiers problem smaller data classes total training time 
note table iii training time dag methods may quite different problem vehicle 
due difference optimal parameter sets 
improve method efforts section iii training speed remains slow 
convergence speed method crammer singer 
especially problems iris vehicle shuttle training time huge 
problems note optimal parameter quite large 
experience shows working set selection convergence decomposition method may slow large difficulty working set selection 
iteration equalities involved hsu lin comparison methods multiclass support vector machines table iii training time testing time number support vectors time seconds best training test time bold faced number svs italicized may interactions groups variables 
regarding testing time decision function complicated binary case experimental results indicate general testing time dominated kernel evaluations 
note save testing time calculate store unique support vector test data 
may places decision function 
observe small kernel evaluations take testing time 
say general testing time proportional number unique support vectors 
observe dag methods dag really little faster testing time 
discuss number support vectors 
see larger problems method returns fewer support vectors binary approaches 
table iv linear kernel best rates bold faced note generally observe parameter set needs fewer support vectors 
consistent results 
really comparison optimal parameters vary approaches 
especially small problems optimal parameters different 
large problems optimal parameters similar table ii svs column method really shows smaller numbers 
testing time important method option 
hand draw method crammer singer 
needs support vectors number huge 
note problem dna parameters get best result validation stage 
applying test data accuracy 
table ii result smallest number support vectors 
ieee transactions neural networks vol 
march training time factors similar approaches 
suggest dag approaches suitable practical 
complete analysis test methods linear kernel results table iv 
due limit computational time report small problems 
parameter test different report best rate 
comparing table ii difference best rates apparent 
method returns worst accuracy problems 
dag perform 
comparison linear nonlinear kernels reveals necessity nonlinear kernels situations 
observation rbf kernel produces better accuracy important need study decomposition methods specially designed nonlinear case 
effective methods solve large problems linear kernel 
draw remarks implementation methods 
training time method improved parameter set binary problems treated independently 
kernel elements solving binary problem stored passed binary problems kernel matrix 
kernel element may calculated times 
expect improvements compete dag training time 
approaches caches implemented problems involved model share 
hand approaches different models different parameter sets fully independent 
caches passing kernel elements model 
vi 
discussion note difference methods method crammer singer include bias terms wondering may affect training time 
give brief discussion issue 
added kkt condition dual additional equalities table number iterations solving difficulty working set selection happens may add objective function 
dual problem solved decomposition method described section iv 
decision function modify code optimal parameters listed column table iii comparison number iterations solving table provide results large problems 
clearly seen adding bias term performance better 
clear number iterations nearly doubled surprising demonstrated binary svm objective function performance decomposition method quite different 
realize working set selection may fast convergence 
second issue discussed stopping criteria 
stopping criteria derivation affected problem size 
smaller dual problem fewer variables involved calculation 
sense approaches smaller dual problems take advantages say earlier 
possible remedy divide left hand side size dual problem 
investigation needed issue 
differences methods take training time remain table iii see approaches problems training time longer 
example method crammer hsu lin comparison methods multiclass support vector machines singer solve variable problem problems letter shuttle relax stopping tolerance 
divide left hand side number classes 
training time compete approach solves dual problems variables 
discussed decomposition implementations methods compared methods binary classifiers dag 
experiments large problems show method dag may suitable practical 
test data large number classes 
especially people suspected may differences methods data set points classes 
acknowledgment part implementations benefited 
chang 
authors weston elisseeff keerthi singer helpful comments 
blake merz 
uci repository machine learning databases 
univ california dept inform 
comput 
sci irvine ca 
online 
available www ics uci edu mlearn ml repository html bottou cortes denker drucker guyon jackel lecun muller sackinger simard vapnik comparison classifier methods case study handwriting digit recognition proc 
int 
conf 
pattern recognition 
pp 

bredensteiner bennett multicategory classification support vector machines comput 

pp 

libsvm library support vector machines 
chang 
lin 

online 
available www csie ntu edu tw cjlin libsvm chin support vector machines applied speech pattern classification master thesis univ cambridge cambridge 
cortes vapnik support vector network machine learning vol 
pp 

crammer singer learnability design output codes multiclass problems comput 
theory pp 

ultraconservative online algorithms multiclass problems school comput 
sci 
eng hebrew univ tech 
rep 
friedman 
approach classification 
dept statist stanford univ stanford ca 
online 
available www stat stanford edu reports friedman poly ps 
cristianini campbell kernel algorithm fast simple learning procedure support vector machines proc 
th int 
conf 
machine learning 
combining discriminant models new multiclass svms loria campus scientifique neuro colt tech 
rep nc tr 

hsu 
lin simple decomposition method support vector machines machine learning vol 
pp 

joachims making large scale svm learning practical advances kernel methods support vector learning sch lkopf burges smola eds 
cambridge ma mit press 
kindermann leopold multi class classification error correcting codes der gi lernen leopold kirsten eds gmd rep 
personnaz dreyfus single layer learning revisited stepwise procedure building training neural network neurocomputing algorithms architectures applications fogelman ed 
new york springer verlag 
kre el pairwise classification support vector machines advances kernel methods support vector learning sch lkopf burges smola eds 
cambridge ma mit press pp 


lin formal analysis stopping criteria decomposition methods support vector machines ieee trans 
neural networks published 
mangasarian successive overrelaxation support vector machines ieee trans 
neural networks vol 
pp 

alpaydin support vector machines multi class classification vol 
pp 

michie spiegelhalter taylor 
machine learning neural statistical classification online 
available ftp ncc pt pub statlog osuna freund girosi training support vector machines application face detection proc 
cvpr 
platt fast training support vector machines sequential minimal optimization advances kernel methods support vector learning sch lkopf burges smola eds 
cambridge ma mit press 
platt cristianini shawe taylor large margin dag multiclass classification advances neural information processing systems 
cambridge ma mit press vol 
pp 

saunders stitson weston bottou sch lkopf smola support vector machine manual univ london london tech 
rep csd tr 
vapnik statistical learning theory 
new york wiley 
weston private communication 
weston watkins multi class support vector machines proc 
esann verleysen ed brussels belgium 
methods feasible directions 
amsterdam netherlands elsevier 
chih wei hsu received degrees computer science information engineering national taiwan university taipei taiwan respectively 
research interests include machine learning data mining applications 
chih jen lin received degree mathematics national taiwan university taipei taiwan 
received ph degrees department industrial operations engineering university michigan ann arbor 
september assistant professor department computer science information engineering national taiwan university 
research interests include machine learning numerical optimization various applications operations research 
