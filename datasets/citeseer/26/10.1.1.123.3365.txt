ieee transactions pattern analysis machine intelligence vol 
march combining classifiers josef kittler member ieee computer society robert duin jiri matas develop common theoretical framework combining classifiers distinct pattern representations show existing schemes considered special cases compound classification pattern representations jointly decision 
experimental comparison various classifier combination schemes demonstrates combination rule developed restrictive assumptions sum rule outperforms classifier combinations schemes 
sensitivity analysis various schemes estimation errors carried show finding justified theoretically 
index terms classification classifier combination error sensitivity 
ultimate goal designing pattern recognition systems achieve best possible classification performance task hand 
objective traditionally led development different classification schemes pattern recognition problem solved 
results experimental assessment different designs basis choosing classifiers final solution problem 
observed design studies designs yield best performance sets patterns misclassified different classifiers necessarily overlap 
suggested different classifier designs potentially offered complementary information patterns classified harnessed improve performance selected classifier 
observations motivated relatively interest combining classifiers 
idea rely single decision making scheme 
designs subset decision making combining individual opinions derive consensus decision 
various classifier combination schemes devised experimentally demonstrated consistently outperform single best classifier 
presently inadequate understanding combination schemes better circumstances 
main reasons combining classifiers efficiency accuracy 
increase efficiency adopt multistage combination rules objects classified simple classifier small set cheap features kittler matas centre vision speech signal processing school electronic engineering information technology mathematics university surrey guildford gu xh united kingdom 
mail kittler ee surrey ac uk 
era technology road kt sa united kingdom 
mail ee surrey ac uk 
duin department applied physics delft university technology cj delft netherlands 
mail bob ph tn tudelft nl 
manuscript received june revised jan 
recommended acceptance hull 
information obtaining reprints article please send mail tpami computer org ieeecs log number 
ieee combination reject option 
difficult objects complex procedures possibly different features sequential pipelined hierarchical 
studies gradual reduction set possible classes 
combination ensembles neural networks different studied neural network literature 
important issue combining classifiers particularly useful different see 
achieved different feature sets different training sets randomly selected cluster analysis 
possible application multistage classifier may stabilize training classifiers small sample size bootstrapping 
variance reduction studied context multiple discriminant function classifier multiple probabilistic classifiers 
classifier combination strategies may reflect local competence individual experts exemplified training process may aim encourage experts achieve local decision making superiority boosting method freund shapire 
interesting issue research concerning classifier ensembles way combined 
labels available majority vote 
label ranking 
continuous outputs posteriori probabilities supplied average linear combination suggested 
depends nature input classifiers feature space theoretically justified 
interesting study possibilities 
classifier outputs interpreted fuzzy membership values belief values evidence fuzzy rules belief functions dempster shafer techniques 
possible train output classifier separately outputs input classifiers new features 
point view analysis basically classifier combination scenarios 
scenario classifiers representation input pattern 
typical example category set nearest kittler combining classifiers neighbor classifiers measurement vector different classifier parameters number nearest neighbors distance metrics determining nearest neighbors 
example set designs neural network classifier fixed architecture having distinct sets weights obtained means different training strategies 
case classifier input pattern considered produce estimate posteriori class probability 
second scenario classifier uses representation input pattern 
words measurements extracted pattern unique classifier 
important application combining classifiers scenario possibility integrate physically different types measurements features 
case longer possible consider computed posteriori probabilities estimates functional value classification systems operate different measurement spaces 
focus classifier combination second scenario 
develop common theoretical framework classifier combination show existing schemes considered special cases compound classification representations jointly decision 
demonstrate different assumptions different approximations derive commonly classifier combination schemes product rule sum rule min rule max rule median rule majority voting 
various classifier combination schemes compared experimentally 
surprising outcome comparative study combination rule developed restrictive assumptions sum rule outperforms classifier combinations schemes 
explain empirical finding investigate sensitivity various schemes estimation errors 
sensitivity analysis shows sum rule resilient estimation errors 
summary contribution twofold 
provide theoretical underpinning existing classifier combination schemes fusing decisions multiple experts employing distinct pattern representation 
furthermore analysis sensitivity schemes estimation errors enhances understanding properties 
byproduct offer methodological machinery developing classifier combination strategies predicting behavior 
problem classifier combination complex issues explanation 
include effect individual expert error distributions choice combination strategy explicit differentiation decision ambiguity competence confidence relationship dimensionality reduction multiple expert fusion implicit dimensionality expansion 
practical decision making schemes complex sequential kind special rules handle rejects exceptions currently difficult envisage results bear design schemes 
theoretical framework analysis small step considerably improved understanding classifier combina tion needed order harness benefits multiple expert fusion full potential 
organized follows 
section formulate classifier combination problem introduce necessary notation 
section derive basic classifier combination schemes product rule sum rule 
basic schemes developed classifier combination strategies section 
combination rules derived sections experimentally compared sections 
section investigates sensitivity basic classifier combination rules estimation errors 
section summarizes main results offers concluding remarks 
theoretical framework consider pattern recognition problem pattern assigned possible classes mh 
assume classifiers representing pattern distinct measurement vector 
denote measurement vector ith classifier xi 
measurement space class modeled probability density function priori probability occurrence denoted 
shall consider models mutually exclusive means model associated pattern 
bayesian theory measurements xi pattern assigned class provided posteriori probability interpretation maximum assign max bayesian decision rule states order utilize available information correctly reach decision essential compute probabilities various hypotheses considering measurements simultaneously 
course correct statement classification problem may practicable proposition 
computation posteriori probability functions depend knowledge high order measurement statistics described terms joint probability density functions difficult infer 
shall attempt simplify rule express terms decision support computations performed individual classifiers exploiting information conveyed vector xi 
shall see rule computationally manageable lead combination rules commonly practice 
approach provide scope development range efficient classifier combination strategies 
shall commence rule consider expressed certain assumptions 
rewrite posteriori probability xr bayes theorem 
ieee transactions pattern analysis machine intelligence vol 
march wk xr xr unconditional measurement joint probability density 
expressed terms conditional measurement distributions xr concentrate numerator terms 
product rule pointed represents joint probability distribution measurements extracted classifiers 
assume representations conditionally statistically independent 
different representations may probable cause independence special cases 
investigate consequences assumption write measurement process model ith representation 
substituting find obtain decision rule assign jj jj kh ki max terms posteriori probabilities yielded respective classifiers assign ar ar jj ij kh ii max decision rule quantifies likelihood hypothesis combining posteriori probabilities generated individual classifiers means product rule 
effectively severe rule fusing classifier outputs sufficient single recognition engine inhibit particular interpretation outputting close zero probability 
shall see undesirable implication decision rule combination classifiers worst case provide respective opinions hypothesized class identity accepted rejected 
sum rule consider decision rule detail 
applications may appropriate assume posteriori probabilities computed respective classifiers deviate dramatically prior probabilities 
strong assumption may readily satisfied available observational discriminatory information highly ambiguous due high levels noise 
situation assume posteriori probabilities expressed ii ki satisfies ki 
substituting posteriori probabilities find ar fc kh ii kh expand product neglect terms second higher order approximate right hand side kh kh kh ki substituting obtain sum decision rule assign rp ew jj pew xij rp kh pd ii nm qp comments proceeding section develop specific classifier combination strategies decision rules pause elaborate assumptions derive product sum rules 
concede conditional independence assumption may deemed unrealistic situations 
important points borne mind results rest applications conditional independence assumption hold 
applications assumption provide adequate workable approximation reality may complex 
draw parallel gaussian assumption frequently situations class distributions patently obey exponential law simplification yields acceptable results 
importantly shall see section derived classifier combination schemes assumption routinely practice 
analysis provides plausible theoretical underpinning combination rules draws attention underlying assumptions schemes users may aware 
far sum rule concerned assumption posterior class probabilities deviate greatly priors unrealistic applications 
kittler combining classifiers observations pattern convey significant discriminatory information sum approximation product introduce gross approximation errors 
shall show section injection errors compensated relatively low sensitivity approximation estimation errors 
classifier combination strategies decision rules constitute basic schemes classifier combination 
interestingly commonly classifier combination strategies developed rules noting ii ii min relationship suggests product sum combination rules approximated upper lower bounds appropriate 
furthermore hardening posteriori probabilities produce binary valued functions dki ki max xij results combining decision outcomes combining posteriori probabilities 
approximations lead rules max rule starting approximating sum maximum posterior probabilities obtain assign rp ew jj rmax pew xij kh ii nm max rmax pw assumption equal priors simplifies assign ij ii max max max min rule starting bounding product posterior probabilities obtain assign ar ew pew xij ar fc kh ii max min assumption equal priors simplifies qp assign min pew xij max min median rule note equal prior assumption sum rule viewed computing average posteriori probability class classifier outputs assign xi max xi rule assigns pattern class average posteriori probability maximum 
classifiers outputs posteriori probability class outlier affect average turn lead incorrect decision 
known robust estimate mean median 
appropriate base combined decision median posteriori probabilities 
leads rule assign ij ii med max med majority vote rule starting assumption equal priors hardening probabilities find assign ji max note class sum right hand side simply counts votes received hypothesis individual classifiers 
class receives largest number votes selected consensus majority decision 
combination schemes relationships represented fig 

experimental comparison classifier combination rules identity verification experiment concerned problem personal identity verification 
different sensing modalities biometric information check claimed identity individual frontal face face profile voice 
verification methods biometric sensing modalities developed part european union project advance communication technologies services vts described 
design verification modules performance testing carried vts database seconds speech video data clients taken times shots period month 
image resolution pixels 
ki ieee transactions pattern analysis machine intelligence vol 
march frontal face face verification system experiments described detail 
robust correlation frontal face image client stored face template corresponding claimed identity 
search optimum correlation performed space valid geometric photometric transformations input image obtain best possible match respect template 
geometric transformation includes translation rotation scaling photometric transformation corrects change mean level illumination 
search technique optimal transformation parameters random exponential perturbations 
accordingly stage transformation test images perturbed random vector drawn exponential distribution change accepted leads improvement matching criterion 
computational efficiency achieved means random sampling sobel sequences allow faster convergence compared uniform sampling 
score function adopted rewards large overlap transformed face image template similarity intensity distributions images 
degree similarity measured robust kernel 
ensures gross errors due instance hair style changes swamp cumulative error matched images 
words matching benevolent aiming find large areas face possible supporting close agreement respective gray level profiles images 
gross errors reflected reduced overlap frames taken account matching criterion 
system trained easily means storing templates client 
image segmented create face mask excludes background torso change time 
testing performed independent test data composed clients impostors 
face profile verification approach involves comparison candidate profile template profile claimed identity 
candidate image profile extracted face profile images means color segmentation 
similarity profiles measured chamfer distance computed sequentially 
efficiency verification process aided precomputing distance map profile 
map stores distance pixel face profile image nearest point profile 
candidate profile subject translation rotation scaling objective matching stage compensate geometric transformation 
parameters compensating transformation determined minimizing chamfer distance template transformed candidate profile 
optimization carried simplex algorithm requires distance function evaluation derivatives 
convergence simplex algorithm local minimum prevented careful initialization transformation parameters 
translation parameters estimated comparing position nose tip matched profile 
scale factor derived comparison profile heights rotation initially set zero 
optimal set transformation parameters determined user accepted rejected depending relationship minimal chamfer distance prespecified threshold 
system trained shots 
profile client shot stored training set 
profiles client single profile selected pairwise comparison profile images 
profile yielding lowest matching distance images considered best representative triplet 
trained system tested shot profiles 
users vts database testing involves correct authentication matches tests 
acceptance threshold selected receiver operating characteristic produce equal error rate false rejection false acceptance 
voice personal identity verification voice employs text dependent approach described 
assumed audio input signal representing uttered sequence digits zero segmented individual words 
segmentation speech data claimed identity verification accomplished speech speaker recognition methods hidden markov models 
audio signal transformed multivariate time series linear predictive cepstral coefficients 
training digit hmms trained segmented speech data shots vts database 
digit models structure number states digit specific 
models allocate state phoneme state transition phonemes 
single gaussian mixture model distribution cepstral coefficient vectors state 
models acquired digit client model world model 
common users captures variability uttered sound large database 
verification claimed identity score computed sum individual digits log likelihood ratio claimed model world model normalized number cepstral coefficient frames 
score mapped interval sigmoid function 
performance assessed independent test set 
experimental results equal error rates obtained individual sensing modalities shown table 
table shows lowest rate percent achieved voice verification 
face profile verification produced equal error rate percent frontal face method yielded percent 
soft decisions output verification systems combined various classifier combination strategies discussed section 
validity conditional independence assumption tested computing average class corre kittler combining classifiers table equal error rates method eer frontal profile speech sum product maximum median minimum fig 

correlation face profile frontal face speech data 
lation matrix data decision making 
dimensionality data exceeds tens thousands impossible full view correlations measurements respective modalities 
adopting visual representation correlation matrix able look correlations representative subspace highly dimensional feature space 
subspace created features samples face image gray levels taken prespecified spatial locations 
profile image represented sample points evenly distributed profile 
sampling registered respect tip nose sampling interval normalized nose length 
profile landmarks needed registration normalization easily detected 
speech data took frames cepstral coefficients 
utterances client time warped client specific template 
created client representation subspace dimensions 
particular face profile variables occupy dimensions followed frontal face image samples speech measurements 
average class correlation matrix computed removing class conditional mean variable 
resulting vectors deviations means compute elements average class covariance matrix 
normalized dividing element product standard deviations ith jth component vector deviations 
normalisation process produced average class correlations values interval 
display purposes taken absolute value correlation coefficients 
result representation variable correlations matrix elements diagonal equal unity displayed gray level strength correlation zero mapped gray level scale 
correlation matrix shown fig 

correlation matrix exhibits block diagonal structure suggests observations generated modality class conditionally dependent 
correlation particularly strong features face profiles similarly speech utterances 
weaker features face image 
owing random spatial sampling face image spatial ordering successive features destroyed consequently correlation matrix block corresponding facial data random structure exception diagonal elements 
note correlations features different modalities considerably weaker modality correlations 
applies particular correlations frontal face modalities 
small subset face profile variables correlations insignificant conditional independence assumption may considered hold 
biometric modalities combined fusion strategies discussed section 
results table show benefits classifier combination 
interesting note sum rule outperformed combination strategies individually best expert 
experimental comparison classifier combination rules handwritten digit recognition second domain assess classifier combination strategies problem handwritten character recognition 
task recognize totally unconstrained handwritten numerals 
samples images isolated numeric characters taken addresses letter envelopes provides postal service 
database cedar cdrom produced center excellence document analysis recognition state university new york buffalo 
images scanned dead letter envelopes provided postal service 
br bs sets database consist isolated images numeric characters 
br set contains samples training set bs set samples served test set 
types classifiers applied perform classification individually 
structural gaussian neural network hidden markov model classifiers 
ieee transactions pattern analysis machine intelligence vol 
march character representation different representations follows pixel level representation gaussian classifier case image numeric character scaled gray level image 
character represented dimensional vector dimension gray level corresponding pixel 
complex object representation case structural classifier 
image thinning process 
skeleton character decomposed number primitives 
primitive line curve loop parameterized number unary measurements size direction addition number binary measurements extracted describe geometrical relations primitive neighbors 
detailed description representation 
hmm classifier image represented signals horizontal vertical profiles 
profile consists binary pattern image horizontal vertical line 
pattern quantized vectors codebook 
pattern index closest vector codebook 
center gravity line profile calculated quantized levels 
feature space consists indices pixel pattern center gravity 
details representation 
pixel representation item starting point derive distinct character description hidden layer neural network employed classifiers 
classification structural classifier handwritten characters natural structures generally composed number smaller elements certain topological relations 
recognize character need identify basic primitives particular structural relations 
binary image decomposed number primitives junctions reflection points serve breaking points 
symbolic numeric attributes describe structure character 
firstly primitives categorized types discretizing criterion zero line curve loop 
connectivity primitives encoded reflect topological structure character 
character code consists code primitive turn consists type primitive number neighbors point types number neighbors second endpoint types 
example code fig 

prototype tree structure 
represents character consisting primitives 
primitive curve connected primitives point lines 
endpoint connected primitive 
numeric information characterize unary attributes primitives relations binary relations primitives 
length primitive direction degree curvature unary measurements 
example binary measurements direction line connecting centers primitive neighbor direction line connecting terminal point primitive center neighbor 
class represented prototypes 
scheme prototypes generated training samples 
samples class divided small groups means levels clustering 
group samples number primitives cluster 
cluster called np group divided types primitives 
example samples consist curve lines grouped 
group cluster called type group divided number clusters containing samples structural code 
cluster level called code group 
code group divided dynamic clustering algorithm clusters produced called dist group 
mean variance actual range mean calculated unary binary measurements characterize particular cluster 
prototypes classes saved multilevel tree structure fig 

classification scheme unknown sample tested root prototype tree decide right np group 
level checked select right type group eventually reaches appropriate code group 
code group class sample reported unknown 
sample checked prototypes kittler combining classifiers code group find closest candidate sample 
probabilistic relaxation algorithm find correspondence primitives sample prototype 
distance measure quantify similarity sample candidate 
pertinent point meaningful measure defined sample compared prototypes number primitives connectivity adjacency matrix 
means number dimensions 
finding correspondence primitives sample prototype matching process attribute vectors associated sample prototypes respectively considered belonging space 
facilitates task finding metric measuring distance 
euclidean distance due fact distance take account second order statistics measurement results satisfactory 
hand distance measure exploits second order statistics mahalanobis distance requires large number samples cluster 
due large variability structure handwritten characters prototypes contain samples training set estimate statistics unsatisfactory 
consequently chose modified euclidean distance difference dimension measurement vector sample prototype penalized exceeds certain value 
value chosen weight multiplied standard deviation particular measurement 
modified distance hg kj bg fy si mi mean ith feature si standard deviation 
threshold constant 
penalizing weight 
values selected experimentally 
estimate priori probability computed follows kh posteriori probability estimated number samples cluster generated prototype 
sample assigned class maximum 
note prototype matches sample structure assigned zero posteriori probability classes 
gaussian classifier classes feature space assumed possess normal distribution mi mi ii mean vector covariance matrix class estimated training phase training data set 
number dimensions feature space 
posteriori probability calculated xh wi wi hidden markov models classifier hidden markov models hmms popular method statistical representation speech processing representation object random process generates sequence states 
model consists number states probabilities probabilities associated transition state 
character classifier scanned vertically horizontally generate corresponding vertical horizontal profiles 
vertical profile consists rows binary image horizontal profile consists columns 
state represents pattern binary pixels line profile 
number possible patterns states numerous 
example binary image possible combinations 
reduce number possible states training patterns clustered centroid cluster serves vector code book vector quantization 
unknown sample compared codebook assigned index closest 
codebook generated means clustering algorithm resulting vector codebook 
clustering process distance measure required determine close sample cluster order decide kept cluster moved closer 
hamming distance natural choice dealing binary vectors 
hamming distance known sensitive shift binary patterns 
slight shifts inevitable problem character recognition 
shift invariant hamming distance minimum hamming distance discrete vectors allowed slide 
advantageous property shift invariance undesirable cases 
example profile letter appear codebook index 
measure distinguish instances 
center gravity line calculated subtracted running average lines 
relative center gravity turn quantized levels 
state representation reduced pair numbers represents pixel pattern index relative center gravity 
discrete hidden markov models generated baum welch reestimation procedure scoring mechanism viterbi algorithm test phase 
scoring result reflects likelihood sample generated class model 
score values soft level assignment classifier posteriori probabilities estimates 
ieee transactions pattern analysis machine intelligence vol 
march table classification rate classifier individual classifier classification rate structural gaussian neural net hmm table classification rate different combining schemes combining rule classification rate majority vote sum rule max rule min rule product rule median rule neural network classifier classifier feed forward neural network multilayer perceptron trained pattern classifier 
momentum back propagation algorithm train network 
network consists nodes input layer corresponding dimensions feature space nodes hidden layer nodes output layer 
node output layer associated class output oi zero range reflects response network corresponding class wi 
facilitate soft level combination responses normalized estimates posteriori probability classes oi combination scheme expert fusion experiment due staff changes unable compute class correlation matrix different representations 
hope distinct representations individual experts satisfy assumption class conditional independence approximately 
different combination schemes applied assumption equal priors results compared 
schemes divided groups format individual classifiers combiner 
hard level combination uses output classifier binarized 
soft level combination hand uses estimates posteriori probability class classifier 
majority vote combiner representative category different operators soft level combiners 
table shows results classification individual classifiers results different combining schemes shown table 
note worst results achieved product rule similar performance min rule 
results rules worse individual classifiers reason classifiers reports correct class posteriori fig 

samples misclassified hmm classifier corrected sum rule combiner 
probability zero output zero correct class identified 
final result reported combiner cases wrong class worst case reject classes assigned zero posteriori probability 
interesting outcome experiments sum rule median rule best classification results 
majority vote rule close performance mean median rules 
max rule better individual classifiers exception hmm classifier 
analysis results analysed results detail see performance system improves decision combination 
hmm classifier yields best classification rate individual classifiers chosen 
examples samples misclassified hmm classifier corrected sum rule combiner shown fig 

numbers character represent true class assigned hmm classifier respectively 
hmm classifier scored quite classification failed classify samples look easy recognize 
table contains corresponding samples posteriori probabilities estimated classifier 
table shows clear difference values assigned samples different classifiers 
classifiers percent sure classification sample probability estimate hmm classifier percent sure true class estimate zero 
note misclassified samples corrected simple sum rule combiner 
important requirement combiner uses output individual classifiers classifiers kittler combining classifiers strongly correlated misclassification classifiers agree misclassify sample assign incorrect class sample 
requirement satisfied certain extent different representations object different feature sets different classification principle individual classifiers 
different representations feature sets leads cases reduction correlation outputs individual classifiers correlation input vectors different representations set features 
different classifiers usually different assumptions structure data stochastic model generates 
leads different estimate posteriori probabilities especially bayes decision boundaries 
pertinent look samples misclassified combiner see full correlation classifiers decision 
samples misclassified samples correctly classified classifier 
fig 
displays misclassified samples sum rule combiner 
fig 
samples recognized individual classifiers 
figs 
samples correctly classified classifier indicated sample 
error sensitivity somewhat surprising outcome experimental comparison classifier combination rules reported sections sum rule developed strongest assumptions table samples misclassified hmm classifier true class hmm decision structural neural net 
gaussian hmm true class class assigned hmm classifier posteriori probabilities estimated classifier 
conditional independence respective representations individual classifiers classes highly ambiguous observations enhance priori class probabilities slightly appear produce reliable decisions 
section shall investigate sensitivity product rule sum rule estimation errors 
shall show sum rule affected estimation errors 
theoretically established behavior consistent experimental findings 
developments sections assumed posteriori class probabilities wj xi terms various classifier combination rules defined computed correctly 
fact classifier produce estimate probability shall denote xi estimate deviates true probability error ji ji estimated probabilities enter classifier combination rules true probabilities 
consider effect estimation errors classifier combination rules 
substituting assign ar fe jj ij ji max ar fc kh ii ki ieee transactions pattern analysis machine intelligence vol 
march fig 

samples misclassified sum rule combiner 
samples classified correctly individual classifier 
samples classified correctly structural classifier 
neural network classifier 
gaussian classifier 
hmm classifier 
assumption eki strong may represent worst case scenario assuming rearrange product term eki eki nm linearized nm ii ki iim qp ol pm qpn substituting get nm max nm assign ar fe jj xij nm ol pm ol im ii qp eki ji xij ii qp qp ki qp comparing apparent term class hypothesis error free classifier combination rule affected error factor nm ki qp ii similar analysis sum rule commences assign jj ij ji pw max rewritten rp kh pd ii eki kittler combining classifiers fig 

classifier combination schemes 
assign om rp ew nm pm jj pew xij nm rp om cw nm pm dw nm max xij qp ki ou pv comparison shows term error free classifier combination rule affected error factor nm eki xii qp comparing error factors sensitivity errors dramatic 
note posteriori class probabilities unity error eki amplified compounded effect xi amplified errors equivalent sum 
contrast sum rule errors amplified 
contrary compounded effect computed sum scaled sum posteriori probabilities 
probable class sum greater result dampening errors 
sum decision rule resilient estimation er ieee transactions pattern analysis machine intelligence vol 
march may plausible explanation superior performance combination strategy observed experimentally sections contributing factor 
follows sum classifier combination rule simple intuitive technique improving reliability decision making different classifier opinions remarkably robust 
problem combining classifiers different representations patterns classified studied 
developed common theoretical framework classifier combination showed existing schemes considered special cases compound classification pattern representations jointly decision 
demonstrated different assumptions different approximations derive commonly classifier combination schemes product rule sum rule min rule max rule median rule majority voting 
various classifier combination schemes compared experimentally 
surprising outcome comparative study combination rule developed restrictive assumptions sum rule outperformed classifier combinations schemes 
explain empirical finding investigated sensitivity various schemes estimation errors 
sensitivity analysis shown sum rule resilient estimation errors may provide plausible explanation superior performance 
acknowledgments supported science engineering research council uk gr european union acts project vts 
authors andrew making available classification results obtained hmm character recognizer kenneth jonsson pandit providing frontal face image voice data respectively 
indebted stephane pigeon providing face profile data verification results gilbert making available voice verification decisions fusion experiments 
ali pazzani link error correlation error reduction decision tree ensembles technical report ics uci 
pal multistage generalization rank nearest neighbor classification rule pattern recognition letters vol 
pp 

cao recognition handwritten numerals multiple feature multistage classifier pattern recognition vol 
pp 

cho kim combining multiple neural networks fuzzy integral robust classification ieee trans 
systems man cybernetics vol 
pp 

cho kim multiple network fusion fuzzy logic ieee trans 
neural networks vol 
pp 

model chromosome recognition hypotheses construction verification pattern recognition letters vol 
pp 

el abdel el multistage algorithm fast classification patterns pattern recognition letters vol 
pp 

abdel interactive level architecture memory network pattern classifier pattern recognition letters vol 
pp 

franke mandler comparison approaches combining votes cooperating classifiers proc 
th iapr int conf 
pattern recognition conf 
pattern recognition methodology systems vol 
pp 

hansen salamon neural network ensembles ieee trans 
pattern analysis machine intelligence vol 
pp 
oct 
hashem schmeiser improving model accuracy optimal linear combinations trained neural networks ieee trans 
neural networks vol 
pp 

ho random decision forests third int conf 
document analysis recognition pp 
montreal aug 
ho hull srihari decision combination multiple classifier systems ieee trans 
pattern analysis machine intelligence vol 
pp 
jan 
kimura handwritten numerical recognition multiple algorithms pattern recognition vol 
pp 

krogh vedelsby neural network ensembles cross validation active learning advances neural information processing systems tesauro touretzky leen eds 
cambridge mass mit press 
identity optimal strategies multistage classifiers pattern recognition letters vol 
pp 

blaha kittler multistage pattern recognition reject option proc 
th iapr int conf 
pattern recognition conf 
pattern recognition methodology systems vol 
pp 

combining results neural network classifiers neural networks vol 
pp 

duin stabilizing classifiers small sample sizes proc 
th iapr int conf 
pattern recognition vienna 
tresp taniguchi combining estimators non constant weighting functions advances neural processing systems tesauro touretzky leen eds 
cambridge mass mit press 
tung lee tsai multi stage pre candidate selection handwritten chinese character recognition systems pattern recognition vol 
pp 

wolpert stacked generalization neural networks vol 
pp 

xu suen methods combining multiple classifiers applications handwriting recognition ieee trans 
systems man cybernetics vol 
pp 

zhou pavlidis discrimination characters multi stage recognition process pattern recognition vol 
pp 

kittler duin combining classifiers proc 
th int conf 
pattern recognition vol 
track pp 
vienna 
kittler matas jonsson ramos nchez combining evidence personal identity verification systems pattern recognition letters pp 

breiman bagging predictors technical report dept statistics univ california berkeley 
freund shapire experiments new boosting algorithm proc 
th int conf 
machine learning 
shapire freund bartlett lee boosting margin new explanation effectiveness voting methods proc 
th int conf 
machine learning 
tumer ghosh analysis decision boundaries linearly combined neural classifiers pattern recognition vol 
pp 

kittler combining classifiers tumer ghosh classifier combining analytical results implications proc 
nat conf 
artificial intelligence portland ore 
woods bowyer combination multiple classifiers local accuracy estimates proc 
cvpr pp 

kittler weighting factors multiple expert fusion proc 
british machine vision conf colchester england pp 

kittler strategies combining classifiers employing shared distinct pattern representations pattern recognition letters appear 
kittler improving recognition rates classifier combination theoretical framework frontiers handwriting recognition eds 
world scientific pp 

huang suen combination multiple experts recognition unconstrained handwritten numerals ieee trans 
pattern analysis machine intelligence vol 
pp 
jan 
devijver kittler pattern recognition statistical approach 
englewood cliffs prentice hall 
kittler constraining probabilistic relaxation symbolic attributes proc 
sixth int conf 
computer analysis images patterns sara eds pp 
prague 
kittler combining symbolic numeric attributes multiclass object recognition problems proc 
second int conf 
image processing vol 
pp 
washington 
connected character recogniser level building hmms proc 
th iapr int conf 
neural networks conf 
pattern recognition methodology systems vol 
pp 

matas jonsson kittler fast face localisation verification clark ed british machine vision conf pp 
bmva press 
pigeon vts multimodal face database release bigun chollet borgefors eds audio video biometric person authentication pp 

springer 
bimbot chollet combining methods improve phone speaker verification decision proc 
int conf 
speech language processing vol pp 
philadelphia 
pigeon profile authentication chamfer matching algorithm bigun chollet borgefors eds audio video biometric person authentication pp 

springer 
josef kittler graduated university cambridge electrical engineering obtained phd pattern recognition scd degree 
joined department electronic electrical engineering surrey university professor charge centre vision speech signal processing 
worked various theoretical aspects pattern recognition applications including automatic inspection ecg diagnosis remote sensing robotics speech recognition document processing 
current research interests include pattern recognition image processing computer vision 
authored book title pattern recognition statistical approach published prentice hall 
published papers 
member editorial boards pattern recognition journal image vision computing pattern recognition letters pattern recognition artificial intelligence machine vision applications 
received bsc degree electrical engineering university iraq 
graduation worked automatic control 
joined university surrey pursue postgraduate studies 
era technology works image video compression 
robert duin studied applied physics delft university technology netherlands 
received phd degree thesis accuracy statistical pattern recognizers 
research included various aspects automatic interpretation measurements learning systems classifiers 
integrated delft image processor reconfigurable pipelined machine interactive research environment image processing 
connection initiated projects comparison evaluation parallel architectures image processing pattern recognition 
moment main research interest comparison neural networks traditional pattern recognition classifiers learning 
stayed visiting professor university malaysia 
spent sabbatical leave university surrey vision speech signal processing group 
held official positions dutch society pattern recognition image processing international association pattern recognition iapr 
member organizing committees international conferences signal processing eusipco pattern recognition icpr scientific committees conferences pattern recognition image processing computer vision 
associate professor faculty applied sciences delft university technology 
leads projects pattern recognition neural network research sponsored dutch government industry 
author large number scientific papers served associate editor ieee transactions pattern analysis machine intelligence pattern recognition letters pattern analysis applications 
phd students coached 
teaching includes undergraduate graduate postgraduate courses statistics pattern recognition neural networks image processing 
jiri george matas received msc degree honors electrical engineering czech technical university prague czechoslovakia phd degree university surrey uk 
currently research fellow centre vision speech signal processing university surrey centre machine perception czech technical university 
interest include pattern recognition computer vision 
