appears proceedings fifteenth national conference arti cial intelligence july madison wi knowledge lean word sense disambiguation ted pedersen department computer science engineering southern methodist university dallas tx pedersen seas smu edu corpus approach word sense disambiguation requires information automatically extracted untagged text 
unsupervised techniques estimate parameters model describing conditional distribution sense group known contextual features 
em algorithm gibbs sampling evaluated determine appropriate data 
compare disambiguation accuracy experiment thirteen di erent words feature sets 
gibbs sampling results small consistent improvement disambiguation accuracy em algorithm 
resolving ambiguity central problem natural language processing 
wide range approaches applied word sense disambiguation 
require manually crafted knowledge annotated text machine readable dictionaries semantic networks aligned bilingual corpora 
corpus approach disambiguation relies strictly knowledge automatically identi able text processed 
avoids dependence external knowledge sources knowledge lean approach 
sentences contain particular ambiguous word 
feature vector fn fn represent selected properties context ambiguous word occurs represents sense ambiguous word 
objective divide instances ambiguous word speci ed number sense groups 
sense groups mapped sense tags order evaluate system performance 
mapping results highest classication accuracy 
wide range unsupervised learning techniques applied problem 
parametric model assign sense group copyright american association arti cial intelligence www aaai org 
rights reserved 
rebecca bruce department computer science university north carolina asheville asheville nc bruce cs edu ambiguous word 
case assign probable group context de ned naive bayes model parameter estimates formulated unsupervised techniques 
advantage approach fold large body evidence recommending naive bayes model word sense disambiguation leacock towell voorhees mooney ng unsupervised techniques parameter estimation developed easily applied parametric forms class decomposable models 
employ expectation maximization em algorithm dempster laird rubin gibbs sampling geman geman estimate model parameters untagged data 
known widely iterative algorithms estimating model parameters presence missing data case missing data senses ambiguous words 
em algorithm formulates maximum likelihood estimate model parameter gibbs sampling simulation technique estimating mode posterior distribution model parameter 
likelihood function approximated normal distribution simulation techniques provide better estimates model parameters 
data typical natural language processing data sparse skewed necessarily characterized large sample approximations 
study compare maximum likelihood estimates produced expensive simulation technique 
describe application naive bayes model word sense disambiguation 
sections introduce em algorithm gibbs sampling respectively 
results extensive evaluation di erent feature sets applied words 
close discussion related directions 
naive bayes model naive bayes model features assumed conditionally independent value classi cation variable 
applied word sense disambiguation model speci es contextual features conditionally independent sense ambiguous word 
joint probability certain combination contextual features particular sense expressed fn ny parameters model andp 
su cient statistics summaries data needed parameter estimation frequency counts events described variables fi 
marginal counts parameter estimates follow directly 
sense tags missing direct estimates possible em algorithm gibbs sampling impute sense group missing data estimate parameters 
em algorithm steps em algorithm expectation step maximization step 
step calculates expected values su cient statistics current parameter estimates 
step maximum likelihood estimates parameters imputed values su cient statistics 
steps alternate parameter estimates iteration andk di er lessthan em algorithm exponential family probabilistic models introduced dempster laird rubin 
naive model member exponential family special properties simplify formulation step lauritzen 
em algorithm naive bayes proceeds follows 
randomly initialize set 
step count fi count fi 
step re estimate count fi count 

go step parameter estimates iteration di er gibbs sampling gibbs sampling markov chain method generating random samples distribution sampling directly distribution di cult 
gibbs sampling impute missing values sample values parameters 
gibbs sampling cast stochastic version em algorithm meng van 
general gibbs sampling applicable wider class problems em algorithm 
gibbs sampler generates chains values missing senses parameters iterative sampling 
chains eventually converge stationary distribution 
early iterations sampler produce values 
suggested portion early iterations discarded 
process commonly known burn 
iteration burn monitor iterations convergence measure proposed geweke converged additional iterations performed 
general procedure gibbs sampling naive bayes model 
burn represents number initial iterations discarded chain size number iterations monitored 

randomly initialize set 
sample value sjf fn fi js fn 
sample parameters 

chain size goto 
chain burn chain size converge 

increase chain size go step prior knowledge conveniently incorporated conjugate prior multinomial distribution dirichlet prior 
resulting posterior dirichlet distribution distribution sampled steps 
experiments assume prior knowledge uninformative priors 
methodology series experiments conducted disambiguate occurrences thirteen di erent words 
di erent feature sets de ned word formulate naive bayes model describing distribution sense groups word 
parameters estimated em algorithm gibbs sampling 
total amounts di erent disambiguation experiments 
addition experiment repeated times order measure variance introduced randomly selecting initial parameter estimates 
disambiguation accuracy gures reported experiments measure automatically de ned sense groups map sense groups established human judge 
data words experiments sense distributions determined judge shown figures 
total count number occurrences 
word limited chief total count highest rank important main common total count phrase common stock belonging shared happening usual total count occasion nearest past public total count concerning people general concerning government people secret private adjective senses bill total count proposed law consideration piece money treasury bill list things bought price concern total count business rm worry anxiety drug total count medicine medicine habit forming substance interest total count money paid money share business readiness give attention line total count wire connecting telephones cord cable orderly series noun senses frequent senses 
frequency features employed lend distinguishing small minority senses 
addition line data reduced senses despite having fairly uniform distribution 
initially done maintain similar total count number senses words 
preliminary experiments senses show degrades considerably approximately percent depending feature set 
indicates di erent features may needed accommodate larger numbers senses 
line data leacock towell voorhees taken acl dci wall street journal corpus american printing house blind corpus tagged wordnet senses 
remaining twelve words bruce wiebe pedersen agree total count concede disagreement share opinion close total count cause cause operation help total count enhance inanimate object assist human object include total count contain addition parts part human subject verb senses taken acl dci wall street journal corpus tagged senses longman dictionary contemporary english 
feature sets de ned di erent feature sets experiments 
objective doing fold study impact space unsupervised parameter estimation study informativeness di erent feature types word sense disambiguation 
feature sets composed various combinations types features 
morphology feature represents morphology ambiguous word 
nouns binary indicating singular plural 
verbs value indicates tense verb possible values 
feature adjectives 
part speech features pli pri represent part speech pos word positions left right respectively word 
experiments 
pos feature possible values noun verb adjective adverb 
tags assigned automatically unix command style 
occurrences features ci representing th frequent content word sentences containing ambiguous word occurs sentence processed 
experiments 
unrestricted collocations features uri indicate word occurring position places left right respectively word 
experiments sense tags evaluation sense groups unsupervised learning procedures 
sense tagged text available evaluation process require manually mapping sense groups sense tags 
morphologically equivalent verb tenses grouped ambiguous morphology addressed 
event full joint naive bayes count event distribution noun interest experiments 
features form possible values 
nineteen correspond frequent words occur xed position sentences contain particular ambiguous word 
indicates position left right nineteen frequent value null indicating position left right falls outside sentence boundary 
content collocations features cl cr indicate content word occurring position place left right respectively ambiguous word 
values features correspond nineteen frequent content words position plus null 
features described de ned small contextual window local context selected produce low dimensional event spaces 
local context features successfully variety approaches disambiguation bruce wiebe ng lee 
feature sets feature sets experiments designated pand formulated shown 
particular feature combinations chosen yield reasonable results preliminary evaluation 
pl pl pr pr joint events marginal events ul ul ur ur joint events marginal events pl pl pr pr cl cr joint events marginal events joint events shows range number possible combinations feature values full joint distribution feature set 
contrast marginal events range possible combinations feature values marginal distributions nineteen distinct word forms recognized control dimensionality feature set allowing recognition relevant correlations 
value arrived empirically values considered 
em feature set feature set feature set gibbs sampling accuracy em versus gibbs naive bayes model 
shows example event count distribution interest smoothed reducing number possible events naive bayes model 
discussion results shows average accuracy standard deviation disambiguation random trials combination word feature set learning algorithm 
included percentage sample composed majority sense 
shows correlation accuracy disambiguation em algorithm versus gibbs sampling combinations words feature sets 
points fall near line associated words disambiguated similar accuracy methods 
method cases gibbs sampling resulted signi cantly accurate disambiguation em algorithm judged tailed test 
signi cant di erences shown bold face 
number signi cant di erences small shows consistent increase accuracy gibbs sampler relative em algorithm 
em algorithm parameter estimates gibbs sampling 
somewhat surprising em algorithm converge local maxima distribution likelihood function approximated normal distribution 
experiments em algorithm converged quite quickly usually iterations global maximum 
results suggest em algorithm gibbs sampling appropriate 
meng van propose gibbs sampler start point feature set feature set feature set maj gibbs em gibbs em gibbs em chief common public adjectives bill concern drug interest line nouns agree close help include verbs experimental results accuracy standard deviation em algorithm converges randomly initialized 
em algorithm local maximum gibbs sampler able escape nd global maximum 
em algorithm global maximum gibbs sampler converge quickly con rm result 
feature set accuracy disambiguation nouns fairly consistent feature sets 
exceptions 
accuracy achieved bill higher feature set accuracy drug hand lower feature set variation feature sets may indicate certain features appropriate certain words 
accuracy verbs highest feature set help glaring exception 
feature set local context collocations 
highest average accuracy achieved adjectives occurs gibbs sampling combination feature set high dimensional feature set additionally sense distributions adjectives skewed 
circumstances em algorithm reliably nd global maximum appears em algorithm local maxima processing common public 
frequency features reduce sparsity useful distinguishing minority senses 
skewed distribution senses data sample frequency features indicative majority sense 
related abundance literature word sense disambiguation 
knowledge lean approach differs require knowledge resources raw text 
corpus approaches supervised learning algorithms sense tagged text leacock towell voorhees bruce wiebe mooney multi lingual parallel corpora gale church yarowsky 
approach signi cantly reduces amount sense tagged data required described yarowsky 
yarowsky suggests variety automatically seeding supervised disambiguation algorithm identify collocations uniquely distinguish senses 
yarowsky achieves accuracy disambiguating senses twelve di erent words 
result demonstrates ectiveness small collocations seeds iterative bootstrapping approach 
comparison em algorithm agglomerative clustering algorithms applied unsupervised word sense disambiguation discussed pedersen bruce 
data study pedersen bruce agglomerative algorithm signi cantly accurate adjectives verbs em algorithm signi cantly accurate nouns 
results indicate analysis counts dissimilar features appropriate highly skewed data sets 
performance gibbs sampling current study falls short adjectives verbs supports previous 
em algorithm naivebayes gale church yarowsky distinguish city names people names 
narrow context words side perform better wider windows 
report accuracy percentage mid nineties applied dixon name quite ambiguous 
knowledge lean approach sense discrimination discussed schutze press 
ambiguous words clustered sense groups second order occurrences instances ambiguous word assigned sense words occur likewise occur similar words training data 
schutze evaluates sense groupings ectiveness information retrieval problems 
issues address 
possibility em algorithm starting point gibbs sampling particularly intriguing addresses limitations approaches 
second models naive bayes knowledge lean approaches 
complicated models potentially resulting distributions inappropriate em algorithm provide stronger disambiguation results combination gibbs sampling 
informative priors gibbs sampling 
continue investigating local context features hopes increasing accuracy minority senses substantially increasing dimensionality problem 
acknowledgments research supported ce naval research number 
bruce wiebe 
word sense disambiguation decomposable models 
proceedings nd annual meeting association computational linguistics 
bruce wiebe pedersen 
measure model 
proceedings conference empirical methods natural language processing 
dempster laird rubin 
maximum likelihood incomplete data em schutze evaluation tagged text required label sense groupings establish accuracy disambiguation experiment 
experiment fully automatic free dependence external knowledge source 
algorithm 
journal royal statistical society 
gale church yarowsky 
method disambiguating word senses large corpus 
computers humanities 
gale church yarowsky 
discrimination decisions dimensional spaces 
journal operations research 
geman geman 
stochastic relaxation gibbs distributions bayesian restoration images 
ieee transactions pattern analysis machine intelligence 
geweke 
evaluating accuracy sampling approaches calculating posterior moments 
bernardo berger dawid smith eds bayesian statistics 
oxford oxford university press 
lauritzen 
em algorithm graphical association models missing data 
computational statistics data analysis 
leacock towell voorhees 
corpus statistical sense resolution 
proceedings arpa workshop human language technology 
meng van 
em algorithm old folk song sung new fast tune discussion 
journal royal statistics society series 
mooney 
comparative experiments disambiguating word senses illustration role bias machine learning 
proceedings conference empirical methods natural language processing 
ng lee 
integrating multiple knowledge sources disambiguate word sense exemplar approach 
proceedings th annual meeting society computational linguistics 
ng 
exemplar word sense disambiguation improvements 
proceedings second conference empirical methods natural language processing 
pedersen bruce 
distinguishing word senses untagged text 
proceedings second conference empirical methods natural language processing 
schutze 
press 
automatic word sense discrimination 
computational linguistics 
yarowsky 
unsupervised word sense disambiguation rivaling supervised methods 
proceedings rd annual meeting association computational linguistics 
