efficient fault tolerant scheduling algorithm real time tasks precedence constraints heterogeneous systems xiao qin hong jiang david swanson department computer science engineering university nebraska lincoln lincoln ne jiang cse unl edu investigate efficient line scheduling algorithm real time tasks precedence constraints executed heterogeneous environment 
provides features capabilities existing algorithms schedule independent tasks real time homogeneous systems 
addition proposed algorithm takes heterogeneities computation communication reliability account improving reliability 
provide faulttolerant capability algorithm employs copy scheme enables system tolerate permanent failures single processor 
scheme backup copy allowed overlap backup copies processor long corresponding primary copies allocated different processors 
tasks judiciously allocated processors reduce schedule length reliability cost defined product processor failure rate task execution time 
addition time detecting handling permanent fault incorporated scheduling scheme making algorithm practical 
quantify combined performance fault tolerance schedulability performability measure introduced 
compared existing scheduling algorithms literature scheduling algorithm achieves average improvement reliability average improvement performability 

heterogeneous distributed systems increasingly scientific commercial applications including real time safety critical applications system depends results computation time instants results available 
examples applications include aircraft control transportation systems medical electronics 
obtain high performance real time heterogeneous systems scheduling algorithms play important role 
scheduling algorithm maps real time tasks processors system deadlines response time requirements met system guarantee functional timing correctness presence faults 
proposed algorithm referred efrcd efficient fault tolerant reliability cost driven algorithm endeavors comprehensively address issues faulttolerance reliability real time task precedence constraints heterogeneity 
tolerate processor permanent failure algorithm uses primary backup technique allocate copies task different processors 
improve quality schedule backup copy allowed overlap backup copies processor long corresponding primary copies allocated different processors 
added measure fault tolerance proposed algorithm considers heterogeneities computation reliability improving reliability extra hardware cost 
precisely tasks judiciously allocated processors reduce schedule length reliability cost defined product processor failure rate task execution time 
addition time detecting handling permanent fault incorporated scheduling scheme making algorithm practical 
rest organized follows 
section briefly presents related literature 
section describes workload system characteristics 
section proposes efrcd algorithm main principles including theorems presenting algorithm 
performance evaluation section 
section concludes summarizing main contributions 
supported nsf eps nebraska university foundation proceedings international conference parallel processing icpp ieee 
related issue scheduling heterogeneous systems studied literature years 
scheduling scheme heterogeneous systems developed 
reliability cost incorporated scheduling algorithms tasks precedence constraints 
algorithms provide fault tolerance support real time applications 
previous done facilitate real time computing heterogeneous systems 
solution dynamic resource management problem real time heterogeneous systems proposed 
algorithms tolerate processor failure 
faulttolerance considered design real time scheduling algorithms systems reliable 
mechanism proposed supporting adaptive fault tolerance real time system 
proposed feasibility check algorithm faulttolerant scheduling 
known rate monotonic fit assignment algorithm extended 
algorithms assume underlying system homogeneous consists single processor 
algorithm real time scheduling algorithm tasks precedence constraint support fault tolerance 
proposed dynamic algorithms schedule real time tasks fault tolerance requirements multiprocessor systems tasks scheduled algorithms independent scheduled line :10.1.1.35.8373:10.1.1.35.8373
martin devised algorithm system task model :10.1.1.35.8373
oh son studied real time fault tolerant scheduling algorithm statically schedules set independent tasks 
common features algorithms tasks independent designed homogeneous systems 
heterogeneous systems considered efrcd considers fault tolerance real time tasks consider 
girault proposed real time scheduling algorithm heterogeneous systems considers fault tolerance tasks precedence constraints 
study far closest efrcd authors literature 
main differences efrcd fold 
efrcd considers heterogeneities computation communication reliability defined shortly considers computational heterogeneity 
hetero 

take reliability cost consideration efrcd reliability cost driven 
allows concurrent execution primary backup copies task efrcd allows backup copies proceedings international conference parallel processing icpp ieee tasks primary copies scheduled different processors overlap 
authors previous static dynamic scheduling schemes heterogeneous realtime systems developed 
similarity algorithms reliability cost driven scheme applied 
exception algorithm algorithms proposed tolerate failure 
algorithm extended relaxing requirement backup copies tasks allowed overlapped 

workload system characteristics real time job dependent tasks modelled directed acyclic graph dag vn set tasks set edges represents communication tasks 
eij vi vj indicates message transmitted task vi vj eij denotes volume data sent 
tolerate permanent faults processor primary backup technique applied 
task copies executed sequentially different processors 
loss generality assumed copies task identical 
proposed approach applied copies task different 
heterogeneous system consists set pm heterogeneous processors connected network 
processor communicates processors message passing 
measure computational heterogeneity modeled function represents execution time task processor 
cj vi denotes execution time vi pj 
measure communicational heterogeneity modeled function communication time sending message vs pi vr pj determined wij communication cost wij weight edge pi pj representing delay involved transmitting message unit length processors 
task denote deadline scheduled start time finish time respectively 
denotes processor allocated 
parameters subject constraints ci wherep pi 
real time job feasible schedule satisfies andf 
timely fault tolerant tft schedule defined schedule task deadlines missed despite arbitrary processor failures 
goal efrcd achieve tft 
reliability cost task vi pj defined product failure rate vi execution time pj 
noted reliability heterogeneity implied reliability cost virtue heterogeneity cj vi rci reliability cost processor fails pi fails schedule set failure rates processors 
rc rci determined equation respectively 
rc rc equation summation term right hand side represents reliability cost due tasks primary copies reside fault free processors second summation term expresses reliability cost due backup copies tasks primary copies reside failed processor 
reliability expression captures ability system complete parallel jobs presence processor permanent failure 
rc rl 
scheduling algorithms section efrcd algorithm objectives total schedule length reduced tasks complete deadlines permanent failures processor tolerated system reliability enhanced reducing reliability cost schedule 
outline key tolerating single processor failure allocate primary backup copies task different processors backup copy subsequently executes primary copy fails complete due processor failure 
backup copies need execute presence single processor failure 
tasks allocated failed processor affected need backup copies executed certain backup copies scheduled overlap 
precisely allowed overlap backup copies processor corresponding primary copies allocated different processors allocated 
feasible schedule primary copies tasks allocated processor backup copies processor overlap backup copies 
statement formally described 
proceedings international conference parallel processing icpp ieee proposition 
vi vj vi vj vi vj vj fig 
shows example illustrating case 
example vi vj fig 
primary copies vi vj allocated respectively backup copies vi vj allocated 
backup copies overlapped 
allocated respectively backup copies vi vj allocated 
backup copies overlapped execute single processor failure model 
algorithm schedules tasks main steps 
tasks ordered deadlines non decreasing order tasks tighter deadlines higher priorities 
second primary copies scheduled 
backup copies scheduled similar manner primary copies may overlapped processors reduce schedule length 
specifically second third steps scheduling task satisfy conditions deadline met processor allocation lead minimum increase reliability cost processors satisfying condition able receive messages predecessors 
addition conditions backup copy extra conditions satisfy allocated processor different assigned primary copy ii start time finish time primary copy plus fault detection time iii allowed overlap backup copies processor primary copies allocated different processors 
condition ii formally described proposition 
proposition 
schedule tft efrcd algorithm facilitate presentation algorithm necessary notations listed table 
table 
definitions notation notation definition set predecessors task vi vi time overlap vj set successors task vi vi set feasible processors allocated determined part theorem 
set predecessors backup copy determined expression 
queue tasks scheduled pi vq andf queue tasks scheduled pi overlap backup copy task vq andf vi vj vi schedule preceding vj vj vi 
vi vj vi message preceding vj sends message vj 
note vi vj implies vi vj inversely 
vi vj vi execution preceding vj vi vj note vi vj implies vi vj vi vj earliest available time pi esti earliest start time processor pi 
esti earliest start time processor pi 
detailed pseudocode efrcd algorithm 
efrcd algorithm input dag distributed system computational communicational reliability heterogeneity output schedule feasibility feasible 

sort tasks deadlines non decreasing order subject precedence constraints generate ordered list ol 
schedule primary copies tasks task ol order schedule rc processor pi check allocated pi calculate est vq tasks scheduled pi vq andf compute earliest start time pi check unoccupied time intervals interspersed currently scheduled tasks accommodate vj max vj ci est max vj break determine earliest esti equation starts executing est completed determine reliability cost rci pi find minimum reliability cost rci rc rci est est pi rc rci proper processor available return fail assign reliability cost minimal update information messages 
schedule backup copies tasks task ordered list schedule backup copy rc determine allocated processor pi feasible processor pi subject proposition theorem identify backup copies scheduled vj pi overlap vj primary copy vj backup copy vj subject proposition copy vj task queue determine strong primary copy theorem vj task queue check unoccupied proceedings international conference parallel processing icpp ieee time intervals time slots occupied backup copies overlap accommodate vj max vj ci est max vj break determine earliest esti equation starts executing est completed determine reliability cost rci pi find minimum rc rci rc rci rc est est pi rc rci proper processor available return fail find assign reliability cost minimal update information messages task vj avoid redundant messages vj sends message possible theorem expression task vj avoid redundant messages vj strong primary copy sends message vj possible theorem return succeed scheduling principles recall est eat important determine proper schedule task eat est indicate time messages predecessors arrived est additionally signifies processor allocated available start execution 
series derivations lead final expressions eat est 
predecessors vj considered earliest available time vj primary backup copies task depends finish time vj earliest message start time transmission time wik message sent vj pk vj 
vj vj pi pk wik consider predecessors clearly wait message predecessors arrived 
earliest available time pi maximum vj predecessors 
eat max eat expression esti pi computed checking queue find processor idle time slot starts task large accommodate task 
procedure described step algorithm 
esti applied derive est earliest start time processor 
expression est 
est est min pi ci min pi esti ci 
est earliest start time computed complex way est 
set predecessors contains exclusively primary copies predecessor tasks set predecessors may contain certain combination primary backup copies predecessor tasks 
order decide necessary introduce notion strong primary copy follows 
note cases may fail execute fails time fails receive messages predecessors 
case illustrated simple example fig 
dotted lines denote messages sent predecessors successors 
vj predecessor andp vj 
suppose time vj vj fails vj execute 
suppose vj schedule preceding receive message vj fail execute 
primary copy task encounters case referred strong primary copy formally defined 
definition 
task strong primary copy execution implies failure time 
alternatively task strong primary copy failures time imply execution recall assumption processor encounter permanent failures observe vi predecessor vj primary copies tasks strong primary copies vi vj fig 
illustrates scenario case formally theorem helpful determining set predecessors backup copy see step 
theorem 
tasks vi vj vi predecessor vj 
vi message preceding vj meaning vi need send message vj vj strong primary copies vi vj 
proof vi vj strong primary copies definition vi vj execute vi vj failed execute due processor failures 
vi vj allocated different processors impossibility 
vi vj execute implying messages need sent vi vj set predecessors itis defined follows 
vi vi vi vi vi strong primary copy strong primary copy vi efrcd algorithm primary copy allocated corresponding backup copy scheduled 
task predecessor vi copies vi allocated algorithm starts scheduling obviously receive vi time vi predecessor primary copy vi backup copy vi successor primary copy vj backup copy vj time vj vi vj fig 
processor fails vi executes 
vj receive message vi vj execute vj vi vj time fig 
vi vj vi schedule preceding vj vi strong primary copy 
vj scheduled processor vi scheduled 
proceedings international conference parallel processing icpp ieee vi vj vj vi vj vj fig 
vi vj vi vj strong primary copies vi vj scheduled different processors 
vi execution preceding vj vi time vj vi fig 
vi predecessor vj vi vj scheduled processor vi strong primary copy 
case vi execution preceding vj vj message vi vi 
maximum earliest available time pi determined primary copies predecessors backup copies tasks messages sent tasks 
expression certain amount time detect handle fault 
eat max max eat max max eat vj esti est denote earliest start time pi earliest start time processor respectively 
computation esti complex esti due need judiciously overlap backup copies processors 
computation esti step algorithm 
scheme implemented step attempts reduce schedule length selectively overlapping backup copies tasks 
expression est est min est ci min pi esti ci 
candidate processor pi chosen directly set selected feasible processors backup copy allocated 
obviously element 
task observed special circumstance scheduled processor primary copy predecessor vi scheduled fig 
illustrates scenario 
set generated help theorem 
theorem 
tasks vi vj vi vj schedule preceding vj strong primary copy vj vi allocated processor 
proof suppose theorem incorrect vj vi allocated processor 
assume vi executes vi combined fact vi strong primary copy implies vi failed time vi vi vj implies vi failed time vj indicating vj execute 
vi execution preceding vj vi schedule preceding vj vi execution preceding vj vj execute 
contradiction 
recall vj expression basic parameter derive expression expression 
vj determined start time message sent pi pk vj 
message allocated link link proceedings international conference parallel processing icpp ieee idle time slot sender finish time large accommodate message 
computed procedure vj mst er mst er message queue containing messages scheduled link pi pk 
procedure behaves similar manner procedure computing esti step 
computation 
check idle time slots idle time slots accommodate return value 
max wik vj wik 
return max wik vj 

return idle time slots mst set scheduling messages proposed algorithm tries avoid sending redundant messages step theorem 
suppose vj successfully executed vi execution preceding vj vi execution preceding vj observe special cases illustrated fig vi vj statement described theorem 
theorem 
tasks vi vj vi vj ifthe primary copies vi vj allocated processor vi strong primary copy vi execution preceding vj meaning sending message vi vj redundant 
proof contradiction assume vi execution preceding vj vi vj execute table 
vi strong primary copy processor vi failed time vi def 

vi vj allocated processor vi vj implying vj execute 
contradiction 
notion strong primary copy appears theorems necessary able determine task strong primary copy 
theorem applied efrcd step suggests approach determining task strong primary copy 
approach assume know predecessors strong primary copies 
approach recursively starting tasks predecessors able determine task strong primary copy 
theorem 
task predecessors strong primary copy 
task vi predecessors vj allocated processor vj strong primary copy allocated different processors backup copy vj message preceding primary copy vi vi strong primary copy 
vj vj vi vi vj vj strong primary copy vi vj vj vi vi strong primary copy 
proof proof straightforward definition omitted 
prove 
suppose time vi processor vi fail 
vj predecessor vi 
possibilities vi vj vj vi implying processor vj fail vj 
vj strong primary copy vj execute 
vi vj vj vi implying processor fails vi receive message task vj recall vj vi 
proven vi receive messages predecessors 
vi execute vj failed time vi 
vi strong primary copy 

performance evaluation section compare performance proposed algorithm algorithms literature ov fgls extensive simulations 
performance measures capture important different aspects algorithms 
measure schedulability sc defined percentage parallel real time jobs successfully scheduled submitted jobs 
second reliability rl defined expression 
combine performances measures third measure performability pf defined product sc rl 
noted algorithms differ aspects 
ov assumes independent tasks homogeneous systems efrcd fgls consider tasks precedence constraints execute heterogeneous systems 
second efrcd fgls incorporate computational communicational reliability heterogeneities scheduling considers computational heterogeneity 
comparison fair fgls efrcd downgraded handle independent tasks execute homogeneous systems 
similarly sections efrcd algorithm downgraded assuming communicational homogeneity fgls algorithm adapted include reliability heterogeneity 
workload workload parameters chosen literature represent realistic workload 
simulation experiment realtime dags generated independently scheduling algorithm follows determine number real time tasks number processors failure rates computation time execution time vector randomly chosen uniformly distributed range 
third data communication real time tasks communi proceedings international conference parallel processing icpp ieee cation weights uniformly selected 
fourth failure rates uniformly selected range 
fault detection time randomly computed uniform distribution 
real time deadlines defined ways 
single deadline associated real time job predetermined set tasks precedence constraints 
deadline referred common deadline employed ov 
fair comparison common deadline applied fgls efrcd simulation studies reported sections 
individual deadlines associated tasks real time job 
deadline definition dynamic scheduling independent real time tasks :10.1.1.35.8373
sections deadline definition adapted tasks precedence constraints 
specifically vi pk vj pl deadline determined vi max vj eij max ck vi eij chosen uniformly range represents individual relative deadline 
schedulability experiment evaluates performance terms schedulability algorithms ov fgls efrcd sc measure 
workload consists sets independent real time tasks executed homogeneous distributed system 
size homogeneous system fixed common deadline selected 
failure rates uniformly selected range execution time random variable uniformly distributed range 
sc measured function task set size shown fig 

fig 
sc function deadline 
fig 
shows sc performances ov efrcd identical fgls 
considering efrcd downgraded comparability result imply efrcd powerful ov efrcd ov efrcd fgls schedule tasks precedence constraints executed heterogeneous systems ov capable 
results reveal ov efrcd significantly outperform fgls sc suggesting fgls suitable scheduling independent tasks 
poor performance fgls explained fact employ scheme 
consequence twofold 
fgls require computing resources efrcd lead relatively low sc number processors fixed 
second backup copies fgls overlap processor may result longer schedule length 
experiment reliability ov fgls efrcd algorithms evaluated function maximum processor failure rate shown fig 

stress reliability performance scs algorithms assumed assigning extremely loose common deadline 
task set size system sizes respectively 
execution time task chosen uniformly range 
failure rates uniformly selected range max max varies hour increments fig 
reliability function max 

observed fig 
rl ov fgls close efrcd 
efrcd perform considerably better ov fgls rl values approximately higher ov fgls 
efrcd algorithms better reliability simply ov fgls consider reliability scheduling schemes efrcd take reliability account 
experimental result validates proposed efrcd algorithm enhance reliability system especially tasks loose deadlines deadlines 
ov fgls efrcd max hour proceedings international conference parallel processing icpp ieee effect computational heterogeneity computational heterogeneity reflected variance execution times computation time vector metric introduced represent computational heterogeneity level min max average value execution time min deviation clearly higher value higher level heterogeneity 
study effect heterogeneity level pf fgls efrcd chosen increments 
fig 
shows pf function heterogeneity level 
performability fgls btree efrcd btree fgls ary tree efrcd ary tree fig 
pf btree ary tree function heterogeneity level 
alpha observation fig 
value pf increases heterogeneity level 
pf product sc rl sc rl higher heterogeneity level increases 
results explained reasons 
individual relative deadlines affected change computational heterogeneity high variance task execution times affect absolute deadlines making deadlines looser sc higher 
second high variance task execution times provides opportunities tasks packed fixed number processors giving rise higher sc 
third rc decreases heterogeneity level increases implying increasing rl 
second interesting observation efrcd outperforms fgls respect pf low heterogeneity levels opposite true high heterogeneity levels 
heterogeneity levels low sc rl efrcd considerably higher fgls 
hand efrcd sc lower fgls high heterogeneity level rls algorithms similar heterogeneity level increases 
efrcd pf product sc rl lower fgls high heterogeneity levels 
result suggests sc objective scheduling fgls suitable systems relatively high levels heterogeneity efrcd suitable scheduling tasks relatively low levels heterogeneity 
contrast rl sole objective efrcd consistently better fgls 
addition fig indicates performability fgls increases rapidly heterogeneity level efrcd implying fgls sensitive change computational heterogeneity efrcd 

efficient fault tolerant real time scheduling algorithm efrcd heterogeneous systems executing tasks precedence constraints studied 
fault tolerant capability incorporated algorithm primary backup pb model task associated copies allocated different processors 
efrcd relaxes requirement forbids overlapping backup copies allow overlapping processor corresponding primary copies allocated different processors 
system reliability enhanced reducing reliability cost scheduling tasks 
algorithm takes system workload heterogeneity consideration explicitly accounting computational communicational reliability heterogeneity 
best knowledge proposed algorithm kind reported literature comprehensively addresses issues fault tolerance reliability real time task precedence constraints heterogeneity 
assess performance efrcd extensive simulation studies conducted quantitatively compare relevant existing scheduling algorithms literature ov fgls 
simulation results indicate efrcd algorithm considerably superior algorithms vast majority cases 
exceptions 
fgls outperforms efrcd marginally task parallelism low 
second computational heterogeneity high efrcd algorithm inferior fgls algorithm 
abdelzaher shin combined task message scheduling distributed real time systems ieee transaction parallel distributed systems vol nov 
alan bertossi luigi mancini federico fault tolerant rate monotonic fit scheduling hard real time systems ieee trans 
parallel distributed systems pp 

reliable matching scheduling precedence constrained tasks heterogeneous distributed computing proc 
th international conference parallel processing pp 

proceedings international conference parallel processing icpp ieee ghosh melhem fault tolerance scheduling aperiodic tasks hard real time multiprocessor systems ieee trans :10.1.1.35.8373
parallel distributed systems 
vol pp 
girault fault tolerant static scheduling real time distributed embedded systems proc 
st international conference distributed computing systems icdcs phoenix usa april 
gonzalez stankovic ramamritham adaptive fault tolerance graceful degradation dynamic hard real time scheduling proc 
th ieee real time systems symposium san francisco california december 
huh welch shirazi heterogeneous resource management dynamic real time systems proc 
th heterogeneous computing workshop 
melhem moss tolerance multiple transient faults aperiodic tasks hard real time systems ieee transactions computers vol september 
ram murthy fault tolerant dynamic scheduling algorithm multiprocessor real time systems analysis ieee transactions parallel distributed systems november :10.1.1.35.8373
fault tolerant real time scheduling execution time constraints sixth international conference real time computing systems applications hong kong china december 
oh son algorithm real time faulttolerant scheduling multiprocessor systems th euromicro workshop real time systems greece pp 
oh son scheduling real time tasks dependability journal operational research society vol 
pp june 
qin jiang dynamic reliability driven scheduling parallel real time jobs heterogeneous systems proc 
th international conference parallel processing valencia spain pp 
qin jiang xie han reliability driven scheduling real time tasks precedence constraints heterogeneous distributed systems proc 
th international conference parallel distributed computing systems pp november 
qin jiang swanson fault tolerant realtime scheduling algorithm precedence constrained tasks distributed heterogeneous systems technical report tr unl cse department computer science engineering university nebraska lincoln september 
agrawal scheduling periodic time critical applications pipelined execution heterogeneous systems proc 
international conference parallel processing valencia spain sept pp 

srinivasan jha reliability driven task allocation distributed systems ieee trans 
parallel distributed systems pp 

