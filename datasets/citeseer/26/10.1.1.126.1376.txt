kluwer academic publishers boston 
manufactured netherlands 
role occam razor knowledge discovery pedro domingos cs washington edu department computer science engineering university washington seattle wa 
kdd systems incorporate implicit explicit preference simpler models occam razor strongly criticized authors schaffer webb :10.1.1.41.589:10.1.1.41.589:10.1.1.36.5295:10.1.1.36.5295
controversy arises partly occam razor interpreted quite different ways 
interpretation simplicity goal essentially correct heart preference comprehensible models 
second interpretation simplicity leads greater accuracy problematic 
critical review theoretical arguments shows unfounded universal principle demonstrably false 
review empirical evidence shows fails practical heuristic 
article argues continued kdd risks causing significant opportunities missed restricted comparatively applications appropriate 
article proposes reviews domain constraints alternative avoiding overfitting examines possible methods handling accuracy comprehensibility trade 
keywords model selection overfitting multiple comparisons comprehensible models domain knowledge 
occam occam razor considered fundamental tenets modern science 
original form states est sin approximately translated means entities multiplied necessity 
formulated william occam late middle ages criticism scholastic philosophy theories grew elaborate corresponding improvement predictive power 
today invoked learning theorists kdd practitioners justification preferring simpler models complex ones 
formulating occam razor kdd terms trickier appear 
difficulty different definitions simplicity possible 
practice simplicity typically equated loosely syntactic size model example number nodes decision tree number conditions rule set number weights neural network general number parameters model 
definitions type obvious limitations consider sophisticated ones 
unfortunately fully satisfactory computable definition simplicity exists possible 
part article concerned heuristic view simplicity view typically implicit practical occam razor 
generalization error model error rate unseen examples domingos training set error error examples learned 
formulation razor closest occam original intent razor models generalization error simpler preferred simplicity desirable 
hand kdd occam razor quite different sense stated second razor models training set error simpler preferred lower generalization error 
believe important distinguish clearly versions occam razor 
largely uncontroversial second taken literally false 
theoretical arguments pieces empirical evidence advanced support reviewed wanting 
article reviews previous theoretical arguments second razor importantly mounting empirical evidence fails practice 
razor revisited refined consequences discussed alternatives second razor proposed 

theoretical arguments second razor 
pac learning argument large fraction computational learning theory literature concerned relationship accuracy simplicity superficially basic argument neatly encapsulated blumer occam razor mathematical results valid indirect relationship second razor prove 
purposes results summarized 
suppose generalization error hypothesis greater 
probability hypothesis correct independent examples smaller hypotheses hypothesis class considered learner probability correct training examples smaller probability disjunction smaller sum probabilities disjuncts 
model zero training set error sufficiently small set models low generalization error 
model arbitrarily complex 
connection result occam razor provided information theoretic notion set models small members distinguished short codes 
way say decision trees fewer nodes trees 
result decision tree nodes extracted set trees preferable nodes extracted set training set error 
occam razor knowledge discovery put way results blumer 
say select sufficiently small set models prior looking data fortune models closely agrees data confident data 
theoretical results give guidance select set models 

bayesian argument claims general theoretical foundation preferring simple models statistical pattern recognition literature 
details vary typically take form approximation optimal prediction procedure bayesian model averaging bma bernardo smith chickering heckerman 
bma single best model selected model chosen space retained posterior probability data computed 
predictions voting models model weighted posterior probability 
true model space prior distribution known optimal prediction procedure sense minimizing generalization error general loss function 
model spaces interest kdd number models far large model average efficiently computable closed form solution 
approximation methods proposed address problem 
number assumption sufficiently large samples distribution model parameters model structure approximately multivariate gaussian 
retain terms approximation increase sample size approximate maximum posteriori parameter settings maximum likelihood ones obtain expression logarithm probability model structure sum terms likelihood term reflecting performance training data term penalizing complexity model number parameters 
criteria type include bayesian information criterion bic schwarz akaike information criterion aic akaike 
similar criteria information theoretic interpretation mml wallace boulton mdl rissanen discussed 
consider bic criterion explicitly proposed approximation bayesian model averaging 
leaving aside fact bic involves sequence approximations assumptions may may valid practice large sample complexity penalty imply simpler models probable bic computes probabilities model structures opposed models 
distinction important 
ax ax bx model structures instantiated different models example 
bic approximates marginal likelihood model structure average likelihoods models instantiate weighted prior probabilities model structure 
bic penalizes model structure dimension higher order spaces effectively contain models lower order ones contain domingos low likelihood models best 
precise terms higher order model structures higher vc dimension haussler considering finite precision numbers literally contain models 
example model space defined ax bx contains models defined ax correct model quadratic structure correspondingly correct may appear linear structure high likelihood model averaged large number vanishingly small likelihoods corresponding poor quadratic models possible 
bearing likelihood model linear model irrespective quadratic degree 
choosing model structure bic instantiating parameters lead suboptimal model 
similar remarks apply mackay 
occam factors appear evidence framework penalize model structures parameters opposed models lead suboptimal choices 

information theoretic argument minimum description length mdl principle rissanen forms second razor applied quinlan rivest 
principle best model minimizes total number bits needed encode model data model 
mdl principle appealing reduces apparently attributes model error rate complexity form bits information 
guarantee select accurate model 
rissanen simply proposes fundamental principle 
closely related minimum message length mml principle wallace boulton derived bayes theorem coding theory 
hypothesis training set 
probable hypothesis maximum 
logarithms sides multiplying equivalent minimizing log log log log log term ignored choosing probable hypothesis hypotheses 
source coding theorem shannon theorem cover thomas need transmit symbols alphabet channel efficient binary encoding symbols length log probable symbols shorter codes 
result interpreted minimizing length bits optimal coding hypothesis plus length bits optimal coding occam razor knowledge discovery data hypothesis 
led researchers believe trade error complexity direct consequence bayes theorem requiring additional assumptions cheeseman 
belief result circular reasoning models probable priori shorter codes shorter codes probable priori 
care taken avoid confusion assigning shortest codes priori probable hypotheses considering syntactically simplest models representation decision trees fewest nodes priori probable ones 
higher priors complex models assigned shorter codes obviously imply preference simpler models original representation 
example model highest prior decision tree nodes assigned bit code leaving longer codes priori trees fewer nodes best tree preference small trees implied 
information theory goal efficient transmission channel direct bearing kdd goal infer predictive comprehensible models data 
having assigned prior probability model space consideration recode models probable ones represented shortest bit strings 
predictive comprehensible 

theoretical arguments second razor 
zero sum arguments number known theoretical results established imply second razor true 
results include schaffer conservation law generalization performance wolpert free lunch theorems turn implicit mitchell demonstration learning impossible 
essence imply domain simpler model accurate complex exists domain reverse true argument preferable general 
derives simple fact absence information knowledge target values training examples implies target values unseen examples models consistent training data possible combinations assignments target values unseen examples equally correct 
free lunch results negate second razor mathematical sense leave open possibility apply real world domains rao 
matter empirical study sections address 

vapnik chervonenkis dimension vapnik theory structural risk minimization shows generalization ability class models function number parameters domingos vc dimension 
vc dimension hypothesis class instance space size largest subset generate possible binary labelings 
vc dimension number parameters related general 
model structures large number parameters generalize quite reliably constrained ways 
model structure class sign sin ax single parameter infinite vc dimension discriminate arbitrarily large arbitrarily labeled set points axis vapnik 

overfitting due multiple testing conventional wisdom overfitting caused overly complex models occam razor introducing preference simpler ones 
drawing study multiple comparison problems statistics miller jensen cohen shown overfitting fact arises complexity se attempting large number models leads high probability finding model fits training data purely chance 
attempting complex models incurs smaller risk overfitting attempting simple ones 
overfitting best second razor multiple testing phenomenon account scoring candidate models 
way bonferroni adjustments jensen schmill may cause underfitting hypotheses tested independent 
general computationally intensive solution randomization testing jensen 
approach explicitly takes dependences account proposed domingos domingos 

bias variance bias variance useful concepts characterizing generalization behavior learning algorithms geman 
bias systematic component generalization error incurred infinite sample available variance additional error incurred finite sample produced random fluctuations 
schuurmans 
shown complexity penalty methods assume particular bias variance profile true profile correspond postulated systematic underfitting overfitting result 
methods optimal specific cases 
particular commonly complexity penalty methods assume variance increases relatively slowly model complexity compared bias 
case regression wide tailed input distributions complexity penalty methods prone catastrophic mistakes choosing hypotheses worse best available 
occam razor knowledge discovery 
empirical evidence second razor arguably kdd researchers routinely apply second razor believe universally true simply generally applies practice 
example piatetsky shapiro argues occam razor true true real world situations section attempt determine case 

pruning simple empirical argument second razor stated pruning works pruning leads models simpler accurate corresponding unpruned ones mingers 
lead lower accuracy schaffer :10.1.1.41.589
easy think simple problems pruning hurt accuracy applying decision tree algorithm learning noise free diagonal frontier 
importantly mentioned cohen jensen shown pruning seen correction overly complex models effective reduction number models attempted 
related papers jensen schmill oates jensen shown empirically correcting multiple testing pruning leads better results mdl related methods 

algorithm oft cited holte observes decision tree containing single node come reasonably close accuracy 
holte experiments rule algorithm average accurate hardly negligible 
closest results obtained measure finds accuracy best possible rule looking test set 
results appear led confusion 
holte points measure algorithm sense consider accuracy competitive similar measure decision trees achieve bayes rate lowest error possible 
experiments suggest advantage going complex models small imply simpler models better elomaa 
shall see results call question 

low variance algorithms generally pieces domingos pazzani friedman suggested simple learners naive bayesian classifier perceptron better complex ones having higher systematic error component bias prone random fluctuations variance 
results imply preference domingos simpler models restricting search 
suitably constrained decision tree rule induction algorithms stable simpler ones accurate 
theory revision systems ourston mooney example produce accurate theories quite complex comparatively little search making incremental changes initial theory complex 

physics second razor justified pointing success hard sciences 
arguments addressed form large part razor appeal 
popular example comes astronomy favors model solar system ptolemy ironically terms predictive error models indistinguishable predict trajectories 
model preferable intrinsic merits simplicity razor 
alternative slightly humorous example provided flat earth vs spherical earth 
second razor clearly favors flat earth theory linear model spherical quadratic better explaining everyday observations middle ages 
favorite example relativity vs newton laws 
passage cover thomas choose simplest explanation consistent observed data 
example easier accept general theory relativity accept correction factor gravitational law explain mercury general theory explains fewer assumptions patched newtonian theory 
fact general theory relativity assumptions newton gravitational law far complex reason preferring 
preference comes fact factor patch applied fit theory particular observation 
pearl notes appropriate connect credibility nature selection procedure properties final product 
explicitly known 
simplicity merely serves rough indicator type processing took place prior discovery 
example maxwell concise elegant equations 
fact equations concise elegant notation differential operators introduced years death 
original form long unwieldy leading complain precluded empirically testing 
occam razor knowledge discovery list goes 
case fact comparatively simple equations proved successful modeling physical phenomena indication true large variety areas kdd applied medicine finance earth sensing molecular biology marketing process control fault detection 

empirical evidence second razor empirical evidence second razor comes main sources experiments carried specifically test aspect relationship simplicity accuracy practical systems learn complex models outperform systems learn simple ones 
consider turn 

experiments testing second razor authors carried experiments directly indirectly investigate relationship simplicity accuracy obtained results contradict second razor 
fisher schlimmer observed concept simplification improved accuracy id cobweb systems dependent training set size degree dependence concept attributes 
murphy pazzani induced consistent decision trees number small noise free domains cases smallest consistent trees accurate ones 
schaffer conducted series experiments showing pruning increase error effect increase noise level 
quinlan cameron jones varied width beam search conducted cn rule learner excessive search leads models simultaneously simpler accurate 
murthy salzberg similar observations varying depth lookahead decision tree induction 
webb showed remarkably accuracy decision trees common datasets consistently increased grafting additional nodes tree data perfectly fit 
chickering heckerman compared different methods approximating likelihood simple bayesian model structures bic mdl approach accurate 
lawrence 
conducted experiments backpropagation synthetic domains training neural networks larger correct led lower errors training networks correct size 

systems learn complex models source evidence second razor growing number practical machine learning systems achieve reductions error learning complex models 
vapnik support vector machines learn polynomials domingos high degree resulting feature spaces dimension outperformed simpler state art models handwritten digit recognition sch lkopf text classification joachims tasks sch lkopf :10.1.1.11.6124
cestnik bratko gams datta kibler show redundancy improve noise resistance accuracy 
schuurmans proposed geometric evaluation measure markedly outperforms complexity penalty ones polynomial regression tasks 
webb grafting procedure developed result experiments mentioned improves accuracy commonly datasets producing larger trees 
example domingos rise system consistently outperforms cn rules common datasets inducing substantially complex rule sets 
rise typically produces short general rules covering training examples large number longer specific ones covering 
removed rise accuracy complexity fall level cn domingos 
reason cn select complex rule sets accurate ones exist unable find simplicity biased search 
arguably practical experience mdl systems provides evidence second razor 
example spending considerable effort find coding trees examples quinlan rivest better results obtained introducing ad hoc coefficient reduce penalty paid complex decision trees 
single significant piece evidence second razor broad success multiple model approaches 
methods bagging breiman boosting freund schapire stacking wolpert error correcting output codes kong dietterich learn models varying training set factors combine predictions 
compared learning single model results reductions error rate large ones drucker quinlan maclin opitz 
model ensemble effectively equivalent single model complex 
verified domingos domain ensemble produced bagging explicitly converted single model original base learner rules 
overwhelming majority cases result complex accurate model obtained base learner directly 
rao potts show bagging builds increasingly accurate complex frontiers simpler accurate ones obtained cart 
success model ensembles shows large error reductions systematically result significantly increased complexity 
evidence supports second razor true general typically false types domains kdd applied 
occam razor knowledge discovery 
razor revisited reason preferring simpler models easier people understand remember cheaper computers store manipulate 
razor justified 
simplicity comprehensibility equivalent 
example decision table may larger similarly accurate decision tree easily understood lines table attributes langley kohavi sommerfield 
induced models comprehensible consistent previous knowledge complex pazzani pazzani 
better form razor state models generalization error comprehensible preferred 
exactly model comprehensible largely domain dependent matter cognitive empirical research kononenko pazzani kohavi sommerfield 

discussion evidence reviewed article shows contrary second razor claim greater simplicity necessarily typically lead greater accuracy 
care taken ensure algorithm perform search data allows search performed complex models simple ones 
second razor trivially true fact assigning simplest representations accurate models 
help finding models place 
simple model just way saying probable model model small space done literature constitutes multiplication entities necessity runs razor applicable kdd research branches science 
importantly lead misconception simpler models initial commonly representation decision tree list rules reason true 
second razor appropriate really believe phenomenon study simple model representation language 
domains representations kdd typically deals empirical evidence bears 
second razor function poor man substitute domain knowledge way avoiding complexities adjusting system domain applying data 
happens overfitting may avoided second razor cost detectable patterns missed unnecessarily low accuracy obtained 
larger database databases millions tens millions records potentially contain data discriminate large number models 
applying second razor mining risks rediscovering broad regularities familiar domain domingos experts missing second order variations payoff data mining lies 

constrained knowledge discovery abandon preference simpler hypotheses ways avoid overfitting learning flexible models 
answer lies constraining discovery domain knowledge educated guesses domain 
create knowledge acquisition bottleneck constraints reflecting weak knowledge quite general assumptions sufficient 
domain constraints appear forms boolean expressions presence absence certain types items discovered rules srikant restrictions sign inequalities antecedents rules pazzani qualitative models induced rules consistent clark matwin knowledge variables causes effects cooper domingos knowledge determinations variables determine russell 
donoho rendell list large number types available fragmentary knowledge successfully subset bankruptcy prediction task example features incompatible units added forming derived features 
abu mostafa similarly describes different types hints learner methods incorporating learning process 
bishop section reviews different ways knowledge symmetries invariances domain incorporated neural network learning 
rl rule induction system clearwater provost allows user specify wide range information attributes preferences constraints rules induced 
instrumental success application carcinogenicity prediction problems lee 
constraints form discovered rules widespread field inductive logic programming name declarative bias dellec 
example cohen allows user specify grammar language rule antecedents expressed 
facility available ripper efficient propositional rule learner cohen :10.1.1.50.8204
similar ideas equation discovery system successfully applied modeling ecosystems todorovski zeroski 
domain constraints association rule discovery typically form extensions sql allow user specify form rules discovered han meo shen kamber 
kdd systems constrained look rules contradict prior knowledge hope rules represent interesting new deviations liu padmanabhan tuzhilin 
constraints accumulated interactive manner letting user reject rules produced system generalizing justifications provost jensen letting user give advice system observes learning progress maclin occam razor knowledge discovery shavlik 
domain constraints combined simplicity bias believed appropriate 
addition accuracy domain constraints improve comprehensibility speed 
accuracy speed improved reducing search needed find accurate model 
comprehensibility improved making results induction consistent previous knowledge 
consequence theory structural risk minimization vapnik restriction model space limits vc dimension priori valid way combat overfitting 
order obtain type continuous trade error complexity typical implementations second razor sequence progressively restricted model spaces structure required 
potentially provided different factors called luckiness functions shawe taylor 

classification learners class predictions obtained thresholding continuous output factor margin minimum distance continuous output correct side threshold 
requiring progressively larger margins correspondingly reduces learner vc dimension ability overfit 
support vector machines example large margin classifier smola margin distance separating hyperplane closest instances class support vectors 
multiple model methods boosting seen large margin classifiers continuous output sum models weighted votes schapire see grove schuurmans :10.1.1.31.2869
general bayesian method overfitting attachment prior probabilities models 
unfortunate penalize complexity 
types information incorporated prior distribution example models differ priori best guess model assigned lower priors heckerman 
structural risk minimization prior distributions take account model space searched learner way searched 
process oriented evaluation domingos domingos computes difference current best model training set error expected generalization error increases search progresses 
intuitively models attempted observed error best model far close true error higher expected true error observed error 
method training error continuously traded amount search conducted absence vc dimension results informative priors 

separating accuracy comprehensibility accept fact accurate models simple easily understandable allow explicit trade 
systems induce accurate model extract comprehensible model variable complexity promising domingos avenue 
methods developed learn neural network assumption appropriate representation extract comprehensible model form set rules decision tree craven andrews diederich 
approaches extracting single models model ensembles developed domingos breiman shang 
comprehensible models obtained methods typically accurate full models expected accurate models type learned directly data 
cases craven domingos possible trade accuracy comprehensibility varying parameters translation system 
comprehensible output complex model obtained extracting parts relevant user immediate goals 
association rule systems allow user sift rules produced example approach imielinski shen 
second stage mining model accomplished visually kohavi kunz brunk 
applications goal immediate insight extraction comprehensible output delayed performance time 
takes form explanations decisions 
generating explanation specific decision easier producing comprehensible global model 

occam razor interpreted ways favoring simpler models generalization error simplicity goal favoring simpler models training set error leads lower generalization error 
article second version provably empirically false argued version simplicity proxy comprehensibility 
resulting prescription kdd research applications prefer simpler models honestly believe target phenomenon simple 
seldom case practice seek constrain induction domain knowledge decouple discovering accurate probably quite complex model extracting comprehensible approximations 
abu mostafa 

learning hints neural networks 
journal complexity 
akaike 

bayesian analysis minimum aic procedure 
annals institute statistical mathematics 
andrews diederich 
eds 

proceedings nips workshop rule extraction trained artificial neural networks 
nips foundation 
bernardo smith 

bayesian theory 
new york ny wiley 
occam razor knowledge discovery bishop 

neural networks pattern recognition 
oxford uk oxford university press 
blumer ehrenfeucht haussler warmuth 

occam razor 
information processing letters 
breiman 

bagging predictors 
machine learning 
breiman shang 

born trees 
technical report statistics department university california berkeley berkeley ca 
brunk kelly kohavi 

integrated system data mining 
proceedings third international conference knowledge discovery data mining pp 

newport beach ca aaai press 
cestnik bratko 

learning redundant rules noisy domains 
proceedings eighth european conference artificial intelligence pp 

munich germany pitman 
cheeseman 

finding probable model 
langley eds computational models scientific discovery theory formation pp 

san mateo ca morgan kaufmann 
chickering heckerman 

approximations marginal likelihood bayesian networks hidden variables 
machine learning 
clark matwin 

qualitative models guide inductive learning 
proceedings tenth international conference machine learning pp 

amherst ma morgan kaufmann 
clearwater provost 

rl tool knowledge induction 
proceedings second ieee international conference tools artificial intelligence pp 

san jose ca ieee computer society press 
cohen 

grammatically biased learning learning logic programs explicit antecedent description language 
artificial intelligence 
cohen 

fast effective rule induction 
proceedings twelfth international conference machine learning pp 

tahoe city ca morgan kaufmann 
cooper 

simple constraint algorithm efficiently mining observational databases causal relationships 
data mining knowledge discovery 
cover thomas 

elements information theory 
new york ny wiley 
craven 

extracting comprehensible models trained neural networks 
phd thesis department computer sciences university wisconsin madison madison wi 
datta kibler 

learning prototypical concept descriptions 
proceedings twelfth international conference machine learning pp 

tahoe city ca morgan kaufmann 
cook holder 

analyzing benefits domain knowledge substructure discovery 
proceedings international conference knowledge discovery data mining pp 

montr canada aaai press 
domingos 

way induction 
international journal artificial intelligence tools 
domingos 

unifying instance rule induction 
machine learning 
domingos 

knowledge acquisition examples multiple models 
proceedings fourteenth international conference machine learning pp 

nashville tn morgan kaufmann 
domingos 

bagging 
bayesian account implications 
proceedings third international conference knowledge discovery data mining pp 

newport beach ca aaai press 
domingos 

process oriented heuristic model selection 
proceedings fifteenth international conference machine learning pp 

madison wi morgan kaufmann 
domingos 

combine predictive causal learning 
proceedings nips workshop integrating supervised unsupervised learning breckenridge nips foundation 
domingos domingos 

process oriented estimation generalization error 
proceedings sixteenth international joint conference artificial intelligence 
stockholm sweden morgan kaufmann 
domingos pazzani 

optimality simple bayesian classifier zero loss 
machine learning 
donoho rendell 

constructive induction fragmentary knowledge 
proceedings thirteenth international conference machine learning pp 

bari italy morgan kaufmann 
drucker cortes jackel lecun vapnik 

boosting machine learning algorithms 
proceedings eleventh international conference machine learning pp 

new brunswick nj morgan kaufmann 


randomization tests 
new york ny marcel dekker 
elomaa 

defense notes learning level decision trees 
proceedings eleventh international conference machine learning pp 

new brunswick nj morgan kaufmann 
fisher schlimmer 

concept simplification prediction accuracy 
proceedings fifth international conference machine learning pp 

ann arbor mi morgan kaufmann 
freund schapire 

experiments new boosting algorithm 
proceedings thirteenth international conference machine learning pp 

bari italy morgan kaufmann 
friedman 

bias variance loss curse dimensionality 
data mining knowledge discovery 
gams 

new measurements highlight importance redundant knowledge 
proceedings fourth european working session learning pp 

montpellier france pitman 
geman bienenstock doursat 

neural networks bias variance dilemma 
neural computation 
grove schuurmans 

boosting limit maximizing margin learned ensembles 
proceedings fifteenth national conference artificial intelligence pp 

madison wi aaai press 
han fu wang chiang gong koperski li lu rajan xia zaiane 

dbminer system mining knowledge large relational databases 
proceedings second international conference knowledge discovery data mining pp 

portland aaai press 
clancey 

strategic explanations diagnostic consultation system 
coombs ed developments expert systems pp 

london uk academic press 
haussler 

quantifying inductive bias ai learning algorithms valiant learning framework 
artificial intelligence 
heckerman geiger chickering 

learning bayesian networks combination knowledge statistical data 
machine learning 
holte 

simple classification rules perform commonly datasets 
machine learning 
imielinski 

application programming interface query language database mining 
proceedings second international conference knowledge discovery data mining pp 

portland aaai press 
jensen 

induction randomization testing decision oriented analysis large data sets 
phd thesis washington university saint louis mo jensen cohen 

multiple comparisons induction algorithms 
machine learning 
appear 
jensen schmill 

adjusting multiple comparisons decision tree pruning 
proceedings third international conference knowledge discovery data mining pp 

newport beach ca aaai press 
occam razor knowledge discovery joachims 

text categorization support vector machines learning relevant features 
proceedings tenth european conference machine learning pp 

chemnitz germany springer verlag 
kamber han chiang 

metarule guided mining multi dimensional association rules data cubes 
proceedings third international conference knowledge discovery data mining pp 

newport beach ca aaai press 
kohavi kunz 

option decision trees majority votes 
proceedings fourteenth international conference machine learning pp 

nashville tn morgan kaufmann 
kohavi sommerfield 

targeting business users decision table classifiers 
proceedings fourth international conference knowledge discovery data mining pp 

new york ny aaai press 
kong dietterich 

error correcting output coding corrects bias variance 
proceedings twelfth international conference machine learning pp 

tahoe city ca morgan kaufmann 
kononenko 

comparison inductive naive bayesian learning approaches automatic knowledge acquisition 
wielinga ed current trends knowledge acquisition 
amsterdam netherlands ios press 
langley 

induction condensed determinations 
proceedings second international conference knowledge discovery data mining pp 

portland aaai press 
lawrence giles tsoi 

lessons neural network training overfitting may harder expected 
proceedings fourteenth national conference artificial intelligence pp 

providence ri aaai press 
lee buchanan aronis 

knowledge learning exploratory science learning rules predict rodent carcinogenicity 
machine learning 
liu hsu chen 

general impressions analyze discovered classification rules 
proceedings third international conference knowledge discovery data mining pp 

newport beach ca aaai press 
mackay 

bayesian interpolation 
neural computation 
maclin opitz 

empirical evaluation bagging boosting 
proceedings fourteenth national conference artificial intelligence providence ri aaai press 
maclin shavlik 

creating advice reinforcement learners 
machine learning 
meo psaila ceri 

new sql operator mining association rules 
proceedings second international conference large databases pp 

bombay india morgan kaufmann 
miller jr 

simultaneous statistical inference nd ed 
new york ny springer verlag 
mingers 

empirical comparison pruning methods decision tree induction 
machine learning 
mitchell 

need biases learning generalizations 
technical report computer science department rutgers university new brunswick nj 
murphy pazzani 

exploring decision forest empirical investigation occam razor decision tree induction 
journal artificial intelligence research 
murthy salzberg 

lookahead pathology decision tree induction 
proceedings fourteenth international joint conference artificial intelligence pp 

montr canada morgan kaufmann 
dellec rouveirol ad bergadano 

declarative bias ilp 
de raedt ed advances inductive logic programming pp 

amsterdam netherlands ios press 
oates jensen 

large datasets lead overly complex models explanation solution 
proceedings fourth international conference knowledge discovery data mining pp 

new york ny aaai press 
ourston mooney 

theory refinement combining analytical empirical methods 
artificial intelligence 
domingos padmanabhan tuzhilin 

belief driven method discovering unexpected patterns 
proceedings fourth international conference knowledge discovery data mining pp 

new york ny aaai press 
pazzani mani 

concise colorful learning intelligible rules 
proceedings third international conference knowledge discovery data mining pp 

newport beach ca aaai press 
pazzani 

influence prior knowledge concept acquisition experimental computational results 
journal experimental psychology learning memory cognition 
pearl 

connection complexity credibility inferred models 
international journal general systems 
piatetsky shapiro 

editorial comments 
kdd nuggets 
provost jensen 

kdd tutorial evaluating knowledge discovery data mining 
new york ny aaai press 
quinlan 

bagging boosting 
proceedings thirteenth national conference artificial intelligence pp 

portland aaai press 
quinlan cameron jones 

oversearching layered search empirical learning 
proceedings fourteenth international joint conference artificial intelligence pp 

montr canada morgan kaufmann 
quinlan rivest 

inferring decision trees minimum description length principle 
information computation 
rao potts 

visualizing bagged decision trees 
proceedings third international conference knowledge discovery data mining pp 

newport beach ca aaai press 
rao gordon spears 

action really equal opposite reaction 
analysis conservation law generalization performance 
proceedings twelfth international conference machine learning pp 

tahoe city ca morgan kaufmann 
rissanen 

modeling shortest data description 
automatica 
russell 

preliminary steps automation induction 
proceedings fifth national conference artificial intelligence pp 

philadelphia pa aaai press 
schaffer 

overfitting avoidance bias 
machine learning 
schaffer 

conservation law generalization performance 
proceedings eleventh international conference machine learning pp 

new brunswick nj morgan kaufmann 
schapire freund bartlett lee 

boosting margin new explanation effectiveness voting methods 
proceedings fourteenth international conference machine learning nashville tn morgan kaufmann 
sch lkopf burges smola 
eds 

advances kernel methods support vector machines 
cambridge ma mit press 
sch lkopf burges vapnik 

extracting support data task 
proceedings international conference knowledge discovery data mining pp 

montr canada aaai press 
schuurmans 

new metric approach model selection 
proceedings fourteenth national conference artificial intelligence pp 

providence ri aaai press 
schuurmans ungar foster 

characterizing generalization performance model selection strategies 
proceedings fourteenth international conference machine learning pp 

nashville tn morgan kaufmann 
schwarz 

estimating dimension model 
annals statistics 
shawe taylor bartlett williamson anthony 

structural risk minimization data dependent hierarchies 
technical report nc tr department computer science royal holloway university london egham uk 
shen ong zaniolo 

data mining 
fayyad piatetsky shapiro smyth uthurusamy eds advances knowledge discovery data mining pp 

menlo park ca aaai press 
occam razor knowledge discovery smola bartlett sch lkopf schuurmans 
eds 

proceedings nips workshop large margin classifiers 
breckenridge nips foundation 
srikant vu agrawal 

mining association rules item constraints 
proceedings third international conference knowledge discovery data mining pp 

newport beach ca aaai press 
todorovski zeroski 

declarative bias equation discovery 
proceedings fourteenth international conference machine learning pp 

nashville tn morgan kaufmann 


ockham studies selections 
la il open court 
vapnik 

nature statistical learning theory 
new york ny springer verlag 
wallace boulton 

information measure classification 
computer journal 
webb 

experimental evidence utility occam razor 
journal artificial intelligence research 
webb 

decision tree grafting 
proceedings fifteenth international joint conference artificial intelligence pp 

nagoya japan morgan kaufmann 
wolpert 

stacked generalization 
neural networks 
wolpert 

lack priori distinctions learning algorithms 
neural computation 
