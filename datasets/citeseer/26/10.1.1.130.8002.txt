seda architecture conditioned scalable internet services matt welsh david culler eric science division university california berkeley culler brewer cs berkeley edu propose new design highly concurrent internet services whichwe call staged event driven architecture seda 
seda intended support massive concurrency demands simplify construc tion conditioned services 
seda applications consist network event driven stages connected explicit queues 
ar chitecture allows services conditioned load preventing resources demand exceeds service ca 
seda set dynamic resource controllers keep stages operating regime despite large fluctuations 
describe control mechanisms automatic tuning load conditioning including thread pool sizing event batching load shedding 
seda design implementation internet services platform architecture 
seda applications high performance server packet router gnutella peer peer file shar ing network 
results show seda applications exhibit higher performance traditional service designs robust load 
internet presents computer systems problem supporting millions users demanding access services responsive robust available 
number sessions hits day internet sites translates higher number network requests placing underlying resources 
yahoo 
receives page views daily aol web caches service day 
internet services experience huge variations service load bursts coinciding times service value 
documented slashdot effect shows uncommon experience fold increases site popular 
demand internet services grows new system design techniques manage load 
information source code re lease software described please see www cs berkeley edu proj seda 
appear symposium operating systems principles sosp chateau lake louise canada october 
permission digital hard copies part classroom granted fee provided copies distributed profit commercial advantage notice full citation page 
copy republish post servers redistribute lists requires prior fee 
copyright acm 
systems challenge magnified trends generality services 
services complex static content replaced dynamic content extensive computation second service logic tends change rapidly increases complexity engineering de 
third services increasingly hosted general purpose facilities platforms carefully engineered service 
trends continue envision rich array novel services authored pushed ture may successful scale millions users 
investigations addressing high level service authorship including naming lookup composition versioning 
focus performance aspect ofthe problem achieving robust performance wide range services subject huge variations load preserving ease authorship replication key aspect service scalability 
service instance sustain certain level performance repli cated sustain fold increase load 
scalable clusters widely obtain replication service site wide area replication increasingly employed specific services content distribution networks 
may orders magnitude greater average practical replicate services handle maximum 
expect large spikes load experienced node 
goal develop general framework concurrent conditioned service instances handle load gracefully unfortunately traditional operating system designs widely promoted models concurrency provide graceful load 
commodity operating systems focus providing maximal transparency giving process abstraction virtual cpu memory disk network 
goal somewhat odds needs internet services demand massive concur extensive control resource usage 
processes threads supported models concurrent programming overhead terms context switch time memory footprint limits concurrency 
transparent resource virtualization making informed decisions vital managing excessive load focused performance robustness specific services :10.1.1.23.5560
services increas ingly dynamic flexible engineering burden excessive 
tools exist aid development highly concurrent conditioned services goal reduce complexity providing general purpose mechanisms aid software developers properties 
propose new design framework highly concurrent call staged event driven architecture seda seda combines aspects threads event program ming models manage concurrency scheduling resource management needs internet services 
seda applications con structed network stages associated incoming event queue 
stage represents robust building block may indi conditioned load thresholding filtering event queue 
addition making event queues explicit allows applications scheduling resource management decisions reordering filtering aggregation requests 
seda dy namic resource throttling control resource allocation scheduling application components allowing system adapt 
describes design architecture seda internet services platform 
platform provides efficient scalable interfaces resource including thread pool sizing dynamic event scheduling 
evaluate framework applications high performance server packet router gnutella peerto peer file sharing network 
performance applications demonstrating seda achieves robustness huge variations load outperforms 
java seda server outperforms popular web servers implemented described section 
seda highly concurrent applications easier build efficient robust load 
right set application designers focus application specific logic details concurrency resource management 
background related seda draws important lines research thread concurrency models ease programming event models extensive concurrency 
section develops lineage ofthis approach outlining key contributions problems steps leading seda design intuitively service conditioned behaves simple pipeline depth pipeline determined network processing stages service 
offered load increases delivered throughput pipeline full throughput saturates additional load degrade throughput 
similarly exhibited service roughly constant light load dominated depth pipeline 
load approaches tion queueing delay dominates 
closed loop scenario typical services client waits response deliv ering request response time increase linearly number clients key property conditioned service graceful degradation offered load exceeds capacity service maintains linear response time penalty impacts clients equally predictably service specific pol icy 
note typical web experience load increases throughput decreases response time increases cally creating impression service crashed 
thread concurrency commonly design server applications thread request model embodied rpc packages java remote method invocation dcom 
model sup ported modern languages programming environments 
seda spanish word silk 
dispatch request request request request request result threaded server design incoming request dispatched thread processes request returns result client 
edges represent control flow components 
note opera tions disk access shown incorporated threads request processing 
model shown accepted request consumes thread synchronization operations protecting shared resources 
operating system overlaps computation threads 
relatively easy program overheads associated including cache tlb misses scheduling overhead lock contention lead serious performance number threads large 
concrete example shows performance simple threaded server number increases 
effective thread limit large general purpose timesharing adequate requirements internet service 
threads processes primarily designed support gramming existing oss strive virtualize hardware resources way transparent applications 
applications rarely participate system wide resource management decisions indication resource availability order adapt changing conditions 
virtualization fundamentally hides fact resources limited shared number systems attempted remedy problem exposing control applications 
scheduler activations application specific handlers operating systems spin exokernel nemesis attempts aug ment limited operating system interfaces giving applications ability specialize policy decisions kernel :10.1.1.28.6923:10.1.1.130.1539:10.1.1.13.9310:10.1.1.117.6702
design systems multiprogramming focus continues safe efficient resource virtualization graceful management high concurrency 
bounded thread pools avoid overuse threads number systems adopt load conditioning serves bound size thread pool associated service 
number requests exceeds fixed limit additional connections accepted 
approach web servers apache iis netscape enterprise server application servers bea weblogic ibm websphere 
lim number concurrent threads server avoid throughput degradation performance robust uncon strained thread task model 
approach introduce great deal unfairness clients server threads busy client requests queue network servicing 
show section cause clients experience waiting times 
request handled single thread difficult throughput tasks sec gamma latency msec delta number threads linear ideal latency threaded server throughput degradation benchmark mea sures simple threaded server creates single thread task pipeline 
receiving task thread performs kb read threads read file data buffer cache 
threads pre allocated server eliminate thread startup measurements tasks generated internally negate network effects 
server implemented running way iii gb memory linux 
number concurrent tasks increases throughput increases number threads throughput degrades substantially 
response time unbounded task queue lengths increase comparison shown linear response time curve note log scale axis 
identify internal performance bottlenecks order perform load conditioning 
consider simple threaded web server requests inexpensive process cached static pages expensive large pages cache 
concurrent requests expensive requests performance bottleneck desirable perform load shedding 
server unable inspect stream implement policy knows thread pool saturated arbitrarily reject knowledge ofthe source bottleneck 
resource containers concept paths scout op erating system techniques bound resource usage tasks server 
mechanisms apply ver tical resource management set software modules allowing resources entire data flow system managed 
case bottleneck described limiting resource usage request avoid degradation due cache misses allow cache hits proceed 
event driven concurrency scalability limits threads led developers entirely employ event driven approach managing concurrency 
approach shown server small number threads typically cpu loop continuously processing events different types queue 
events may operating system internally application generally correspond network disk readiness timers application specific events 
eventdriven approach implements processing task finite statemachine transitions states fsm triggered events 
way server maintains continuation state foreach task relying thread context 
event driven design number systems including scheduler network disk request fsm request fsm request fsm request fsm request fsm event driven server design shows flow event driven server 
main thread processes incoming events network disk sources uses drive execution finite state machines 
fsm represents single request flow execution system 
key source complexity design scheduler control execution fsm 
flash thttpd zeus jaws web servers andthe harvest web cache 
flash component server responds particular types events socket connections accesses 
main server process responsible continually dispatching events components imple mented library calls 
certain operations case filesystem access asynchronous interfaces main handles events dispatching helper processes ipc 
helper processes issue blocking requests return main process completion 
harvest structure similar single threaded event driven exception protocol implemented separate process 
tradeoffs threaded event driven concurrency mod els studied extensively jaws web server 
jaws provides framework web server construction allowing model protocol processing code cached filesystem components customized 
seda jaws importance adaptivity service design facilitating static dynamic adaptations service framework 
knowledge jaws evaluated light loads concurrent clients addressed adaptivity heavy load 
event driven systems tend robust load little tion throughput offered load increases saturation 
shows throughput achieved event driven implementation ofthe service 
number tasks increases server throughput increases pipeline fills bottleneck case saturated 
number tasks pipeline increased excess tasks absorbed server event queue throughput remains constant huge range load latency task increasing linearly important limitation model assumes threads block reason nonblocking employed 
prior investigated scalable primitives event block regardless mechanisms due interrupts page faults garbage collection event driven design raises number additional challenges application developer 
scheduling ordering events important concern application responsible deciding process incoming event order process multiple flows 
order balance fairness low response time application carefully multiplex execution multiple throughput tasks sec gamma latency msec delta number tasks pipeline linear ideal latency event driven server throughput benchmark measures driven version server 
case server uses single thread process tasks task reads kb single 
filesystem interface provided operating system linux blocking disk data cache estimates best possible performance nonblocking disk layer 
shows throughput remains constant load large number tasks note change horizontal axis scale response time linear note log scale axis 
fsms 
choice event scheduling algorithm specific application new functionality may require algorithm redesigned 
modularity achieve code implementing state trusted block consume large number resources stall event handling thread 
structured event queues variants standard event driven design pro posed counter problems outlined 
common aspect designs structure event driven application set queues improve code modularity simplify application design click modular packet router example 
click packet processing stages implemented separate code private state 
click optimized improve packet latency router allowing single thread call multiple packet processing stages 
design targeted specific application routing single thread services 
click assumption modules bounded processing times leading relatively static resource management poli cies 
describe techniques scheduling load conditioning software router seda design controllers adjust runtime parameters dynamically load gribble distributed data structures dds layer structured event processing framework :10.1.1.32.6324
dds emulate asynchronous network disk interfaces making fixed size thread pools software components com posed explicit event queues implicit upcalls 
crews tss queue scanner examples structured event queues limited numbers threads manage concurrency 
systems event queue decouples execution components improves modularity robustness stagedserver system modules communicating explicit event queues :10.1.1.14.141
case goal processor cache locality carefully scheduling threads events module 
aggregating execution events queue locality enhanced leading greater performance lauer needham classic discusses merits processes communicating messages contrasts approach procedures closely related threaded model described 
seda seen instance message oriented model dis cussed 
authors claim message models duals program imple mented model just efficiently implemented 
agree basic sentiment argument overlooks building scalable general purpose multithreading inherent difficulties adapting load thread model explicit request queue 
staged event driven architecture section propose new software architecture staged event driven architecture seda designed enable high concurrency load conditioning ease engineering internet services seda decomposes application network stages separated event queues introduces notion dynamic resource con allow applications adjust dynamically changing load 
overview seda approach service design shown fig ure 
goals primary goals seda follows support massive concurrency avoid performance threads seda event driven execution possible 
requires system provide efficient scal able primitives 
simplify construction conditioned services complexity building internet services seda shields application programmers details scheduling resource man agement 
design supports modular construction applications provides support debugging performance profiling 
enable introspection applications able analyze re quest stream adapt behavior changing load conditions 
example system able prioritize filter requests service heavy load 
support self tuning resource management priori knowledge application resource requirements client load characteristics system adjust resource management pa rameters dynamically meet performance targets 
example number threads allocated stage determined perceived concurrency demands hard coded programmer administrator 
stages robust building blocks fundamental unit processing seda stage 
self contained application component consisting event handler incoming event queue thread pool depicted fig ure 
stage managed controller affects scheduling thread allocation described 
stage threads operate batch events incoming event queue invoking application supplied event handler 
event handler batch events dispatches zero events enqueuing event queues stages 
packet hit connection file data socket read socket listen file socket write staged event driven seda server structural representation seda web server described detail section 
composed set stages separated queues 
edges represent flow events stages 
stage independently managed stages run sequence parallel combination 
event queues allows stage individually load conditioned example thresholding event queue 
simplicity event paths stages elided 
event queue controller event handler thread pool seda stage stage consists incoming event queue pool application supplied event handler 
stage operation managed controller adjusts resource allocations 
threads basic concurrency mechanism seda limited small number threads stage single thread task system 
see section automatically tune number threads allocated stage demand design allows run sequence parallel combination depending characteristics thread system scheduler 
assume preemptive os supported threads smp environment choice fundamental seda design 
forexample thread system designed cognizant staged structure application schedules threads accordingly return issue section 
core logic stage provided event handler batch multiple events 
event handlers direct control queue operations threads 
separating logic thread management scheduling stage able control execution event handler implement management policies 
example number ordering events passed event handler controlled externally environment 
application may implement scheduling policy filtering reordering event batch 
applications network stages seda application constructed network stages connected queues 
event handlers may enqueue events stage allocating separate thread pool stage possible multiple stages share thread pool 
simplify discussion de scribe seda terms private thread pool stage 
note number stages application typically smaller numberof threads system support separate thread pool stage reasonable 
event handler thread pool threshold observe adjust length size adjust observe event handler thread pool running avg stages rate thread pool controller batching controller seda resource controllers stage associated adjusts resource allocation behavior keep application operating regime 
thread pool controller adjusts number stage batching controller adjusts number events processed iteration event handler 
obtaining handle stage incoming event queue provided lookup routine invoking enqueue operation queue important aspect event queues seda may finite enqueue operation may fail queue wishes entries say reached threshold 
applications may backpressure blocking full queue load shed ding dropping events enqueue operations fail 
alternately application may wish take service specific action error user performing alternate function providing degraded service illustrates structure seda application case haboob web server described section 
number application specific stages process requests implement page cache forth provided runtime support asynchronous interfaces described section queue stages decouples execution introducing explicit control boundary 
model execution thread stage thread may pass data control boundary enqueuing event 
basic question code modules communicate means queue directly subroutine call 
introducing queue provides isolation modularity independent load management may increase latency 
example third party code isolated stage allowing stages communicate event queue calling directly seda design facilitates debugging performance analysis services traditionally challenge complex multi threaded servers 
decomposition application code stages explicit event delivery mechanisms facilitates inspection example debugging tool trace flow events system visualize interactions stages 
stages interact queue length thread pool size gamma time sec intervals clients clients clients stage stage seda thread pool controller graph shows operation ofthe thread pool controller run haboob web server described section 
controller adjusts size stage thread pool basedon length corresponding event queue 
run queue length sampled seconds thread added pool entries maximum stage limit threads 
threads removed pool idle seconds 
stage uses controller threshold queue entries exaggerate controller behavior 
event dispatch protocol traditional api ward interpose proxy stages components debugging performance profiling 
mechanism prototype capable generating graph depicting set application stages relationship 
prototype generate temporal vi event queue lengths memory usage system properties valuable understanding performance 
dynamic resource controllers key goal enabling ease service engineering shield program mers complexity performance tuning 
order keep stage operating regime seda set automatically adapt resource usage stage observed performance demand 
abstractly runtime characteristics stage adjusts allocation scheduling parameters meet performance targets 
controllers entirely local knowledge particular stage concert global state implemented resource controllers seda shown 
thread pool controller adjusts number threads executing stage 
goal avoid allocating threads meet concurrency demands stage 
controller periodically samples input queue adds thread exceeds threshold maximum number threads stage 
threads removed stage idle period time 
shows effect thread pool controller operating web server described section operation discussed detail section 
second batching controller adjusts number processed invocation event handler stage batching factor 
observed processing increases throughput cache locality task aggregation performed :10.1.1.14.141
large batching factor crease response time 
controller attempts trade effects searching smallest batching factor sustains high output rate events sec gamma batching factor time ms intervals controller input controller output seda batching controller graph shows operation controller simple benchmark consisting single stage generating events oscillating rate 
causes measured output rate vary shown top portion 
output rate increases controller decreases batching factor 
output rate de creases controller increases batching factor 
batching factor reset maximum value sudden drop output rate 
put 
operates observing output rate events stage moving average samples decreases batching factor throughput begins degrade 
throughput de grades slightly batching factor increased small amount 
controller responds sudden drops load resetting maximum value 
shows batching controller mechanisms represent simple examples dynamic control seda 
possible introduce complex controllers system example controller adjust thread pool sizes global notion stage priority keep number entire system threshold 
option adjust thread scheduling parameters stage progress steere 
seda asynchronous sockets library described section contains optional controller rate packets read network 
section describe application specific controller adaptively sheds meet response time target 
seda structure facilitates inspection control underlying application range control strate gies possible model 
important aspect dynamic control seda application adapt changing conditions despite particular algorithms underlying operating system 
sense seda controllers naive resource management policies os 
example seda batching controller aware ofthe os thread scheduling policy influences thread scheduling external observations application performance 
cases may desirable exert control underlying os example provide quality service guarantees stages threads believe basic resource management mechanisms provided commodity operating systems sub ject application level control adequate needs internet services 
seda prototype implemented seda internet services platform 
implemented entirely java set native libraries nonblocking socket described sec tion 
latest java implementations coupled java language features software engineering robustness benefits java performance tradeoffs 
instance rely java automated memory management garbage collect expired events system greatly simplifies code components responsible tracking lifetime events 
java statically compiled languages closing fact java seda web server outperforms popular implemented described section 
application module implements simple interface single method call processes batch events pulled stage queue 
applications create manage threads responsibility runtime system associated controllers 
sand storm provides thread manager interface tailored implement various thread allocation scheduling policies manages pool threads stage relies underlying os scheduling 
provides apis creating destroying stages performing queue operations controlling queue thresholds profiling debugging 
file mechanisms described section provided standard interfaces runtime consists lines code non commenting source statements 
core runtime facilities 
asynchronous primitives meet seda goal supporting high concurrency requires efficient robust interfaces 
section describes seda concepts implement interfaces existing os primitives 
asynchronous network socket layer nonblocking provided operating system layer uses blocking os calls thread pool expose nonblocking behavior 
layers implemented setof seda stages applications provide fast asynchronous asynchronous socket asynchronous socket layer provides nonblocking sockets interface services 
applications create instances classes socket initiate outgoing incoming socket connections 
connection established object pushed queue provided user typically queue associated requesting stage 
incoming packets enqueued user queue implements queue interface outgoing packets placed 
outgoing packet may associated event queue completion event pushed packet transmitted 
error notification events user similar way 
internally layer implemented stages shared sockets shown 
reads network packets responds user requests initiate new socket 
writes packets network establishes new outgoing connections 
accepts connections responds user requests listen new port 
operation async converted request placed appropriate stage request queue stage services separate event queues request queue user readiness completion event queue connect ready request complete connection request write operating system write application seda asynchronous sockets layer sock ets interface consists stages read write listen 
read stage responds network readiness events reads data sockets packets application stage 
write stage accepts outgoing packets schedules writing appropriate socket 
establishes socket connections 
listen stage accepts new tcp connections pushes connection events application 
operating system 
thread stage alternately ser vices queue simple timeout mechanism toggle 
event queue implemented library operations invoke appropriate os call retrieve events 
current implementation supports standard unix poll system call dev poll interface event delivery 
native library provide nonblocking socket calls java increase fairness sockets stage randomizes order processes events delivered operating system 
thisis necessary os generally returns socket events fixed order increasing order file descriptor operates performing socket read readiness event indicates socket data available 
kb pre allocated buffer enqueues resulting packet event queue provided user 
case er ror peer closed connection stage closes socket pushes appropriate notification event user 
read requires allocation new packet buffer potentially cause great deal garbage collection overhead performance issue 
note system implemented java explicit deallocation expired necessary 
provides optional rate controller throttle rate packets read network useful performing load shedding overload conditions 
controller implemented calculating moving incoming packet rate introducing artificial delays event processing loop achieve certain rate target receives packet write requests user enqueues internal queue associated 
os indicates socket ready writing attempts write packet socket outgoing queue 
section socket queue may thresholded prevent slow sockets consuming resources server evaluate performance implemented simple server application accepts bursts kb packets num ber clients responding single byte ack burst packets 
somewhat artificial application meant network layer measure scalability number clients increases 
shows aggregate throughput server bandwidth mbit sec gamma number connections run connections seda asynchronous socket asynchronous socket layer asynchronous sockets layer performance graph performance seda asynchronous socket layer function number simultaneous connections 
client opens connection tothe server issues bursts kb packets server responds single byte ack burst packets 
machines switched gigabit ethernet running linux 
seda server nonblocking primitives provided operating system performance compared compatibility layer blocking sockets multiple threads emulate asynchronous thread unable accept simultaneous connections number threads required exceed user thread limit linux 
number clients increases 
server way mhz pentium iii systems interconnected gigabit ethernet running linux ibm jdk implementations socket layer shown 
layer nonblocking provided os dev poll event delivery mechanism 
compared compatibility layer uses blocking sockets thread pool asynchronous layer creates thread connection process socket read events fixed size pool handle socket writes 
compatibility layer originally developed provide asynchronous java functionality directly 
nonblocking implementation clearly outperforms degrades rapidly number connections increases 
fact threaded implementation crashes receiving connections number threads required exceeds user thread limit linux 
slight throughput degradation non blocking layer due part lack scalability linux network stack highly optimized dev poll mechanism socket event notification number sockets increases overhead involved polling readiness events operating significantly 
asynchronous file file layer represents different de sign point 
underlying operating system provide nonblocking file primitives forced blocking bounded thread pool implement layer users perform file object supports patches providing nonblocking file support available linux part standard distributions 
furthermore patches kernel level thread pool implement nonblocking file writes seda control 
familiar interfaces read write seek stat close 
op erations translates request placed stage event queue 
threads dequeue request perform blocking operation file 
ensure multiple requests file executed serially process events particular file time 
request completes corresponding completion event enqueued user event queue 
stage initialized single thread 
seda thread pool controller responsible dynamically adjusting size thread pool observed 
shows thread pool controller run seda web server described section 
run periods corresponding increasing number clients note client load extremely bursty 
bursts arrive controller adds threads stage thread pool saturating maximum threads 
periods demand thread pool shrinks 
stages require threads increasing client load number threads needed service file decreases 
underlying filesystem buffer cache warming service disk requests rapidly 
thread pool controller infers fewer threads needed manage disk concurrency avoids creating threads needed 
applications evaluation section performance load conditioning evalu ation applications haboob high performance server packet router gnutella peer peer file sharing network haboob closed loop server clients issue requests wait responses gnutella packet router exam ple open loop server server performance act limiting factor offered load 
haboob high performance server web servers form archetypal component scalable internet ser vices 
prior investigated engineering aspects building high performance servers little said conditioning robustness ease construction 
benefit studying servers variety industry standard bench marks exist measure performance 
chosen load model specweb benchmark suite basis important modifications 
measure performance static web page accesses constitute specweb load mix 
second keep web page file set fixed gb disk files corresponding specweb target load connections 
files range size bytes accessed zipf request distribution specweb 
details 
haboob architecture structure haboob shown 
server con sists stages devoted asynchronous socket disk described previous section 
stage re accepting new client connections protocol processing incoming packets 
stage accepts request events passes stage represent disk files generates responses directly pages generated gather server statistics 
implements memory web page cache implemented hashtable haboob large dust storm occurring desert sudan 
indexed url entry contains response packet con header web page payload 
stage responsible handling page cache misses chronous file layer read contents requested page disk 
sends responses client han aspects connection management statistics gathering 
additional stage shown generates dynamic webpages html templates embedded code written python scripting language 
feature provides general purpose server side scripting akin java server pages 
page cache attempts keep cache size thresh old set kb measurements provided 
aggressively buffers capacity misses allowing garbage collected java runtime approach yield noticeable performance advantage 
cache application specific event scheduling increase performance 
particular implements shortest connection scf scheduling reorders request stream send short cache entries long ones prioritizes cache hits misses 
applied set events provided batching controller starvation requests issue constructing haboob set stages greatly increased modularity design stage embodies robust reusable individually conditioned load 
able test different implementations page cache cation rest code runtime simply instantiates different stage place original page cache 
likewise prior knowledge haboob structure able replace haboob asynchronous file layer alternate little effort 
including platform web server code consists non commenting source state ments devoted protocol processing library 
benchmark configuration comparison performance measurements pop ular apache web server version shipped linux red hat systems flash web server rice univer sity 
apache fixed size process pool processes process manages single connection time reading file disk sending client kb chunks blocking operations 
flash uses efficient event driven design asingle process handling request processing tasks 
set helper processes perform blocking disk pathname resolution 
maximum size flash static page cache set kb size haboob 
apache flash haboob implemented java 
measurements taken server running way smp mhz pentium iii system gb ram linux 
ibm jdk java platform 
machines ofa similar configuration load generation client machine number threads simulate actual clients 
interconnected switched gigabit ethernet 
configuration simulate wide area network effects performance stability server heavy load client load generator loops continually requesting web page distribution specified specweb suite reading sleeping fixed time milliseconds requesting page 
closely simulate connection clients wide area client closes tcp connection requests connection contin 
value chosen observations traffic benchmarks run warm filesystem webpage caches 
note file set size gb larger physical memory static page cache haboob set mb measurements include large amount disk performance analysis shows performance haboob compared flash terms aggregate throughput response time 
shown jain fairness index number requests com client 
metric defined xi xi number requests clients 
indicates server equally fair clients smaller values indicate fairness 
intuitively clients receive share service clients receive service jain fairness index equal shows haboob throughput stable number clients increases sustaining mbps clients 
apache exhibit stable throughput slightly haboob 
result surprising expect apache server degrade performance number clients large 
recall apache accepts connections time difficult sustain high throughput process concurrency 
number exceeds amount clients wait increasingly longer periods time accepted system 
flash similar problem caps number simultaneous connections due limitation number file descriptors select system call 
server saturated clients wait long periods time establishing connection effect demonstrated shows cumulative distribution response times server clients response time defined total time server respond request including time establish tcp connection 
servers approximately average response times distribution differ ent 
apache flash show greater distribution low response times haboob long tails exceeding tens seconds percentage requests 
note log scale length tail 
maximum re sponse time apache seconds seconds flash 
long tail response times caused exponential back tcp retransmission timer establishing new connection linux grow large seconds apache client lucky connection accepted quickly requests handled single server process process competition processes manageable number systems 
explains low response times 
client unlucky wait server process available tcp means wait time large 
unequal note web servers configured higher limit number requests connection unrealistic benchmark results 
worth noting apache flash sensitive benchmark configuration testing revealed bugs leading ously degraded performance certain conditions 
example apache throughput drops considerably server client closes connection 
results represent optimistic results servers 
throughput mbit sec gamma fairness delta number clients throughput fairness haboob prob response time response time msec apache flash haboob throughput vs number clients cumulative distribution response time clients clients throughput rt mean rt max fairness throughput rt mean rt max fairness apache mbps ms ms mbps ms ms flash mbps ms ms mbps ms ms haboob mbps ms ms mbps ms ms haboob web server performance shows performance haboob web server compared apache flash 
shows server fileset gbytes number clients increases 
shown jain fairness index delivered server 
fairness index indicates server equally fair clients smaller values indicate fairness 
shows cumulative distribution function response times clients 
apache flash exhibit high frequency low response times heavy tail maximum response time corresponding minutes 
treatment clients reflected lower value fairness apache 
flash clients accepted system quickly subject queueing delays server 
low response times flash owe mainly efficient implementation including protocol processing library performed optimizations haboob 
fact flash accepts connections means heavy load tcp backoff issue leading long tail response time distribution contrast haboob exhibits great degree fairness clients overloaded 
mean response time ms max imum sec 
keeping goal graceful degradation server overloaded unfairly ize waiting requests arbitrary wait times 
haboob rapidly accepts new client connections allows requests queue ap plication serviced fairly pass stages 
load visible service allowing various policies applied 
example provide differentiated service necessary efficiently accept connections inspec tion 
tradeoff low average response time versus low variance response time 
haboob opted 
adaptive load shedding section evaluate behavior haboob overload application specific controller attempts keep response times threshold load shedding benchmark client repeatedly requests dynamic web page requires significant amount computation generate subjecting server large number bottleneck requests induce heavier load generally possible static web pages 
request server performs iterations loop file reads data generates sum data 
processing server returns kb response client 
apache implemented perl module runs con text apache server process 
flash provided fast cgi interface creates number persistent server pro cesses handle dynamic requests 
cgi request idle server process handle request new process created idle 
haboob bottleneck implemented separate stage allowing number threads devoted stage processing tobe determined thread pool controller thresholding stage incoming event queue reject excess load 
implementations amount performed dynamic page generation calibrated cause side delay ms request 
shows cumulative distribution response times forthe servers clients 
apache haboob exhibit large response times different reasons 
apache response caused tcp retransmit backoff explained concurrent processes queueing delay server 
haboob concurrent requests queued server time leading large queueing delay 
flash apparent low response time due bug cgi processing code causes close connections prematurely unable fork new cgi process 
clients may cgi processes system time processes exceeds user process limit linux 
fork fails flash closes client connection immediately response error message client 
run requests resulted prematurely closed connection mechanism suggests effective way bound response time requests server adaptively shed load detects overloaded 
demonstrate idea constructed application specific controller bottleneck stage observes average response time requests passing stage 
response time exceeds threshold sec exponentially reduces stage queue threshold 
prob response time response time msec haboob control haboob control flash apache response time controller graph shows effect specific controller sheds load order keep response times target value 
clients repeatedly requesting page requires computation generate 
apache haboob control process requests leading large response times flash rejects large number requests due bug cgi processing code clients informed server busy 
response time con enabled haboob rejects requests error message average response time exceeds threshold sec 
response time threshold controller increases fixed amount 
stage unable enqueue new request bottleneck stage event queue queue threshold exceeded error message returned client 
note just example load shedding policy alternatives send redirect node server farm provide degraded service shows cumulative response time distribution response time controller enabled 
case controller response time requests server requests exhibiting response time sec maximum re sponse time sec 
run requests rejected server due queue thresholding 
note controller guarantee response time target value bursts occurring queue threshold high induce spikes time experienced clients 
gnutella packet router chose implement gnutella packet router demonstrate seda non traditional internet services 
gnutella router represents different style service server routing packets participants peer peer file sharing network 
services gnutella increasing importance distributed applications developed take advantage connectedness hosts wide area 
peer adopted distributed storage systems freenet oceanstore intermemory gnutella allows user search download files gnutella users 
protocol entirely decentralized gnutella client form ad hoc multihop routing network layered tcp ip nodes communicate forwarding neighbors 
gnutella nodes tend connect typically nodes initial nodes network accomplished known host 
message types gnutella ping discover network pong response ping query search files served gnutella hosts re sponse query push allow clients download files firewall 
packet router responsible ping query messages neighbors routing pong push messages path correspond ing ping query message 
details message formats routing protocol 
architecture seda gnutella packet router implemented stages addition asynchronous socket layer 
code consists non commenting source statements devoted gnutella protocol processing 
stage accepts tcp connections processes packets passing packet stage performs actual packet routing maintenance routing tables 
helper stage join gnutella network contacting known site receive list hosts connect 
attempts maintain neous connections network addition connections established wide area clients 
joining live gnutella routing packets allows test seda real world environment measure traffic passing router 
hour run router processed packets average packets sec received connections network average simultaneous connections time 
router capable sustaining packets 
protection slow sockets original packet router prototype exhibited interesting hours correctly routing packets network server crash running memory 
various stage queue lengths allowed easily detect source problem large number outgoing packets queueing certain wide area connections causing queue length memory usage unbounded 
measured size gnutella messages approximately bytes packet rate just packets second saturate commonly users gnutella software 
solution case impose threshold queue socket close connections exceed threshold 
solution acceptable gnutella clients auto matically discover connect multiple hosts network redundancy network nodes means clients need particular host remain connected network 
load conditioning behavior evaluate seda resource controllers load condition ing introduced deliberate bottleneck gnutella router query message induces servicing delay ms having application event handler sleep ms query packet received 
implemented load connects server generates streams packets distribution approximating real gnutella traffic 
gnutella traffic model query messages constitute generated packets 
single thread performing packet routing number packets flowing server increases delay cause large messages shows average latencies ping query packets passing server offered load increasing packets sec 
client server machines configuration server benchmarks 
packet latencies crease dramatically offered load exceeds server capacity 
gamma gamma gamma delta delta delta delta delta delta latency ms offered load packets sec packet query gamma gamma gamma delta delta delta delta delta delta latency ms offered load packets sec packet query queue length time ms intervals threads added pool stage queue length single thread thread pool controller queue length profile gnutella packet router latency graphs show average latency ping query packets passing gnutella packet router incoming packet rates 
query packets packet mix induce server side delay ms 
shows latency single thread processing packets 
note latency increases dramatically offered load exceeds server capacity packets sec server ran memory taken 
shows latency thread pool controller enabled 
note packets sec threads added application stage event queue reached threshold value 
explains higher packet latencies compared packets sec threads added stage 
shows queue length time load packets sec thread pool controller active 
controller added thread stage points indicated 
case packets sec server crashed due running outof memory buffering incoming packets latency measurement taken point load conditioning policies may employed 
simple policy threshold stage incoming event drop packets threshold exceeded 
alternately approach similar random early detection red congestion avoidance schemes packets dropped probabilistically length input queue :10.1.1.128.5092
policies cause packets dropped overload due lossy nature gnutella network traffic may solution 
alternate policy admit packets system application event handler filter query pack ets source overload 
policy input rate controller bound rate packets system 
alternate approach seda resource overcome bottleneck automatically 
approach thread pool controller adds threads stage de additional concurrency required mechanism similar dynamic worker allocation cluster tacc system shows average latencies gnutella router seda thread pool controller enabled 
shown threads added thread pool allowing server handle increasing packet loads despite bottleneck 
matches theoretical value obtained little result model stage queueing system threads arrival rate query packet frequency query ser delay seconds number threads needed main tain completion rate pl ms threads 
discussion internet services give rise new set systems design requirements massive concurrency provided robust easy program manner gracefully handles vast variations load 
seda establishing design principles regime 
seda design execution model introducing notion stages connected explicit event queues 
seda set dynamic controllers manage resource usage scheduling stage described controllers includ ing manage thread allocation stages degree batching internally stage 
efficient asynchronous components applications built seda design showing seda exhibits robust behavior load 
seda model opens new questions internet space 
explicit event queues dynamic resource controllers raise potential novel scheduling resource management algorithms specifically tuned services 
plan implement generalized flow control scheme commu nication stages scheme event requires certain number credits enqueue target stage event queue 
variable number credits event interesting policies implemented believe measurement control key resource management overload protection busy internet services 
long standing approaches resource containment assign fixed resources task process thread request system strive contain resources consumed task 
techniques success providing differentiated service internet services containment typically mandates priori assignment re sources task limiting range applicable load conditioning policies 
argue dynamic resource control coupled specific adaptation face overload right way approach load conditioning new challenges arise control considered basis resource management 
detecting overload conditions variables affect delivered performance service determining service fact overloaded cause interesting problem 
second determining appropriate control strategy counter overload 
plan improvements tothe resource controllers current implementation new controllers optimize alternate metrics 
example consumption may desirable prioritize stages free resources consume 
seda body control systems brought bear management scratched surface potential technique common concern event driven concurrency models ease programming 
modern languages programming tools development debugging threaded applications developers believe event driven programming inherently 
fact event driven server applications quite complex somewhat ad hoc design 
experience programming seda model easier multithreaded application design traditional event driven model 
threads isolated single stage issues thread synchronization race conditions straightforward 
message oriented communication stages establishes explicit orderings traditional event driven design trace flow events system 
view seda ideal middle ground threaded event driven designs exploration programming model important direction seda facilitates construction conditioned services commodity operating systems seda model directions os design 
envision os supports seda execution model directly provides applications scheduling resource usage 
approach similar various research systems en able application specific resource management 
radically seda operating system need designed allow multi ple applications share resources transparently 
internet services highly specialized designed share machine generally undesirable say web server run machine database engine mention word processor 
os may enforce protection prevent stage corrupting state kernel stage system need virtualize resources way masks availability applications 
acknowledgments research supported defense advanced agency dabt national science foundation eia intel corpora tion nortel networks royal philips electronics 
matt welsh supported national science foundation graduate fellowship 
wewould steve gribble joe hellerstein marti hearst valuable input 
eric fraser matt philip provided support berkeley millennium cluster obtain performance measurements 
eric wag ner provided server side scripting functionality haboob web server 
especially indebted shepherd andrew myers andthe anonymous reviewers helpful comments 
akamai www akamai com 
america online press data points 
aol com press press datapoints html 
digital island www com 
acme labs 
thttpd tiny turbo throttling server 
www acme com software thttpd 
anderson bershad lazowska levy :10.1.1.13.9310
scheduler tions effective kernel support user level management parallelism 
acm transactions computer systems february 
apache software foundation 
apache web server 
www apache org 
banga druschel mogul 
resource containers new resource management server systems 
proc 
third symposium operating systems design implementation osdi february 
banga mogul 
scalable kernel performance internet realistic loads 
proc 
annual usenix technical conference new orleans la june 
banga mogul druschel 
scalable explicit mechanism unix 
proc 
usenix annual technical conference monterey ca june 
bea systems 
bea weblogic 
www com products weblogic 
bershad savage pardyak sirer becker ski chambers eggers :10.1.1.117.6702
extensibility safety performance spin operating system 
proc 
th acm symposium principles sosp 
chankhunthod danzig neerdaels schwartz worrell 
hierarchical internet object cache 
proc 
usenix annual technical conference pages january 
chen goldberg gottlieb yianilos 
implementation archival intermemory 
proc 
fourth acm conference digital libraries dl berkeley ca 
clarke sandberg wiley hong 
freenet dis tributed anonymous information storage retrieval system designing privacy enhancing technologies 
proc 
icsi workshop design anonymity unobservability berkeley ca 
crovella harchol balter 
connection web servers 
proc 
usenix symposium internet technologies systems usits october 

computing 
scientific american july 
floyd jacobson :10.1.1.128.5092
random early detection gateways tion avoidance 
ieee acm transactions networking august 
fox gribble chawathe brewer gauthier 
cluster scalable network services 
proc 
th acm symposium operating systems principles st malo france october 
gnutella 
gnutella wego com 
gribble brewer hellerstein culler :10.1.1.32.6324
scalable structures internet service construction 
proc 
fourth symposium operating systems design implementation osdi oc 
gribble welsh von behren brewer culler czerwinski gummadi hill joseph katz mao ross zhao 
ninja architecture robust internet scale systems 
computer networks june 
special issue pervasive computing 
hewlett packard 
speak open services platform 
www speak net 
hu schmidt 
techniques developing mea high performance web servers atm networks 
proc 
infocom march april 
hu pyarali schmidt 
high performance web windows nt design performance 
proc 
usenix windows nt workshop august 
ibm 
ibm websphere application server 
www ibm com software 
java server pages api 
java sun com products jsp 
jain chiu 
quantitative measure fairness dis resource allocation shared computer systems 
technical report tr dec research september 
kaashoek engler ganger brice hunt mazi eres grimm jannotti mackenzie :10.1.1.130.1539
application performance flexibility exokernel systems 
proc 
symposium operating systems principles sosp october 

problem 
www com html 
kubiatowicz bindel chen czerwinski eaton geels gummadi rhea weatherspoon weimer wells zhao 
oceanstore architecture global scale persistent storage proc 
ninth international conference architectural support programming languages operating systems asplos november 
larus parkes :10.1.1.14.141
cohort scheduling enhance server perfor mance 
technical report msr tr microsoft research march 
lauer needham 
duality operating system structures proc 
second international symposium operating systems october 
lemon 
freebsd kernel event queue patch 
www com 
leslie mcauley black roscoe barham fair :10.1.1.28.6923
design implementation operating system support distributed multimedia applications 
ieee journal areas communications september 
lett 
tss time shared operating sys tem 
proc 
fall joint computer conference volume part pages 
lutz 
programming python 
reilly associates march 
microsoft 
dcom technical overview 
msdn microsoft com library html msdn htm 
microsoft 
iis overview 
www microsoft com windows library iis iis rview asp 
mogul 
case persistent connection 
proc 
october 
morris kohler jannotti kaashoek 
click 
proc 
th acm symposium operating systems principles sosp pages kiawah island south carolina december 
mosberger peterson 
making paths explicit scout ing system 
proc 
osdi october 
netscape 
netscape enterprise server 
home netscape com enterprise index html 
ogata 
modern control engineering 
prentice hall 
pai druschel zwaenepoel 
flash efficient server 
proc 
annual usenix technical conference june 
parekh gandhi hellerstein tilbury jayram control theory achieve service level objectives performance management 
proc 
ifip ieee international symposium management seattle wa may 
lever 
scalable network linux 
technical tr university michigan center information technology integration may 
peterson karlin 
scheduling computations ona software router 
proc 
sigmetrics june 
russinovich 
inside completion ports 
www com htm 
spatscheck petersen 
defending denial service scout 
proc 
rd symposium operating systems design implementation february 
standard performance evaluation 
specweb bench mark 
www spec org osg web 
steere goel mcnamee pu walpole feedback driven proportion allocator real rate scheduling 
proc 
rd usenix symposium operating systems design implementation osdi pages 
sun microsystems 
rpc remote procedure call protocol 
internet network working group rfc june 
sun microsystems enterprise java beans technology 
java sun com products ejb 
sun microsystems java remote method invocation 
java sun com products jdk rmi 
sun microsystems jini connection technology 
www sun com jini 
vandevoorde roberts 
crews abstraction control ling parallelism 
technical report research report digital equipment systems research center february 
voigt tewari 
kernel mechanisms differentiation overloaded web servers 
proc 
usenix annual technical conference boston june 
wald schwarz 
southern california seismic net bulletin 
research letters july august 
wallach engler kaashoek 
application specific handlers high performance messaging 
proc 
acm sigcomm conference applications technologies architectures computer communication pages stanford california august 
welsh 
nonblocking java 
www cs berkeley edu proj java 
welsh culler 
virtualization considered harmful os conditioned services 
proc 
th workshop hot topics operating systems hotos viii schloss germany may 
yahoo 
yahoo 
reports second quarter financial results 
docs yahoo com docs pr release html 
zeus technology 
zeus web server 
www zeus uk products ws 
