dimacs series discrete mathematics theoretical computer science synopsis data structures massive data sets phillip gibbons yossi matias 
massive data sets terabytes data commonplace 
increasing demand algorithms data structures provide fast response times queries data sets 
describe context algorithmic relevant massive data sets framework evaluating 
consider synopsis data structures little space provide fast typically approximated answers queries 
design analysis ective synopsis data structures er algorithmic challenges 
discuss number concrete examples synopsis data structures describe fast algorithms keeping date presence online updates data sets 

growing number applications demand algorithms data structures enable cient processing data sets gigabytes terabytes petabytes data 
massive data sets necessarily reside disks tapes making accesses base data set comparably slow single disk access times slower single memory access 
fast processing queries data sets disk accesses minimized 
focuses data structures supporting queries massive data sets minimizing avoiding disk accesses 
particular advocate study small space data structures 
denote synopsis data structures data structures substantively smaller base data sets 
synopsis data structures advantages non synopsis data structures fast processing synopsis data structure may reside main memory providing fast processing queries data structure updates avoiding disk accesses altogether 
fast swap transfer synopsis data structure resides disks swapped memory minimal disk accesses purposes processing queries updates 
synopsis data structure mathematics subject classi cation 
primary secondary 
research second author supported part israel science foundation founded academy humanities israeli ministry science 
copyright holder phillip gibbons yossi matias pushed pulled remotely internet minimal cost amount data transferred small 
lower cost synopsis data structure minimal impact space requirements data set supporting data structures cost system 
better system performance synopsis data structure leaves space memory data structures 
importantly space processing processing involves disks uses memory cache disks 
data warehousing environment example main memory needed query processing working space building hash tables hash joins caching disk blocks 
importance available main memory algorithms seen external memory algorithms literature upper lower time bounds fundamental problems inversely proportional logarithm available memory size vit 
see section examples 
machines large main memories increasingly commonplace memory available synopsis data structures remains precious resource 
small surrogate synopsis data structure serve asa small surrogate data set data set currently expensive access 
contrast linear space data structures massive data sets reside memory slow swap transfer times increase space requirements cost system constant factors hog memory serve small surrogate 
traditional viewpoint algorithms literature linear space data structure appropriate massive data sets data structures fail provide satisfactory application performance 
hand synopsis data structures small maintain base data sets summarize data set responses provide queries typically approximate ones 
challenges determine synopsis full data set keep limited space order maximize accuracy con dence approximate responses ciently compute synopsis maintain presence updates data set 
due importance applications number synopsis data structures literature existing systems 
examples include uniform biased random samples various types histograms statistical summary information frequency moments data structures resulting lossy compression data set synopsis data structures heuristic way formal properties proved performance accuracy especially presence updates data set 
ongoing seeks provide systematic study synopsis data structures including design analysis synopsis data structures performance accuracy guarantees presence data updates 
algorithmic relevant massive data sets framework evaluating 
brief combine pdm external memory model vs input output conventions typical synopsis data structures study online data structure problems 
general scenarios considered input resides disks pdm input arrives online pdm memory 
describe synopsis data structures highlight results problem domains database literature frequency moments hot list queries histograms quantiles 
outline 
section describes framework detail 
results frequency moments hot list queries histograms described sections respectively 
related results discussed section 
framework section rst describe context data structure problems massive data sets 
introduce synopsis data structures model analysis 
discuss example application domains 

problem set 
data structure questions consider number data sets set query classes qk data sets 
query classes priori may apply individual data sets multiple data sets 
data structure performance analyzed model computation distinguishes types storage fast slow fast storage limited size 
equate fast storage computer system main memory slow storage disks relevant model computation details section 
framework results relevant scenarios fast storage disks slow storage tapes fast storage processor cache memory slow storage main memory 
static ine scenario data sets input residing disks 
class queries goal design data structure class minimizes response time answer queries maximizes accuracy con dence answers minimizes preprocessing time needed build data structure 
dynamic online scenario models ongoing loading new data data set data sets arrive online memory stored disks 
speci cally input consists sequence operations arrive online processed data structure operation insertion new data item deletion existing data item query 
class queries goal design data structure class minimizes response time answer queries maximizes accuracy con dence answers minimizes update time needed maintain data structure 
interested additional overheads maintaining data structure charge updating data sets 
set re ects environments processing massive data sets 
example re ects data warehousing environments warehouse sales transactions 
data sets far insertions deletions 
important exception sliding window data set comprised data data source months sales transactions 
data sets batches old data periodically deleted room new data making number insertions comparable number deletions 
phillip gibbons yossi matias handle data sets query classes large number synopsis data structures may needed 
assume considering data structure problem isolation amount memory available data structure small fraction total amount memory 
evaluate ectiveness data structure function space usage footprint 
example common practice evaluate ectiveness histogram range selectivity queries function footprint see 
note online environments data set stored alongside data structure resides remote computer system may currently unavailable 
cases online view data ectively view data maintain data structure answer queries 
denote scenario purely online scenario 

synopsis data structures 
set motivates need data structures small footprints 
denote synopsis data structures data structures substantively smaller base data sets 
data structures small maintain full characterization base data sets regards class queries responses provide queries typically approximate ones 
synopsis data structures seek characterize data succinct representations 
natural synopsis data structure uniform random sample known random sample data set quite useful support variety queries set 
classes queries uniform sampling best choice 
trivial example class number items set queries single counter better 
interesting examples rest 
de ne synopsis data structure follows 
definition 
synopsis data structure class queries data structure providing exact approximate answers queries uses space data set size constant 
sublinear space data structure may bean important improvement linear space data structure de nition demands polynomial savings space savings bene ts synopsis data structures outlined section realized 
example massive data sets typically exceed available memory size polynomial factor data structure residing memory ao footprint 
traditional data structures synopsis data structure evaluated metrics coverage range importance queries answer quality accuracy con dence approximate answers queries footprint space bound smaller better 
query time 
computation update time preprocessing time static scenario update time dynamic scenario 
ideally better queries updates require constant number memory operations disks operations answers exact 
synopsis data structures 
cost model 
query times computation update times analyzed models computation depending target computer system including parallel distributed models 
concreteness parallel disk model pdm vitter shriver vs vit adapted scenarios discussed 
pdm processors disks internal memory size processor 
disk partitioned blocks size unbounded size 
input size partitioned striped evenly disks dd ith block input data bi block data mod th disk 
output required similarly striped 
size parameters andb units input data items db 
internal memory small hold input su ciently large hold blocks disks 
algorithms analyzed metrics number operations processing time amount disk space 
single read write disks simultaneously transfer block respectively internal memory 
processing time analyzed assuming processor unit cost ram memory computation times processors connected network properties depend setting 
algorithmic pdm focused reducing number operations proving matching lower bounds 
mentioned bounds inversely proportional logarithm available memory speci cally proportional log 
examples discussed vit include sorting permuting matrix transpose computing fast fourier transform various batched problems computational geometry 
problems matrix multiplication lu factorization bounds inversely proportional main deviation pdm input output requirements 
query times computation update times analyzed pdm input output requirements adapted set described section 
rst deviation supplement pdm write output memory size 
static scenario input resides disks pdm allowed preprocess data store resulting data structures internal memory 
response query output written output memory contrast pdm 
processing query may incur operations 
dynamic scenario input arrives online internal memory form insertions data set deletions data set queries 
data structures maintained answering queries 
static scenario data structures may stored internal memory responses queries written output memory queries may answered incurring operations 
reducing number operations important pointed section operation take time memory operations modern computer systems 
note insertions deletions dynamic scenario applied base data set charge current state data set resides disks times 
cost reading data depends phillip gibbons yossi matias setting needs speci ed algorithms perform reads 
consider variety settings cases base data striped disks cases various indices trees exploited 
purely online scenario base data unavailable 
massive data sets case input size just larger memory size assumed fact polynomially larger constant 
note algorithm dynamic scenario updates incur amortized processing time update operations yields algorithm computing synopsis data structure static scenario pass data set db operations nt processing time 
simplicity remainder pdm single processor 

applications approximate query answering cost estimation 
important application domain synopsis data structures approximate query answering ad hoc queries large data warehouses gm 
large data recording warehousing environments advantageous provide fast approximated answers complex decision support queries see tpc benchmark tpc examples queries 
goal provide estimated response orders magnitude time time compute exact answer avoiding minimizing number accesses base data 
approximate query answering aqua project gmp gpa bell labs seek provide fast approximate answers queries synopsis data structures 
traditional data warehouse set depicted query answered exactly data warehouse aqua considers set depicted 
set new data loaded data warehouse observed approximate answer engine 
engine maintains various synopsis data structures answering queries 
queries responses new data data warehouse 
traditional data warehouse queries responses approx 
answer engine new data data warehouse 
data warehouse set providing approximate query answers queries sent approximate answer engine 
possible engine promptly returns response consisting approximated answer con dence measure con dence interval 
user decide answer computed base data user desire exact answer estimated time computing exact answer determined query optimizer approximate answer engine 
number scenarios user may prefer approximate answer seconds exact answer requires tens minutes compute drill query sequence data mining sks 
synopsis data structures discussed section base data remote currently unavailable exact answer option data available 
important application domain synopsis data structures cost estimation query optimizer 
commercial database systems limited storage set aside synopsis data structures histograms query optimizer estimate cost primitive operations comprising complex sql query estimates number items satisfy predicate estimates size join operation 
query optimizer uses cost estimates decide alternative query plans informed scheduling allocation load balancing decisions multi query multiprocessor database environments order minimize query response times maximize query throughput 
application domains highlight fact synopsis data structures useful providing fast approximate answers user queries speeding time compute exact answer 
sections highlight detail synopsis data structures problem domains 
sections meant comprehensive provide avor di culties techniques 
details including proofs experimental results omitted reader referred cited papers details 
rst problem domain estimating frequency moments data set number distinct values maximum frequency 
second problem domain estimating frequently occurring values data set 
third problem domain approximating quantiles types histograms data set 
note emphasis sections synopses keep limited space maintain synopses proved quality provide challenges particular synopsis data structures 
traditional data structure issues concerning representation store synopsis impact query time update time important somewhat secondary main emphasis 
seen techniques sections randomization approximation essential features study synopsis data structures problems essential problems 

frequency moments section highlight results synopsis data structures estimating frequency 
sequence elements ai member ug 
simplicity exposition assume mi jfj aj denote number occurrences sequence frequency demographic information frequencies data set described maintaining full histogram mu 
desired footprint substantially smaller succinct representation frequencies required 
detailed analysis show synopsis data structures reported section su ces constant 
phillip gibbons yossi matias de ne fk mk particular number distinct elements appearing sequence length sequence repeat rate gini index homogeneity needed order compute surprise index sequence see goo 
de ne max mi 
moment fk de ned sum powers numbers mi th root sum quantity denoted 
numbers fk called frequency moments frequency moments data set represent useful demographic information data instance context database applications 
indicate degree skew data major consideration parallel database applications 
example degree skew may determine selection algorithms data partitioning discussed dewitt see 
discuss estimation frequency moments available memory smaller full histogram available 
rst consider problem estimating demonstrates advantages viewing input online versus ad hoc sampling data set 
particular results showing ectively estimated synopsis data structure footprint log ectively estimated solely random sample data set memory employed 
discuss space cient algorithms estimating fk log synopsis data structures improved log synopsis data structure estimating 
lower bounds estimation fk mentioned results showing randomization approximation essential evaluating fk 

estimating number distinct values 
estimating number distinct values data set problem frequently occurs database applications particular subproblem query optimization 
haas claim virtually query optimization methods relational object relational database systems require means assessing number distinct values attribute relation function sequence consisting attribute values item relation 
synopsis data structure maintained best methods estimating sampling 
haas consider algorithms estimating 
propose hybrid approach inwhich algorithm selected degree skew data measured essentially function 
observe fairly poor performance obtained standard statistical estimators estimating sampling hard relatively unsolved problem 
consistent assertion known estimators give large errors data sets 
chaudhuri cmn show large error unavoidable relatively large samples regardless estimator 
exist estimator guarantee reasonable error reasonable probability sample size close size database 
formally 
theorem 
cmn consider estimator number distinct values sample size relation tuples 
error estimator de ned error maxf dg 
synopsis data structures exists choice relation probability error ln contrast algorithm demonstrates log synopsis data structure enables estimation arbitrary xed error bound high probability data set 
note synopsis data structure maintained observing entire data set 
practice realized data set loaded disks synopsis data structure maintained main memory small overhead 
flajolet martin fm fm described randomized algorithm estimating log memory bits analyzed performance assuming may algorithm explicit family hash functions exhibits ideal random properties 
log synopsis data structure consists bit vector initialized 
main idea algorithm item data set select random bit set quasi geometric distribution selected probability selection random hash function items value selection 
result expected number items selecting largest estimate 
alon ams adapted algorithm linear hash functions obtaining :10.1.1.102.5483
theorem 
fm ams exists algorithm sequence members ug computes number log memory bits probability ratio proof 
smallest integer consider members elements nite eld gf represented binary vectors length random members chosen uniformly independently 
member ai sequence appears compute zi ai product addition computed eld zi represented binary vector length binary vector denote largest rightmost bits ri zi 
maximum value ri maximum taken elements ai sequence output algorithm note order implement algorithm keep log bits representing irreducible polynomial needed order perform operations log bits representing maintain log log bits representing current maximum ri value 
suppose correct number distinct elements appear sequence estimate probability deviates considerably 
properties random mapping ax maps ai zi need xed ai zi uniformly distributed probability zi precisely mapping pairwise independent 
xed distinct ai aj probability zi zj precisely fix element appears sequence wx indicator random variable value ax zr wx ranges elements appear phillip gibbons yossi matias sequence linearity expectation wx expectation ofz pairwise independence variance markov inequality cf prob zr zr similarly chebyshev inequality ifc prob zr var zr zr prob zr var zr zr zr 
algorithm outputs maximum zr inequalities show probability ratio smaller 
log synopsis data structure class queries designed dynamic scenario insertions queries 
analyzed cost model section query update times processing time query update operations 

estimating fk 
alon ams developed algorithm sequence parameter estimate fk small constant factor high probability log synopsis data structure :10.1.1.102.5483
description taken considered implementation issues algorithm showed algorithm coined sample count adapted support deletions data set 
idea sample count algorithm simple random sample locations selected sequence data items inserted data set 
random selection easily done items inserted 
reach item chosen sample count number incoming items value 
turns count sample point random variable satis es fk variance reasonably small small desired accuracy con dence nal estimate obtained applying averaging techniques counts sample items 
speci cally thenumber memory words algorithm parameter determines accuracy result determines con dence input set relative error estimate exceeds probability algorithm computes random variables ys outputs median yi average random variables xij xij independent identically distributed random variables 
averaging reduce variance error chebyshev inequality median boost con dence cherno bounds 
xij computed sequence way follows choose random member ap sequence index chosen randomly uniformly numbers suppose ap ug 
aq number occurrences members sequence ap inclusive 
de ne 
ams estimate computed algorithm satis es prob jy accurate estimate guaranteed high probability synopsis data structures selecting ands log :10.1.1.102.5483
generally selecting ku log obtain 
theorem 
ams exists randomized algorithm computes sequence members ug log log logn memory bits number probability deviates fk fk xed log synopsis data structure class fk queries designed dynamic scenario :10.1.1.102.5483
waiting query time compute averages yi result query time cost model 
averages maintained running averages updates arrive resulting processing time query operations 
representing samples ap concise sample de ned section dynamic dictionary data structure update time likewise reduced processing time update operations 

improved estimation 
improved estimation algorithm ams :10.1.1.102.5483
sequence estimated small constant factor high probability log synopsis data structure 
description considers implementation issues algorithm shows algorithm coined war adapted support deletions data set 
tug war algorithm illustrated follows suppose crowd consists groups varying numbers people goal estimate skew distribution people groups 
estimate set ai group th person belongs 
arrange tug war forming teams having group assigned random teams 
equating displacement rope original location di erence sizes teams shown ams expected square rope displacement variance reasonably small :10.1.1.102.5483
approach implemented small memory observation persons crowd come contribute displacement incremental fashion 
addition updated displacements thing requires recording process assignment groups teams done succinctly appropriate pseudo random hash function 
sample count number memory words tug war parameter determines accuracy result determines con dence 
output median random variables ys average random variables xij xij independent identically distributed random variables 
xij computed sequence way follows select random wise independent mapping 
ug 
imi 
phillip gibbons yossi matias accurate estimates xed error guaranteed xed probability constant values su ce 
speci cally selecting log obtained 
theorem 
ams exists randomized algorithm computes sequence members pass log log logn memory bits number probability deviates xed algorithm implemented member sequence constant number arithmetic log log bits :10.1.1.102.5483
nite eld operations elements xed wehave log synopsis data structure class queries designed dynamic scenario 
query update times processing time query update operations 

lower bounds 
mention lower bounds ams space complexity randomized algorithms approximate frequency moments fk :10.1.1.102.5483
lower bounds obtained reducing problem appropriate communication complexity problem yao bfs ks raz set disjointness problem obtaining 
theorem 
ams xed randomized algorithm outputs pass input sequence elements ng number zk prob fk requires memory bits :10.1.1.102.5483
theorem 
ams randomized algorithm outputs pass input sequence elements ng number prob jy xed requires memory bits :10.1.1.102.5483
rst theorem places lower bound footprint synopsis data structure estimate fk constant factors purely online scenario distributions 
second theorem shows synopsis data structure exists estimating constant factors purely online scenario distributions 
discussed section synopsis data structures exist skewed distributions may practical interest 
number elements computed deterministically exactly log synopsis data structure simple counter 
theorems show randomization approximation essential evaluating fk 
theorem 
ams nonnegative integer algorithm outputs pass input sequence elements ng anumber fk probability xed requires memory bits :10.1.1.102.5483
theorem 
ams nonnegative integer deterministic algorithm outputs pass input sequence elements synopsis data structures ng number jy fk requires memory bits :10.1.1.102.5483
proof 
family subsets cardinality distinct members elements common 
existence follows standard results coding theory proved simple counting argument 
fix deterministic algorithm approximates fk xed nonnegative 
members sequence length starting members sorted order set members sorted order 
algorithm runs sequence form memory con guration reads rst elements sequence depends 
pigeonhole principle memory log bits distinct sets content memory reading elements equal content reading elements 
means algorithm give nal output sequences 
contradicts assumption values fk sequences di er considerably fk 
answer algorithm relative error exceeds sequences 
follows space algorithm log completing proof 

hot list queries section highlight results synopsis data structures answering hot list related queries 
hot list ordered set counti pairs frequently occurring values data set prespeci ed various contexts hot lists denoted high biased histograms ic ofm buckets rst mode statistics largest itemsets 
hot lists variety data analysis contexts including best sellers lists top lists example top selling items database sales transactions 
selectivity estimation query optimization hot lists capture skewed popular values relation shown quite useful estimating predicate selectivities join sizes see ioa ic ip 
load balancing mapping values parallel processors disks skewed values limit number processors disks load balance obtained 
market basket analysis sequence sets values goal determine popular itemsets tuples values occur sets 
hot lists maintained tuples values speci ed indicate positive correlation values itemsets hot list 
produce association rules specifying seemingly causal relation certain values 
example grocery store sequence customers set phillip gibbons yossi matias items purchased customer association rule customers buy bread typically buy butter 
caching policies frequently goal retain cache frequently items evict frequently cache full 
example frequently called countries list caller pro les real time telephone fraud detection pre fact early version hot list algorithm described contexts years 
examples suggest input need simply sequence individual values tuples various elds purposes hot list value associated tuple contribution tuple value count functions elds 
simplicity exposition discuss hot lists terms sequence values contributing value count 
hot lists trivial compute maintain su cient space hold full histogram data set 
data sets histograms require space linear size data set 
synopsis data structures hot list queries succinct representation required particular counts maintained value 
note di culty maintaining hot lists dynamic scenario detecting values infrequent frequent due shifts distribution arriving data 
small footprint detection di cult insu cient space keep track infrequent values expensive impossible purely online scenario access base data disks 
related seemingly simpler problem hot list queries popular items queries 
popular items query returns set counti pairs values frequency data set exceeds prespeci ed threshold data set 
hot list queries number pairs output frequency lower bound popular items queries frequency lower bound number pairs 
approximate answer popular items query readily obtained sampling sample size needed obtain desired answer quality predetermined frequency threshold 
example prespeci ed threshold percentage cherno bounds value frequency exceeds threshold occur times sample size probability fang techniques improving accuracy con dence popular items queries 
considered generalization tuples functions elds mentioned hot list queries denoted class queries iceberg queries 
algorithms combining sampling multiple hash functions perform coarse grained counting order signi cantly improve quality naive sampling approach 
remainder section describe results gm presenting studying synopsis data structures concise samples counting samples 
mentioned section synopsis data structures estimating count frequently occurring value constant factors purely online scenario distributions 
synopsis data structure exists di cult problem approximating hot list purely synopsis data structures online scenario distributions 
hand concise samples counting samples shown gm analytically experimentally produce accurate approximate hot lists previous methods perform quite skewed distributions interest practice 

concise samples 
consider hot list query data set size possible synopsis data structure set values uniform random sample data set proposed popular items queries 
frequently occurring values sample returned response query counts scaled note value occurring frequently sample wasteful available space 
represent copies value pair hv ki assuming values counts amount space freed space additional sample points 
simple observation leads synopsis data structure 
definition 
concise representation multiset values appearing multiset represented value count 
concise sample size uniform random sample data set concise representation footprint quantify advantage concise samples traditional samples terms number additional sample points footprint 
fhv vj concise sample data set values 
de size tobe pj ci 
note footprint ofs depends number bits value count 
example variable length encoding counts dlog xe bits needed store count reduces footprint complicates memory management 
approximate counts mor dlog log xe bits needed store power 
simplicity exposition consider xed length encoding log bits count value including bits needed distinguish values counts footprint log traditional sample sample points sample size footprint concise samples worse traditional samples encoding assumptions exponentially better depending data distribution 
example log data set concise sample size sample size case concise sample full histogram 
sample size concise sample may arbitrarily larger footprint lemma 
gm footprint logn exists data sets sample size concise sample times larger footprint size data set 
exponential distributions advantage exponential lemma 
gm consider family exponential distributions pr 
expected sample size concise sample footprint log proof 

note values counts footprint 
expected sample size lower bounded phillip gibbons yossi matias expected number randomly selected tuples rst tuple attribute value greater probability selecting value greater expected number tuples selected event expected gain concise sample traditional sample arbitrary data sets function frequency moments fk data set 
recall section fk taken values represented set mj number set elements value theorem 
gm data set concise sample sample size expected gain number distinct values mx fk nk proof 
pj mj probability item selected random set value xi indicator random variable xi ith item selected traditional sample value represented sample xi 
pr xi pj pj taken values represented set xi value selected selected rst steps 
clearly pm xi number distinct values traditional sample 
evaluate number distinct values mx mx xi pj pj mx pj pj mx pj pj pj mx fk nk pj pk mx note footprint concise sample log times number distinct values footprint traditional sample sample size log maintaining concise samples 
describe algorithm gm maintaining concise sample footprint bound new data inserted data set 
number sample points provided concise sample depends data distribution problem maintaining concise sample new data arrives di cult traditional samples 
traditional samples reservoir sampling algorithm vitter vit maintain sample presence insertions new data see section details 
algorithm relies heavily priori knowledge target sample size traditional samples equals footprint divided log 
concise samples sample size depends data distribution synopsis data structures date changes data distribution re ected sampling frequency 
maintenance algorithm follows 
entry threshold initially new data selected sample 
current concise sample consider insertion data item value probability add preserving concise representation 
footprint fors exceeds prespeci ed footprint bound raise threshold subject higher threshold 
speci cally sample size sample points evicted probability expected sample size sample points evicted 
note footprint decreased counti pair reverts singleton value removed altogether 
footprint decreased repeat higher threshold 
complete exibility selecting sequence increasing thresholds gm discussed variety approaches tradeo ways improve constant factors 
theorem 
gm algorithm maintains concise sample prespeci ed size bound constant amortized expected update time insert operations 
proof 
algorithm maintains uniform random sample threshold raised preserves invariant item data set treated probabilistically threshold new threshold 
look ups done constant expected time dynamic dictionary data structure hash table 
raising threshold costs processing time sample size concise sample threshold raised 
case threshold raised constant factor time expect constant number coin tosses resulting sample points retained sample point evicted 
amortize retained evicted amortize evicted insertion sample sample point evicted 

counting samples 
counting samples variation concise samples counts keep track occurrences value inserted data set value selected sample 
de nition motivated sampling counting process type static data set definition 
counting sample data set threshold subset stored concise representation de ned de nition obtained process probabilistically equivalent process value occurring times ip coin probability heads rst heads coin tosses ith coin toss heads occurs times subset subset 
counting sample di ers approach section allowing multiple counts value threshold adapt data distribution versus prespeci ed sample size 
counting samples uniform random samples data set concise sample obtained counting sample considering pair hv ci counting sample turn ipping coin probability heads times reducing count tails 
phillip gibbons yossi matias maintaining counting samples 
algorithm gm maintaining counting sample footprint bound dynamic scenario 
entry threshold initially new data selected sample 
current counting sample consider insertion data item value represented counti pair increment count 
singleton create pair count set 
add probability footprint fors exceeds prespeci ed footprint bound raise threshold subject value higher threshold 
speci cally value counting sample ip biased coin decrementing observed count tails count reaches zero heads 
rst coin toss probability heads subsequent coin toss probability heads values count zero removed counting sample values remain counting sample typically reduced counts 
advantage counting samples concise samples maintain counting samples presence deletions data set 
maintaining concise samples presence deletions di cult fail delete sample point response delete operation sample fail subset data set 
hand delete sample point sample may longer random sample data set 
counting samples di culty 
delete value su ces reverse increment procedure decrementing count converting pair singleton removing singleton appropriate 
concise samples complete exibility algorithm selecting sequence increasing thresholds gm discussed avariety approaches tradeo ways improve constant factors 
theorem 
gm sequence insertions deletions dynamic scenario algorithm maintains counting sample prespeci ed footprint constant amortized expected update time operations 
proof 
show requirement de nition counting sample preserved insert occurs delete occurs threshold raised 
data set counting sample 
insert value increases count ina 
coin ips date heads increment count ins 
coin ips date heads algorithm ips coin appropriate probability 
values untouched requirement preserved 
delete value decreases count algorithm decrements count may drop count 
coin ips occurred date tails rst tails value remains omitted values untouched requirement preserved 
consider raising threshold occurring times equivalent coin ips heads probability came tails 
probabilistic events fail come heads new stricter coin heads probability 
applications random samples ective alternative approach collect uniform samples inserted data deleted data 
synopsis data structures count equivalent ofc coin ips heads probability came tails probabilistic events come tails stricter coin 
followed equivalent ofa coin ip heads probability came heads algorithm ips coin heads probability result equivalent coin ip probability 
coin comes tails subsequent coin ips value heads probability way requirement preserved values 
update time bounds argued proof theorem 
note concise samples counting samples amortized update times counting samples slower update concise samples concise sample perform look counting sample update data set 
hand counting samples guarantees counts stronger exact counting values sample 

application hot list queries 
consider hot list query requesting pairs 
concise sample footprint log approximate hot list reported computing th largest count ck linear time selection algorithm reporting pairs counts max ck scaling counts con dence threshold 
note pairs reported larger fewer may reported 
response time reporting processing time operations 
alternatively trade update time versus query time keeping concise sample sorted counts 
allows reporting time 
counting sample footprint log threshold approximate hot list reported computing th largest count ck reporting pairs counts max ck compensation added reported count serves compensate inserts value data set prior successful coin toss placed counting sample 
analysis gm argued 
conversion counting samples concise samples discussed section seen similar 
analytical bounds experimental results gm quantifying accuracy approximate hot lists reported concise samples counting samples 
example plot data drawn zipf distribution parameter footprint measured memory words 

histograms quantiles histograms approximate data set grouping values buckets subsets approximating distribution values data set summary statistics maintained bucket see 
histograms commonly practice various databases db informix ingres oracle microsoft sql server sybase teradata 
selectivity estimation purposes query optimizer query execution progress approximate query answering 
phillip gibbons yossi matias full histogram concise samples counting samples traditional samples data values zipf parameter footprint frequent values 
comparison algorithms hot list query depicting frequency frequent values reported full histogram concise sample counting sample traditional sample histogram classes extensively database systems equi depth histograms compressed histograms 
equi depth equi height histogram contiguous ranges values grouped buckets data items falling bucket 
endpoints value ranges denoted bucket boundaries quantiles 
compressed histogram highest frequency values stored separately single valued buckets rest partitioned equi depth histogram 
compressed histograms typically provide accurate estimates equi depth histograms 
common problem histograms dynamic maintenance 
data set updated distribution values change histogram supposed re ect distribution change estimates histogram increasingly inaccurate 
section describe gmp algorithms maintaining approximate equi depth compressed histograms synopsis data structures dynamic scenario 
discuss related manku mrl computing approximate quantiles :10.1.1.6.6513
concern histograms construction costs static scenario 
sampling improve construction times see discuss chaudhuri cmn sampling construct approximate equi depth histograms static scenario 
important feature algorithms maintaining approximate histograms backing sample 
backing samples interesting reasons synopsis data structures convert sampling algorithms static scenario algorithms dynamic scenario example hierarchical approach synopsis data structures 

backing samples 
backing sample data set sample kept date presence updates gmp 
sampling estimation techniques sample size needed entire relation scanned extract sample random disk blocks read 
case values disk block may highly correlated obtain truly random sample disk blocks may need read single value block 
contrast backing sample synopsis data structure may reside main memory accessed operations 
typically case databases data item record tuple comprised elds attributes elds desired sampling need retained synopsis 
case samples histograms example eld needed histogram need retained 
backing sample stored disks packed densely disk blocks allowing quickly swapped memory 
indexing structure sample maintained enable fast access sample values certain range 
clearly sample sample points convert algorithm requiring operations sampling algorithm potentially requires operations 
maintaining backing samples 
uniform random sample target size maintained insertions data set vitter reservoir sampling technique vit algorithm proceeds inserting rst items reservoir 
random number new items skipped item replaces randomly selected item reservoir 
random number items skipped forth 
distribution function length random skip depends explicitly number items far chosen point item data set equally reservoir 
speci cally size data set probability item selected backing sample size random skipping employed order reduce constant factors update times compared approach ipping coin new item 
reservoir sampling maintains traditional random sample backing sample alternative concise sample counting sample backing sample maintain discussed section 
discussed section di culties maintaining uniform random samples deletions data set possible solutions counting samples deletion samples 
gmp assumed data item unique id row id database table deletion removes unique item data set 
retained row id sample point precludes concise samples counting samples 
row ids deletions handled removing item sample sample 
deletions decrease size sample target size apparent howto subsequent insertions obtain provably random sample size sample dropped maintained sample size initially prespeci ed upper bound phillip gibbons yossi matias allowed decrease result deletions sample items prespeci ed lower bound sample size dropped data set read disks order re populate random sample data reading random disk blocks 
sampling independent deletions deletion fraction sample expected occur deletion fraction data set 
gmp techniques reducing constant factors update times 
example algorithm maintains random sample independent order updates data set postponed processing deletes insert selected backing sample 
reduced maintenance insert case random skipping employed having deletions intermixed insertions random skipping 
note backing sample xed sample prespeci ed size may desirable augment sample refresh sample appropriate particular application 
backing samples synopsis hierarchy 
gmp backing sample support dynamically maintaining histograms 
scenario considered histogram resided main memory backing sample somewhat larger histogram resided disks 
goal maintain histogram dynamic scenario minimizing accesses updates backing sample order minimize number operations 
backing sample traditional random sample maintained reservoir sampling 
size data set probability item selected backing sample size maintaining backing sample operation expected insertions 
process maintaining backing sample data set grows operation expected average log insertions 
overhead small large small goal design algorithm maintaining histograms minimized number accesses backing sample 

equi depth histograms 
equi depth histogram partitions range possible values buckets falls bucket buckets 
approximate equi depth histogram approximates exact histogram relaxing requirement number data items falling bucket accuracy counts associated buckets 
number items data set count count fb number items falling bucket gmp de ned error metrics evaluating approximate equi depth histograms 
rst metric ed de ned standard deviation bucket sizes mean bucket size normalized respect mean bucket size ed vu fb simplicity ignore issues attribute items buckets items data value equal bucket boundaries issues addressed gmp 
synopsis data structures second error metric count de ned standard deviation bucket counts actual number items bucket normalized respect mean bucket count count vu fb bi count gmp rst low overhead algorithms maintaining highly accurate approximate equi depth histograms 
algorithm relied sample xed size dependent simplest algorithm denoted equi depth simple worked follows 
start phase compute approximate equi depth histogram sorting jsj th item bucket boundary 
set bucket counts number items data set phase 
tunable performance parameter 
larger values allow greater imbalance buckets order fewer phases 
new item inserted data set increment count appropriate bucket 
count exceeds threshold start new phase 
theorem 
gmp 
ln 
consider equi depth simple applied ofn inserts items initially empty data set 
sample size tuples drawn uniformly relation replacement 
cln equi depth simple computes approximate equi depth histogram probability pc ed count proof 
approximate equi depth histogram computed equidepth simple algorithm items inserted data set 
current phase algorithm number items data set phase count ed errors count ed respectively resulting extracting approximate histogram phase phase follows gmp ed count probability atleast phase value inserted bucket bi increments fb bi count 
de nition count value change phase time phase count count probability remains bound ed bi count values fb bi count respectively atthe phase fb claim note max fb 
claim follows fb bi count bi count bi count bi count bi count 
claim phillip gibbons yossi matias note fb 
substituting de nition ed obtain theorem follows 
vu ed ed ed second algorithm gmp reduced number recomputations trying balance buckets local expensive procedure 
algorithm denoted equi depth worked phases 
equidepth simple phase threshold new item inserted data set increment count appropriate bucket 
count exceeds threshold split bucket half 
order maintain xed merge adjacent buckets total count pair buckets 
merge possible recompute approximate equi depth histogram merge buckets sum counts buckets dispose boundary 
split bucket select approximate median serve bucket boundary new buckets selecting median items fall split merge operation illustrated 
note split merge occur 
buckets split merge attribute values insert threshold median 
split merge operation equi depth histogram maintenance number splits number phases bounded follows 
theorem 
gmp consider equi depth buckets performance parameter applied sequence ofn inserts 
total number phases log total number splits log handle deletions data set bn lower threshold bucket counts tunable performance parameter 
item deleted data set decrement count appropriate bucket 
bucket count drops threshold merge bucket synopsis data structures adjacent buckets split bucket largest count long count 
note may newly merged bucket 
exists recompute approximate equi depth histogram merge split operation illustrated 
buckets 
split 
merge attribute values insert threshold median delete threshold 
merge split operation equi depth histogram maintenance related 
manku mrl new algorithms computing approximate quantiles large data sets single pass data limited main memory :10.1.1.6.6513
equi depth histogram desired ranks quantiles regular intervals considered arbitrary prespeci ed ranks 
compared earlier algorithm munro paterson mp deterministic algorithm restricts attention single pass improves constants memory requirements 
speci cally item approximate quantile data set items rank sorted data set ne ne 
manku deterministic algorithm computes approximate quantiles single pass log memory 
note algorithm performs db operations static scenario class queries ranks desired quantiles prespeci ed 
manku analyzed approach rst random sample running deterministic algorithm sample order reduce memory requirements massive data sets 
explicitly consider dynamic maintenance quantiles attempted minimize query time output approximate quantiles output operation occurs pass data 
backing sample residing memory algorithm dynamic scenario operations update time query time 
chaudhuri cmn studied problem sampling needed guarantee approximate equi depth histogram certain accuracy 
error metric evaluate accuracy maximum buckets number data items number buckets fb number items falling argued error metric appropriate ed metric considered providing guarantees accuracy approximate answers range queries 
see improved quality guarantees histograms answer range queries 
chaudhuri provided tighter phillip gibbons yossi matias analysis gmp analyzing accuracy equi depth histograms computed sample 
studied static scenario constructing equi depth histogram including discussion techniques extracting multiple sample points sampled disk block 
backing sample issues longer concern analysis improve guarantees algorithms gmp maintaining equi depth histograms dynamic scenario 

compressed histograms 
equi depth histogram values high frequencies span waste buckets sequence spanned buckets value replaced single bucket single count resulting information smaller footprint 
compressed histogram set singleton buckets equi depth histogram values singleton buckets 
target compressed histogram buckets equi depth buckets singleton buckets requirements hold equi depth bucket tuples total number data items equi depth buckets ii single value spans equi depth bucket set bucket boundaries distinct conversely iii value singleton bucket frequency approximate compressed histogram approximates exact histogram relaxing requirements accuracy counts associated buckets 
gmp rst low overhead algorithm maintaining highly accurate approximate compressed histograms 
equi depth case algorithm relied backing sample approximate compressed histogram computed follows 
initially jsj number items tentatively equi depth buckets 
consider frequent values occurring order maximum frequency 
frequency value divided thenumber equi depth buckets create singleton bucket value count fn jsj decrease creating singleton buckets produce equi depth histogram remaining values approach previous subsection setting bucket counts jsj 
algorithm reduced number recomputations employing local procedure adjusting bucket boundaries 
similar equi depth algorithm algorithm worked phases phase upper threshold triggering equi depth bucket splits lower threshold triggering bucket merges 
steps updating bucket boundaries similar equi depth histogram address additional concerns 
new values added data set may values warrant singleton buckets may belong singleton buckets 

threshold singleton buckets grows thenumber items equi depth buckets 
values singleton buckets smaller may longer belong singleton buckets increases 

concerns number equi depth buckets grows shrinks adjust equi depth buckets accordingly 
synopsis data structures 
likewise number items equi depth buckets grows shrinks dramatically sets items removed added singleton buckets 
ideal maintain items equi depth bucket growing shrinking 
brie informally algorithm gmp addressed concerns follows 
address concern fact large number updates value suitably increase count depth bucket containing cause bucket split 
bucket split doing creates adjacent bucket boundaries value bucket created 
address concern algorithm allowed singleton buckets relatively small counts merged back equi depth buckets 
concerns procedures splitting merging buckets grow shrink number buckets maintaining approximate equidepth buckets histogram recomputed imbalance equi depth buckets controlled thresholds depend tunable performance parameters equi depth algorithm 
equi depth bucket converted singleton bucket vice versa algorithm ensured time bucket factor average number items equi depth bucket additional splits merges required 
average roughly maintained equi depth buckets added subtracted 
requirements bucket split buckets merged involved equi depth algorithm bucket candidate split bucket equi depth bucket count singleton bucket count bounded 
buckets bi bj merge pair adjacent equidepth buckets singleton bucket equi depth bucket singleton value belongs sum counts candidate split bucket candidate merge pair algorithm selected largest smallest combined respectively bucket count 
gmp analytical experimental studies algorithms discussed maintaining equi depth histograms maintaining compressed histograms dynamic scenario 

related results concept related synopsis data structures condensed representations mannila toivonen mt man class structures data collection class patterns representation data structure possible answer queries form times occur approximately correctly ciently looking 
related structures include data cube pruned cached data structures considered machine learning cat ml nets widely computational geometry mul 
mannila toivonen proposed approximation metric structures denoted adequate representation 
approximate data structures provide fast approximate answers proposed studied matias 
example priority queue data structure supports operations insert phillip gibbons yossi matias deletemin approximate priority queue supports operations smaller overheads reporting approximate min response deletemin operations 
data structures considered linear space footprints synopsis data structures 
adapted provide synopsis approximate priority queue footprint determined approximation error 
papers discussing systems techniques providing approximate query answers bene precomputed maintained synopsis data structures 
hellerstein proposed framework approximate answers aggregation queries called online aggregation base data scanned certain order query time approximate answer aggregation query updated scan proceeds 
bayardo miranker bm devised techniques fast rst query processing goal quickly provide tuples query answer base data 
oracle rdb system az provides support fast rst query processing running multiple query plans simultaneously 
liu vl see described query processor provides approximate answers queries form subsets supersets converge exact answer 
query processor uses various class hierarchies iteratively fetch blocks base data relevant answer producing tuples certain answer narrowing possible classes contain answer 
approaches read base data query time incur multiple operations query time 
survey barbara bdf describes state art data reduction techniques reducing massive data sets big picture providing quick approximate answers queries 
data reduction techniques surveyed singular value decomposition wavelets regression log linear models histograms clustering techniques index trees sampling 
technique described brie see details techniques related evaluated qualitatively ectiveness suitability data types distributions maintained insertions deletions data set supports answers progressively improve approximation time 
list data structures considered synopsis data structures extensive 
described works mention 
krishnan proposed studied compact su tree structure estimating selectivity alphanumeric predicate wildcards 
manber man considered concise signatures nd similarities les 
broder studied approximate min wise independent families permutations signatures related context detecting ltering near duplicate documents :10.1.1.121.8215
synopsis data structures includes multi fractals wavelets synopsis data structures fms samples queries join multiple sets gpa 

considers synopsis data structures algorithmic framework relevant massive data sets 
data sets available memory synopsis data structures substantially smaller size data 
synopsis data structures small maintain full characterization base data sets responses provide queries typically approximate ones 
challenges determine synopsis data keep limited space order maximize accuracy con dence approximate responses ciently compute synopsis maintain presence updates data set 
context synopsis data structures presents algorithmic challenges 
problems may easy cient solutions linear space data structures may di cult address limited memory synopsis data structures 
discussed problems frequency moments hot list queries histograms 
di erent classes queries may require di erent synopsis data structures 
classes queries considered need consider classes queries context synopsis data structures analyze ectiveness providing accurate approximate answers queries 
hope motivate algorithms community study problems 
due increasing prevalence massive data sets improvements area nd immediate applications 
acknowledgments 
andy ramesh discussions estimation problems database systems ncr teradata dbs 
collaborated results surveyed 
research described section joint noga alon mario szegedy 
research described section joint poosala 
aqua project discussed brie section joint swarup acharya poosala sridhar ramaswamy torsten suel additional contributions yair bartal muthukrishnan 
collaborators synopsis data structures research mentioned brie section christos faloutsos avi silberschatz je vitter min wang 
torsten suel helpful comments earlier draft 
alon gibbons matias szegedy dynamic probabilistic maintenance self join sizes limited storage manuscript february 
ams alon matias space complexity approximating frequency moments proc :10.1.1.102.5483
th acm symp 
theory computing may full version appear jcss special issue stoc pp 

agrawal srikant fast algorithms mining association rules large databases proc 
th international conf 
large data bases september pp 

az query processing optimization oracle rdb vldb journal 
broder charikar frieze mitzenmacher min wise independent permutations proc :10.1.1.121.8215
th acm symp 
theory computing may pp 

bdf barbara dumouchel faloutsos haas hellerstein ioannidis jagadish johnson ng poosala ross sevcik new jersey data reduction report bulletin technical committee data engineering 
bfs babai frankl simon complexity classes communication complexity theory proc 
th ieee symp 
foundations computer science october pp 

phillip gibbons yossi matias bm bayardo jr miranker processing queries rst answers proc 
th international conf 
information knowledge management november pp 

brin motwani ullman tsur dynamic itemset counting implication rules market basket data proc 
acm sigmod international conf 
management data may pp 

cat catlett choosing attributes ciently machine learning proc 
th international workshop ml july pp 

cmn chaudhuri motwani narasayya random sampling histogram construction proc 
acm sigmod international conf 
management data june pp 

dewitt naughton schneider seshadri practical skew handling parallel joins proc 
th international conf 
large data bases august pp 

faloutsos jagadish sidiropoulos recovering information summary data proc 
rd international conf 
large data bases august pp 

fm flajolet martin probabilistic counting proc 
th ieee symp 
foundations computer science november pp 

fm flajolet martin probabilistic counting algorithms data base applications computer system sciences 
fms faloutsos matias silberschatz modeling skewed distribution multifractals law proc 
rd international conf 
large data bases september pp 

fang shivakumar garcia molina motwani ullman computing iceberg queries ciently proc 
th international conf 
large data bases august pp 

gray chaudhuri bosworth layman pirahesh data cube relational aggregation operator generalizing group cross tabs sub totals data mining knowledge discovery 
ganguly gibbons matias silberschatz bifocal sampling skew resistant join size estimation proc 
acm sigmod international conf 
management data june pp 

gm gibbons matias new sampling summary statistics improving approximate query answers proc 
acm sigmod international conf 
management data june pp 

gmp gibbons matias poosala aqua project white tech 
report bell laboratories murray hill new jersey december 
gmp gibbons matias poosala fast incremental maintenance histograms proc 
rd international conf 
large data bases august pp 

goo surprise indexes values statistical computation simulation 
gpa gibbons poosala acharya bartal matias muthukrishnan ramaswamy suel aqua system techniques approximate query answering tech 
report bell laboratories murray hill new jersey february 
hellerstein haas wang online aggregation proc 
acm sigmod international conf 
management may pp 

haas naughton seshadri stokes sampling estimation number distinct values attribute proc 
st international conf 
large data bases september pp 

ic ioannidis christodoulakis optimal histograms limiting worst case error propagation size join results acm transactions database systems 
ioa ioannidis universality serial histograms proc 
th international conf 
large data bases august pp 

synopsis data structures ip ioannidis poosala balancing histogram optimality practicality query result size estimation proc 
acm sigmod international conf 
management data may pp 

jagadish koudas muthukrishnan poosala sevcik suel optimal histograms quality guarantees proc 
th international conf 
large data bases august pp 

ks schnitger probabilistic communication complexity set intersection proc 
nd structure complexity theory conf june pp 

krishnan vitter iyer estimating alphanumeric selectivity presence wildcards proc 
acm sigmod international conf 
management data june pp 

man manber finding similar les large le system proc 
usenix winter technical conf january pp 

man mannila inductive databases condensed representations data mining proc 
international logic programming symposium pp 

ml moore lee cached su cient statistics cient machine learning large datasets tech 
report cmu ri tr robotics institute carnegie mellon university appear arti cial intelligence research 
mor morris counting large numbers events small registers communications acm 
mp munro paterson selection sorting limited storage theoretical computer science 
mrl manku rajagopalan approximate medians quantiles pass limited memory proc :10.1.1.6.6513
acm sigmod international conf 
management data june pp 

matias young performance evaluation approximate priority queues dimacs fifth implementation challenge priority queues dictionaries point sets organized johnson mcgeoch october 
mt mannila toivonen multiple uses frequent sets condensed representations proc 
nd international conf 
knowledge discovery data mining august pp 

mul mulmuley computational geometry randomized algorithms prentice hall englewood cli nj 
matias vitter 
ni dynamic generation discrete random variates proc 
th acm siam symp 
discrete algorithms january pp 

matias vitter wang wavelet histograms selectivity estimation proc 
acm sigmod international conf 
management data june pp 

matias vitter young approximate data structures applications proc 
th acm siam symp 
discrete algorithms january pp 

olken random sampling databases ph thesis computer science berkeley april 
poosala ioannidis haas shekita improved histograms selectivity estimation range predicates proc 
acm sigmod international conf 
management data june pp 

pre pregibon mega monitoring developing telecommunications signatures october invited talk dimacs workshop massive data sets telecommunications 
raz razborov distributional complexity disjointness theoretical computer science 
sks silberschatz korth sudarshan database system concepts third ed mcgraw hill new york 
tpc tpc committee 
transaction processing council tpc www tpc org phillip gibbons yossi matias vit vitter random sampling reservoir acm transactions mathematical software 
vit vitter external memory algorithms proc 
th acm symp 
principles database systems june pp 

vl liu approximate query processor produces monotonically improving approximate answers 
knowledge data engineering 
vs vitter shriver algorithms parallel memory level memories algorithmica 
yao yao lower bounds probabilistic arguments proc 
th ieee symp 
foundations computer science november pp 

information sciences research center bell laboratories room lucent technologies mountain avenue murray hill new jersey mail address gibbons research bell labs com url www bell labs com department computer science tel aviv university tel aviv israel information sciences research center bell laboratories mail address matias math tau ac il url www math tau ac il matias 
