nonparametric belief propagation erik sudderth alexander william freeman alan willsky department electrical engineering computer science massachusetts institute technology cambridge ma mit edu mit edu ai mit edu willsky mit edu mit laboratory information decision systems technical report october applications graphical models arising fields computer vision hidden variables interest naturally specified continuous non gaussian distributions 
due limitations existing inference algorithms necessary form coarse discrete approximations models 
develop nonparametric belief propagation nbp algorithm uses stochastic methods propagate kernel approximations true continuous messages 
nbp message update efficient sampling procedure accomodate extremely broad class potential functions allowing easy adaptation new application areas 
validate method comparisons continuous bp gaussian networks application stereo vision problem 
graphical models provide efficient powerful framework modeling probabilistic relationships 
existing applications graphical models employ hidden variables take values discrete finite set 
fields error correcting codes discrete models entirely appropriate 
areas computer vision typically arise quantizations continuous variables 
costs invariably limit accuracy quantizations leading biases artifacts resulting estimates 
popularity discrete graphical models traced existence efficient algorithms solving learning inference problems 
discrete models difficulties associated inference purely computational involve finding ways efficiently decompose approximate sums exponentially terms 
exact inference general discrete graphs np hard approximate inference algorithms loopy belief propagation bp shown produce results wide range interesting models :10.1.1.116.8559
graphical models containing continuous hidden variables additional representational challenge 
conditional distributions sought inference algorithms finite vectors continuous functions 
unfortunately potential functions specifying graphical model simple parametric forms integral equations propagate information graph rarely tractable analytic solutions 
notable exception occurs variables jointly gaussian distributions may parameterized mean covariance 
real world phenomena exhibit significant outliers statistical features far gaussian 
conditional densities arising non gaussian graphical models lack tractable parametric forms natural consider nonparametric approximations 
temporal inference problems defined markov chains variety density representations explored 
gaussian sum filters local linearizations deterministically propagate gaussian mixture models 
contrast particle filters monte carlo methods stochastically update set weighted point samples 
stability robustness particle filters improved regularization methods chapter smoothing kernels explicitly represent uncertainty associated point sample 
general graphs junction tree representation develop structured approximate inference techniques 
wide variety algorithms specified combining approximate clique variable representation local methods updating approximations :10.1.1.141.1195
example distributions large cliques discrete variables approximated set weighted point samples related neighboring nodes standard message passing recursions 
koller propose sophisticated framework current clique potential estimate guide message computations allowing approximations gradually refined successive iterations 
sample algorithm provide limited networks containing mixtures discrete gaussian variables 
addition graphs nearest neighbor grids size junction tree largest cliques grows exponentially problem size requiring estimation extremely high dimensional distributions 
nonparametric belief propagation nbp algorithm develop extends earlier key ways 
graphs cycles form junction tree iterate local message updates convergence loopy bp 
advantage greatly reducing dimensionality spaces infer distributions 
addition provide message update algorithm specifically adapted graphs containing continuous non gaussian potentials 
message represented kernel nonparametric density estimate message products efficient local gibbs sampling algorithm 
validate nbp algorithm small gaussian network stereo vision results demonstrating effectiveness 
undirected graphical models undirected graph defined set nodes corresponding set edges neighborhood node defined gamma set nodes directly connected graphical models associate node unobserved hidden random variable xs noisy local observation ys 
denote sets hidden observed variables respectively 
simplify presentation consider models pairwise potential functions factorizes xs xt xs ys nonparametric updates may directly extended models higher order potential functions 
focus calculation conditional marginal distributions xs nodes distributions may calculate best estimates hidden variables xs relative wide range criteria 
example conditional mean xs provides bayes squares estimate 
generally conditional distributions useful provide information degree uncertainty estimate hidden node 
belief propagation graphs acyclic tree structured desired conditional distributions xs directly calculated local message passing algorithm known belief propagation bp 
iteration bp algorithm node calculates message xs sent neighboring node gamma xs ff xt xs xt xt yt gamma ns mn gamma ut xt dxt ff denotes arbitrary proportionality constant 
iteration node produce approximation pn xs marginal distributions xs combining incoming messages local observation potential pn xs ffs xs ys gamma xs tree structured graphs approximate marginals beliefs pn xs converge true marginals xs messages node propagated node graph 
conceptually steady state bp message mts xs sufficient statistic observations subgraph node separates node iteration bp algorithm involves local message updates applied graphs cycles 
graphs statistical dependencies bp messages properly accounted sequence beliefs pn xs converge true marginal distributions 
applications resulting loopy bp algorithm exhibits excellent empirical performance 
theoretical studies provided insight approximations loopy bp partially justifying application graphs cycles 
nonparametric representations exact evaluation bp update equation involves integration discussed analytically tractable continuous hidden variables 
interesting alternative represent resulting message mts xs kernel density estimate 
lambda denote value gaussian density mean covariance lambda point may approximate mts xs mixture gaussian kernels mts xs mx gamma xs xis lambda delta wis weight associated ith kernel mean xis lambda bandwidth smoothing parameter 
choices kernel functions possible restrict attention mixtures diagonal covariance gaussians 
section describe stochastic methods determining kernel centers xis associated weights wis resulting nonparametric representations meaningful messages mts xs finitely integrable guarantee sufficient assume potentials satisfy constraints xs xs xt xs xs ys assumptions simple induction argument show messages normalizable 
heuristically equation requires potentials informative fixing value variable constrains locations 
application domains trivially achieved assuming hidden variables take values large bounded range 
nonparametric message updates conceptually bp update equation naturally decomposes stages 
message product xt yt gamma ut xt combines information neighboring nodes local evidence yt produce likelihood function xt 
second likelihood function combined compatibility potential xs xt integrated produce likelihoods xs 
nonparametric belief propagation nbp algorithm stochastically approximates stages producing consistent nonparametric representations messages mts xs 
approximate marginals xs may determined messages applying section stochastic product algorithm equation 
message products moment assume local observation potentials xt yt represented weighted gaussian mixtures potentials arise naturally learning approaches model identification 
product gaussian densities gaussian mean covariance dy lambda gamma lambda delta lambda gamma dx lambda gamma lambda gamma dx lambda gamma bp update operation multiplies gaussian mixtures containing components produce gaussian mixture components 
weight associated product mixture component gamma lambda delta qd win lambda gamma lambda delta weights associated input gaussians 
integration gaussian mixtures straightforward principle bp message updates performed exactly repeated equations 
practice exponential growth number mixture components forces approximations 
input mixtures gaussian nbp algorithm approximates component product mixture drawing independent samples 
direct sampling product achieved explicitly calculating product component weights require operations 
complexity probabilistically bp messages likelihood functions ts xs xs densities necessarily integrable xs independent 

pi oe gibbs sampler product gaussian mixtures kernels 
new indices sampled weights arrows determined fixed components solid 
final labels identify components product density dashed 
associated sampling combinatorial product component defined labels li identifies kernel ith input mixture 
joint distribution labels complex conditional distribution individual label lj simple 
particular assuming fixed values equation sample conditional distribution lj operations 
mixture label conditional distributions tractable may gibbs sampler draw asymptotically unbiased samples product distribution 
iteration labels gamma input mixtures fixed new value jth label chosen equation 
iteration newly chosen lj fixed label updated see 
final iteration mean covariance selected mixture component equation sample point drawn 
assuming number iterations gibbs sampler independent may draw samples product mixture dm operations 
formal verification gibbs sampler convergence difficult experiments observed performance far fewer computations required direct sampling 
note nbp algorithm gibbs sampler involves hidden variables contrast large state spaces classic simulated annealing 
applications observation potentials xt yt naturally specified analytic functions 
previously proposed gibbs sampler may easily adapted case importance sampling 
iteration weights produced equation rescaled yt observation likelihood kernel center 
final sample xis assigned weight wis gamma xis yt delta yt account variations analytic potential kernel support 
procedure effective xt yt varies slowly relative typical kernel bandwidth 
message propagation second stage nbp algorithm information contained incoming message product propagated stochastically approximating belief update integral 
perform stochastic integration pairwise potential xs xt decomposed separate marginal influence xt conditional relationship defines xs xt 
marginal influence function xt determined relative weight assigned xs values xt xt xs xs xt nbp algorithm accounts marginal influence xs xt incorporating xt gibbs sampler 
xs xt gaussian mixture extraction xt trivial 
alternately xt evaluated approximated pointwise analytic pairwise potentials may dealt importance sampling 
common case pairwise potentials depend difference arguments gamma xt constant neglected 
complete stochastic integration particle xjt produced gibbs sampler propagated node sampling xjs ffs xs xjt 
note assumptions section ensure xs xjt normalizable xjt method sampling step performed depend specific functional form xs xt may involve importance sampling mcmc techniques 
having produced set independent samples desired output message mts xs nbp choose kernel bandwidth complete nonparametric density estimate 
ways choice results leave likelihood cross validation 
gaussian graphical models gaussian graphical models provide continuous distributions bp algorithm may implemented exactly 
reason gaussian models may test accuracy nonparametric approximations nbp 
note hope nbp outperform algorithms gaussian bp designed take advantage linear structure underlying gaussian problems 
goal verify nbp performance situation exact comparisons possible 
tested nbp algorithm gaussian models range graphical structures including chains trees grids 
similar results observed cases data single typical theta nearest neighbor grid randomly selected inhomogeneous potential functions 
node gaussian bp converges steady state estimate marginal mean variance oe iterations 
evaluate nbp performed iterations nbp message updates different particle set sizes 
marginal mean variance oe estimates implied final nbp density estimates 
tested particle set size nbp comparison repeated times 
data nbp trial computed error mean variance estimates normalized node behaved unit variance gaussian gamma oes oe oe gamma oe oe shows mean variance error statistics nodes trials different particle set sizes nbp algorithm provides unbiased estimates conditional mean overly large variance estimates 
number particles number particles mean variance oe nbp performance theta grid gaussian potentials observations 
plots show mean solid line standard deviation dashed line normalized error measures equation function particle set size bias decreases particles due smoothing inherent kernel density estimates 
expected samples drawn gaussian distributions standard deviation error measures falls gamma 
stereo vision stereo vision algorithms infer point correspondences horizontally aligned image pairs dimensional scene :10.1.1.127.3572
distance disparity matching pixels inversely proportional corresponding object distance camera 
estimation accurate dense disparity maps complicated textureless regions effects occlusions 
reasons prior knowledge disparity variations plays critical role stereo computation 
prior assumptions underlying stereo vision algorithms represented grid structured graphical models pairwise potentials correlate neighboring disparity values observation potentials measure similarities local image features :10.1.1.127.3572
types potential functions suggested encoding different assumptions underlying disparity worlds world pairwise potentials assume disparity differences gaussian 
similarly observation potentials assign gaussian distributions image feature differences 
note resulting disparity likelihoods gaussian 
world ii gaussian potentials world augmented binary outlier processes model occlusions depth discontinuities 
gaussians contaminated uniform noise suggested 
similar model discrete bp algorithm 
world iii observation potentials world ii hidden variable node augmented model horizontal vertical disparity gradients 
uniform contaminated gaussian potentials model differences adjacent gradients 
goal state art stereo results explore characteristics nbp algorithm unique features world 
worlds ii compares nbp algorithm performance discretized bp theta patches containing interesting disparity features 
patches extracted standard venus sawtooth stereo test images 
discrete bp venus image true disparity sawtooth image true disparity nbp discrete bp nbp discrete bp nbp discrete bp stereo results venus sawtooth patches 
top row image true disparities 
second row world third row world ii 
quantized dimensional disparity space levels nbp particles sampled mcmc iterations 
limited range possible disparities discrete bp results approximate true continuous bp results extremely accurately 
see nbp accurately captures qualitative features worlds matching discrete bp results extremely 
world iii objects venus sawtooth images slopes nearly parallel image plane 
reason gradient information world iii sophisticated prior significantly modify world ii results 
constructed synthetic stereo problem shown consisting single constant slope scan line 
ends scan line contain observations central section contains disparity information caused textureless image region 
figures show world ii bias horizontal surfaces inappropriate data set 
shows performance attained discrete bp slopes chosen exactly match true surface 
practical situations possible discretize dimensional world iii state space finely achieve 
shows heavy quantization artifacts produced discrete bp theta theta points 
contrast points nbp produces high resolution density estimates 
discussion developed nonparametric sampling variant belief propagation algorithm graphical models continuous non gaussian random variables 
stereo vision results suggest nbp achieves performance comparable stereo results synthetic scan line 
plot columnwise representation nodes marginal distributions 
observation likelihoods 
world ii discrete bp 
world ii nbp 
world iii ideal 
world iii discrete bp 
world iii nbp 
discretization low dimensional hidden variables may offer significant advantages higher dimensional spaces 
application sophisticated monte carlo density estimation techniques improve statistical accuracy computational efficiency message updates 
hope nonparametric approach allow richer realistic models designed applications continuous variables naturally arise 
acknowledgments research supported part onr afosr muri funded aro daad 
supported fellowship 
yedidia freeman weiss 
understanding belief propagation generalizations 
ijcai august 
freeman pasztor 
learning low level vision 
ijcv 
geman geman 
stochastic relaxation gibbs distributions bayesian restoration images 
ieee trans 
pami november 
belhumeur 
bayesian approach binocular stereopsis 
ijcv 
sun shum zheng 
stereo matching belief propagation 
eccv pages 
pearl 
probabilistic reasoning intelligent systems 
morgan kaufman san mateo 
cooper :10.1.1.116.8559
computational complexity probabilistic inference bayesian belief networks 
artificial intelligence 
wainwright jaakkola willsky 
tree reparameterization approximate inference loopy graphs 
nips 
mit press 
weiss freeman 
correctness belief propagation gaussian graphical models arbitrary topology 
neural comp 

stochastic processes filtering theory 
academic press new york 
sorenson 
nonlinear bayesian estimation gaussian sum approximations 
ieee trans 
ac august 
doucet de freitas gordon editors 
sequential monte carlo methods practice 
springer verlag new york 
parzen 
estimation probability density function mode 
ann 
math stats 
silverman 
density estimation statistics data analysis 
chapman hall london 
lauritzen 
graphical models 
oxford university press oxford 
dawid kjaerulff lauritzen 
hybrid propagation junction trees 
advances intelligent computing pages 
koller lerner 
general algorithm approximate inference application hybrid bayes nets 
uai pages 
kjaerulff 
hugs combining exact inference gibbs sampling junction trees 
uai pages 
hern andez moral 
mixing exact importance sampling propagation algorithms dependence graphs 
int 
intelligent systems 
scharstein szeliski 
taxonomy evaluation dense frame stereo correspondence algorithms 
ijcv 
black rangarajan 
unification line processes outlier rejection robust statistics applications early vision 
ijcv 
