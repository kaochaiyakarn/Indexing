apoidea decentralized peer peer architecture crawling world wide web singh ling liu todd miller college computing georgia institute technology atlanta ga cc gatech edu 
describes decentralized peer peer model building web crawler 
current systems centralized client server model crawl done tightly coupled machines distribution crawling jobs collection crawled results managed centralized system centralized url repository 
centralized solutions known problems link congestion single point failure expensive administration 
requires horizontal vertical scalability solutions manage network file systems nfs load balancing dns requests 
architecture completely distributed decentralized peer peer crawler called apoidea self managing uses geographical proximity web resources peers better faster crawl 
distributed hash table dht protocols perform critical url duplicate content duplicate tests 
search engine technology played important role growth www 
ability reach desired content amidst huge amounts data businesses efficient productive 
important component technology process crawling 
refers process traversing www hyperlinks storing downloaded pages 
currently available web crawling systems envisioned system run single organization :10.1.1.109.4049
architecture system central server model determines urls crawl web pages store dump case similar page available database crawling mirror site 
system requires organizations employ extremely machines various experienced administrators manage process 
example mercator machine gb ram gb local disk 
google utilizes special hardware configuration centralized management tens thousands machines crawl web :10.1.1.109.4049
higher costs important problem arises congestion link joining crawling processes central server data kept 
emergence successful applications gnutella kazaa freenet peer peer technology received significant visibility past years :10.1.1.140.3129
peer peer systems massively distributed computing systems peers nodes communicate directly distribute tasks exchange information accomplish tasks 
research partially supported nsf cns ccr nsf itr doe scidac darpa research ibm faculty award ibm sur hp equipment llnl 
opinions findings recommendations expressed project material authors necessarily reflect views sponsors 
computing number attractive properties 
requires additional hardware computing resources grow clients 
second self organizing incurring additional administrative costs due scaling 
third fault tolerant design 
systems differ terms exchange data distribute tasks locate information lookup service overlay network 
peer peer approach crawl web 
apoidea fully decentralized system central authority family bees derives name 
bees peers independently commanding authority decide crawling data kept distributed network 
task crawling divided peers dht distributed lookup information exchange protocols exchange vital information peers 
bloom filters store list urls crawled peer :10.1.1.20.2080
memory requirements peer reasonable easily available normal pcs today 
option distributing task crawling internet provides opportunity exploit geographical proximity crawlers domains crawling 
initial results indicate feature significantly speed crawling process 
having decentralized system possible crawl web minimal supervision studied replication mechanisms design inherently fault tolerant 
please note focus crawling www 
mechanisms indexing providing complete search engine 
mechanisms studied top design architecture 
main advantages apoidea follows efficient apoidea utilizes peers geographically closer web resources crawl 
lead significant speed improvements greater efficiency 
serves automatic load balancing mechanism dns servers local proxy servers 
fully decentralized apoidea completely decentralized 
prevents link congestion occur communications central server 
peers perform jobs independently exchange information need 
low cost apoidea self managing system 
means need manual administration 
system automatically handles dynamics systems entry exit peers 
resource requirements peer reasonable 
greater reach increase number peers system potentially reach greater part www opposed conventional systems 
scalability apoidea constructed aim scalability mind envision system efficiently huge www growth rates 
fault tolerance apoidea takes consideration entry exit peers system design fault tolerant ones available today single point failure 
rest organized follows section describes basics crawler design look dht distributed lookup protocols bloom filters 
section describes complete architecture working details apoidea 
section initial results observations performance system 
section discuss apoidea build world wide web search engine 
section discuss related 
section contains directions research 
apoidea family bees queen bee system design basics section briefly describe general design requirements web crawler 
talk done area dht systems bloom filters intend architecture 
section concretely explain system put place workflow details 
design considerations section describe basic design components crawler specifically important design principles taken consideration distributed decentralized crawlers 
typical web crawler consists primary components downloader component reads list urls requests get web pages 
url extractor component responsible extracting urls downloaded web page 
puts url crawled list 
duplicate tester component responsible checking url content duplicates 
crawl maintains data structures crawl jobs list urls crawled 
seen urls list urls crawled 
seen content list fingerprints hash signature checksum pages crawled 
downloader picks urls crawl jobs data structure downloads page 
checked content duplication 
turns duplicate thrown away 
new page fingerprint added seen content data structure 
url extractor component processes page extract urls normalizes 
urls checked possible duplicates new added crawl jobs list 
added seen urls list 
duplicate tests proved important 
encountering duplicate url extremely common due popularity content website having common headers web pages 
duplicate pages occur due mirror sites symbolic links web server example home html index html point page web servers 
mercator experiments pages duplicates 
distributed crawler decisions selecting crawler gets crawl particular url 
simple just picking urls crawl jobs sequentially giving crawlers round robin fashion complicated policy 
example google uses various machines round robin manner internet archive crawler distributes domains crawled :10.1.1.109.4049
designing decentralized crawler new challenges 

division labor issue important decentralized crawler centralized counterpart 
distributed crawlers crawl distinct portions web times 
potential optimizations geographical distribution crawlers globe 
section describe exploit global presence peers apoidea system 

duplicate checking assuming peer crawling distinct part web encounter duplicate urls duplicate content 
single repository seen urls seen content data structures peers communicate decide newness url page 
extremely important keep communication costs minimum potentially thousands peers globe crawling web fast speed 
dht protocols achieves lookups log time total number peers 
important notice components entirely disconnected 
having division labor scheme potentially save duplicate decision making part 
crawling web peer peer network past research systems targeted improving performance search 
led emergence class systems include chord pastry tapestry :10.1.1.111.1818:10.1.1.140.3129:10.1.1.105.3673:10.1.1.105.3673:10.1.1.105.3673:10.1.1.28.5987
techniques fundamentally distributed hash tables slightly differ algorithmic implementation details 
store mapping particular key value distributed manner network storing single location conventional hash table 
achieved maintaining small routing table node 
key techniques guarantee location value bounded number hops network 
achieve peer identifier responsible certain set keys 
assignment typically done normalizing key peer identifier common space hashing hash function having policies numerical closeness contiguous regions node identifiers identify regions peer responsible 
example context file sharing application key file name identifier ip address peer 
available peers ip addresses hashed hash function store small routing table example chord routing table entries bit hash function locate peers 
locate particular file filename hashed hash function depending policy peer responsible file obtained 
operation locating appropriate peer called lookup 
typical dht system provide guarantees lookup operation key guaranteed succeed data corresponding key system 
lookup operation guaranteed terminate small finite number hops 
key identifier space uniformly statistically divided currently active peers 
system capable handling dynamic peer joins leaves 
apoidea uses protocol similar chord simplicity implementation :10.1.1.105.3673:10.1.1.105.3673
apoidea kinds keys urls page content 
term peer responsible particular url means url crawled particular peer page content means peer information page content downloaded probably crawling mirror site 
url lookup done follows 
hash domain name url 
protocol performs lookup value returns ip address peer responsible url 
note choice result single domain crawled peer system 
designed manner peer connection alive parameter protocol single tcp ip socket downloading multiple pages particular domain 
significantly crawl save costs establishing tcp ip connection domain url crawled case just hashed complete url done lookup 
page content key intend check duplicate downloaded pages 
page content hashed lookup initiated get peer responsible 
note page case downloaded peer lookup url domain name 
peer responsible page content maintains information store entire page 
nature protocol handle dynamic peer join leave operations 
new peer joins system assigned responsibility certain portion key space 
context web crawler new peer assigned domains hash 
peers currently responsible domains required send information list urls crawled domains newly entering node 
similar technique redistribute page signature information 
peer wants leave network sends url page content information currently assigned nodes 
peer fail able send information peers information keys held peer temporarily lost 
problem circumvented replicating information 
domain mapped multiple keys kr turn handled distinct peers 
peer responsible key primary replica responsible crawling domain peers responsible secondary replicas keys kr simply store information urls domain crawled 
peer responsible primary replica crashes information held recovered secondary replicas 
sake simplicity consider having key domain rest 
managing duplicate urls bloom filters bloom filters provide elegant technique representing set key elements kn support membership queries 
main idea allocate vector bits initially set choose independent hash functions hl range 
key bit positions hl set 
note particular bit location may set key 
membership check performed follows 
key check bits positions hl vector zero certainly assume key wrong certain probability 
called false positive salient feature bloom filters fact trade space probability false positive 
shown ln independent hash functions reduce probability false positive achieve probability false positive low independent hash functions :10.1.1.20.2080
context web crawler bloom filter maintain information url having crawled 
urls crawled form set test url newness hash value check membership indicate url crawled 
assuming www contains pages setting size bloom filter bits gb 
note practical implementation bloom filters maintain entire filter main memory 
infeasible hold entire bloom filter required scale www single machine 
design divide urls www cooperating peers 
assuming cooperating peers different geographical locations peers handle web pages turn require bloom filter size mb 
today technology peer easily afford maintain mb bloom filter main memory 
system architecture apoidea consists number peers www 
explained previous section peer responsible certain portion web 
section describe perform division labor duplicate checking done apoidea 
briefly describe peer peer architecture apoidea especially membership formation network formation peers single peer architecture apoidea including peer participates apoidea web crawling performs assigned crawling jobs 
division labor discussed section dht system distributing www space peers network 
peer responsible urls domain name hashes 
apoidea contiguous region policy clearer example shown 
assume peers system peer ips hashed bit space located points modulo ring 
various domain names hashed space occupy slots 
peer responsible space peer 
peer responsible space peer peer rest 
peer responsible domain www cc gatech edu 
peer peer crawling urls domain 
peer gets urls belonging domain batches periodically send peer random assignment urls peers exploit geographical proximity information 
order ensure domain crawled peer closer domain relax tight mapping domain peer follows 
peer maintains list leaf peers close peer identifier space 
context chord protocol peer maintains pointers successor peers predecessor peers identifier ring 
peer assigned domain picks leaf peer closest domain forwards batch urls crawling 
duplicate checking assume peer responsible domain peer leaf peer peer crawling domain peer encounters url domain sends url peer responsible domain peer batches collection urls domain forwards peer recall peer maintains bloom filter indicates url crawled 
peer checks url crawled 
url crawled simply dropped added crawl jobs list 
order check page duplication hash page contents identifier space distribute various peers network 
shows example 
page content www cc gatech edu research hashed ring peer responsible 
note page crawled peer domain www cc gatech edu hashed 
peer encounters page needs check page duplicate 
looks hash page content finds peer responsible 
peer batch requests periodically query peer newness 
peer batch replies back 
modifies local seen content list note new pages downloaded peer note delay getting information page performance effecting significant delay downloading particular page processing page 
downloads typically happen faster rate processing result processing significant lag download 
www unsw edu au batch www cc gatech edu batch www iitb ac www cc gatech edu research www cc gatech edu people www cc gatech edu admissions fig 

division labor apoidea peer peer network formation page duplication www unsw edu au www cc gatech edu research fig 

content duplicate checking www cc gatech edu www iitb ac peers apoidea system user machines internet execute web crawling applications 
peers act clients servers terms roles performing crawling jobs 
url crawling job posted peer system 
scheduling node system peer global knowledge topology system 
main mechanisms apoidea peer peer system 

peer membership mechanism overlay network membership 
peer membership allows peers communicate directly distribute tasks exchange information 
new node join apoidea system contacting existing peer entry node apoidea network 
bootstrapping methods determine entry node 
may assume apoidea service associated dns domain name 
takes care resolving mapping apoidea domain name ip address apoidea bootstrapping nodes 
bootstrapping node maintains short list apoidea nodes currently alive system 
join apoidea web crawling new node looks apoidea domain name dns obtain bootstrapping node ip address 
bootstrapping node randomly chooses entry nodes short list nodes supplies ip addresses 
contacting entry node apoidea new node integrated system apoidea protocol initialization procedures 
revisit process entry exit nodes section 
protocol second mechanism apoidea protocol including partitioning web crawling jobs lookup algorithm 
apoidea peer participates process crawling web peer post new url crawled 
new url encountered peer peer determines peer responsible crawling url 
achieved lookup operation provided apoidea protocol 
apoidea protocol ways similar chord :10.1.1.105.3673:10.1.1.105.3673
specifies important types peer coordination find peers best crawl url new nodes join system apoidea manages failures departures existing nodes 
unique feature protocol ability provide fast distributed computation hash function mapping url crawl jobs nodes responsible 

crawling third mechanism processing url crawling requests 
url crawling job assigned peer identifier matching domain url 
identifier matching criteria urls crawled assigned peers cleanly migrated peers presence failure peer entrance departure 
user point view peer network equipped apoidea middleware layer software system 
lower layer apoidea protocol layer responsible peer peer communication 
upper layer web crawling subsystem responsible crawling assigned urls resolving seen urls avoid duplicate crawling processing crawled pages 
application specific crawling requirements incorporated layer 
subsequent sections discuss apoidea middleware runs apoidea peer describe design crawling subsystem detail 
report initial experimental results obtained evaluation prototype apoidea web crawlers 
single peer architecture architecture peer composed main units 
storage unit contains data structures peer maintain 

workers unit consists modules fetching pages www processing extract urls 

interface unit forms interface peer handling communications apoidea network 
complete architecture shown 
storage unit contains data structures maintained peer 
crawl jobs list urls crawled peer 
batch urls peer domain wise maintain hashtable domains keys urls belonging domain corresponding values 
storage workers crawl jobs processing jobs seen urls seen content routing table batch downloader extractor url page duplication url receive url receive query send reply send url page duplication send query fig 

system architecture single apoidea peer world wide web processing jobs list downloaded pages processed 
list just vector pointers pages 
certain number pages rest stored disk 
seen urls list urls crawled 
prevent url crawled 
maintain bloom filter 
seen content list page signatures hash seen peer network 
important point noted data structure peer contains page signatures peer responsible pages downloads 
bloom filter data structure 
routing table route lookup queries right peer contains information small number nodes system :10.1.1.105.3673:10.1.1.105.3673
biggest complexity component choice data structures web crawler extensive resources especially memory 
crawl jobs processing jobs lists usually easily maintainable number downloaders extractors working 
routing table small data structure 
main requirement efficiently manage seen urls seen content data structures 
data structures increase size passage time 
order maintain data structures bloom filters 
assume www web pages 
experiments observed domain urls 
number domains www 
assuming peers participate crawling operation peer crawl domains 
domains maintain bloom filter indicates particular url domain crawled 
noting fact bloom filters need hold urls chose size bloom filter kb probability false positive remains formula section kb bits urls 
peer maintains bloom filter size mb kb domain domains peer 
peers maintain association domain name bloom filter hash table 
observing fact average length url hash table needs hold entries domains size hash table kb 
maintaining domain interface receive reply bloom filters essential handling peer joins leaves 
recall peer assigned domains 
newly joined node construct bloom filter domains assigned simply copying relevant bloom filter peer previously responsible domain 
note bloom filter specifically avoided storing seen urls memory 
fact mercator uses approach storing urls domain currently crawling main memory 
mercator reports hit rate warrants costly disk operations 
costly disk operations suitable peer entirely dedicated crawling 
scenario minimize operations significantly affect normal functioning peer 
peer crawl just single domain time inefficient having problem domain say shut maintenance significantly reduce throughput system 
workers unit mainly consists sets threads downloader extractor 

downloader downloader threads pick domain crawl jobs list queries neighbors maintained neighbor list find peer crawl domain fastest 
done short message exchanges peer measuring round trip time domain 
peer fastest peer just pass urls fastest neighbor 
usually happens geographical proximity neighbor domain crawled 
initial experiments indicate important factor total speed crawling www 
robot exclusion protocol taken consideration crawling particular website site wish crawled crawled 
mentioned domain assigned single peer 
significantly speeds crawl peer uses connection alive feature protocol single open socket download number pages 
page downloaded stored temporary data structure checked page duplication 
new page inserted processing jobs list dropped 

extractor extractor threads pick pages processing jobs list extracts urls particular page 
urls extracted regular expression library java 
extraction urls normalized changing relative urls exact urls removing extraneous extensions javascript handled crawler 
urls kept temporary data structure periodically picked batched submitted appropriate peers peers responsible 
temporary data structure hash table urls belonging domain bucket 
interface crawling peers communicate scenarios 
submission urls peer may extract urls page crawled peer responsible domain urls hashed belonging peer peer batches urls periodically send urls peer peer simply ignores urls crawled adds crawl jobs list 

checking page duplication peer downloads page check similar page available network 
lookup page hash contacts peer responsible hash value say peer case peer page peer drop page stores 
peer notes page available network 

protocol requirements communications required maintain stabilize network 
includes periodic message exchange neighboring peers identify new peer joined network peer exited network 
low cost udp messages exchange information set batching threads periodically pick batches information various peers send data 
entry exit peers section illustrate apoidea handles dynamics associated system entry exit peers 
entry system bootstrap servers times active 
servers act entry points network 
peer wants join apoidea network contact bootstrap server point node active network 
depending ip address entering peer node help get routing table required participate network 
network stabilizes recognizing entry new node 
procedure inexpensive easily accomplished known methods provided chord :10.1.1.105.3673:10.1.1.105.3673
stabilization process node holds list seen urls new peer responsible send data new peer 
data just short bloom filter domains hash new peer 
similar transfer take place page content 
peer joined network periodically get urls domains responsible start crawling process 
exit peer needs leave system seen urls seen content transferred peer responsible 
new responsible peer neighbor peer ring space 
neighbor information stored peer part routing table 
exit just send seen urls seen content information neighbor 
case peer failure network stabilization process corrects routing tables peers network 
lost information regarding seen urls seen content 
prevent mentioned section maintain multiple replicas information 
requirements memory due bloom filters possible peers act primary secondary replicas various keys 
results observations conducted experiments geographical proximity scalability apoidea 
primitive java implementation crawler crawled similar domains terms networking infrastructure georgia institute technology edu domains starting form www cmu edu usa ac jp domains starting www tokyo ac jp japan edu au domains starting www unsw edu au australia ac domains starting www iitb ac india 
results experiment shown 
clearly shows edu domains geographically closest crawled twice speed geographically farther domains edu au 
fact networking infrastructure domains comparable domain www unsw edu au crawled twice speed geographically closer location 
turn huge impact total speed web crawl 
performed experiments scalability 
experiment measured resource requirements cpu memory single peer 
ran apoidea intel pentium mhz machine scenarios high load running ns simulator low load 
note intentionally chose heavily loaded machine restrict resource utilization apoidea 
shows performance apoidea terms number urls crawled second varying number java threads 
observe machine heavily loaded apoidea performance increase number threads threads jvm overhead handling threads time threads stay sleep state 
note high load apoidea cpu utilization restricted memory consumption limited mb 
lightly loaded conditions maximum rate achieved peer employs threads 
note lightly loaded conditions cpu utilization memory consumption mb 
number urls crawled edu au domain ac jp domain ac domain edu domain time min fig 

impact geographical proximity urls crawled low load high load number threads fig 

apoidea performance peer urls crawled sec peer scalability low load scalability high load number peers fig 

scalability apoidea number peers second experiment apoidea scalability increased number peers participating system 
experimented maximum machines local area network georgia institute technology 
note peers heavily loaded lightly loaded described previous experiment 
peer threads heavy load threads light load obtain best performance 
shows performance apoidea varying number peers 
note peer running apoidea cost interaction peers 
adding peer lowers apoidea performance 
note subsequent addition peers performance flattens small peer population order irrespective load peers 
near propose study performance apoidea presence large peer population possibly multiple geographical locations 
require volunteers world run apoidea 
second measure crawl refreshing speed having crawled entire www want measure speed www crawled updates second subsequent crawls 
third want perform experiments reliability system see impact dynamic peer failures replication network 
application world wide web search engine papers proposed architectures build scalable distributed indexes keyword search networks 
designs layered distributed hash table systems chord :10.1.1.105.3673:10.1.1.105.3673
inspired ranking system google exploits statistics popular documents hosts building distributed inverted indexes answers query computing join indexes matching query keywords :10.1.1.28.5987
hand builds distributed inverted index keyed combinations multiple keywords 
avoids overheads computing joins saving network bandwidth cost storage space note total storage space required grows exponentially set size keyword combination build inverted index 
apoidea conjunction distributed indexing architectures implement search engine world wide web 
envision distributed search engine apoidea distributed indexing architectures exist node network 
apoidea crawls world wide web extracts relevant keywords web pages 
indexing architecture builds distributed inverted index evaluates keyword search queries 
note apoidea distributed indexing system share common underlying lookup protocol layer dht systems 
package apoidea distributed indexing scheme screen seti home search engine node uses idle cpu cycles network bandwidth 
related research area web crawlers centralized architectures 
single machine architectures presents design distributed crawler :10.1.1.109.4049
common feature architectures centralized location storing critical data structures central manager controlling various crawler components 
architecture complicated requires high maintenance 
controlling organization single link internet requires high bandwidth connection 
addition requires complicated load balancing dns requests 
decentralized solutions addition suffering mentioned issues potential benefits 
having autonomous solution prevents costly administration requirements fault tolerant 
immense potential exploit geographical proximity various peers web resources 
attempts developing decentralized versions crawlers 
boldi looked fault tolerance properties crawlers showing architecture fault tolerant 
attempted provide loosely connected design 
works provide concrete mechanism assigning jobs various peers load balancing 
mention done area distributed lookup protocols 
protocols proposed far :10.1.1.111.1818:10.1.1.140.3129:10.1.1.28.5987
apoidea web crawler built top chord :10.1.1.105.3673:10.1.1.105.3673
routed query protocols motivated file sharing applications 
really suitable file sharing applications due problems providing flexible mappings file name search phrases file identifiers 
insufficient mapping provided 
interesting see web crawling benefit approach extremely suitable application domain routed query protocols 
decentralized architecture web crawler 
described complete design structure system 
show optimizations geographical proximity peers web resources significantly better crawling performance results 
sample mapping scheme achieves goal 
initial experiments shown nice scalability architecture 
run system wide scenario test properties 
research side enhance url mapping scheme way implicitly handles geographical proximity 
security key problem autonomous collection peers 
intend explore issues preventing malicious behavior network 

aberer 
grid self organizing access structure information systems 
sixth international conference cooperative information systems 

zhao joseph 
tapestry infrastructure fault tolerance wide area location routing 
technical report ucb csd university california berkeley 

burton bloom 
space time trade offs hash coding allowable errors 
communications acm july 

boldi santini vigna 
ubicrawler scalability fault tolerance issues 
poster proceedings th international world wide web conference 

sergey brin lawrence page 
anatomy large scale hypertextual web search engine 
computer networks isdn systems 

burner 
crawling eternity building archive world wide web 
web techniques 

ian clarke oskar sandberg brandon wiley theodore hong 
freenet distributed anonymous information storage retrieval system 
lecture notes computer science 


keyword set search system peer peer networks 

gnutella 
gnutella home page 
gnutella wego com 

allan heydon marc najork 
mercator scalable extensible web crawler 
world wide web 

kazaa 
kazaa home page 
www kazaa com 

lu sinha 
scalable distributed index keyword search 
technical report 

ratnasamy francis handley karp shenker 
scalable content addressable network 


rowstron druschel 
pastry scalable distributed object location routing large scale peer peer systems 
proceedings th ifip acm international conference distributed systems platforms middleware november 

seti home 
seti home home page 
ssl berkeley edu 

shkapenyuk torsten suel 
design implementation high performance distributed web crawler 
proceedings international conference data engineering 

stoica morris karger kaashoek balakrishnan 
chord scalable peer peer lookup service internet applications 
proceedings sigcomm annual conference data communication august 

takahashi 
world wide web crawler 
poster proceedings th international world wide web conference 
