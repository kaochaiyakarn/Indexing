bayesian analysis number variational inference dirichlet process mixtures david blei school computer science carnegie mellon university michael jordan department statistics computer science division university california berkeley 
dirichlet process dp mixture models cornerstone nonparametric bayesian statistics development monte carlo markov chain mcmc sampling methods dp mixtures enabled application nonparametric bayesian methods variety practical data analysis problems 
mcmc sampling prohibitively slow important explore alternatives 
class alternatives provided variational methods class deterministic algorithms convert inference problems optimization problems opper saad wainwright jordan 
far variational methods mainly explored parametric setting particular formalism exponential family attias ghahramani beal blei :10.1.1.110.4050
variational inference algorithm dp mixtures 
experiments compare algorithm gibbs sampling algorithms dp mixtures gaussians application large scale image analysis problem 
keywords dirichlet processes hierarchical models variational inference image processing bayesian computation methodology monte carlo markov chain mcmc sampling bayesian statistics decade providing systematic approach computation likelihoods posterior distributions permitting deployment bayesian methods rapidly growing number applied problems 
success story mcmc unqualified mcmc methods slow converge convergence difficult diagnose 
research sampling needed important explore alternatives particularly context large scale problems 
class alternatives provided variational inference methods ghahramani beal jordan opper saad wainwright jordan 
mcmc variational inference methods roots statistical physics mcmc methods deterministic 
basic idea variational inference formulate computation marginal international society bayesian analysis ba variational inference dirichlet process mixtures conditional probability terms optimization problem 
generally intractable problem relaxed yielding simplified optimization problem depends number free parameters known variational parameters 
solving variational parameters gives approximation marginal conditional probabilities interest 
variational inference methods developed principally context exponential family convexity properties natural parameter space cumulant function yield elegant general variational formalism wainwright jordan 
example variational methods developed parametric hierarchical bayesian models general exponential family specifications ghahramani beal 
mcmc methods seen wider application 
particular development mcmc algorithms nonparametric models dirichlet process led increased interest nonparametric bayesian methods 
current aim close gap developing variational methods dirichlet process mixtures 
dirichlet process dp introduced ferguson measure measures 
dp parameterized base distribution positive scaling parameter 
suppose draw random measure dirichlet process independently draw random variables dp 

marginalizing random measure joint distribution 
follows urn scheme blackwell macqueen 
positive probability assigned configurations different take identical values underlying random measure discrete probability 
seen directly stick breaking representation dp represented explicitly infinite sum atomic measures sethuraman 
dirichlet process mixture model antoniak adds level hierarchy treating parameter distribution nth observation 
discreteness dp mixture interpretation mixture model unbounded number mixture components 
sample 
xn dp mixture goal compute predictive density 
xn 
xn hierarchical bayesian models posterior distribution 
xn complicated available closed form 
mcmc provides class approximations posterior predictive density maceachern escobar west neal :10.1.1.51.1747
ferguson parameterizes dirichlet process single base measure notation 
blei jordan variational inference algorithm dp mixtures stick breaking representation underlying dp 
algorithm involves probability distributions posterior distribution variational distribution endowed free variational parameters algorithmic problem adjust parameters approximates stick breaking representation case truncate representation yield representation 
principle truncate turning model finite dimensional model important emphasize outset approach truncate variational distribution 
organized follows 
section provide basic background dp mixture models focusing case exponential family mixtures 
section variational inference algorithms dp mixtures 
section overviews mcmc algorithms dp mixture discussing algorithms urn representation stick breaking representation 
section presents results experimental comparisons section presents analysis natural image data section presents 
dirichlet process mixture models continuous random variable non atomic probability distribution positive real valued scalar 
random measure distributed dirichlet process dp ferguson scaling parameter base distribution natural numbers partitions 
bk 
bk dir 
bk 
integrating joint distribution collection variables 
exhibits clustering effect conditioning draws nth value positive probability exactly equal draws 

variables 
randomly partitioned variables equal value distribution partition obtained urn scheme blackwell macqueen 

denote distinct values 

cn assignment variables ci denote number cells partition 
distribution follows urn distribution cj prob 
prob 
cj number times value occurs 

variational inference dirichlet process mixtures dirichlet process mixture model dp nonparametric prior hierarchical bayesian specification antoniak dp xn xn 
data generated model partitioned distinct values parameter 
view dp mixture natural interpretation flexible mixture model number components number cells partition random grows new data observed 
definition dp finite dimensional distributions equation kolmogorov consistency theorem ferguson 
sethuraman provides explicit characterization dp terms stick breaking construction 
consider infinite collections independent random variables vi beta 
stick breaking representation follows vi vj 
representation dp clear discrete probability support consists countably infinite set atoms drawn independently 
mixing proportions successively breaking unit length stick infinite number pieces 
size successive piece proportional rest stick independent draw beta distribution 
dp mixture vector comprises infinite vector mixing proportions 
atoms representing mixture components 
zn assignment variable mixture component data point xn associated 
data described arising process 
draw vi beta 

draw 

nth data point draw zn 
mult 
draw xn zn xn zn 
restrict dp mixtures observable data drawn exponential family distribution base distribution dp corresponding conjugate prior 
blei jordan graphical model representation exponential family dp mixture 
nodes denote random variables edges denote possible dependence plates denote replication 
stick breaking construction dp mixture depicted graphical model 
conditional distributions vk zn described 
distribution xn conditional zn 
xn zn 
xn exp xn zn appropriate cumulant function assume simplicity sufficient statistic natural parameter 
vector sufficient statistics corresponding conjugate family base distribution exp decompose hyperparameter contains dim components scalar 
variational inference dp mixtures direct way compute posterior distribution dp mixture prior 
approximate inference methods required dp mixtures markov chain monte carlo mcmc sampling methods methodology choice maceachern escobar west maceachern neal james :10.1.1.51.1747
variational inference provides alternative deterministic methodology approximating likelihoods posteriors wainwright jordan 
consider model hyperparameters latent variables 
wm observations 
xn 
posterior distribution latent variables exp log log 
variational inference dirichlet process mixtures working directly posterior typically precluded need compute normalizing constant 
log marginal probability observations log log dw may difficult compute latent variables dependent conditioning observed data 
mcmc algorithms circumvent computation constructing approximate posterior samples markov chain stationary distribution posterior interest 
gibbs sampling simplest mcmc algorithm iteratively samples latent variable conditioned previously sampled values latent variables wi exp log log 
normalizing constants conditional distributions assumed available analytically settings gibbs sampling appropriate 
variational inference reformulating problem computing posterior distribution optimization problem perturbing relaxing problem finding solutions perturbed problem wainwright jordan 
particular class variational methods known mean field methods 
optimizing kullback leibler kl divergence respect called variational distribution 
particular family distributions indexed variational parameter 
aim minimize kl divergence eq log eq log log omit variational parameters subscript expectation 
notice problematic marginal probability depend variational parameters ignored optimization 
minimization equation cast alternatively maximization lower bound log marginal likelihood log eq log eq log 
gap bound divergence true posterior 
mean field framework yield computationally effective inference method necessary choose family distributions tractably optimize equation 
constructing family typically breaks dependencies latent variables true posterior difficult compute 
sections consider fully factorized variational distributions break dependencies 
blei jordan mean field variational inference exponential families latent variable assume conditional distribution wi member exponential family wi wi exp gi wi gi gi natural parameter wi conditioning remaining latent variables observations 
setting natural consider family distributions variational approximations ghahramani beal exp wi wi 
variational parameters 
turns variational algorithm obtain fully factorized family reminiscent gibbs sampling 
particular show appendix optimization kl divergence respect single variational parameter achieved computing expectation eq gi 
repeatedly updating parameter turn computing expectation amounts performing coordinate ascent kl divergence 
notice interesting relationship algorithm gibbs sampler 
gibbs sampling iteratively draw latent variables wi distribution wi 
mean field variational inference iteratively update variational parameter setting equal expected value gi 
expectation computed variational distribution 
dp mixtures section develop mean field variational algorithm dp mixture 
algorithm stick breaking representation dp mixture see 
representation latent variables stick lengths atoms cluster assignments 
hyperparameters scaling parameter parameter conjugate base distribution 
general recipe equation write variational bound examples models wi exponential family distribution include hidden markov models mixture models state space models hierarchical bayesian models conjugate mixture conjugate priors 
variational inference dirichlet process mixtures log marginal probability data log eq log eq log eq log zn eq log xn zn eq log exploit bound find family variational distributions approximates distribution infinite dimensional random measure random measure expressed terms infinite sets 

considering truncated stick breaking representations 
fix value vt implies mixture proportions equal zero see equation 
truncated stick breaking representations considered previously james context sampling inference approximation dp mixture model 
note truncation different 
case model full dirichlet process truncated variational distribution truncated 
truncation level variational parameter freely set part prior model specification see section 
propose factorized family variational distributions variational inference vt zn vt beta distributions exponential family distributions natural parameters zn multinomial distributions 
notation section free variational parameters 



important note different variational parameter latent variable variational distribution 
example choice mixture component zn nth data point governed multinomial distribution indexed variational parameter reflects conditional nature variational inference 
coordinate ascent algorithm section explicit coordinate ascent algorithm optimizing bound equation respect variational parameters 
terms bound involve standard computations exponential family third term 
rewrite third term indicator random blei jordan variables eq log zn eq log vi zn zn zn eq log vi zn eq log vi recall eq log vt zn 
consequently truncate summation eq log zn zn eq log vi zn eq log vi zn zn eq log vi eq log vi 
digamma function denoted arises derivative log normalization factor beta distribution 
general expression equation derive mean field coordinate ascent algorithm 
yields txn 


exp st st eq log vt eq log vi eq xn eq iterating updates optimizes equation respect variational parameters defined equation 
practical applications variational methods address initialization variational distribution 
algorithm yields bound starting values variational parameters poor choices initialization lead local maxima yield poor bounds 
initialize variational distribution incrementally updating parameters random permutation data points 
viewed variational version sequential importance sampling 
run algorithm multiple times choose final parameter settings give best bound marginal likelihood 
variational inference dirichlet process mixtures compute predictive distribution variational posterior manner analogous way empirical approximation mcmc sampling algorithm 
predictive distribution xn xn dp 
factorized variational approximation posterior distribution atoms stick lengths decoupled infinite sum truncated 
consequently approximate predictive distribution product expectations straightforward compute variational approximation xn depends implicitly 
eq eq xn possible extensions 
conjugate simple coordinate ascent update may available particularly exponential family 
update available special case mixture conjugate distributions 
second important applications integrate diffuse prior scaling parameter 
show appendix straightforward extend variational algorithm include gamma prior 
gibbs sampling comparison variational inference review collapsed gibbs sampler blocked gibbs sampler dp mixtures 
collapsed gibbs sampling collapsed gibbs sampler dp mixture conjugate base distribution maceachern integrates random measure distinct parameter values 

markov chain defined latent partition 
cn 
recall denotes number cells partition 
algorithm iteratively samples assignment variable cn 
conditional cells partition assignment cn values nth data point cell data points cell 
exchangeability implies cn multinomial distribution cn xn cn cn 
blei jordan term ratio normalizing constants posterior distribution kth parameter including excluding nth data point xn cn exp cm xm xn cm exp cm xm cm second term urn scheme existing cell partition cn new cell partition denotes number data points kth cell partition chain reached stationary distribution collect samples 
cb approximate posterior 
approximate predictive distribution average predictive distributions monte carlo samples xn 
xn sample distribution xn cb cb xn cb 
cn cb xn cb cn 
conjugate distribution equation simple closed form 
effective algorithms handling case neal 
blocked gibbs sampling collapsed gibbs sampler assignment variable cn drawn distribution depends sampled values assignment variables 
consequently variables updated time potentially slow algorithm compared blocking strategy 
james developed blocked gibbs sampling algorithm stick breaking representation 
main issue face developing blocked gibbs sampler stick breaking dp mixture needs sample infinite collection stick lengths sampling finite collection cluster assignments james face issue defining truncated dirichlet process tdp vk set equal fixed value yields converts infinite sum equation finite sum 
james justify substituting variational inference dirichlet process mixtures tdp mixture model full dp mixture model showing truncated process closely approximates true dirichlet process truncation level chosen large relative number data points 
tdp mixture state markov chain consists beta variables 
vk mixture component parameters 
indicator variables 
zn 
blocked gibbs sampler iterates steps 

independently sample zn zn xn 

independently sample vk beta zn zn step follows conjugacy multinomial distribution truncated stick breaking construction generalized dirichlet distribution connor 


independently sample 
distribution family base distribution parameters zi xi zi chain reached stationary distribution collect samples construct approximate predictive distribution 
distribution average predictive distributions collected samples 
predictive distribution particular sample xn 
xn 
expectation product independent beta variables equation 
distribution depends variables needed gibbs sampling procedure integrated 
note approximation form similar approximate predictive distribution variational distribution equation 
variational case averaging done parametrically variational distribution monte carlo integral 
tdp sampler readily handles non conjugacy provided method sampling posterior 
blei jordan initial iteration iteration approximate predictive distribution variational inference different stages algorithm 
data points generated gaussian dp mixture model fixed diagonal covariance 
empirical comparison qualitatively variational methods offer potential advantages gibbs sampling 
deterministic optimization criterion equation assess convergence 
contrast assessing convergence gibbs sampler determining markov chain reached stationary distribution active field research 
theoretical bounds mixing time little practical consensus choose empirical methods developed purpose robert casella 
potential disadvantages variational methods 
optimization procedure fall prey local maxima variational parameter space 
local maxima mitigated restarts removed incorporation additional variational parameters strategies may slow convergence procedure 
second fixed variational representation yields approximation posterior 
methods considering hierarchies variational representations approach posterior limit methods may incur serious computational costs 
lacking theory issues evaluated general setting dp mixtures turn experimental evaluation 
studied performance variational algorithm section gibbs samplers section setting dp mixtures gaussians fixed inverse covariance matrix dp mixes mean gaussian 
natural conjugate base distribution dp gaussian covariance see equation 
provides illustrative example variational inference small problem involving data points sampled dimensional dp mixture gaussians diagonal covariance 
panel plots data presents variational inference dirichlet process mixtures mean convergence time standard error data sets dimension variational inference tdp gibbs sampling collapsed gibbs sampler 
predictive distribution variational inference algorithm iteration see equation 
truncation level set 
seen panel initialization variational parameters yields largely flat distribution 
iteration algorithm modes predictive distribution convergence refined modes 
mixture components represented variational distribution fitted approximate posterior uses 
compare variational inference algorithm gibbs sampling algorithms conducted systematic set simulation experiments dimensionality data varied 
covariance matrix autocorrelation matrix order autoregressive process chosen components highly dependent 
base distribution zero mean gaussian covariance appropriately scaled comparison dimensions 
scaling parameter set equal 
case generated data points dp mixture gaussians model chosen dimensionality generated additional points held data 
testing held data treated point st data point collection computed conditional probability algorithm approximate predictive distribution 
blei jordan dim mean held log probability std err variational collapsed gibbs truncated gibbs table average held log probability predictive distributions variational inference tdp gibbs sampling collapsed gibbs sampler 
log marginal probability bound truncation level held score iteration optimal bound log probability function truncation level left 
clusters simulated dimensional dp mixture gaussians data set 
held probability function iteration variational inference simulated data set right 
relative change log probability bound observations labeled selected iterations 
variational inference dirichlet process mixtures autocorrelation lag autocorrelation plots size largest component truncated dp gibbs sampler left collapsed gibbs sampler right example dataset dimensional gaussian data 
tdp approximation truncated components 
variational algorithm truncation level components 
note case truncation level simply variational parameter 
held fixed simulations possible optimize respect kl divergence 
left shows optimal kl divergence changes function truncation level simulated data sets 
ran algorithms convergence measured computation time 
collapsed gibbs sampler assessed convergence stationary distribution diagnostic raftery lewis collected additional samples estimate predictive distribution diagnostic provides appropriate lag collect uncorrelated samples 
assessed convergence blocked gibbs sampler statistic collapsed gibbs sampler number samples form approximate predictive distribution 
variational inference measured convergence relative change log marginal probability bound equation stopping algorithm certain inevitable arbitrariness choices general difficult envisage measures computation time allow stochastic mcmc algorithms deterministic variational algorithms compared standardized way 
consider reasonable pragmatic choices 
particular choice stopping time variational algorithm quite conservative illustrated right 
illustrates average convergence time datasets dimension 
caveats mind regarding convergence time measurement appears variational algorithm quite competitive mcmc algorithms 
variational timing computations pentium iii ghz desktop machine 
typically hundreds thousands samples mcmc algorithms form approximate posterior 
approximations offer additional predictive performance simulated data 
fair mcmc timing comparisons small number samples estimate predictive distributions 
autocorrelation lag blei jordan sample clusters dp mixture analysis images associated press 
left column posterior mean cluster followed top images associated 
clusters capture patterns data basketball shots outdoor scenes gray days faces pictures blue backgrounds 
algorithm faster exhibited significantly variance convergence time 
little evidence increase convergence time dimensionality variational algorithm range tested 
note collapsed gibbs sampler converged faster tdp gibbs sampler 
iteration collapsed gibbs slower iteration tdp gibbs tdp gibbs sampler required longer burn greater lag obtain uncorrelated samples 
illustrated autocorrelation plots 
comparing mcmc algorithms advantage truncated approximation 
table illustrates average log likelihood assigned held data approximate predictive distributions 
notice collapsed dp gibbs sampler assigned likelihood posterior tdp gibbs sampler indication quality tdp approximating dp 
importantly predictive distribution variational posterior assigned similar score samples true posterior 
approximation posterior resulting predictive distributions accurate class dp mixtures 
image analysis finite gaussian mixture models widely computer vision model natural images purposes automatic clustering retrieval classification barnard 
applications large scale data analysis problems involving thousands data points images hundreds dimensions pixels 
ap variational inference dirichlet process mixtures expected number images factor prior posterior expected number images allocated component variational posterior left 
posterior uses components describe data 
prior scaling parameter approximate posterior variational distribution right 
number mixture components problems generally unknown dp mixtures provide attractive alternative current methods 
deployment dp mixtures problems crucially requires inferential methods computationally efficient 
demonstrate applicability variational approach dp mixtures setting large datasets analyzed collection images associated press assumptions dp mixture gaussians model 
image reduced dimensional real valued vector grid average red green blue values 
fit dp mixture model mixture components gaussian mean covariance matrix base distribution product measure gamma 
furthermore placed gamma prior dp scaling parameter described appendix 
truncation level variational distribution 
variational algorithm required approximately hours converge 
resulting approximate posterior mixture components describe collection 
rough comparison gibbs sampling iteration collapsed gibbs takes minutes data set 
hours perform iterations 
chain converge stationary distribution provide sufficient number uncorrelated samples construct empirical estimate posterior 
left illustrates expected number images allocated component variational approximation posterior 
illustrates pictures highest approximate posterior probability associated components 
clusters appear capture basketball shots outdoor scenes gray days faces blue backgrounds 
right illustrates prior scaling parameter approximate posterior fitted variational distribution 
see blei jordan approximate posterior peaked different prior indicating data provided information regarding 
developed mean field variational inference algorithm dirichlet process mixture model demonstrated applicability kinds multivariate data gibbs sampling algorithms exhibit slow convergence 
variational inference faster gibbs sampling simulations convergence time independent dimensionality range tested 
variational mcmc methods strengths weaknesses methodology dominate general 
mcmc sampling provides theoretical guarantees accuracy variational inference provides fast deterministic approximation unattainable posteriors 
mcmc variational methods computational paradigms providing wide variety specific algorithmic approaches trade speed accuracy ease implementation different ways 
investigated deployment simplest form variational method dp mixtures mean field variational algorithm worth noting variational approaches described wainwright jordan worthy consideration nonparametric context 
variational inference exponential families appendix derive coordinate ascent algorithm variational inference described section 
recall considering latent variable model hyperparameters observed variables 
xn latent variables 
wm 
posterior written exp log log 
variational bound log marginal probability log eq log eq log 
bound holds distribution 
optimization bound computationally tractable restrict fully factorized variational distributions form wi 
variational parameters distribution exponential family ghahramani beal 
derive coordinate ascent algorithm iteratively maximize bound respect holding variational parameters fixed 
variational inference dirichlet process mixtures rewrite bound equation chain rule log log eq log wm 
wm eq log wm 
optimize respect reorder wi list 
portion equation depending eq log wi eq log wi 
variational distribution wi exponential family wi wi exp wi equation simplifies follows log wi log wi wi eq eq wi 
eq log wi eq log wi derivative respect optimal satisfies eq log wi eq log wi 
eq log wi eq log wi 
result equation general 
applications mean field methods including current simplification achieved 
particular conditional distribution wi exponential family distribution wi wi exp gi wi gi gi denotes natural parameter wi conditioning remaining latent variables observations 
yields simplified expressions expected log probability wi derivative eq log wi eq log wi eq gi eq gi eq log wi eq log wi eq gi 
derivative equation maximum attained eq gi 
blei jordan define coordinate ascent algorithm equation iteratively updating 

algorithm finds local maximum equation proposition bertsekas condition right hand side equation strictly convex 
relaxing assumptions complicates algorithm basic idea remains 
wi exponential family may analytic expression update equation 
fully factorized distribution second term bound equation eq log wi subsequent simplifications may applicable 
perspectives algorithms kind xing 
ghahramani beal 
general treatment variational methods statistical inference see wainwright jordan 
placing prior scaling parameter scaling parameter significant effect growth number components grows data generally important consider extended models integrate 
urn samplers escobar west place gamma prior implement corresponding gibbs updates auxiliary variable methods 
stick breaking representation gamma distribution convenient conjugate stick lengths 
write gamma distribution canonical form exp log shape parameter inverse scale parameter 
distribution conjugate beta 
log normalizer log log posterior parameters conditional data 
vk log vi extend variational inference algorithm include posterior updates scaling parameter 
variational distribution gamma 
variational parameters updated follows eq log vi replace expectation eq updates equation 
variational inference dirichlet process mixtures antoniak 
mixtures dirichlet processes applications bayesian nonparametric problems 
annals statistics 
attias 
variational bayesian framework graphical models 
advances neural information processing systems eds 
solla leen muller 
cambridge ma mit press 
barnard duygulu de freitas forsyth blei jordan 

matching words pictures 
journal machine learning research 
bertsekas 
nonlinear programming 
nh athena scientific 
blackwell macqueen 

ferguson distributions urn schemes 
annals statistics 
blei ng jordan 

latent dirichlet allocation 
journal machine learning research 
connor 

concepts independence proportions generalization dirichlet distribution 
journal american statistical association 
escobar west 

bayesian density estimation inference mixtures 
journal american statistical association 
ferguson 
bayesian analysis nonparametric problems 
annals statistics 
ghahramani beal 

propagation algorithms variational bayesian learning 
advances neural information processing systems eds 
leen dietterich tresp 
cambridge ma mit press 
james 

gibbs sampling methods stick breaking priors 
journal american statistical association 
lavrenko manmatha 

automatic image annotation retrieval cross media relevance models 
proceedings th annual international acm sigir conference research development information retrieval 
acm press 
jordan ghahramani jaakkola saul 

variational methods graphical models 
machine learning 
maceachern 
estimating normal means conjugate style dirichlet process prior 
communications statistics 


computational methods mixture dirichlet process models 
practical nonparametric semiparametric bayesian statistics eds 
dey muller sinha 
springer 
blei jordan neal 
markov chain sampling methods dirichlet process mixture models 
journal computational graphical statistics 
opper saad 

advanced mean field methods theory practice 
cambridge ma mit press 
raftery lewis 

long run diagnostics implementation strategies markov chain monte carlo 
statistical science 
robert casella 

monte carlo statistical methods 
new york ny springer verlag 
sethuraman 
constructive definition dirichlet priors 
statistica sinica 
wainwright jordan 

graphical models exponential families variational inference 
tech 
rep berkeley dept statistics 

variational approximations mean field theory junction tree algorithm 
proceedings th annual conference uncertainty artificial intelligence uai eds 
boutilier goldszmidt 
san francisco ca morgan kaufmann publishers 
xing jordan russell 

generalized mean field algorithm variational inference exponential families 
proceedings th annual conference uncertainty artificial intelligence uai eds 
meek kj 
san francisco ca morgan kaufmann publishers 
acknowledgments edwards providing ap image data 
want acknowledge support intel microsoft research darpa support calo project 
authors david blei postdoctoral researcher carnegie mellon university 
michael jordan professor statistics professor electrical engineering computer science university california berkeley 
