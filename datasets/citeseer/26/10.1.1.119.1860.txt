hierarchical bayesian markovian model motifs biopolymer sequences eric xing michael jordan richard karp stuart russell computer science division university california berkeley berkeley ca jordan karp russell cs berkeley edu propose dynamic bayesian model motifs biopolymer sequences captures rich biological prior knowledge positional dependencies motif structure principled way 
model posits position specific multinomial parameters monomer distribution distributed latent dirichlet mixture random variable position specific dirichlet component determined hidden markov process 
model parameters fit training motifs variational em algorithm empirical bayesian framework 
variational inference detecting hidden motifs 
model improves previous models ignore biological priors positional dependence 
higher sensitivity motifs detection notable ability distinguish genuine motifs false recurring patterns 
identification motif structures biopolymer sequences proteins dna important task computational biology essential advancing knowledge biological systems 
example gene regulatory motifs dna provide key clues regulatory network underlying complex control coordination gene expression response physiological environmental changes living cells 
lines research statistical modeling motifs led algorithms motif detection meme unfortunately algorithms simple motif patterns incapable distinguishing biologists recognize true motif random recurring pattern provide mechanism incorporating biological knowledge motif structure sequence composition :10.1.1.40.3938
motif models assume independence position specific multinomial distributions monomers nucleotides nt acids aa 
strategies contradict intuition sites motifs naturally possess spatial dependencies functional reasons 
furthermore vague dirichlet prior models acts smoother little consideration rich prior knowledge biologically identified motifs 
describe new model monomer distribution motifs 
model finite set informative dirichlet distributions order markov model transitions 
distribution monomers continuous mixture position specific multinomials admit dirichlet prior hidden markov states introducing multi modal prior information dependencies 
propose framework decomposing general motif model local alignment model motif pattern global model motif instance distribution allows complex models developed modular way 
simplify discussion dna motif modeling running example clear model applicable sequence modeling problems 
preliminaries dna motifs short bp stochastic string patterns regulatory sequences genes facilitate control functions interacting specific transcriptional regulatory proteins 
motif typically appears multiple times control regions small set genes 
gene usually motifs 
know patterns motifs gene appear appear 
goal motif detection identify instances possible motifs hidden sequences learn model motif prediction 
regulatory dna sequence fully specified character string indicator string signals locations motif occurrences 
reason call motif stochastic string pattern word due variability spellings different instances motif genome 
conventionally biologists display motif pattern length multi alignment instances 
stochasticity motif patterns reflected heterogeneity nucleotide species appearing column corresponding position site motif multi alignment 
denote multi alignment instances motif specified indicator string sequence characterized nucleotide counts column define counting matrix column integer vector elements giving number occurrences nucleotide position motif 
similarly define counting vector sequence settings model nt distribution position motif position specific multinomial distribu tion formally problem inferring called position weight matrix pwm sequence set motif detection nutshell abf mat gal mcb gcn mig gcr crp yeast motifs solid line bp flanking regions dashed line 
axis indexes position axis represents information content multinomial distribution nt po sition note typical patterns shape bell shape 
xt left general motif model bayes ian multinet 
conditional value admits different distributions round boxes parameterized 
right model motif instances specified boxes plates representing replicates 
multiple motif detection formulated similar way simplicity omit elaboration 
see full details 
simplicity omit superscript sequence index variable unnecessary 
generative models regulatory dna sequences general setting related loss generality assume occurrences motifs dna sequence indicated governed global distribution type motif nucleotide sequence pattern shared instances admits local alignment model 
usually background non motif sequences modeled model simple conditional background nt distribution parameters assumed learned priori entire sequence supplied constants motif detection process 
symbols stand parameters model classes respective submodels 
likelihood regulatory sequence note necessarily equivalent position specific multinomial parameters eq 
generic symbol parameters general model aligned motif instances 
model captures properties frequencies different motifs dependencies motif occurrences 
specifying model important aspect motif detection remains largely unexplored defer issue 
current focus capturing intrinsic properties motifs help improve sensitivity specificity genuine motif patterns 
key lies model local alignment determines pwm motif 
depending value latent indicator motif position admits different probabilistic models motif alignment model background model 
sequence characterized bayesian multinet mixture model component mixture specific nt distribution model corresponding sequences particular nature 
goal develop expressive model local alignment capable capturing characteristic site dependencies motifs 
standard product multinomial pm model local alignment columns pwm assumed independent 
likelihood multi alignment popular model motif finders pm sensitive noise random trivial recurrent patterns unable capture potential site dependencies inside motifs 
pattern driven auxiliary submodels fragmentation model heuristics split block motif coupled sub motifs developed handle special patterns shaped motifs inflexible difficult generalize 
literature introduced vague dirichlet priors pm primarily smoothing explicitly incorporating prior motifs 
depart pm model introduce dynamic hierarchical bayesian model motif alignment captures site dependencies inside motif predict biologically plausible motifs incorporate prior knowledge nucleotide frequencies general motif sites 
order keep local alignment model main focus simplifying presentation adopt idealized global motif distribution model called sequence name suggests assumes sequence motif instance unknown location 
generalization expressive global models straightforward described full 
hidden markov dirichlet multinomial model model assume underlying latent nt distribution prototypes position specific multinomial distributions nt determined prototype represented dirichlet distribution 
furthermore choice prototype position motif governed order markov process 
precisely multi alignment containing motif instances generated process 
sample sequence prototype indicators order markov process initial distribution transition matrix repeat column component mixture picked indicator say picked 
multinomial distribution sampled probability defined dirichlet component distributions 
nucleotides column generated multi complete likelihood motif alignment characterized counting matrix major role impose dynamic priors modeling data distributions exhibit temporal spatial dependencies 
clear model simple hmm discrete sequences 
model transition emission models multinomials output time single data instance sequence 
transitions different priors emission models direct output hmm parameter vector generative model sampled multiple times position generate random instances 
approach especially useful empirical learned prior knowledge dynamics data modeled 
example case motifs biological evidence show conserved positions manifested low entropy multinomial nt distribution concatenate conserved positions 
conserved conserved positions interpolated 
called site clustering main motivations model 
inference learning variational bayesian learning order bayesian estimation motif parameter predict locations motif instances need able compute distribution posterior infeasible complex motif model 
turn variational approximation 
seek approximate joint posterior parameters states hidden simpler distribution time thought free distributions optimized 
jensen inequality lower log likelihood kl maximizing lower bound log likelihood call free distributions true joint posterior variational approximation 
keeping respect equivalent minimizing kl divergence fixed maximizing respect obtain coupled updates motif model prior conditional submodels form conjugate exponential pair dirichlet multinomial 
shown case essentially recover form original conditional prior distributions variational approximations parameterization augmented appropriate bayesian posterior updates respectively 
eqs 
clear locality inference marginalization latent variables preserved variational approximation means probabilistic calculations performed prior conditional models separately iteratively 
motif modeling modular property means motif alignment model motif distribution model treated separately simple interface posterior mean motif parameters expected sufficient statistics motif instances 
natural parameter inference learning eq 
replace counting matrix eq 
output model expected counting matrix obtained inference global distribution model handle locality preservation property inference variational approximations proceed inference observations integrating marginal distribution standard hmm emission probability compute posterior probability hidden matrix states probabilities occurrence standard forward backward algorithm 
compute expectation natural parameters parameters 
observations posterior mean computed follows multinomial posterior probability hidden state output forward backward algorithm digamma function 
eq 
posterior means multinomial parameters computing expected counting matrix sequence global model sequence set straightforward eq 
simply give final results posterior dirichlet parameters bayesian estimates multinomial parameters position specific nt distribution motif obtained fixed point iteration em procedure variational step compute expected sufficient statistic count matrix inference global motif model variational step compute expected natural parameter inference local motif alignment model basic inference learning procedure provides framework scales readily complex models 
example motif distribution model sophisticated model complex properties multiple motifs motif level dependencies occurrence overlaps concentration regulatory modules complicating inference local alignment model 
similarly motif alignment model expressive mixture interfering inference motif distribution model 
experiments test model motif collection promoter database saccharomyces cerevisiae 
dataset contains motifs instances identified biological experiments 
experiment showing capture intrinsic properties motifs 
posterior distribution position specific multinomial parameters reflected parameters dirichlet mixtures learned data reveal patterns motifs 
examining transition probabilities different dirichlet components tells dependencies adjacent positions indirectly reveals shape information 
set total number dirichlet components intelligent guess biological intuition shows dirichlet parameters fitted dataset empirical bayes estimation 
dirichlet components numbers favor pure distribution single nucleotides respectively suggesting correspond homogeneous prototypes 
numbers favor near uniform distribution nt types heterogeneous prototypes 
components somewhat 
patterns agree biological definition motifs 
interestingly learned transition model hmm seen transition probability homogeneous prototype heterogeneous prototype significantly homogeneous heterogeneous prototypes confirming empirical speculation biology motifs called site clustering property 
abf hit abf mis hit gal hit gal mis hit dirichlet hyperparameters 
markov transition matrix 
boxplots hit rate pm motifs training 
motif properties captured useful motif detection 
examine trained complete dataset ability detect motifs training presence decoy permuted motif 
randomly permuting positions motif shapes shaped motifs abf gal change dramatically 
insert instance motif decoy pair bp random background sequence random position allow bp offset tolerance window score hit mis hit motif instance 
mis hit rate proportion mis hits total number motif instances experiment 
shows boxplot hit rate abf gal randomly generated experiments 
note dramatic contrast sensitivity true motifs compared pm model essentially meme model 
abf mat gal mcb gcn mig gcr crp abf mat gal mcb position gcn mig gcr crp true motif true motif decoy motif detection independent test dataset motifs 
models indexed 
bell 

mixture 
pm 
boxplot hit rate randomly generated experiments center notch median 
generalize 
split data training set testing set divide training set roughly bell shaped shaped patterns train different respectively mixture 
motif finding task sequences true motif instance random position 
results 
see motifs mixtures significantly improves performance pm model 
cases comparable motif mcb models lose 
note mcb conserved fact atypical training set 
short diminishes utility hmm 
interesting observation perform poorly mixtures perform mat presumably extra flexibility provided mixture model 
second task challenging biologically realistic true motifs permuted show hit rate experiments 
cases mixture outperforms pm 
generative probabilistic framework modeling motifs biopolymer sequences 
naively categorical random variables spatial temporal dependencies modeled standard hmm multinomial emission models 
limited flexibility multinomial distribution concomitant need potentially large number states model complex domains may require large parameter count lead overfitting 
infinite hmm solve issue replacing emission model dirichlet process provides potentially infinite flexibility 
approach purely data driven provides mechanism explicitly capturing multi modality permutation mean time permuted order applied instances motif multinomial distribution position changed order changed 
resisted temptation biological background sequences know motifs sequences renders ill suited purposes evaluation 
emission transition models incorporating informative priors 
furthermore output hmm involves hidden variables case motif detection inference learning complicated 
assumes positional dependencies induced higher level finite number informative dirichlet priors multinomials 
framework explicitly capture multi modalities multinomial distributions governing categorical variable motif sequences different positions dependencies modalities learning model parameters training data predictions 
motif modeling strategy capture different distribution patterns nucleotides homogeneous heterogeneous transition properties patterns site clustering 
prior proves beneficial searching unseen motifs experiment helps distinguish probable motifs biologically meaningless random recurrent patterns 
motif detection setting model involves complex missing data problem output internal states hidden show variational bayesian learning procedure allows probabilistic inference prior model motif sequence patterns global distribution model motif locations carried virtually separately bayesian interface connecting processes 
divide conquer strategy easier develop sophisticated models various aspects motif analysis somewhat daunting complexity full motif problem 
bailey elkan :10.1.1.40.3938
unsupervised learning multiple motifs biopolymers em 
machine learning 
bailey elkan 
value prior knowledge discovering motifs meme 
proc 
rd international conf 
intelligent systems molecular biology 
beal ghahramani rasmussen 
infinite hidden markov model 
proc 
th conference advances neural information processing systems 
eisen 
structural properties transcription factor dna interactions inference sequence specificity 
manuscript preparation 
ghahramani beal 
propagation algorithms variational bayesian learning 
proc 
th conference advances neural information processing systems 
heckerman geiger chickering 
learning bayesian networks combination knowledge statistics data 
machine learning 
lawrence reilly 
expectation maximization em algorithm identification characterization common sites unaligned biopolymer sequences 
proteins 
lawrence altschul boguski liu neuwald 
detecting subtle sequence signals gibbs sampling strategy multiple alignment 
science 
liu liu brutlag 
discovering conserved dna motifs upstream regulatory regions expressed genes 
proc 
psb 
liu neuwald lawrence 
bayesian models multiple local sequence alignment gibbs sampling strategies 
amer 
statistical assoc 

deciphering genetic regulatory codes challenge functional genomics 
proc 
natl 
acad 
sci 
usa 
